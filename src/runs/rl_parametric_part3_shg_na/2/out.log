Using TensorFlow backend.
[2019-03-26 09:44:34,535] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0005, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='None', model_param=[19, 1], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-03-26 09:44:34,536] A3C_AGENT_MAIN INFO:Start compiling...
2019-03-26 09:44:34.567662: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-03-26 09:44:50,752] A3C_AGENT_MAIN INFO:Start the learning...
[2019-03-26 09:44:50,752] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-03-26 09:44:50,761] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-03-26 09:44:50,766] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-03-26 09:44:50,770] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-03-26 09:44:50,775] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-03-26 09:44:50,778] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-03-26 09:44:50,778] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:50,779] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-03-26 09:44:50,829] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:50,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-03-26 09:44:51,780] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:51,781] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-03-26 09:44:51,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:51,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-03-26 09:44:51,945] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 09:44:51,946] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 09:44:51,946] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 09:44:51,946] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:51,947] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 09:44:51,947] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 09:44:51,947] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 09:44:51,947] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:51,947] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:51,948] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:51,948] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:51,951] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run1
[2019-03-26 09:44:51,958] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run1
[2019-03-26 09:44:51,959] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run1
[2019-03-26 09:44:51,964] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run1
[2019-03-26 09:44:51,984] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run1
[2019-03-26 09:44:52,782] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:52,783] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-03-26 09:44:52,871] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:52,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-03-26 09:44:53,784] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:53,785] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-03-26 09:44:53,844] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:53,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-03-26 09:44:54,786] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:54,787] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-03-26 09:44:54,864] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:54,866] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-03-26 09:44:55,788] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:55,797] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-03-26 09:44:55,851] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:55,852] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-03-26 09:44:56,796] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:56,799] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-03-26 09:44:56,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:56,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-03-26 09:44:57,799] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:57,804] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-03-26 09:44:57,857] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:57,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-03-26 09:44:58,804] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:58,807] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-03-26 09:44:58,863] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:58,864] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-03-26 09:44:59,807] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:44:59,810] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-03-26 09:44:59,864] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:44:59,865] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-03-26 09:45:00,810] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:45:00,814] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-03-26 09:45:00,865] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:45:00,866] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-03-26 09:45:01,813] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:45:01,813] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-03-26 09:45:01,918] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:45:01,919] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-03-26 09:45:02,816] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:45:02,823] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-03-26 09:45:02,899] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:45:02,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-03-26 09:45:03,822] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:45:03,825] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-03-26 09:45:03,882] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:45:03,883] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-03-26 09:45:04,825] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:45:04,829] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-03-26 09:45:04,883] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:45:04,883] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-03-26 09:45:05,827] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 09:45:05,828] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-03-26 09:45:05,931] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:45:05,932] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-03-26 09:45:16,055] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 09:45:16,057] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.13333333333334, 86.33333333333334, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.250408988893335, 6.9112, 6.9112, 178.6582176852504, 639429.5955889239, 639429.5955889239, 252801.4314873759]
[2019-03-26 09:45:16,058] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 09:45:16,065] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0535947  0.14347851 0.39987195 0.01719494 0.38585994], sampled 0.23610746222774603
[2019-03-26 09:45:32,263] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 09:45:32,264] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.11527162166666, 94.99084746999999, 1.0, 2.0, 0.1821854517180591, 1.0, 1.0, 0.1821854517180591, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 556576.1197687534, 556576.1197687534, 239538.2842379324]
[2019-03-26 09:45:32,265] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:45:32,272] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.06101233 0.16453105 0.4315959  0.02605807 0.3168027 ], sampled 0.7420845780752665
[2019-03-26 09:45:34,816] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 09:45:34,818] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.95, 60.0, 1.0, 2.0, 0.3730536691753646, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6291586966774065, 6.9112, 6.9112, 168.912956510431, 1042737.898484675, 1042737.898484675, 245842.8902953125]
[2019-03-26 09:45:34,818] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 09:45:34,821] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.07842515 0.25617522 0.3056768  0.01799875 0.3417241 ], sampled 0.8160393240800407
[2019-03-26 09:45:37,252] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 09:45:37,253] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.49870068333333, 100.0, 1.0, 1.0, 0.4353724731848373, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 149.0743665178227, 721860.0099612137, 721860.0099612141, 182562.2140968803]
[2019-03-26 09:45:37,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 09:45:37,260] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.07066834 0.20412642 0.43911436 0.02954604 0.2565449 ], sampled 0.9116656339240261
[2019-03-26 09:46:03,691] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 09:46:03,693] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.38103824666667, 80.27954042333333, 1.0, 1.0, 0.4905362347536957, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690347.7645093804, 690347.7645093804, 182756.8328475744]
[2019-03-26 09:46:03,694] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 09:46:03,696] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.042978   0.3231816  0.42545018 0.01476634 0.19362386], sampled 0.4732934713972764
[2019-03-26 09:46:34,681] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 09:46:34,681] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.83333333333333, 96.5, 1.0, 2.0, 0.4782711945604676, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8041290940658217, 6.911199999999999, 6.9112, 168.912956510431, 1337020.968496157, 1337020.968496158, 291970.472743698]
[2019-03-26 09:46:34,683] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 09:46:34,685] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.05419596 0.26274037 0.33024922 0.00826204 0.34455237], sampled 0.6373786567135268
[2019-03-26 09:46:41,988] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 09:46:41,989] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.97594218333334, 87.34692638333334, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 1.0, 1.0, 0.2284255170402581, 6.9112, 6.9112, 171.5212843490159, 585468.070235454, 585468.070235454, 243165.0309737229]
[2019-03-26 09:46:41,990] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:46:41,992] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0528699  0.12051538 0.41448113 0.01278071 0.3993529 ], sampled 0.6930837613195721
[2019-03-26 09:46:45,304] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3691.9713 3479969273.2159 1325.0000
[2019-03-26 09:46:45,384] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 3899.0030 3322720570.4903 1113.0000
[2019-03-26 09:46:45,420] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3784.6397 3181891665.5123 745.0000
[2019-03-26 09:46:45,426] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 4083.6651 3258764925.3631 829.0000
[2019-03-26 09:46:45,525] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 3968.9096 3142529664.6362 564.0000
[2019-03-26 09:46:46,540] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3691.971337022131, 3479969273.2158566, 1325.0, 4083.665122791275, 3258764925.363056, 829.0, 3968.9095944169767, 3142529664.6362004, 564.0, 3899.003020553077, 3322720570.4902754, 1113.0, 3784.6397149924915, 3181891665.5123124, 745.0]
[2019-03-26 09:46:53,129] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0471294e-06 6.1379382e-03 9.8248518e-01 1.3093192e-07 1.1371610e-02], sum to 1.0000
[2019-03-26 09:46:53,134] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9769
[2019-03-26 09:46:53,239] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.83333333333334, 86.33333333333334, 1.0, 2.0, 0.1766542933670885, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3141249776078392, 6.911200000000001, 6.9112, 168.912956510431, 544875.285667902, 544875.2856679014, 195937.5474385563], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 78000.0000, 
sim time next is 78600.0000, 
raw observation next is [22.76666666666667, 86.66666666666666, 1.0, 2.0, 0.1763287070975055, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3136696274056821, 6.911199999999999, 6.9112, 168.912956510431, 544213.7421216262, 544213.7421216267, 195905.3640356013], 
processed observation next is [1.0, 0.9130434782608695, 0.2780410742496052, 0.8666666666666666, 1.0, 1.0, 0.007624948310247582, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.16301174073863672, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15117048392267393, 0.1511704839226741, 0.29239606572477805], 
reward next is 0.7076, 
noisyNet noise sample is [array([0.50529957], dtype=float32), 1.2613119]. 
=============================================
[2019-03-26 09:47:05,458] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7869: loss 0.0073
[2019-03-26 09:47:05,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7869: learning rate 0.0005
[2019-03-26 09:47:05,570] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7905: loss 0.0001
[2019-03-26 09:47:05,570] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7905: learning rate 0.0005
[2019-03-26 09:47:05,625] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7935: loss 0.0679
[2019-03-26 09:47:05,629] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7938: learning rate 0.0005
[2019-03-26 09:47:05,632] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7939: loss 0.0996
[2019-03-26 09:47:05,633] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7939: learning rate 0.0005
[2019-03-26 09:47:05,657] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7951: loss 0.1312
[2019-03-26 09:47:05,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7952: learning rate 0.0005
[2019-03-26 09:47:05,676] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7960: loss 0.1541
[2019-03-26 09:47:05,678] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7960: learning rate 0.0005
[2019-03-26 09:47:05,717] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7981: loss 0.1023
[2019-03-26 09:47:05,719] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7982: learning rate 0.0005
[2019-03-26 09:47:05,736] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7991: loss 0.1644
[2019-03-26 09:47:05,737] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7992: learning rate 0.0005
[2019-03-26 09:47:05,765] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 8004: loss 0.0900
[2019-03-26 09:47:05,772] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 8006: learning rate 0.0005
[2019-03-26 09:47:05,773] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 8006: loss 0.0972
[2019-03-26 09:47:05,778] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 8009: learning rate 0.0005
[2019-03-26 09:47:05,797] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8019: loss 0.0391
[2019-03-26 09:47:05,799] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8020: learning rate 0.0005
[2019-03-26 09:47:05,802] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8021: loss 0.0138
[2019-03-26 09:47:05,803] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8021: learning rate 0.0005
[2019-03-26 09:47:05,844] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8037: loss 0.0165
[2019-03-26 09:47:05,848] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8037: learning rate 0.0005
[2019-03-26 09:47:05,860] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8044: loss 0.0021
[2019-03-26 09:47:05,863] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8044: learning rate 0.0005
[2019-03-26 09:47:05,878] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8054: loss 0.0014
[2019-03-26 09:47:05,879] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8054: learning rate 0.0005
[2019-03-26 09:47:05,895] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8062: loss 0.0033
[2019-03-26 09:47:05,899] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8062: learning rate 0.0005
[2019-03-26 09:47:16,367] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2482039e-11 9.9999857e-01 1.1651751e-06 1.7746033e-14 2.4584855e-07], sum to 1.0000
[2019-03-26 09:47:16,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8010
[2019-03-26 09:47:16,487] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 53.0, 1.0, 2.0, 0.580819256691232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 951152.6045233355, 951152.6045233348, 211520.7097591924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 486000.0000, 
sim time next is 486600.0000, 
raw observation next is [25.06666666666667, 53.0, 1.0, 2.0, 0.5029257488295957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 823978.8810772432, 823978.8810772427, 196343.2298963897], 
processed observation next is [1.0, 0.6521739130434783, 0.38704581358609813, 0.53, 1.0, 1.0, 0.40111536003565745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22888302252145645, 0.2288830225214563, 0.29304959686028315], 
reward next is 0.7070, 
noisyNet noise sample is [array([1.3681407], dtype=float32), -0.94011784]. 
=============================================
[2019-03-26 09:47:17,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2089207e-13 9.9999964e-01 2.5521479e-07 2.6865169e-17 1.1653058e-07], sum to 1.0000
[2019-03-26 09:47:17,736] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5783
[2019-03-26 09:47:17,847] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 79.5, 1.0, 2.0, 0.2408018876667755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 398325.5686412163, 398325.5686412163, 159918.3715501579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 505800.0000, 
sim time next is 506400.0000, 
raw observation next is [19.96666666666667, 80.0, 1.0, 2.0, 0.2409283209151263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 398606.4369118143, 398606.436911815, 159924.9910059306], 
processed observation next is [1.0, 0.8695652173913043, 0.14533965244865735, 0.8, 1.0, 1.0, 0.08545580833147746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11072401025328175, 0.11072401025328195, 0.2386940164267621], 
reward next is 0.7613, 
noisyNet noise sample is [array([1.576691], dtype=float32), -2.225777]. 
=============================================
[2019-03-26 09:47:22,918] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15849: loss 0.3937
[2019-03-26 09:47:22,921] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15849: learning rate 0.0005
[2019-03-26 09:47:22,996] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15885: loss 0.5713
[2019-03-26 09:47:22,998] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15886: loss 0.4676
[2019-03-26 09:47:22,998] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15886: learning rate 0.0005
[2019-03-26 09:47:23,000] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15886: learning rate 0.0005
[2019-03-26 09:47:23,037] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15906: loss 0.4690
[2019-03-26 09:47:23,039] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15906: learning rate 0.0005
[2019-03-26 09:47:23,096] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15932: loss 0.3042
[2019-03-26 09:47:23,099] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15934: learning rate 0.0005
[2019-03-26 09:47:23,183] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15972: loss 0.1168
[2019-03-26 09:47:23,189] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15973: loss 0.0865
[2019-03-26 09:47:23,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15972: learning rate 0.0005
[2019-03-26 09:47:23,193] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15973: learning rate 0.0005
[2019-03-26 09:47:23,263] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16009: loss 0.0219
[2019-03-26 09:47:23,265] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16010: loss 0.0031
[2019-03-26 09:47:23,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16010: learning rate 0.0005
[2019-03-26 09:47:23,270] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16010: learning rate 0.0005
[2019-03-26 09:47:23,298] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16020: loss 0.0014
[2019-03-26 09:47:23,304] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16022: learning rate 0.0005
[2019-03-26 09:47:23,310] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 16024: loss 0.0065
[2019-03-26 09:47:23,316] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 16024: learning rate 0.0005
[2019-03-26 09:47:23,331] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16035: loss 0.0008
[2019-03-26 09:47:23,337] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16037: learning rate 0.0005
[2019-03-26 09:47:23,372] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16060: loss 0.0013
[2019-03-26 09:47:23,377] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16062: learning rate 0.0005
[2019-03-26 09:47:23,400] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16075: loss 0.0021
[2019-03-26 09:47:23,401] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16075: learning rate 0.0005
[2019-03-26 09:47:23,417] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16080: loss 0.0036
[2019-03-26 09:47:23,421] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16080: learning rate 0.0005
[2019-03-26 09:47:23,438] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16088: loss 0.0053
[2019-03-26 09:47:23,439] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16088: learning rate 0.0005
[2019-03-26 09:47:29,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5640924e-15 1.0000000e+00 3.6848962e-09 2.9710511e-17 1.2640765e-09], sum to 1.0000
[2019-03-26 09:47:29,057] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7225
[2019-03-26 09:47:29,173] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.6, 93.0, 1.0, 2.0, 0.2214840591204185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 368918.9041889188, 368918.9041889194, 157741.2396162729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 702000.0000, 
sim time next is 702600.0000, 
raw observation next is [17.6, 92.83333333333333, 1.0, 2.0, 0.220778831403438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367787.6481783985, 367787.6481783985, 157666.9052754362], 
processed observation next is [1.0, 0.13043478260869565, 0.0331753554502371, 0.9283333333333332, 1.0, 1.0, 0.06117931494390118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10216323560511069, 0.10216323560511069, 0.23532373921706895], 
reward next is 0.7647, 
noisyNet noise sample is [array([-0.22270745], dtype=float32), -1.0852393]. 
=============================================
[2019-03-26 09:47:31,752] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0990387e-12 9.9999964e-01 1.4576648e-07 5.9774497e-15 1.9929902e-07], sum to 1.0000
[2019-03-26 09:47:31,761] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7432
[2019-03-26 09:47:31,860] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 49.5, 1.0, 2.0, 0.5500315423677762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 905698.99976021, 905698.9997602093, 205294.7593062091], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 747000.0000, 
sim time next is 747600.0000, 
raw observation next is [25.23333333333333, 49.66666666666666, 1.0, 2.0, 0.6109809502398384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1006422.373508957, 1006422.373508958, 217980.0518838072], 
processed observation next is [1.0, 0.6521739130434783, 0.39494470774091617, 0.4966666666666666, 1.0, 1.0, 0.5313023496865523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2795617704191547, 0.27956177041915503, 0.3253433610206078], 
reward next is 0.6747, 
noisyNet noise sample is [array([-0.0678428], dtype=float32), 0.019438617]. 
=============================================
[2019-03-26 09:47:37,434] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.3603164e-16 1.0000000e+00 2.0730684e-09 3.2578837e-19 9.5103120e-11], sum to 1.0000
[2019-03-26 09:47:37,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2282
[2019-03-26 09:47:37,539] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 85.33333333333333, 1.0, 2.0, 0.3024213540901036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480559.3748945583, 480559.3748945583, 165644.8633146615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 855600.0000, 
sim time next is 856200.0000, 
raw observation next is [21.83333333333334, 85.66666666666667, 1.0, 2.0, 0.3034191572796478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482038.306052571, 482038.306052571, 165749.320943273], 
processed observation next is [0.0, 0.9130434782608695, 0.23380726698262277, 0.8566666666666667, 1.0, 1.0, 0.16074597262608167, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13389952945904748, 0.13389952945904748, 0.24738704618398955], 
reward next is 0.7526, 
noisyNet noise sample is [array([-0.07202433], dtype=float32), -2.1251328]. 
=============================================
[2019-03-26 09:47:37,889] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.0352486e-17 1.0000000e+00 1.8047858e-09 2.0007628e-20 2.1995230e-11], sum to 1.0000
[2019-03-26 09:47:37,900] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7051
[2019-03-26 09:47:37,904] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 87.5, 1.0, 2.0, 0.306765042468953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486322.4256024822, 486322.4256024816, 166040.3258112925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 858600.0000, 
sim time next is 859200.0000, 
raw observation next is [21.66666666666667, 88.0, 1.0, 2.0, 0.3075302115573703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487202.2530669138, 487202.2530669138, 166097.8522348183], 
processed observation next is [0.0, 0.9565217391304348, 0.22590837282780438, 0.88, 1.0, 1.0, 0.16569905006912086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13533395918525384, 0.13533395918525384, 0.24790724214151985], 
reward next is 0.7521, 
noisyNet noise sample is [array([-1.1706144], dtype=float32), -0.21907547]. 
=============================================
[2019-03-26 09:47:39,744] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23833: loss 0.0616
[2019-03-26 09:47:39,745] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23833: learning rate 0.0005
[2019-03-26 09:47:39,789] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23855: loss 0.1119
[2019-03-26 09:47:39,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23855: learning rate 0.0005
[2019-03-26 09:47:39,935] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23921: loss 0.0062
[2019-03-26 09:47:39,938] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23921: learning rate 0.0005
[2019-03-26 09:47:39,971] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23939: loss 0.0002
[2019-03-26 09:47:39,972] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23940: learning rate 0.0005
[2019-03-26 09:47:39,997] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23952: loss 0.0075
[2019-03-26 09:47:40,000] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23952: learning rate 0.0005
[2019-03-26 09:47:40,001] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23954: loss 0.0042
[2019-03-26 09:47:40,001] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23954: loss 0.0094
[2019-03-26 09:47:40,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23954: learning rate 0.0005
[2019-03-26 09:47:40,006] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23954: learning rate 0.0005
[2019-03-26 09:47:40,035] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23969: loss 0.0156
[2019-03-26 09:47:40,037] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23969: learning rate 0.0005
[2019-03-26 09:47:40,120] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24007: loss 0.0467
[2019-03-26 09:47:40,123] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24008: learning rate 0.0005
[2019-03-26 09:47:40,149] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24020: loss 0.0519
[2019-03-26 09:47:40,152] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24021: learning rate 0.0005
[2019-03-26 09:47:40,161] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 24027: loss 0.0292
[2019-03-26 09:47:40,165] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 24027: learning rate 0.0005
[2019-03-26 09:47:40,204] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 24046: loss 0.0123
[2019-03-26 09:47:40,205] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 24046: learning rate 0.0005
[2019-03-26 09:47:40,236] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24061: loss 0.0004
[2019-03-26 09:47:40,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24061: learning rate 0.0005
[2019-03-26 09:47:40,238] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24061: loss 0.0021
[2019-03-26 09:47:40,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24062: learning rate 0.0005
[2019-03-26 09:47:40,286] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24082: loss 0.0026
[2019-03-26 09:47:40,289] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24083: learning rate 0.0005
[2019-03-26 09:47:40,408] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24142: loss 0.0550
[2019-03-26 09:47:40,409] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24142: learning rate 0.0005
[2019-03-26 09:47:42,254] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 09:47:42,258] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 09:47:42,258] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 09:47:42,259] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:47:42,260] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 09:47:42,261] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 09:47:42,260] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:47:42,264] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:47:42,259] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 09:47:42,265] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:47:42,274] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:47:42,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run2
[2019-03-26 09:47:42,293] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run2
[2019-03-26 09:47:42,309] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run2
[2019-03-26 09:47:42,311] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run2
[2019-03-26 09:47:42,325] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run2
[2019-03-26 09:47:45,119] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01659039], dtype=float32), 0.0012367091]
[2019-03-26 09:47:45,122] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.1, 96.0, 1.0, 2.0, 0.3750105758785816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 571163.1231693695, 571163.1231693688, 172279.1118634258]
[2019-03-26 09:47:45,124] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 09:47:45,127] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.3662275e-16 1.0000000e+00 2.1971898e-09 1.2624630e-19 6.5695732e-10], sampled 0.6081559207188102
[2019-03-26 09:48:13,030] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01659039], dtype=float32), 0.0012367091]
[2019-03-26 09:48:13,033] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.0, 91.0, 1.0, 2.0, 0.4840639968560894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 677106.1644338695, 677106.1644338701, 181227.7207482609]
[2019-03-26 09:48:13,034] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 09:48:13,036] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7429319e-15 1.0000000e+00 4.0187520e-09 4.0940095e-19 2.3534490e-09], sampled 0.6042641405207885
[2019-03-26 09:48:32,429] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01659039], dtype=float32), 0.0012367091]
[2019-03-26 09:48:32,429] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.64991820333333, 74.83517815333335, 1.0, 2.0, 0.5420982720306206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757519.0857774548, 757519.0857774554, 190455.5935176703]
[2019-03-26 09:48:32,431] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 09:48:32,434] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0757853e-16 1.0000000e+00 8.9943342e-10 2.5709126e-20 2.3488692e-10], sampled 0.6148458821371825
[2019-03-26 09:48:47,437] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01659039], dtype=float32), 0.0012367091]
[2019-03-26 09:48:47,437] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.64560339333333, 79.84497592333334, 1.0, 2.0, 0.4629188138339755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653379.9923185657, 653379.9923185657, 178834.7061344169]
[2019-03-26 09:48:47,441] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 09:48:47,443] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3546862e-15 1.0000000e+00 3.5337608e-09 3.6360127e-19 2.0187714e-09], sampled 0.6375628264428943
[2019-03-26 09:49:11,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01659039], dtype=float32), 0.0012367091]
[2019-03-26 09:49:11,526] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.45, 67.0, 1.0, 2.0, 0.5389756018257145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753153.971525092, 753153.9715250926, 189927.5160985499]
[2019-03-26 09:49:11,527] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 09:49:11,529] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8163396e-16 1.0000000e+00 2.1267033e-09 1.3993714e-19 6.6873374e-10], sampled 0.14309356023113418
[2019-03-26 09:49:13,087] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01659039], dtype=float32), 0.0012367091]
[2019-03-26 09:49:13,088] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.66916177666667, 66.95131177666667, 1.0, 2.0, 0.5166838956005045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721993.3906769418, 721993.3906769418, 186250.625671226]
[2019-03-26 09:49:13,090] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:49:13,092] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0243367e-15 1.0000000e+00 4.7342446e-09 5.8353430e-19 2.3729525e-09], sampled 0.23791965526447123
[2019-03-26 09:49:22,214] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01659039], dtype=float32), 0.0012367091]
[2019-03-26 09:49:22,219] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.63978813, 58.90129339000001, 1.0, 2.0, 0.3340334891951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 526039.1193374639, 526039.1193374639, 168984.5437486799]
[2019-03-26 09:49:22,221] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:49:22,225] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.0701751e-15 1.0000000e+00 5.5009091e-09 1.3913294e-18 2.4434030e-09], sampled 0.21099289947610067
[2019-03-26 09:49:35,402] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6567 3164087800.8962 1778.0000
[2019-03-26 09:49:35,735] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4308 2779149198.4617 933.0000
[2019-03-26 09:49:35,885] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0142 3007608811.4011 1766.0000
[2019-03-26 09:49:35,984] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5752 2842451949.6408 1131.0000
[2019-03-26 09:49:36,162] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-26 09:49:37,178] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 25000, evaluation results [25000.0, 7882.656730137187, 3164087800.8961525, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.430759135581, 2779149198.4616733, 933.0, 7999.014205657292, 3007608811.4011106, 1766.0, 8497.575228127671, 2842451949.640752, 1131.0]
[2019-03-26 09:49:37,325] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5094574e-16 1.0000000e+00 7.4676665e-10 8.0092887e-20 6.4623785e-11], sum to 1.0000
[2019-03-26 09:49:37,336] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7060
[2019-03-26 09:49:37,454] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 93.0, 1.0, 2.0, 0.3407761778038421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527468.8128107061, 527468.8128107061, 168874.2556418068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 945000.0000, 
sim time next is 945600.0000, 
raw observation next is [21.9, 93.33333333333334, 1.0, 2.0, 0.3397946603984714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526082.1326077123, 526082.132607713, 168767.1717017065], 
processed observation next is [0.0, 0.9565217391304348, 0.23696682464454974, 0.9333333333333335, 1.0, 1.0, 0.2045718799981583, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14613392572436454, 0.14613392572436473, 0.25189130104732316], 
reward next is 0.7481, 
noisyNet noise sample is [array([-0.7044588], dtype=float32), -0.86490846]. 
=============================================
[2019-03-26 09:49:44,702] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5622109e-14 1.0000000e+00 3.0573766e-08 1.4120709e-17 2.7018578e-09], sum to 1.0000
[2019-03-26 09:49:44,711] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4537
[2019-03-26 09:49:44,716] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.55, 95.0, 1.0, 2.0, 0.2985181058265153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475780.7580911931, 475780.7580911931, 165327.4756442262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1061400.0000, 
sim time next is 1062000.0000, 
raw observation next is [20.6, 95.0, 1.0, 2.0, 0.3003555778695772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478186.0030671549, 478186.0030671549, 165490.4903227347], 
processed observation next is [1.0, 0.30434782608695654, 0.17535545023696694, 0.95, 1.0, 1.0, 0.1570549130958761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13282944529643192, 0.13282944529643192, 0.24700073182497714], 
reward next is 0.7530, 
noisyNet noise sample is [array([-0.31695554], dtype=float32), 1.2351578]. 
=============================================
[2019-03-26 09:49:44,734] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[67.31887 ]
 [67.322014]
 [67.3682  ]
 [67.37261 ]
 [67.41016 ]], R is [[67.37281036]
 [67.45232391]
 [67.53127289]
 [67.60964203]
 [67.68729401]].
[2019-03-26 09:49:50,859] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5145549e-14 1.0000000e+00 1.9359350e-09 1.6761541e-16 6.2664200e-11], sum to 1.0000
[2019-03-26 09:49:50,871] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9493
[2019-03-26 09:49:50,986] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 60.66666666666667, 1.0, 2.0, 0.9738410104176051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1474473.84463667, 1474473.844636671, 307746.788348244], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1174200.0000, 
sim time next is 1174800.0000, 
raw observation next is [27.6, 60.33333333333334, 1.0, 2.0, 0.9746844197968386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1478238.822412414, 1478238.822412414, 308355.7544564822], 
processed observation next is [1.0, 0.6086956521739131, 0.5071090047393366, 0.6033333333333334, 1.0, 1.0, 0.9694993009600466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.41062189511455943, 0.41062189511455943, 0.46023246933803313], 
reward next is 0.5398, 
noisyNet noise sample is [array([0.02232723], dtype=float32), 1.0416349]. 
=============================================
[2019-03-26 09:49:52,274] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31851: loss 0.2602
[2019-03-26 09:49:52,276] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31852: learning rate 0.0005
[2019-03-26 09:49:52,287] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31858: loss 0.2730
[2019-03-26 09:49:52,288] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31858: learning rate 0.0005
[2019-03-26 09:49:52,333] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31880: loss 0.3321
[2019-03-26 09:49:52,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31880: learning rate 0.0005
[2019-03-26 09:49:52,379] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31900: loss 0.3128
[2019-03-26 09:49:52,381] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31900: learning rate 0.0005
[2019-03-26 09:49:52,427] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 31920: loss 0.2025
[2019-03-26 09:49:52,432] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 31920: learning rate 0.0005
[2019-03-26 09:49:52,439] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31925: loss 0.1617
[2019-03-26 09:49:52,443] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31926: learning rate 0.0005
[2019-03-26 09:49:52,519] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31972: loss 0.0489
[2019-03-26 09:49:52,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31972: learning rate 0.0005
[2019-03-26 09:49:52,544] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 31981: loss 0.0016
[2019-03-26 09:49:52,546] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 31982: learning rate 0.0005
[2019-03-26 09:49:52,561] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31987: loss 0.0062
[2019-03-26 09:49:52,563] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31989: learning rate 0.0005
[2019-03-26 09:49:52,652] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32030: loss 0.0598
[2019-03-26 09:49:52,653] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32031: loss 0.0476
[2019-03-26 09:49:52,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32032: learning rate 0.0005
[2019-03-26 09:49:52,656] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32032: learning rate 0.0005
[2019-03-26 09:49:52,683] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32044: loss 0.1259
[2019-03-26 09:49:52,686] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32046: learning rate 0.0005
[2019-03-26 09:49:52,687] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32046: loss 0.1273
[2019-03-26 09:49:52,689] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32046: learning rate 0.0005
[2019-03-26 09:49:52,754] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32076: loss 0.0777
[2019-03-26 09:49:52,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32076: learning rate 0.0005
[2019-03-26 09:49:52,893] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 32141: loss 0.0748
[2019-03-26 09:49:52,894] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 32141: learning rate 0.0005
[2019-03-26 09:49:52,937] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 32159: loss 0.0473
[2019-03-26 09:49:52,942] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 32161: learning rate 0.0005
[2019-03-26 09:49:53,084] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3918292e-17 1.0000000e+00 1.5478006e-10 5.0962601e-20 9.0522256e-11], sum to 1.0000
[2019-03-26 09:49:53,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1760
[2019-03-26 09:49:53,096] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 87.66666666666667, 1.0, 2.0, 0.3536174830784391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546127.3581518098, 546127.3581518098, 170362.154002153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1208400.0000, 
sim time next is 1209000.0000, 
raw observation next is [22.65, 87.83333333333334, 1.0, 2.0, 0.3526292549418842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 545029.5503396417, 545029.5503396412, 170282.855050377], 
processed observation next is [1.0, 1.0, 0.2725118483412322, 0.8783333333333334, 1.0, 1.0, 0.22003524691793275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15139709731656714, 0.151397097316567, 0.25415351500056266], 
reward next is 0.7458, 
noisyNet noise sample is [array([-1.8493415], dtype=float32), 0.408306]. 
=============================================
[2019-03-26 09:49:53,111] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.51785 ]
 [79.457054]
 [79.378654]
 [79.30152 ]
 [79.23685 ]], R is [[79.53604126]
 [79.48641205]
 [79.43777466]
 [79.38913727]
 [79.34078217]].
[2019-03-26 09:49:53,718] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.2540732e-14 1.0000000e+00 2.0385057e-08 6.1674964e-17 3.2265490e-08], sum to 1.0000
[2019-03-26 09:49:53,731] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7031
[2019-03-26 09:49:53,831] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.0, 1.0, 2.0, 0.3420071343676661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531989.4497294846, 531989.4497294846, 169310.1688530309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1220400.0000, 
sim time next is 1221000.0000, 
raw observation next is [21.88333333333333, 92.16666666666667, 1.0, 2.0, 0.3928120115120785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610927.8457835277, 610927.8457835283, 176069.1309924747], 
processed observation next is [1.0, 0.13043478260869565, 0.2361769352290678, 0.9216666666666667, 1.0, 1.0, 0.26844820664105845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16970217938431326, 0.16970217938431342, 0.2627897477499622], 
reward next is 0.7372, 
noisyNet noise sample is [array([1.4141488], dtype=float32), 0.73869216]. 
=============================================
[2019-03-26 09:49:53,851] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[65.920975]
 [65.917305]
 [65.95215 ]
 [65.96503 ]
 [65.98631 ]], R is [[66.1131134 ]
 [66.19927979]
 [66.28392029]
 [66.36707306]
 [66.45028687]].
[2019-03-26 09:49:54,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8158002e-17 1.0000000e+00 1.7048921e-09 2.7841506e-19 6.9356861e-11], sum to 1.0000
[2019-03-26 09:49:54,593] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1140
[2019-03-26 09:49:54,598] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 88.5, 1.0, 2.0, 0.3846810340594198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583261.3342351102, 583261.3342351095, 173278.2983729321], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1234200.0000, 
sim time next is 1234800.0000, 
raw observation next is [23.4, 88.0, 1.0, 2.0, 0.3865048400023153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584469.8819849034, 584469.8819849034, 173343.6761196216], 
processed observation next is [1.0, 0.30434782608695654, 0.30805687203791465, 0.88, 1.0, 1.0, 0.2608492048220666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16235274499580649, 0.16235274499580649, 0.25872190465615164], 
reward next is 0.7413, 
noisyNet noise sample is [array([-0.20202412], dtype=float32), -0.42240167]. 
=============================================
[2019-03-26 09:49:55,550] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0770065e-13 1.0000000e+00 6.4632077e-10 8.0414242e-15 1.6146874e-11], sum to 1.0000
[2019-03-26 09:49:55,558] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7314
[2019-03-26 09:49:55,569] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1925724.258143142 W.
[2019-03-26 09:49:55,682] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.23333333333333, 72.16666666666667, 1.0, 2.0, 0.7361554407593548, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.964881011916841, 6.9112, 168.9126373799938, 1925724.258143142, 1887641.17268272, 394008.9915185338], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1253400.0000, 
sim time next is 1254000.0000, 
raw observation next is [28.26666666666667, 72.33333333333334, 1.0, 2.0, 0.4671974193128942, 1.0, 1.0, 0.4671974193128942, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1306037.122486479, 1306037.122486479, 290371.0835819863], 
processed observation next is [1.0, 0.5217391304347826, 0.53870458135861, 0.7233333333333334, 1.0, 1.0, 0.3580691798950533, 1.0, 0.5, 0.3580691798950533, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.36278808957957753, 0.36278808957957753, 0.43338967698803926], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.24510364], dtype=float32), 1.6966403]. 
=============================================
[2019-03-26 09:49:55,693] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[46.612606]
 [46.81204 ]
 [47.55285 ]
 [47.71554 ]
 [47.917843]], R is [[45.88964844]
 [45.4307518 ]
 [45.46525574]
 [45.43930054]
 [44.98490906]].
[2019-03-26 09:49:56,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6983385e-14 1.0000000e+00 7.3328480e-13 3.0940073e-11 5.3326670e-14], sum to 1.0000
[2019-03-26 09:49:56,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7765
[2019-03-26 09:49:57,080] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4729806794215503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660905.2686308747, 660905.2686308747, 179470.7581163715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1278000.0000, 
sim time next is 1278600.0000, 
raw observation next is [25.85, 84.83333333333333, 1.0, 2.0, 0.4717735108318618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659952.5064163224, 659952.5064163231, 179386.5780813171], 
processed observation next is [1.0, 0.8260869565217391, 0.4241706161137442, 0.8483333333333333, 1.0, 1.0, 0.3635825431709178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18332014067120067, 0.18332014067120087, 0.26774116131539866], 
reward next is 0.7323, 
noisyNet noise sample is [array([-0.13522692], dtype=float32), 1.5683591]. 
=============================================
[2019-03-26 09:50:04,036] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8499584e-20 1.0000000e+00 5.0704166e-18 6.7248665e-21 3.7162994e-16], sum to 1.0000
[2019-03-26 09:50:04,047] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5492
[2019-03-26 09:50:04,054] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.51666666666667, 98.0, 1.0, 2.0, 0.3134668458917984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495934.7954368402, 495934.7954368402, 166727.1465853586], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1397400.0000, 
sim time next is 1398000.0000, 
raw observation next is [20.53333333333333, 98.0, 1.0, 2.0, 0.3142556772399356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496983.3143755, 496983.3143754994, 166801.1078839], 
processed observation next is [0.0, 0.17391304347826086, 0.17219589257503945, 0.98, 1.0, 1.0, 0.17380202077100673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1380509206598611, 0.13805092065986094, 0.24895687743865672], 
reward next is 0.7510, 
noisyNet noise sample is [array([0.8368717], dtype=float32), 0.31891534]. 
=============================================
[2019-03-26 09:50:04,067] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.06663 ]
 [71.231995]
 [71.45818 ]
 [71.6519  ]
 [71.73903 ]], R is [[70.99240875]
 [71.033638  ]
 [71.07445526]
 [71.11479187]
 [71.15483856]].
[2019-03-26 09:50:05,000] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3293275e-20 1.0000000e+00 3.9622911e-17 1.0404955e-20 3.0576209e-16], sum to 1.0000
[2019-03-26 09:50:05,007] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2280
[2019-03-26 09:50:05,011] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 88.0, 1.0, 2.0, 0.3433125892119612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532251.3707934082, 532251.3707934082, 169283.634421224], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1413000.0000, 
sim time next is 1413600.0000, 
raw observation next is [22.7, 87.33333333333333, 1.0, 2.0, 0.3444935536151093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 533157.7505699028, 533157.7505699035, 169330.8309994396], 
processed observation next is [0.0, 0.34782608695652173, 0.27488151658767773, 0.8733333333333333, 1.0, 1.0, 0.2102331971266377, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14809937515830635, 0.14809937515830654, 0.25273258358125317], 
reward next is 0.7473, 
noisyNet noise sample is [array([0.908001], dtype=float32), -1.9273541]. 
=============================================
[2019-03-26 09:50:08,828] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1644915e-19 1.0000000e+00 9.7107947e-17 1.2794853e-19 5.7621593e-15], sum to 1.0000
[2019-03-26 09:50:08,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4723
[2019-03-26 09:50:08,940] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 96.0, 1.0, 2.0, 0.3419572955289975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 530467.4887371445, 530467.4887371439, 169148.6773291997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1474200.0000, 
sim time next is 1474800.0000, 
raw observation next is [21.46666666666667, 96.0, 1.0, 2.0, 0.3413674513089504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530039.9030655492, 530039.9030655499, 169127.8221798184], 
processed observation next is [0.0, 0.043478260869565216, 0.21642969984202226, 0.96, 1.0, 1.0, 0.20646680880596435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.147233306407097, 0.1472333064070972, 0.2524295853430126], 
reward next is 0.7476, 
noisyNet noise sample is [array([-0.36220735], dtype=float32), 0.7236358]. 
=============================================
[2019-03-26 09:50:09,869] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39834: loss 0.0547
[2019-03-26 09:50:09,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39835: learning rate 0.0005
[2019-03-26 09:50:09,925] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39862: loss 0.1074
[2019-03-26 09:50:09,927] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39862: learning rate 0.0005
[2019-03-26 09:50:09,994] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39897: loss 0.0653
[2019-03-26 09:50:09,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39898: learning rate 0.0005
[2019-03-26 09:50:10,061] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39922: loss 0.0255
[2019-03-26 09:50:10,063] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39922: learning rate 0.0005
[2019-03-26 09:50:10,078] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 39930: loss 0.0189
[2019-03-26 09:50:10,082] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 39932: learning rate 0.0005
[2019-03-26 09:50:10,157] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 39971: loss 0.0013
[2019-03-26 09:50:10,160] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 39972: learning rate 0.0005
[2019-03-26 09:50:10,162] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39973: loss 0.0010
[2019-03-26 09:50:10,164] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39974: loss 0.0022
[2019-03-26 09:50:10,167] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39975: learning rate 0.0005
[2019-03-26 09:50:10,172] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39979: loss 0.0101
[2019-03-26 09:50:10,172] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39978: learning rate 0.0005
[2019-03-26 09:50:10,179] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39980: learning rate 0.0005
[2019-03-26 09:50:10,201] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39988: loss 0.0084
[2019-03-26 09:50:10,201] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39988: loss 0.0071
[2019-03-26 09:50:10,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39990: learning rate 0.0005
[2019-03-26 09:50:10,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39990: learning rate 0.0005
[2019-03-26 09:50:10,300] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40034: loss 0.0217
[2019-03-26 09:50:10,303] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40035: learning rate 0.0005
[2019-03-26 09:50:10,344] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40053: loss 0.0141
[2019-03-26 09:50:10,347] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40054: learning rate 0.0005
[2019-03-26 09:50:10,422] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40086: loss 0.0021
[2019-03-26 09:50:10,426] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40088: learning rate 0.0005
[2019-03-26 09:50:10,460] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 40107: loss 0.0030
[2019-03-26 09:50:10,461] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 40107: learning rate 0.0005
[2019-03-26 09:50:10,669] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 40201: loss 0.0165
[2019-03-26 09:50:10,673] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 40203: learning rate 0.0005
[2019-03-26 09:50:11,646] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4163902e-22 1.0000000e+00 3.4021045e-18 1.1858586e-23 3.4927235e-17], sum to 1.0000
[2019-03-26 09:50:11,657] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6996
[2019-03-26 09:50:11,664] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.85, 57.33333333333333, 1.0, 2.0, 0.3572748588014932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546906.9123869502, 546906.9123869502, 170285.8858871213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1527000.0000, 
sim time next is 1527600.0000, 
raw observation next is [27.7, 57.66666666666667, 1.0, 2.0, 0.3549844035424509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544589.4044788288, 544589.4044788294, 170129.0385365109], 
processed observation next is [0.0, 0.6956521739130435, 0.5118483412322274, 0.5766666666666667, 1.0, 1.0, 0.2228727753523505, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15127483457745244, 0.1512748345774526, 0.2539239381141954], 
reward next is 0.7461, 
noisyNet noise sample is [array([0.35623503], dtype=float32), 1.210182]. 
=============================================
[2019-03-26 09:50:21,163] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9519460e-18 1.0000000e+00 1.0359396e-15 3.6496906e-17 1.2618823e-16], sum to 1.0000
[2019-03-26 09:50:21,168] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4911
[2019-03-26 09:50:21,172] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.08333333333334, 84.33333333333333, 1.0, 2.0, 0.9243969494924512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1292061.873386663, 1292061.873386664, 276735.2945806855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1687800.0000, 
sim time next is 1688400.0000, 
raw observation next is [27.2, 84.0, 1.0, 2.0, 0.9031370624274396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1262328.521679329, 1262328.52167933, 270800.7116514287], 
processed observation next is [1.0, 0.5652173913043478, 0.4881516587677725, 0.84, 1.0, 1.0, 0.8832976655752285, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35064681157759137, 0.35064681157759164, 0.4041801666439235], 
reward next is 0.5958, 
noisyNet noise sample is [array([0.11418851], dtype=float32), -0.999106]. 
=============================================
[2019-03-26 09:50:27,259] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47853: loss 0.0269
[2019-03-26 09:50:27,263] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47854: learning rate 0.0005
[2019-03-26 09:50:27,332] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47889: loss 0.1457
[2019-03-26 09:50:27,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47889: learning rate 0.0005
[2019-03-26 09:50:27,340] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 47891: loss 0.0535
[2019-03-26 09:50:27,341] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 47891: learning rate 0.0005
[2019-03-26 09:50:27,394] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47914: loss 0.0863
[2019-03-26 09:50:27,397] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47914: learning rate 0.0005
[2019-03-26 09:50:27,462] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47949: loss 0.0156
[2019-03-26 09:50:27,463] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47949: loss 0.0814
[2019-03-26 09:50:27,464] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47949: learning rate 0.0005
[2019-03-26 09:50:27,467] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47949: learning rate 0.0005
[2019-03-26 09:50:27,469] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47950: loss 0.0421
[2019-03-26 09:50:27,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47950: learning rate 0.0005
[2019-03-26 09:50:27,506] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47970: loss 0.0145
[2019-03-26 09:50:27,509] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 47970: loss 0.0141
[2019-03-26 09:50:27,510] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47970: learning rate 0.0005
[2019-03-26 09:50:27,514] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 47971: learning rate 0.0005
[2019-03-26 09:50:27,536] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47985: loss 0.0143
[2019-03-26 09:50:27,538] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47986: learning rate 0.0005
[2019-03-26 09:50:27,543] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1664699e-17 1.0000000e+00 2.6479954e-18 3.2729720e-17 1.2908913e-15], sum to 1.0000
[2019-03-26 09:50:27,546] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4207
[2019-03-26 09:50:27,662] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.38333333333333, 92.66666666666666, 1.0, 2.0, 0.32651692131266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513805.7873566918, 513805.7873566911, 168023.048003581], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1803000.0000, 
sim time next is 1803600.0000, 
raw observation next is [21.4, 93.0, 1.0, 2.0, 0.3272048030727237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514163.28648065, 514163.28648065, 168034.7519484025], 
processed observation next is [1.0, 0.9130434782608695, 0.21327014218009477, 0.93, 1.0, 1.0, 0.18940337719605266, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14282313513351388, 0.14282313513351388, 0.25079813723642164], 
reward next is 0.7492, 
noisyNet noise sample is [array([1.2240995], dtype=float32), 1.5870286]. 
=============================================
[2019-03-26 09:50:27,671] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48004: loss 0.0022
[2019-03-26 09:50:27,672] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48004: learning rate 0.0005
[2019-03-26 09:50:27,710] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48024: loss 0.0139
[2019-03-26 09:50:27,712] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48024: learning rate 0.0005
[2019-03-26 09:50:27,786] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48066: loss 0.0492
[2019-03-26 09:50:27,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48066: learning rate 0.0005
[2019-03-26 09:50:27,874] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48105: loss 0.0086
[2019-03-26 09:50:27,878] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48105: learning rate 0.0005
[2019-03-26 09:50:27,956] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 48140: loss 0.0011
[2019-03-26 09:50:27,960] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 48140: learning rate 0.0005
[2019-03-26 09:50:27,980] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48152: loss 0.0135
[2019-03-26 09:50:27,983] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48153: learning rate 0.0005
[2019-03-26 09:50:29,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5689385e-19 1.0000000e+00 6.2038727e-20 3.0812534e-21 8.6016791e-17], sum to 1.0000
[2019-03-26 09:50:29,664] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6916
[2019-03-26 09:50:29,670] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.73333333333333, 94.0, 1.0, 2.0, 0.3728431648453216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561758.9606749591, 561758.9606749597, 171270.8390035334], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1837200.0000, 
sim time next is 1837800.0000, 
raw observation next is [22.85, 93.5, 1.0, 2.0, 0.3781574601829451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 568738.5512930684, 568738.5512930684, 171848.433533942], 
processed observation next is [1.0, 0.2608695652173913, 0.28199052132701435, 0.935, 1.0, 1.0, 0.2507921207023435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15798293091474125, 0.15798293091474125, 0.25649019930439104], 
reward next is 0.7435, 
noisyNet noise sample is [array([0.7135921], dtype=float32), 0.29301465]. 
=============================================
[2019-03-26 09:50:31,645] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7503148e-18 1.0000000e+00 2.1508607e-17 7.7046150e-17 1.9910678e-15], sum to 1.0000
[2019-03-26 09:50:31,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1367
[2019-03-26 09:50:31,665] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1675349.489202601 W.
[2019-03-26 09:50:31,668] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.93333333333333, 87.33333333333334, 1.0, 2.0, 0.5992051842296731, 1.0, 2.0, 0.5992051842296731, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1675349.489202601, 1675349.489202602, 333955.7454893625], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1874400.0000, 
sim time next is 1875000.0000, 
raw observation next is [26.91666666666667, 87.66666666666667, 1.0, 2.0, 0.5961687952667315, 1.0, 2.0, 0.5961687952667315, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1666853.286155391, 1666853.28615539, 332838.5866910427], 
processed observation next is [1.0, 0.6956521739130435, 0.4747235387045816, 0.8766666666666667, 1.0, 1.0, 0.5134563798394355, 1.0, 1.0, 0.5134563798394355, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.46301480170983084, 0.46301480170983056, 0.4967740099866309], 
reward next is 0.5032, 
noisyNet noise sample is [array([0.743608], dtype=float32), 1.6458924]. 
=============================================
[2019-03-26 09:50:31,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[58.275524]
 [57.81142 ]
 [59.84695 ]
 [59.136383]
 [58.855824]], R is [[58.48903656]
 [57.9041481 ]
 [57.32510757]
 [56.75185776]
 [56.70139694]].
[2019-03-26 09:50:31,936] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 09:50:31,936] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 09:50:31,937] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 09:50:31,938] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:50:31,938] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 09:50:31,942] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 09:50:31,944] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 09:50:31,944] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:50:31,938] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:50:31,945] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:50:31,945] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:50:31,961] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run3
[2019-03-26 09:50:31,978] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run3
[2019-03-26 09:50:31,981] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run3
[2019-03-26 09:50:31,981] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run3
[2019-03-26 09:50:32,017] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run3
[2019-03-26 09:51:03,658] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.07542464], dtype=float32), -0.0041387607]
[2019-03-26 09:51:03,659] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.84677927666667, 83.18511197500001, 1.0, 2.0, 0.5250516802905327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733690.2361253521, 733690.2361253527, 187615.0706892335]
[2019-03-26 09:51:03,660] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:51:03,664] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5434740e-16 1.0000000e+00 1.0529565e-14 7.5779604e-18 8.9942478e-14], sampled 0.06774433760456477
[2019-03-26 09:51:13,318] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.07542464], dtype=float32), -0.0041387607]
[2019-03-26 09:51:13,320] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.1, 80.66666666666666, 1.0, 2.0, 0.5374608725201004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751036.5701119794, 751036.5701119794, 189673.1164859913]
[2019-03-26 09:51:13,320] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 09:51:13,322] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7469782e-16 1.0000000e+00 1.3173648e-14 8.9224143e-18 8.1547995e-14], sampled 0.03615345536458314
[2019-03-26 09:51:15,288] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.07542464], dtype=float32), -0.0041387607]
[2019-03-26 09:51:15,289] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.75, 74.5, 1.0, 2.0, 0.7961738331061549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1112746.035528015, 1112746.035528015, 242975.1552544503]
[2019-03-26 09:51:15,290] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 09:51:15,294] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.5266393e-16 1.0000000e+00 4.4493936e-14 8.4807361e-17 3.2171878e-13], sampled 0.17411890705142852
[2019-03-26 09:51:23,284] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.07542464], dtype=float32), -0.0041387607]
[2019-03-26 09:51:23,286] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.394496165, 59.35329029166667, 1.0, 2.0, 0.5867606581979287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834984.4614097686, 834984.4614097686, 200238.6206349694]
[2019-03-26 09:51:23,287] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 09:51:23,290] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.1935753e-16 1.0000000e+00 3.6481997e-14 4.3656257e-17 2.4583525e-13], sampled 0.8343369696889776
[2019-03-26 09:51:31,650] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.07542464], dtype=float32), -0.0041387607]
[2019-03-26 09:51:31,652] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.27279533, 96.88543695166666, 1.0, 2.0, 0.7090740134589443, 1.0, 2.0, 0.6751270462437348, 1.0, 1.0, 1.03, 6.997951623683272, 6.9112, 184.5923449428631, 2832602.672592512, 2765345.192890669, 528430.6636698932]
[2019-03-26 09:51:31,653] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 09:51:31,659] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.0494866e-15 1.0000000e+00 2.0872602e-13 1.9025736e-15 9.4910213e-13], sampled 0.18355679597342123
[2019-03-26 09:51:31,662] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2832602.672592512 W.
[2019-03-26 09:52:04,805] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.07542464], dtype=float32), -0.0041387607]
[2019-03-26 09:52:04,806] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.00155493666666, 61.13818327666667, 1.0, 2.0, 0.5375440739297068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751152.875128115, 751152.8751281144, 189688.0463006024]
[2019-03-26 09:52:04,807] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:52:04,810] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.1427897e-16 1.0000000e+00 2.9478332e-14 2.4937767e-17 2.2178842e-13], sampled 0.19375762578716138
[2019-03-26 09:52:26,862] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-26 09:52:27,232] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5719 3007655079.7799 1766.0000
[2019-03-26 09:52:27,329] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9454 2927400465.3596 1338.0000
[2019-03-26 09:52:27,380] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0954 2842522369.8966 1131.0000
[2019-03-26 09:52:27,405] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4308 2779149198.4617 933.0000
[2019-03-26 09:52:28,420] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 50000, evaluation results [50000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8252.945441148015, 2927400465.359626, 1338.0, 8661.430759135581, 2779149198.4616733, 933.0, 7997.5719302320385, 3007655079.7799497, 1766.0, 8496.095385747887, 2842522369.896584, 1131.0]
[2019-03-26 09:52:29,167] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6487822e-19 1.0000000e+00 8.7236767e-18 5.4522168e-22 2.0220071e-14], sum to 1.0000
[2019-03-26 09:52:29,176] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7086
[2019-03-26 09:52:29,182] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 87.83333333333334, 1.0, 2.0, 0.4656988414344334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659394.8663762542, 659394.8663762542, 179512.6298097052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1889400.0000, 
sim time next is 1890000.0000, 
raw observation next is [25.1, 88.0, 1.0, 2.0, 0.4644061241375815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658324.6432128618, 658324.6432128613, 179418.0188848631], 
processed observation next is [1.0, 0.9130434782608695, 0.38862559241706174, 0.88, 1.0, 1.0, 0.3547061736597368, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18286795644801718, 0.182867956448017, 0.26778808788785535], 
reward next is 0.7322, 
noisyNet noise sample is [array([-0.16696936], dtype=float32), -1.0364656]. 
=============================================
[2019-03-26 09:52:29,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[61.41039 ]
 [61.960365]
 [62.240314]
 [63.265915]
 [63.26763 ]], R is [[61.01203537]
 [61.13398743]
 [61.25466919]
 [61.37423706]
 [61.49286652]].
[2019-03-26 09:52:31,525] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0549550e-21 1.0000000e+00 1.8624348e-20 7.0148285e-27 6.3416162e-15], sum to 1.0000
[2019-03-26 09:52:31,535] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0880
[2019-03-26 09:52:31,539] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.91666666666667, 86.33333333333333, 1.0, 2.0, 0.482797476369263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696694.9731691432, 696694.9731691432, 183750.4049189779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1926600.0000, 
sim time next is 1927200.0000, 
raw observation next is [25.03333333333333, 85.66666666666667, 1.0, 2.0, 0.4394978405363881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 633579.5679291823, 633579.5679291817, 177160.6749761763], 
processed observation next is [1.0, 0.30434782608695654, 0.38546603475513425, 0.8566666666666667, 1.0, 1.0, 0.324696193417335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17599432442477286, 0.1759943244247727, 0.26441891787488997], 
reward next is 0.7356, 
noisyNet noise sample is [array([0.70992297], dtype=float32), 0.121567436]. 
=============================================
[2019-03-26 09:52:39,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1502107e-21 1.0000000e+00 6.2978830e-23 4.9667675e-20 5.3152375e-23], sum to 1.0000
[2019-03-26 09:52:39,567] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1764
[2019-03-26 09:52:39,674] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 92.66666666666667, 1.0, 2.0, 0.4732750241149288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661416.11784795, 661416.11784795, 179527.3883171952], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2067600.0000, 
sim time next is 2068200.0000, 
raw observation next is [24.75, 93.0, 1.0, 2.0, 0.472992022023516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661204.8724338688, 661204.8724338694, 179509.1613234807], 
processed observation next is [0.0, 0.9565217391304348, 0.3720379146919432, 0.93, 1.0, 1.0, 0.36505062894399515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1836680201205191, 0.18366802012051928, 0.2679241213783294], 
reward next is 0.7321, 
noisyNet noise sample is [array([-0.06814809], dtype=float32), 0.43903822]. 
=============================================
[2019-03-26 09:52:41,218] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55860: loss 0.0074
[2019-03-26 09:52:41,221] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55861: learning rate 0.0005
[2019-03-26 09:52:41,253] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55875: loss 0.0005
[2019-03-26 09:52:41,256] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55876: loss 0.0006
[2019-03-26 09:52:41,258] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55876: learning rate 0.0005
[2019-03-26 09:52:41,260] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55876: learning rate 0.0005
[2019-03-26 09:52:41,350] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 55919: loss 0.0001
[2019-03-26 09:52:41,351] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 55920: learning rate 0.0005
[2019-03-26 09:52:41,385] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55935: loss 0.0010
[2019-03-26 09:52:41,391] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55937: learning rate 0.0005
[2019-03-26 09:52:41,406] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55945: loss 0.0025
[2019-03-26 09:52:41,412] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55946: loss 0.0012
[2019-03-26 09:52:41,414] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55948: loss 0.0079
[2019-03-26 09:52:41,415] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55946: learning rate 0.0005
[2019-03-26 09:52:41,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55950: learning rate 0.0005
[2019-03-26 09:52:41,419] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55949: learning rate 0.0005
[2019-03-26 09:52:41,458] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 55969: loss 0.0000
[2019-03-26 09:52:41,460] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 55969: learning rate 0.0005
[2019-03-26 09:52:41,483] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55978: loss 0.0056
[2019-03-26 09:52:41,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55980: learning rate 0.0005
[2019-03-26 09:52:41,504] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55990: loss 0.0011
[2019-03-26 09:52:41,511] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55991: learning rate 0.0005
[2019-03-26 09:52:41,525] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55994: loss 0.0009
[2019-03-26 09:52:41,532] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55996: learning rate 0.0005
[2019-03-26 09:52:41,805] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56127: loss 0.0004
[2019-03-26 09:52:41,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56128: learning rate 0.0005
[2019-03-26 09:52:41,815] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56133: loss 0.0006
[2019-03-26 09:52:41,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56133: learning rate 0.0005
[2019-03-26 09:52:41,861] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56149: loss 0.0040
[2019-03-26 09:52:41,862] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56150: learning rate 0.0005
[2019-03-26 09:52:41,890] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 56161: loss 0.0130
[2019-03-26 09:52:41,892] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 56162: learning rate 0.0005
[2019-03-26 09:52:58,683] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 63857: loss 4.1079
[2019-03-26 09:52:58,683] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63857: loss 4.3709
[2019-03-26 09:52:58,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 63857: learning rate 0.0005
[2019-03-26 09:52:58,685] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63857: learning rate 0.0005
[2019-03-26 09:52:58,699] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 63863: loss 1.4697
[2019-03-26 09:52:58,701] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 63863: learning rate 0.0005
[2019-03-26 09:52:58,735] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 63877: loss 0.8043
[2019-03-26 09:52:58,738] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 63877: learning rate 0.0005
[2019-03-26 09:52:58,823] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63920: loss 2.5326
[2019-03-26 09:52:58,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63920: learning rate 0.0005
[2019-03-26 09:52:58,854] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63933: loss 3.8702
[2019-03-26 09:52:58,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63934: learning rate 0.0005
[2019-03-26 09:52:58,918] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 63961: loss 3.0090
[2019-03-26 09:52:58,920] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 63961: learning rate 0.0005
[2019-03-26 09:52:58,925] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63964: loss 1.2087
[2019-03-26 09:52:58,930] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63965: learning rate 0.0005
[2019-03-26 09:52:58,968] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63986: loss 0.2570
[2019-03-26 09:52:58,972] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63986: learning rate 0.0005
[2019-03-26 09:52:58,976] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63989: loss 1.7439
[2019-03-26 09:52:58,983] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63993: learning rate 0.0005
[2019-03-26 09:52:59,004] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 64002: loss 0.6114
[2019-03-26 09:52:59,011] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 64002: learning rate 0.0005
[2019-03-26 09:52:59,048] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64021: loss 0.8046
[2019-03-26 09:52:59,052] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64022: learning rate 0.0005
[2019-03-26 09:52:59,239] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 64108: loss 0.2073
[2019-03-26 09:52:59,241] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64108: learning rate 0.0005
[2019-03-26 09:52:59,300] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64138: loss 1.3284
[2019-03-26 09:52:59,301] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64139: learning rate 0.0005
[2019-03-26 09:52:59,352] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64163: loss 0.0719
[2019-03-26 09:52:59,354] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64164: learning rate 0.0005
[2019-03-26 09:52:59,384] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 64182: loss 0.2213
[2019-03-26 09:52:59,387] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 64182: learning rate 0.0005
[2019-03-26 09:52:59,934] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3208555e-20 1.0000000e+00 1.3334924e-27 1.2766956e-13 2.0380944e-26], sum to 1.0000
[2019-03-26 09:52:59,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8315
[2019-03-26 09:52:59,950] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.2, 80.0, 1.0, 2.0, 0.5678562392396902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793526.2844790141, 793526.2844790141, 194910.2265544807], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2417400.0000, 
sim time next is 2418000.0000, 
raw observation next is [29.16666666666666, 80.0, 1.0, 2.0, 0.5650714471557491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789633.350035203, 789633.350035203, 194419.1723215128], 
processed observation next is [1.0, 1.0, 0.5813586097946285, 0.8, 1.0, 1.0, 0.4759896953683724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21934259723200084, 0.21934259723200084, 0.2901778691365863], 
reward next is 0.7098, 
noisyNet noise sample is [array([-0.2533457], dtype=float32), -0.77945065]. 
=============================================
[2019-03-26 09:52:59,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.133415]
 [72.06647 ]
 [72.004295]
 [71.94983 ]
 [71.86781 ]], R is [[72.18357849]
 [72.1708374 ]
 [72.15751648]
 [72.14395142]
 [72.13034058]].
[2019-03-26 09:53:02,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.3735789e-17 1.0000000e+00 4.3237684e-21 1.2399226e-09 7.3907605e-19], sum to 1.0000
[2019-03-26 09:53:02,202] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4236
[2019-03-26 09:53:02,211] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1908159.216489207 W.
[2019-03-26 09:53:02,314] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.4, 88.0, 1.0, 2.0, 0.7236036836708348, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.976689239286002, 6.9112, 168.9125116643499, 1908159.216489207, 1861699.018531104, 390869.0858301867], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2455200.0000, 
sim time next is 2455800.0000, 
raw observation next is [26.28333333333333, 88.16666666666667, 1.0, 2.0, 0.7109758624953858, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.972512683923298, 6.9112, 168.912537932965, 1890488.021918809, 1846990.801513151, 388184.2907886241], 
processed observation next is [1.0, 0.43478260869565216, 0.4447077409162717, 0.8816666666666667, 1.0, 1.0, 0.6517781475848021, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006131268392329759, 0.0, 0.829437889745319, 0.5251355616441136, 0.5130530004203198, 0.5793795384904837], 
reward next is 0.1141, 
noisyNet noise sample is [array([1.5265888], dtype=float32), -0.1933675]. 
=============================================
[2019-03-26 09:53:06,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2890839e-15 1.0000000e+00 1.2548575e-19 1.8582874e-08 2.9791837e-20], sum to 1.0000
[2019-03-26 09:53:06,427] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6004
[2019-03-26 09:53:06,430] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 96.5, 1.0, 2.0, 0.6676652381900295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 933061.3424901541, 933061.3424901547, 214021.5692119592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2525400.0000, 
sim time next is 2526000.0000, 
raw observation next is [26.26666666666667, 96.33333333333334, 1.0, 2.0, 0.7307054591673569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1021202.330666356, 1021202.330666356, 227613.4451615379], 
processed observation next is [1.0, 0.21739130434782608, 0.44391785150079005, 0.9633333333333334, 1.0, 1.0, 0.6755487459847673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28366731407398776, 0.28366731407398776, 0.3397215599425939], 
reward next is 0.6603, 
noisyNet noise sample is [array([-0.04260768], dtype=float32), -0.8811445]. 
=============================================
[2019-03-26 09:53:06,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[57.027348]
 [57.001003]
 [56.858208]
 [57.06228 ]
 [57.11725 ]], R is [[57.09171677]
 [57.20136642]
 [57.31803513]
 [57.39458847]
 [57.50965118]].
[2019-03-26 09:53:15,925] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71795: loss 0.2772
[2019-03-26 09:53:15,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71795: learning rate 0.0005
[2019-03-26 09:53:15,936] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71799: loss 0.2735
[2019-03-26 09:53:15,939] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71799: learning rate 0.0005
[2019-03-26 09:53:16,379] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71902: loss 0.3883
[2019-03-26 09:53:16,381] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71902: learning rate 0.0005
[2019-03-26 09:53:16,434] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 71927: loss 0.3925
[2019-03-26 09:53:16,438] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 71928: learning rate 0.0005
[2019-03-26 09:53:16,456] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71934: loss 0.4306
[2019-03-26 09:53:16,459] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71934: learning rate 0.0005
[2019-03-26 09:53:16,472] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 71944: loss 0.4581
[2019-03-26 09:53:16,474] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 71944: learning rate 0.0005
[2019-03-26 09:53:16,531] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71974: loss 0.4096
[2019-03-26 09:53:16,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71974: learning rate 0.0005
[2019-03-26 09:53:16,544] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71977: loss 0.3065
[2019-03-26 09:53:16,545] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71978: learning rate 0.0005
[2019-03-26 09:53:16,548] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71978: loss 0.3003
[2019-03-26 09:53:16,550] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71979: learning rate 0.0005
[2019-03-26 09:53:16,563] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71984: loss 0.3397
[2019-03-26 09:53:16,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71985: learning rate 0.0005
[2019-03-26 09:53:16,584] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71992: loss 0.2515
[2019-03-26 09:53:16,586] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71994: learning rate 0.0005
[2019-03-26 09:53:16,591] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71995: loss 0.2382
[2019-03-26 09:53:16,592] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71995: learning rate 0.0005
[2019-03-26 09:53:16,713] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 72053: loss 0.1995
[2019-03-26 09:53:16,718] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 72055: learning rate 0.0005
[2019-03-26 09:53:16,853] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72115: loss 0.0940
[2019-03-26 09:53:16,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72117: learning rate 0.0005
[2019-03-26 09:53:16,998] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72182: loss 0.0007
[2019-03-26 09:53:17,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72184: learning rate 0.0005
[2019-03-26 09:53:17,074] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 72220: loss 0.0065
[2019-03-26 09:53:17,077] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 72221: learning rate 0.0005
[2019-03-26 09:53:22,999] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 09:53:23,003] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 09:53:23,003] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 09:53:23,005] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:53:23,006] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:53:23,006] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 09:53:23,007] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 09:53:23,008] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:53:23,005] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 09:53:23,009] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:53:23,014] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:53:23,025] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run4
[2019-03-26 09:53:23,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run4
[2019-03-26 09:53:23,058] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run4
[2019-03-26 09:53:23,058] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run4
[2019-03-26 09:53:23,059] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run4
[2019-03-26 09:53:38,470] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15407552], dtype=float32), 0.06529791]
[2019-03-26 09:53:38,471] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.06343449666667, 95.15164337333334, 1.0, 2.0, 0.3368607009925742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529590.9960348402, 529590.9960348397, 169247.7018880713]
[2019-03-26 09:53:38,473] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 09:53:38,476] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.01085154e-20 1.00000000e+00 4.35250912e-23 1.79309001e-19
 1.29929951e-23], sampled 0.8897405542909177
[2019-03-26 09:53:47,750] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15407552], dtype=float32), 0.06529791]
[2019-03-26 09:53:47,753] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.46666666666667, 80.66666666666667, 1.0, 2.0, 0.4940501355323895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 690513.0432099277, 690513.0432099272, 182688.0869238264]
[2019-03-26 09:53:47,755] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 09:53:47,758] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2507370e-20 1.0000000e+00 2.8156864e-22 4.7146057e-18 7.1112447e-23], sampled 0.6169236433554232
[2019-03-26 09:53:51,956] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15407552], dtype=float32), 0.06529791]
[2019-03-26 09:53:51,956] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.33333333333333, 95.0, 1.0, 2.0, 0.3853029166809285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585190.4892718601, 585190.4892718601, 173477.9032924265]
[2019-03-26 09:53:51,957] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 09:53:51,959] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3301034e-20 1.0000000e+00 6.2573880e-23 2.3576736e-19 2.2098542e-23], sampled 0.7220263930705699
[2019-03-26 09:53:55,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15407552], dtype=float32), 0.06529791]
[2019-03-26 09:53:55,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.85046001666667, 79.49352414166667, 1.0, 2.0, 1.012260628950093, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564472268, 1414953.84691336, 1414953.846913361, 302692.8476580047]
[2019-03-26 09:53:55,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 09:53:55,766] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.227806e-18 1.000000e+00 2.396226e-20 1.111622e-14 7.532097e-21], sampled 0.34228506499553246
[2019-03-26 09:54:32,799] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15407552], dtype=float32), 0.06529791]
[2019-03-26 09:54:32,801] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.6311768945550326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 882047.7527259204, 882047.7527259204, 206687.8484651671]
[2019-03-26 09:54:32,803] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 09:54:32,807] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.06772545e-19 1.00000000e+00 7.51088932e-22 4.22065144e-18
 1.58654632e-22], sampled 0.8723573581832392
[2019-03-26 09:54:37,036] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15407552], dtype=float32), 0.06529791]
[2019-03-26 09:54:37,037] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.63333333333333, 80.66666666666667, 1.0, 2.0, 0.5993549282319962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 837560.1276246601, 837560.1276246608, 200624.8607295367]
[2019-03-26 09:54:37,038] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 09:54:37,040] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.3708178e-20 1.0000000e+00 3.5591713e-22 3.3298683e-18 1.5380209e-22], sampled 0.9221411583979613
[2019-03-26 09:55:17,929] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-26 09:55:18,045] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.15407552], dtype=float32), 0.06529791]
[2019-03-26 09:55:18,046] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.51405464666667, 87.63119189166666, 1.0, 2.0, 0.7055549482763483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 986036.7468479008, 986036.7468479015, 222052.5312622813]
[2019-03-26 09:55:18,047] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:55:18,053] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.8699231e-20 1.0000000e+00 4.0076416e-22 2.3245430e-18 8.3519052e-23], sampled 0.6297698605549317
[2019-03-26 09:55:18,064] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-26 09:55:18,514] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-26 09:55:18,543] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 09:55:18,608] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-26 09:55:19,624] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 75000, evaluation results [75000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 09:55:22,909] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.5775490e-23 1.0000000e+00 2.4801812e-25 1.6048640e-20 5.5707774e-27], sum to 1.0000
[2019-03-26 09:55:22,915] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7620
[2019-03-26 09:55:22,925] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3481086805393849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 536252.434237533, 536252.4342375337, 169509.2681583941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2857200.0000, 
sim time next is 2857800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3479205549480015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535962.7626764473, 535962.7626764467, 169485.5977016161], 
processed observation next is [1.0, 0.043478260869565216, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21436211439518252, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14887854518790203, 0.14887854518790186, 0.2529635786591285], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.7674347], dtype=float32), -1.6193175]. 
=============================================
[2019-03-26 09:55:30,262] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79817: loss 0.3863
[2019-03-26 09:55:30,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79818: learning rate 0.0005
[2019-03-26 09:55:30,440] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79869: loss 0.6450
[2019-03-26 09:55:30,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79869: learning rate 0.0005
[2019-03-26 09:55:30,624] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 79928: loss 0.5108
[2019-03-26 09:55:30,629] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 79928: learning rate 0.0005
[2019-03-26 09:55:30,716] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79940: loss 0.5243
[2019-03-26 09:55:30,720] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79940: learning rate 0.0005
[2019-03-26 09:55:30,723] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79940: loss 0.6199
[2019-03-26 09:55:30,723] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 79940: loss 0.4830
[2019-03-26 09:55:30,801] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 79942: learning rate 0.0005
[2019-03-26 09:55:30,804] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79941: learning rate 0.0005
[2019-03-26 09:55:30,958] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79952: loss 0.3850
[2019-03-26 09:55:30,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79952: learning rate 0.0005
[2019-03-26 09:55:30,963] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79952: loss 0.3710
[2019-03-26 09:55:31,039] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79952: learning rate 0.0005
[2019-03-26 09:55:31,122] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79965: loss 0.3492
[2019-03-26 09:55:31,124] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79965: learning rate 0.0005
[2019-03-26 09:55:31,127] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79966: loss 0.3172
[2019-03-26 09:55:31,128] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79966: loss 0.2662
[2019-03-26 09:55:31,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79966: learning rate 0.0005
[2019-03-26 09:55:31,209] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79966: learning rate 0.0005
[2019-03-26 09:55:31,389] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79991: loss 0.1906
[2019-03-26 09:55:31,391] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79992: learning rate 0.0005
[2019-03-26 09:55:31,501] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80014: loss 0.1196
[2019-03-26 09:55:31,502] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80014: learning rate 0.0005
[2019-03-26 09:55:31,601] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80031: loss 0.1017
[2019-03-26 09:55:31,605] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80032: learning rate 0.0005
[2019-03-26 09:55:32,105] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80235: loss 0.2782
[2019-03-26 09:55:32,107] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80235: learning rate 0.0005
[2019-03-26 09:55:32,199] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 80243: loss 0.2981
[2019-03-26 09:55:32,201] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 80243: learning rate 0.0005
[2019-03-26 09:55:36,623] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9107953e-23 1.0000000e+00 1.9546403e-26 3.9644469e-20 5.8777730e-25], sum to 1.0000
[2019-03-26 09:55:36,632] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8190
[2019-03-26 09:55:36,637] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 98.0, 1.0, 2.0, 0.4229290631167505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618778.816211871, 618778.816211871, 175963.6500937533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3093600.0000, 
sim time next is 3094200.0000, 
raw observation next is [23.0, 97.0, 1.0, 2.0, 0.4176422193116753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 613899.0158457832, 613899.0158457839, 175577.3353345396], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.97, 1.0, 1.0, 0.29836411965262083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17052750440160644, 0.17052750440160663, 0.2620557243799098], 
reward next is 0.7379, 
noisyNet noise sample is [array([-1.614331], dtype=float32), 0.47349465]. 
=============================================
[2019-03-26 09:55:40,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8309402e-19 1.0000000e+00 2.5643658e-23 5.0886064e-20 1.3687501e-19], sum to 1.0000
[2019-03-26 09:55:40,849] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1947
[2019-03-26 09:55:40,854] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.992492269360779, 6.9112, 168.9123707921927, 1511465.756393311, 1453794.422625122, 311346.3173510347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3163800.0000, 
sim time next is 3164400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.1114298855325, 6.9112, 168.9116008852936, 1595901.043383677, 1453852.211240108, 311346.3237198514], 
processed observation next is [1.0, 0.6521739130434783, 0.4312796208530806, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.02002298855325, 0.0, 0.8294332884122703, 0.44330584538435475, 0.40384783645558553, 0.464696005552017], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3152152], dtype=float32), 2.8404243]. 
=============================================
[2019-03-26 09:55:48,672] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87813: loss 0.0170
[2019-03-26 09:55:48,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87815: learning rate 0.0005
[2019-03-26 09:55:48,682] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87820: loss 0.0171
[2019-03-26 09:55:48,686] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87820: learning rate 0.0005
[2019-03-26 09:55:48,892] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87919: loss 0.0097
[2019-03-26 09:55:48,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87920: learning rate 0.0005
[2019-03-26 09:55:48,906] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87921: loss 0.0145
[2019-03-26 09:55:48,910] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87923: learning rate 0.0005
[2019-03-26 09:55:48,920] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87930: loss 0.0088
[2019-03-26 09:55:48,925] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87931: learning rate 0.0005
[2019-03-26 09:55:48,947] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87940: loss 0.0239
[2019-03-26 09:55:48,948] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 87940: loss 0.0147
[2019-03-26 09:55:48,952] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87940: learning rate 0.0005
[2019-03-26 09:55:48,953] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 87941: learning rate 0.0005
[2019-03-26 09:55:48,964] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87950: loss 0.0252
[2019-03-26 09:55:48,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87951: learning rate 0.0005
[2019-03-26 09:55:48,997] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 87963: loss 0.0322
[2019-03-26 09:55:48,998] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87963: loss 0.0570
[2019-03-26 09:55:48,999] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 87964: learning rate 0.0005
[2019-03-26 09:55:49,002] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87965: learning rate 0.0005
[2019-03-26 09:55:49,014] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 87969: loss 0.0624
[2019-03-26 09:55:49,018] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 87969: learning rate 0.0005
[2019-03-26 09:55:49,057] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0353119e-24 1.0000000e+00 2.4076341e-27 6.0549946e-24 2.8618955e-28], sum to 1.0000
[2019-03-26 09:55:49,069] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87992: loss 0.0720
[2019-03-26 09:55:49,070] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2955
[2019-03-26 09:55:49,071] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87992: learning rate 0.0005
[2019-03-26 09:55:49,076] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4210486180772605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616494.8503476972, 616494.8503476972, 175757.6330680395], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3301800.0000, 
sim time next is 3302400.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4214444541305093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617074.541790927, 617074.541790927, 175813.1796244131], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.83, 1.0, 1.0, 0.302945125458445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17140959494192415, 0.17140959494192415, 0.26240773078270613], 
reward next is 0.7376, 
noisyNet noise sample is [array([-1.9434338], dtype=float32), -0.36683995]. 
=============================================
[2019-03-26 09:55:49,220] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88061: loss 0.0054
[2019-03-26 09:55:49,221] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88061: learning rate 0.0005
[2019-03-26 09:55:49,238] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88067: loss 0.0071
[2019-03-26 09:55:49,242] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88068: learning rate 0.0005
[2019-03-26 09:55:49,617] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88246: loss 0.0071
[2019-03-26 09:55:49,623] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88249: learning rate 0.0005
[2019-03-26 09:55:49,668] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 88271: loss 0.0057
[2019-03-26 09:55:49,673] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 88271: learning rate 0.0005
[2019-03-26 09:55:57,254] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3544094e-14 9.9997449e-01 1.5450016e-16 2.5468491e-05 9.6278827e-18], sum to 1.0000
[2019-03-26 09:55:57,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2278
[2019-03-26 09:55:57,270] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5163119925786053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721473.5317996818, 721473.5317996818, 186191.4310685435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3444000.0000, 
sim time next is 3444600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5171221407009972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722605.9851274666, 722605.9851274659, 186322.3274356576], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4182194466277074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20072388475762962, 0.20072388475762942, 0.27809302602336955], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.8581761], dtype=float32), -0.010173446]. 
=============================================
[2019-03-26 09:56:01,542] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6557353e-11 3.3586717e-03 1.2223239e-12 9.9664128e-01 7.7281056e-17], sum to 1.0000
[2019-03-26 09:56:01,553] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3496
[2019-03-26 09:56:01,558] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.66666666666667, 71.66666666666667, 1.0, 2.0, 0.2800583009444972, 1.0, 2.0, 0.2800583009444972, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 782704.0958982086, 782704.0958982086, 246146.1400563848], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3522000.0000, 
sim time next is 3522600.0000, 
raw observation next is [30.5, 72.5, 1.0, 2.0, 0.2794220470774578, 1.0, 2.0, 0.2794220470774578, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 780925.2526870453, 780925.2526870453, 246030.6957295699], 
processed observation next is [1.0, 0.782608695652174, 0.6445497630331753, 0.725, 1.0, 1.0, 0.13183379165958772, 1.0, 1.0, 0.13183379165958772, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21692368130195702, 0.21692368130195702, 0.36720999362622375], 
reward next is 0.6328, 
noisyNet noise sample is [array([1.7730643], dtype=float32), -0.32331052]. 
=============================================
[2019-03-26 09:56:06,231] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95873: loss 0.3885
[2019-03-26 09:56:06,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95873: learning rate 0.0005
[2019-03-26 09:56:06,237] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95877: loss 0.1841
[2019-03-26 09:56:06,239] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95877: learning rate 0.0005
[2019-03-26 09:56:06,326] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95911: loss 0.6421
[2019-03-26 09:56:06,331] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95912: learning rate 0.0005
[2019-03-26 09:56:06,376] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95938: loss 0.2907
[2019-03-26 09:56:06,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95938: learning rate 0.0005
[2019-03-26 09:56:06,398] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95945: loss 0.6590
[2019-03-26 09:56:06,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95946: learning rate 0.0005
[2019-03-26 09:56:06,416] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95959: loss 0.7429
[2019-03-26 09:56:06,419] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95959: learning rate 0.0005
[2019-03-26 09:56:06,437] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95964: loss 0.7439
[2019-03-26 09:56:06,441] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95966: learning rate 0.0005
[2019-03-26 09:56:06,451] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 95969: loss 0.5674
[2019-03-26 09:56:06,454] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 95971: learning rate 0.0005
[2019-03-26 09:56:06,461] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95974: loss 0.5167
[2019-03-26 09:56:06,462] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95974: learning rate 0.0005
[2019-03-26 09:56:06,490] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95985: loss 0.6414
[2019-03-26 09:56:06,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95987: learning rate 0.0005
[2019-03-26 09:56:06,505] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95992: loss 0.7435
[2019-03-26 09:56:06,506] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95992: learning rate 0.0005
[2019-03-26 09:56:06,526] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96000: loss 0.6259
[2019-03-26 09:56:06,528] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96000: learning rate 0.0005
[2019-03-26 09:56:06,623] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96050: loss 0.3637
[2019-03-26 09:56:06,627] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96051: learning rate 0.0005
[2019-03-26 09:56:06,640] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96057: loss 0.3256
[2019-03-26 09:56:06,643] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96058: learning rate 0.0005
[2019-03-26 09:56:06,834] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96148: loss 0.0822
[2019-03-26 09:56:06,835] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96148: learning rate 0.0005
[2019-03-26 09:56:06,953] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 96201: loss 0.1273
[2019-03-26 09:56:06,955] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 96201: learning rate 0.0005
[2019-03-26 09:56:07,356] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1446053e-20 1.0000000e+00 1.3155617e-18 2.8291415e-13 4.8082934e-24], sum to 1.0000
[2019-03-26 09:56:07,365] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7287
[2019-03-26 09:56:07,371] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.5245965319221539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733054.0070592929, 733054.0070592923, 187539.6007289532], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3616200.0000, 
sim time next is 3616800.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.5264896050064825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735700.2414383219, 735700.2414383226, 187850.4142269838], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.74, 1.0, 1.0, 0.42950554820058123, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20436117817731164, 0.20436117817731184, 0.28037375257758773], 
reward next is 0.7196, 
noisyNet noise sample is [array([0.22610211], dtype=float32), -2.524328]. 
=============================================
[2019-03-26 09:56:09,208] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4812441e-15 1.0000000e+00 1.1530628e-15 9.8471870e-11 1.0138941e-21], sum to 1.0000
[2019-03-26 09:56:09,218] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1233
[2019-03-26 09:56:09,316] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 81.5, 1.0, 2.0, 0.6408547386061891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 895577.9052364004, 895577.9052364004, 208593.1511358148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3645000.0000, 
sim time next is 3645600.0000, 
raw observation next is [26.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6271433952662702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 876408.7521644707, 876408.7521644714, 205901.485104607], 
processed observation next is [1.0, 0.17391304347826086, 0.44707740916271754, 0.8233333333333335, 1.0, 1.0, 0.5507751750196026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24344687560124187, 0.24344687560124206, 0.3073156494098612], 
reward next is 0.6927, 
noisyNet noise sample is [array([-0.08991042], dtype=float32), 0.42497966]. 
=============================================
[2019-03-26 09:56:14,643] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.06996660e-14 1.00000000e+00 4.49770260e-15 3.20145830e-08
 1.09087256e-17], sum to 1.0000
[2019-03-26 09:56:14,655] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0224
[2019-03-26 09:56:14,659] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333333, 75.66666666666666, 1.0, 2.0, 0.7779722481120304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1087510.913913973, 1087510.913913973, 238602.1432485454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3739200.0000, 
sim time next is 3739800.0000, 
raw observation next is [27.66666666666667, 74.83333333333334, 1.0, 2.0, 0.7895435816668988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1103474.668945929, 1103474.668945929, 241356.8499119864], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012641, 0.7483333333333334, 1.0, 1.0, 0.7464380502010828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3065207413738692, 0.3065207413738692, 0.36023410434624836], 
reward next is 0.6398, 
noisyNet noise sample is [array([-1.7661554], dtype=float32), -0.122650765]. 
=============================================
[2019-03-26 09:56:15,175] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 09:56:15,177] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 09:56:15,178] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 09:56:15,179] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:56:15,183] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 09:56:15,183] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:56:15,184] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 09:56:15,185] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:56:15,185] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 09:56:15,187] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:56:15,189] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:56:15,204] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run5
[2019-03-26 09:56:15,221] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run5
[2019-03-26 09:56:15,240] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run5
[2019-03-26 09:56:15,262] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run5
[2019-03-26 09:56:15,264] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run5
[2019-03-26 09:56:17,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.29572186], dtype=float32), 0.084604405]
[2019-03-26 09:56:17,527] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.68048067166667, 84.74116276666668, 1.0, 2.0, 0.3930400745540952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626940.6200937877, 626940.6200937877, 177536.7692355873]
[2019-03-26 09:56:17,528] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:56:17,530] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2499822e-15 1.0000000e+00 4.2010267e-16 5.3714411e-09 6.1308236e-20], sampled 0.47549582919321276
[2019-03-26 09:56:24,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.29572186], dtype=float32), 0.084604405]
[2019-03-26 09:56:24,247] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.88013161666667, 77.91431047, 1.0, 2.0, 0.2146537277062341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 358721.361321229, 358721.361321229, 156641.5307246843]
[2019-03-26 09:56:24,250] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 09:56:24,253] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6037248e-16 1.0000000e+00 7.3773605e-17 1.0065878e-09 8.8246770e-21], sampled 0.22065033467466044
[2019-03-26 09:57:10,343] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.29572186], dtype=float32), 0.084604405]
[2019-03-26 09:57:10,344] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.01120307333333, 68.57499231, 1.0, 2.0, 0.6121273230493878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 875400.0530560736, 875400.0530560742, 205609.5225808203]
[2019-03-26 09:57:10,345] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:57:10,348] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.2853806e-14 9.9999917e-01 1.6921772e-14 7.8703164e-07 2.8224865e-18], sampled 0.530985992046169
[2019-03-26 09:57:12,594] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.29572186], dtype=float32), 0.084604405]
[2019-03-26 09:57:12,596] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.73333333333333, 47.66666666666667, 1.0, 2.0, 0.6138942343500475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857886.0822230475, 857886.0822230475, 203361.6638448685]
[2019-03-26 09:57:12,598] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 09:57:12,600] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.7302392e-14 9.9999726e-01 2.0761399e-14 2.7442193e-06 3.1187242e-18], sampled 0.08714789460496519
[2019-03-26 09:57:43,946] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.29572186], dtype=float32), 0.084604405]
[2019-03-26 09:57:43,949] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.97636240333333, 91.28733352333333, 1.0, 2.0, 0.6451954720836316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 901646.5427902336, 901646.5427902336, 209461.498175164]
[2019-03-26 09:57:43,949] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:57:43,951] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.72397461e-14 9.99999166e-01 1.14368786e-14 7.96694053e-07
 1.63573725e-18], sampled 0.2844782769533504
[2019-03-26 09:57:58,897] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.29572186], dtype=float32), 0.084604405]
[2019-03-26 09:57:58,900] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.40382785000001, 91.37124208333334, 1.0, 2.0, 0.4652012499726022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 663880.0509201541, 663880.0509201547, 180099.8954853753]
[2019-03-26 09:57:58,902] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 09:57:58,905] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2080093e-15 1.0000000e+00 9.3592196e-16 4.1609635e-08 1.3235950e-19], sampled 0.20054999799153728
[2019-03-26 09:58:10,104] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8502.7553 2967034143.5357 608.0000
[2019-03-26 09:58:10,504] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8849.3303 2768257038.4292 511.0000
[2019-03-26 09:58:10,672] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8610.8050 2903607970.6377 566.0000
[2019-03-26 09:58:10,781] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8278.0518 3129418740.6208 756.0000
[2019-03-26 09:58:10,801] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8762.0318 2823103746.0799 497.0000
[2019-03-26 09:58:11,816] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 100000, evaluation results [100000.0, 8278.051806098792, 3129418740.620777, 756.0, 8610.804965606036, 2903607970.6376886, 566.0, 8849.330303739214, 2768257038.4291577, 511.0, 8502.755264175683, 2967034143.535731, 608.0, 8762.031768304492, 2823103746.079878, 497.0]
[2019-03-26 09:58:12,670] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9906208e-14 1.0135054e-10 6.5347614e-12 1.0000000e+00 8.2772993e-17], sum to 1.0000
[2019-03-26 09:58:12,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0478
[2019-03-26 09:58:12,686] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.83333333333334, 60.0, 1.0, 2.0, 0.8929775654713279, 1.0, 2.0, 0.7670788222499266, 1.0, 1.0, 1.03, 7.005112953193682, 6.9112, 170.5573041426782, 3219175.608346144, 3151901.939838818, 589213.5751886483], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3766200.0000, 
sim time next is 3766800.0000, 
raw observation next is [34.66666666666667, 60.00000000000001, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.673011144707516, 6.9112, 170.5573041426782, 3455681.742289127, 2909965.430839048, 549394.9468617527], 
processed observation next is [1.0, 0.6086956521739131, 0.8420221169036337, 0.6000000000000001, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.0761811144707516, 0.0, 0.8375144448122397, 0.959911595080313, 0.8083237307886244, 0.8199924580026159], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.47872463], dtype=float32), 0.55895627]. 
=============================================
[2019-03-26 09:58:15,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4952475e-16 1.0000000e+00 1.6502108e-17 1.2724190e-10 3.6215454e-20], sum to 1.0000
[2019-03-26 09:58:15,143] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3350
[2019-03-26 09:58:15,147] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5210283407119982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728066.2248126034, 728066.2248126034, 186956.1836537216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3802800.0000, 
sim time next is 3803400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5200806170125459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726741.4568506749, 726741.4568506742, 186801.982092354], 
processed observation next is [0.0, 0.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42178387591873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20187262690296526, 0.20187262690296506, 0.27880892849605077], 
reward next is 0.7212, 
noisyNet noise sample is [array([-1.3619298], dtype=float32), 0.5903527]. 
=============================================
[2019-03-26 09:58:17,448] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.7310014e-18 1.0000000e+00 8.5255099e-22 6.3797898e-13 5.3179556e-23], sum to 1.0000
[2019-03-26 09:58:17,457] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5698
[2019-03-26 09:58:17,463] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.25, 62.0, 1.0, 2.0, 0.6165147167353393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 861549.5588072789, 861549.5588072783, 203863.8159743474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3846600.0000, 
sim time next is 3847200.0000, 
raw observation next is [34.33333333333334, 61.66666666666667, 1.0, 2.0, 0.6165615271090111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861615.000578552, 861615.000578552, 203872.7903916272], 
processed observation next is [0.0, 0.5217391304347826, 0.8262243285939973, 0.6166666666666667, 1.0, 1.0, 0.538025936275917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2393375001607089, 0.2393375001607089, 0.30428774685317495], 
reward next is 0.6957, 
noisyNet noise sample is [array([0.5448035], dtype=float32), 1.9275817]. 
=============================================
[2019-03-26 09:58:19,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9163146e-19 1.0000000e+00 1.4788963e-23 2.2433417e-18 3.5433681e-26], sum to 1.0000
[2019-03-26 09:58:19,833] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6677
[2019-03-26 09:58:19,838] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 76.0, 1.0, 2.0, 0.5450201728930366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761603.5650822597, 761603.5650822604, 190950.6089418575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3879600.0000, 
sim time next is 3880200.0000, 
raw observation next is [29.16666666666667, 77.5, 1.0, 2.0, 0.5483971457951963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766324.2020067868, 766324.2020067868, 191526.43717965], 
processed observation next is [0.0, 0.9130434782608695, 0.581358609794629, 0.775, 1.0, 1.0, 0.45590017565686297, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21286783389077413, 0.21286783389077413, 0.2858603539994776], 
reward next is 0.7141, 
noisyNet noise sample is [array([-0.5968745], dtype=float32), 0.37917078]. 
=============================================
[2019-03-26 09:58:20,560] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103823: loss 0.0134
[2019-03-26 09:58:20,563] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103825: learning rate 0.0005
[2019-03-26 09:58:20,589] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103837: loss 0.0006
[2019-03-26 09:58:20,592] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103837: learning rate 0.0005
[2019-03-26 09:58:20,647] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103861: loss 0.0029
[2019-03-26 09:58:20,649] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103861: learning rate 0.0005
[2019-03-26 09:58:20,712] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103893: loss 0.0013
[2019-03-26 09:58:20,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103894: learning rate 0.0005
[2019-03-26 09:58:20,763] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 103917: loss 0.0173
[2019-03-26 09:58:20,765] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 103917: learning rate 0.0005
[2019-03-26 09:58:20,824] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103943: loss 0.0286
[2019-03-26 09:58:20,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103944: learning rate 0.0005
[2019-03-26 09:58:20,860] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103963: loss 0.0234
[2019-03-26 09:58:20,863] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103963: learning rate 0.0005
[2019-03-26 09:58:20,919] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103987: loss 0.0218
[2019-03-26 09:58:20,922] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103988: learning rate 0.0005
[2019-03-26 09:58:20,945] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103997: loss 0.0120
[2019-03-26 09:58:20,947] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103997: learning rate 0.0005
[2019-03-26 09:58:20,978] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104011: loss 0.0024
[2019-03-26 09:58:20,984] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104012: learning rate 0.0005
[2019-03-26 09:58:20,994] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104015: loss 0.0008
[2019-03-26 09:58:20,995] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104015: learning rate 0.0005
[2019-03-26 09:58:21,008] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104022: loss 0.0021
[2019-03-26 09:58:21,014] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104022: learning rate 0.0005
[2019-03-26 09:58:21,097] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104068: loss 0.0058
[2019-03-26 09:58:21,100] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104068: learning rate 0.0005
[2019-03-26 09:58:21,205] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104113: loss 0.0541
[2019-03-26 09:58:21,213] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104117: learning rate 0.0005
[2019-03-26 09:58:21,252] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104140: loss 0.0246
[2019-03-26 09:58:21,258] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104141: learning rate 0.0005
[2019-03-26 09:58:21,367] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 104189: loss 0.0322
[2019-03-26 09:58:21,369] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 104190: learning rate 0.0005
[2019-03-26 09:58:25,249] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.6465804e-19 1.0000000e+00 9.2191486e-23 2.4003985e-12 1.3390310e-23], sum to 1.0000
[2019-03-26 09:58:25,255] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9823
[2019-03-26 09:58:25,259] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 84.0, 1.0, 2.0, 0.5962188130494563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833175.8885971106, 833175.8885971106, 200043.0258720054], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3977400.0000, 
sim time next is 3978000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5919392236872658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 827193.1200939014, 827193.1200939008, 199253.0817238713], 
processed observation next is [1.0, 0.043478260869565216, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5083605104665853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22977586669275038, 0.22977586669275024, 0.29739265928936015], 
reward next is 0.7026, 
noisyNet noise sample is [array([-0.27259803], dtype=float32), 1.1754833]. 
=============================================
[2019-03-26 09:58:25,277] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[65.362015]
 [65.58197 ]
 [65.62696 ]
 [65.85761 ]
 [66.05937 ]], R is [[65.36299133]
 [65.41079712]
 [65.45696259]
 [65.50138092]
 [65.54309082]].
[2019-03-26 09:58:25,356] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7177816e-13 9.9999940e-01 6.4757985e-15 5.8839254e-07 1.3944277e-16], sum to 1.0000
[2019-03-26 09:58:25,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4779
[2019-03-26 09:58:25,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2366331.375126337 W.
[2019-03-26 09:58:25,377] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.5, 81.5, 1.0, 2.0, 0.8460776566284705, 1.0, 2.0, 0.8460776566284705, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2366331.375126337, 2366331.375126337, 442931.7744719033], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3983400.0000, 
sim time next is 3984000.0000, 
raw observation next is [29.66666666666667, 80.66666666666667, 1.0, 2.0, 0.9234545027062884, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005989196627487, 6.9112, 168.9123930948152, 2187866.823018542, 2120620.328842629, 440633.5175626731], 
processed observation next is [1.0, 0.08695652173913043, 0.6050552922590839, 0.8066666666666668, 1.0, 1.0, 0.9077765092846848, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00947891966274872, 0.0, 0.8294371785236964, 0.6077407841718172, 0.5890612024562859, 0.6576619665114524], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.61878073], dtype=float32), 0.7653439]. 
=============================================
[2019-03-26 09:58:25,393] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[56.961983]
 [57.240917]
 [61.06754 ]
 [63.42811 ]
 [63.385925]], R is [[56.2669754 ]
 [55.70430756]
 [55.14726639]
 [54.59579468]
 [54.75406647]].
[2019-03-26 09:58:34,095] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7219282e-27 1.0000000e+00 9.5211642e-30 8.0429987e-35 2.2339929e-31], sum to 1.0000
[2019-03-26 09:58:34,108] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0195
[2019-03-26 09:58:34,114] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 77.66666666666667, 1.0, 2.0, 0.6193364997335972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865494.471020559, 865494.471020559, 204404.5528643883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4128600.0000, 
sim time next is 4129200.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6184073565569635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864195.5076228321, 864195.5076228321, 204226.0443763449], 
processed observation next is [1.0, 0.8260869565217391, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5402498271770645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24005430767300892, 0.24005430767300892, 0.3048149916064849], 
reward next is 0.6952, 
noisyNet noise sample is [array([-0.19364212], dtype=float32), 0.27497527]. 
=============================================
[2019-03-26 09:58:37,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.6737046e-12 1.0000000e+00 1.8079321e-12 4.0637097e-14 2.0047876e-14], sum to 1.0000
[2019-03-26 09:58:37,536] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0161
[2019-03-26 09:58:37,544] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2597364.621627934 W.
[2019-03-26 09:58:37,549] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.16666666666667, 55.5, 1.0, 2.0, 0.9285974255068775, 1.0, 2.0, 0.9285974255068775, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2597364.621627934, 2597364.621627935, 487340.1269690425], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4187400.0000, 
sim time next is 4188000.0000, 
raw observation next is [35.33333333333334, 55.0, 1.0, 2.0, 0.6181500837552131, 1.0, 2.0, 0.6181500837552131, 1.0, 1.0, 1.03, 6.960127112240851, 6.9112, 170.5573041426782, 2593522.200292996, 2558473.719567195, 494654.2971179267], 
processed observation next is [1.0, 0.4782608695652174, 0.8736176935229073, 0.55, 1.0, 1.0, 0.5399398599460399, 1.0, 1.0, 0.5399398599460399, 1.0, 0.5, 1.0365853658536586, 0.004892711224085122, 0.0, 0.8375144448122397, 0.7204228334147211, 0.7106871443242209, 0.738289995698398], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5171718], dtype=float32), 0.20056811]. 
=============================================
[2019-03-26 09:58:37,563] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[28.541063]
 [26.983192]
 [26.456928]
 [25.608261]
 [25.437767]], R is [[28.89650154]
 [28.88016319]
 [28.591362  ]
 [28.30544853]
 [28.02239418]].
[2019-03-26 09:58:37,655] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111776: loss -129.7462
[2019-03-26 09:58:37,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111777: learning rate 0.0005
[2019-03-26 09:58:37,963] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111919: loss -125.7797
[2019-03-26 09:58:37,970] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111921: loss -121.0775
[2019-03-26 09:58:37,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111920: learning rate 0.0005
[2019-03-26 09:58:37,972] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111922: learning rate 0.0005
[2019-03-26 09:58:37,980] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111926: loss -83.5319
[2019-03-26 09:58:37,982] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111927: loss -122.8981
[2019-03-26 09:58:37,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111927: learning rate 0.0005
[2019-03-26 09:58:37,984] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111927: learning rate 0.0005
[2019-03-26 09:58:38,037] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111949: loss -130.2791
[2019-03-26 09:58:38,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111949: learning rate 0.0005
[2019-03-26 09:58:38,095] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111977: loss -157.7656
[2019-03-26 09:58:38,096] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111978: learning rate 0.0005
[2019-03-26 09:58:38,117] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111989: loss -126.1997
[2019-03-26 09:58:38,119] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111989: learning rate 0.0005
[2019-03-26 09:58:38,142] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112000: loss -117.7132
[2019-03-26 09:58:38,144] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112000: learning rate 0.0005
[2019-03-26 09:58:38,157] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112005: loss -177.3913
[2019-03-26 09:58:38,160] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112006: learning rate 0.0005
[2019-03-26 09:58:38,195] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112020: loss -140.9599
[2019-03-26 09:58:38,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112020: learning rate 0.0005
[2019-03-26 09:58:38,233] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112037: loss -138.2813
[2019-03-26 09:58:38,236] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112037: learning rate 0.0005
[2019-03-26 09:58:38,241] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112041: loss -89.2455
[2019-03-26 09:58:38,242] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112041: learning rate 0.0005
[2019-03-26 09:58:38,308] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112075: loss -146.4814
[2019-03-26 09:58:38,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112075: learning rate 0.0005
[2019-03-26 09:58:38,386] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112104: loss -138.8567
[2019-03-26 09:58:38,387] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112104: learning rate 0.0005
[2019-03-26 09:58:38,473] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 112151: loss -71.6245
[2019-03-26 09:58:38,474] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 112152: learning rate 0.0005
[2019-03-26 09:58:41,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7234597e-19 1.0000000e+00 2.9796073e-18 2.5633179e-24 4.9808458e-20], sum to 1.0000
[2019-03-26 09:58:41,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7404
[2019-03-26 09:58:41,350] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.8695328207646479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1215332.475601116, 1215332.475601116, 261695.6773657594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4251600.0000, 
sim time next is 4252200.0000, 
raw observation next is [29.0, 79.00000000000001, 1.0, 2.0, 0.8900807917445963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1244068.880276002, 1244068.880276002, 267225.0427828685], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.7900000000000001, 1.0, 1.0, 0.8675672189693932, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3455746889655561, 0.3455746889655561, 0.39884334743711714], 
reward next is 0.6012, 
noisyNet noise sample is [array([-1.2683853], dtype=float32), 0.59764993]. 
=============================================
[2019-03-26 09:58:51,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3302634e-16 1.0000000e+00 6.5485827e-13 5.2164605e-19 2.8284430e-11], sum to 1.0000
[2019-03-26 09:58:51,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0015
[2019-03-26 09:58:51,174] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 88.16666666666667, 1.0, 2.0, 0.6086994969410618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850623.793003844, 850623.793003844, 202377.6955014523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4417800.0000, 
sim time next is 4418400.0000, 
raw observation next is [29.0, 87.33333333333334, 1.0, 2.0, 0.6034250682298955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 843250.1454380996, 843250.145438099, 201385.6120817494], 
processed observation next is [0.0, 0.13043478260869565, 0.5734597156398105, 0.8733333333333334, 1.0, 1.0, 0.5221988773854163, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23423615151058322, 0.23423615151058308, 0.3005755404205215], 
reward next is 0.6994, 
noisyNet noise sample is [array([-0.47625023], dtype=float32), -0.16139193]. 
=============================================
[2019-03-26 09:58:54,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0730283e-21 1.0000000e+00 3.9924021e-20 2.6564468e-23 4.4775080e-20], sum to 1.0000
[2019-03-26 09:58:54,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7058
[2019-03-26 09:58:54,341] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5509894483882853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769947.97032294, 769947.97032294, 191970.2304227675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4480200.0000, 
sim time next is 4480800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5507253237462932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769578.751089806, 769578.7510898054, 191924.8852004196], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4587052093328834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21377187530272387, 0.2137718753027237, 0.28645505253793974], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.24924634], dtype=float32), 0.050654374]. 
=============================================
[2019-03-26 09:58:55,087] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119736: loss 1.0652
[2019-03-26 09:58:55,090] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119737: learning rate 0.0005
[2019-03-26 09:58:55,380] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119879: loss 0.6565
[2019-03-26 09:58:55,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119879: learning rate 0.0005
[2019-03-26 09:58:55,440] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119905: loss 0.5106
[2019-03-26 09:58:55,443] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119907: learning rate 0.0005
[2019-03-26 09:58:55,488] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119928: loss 0.5622
[2019-03-26 09:58:55,492] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119928: learning rate 0.0005
[2019-03-26 09:58:55,510] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 119940: loss 0.2931
[2019-03-26 09:58:55,511] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 119940: loss 0.4564
[2019-03-26 09:58:55,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 119941: learning rate 0.0005
[2019-03-26 09:58:55,517] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 119941: learning rate 0.0005
[2019-03-26 09:58:55,540] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119953: loss 0.4033
[2019-03-26 09:58:55,542] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119953: learning rate 0.0005
[2019-03-26 09:58:55,579] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119969: loss 0.5930
[2019-03-26 09:58:55,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119969: learning rate 0.0005
[2019-03-26 09:58:55,586] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119973: loss 0.3588
[2019-03-26 09:58:55,588] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119973: learning rate 0.0005
[2019-03-26 09:58:55,614] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119987: loss 0.3139
[2019-03-26 09:58:55,618] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119987: learning rate 0.0005
[2019-03-26 09:58:55,663] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120010: loss 0.2884
[2019-03-26 09:58:55,666] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120011: learning rate 0.0005
[2019-03-26 09:58:55,721] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120038: loss 0.0943
[2019-03-26 09:58:55,724] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120039: learning rate 0.0005
[2019-03-26 09:58:55,852] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120100: loss 0.0235
[2019-03-26 09:58:55,854] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120101: learning rate 0.0005
[2019-03-26 09:58:55,888] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120117: loss 0.0188
[2019-03-26 09:58:55,889] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120117: learning rate 0.0005
[2019-03-26 09:58:55,907] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120123: loss 0.0108
[2019-03-26 09:58:55,909] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120123: learning rate 0.0005
[2019-03-26 09:58:56,060] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 120196: loss 0.0082
[2019-03-26 09:58:56,063] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 120198: learning rate 0.0005
[2019-03-26 09:58:56,690] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.7389750e-21 1.0000000e+00 8.3064584e-18 2.6881411e-22 1.8872013e-20], sum to 1.0000
[2019-03-26 09:58:56,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4255
[2019-03-26 09:58:56,708] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5134090715666738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717415.7373815287, 717415.7373815293, 185723.4554471353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4520400.0000, 
sim time next is 4521000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5134148259972892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717423.7810905465, 717423.7810905459, 185724.3795693072], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41375280240637247, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1992843836362629, 0.19928438363626275, 0.27720056652135405], 
reward next is 0.7228, 
noisyNet noise sample is [array([2.7524886], dtype=float32), -0.15425535]. 
=============================================
[2019-03-26 09:58:56,718] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[66.53509 ]
 [66.48433 ]
 [66.43203 ]
 [66.380806]
 [66.335785]], R is [[66.63396454]
 [66.69042969]
 [66.74642181]
 [66.80194092]
 [66.85697174]].
[2019-03-26 09:58:57,349] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5454489e-22 1.0000000e+00 1.7168692e-19 2.5025636e-24 1.8137609e-21], sum to 1.0000
[2019-03-26 09:58:57,355] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3649
[2019-03-26 09:58:57,361] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.5013865127069486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700610.3909894257, 700610.3909894257, 183813.1069169299], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4524600.0000, 
sim time next is 4525200.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.497565174219769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695268.9118359083, 695268.9118359083, 183214.9644337025], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.74, 1.0, 1.0, 0.39465683640936017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1931302532877523, 0.1931302532877523, 0.2734551707965709], 
reward next is 0.7265, 
noisyNet noise sample is [array([1.9514731], dtype=float32), 0.8917448]. 
=============================================
[2019-03-26 09:58:57,435] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.5897734e-23 1.0000000e+00 2.2309906e-20 7.9645381e-26 5.4383799e-23], sum to 1.0000
[2019-03-26 09:58:57,445] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7770
[2019-03-26 09:58:57,451] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 68.16666666666666, 1.0, 2.0, 0.5490514145734481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767238.8002684446, 767238.8002684452, 191637.9977070911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4535400.0000, 
sim time next is 4536000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5467411075832034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764009.2399946371, 764009.2399946364, 191243.1930553359], 
processed observation next is [0.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4539049488954257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21222478888739918, 0.21222478888739899, 0.2854376015751282], 
reward next is 0.7146, 
noisyNet noise sample is [array([-0.42133284], dtype=float32), 0.41099173]. 
=============================================
[2019-03-26 09:58:57,470] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[70.86063]
 [70.86808]
 [70.85602]
 [70.83212]
 [70.80173]], R is [[70.89981079]
 [70.90478516]
 [70.90956116]
 [70.91430664]
 [70.9188385 ]].
[2019-03-26 09:58:57,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.6003362e-24 1.0000000e+00 9.8897971e-23 7.0744904e-26 2.7087861e-25], sum to 1.0000
[2019-03-26 09:58:57,527] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0952
[2019-03-26 09:58:57,531] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 76.5, 1.0, 2.0, 0.5034308039703232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703467.9188803635, 703467.9188803635, 184135.4065179983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4527000.0000, 
sim time next is 4527600.0000, 
raw observation next is [28.0, 77.33333333333333, 1.0, 2.0, 0.5069479643517638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708384.2538676553, 708384.2538676547, 184691.9683344368], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.7733333333333333, 1.0, 1.0, 0.40596140283345034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19677340385212647, 0.1967734038521263, 0.2756596542305027], 
reward next is 0.7243, 
noisyNet noise sample is [array([0.3095377], dtype=float32), 0.9011995]. 
=============================================
[2019-03-26 09:59:01,034] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4900624e-12 1.0000000e+00 2.1081629e-08 3.1407326e-09 4.1390413e-10], sum to 1.0000
[2019-03-26 09:59:01,042] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6673
[2019-03-26 09:59:01,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1785928.010001174 W.
[2019-03-26 09:59:01,053] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.0, 85.66666666666667, 1.0, 2.0, 0.4258144984131436, 1.0, 2.0, 0.4258144984131436, 1.0, 2.0, 0.7394989647371646, 6.9112, 6.9112, 170.5573041426782, 1785928.010001174, 1785928.010001174, 367638.3013573977], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4588800.0000, 
sim time next is 4589400.0000, 
raw observation next is [28.0, 84.83333333333333, 1.0, 2.0, 0.5986347324553951, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.912419547919808, 6.9112, 168.9128798746201, 1673766.012445746, 1672900.82357037, 364694.2849510412], 
processed observation next is [1.0, 0.08695652173913043, 0.5260663507109005, 0.8483333333333333, 1.0, 1.0, 0.516427388500476, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00012195479198080505, 0.0, 0.8294395688353872, 0.46493500345715166, 0.46469467321399166, 0.5443198282851361], 
reward next is 0.4496, 
noisyNet noise sample is [array([-0.6744664], dtype=float32), -0.27473187]. 
=============================================
[2019-03-26 09:59:04,088] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7800750e-14 1.5364313e-01 8.4635681e-01 7.7253527e-17 4.7817106e-16], sum to 1.0000
[2019-03-26 09:59:04,101] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5293
[2019-03-26 09:59:04,108] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.2797719899004428, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4841142665047323, 6.911200000000001, 6.9112, 168.912956510431, 781906.399618737, 781906.3996187365, 215519.1753444789], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4645800.0000, 
sim time next is 4646400.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.2789427440845881, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4826790665567279, 6.9112, 6.9112, 168.912956510431, 779587.9739847527, 779587.9739847527, 215272.7659034921], 
processed observation next is [1.0, 0.782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.1312563181742025, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.36912081287405835, 0.0, 0.0, 0.8294399451523027, 0.21655221499576463, 0.21655221499576463, 0.3213026356768539], 
reward next is 0.6787, 
noisyNet noise sample is [array([0.00520645], dtype=float32), 0.4253305]. 
=============================================
[2019-03-26 09:59:06,184] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4608944e-13 1.6346978e-03 9.9836534e-01 9.4762479e-13 5.3175484e-17], sum to 1.0000
[2019-03-26 09:59:06,193] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9744
[2019-03-26 09:59:06,197] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.3183787518210954, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5433621135775227, 6.9112, 6.9112, 168.912956510431, 889849.7268055704, 889849.7268055704, 226997.5295888985], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4684800.0000, 
sim time next is 4685400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.3170768245171375, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5411233523981996, 6.9112, 6.9112, 168.912956510431, 886209.3990846409, 886209.3990846409, 226561.7015735655], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.17720099339414153, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.44039433219292634, 0.0, 0.0, 0.8294399451523027, 0.24616927752351137, 0.24616927752351137, 0.3381517933933813], 
reward next is 0.6618, 
noisyNet noise sample is [array([0.7131391], dtype=float32), 0.09943312]. 
=============================================
[2019-03-26 09:59:06,557] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 09:59:06,559] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 09:59:06,560] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 09:59:06,560] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:59:06,562] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 09:59:06,563] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:59:06,564] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:59:06,565] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 09:59:06,567] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 09:59:06,569] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:59:06,569] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 09:59:06,585] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run6
[2019-03-26 09:59:06,603] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run6
[2019-03-26 09:59:06,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run6
[2019-03-26 09:59:06,605] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run6
[2019-03-26 09:59:06,637] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run6
[2019-03-26 09:59:30,101] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.25941303], dtype=float32), 0.17053892]
[2019-03-26 09:59:30,103] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.96666666666667, 79.33333333333334, 1.0, 2.0, 0.1943004395167671, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3377707224108739, 6.911200000000001, 6.9112, 168.912956510431, 578689.0939952534, 578689.0939952529, 197526.335096919]
[2019-03-26 09:59:30,106] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 09:59:30,108] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.2212084e-12 4.5563476e-03 9.9544370e-01 8.5263940e-12 3.5114100e-14], sampled 0.9470410571796499
[2019-03-26 09:59:33,946] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.25941303], dtype=float32), 0.17053892]
[2019-03-26 09:59:33,948] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.06666666666667, 87.50000000000001, 1.0, 2.0, 0.1818516239772323, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3211194031992858, 6.9112, 6.9112, 168.912956510431, 554762.6100602137, 554762.6100602137, 196386.8978773965]
[2019-03-26 09:59:33,949] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 09:59:33,951] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.0863016e-12 9.2552723e-03 9.9074477e-01 6.0802032e-12 4.6377964e-14], sampled 0.8815499936471555
[2019-03-26 10:00:51,399] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.25941303], dtype=float32), 0.17053892]
[2019-03-26 10:00:51,400] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.16666666666666, 79.66666666666666, 1.0, 2.0, 0.2849268309845198, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4917811834491354, 6.9112, 6.9112, 168.912956510431, 796318.5432824278, 796318.5432824278, 216950.8656366649]
[2019-03-26 10:00:51,401] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:00:51,404] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.8951650e-12 3.1586196e-02 9.6841383e-01 7.4663149e-12 5.7509442e-15], sampled 0.6452835741298941
[2019-03-26 10:00:54,318] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.25941303], dtype=float32), 0.17053892]
[2019-03-26 10:00:54,320] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.56666666666667, 82.66666666666667, 1.0, 2.0, 0.2874157956471275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462797.0075401866, 462797.0075401873, 164463.953213106]
[2019-03-26 10:00:54,321] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:00:54,323] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1611925e-13 9.6828598e-01 3.1713981e-02 8.4280041e-15 3.2517759e-16], sampled 0.937076807357339
[2019-03-26 10:00:54,433] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.25941303], dtype=float32), 0.17053892]
[2019-03-26 10:00:54,434] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.536904265, 73.13606968833334, 1.0, 2.0, 0.2334279159336283, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3903728567710331, 6.911200000000001, 6.9112, 168.912956510431, 657022.1793829596, 657022.179382959, 202280.7829204715]
[2019-03-26 10:00:54,435] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:00:54,438] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.7068486e-12 2.2273646e-01 7.7726352e-01 1.2494915e-11 4.8335109e-15], sampled 0.5127828771551179
[2019-03-26 10:01:01,170] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.25941303], dtype=float32), 0.17053892]
[2019-03-26 10:01:01,171] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.6, 62.5, 1.0, 2.0, 0.2797450154545327, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5032079111688229, 6.9112, 6.9112, 168.912956510431, 879416.3526024501, 879416.3526024501, 224225.3143882331]
[2019-03-26 10:01:01,172] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:01:01,175] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8520609e-11 4.3979348e-03 9.9560201e-01 9.1946915e-11 1.0539272e-14], sampled 0.16529652350301627
[2019-03-26 10:01:01,260] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7484.3220 3108299461.7215 866.0000
[2019-03-26 10:01:01,855] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6933.4787 3326770417.4581 1600.0000
[2019-03-26 10:01:02,258] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7582.4838 2960614517.8133 647.0000
[2019-03-26 10:01:02,286] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7201.2826 3179845633.0411 1379.0000
[2019-03-26 10:01:02,426] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7440.0673 3018136427.1036 828.0000
[2019-03-26 10:01:03,438] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 125000, evaluation results [125000.0, 6933.478678550307, 3326770417.458104, 1600.0, 7484.322009834046, 3108299461.7215157, 866.0, 7582.483789631299, 2960614517.8132844, 647.0, 7201.282552072471, 3179845633.0410633, 1379.0, 7440.067348630512, 3018136427.1036286, 828.0]
[2019-03-26 10:01:05,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.33762626e-14 4.39770321e-16 1.00000000e+00 1.16523654e-14
 4.41219908e-11], sum to 1.0000
[2019-03-26 10:01:05,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3657
[2019-03-26 10:01:05,965] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.2588182953897669, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4415995295071624, 6.911199999999999, 6.9112, 168.912956510431, 723325.102033454, 723325.1020334547, 209000.173861959], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4730400.0000, 
sim time next is 4731000.0000, 
raw observation next is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.2598615253168211, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4429101985489076, 6.911199999999999, 6.9112, 168.912956510431, 726241.6359888734, 726241.635988874, 209242.2322395436], 
processed observation next is [1.0, 0.782608695652174, 0.6129541864139019, 0.7066666666666667, 1.0, 1.0, 0.10826689797207362, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.32062219335232633, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20173378777468706, 0.20173378777468723, 0.3123018391634979], 
reward next is 0.6877, 
noisyNet noise sample is [array([-1.0456066], dtype=float32), -0.7241122]. 
=============================================
[2019-03-26 10:01:05,978] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[18.590683]
 [18.05276 ]
 [17.918823]
 [17.129938]
 [15.76027 ]], R is [[19.96678734]
 [20.45517921]
 [20.93887711]
 [21.41810799]
 [21.89373016]].
[2019-03-26 10:01:09,510] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127741: loss 4.5302
[2019-03-26 10:01:09,512] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127741: learning rate 0.0005
[2019-03-26 10:01:09,816] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127880: loss 2.1167
[2019-03-26 10:01:09,818] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127880: learning rate 0.0005
[2019-03-26 10:01:09,846] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 127895: loss 2.1119
[2019-03-26 10:01:09,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 127895: learning rate 0.0005
[2019-03-26 10:01:09,924] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127929: loss 2.0046
[2019-03-26 10:01:09,926] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127930: learning rate 0.0005
[2019-03-26 10:01:09,939] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127935: loss 2.1240
[2019-03-26 10:01:09,942] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127936: learning rate 0.0005
[2019-03-26 10:01:09,993] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 127959: loss 2.2536
[2019-03-26 10:01:09,996] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 127959: learning rate 0.0005
[2019-03-26 10:01:10,010] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 127967: loss 2.0291
[2019-03-26 10:01:10,011] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 127967: learning rate 0.0005
[2019-03-26 10:01:10,022] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127972: loss 2.3662
[2019-03-26 10:01:10,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127972: learning rate 0.0005
[2019-03-26 10:01:10,061] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127991: loss 2.3815
[2019-03-26 10:01:10,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127991: learning rate 0.0005
[2019-03-26 10:01:10,093] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128007: loss 2.0408
[2019-03-26 10:01:10,095] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128007: learning rate 0.0005
[2019-03-26 10:01:10,118] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 128018: loss 2.2550
[2019-03-26 10:01:10,120] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 128019: learning rate 0.0005
[2019-03-26 10:01:10,133] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 128025: loss 2.6509
[2019-03-26 10:01:10,134] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 128025: learning rate 0.0005
[2019-03-26 10:01:10,212] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128060: loss 1.9263
[2019-03-26 10:01:10,223] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128064: learning rate 0.0005
[2019-03-26 10:01:10,236] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128074: loss 1.7975
[2019-03-26 10:01:10,239] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128075: learning rate 0.0005
[2019-03-26 10:01:10,333] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128117: loss 1.9551
[2019-03-26 10:01:10,342] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128119: learning rate 0.0005
[2019-03-26 10:01:10,432] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 128161: loss 2.0219
[2019-03-26 10:01:10,435] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 128161: learning rate 0.0005
[2019-03-26 10:01:11,090] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.8540516e-15 3.5010101e-23 1.0000000e+00 4.5660009e-13 2.1613156e-16], sum to 1.0000
[2019-03-26 10:01:11,099] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5922
[2019-03-26 10:01:11,104] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.83333333333334, 63.5, 1.0, 2.0, 0.5909583787712086, 1.0, 2.0, 0.5909583787712086, 1.0, 2.0, 1.02629927100334, 6.9112, 6.9112, 170.5573041426782, 2479322.9782481, 2479322.9782481, 483741.6849154126], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [31.66666666666667, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.695638452468485, 6.9112, 168.9089810947286, 2846081.697939218, 2289587.159039713, 474598.783417061], 
processed observation next is [1.0, 0.6956521739130435, 0.6998420221169038, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.07844384524684847, 0.0, 0.8294204240412507, 0.7905782494275606, 0.6359964330665869, 0.7083563931597925], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8872932], dtype=float32), 0.7944463]. 
=============================================
[2019-03-26 10:01:12,935] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2095294e-15 1.5898290e-12 1.0000000e+00 7.2598029e-16 2.3809869e-16], sum to 1.0000
[2019-03-26 10:01:12,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0958
[2019-03-26 10:01:12,948] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 82.33333333333334, 1.0, 2.0, 0.3064177630884508, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5154170544956933, 6.9112, 6.9112, 168.912956510431, 856405.9947276551, 856405.9947276551, 222261.6843515352], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4851600.0000, 
sim time next is 4852200.0000, 
raw observation next is [27.0, 83.16666666666666, 1.0, 2.0, 0.30789126555215, 0.0, 2.0, 0.0, 1.0, 2.0, 0.518789231808519, 6.9112, 6.9112, 168.912956510431, 860525.9502292677, 860525.9502292677, 222826.8805444795], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.8316666666666666, 1.0, 1.0, 0.16613405488210845, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4131575997664865, 0.0, 0.0, 0.8294399451523027, 0.2390349861747966, 0.2390349861747966, 0.33257743364847686], 
reward next is 0.6674, 
noisyNet noise sample is [array([-0.7481652], dtype=float32), -1.5178475]. 
=============================================
[2019-03-26 10:01:26,555] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 135672: loss 6.9154
[2019-03-26 10:01:26,559] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 135672: learning rate 0.0005
[2019-03-26 10:01:26,981] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135887: loss 4.6253
[2019-03-26 10:01:26,986] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135888: learning rate 0.0005
[2019-03-26 10:01:27,025] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 135902: loss 3.9646
[2019-03-26 10:01:27,028] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 135902: learning rate 0.0005
[2019-03-26 10:01:27,087] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 135929: loss 4.6475
[2019-03-26 10:01:27,089] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 135929: learning rate 0.0005
[2019-03-26 10:01:27,158] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 135963: loss 5.4253
[2019-03-26 10:01:27,162] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 135963: loss 4.8008
[2019-03-26 10:01:27,162] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 135963: learning rate 0.0005
[2019-03-26 10:01:27,165] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 135963: learning rate 0.0005
[2019-03-26 10:01:27,192] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 135973: loss 5.6099
[2019-03-26 10:01:27,194] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 135973: learning rate 0.0005
[2019-03-26 10:01:27,198] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 135976: loss 5.3868
[2019-03-26 10:01:27,205] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 135979: learning rate 0.0005
[2019-03-26 10:01:27,218] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 135984: loss 5.9972
[2019-03-26 10:01:27,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 135985: learning rate 0.0005
[2019-03-26 10:01:27,257] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136003: loss 5.9792
[2019-03-26 10:01:27,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136004: learning rate 0.0005
[2019-03-26 10:01:27,273] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2015433e-19 9.9999988e-01 8.2949903e-08 1.7284338e-20 4.0065405e-22], sum to 1.0000
[2019-03-26 10:01:27,275] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7763
[2019-03-26 10:01:27,280] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5164354593231327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721646.1178474353, 721646.1178474353, 186210.8231642146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5095800.0000, 
sim time next is 5096400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5156883475053693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720601.7797672314, 720601.7797672321, 186090.2565777657], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.416491984946228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20016716104645318, 0.20016716104645338, 0.2777466516086055], 
reward next is 0.7223, 
noisyNet noise sample is [array([0.4800736], dtype=float32), 2.35502]. 
=============================================
[2019-03-26 10:01:27,316] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 136036: loss 6.3804
[2019-03-26 10:01:27,317] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 136036: learning rate 0.0005
[2019-03-26 10:01:27,320] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 136037: loss 6.1159
[2019-03-26 10:01:27,326] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 136042: learning rate 0.0005
[2019-03-26 10:01:27,388] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 136070: loss 5.4876
[2019-03-26 10:01:27,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 136071: learning rate 0.0005
[2019-03-26 10:01:27,393] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136072: loss 5.5005
[2019-03-26 10:01:27,396] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136072: learning rate 0.0005
[2019-03-26 10:01:27,509] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136127: loss 4.4953
[2019-03-26 10:01:27,513] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136127: learning rate 0.0005
[2019-03-26 10:01:27,695] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 136226: loss 3.8486
[2019-03-26 10:01:27,696] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 136226: learning rate 0.0005
[2019-03-26 10:01:33,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6698128e-13 9.9999833e-01 1.6459595e-06 7.6863440e-13 2.5274262e-17], sum to 1.0000
[2019-03-26 10:01:33,590] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4407
[2019-03-26 10:01:33,598] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 82.33333333333334, 1.0, 2.0, 0.7551541827924095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1055387.78787028, 1055387.78787028, 233199.0512750815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5206800.0000, 
sim time next is 5207400.0000, 
raw observation next is [27.5, 81.5, 1.0, 2.0, 0.7578257801204309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1059123.418745315, 1059123.418745315, 233820.490975015], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.815, 1.0, 1.0, 0.7082238314703987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29420094965147636, 0.29420094965147636, 0.34898580742539553], 
reward next is 0.6510, 
noisyNet noise sample is [array([-0.14995828], dtype=float32), 0.3067042]. 
=============================================
[2019-03-26 10:01:43,964] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 143693: loss -24.2038
[2019-03-26 10:01:43,966] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 143693: learning rate 0.0005
[2019-03-26 10:01:44,306] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143854: loss -64.4072
[2019-03-26 10:01:44,308] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143854: learning rate 0.0005
[2019-03-26 10:01:44,349] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 143867: loss -60.5206
[2019-03-26 10:01:44,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 143867: learning rate 0.0005
[2019-03-26 10:01:44,468] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 143926: loss -15.2357
[2019-03-26 10:01:44,470] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 143927: learning rate 0.0005
[2019-03-26 10:01:44,486] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 143934: loss -40.8058
[2019-03-26 10:01:44,487] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 143934: learning rate 0.0005
[2019-03-26 10:01:44,564] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 143964: loss -8.3444
[2019-03-26 10:01:44,567] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 143964: learning rate 0.0005
[2019-03-26 10:01:44,622] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 143997: loss -69.6953
[2019-03-26 10:01:44,625] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 143998: learning rate 0.0005
[2019-03-26 10:01:44,634] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 144000: loss -43.8266
[2019-03-26 10:01:44,636] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 144000: learning rate 0.0005
[2019-03-26 10:01:44,663] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 144013: loss -47.1227
[2019-03-26 10:01:44,665] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 144014: learning rate 0.0005
[2019-03-26 10:01:44,679] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144019: loss -55.1222
[2019-03-26 10:01:44,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144019: learning rate 0.0005
[2019-03-26 10:01:44,682] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 144019: loss -1.2422
[2019-03-26 10:01:44,686] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 144020: learning rate 0.0005
[2019-03-26 10:01:44,711] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 144030: loss -70.4854
[2019-03-26 10:01:44,713] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 144030: learning rate 0.0005
[2019-03-26 10:01:44,808] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 144079: loss -34.1567
[2019-03-26 10:01:44,809] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 144079: learning rate 0.0005
[2019-03-26 10:01:44,820] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 144085: loss -32.2566
[2019-03-26 10:01:44,821] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 144085: learning rate 0.0005
[2019-03-26 10:01:44,838] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144093: loss -58.3608
[2019-03-26 10:01:44,839] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144093: learning rate 0.0005
[2019-03-26 10:01:45,205] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 144258: loss 21.7393
[2019-03-26 10:01:45,206] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 144258: learning rate 0.0005
[2019-03-26 10:01:45,992] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.3428192e-33 1.0000000e+00 1.3062748e-34 0.0000000e+00 1.5674600e-35], sum to 1.0000
[2019-03-26 10:01:46,001] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9056
[2019-03-26 10:01:46,006] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 79.66666666666667, 1.0, 2.0, 0.6116229153945442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 854710.751960657, 854710.751960657, 202932.1652566262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5425800.0000, 
sim time next is 5426400.0000, 
raw observation next is [30.8, 80.33333333333334, 1.0, 2.0, 0.6167525996765194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861882.12361638, 861882.12361638, 203909.3314807037], 
processed observation next is [1.0, 0.8260869565217391, 0.6587677725118484, 0.8033333333333335, 1.0, 1.0, 0.5382561441885776, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23941170100455, 0.23941170100455, 0.30434228579209505], 
reward next is 0.6957, 
noisyNet noise sample is [array([-0.30830708], dtype=float32), -1.0414392]. 
=============================================
[2019-03-26 10:01:49,814] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.0073612e-21 1.0000000e+00 2.0867889e-22 6.6216176e-21 6.7545406e-27], sum to 1.0000
[2019-03-26 10:01:49,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5919
[2019-03-26 10:01:49,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3499227.920396667 W.
[2019-03-26 10:01:49,831] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 65.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.733730161705064, 6.9112, 170.5573041426782, 3499227.920396667, 2910016.107179549, 549026.9991474095], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5482800.0000, 
sim time next is 5483400.0000, 
raw observation next is [35.11666666666667, 63.83333333333334, 1.0, 2.0, 0.897245438840963, 1.0, 2.0, 0.7692127589347441, 1.0, 1.0, 1.03, 7.005113289932327, 6.9112, 170.5573041426782, 3228142.608365779, 3160868.698638864, 590872.9630568892], 
processed observation next is [1.0, 0.4782608695652174, 0.863349131121643, 0.6383333333333334, 1.0, 1.0, 0.8761993239047746, 1.0, 1.0, 0.7219430830539085, 1.0, 0.5, 1.0365853658536586, 0.009391328993232672, 0.0, 0.8375144448122397, 0.8967062801016052, 0.87801908295524, 0.8818999448610287], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29316032], dtype=float32), 0.48239484]. 
=============================================
[2019-03-26 10:01:54,817] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9636924e-17 1.0000000e+00 2.3589601e-19 3.1425660e-13 1.6270236e-21], sum to 1.0000
[2019-03-26 10:01:54,828] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2483
[2019-03-26 10:01:54,837] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1949246.284402324 W.
[2019-03-26 10:01:54,843] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.7, 51.33333333333334, 1.0, 2.0, 0.7529636562444041, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.985985265081936, 6.9112, 168.9125104323568, 1949246.284402324, 1896191.1835553, 397285.7489106591], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5577000.0000, 
sim time next is 5577600.0000, 
raw observation next is [33.8, 50.66666666666667, 1.0, 2.0, 0.9692401540917849, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.982376798266989, 6.9112, 168.9124729710209, 2251948.781243599, 2201453.655387166, 454833.8587584196], 
processed observation next is [1.0, 0.5652173913043478, 0.800947867298578, 0.5066666666666667, 1.0, 1.0, 0.9629399446888974, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0071176798266988864, 0.0, 0.8294375707524337, 0.6255413281232219, 0.6115149042742127, 0.6788565056095814], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4682729], dtype=float32), 0.68254447]. 
=============================================
[2019-03-26 10:01:57,804] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 10:01:57,809] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:01:57,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:01:57,810] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:01:57,811] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:01:57,812] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:01:57,813] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:01:57,814] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:01:57,812] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:01:57,817] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:01:57,817] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:01:57,835] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run7
[2019-03-26 10:01:57,835] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run7
[2019-03-26 10:01:57,873] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run7
[2019-03-26 10:01:57,874] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run7
[2019-03-26 10:01:57,890] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run7
[2019-03-26 10:02:03,465] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.21389194], dtype=float32), 0.24719046]
[2019-03-26 10:02:03,466] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.43333333333333, 72.16666666666667, 1.0, 2.0, 0.4887070843447773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794751.4259406975, 794751.4259406975, 193622.5392612843]
[2019-03-26 10:02:03,468] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:02:03,471] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5968256e-20 1.0000000e+00 1.1027954e-20 4.2747658e-13 2.3985191e-22], sampled 0.2723880722817966
[2019-03-26 10:02:05,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.21389194], dtype=float32), 0.24719046]
[2019-03-26 10:02:05,200] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.05, 79.5, 1.0, 2.0, 0.2408018876667755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 398325.5686412163, 398325.5686412163, 159918.3715501579]
[2019-03-26 10:02:05,201] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:02:05,203] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2637957e-22 1.0000000e+00 3.9471606e-24 3.0269726e-15 1.0742262e-25], sampled 0.7354939027143921
[2019-03-26 10:02:20,448] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.21389194], dtype=float32), 0.24719046]
[2019-03-26 10:02:20,449] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.9, 49.33333333333334, 1.0, 2.0, 0.3119110576327491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494983.5254425231, 494983.5254425231, 166684.3291366654]
[2019-03-26 10:02:20,450] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:02:20,454] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.0864532e-21 1.0000000e+00 5.6556056e-22 7.2117415e-14 1.3183114e-23], sampled 0.9619531968876043
[2019-03-26 10:02:47,597] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.21389194], dtype=float32), 0.24719046]
[2019-03-26 10:02:47,598] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.9, 64.0, 1.0, 2.0, 0.5361597666163679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749217.7907667011, 749217.7907667018, 189454.8406650504]
[2019-03-26 10:02:47,599] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:02:47,602] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.3977219e-22 1.0000000e+00 4.9729025e-23 2.1377283e-13 1.7215605e-24], sampled 0.20450084686527026
[2019-03-26 10:03:01,462] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21389194], dtype=float32), 0.24719046]
[2019-03-26 10:03:01,465] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.428161445, 57.72044066166667, 1.0, 2.0, 0.9197726828799366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564847157, 1285594.46122645, 1285594.46122645, 275441.6618437517]
[2019-03-26 10:03:01,467] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:03:01,470] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5256651e-18 1.0000000e+00 2.0513070e-19 7.4966332e-11 1.6248899e-21], sampled 0.47097631044665234
[2019-03-26 10:03:24,537] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21389194], dtype=float32), 0.24719046]
[2019-03-26 10:03:24,539] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.02779448666667, 80.97870358166665, 1.0, 2.0, 0.5458722869406213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762794.7248784729, 762794.7248784729, 191094.2505301136]
[2019-03-26 10:03:24,541] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:03:24,543] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3037325e-21 1.0000000e+00 2.9938657e-22 5.2388585e-14 9.0003278e-24], sampled 0.8633360000634749
[2019-03-26 10:03:36,665] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21389194], dtype=float32), 0.24719046]
[2019-03-26 10:03:36,668] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.053991695, 54.84014062166666, 1.0, 2.0, 0.4344026240333126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669054.3499257935, 669054.3499257929, 181478.6748151546]
[2019-03-26 10:03:36,670] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:03:36,673] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7847016e-20 1.0000000e+00 1.0184222e-20 6.0440205e-13 2.2046350e-22], sampled 0.6999185080306777
[2019-03-26 10:03:47,044] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21389194], dtype=float32), 0.24719046]
[2019-03-26 10:03:47,045] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.09302255166667, 71.09896909000001, 1.0, 2.0, 0.397771921459887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594707.2141184966, 594707.2141184966, 174084.9774822434]
[2019-03-26 10:03:47,046] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:03:47,048] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.1799539e-22 1.0000000e+00 8.7663550e-23 5.3352613e-14 2.5221339e-24], sampled 0.6002959899364275
[2019-03-26 10:03:52,762] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7902.3891 3162464212.1919 1728.0000
[2019-03-26 10:03:53,106] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8262.2822 2926709096.4961 1320.0000
[2019-03-26 10:03:53,328] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8668.4927 2778274922.9665 908.0000
[2019-03-26 10:03:53,499] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8034.0535 3004359448.4618 1685.0000
[2019-03-26 10:03:53,567] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8506.9194 2841277054.7534 1098.0000
[2019-03-26 10:03:54,583] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 150000, evaluation results [150000.0, 7902.3891284941055, 3162464212.191929, 1728.0, 8262.282240950351, 2926709096.496121, 1320.0, 8668.492742754912, 2778274922.966488, 908.0, 8034.053495258048, 3004359448.461755, 1685.0, 8506.919363335419, 2841277054.7533903, 1098.0]
[2019-03-26 10:03:58,148] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 151581: loss 0.2632
[2019-03-26 10:03:58,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 151581: learning rate 0.0005
[2019-03-26 10:03:58,597] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 151789: loss 0.1541
[2019-03-26 10:03:58,601] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 151791: learning rate 0.0005
[2019-03-26 10:03:58,698] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151839: loss 0.0909
[2019-03-26 10:03:58,701] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151841: learning rate 0.0005
[2019-03-26 10:03:58,816] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 151895: loss 0.0248
[2019-03-26 10:03:58,818] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 151895: learning rate 0.0005
[2019-03-26 10:03:58,851] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 151905: loss 0.0277
[2019-03-26 10:03:58,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 151909: learning rate 0.0005
[2019-03-26 10:03:58,976] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 151970: loss 0.0026
[2019-03-26 10:03:58,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 151972: learning rate 0.0005
[2019-03-26 10:03:59,014] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 151986: loss 0.0023
[2019-03-26 10:03:59,017] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 151988: learning rate 0.0005
[2019-03-26 10:03:59,030] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 151992: loss 0.0009
[2019-03-26 10:03:59,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 151994: learning rate 0.0005
[2019-03-26 10:03:59,036] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 151995: loss 0.0012
[2019-03-26 10:03:59,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 151996: learning rate 0.0005
[2019-03-26 10:03:59,094] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 152023: loss 0.0003
[2019-03-26 10:03:59,097] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 152023: learning rate 0.0005
[2019-03-26 10:03:59,117] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 152033: loss 0.0020
[2019-03-26 10:03:59,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 152034: learning rate 0.0005
[2019-03-26 10:03:59,157] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 152054: loss 0.0060
[2019-03-26 10:03:59,159] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 152054: learning rate 0.0005
[2019-03-26 10:03:59,338] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152141: loss 0.0185
[2019-03-26 10:03:59,344] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152142: learning rate 0.0005
[2019-03-26 10:03:59,346] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 152142: loss 0.0043
[2019-03-26 10:03:59,354] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 152142: learning rate 0.0005
[2019-03-26 10:03:59,409] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 152169: loss 0.0337
[2019-03-26 10:03:59,412] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 152169: learning rate 0.0005
[2019-03-26 10:03:59,762] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 152331: loss 0.0671
[2019-03-26 10:03:59,764] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 152331: learning rate 0.0005
[2019-03-26 10:03:59,850] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2768276e-22 1.0000000e+00 2.0637048e-22 7.2870746e-13 1.8270345e-25], sum to 1.0000
[2019-03-26 10:03:59,860] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5616
[2019-03-26 10:03:59,867] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333333, 87.0, 1.0, 2.0, 0.5113840476243214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714585.1043146537, 714585.1043146544, 185398.5746482973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5708400.0000, 
sim time next is 5709000.0000, 
raw observation next is [26.41666666666666, 87.0, 1.0, 2.0, 0.5106017814643823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713491.6336039457, 713491.6336039457, 185273.4712012899], 
processed observation next is [0.0, 0.043478260869565216, 0.4510268562401261, 0.87, 1.0, 1.0, 0.4103635921257618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19819212044554047, 0.19819212044554047, 0.27652756895714914], 
reward next is 0.7235, 
noisyNet noise sample is [array([-1.4599106], dtype=float32), -0.5557215]. 
=============================================
[2019-03-26 10:03:59,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[59.798664]
 [60.01762 ]
 [60.291866]
 [60.613094]
 [60.931526]], R is [[59.66163254]
 [59.78830338]
 [59.91350937]
 [60.03721237]
 [60.15930557]].
[2019-03-26 10:04:01,944] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2491103e-24 1.0000000e+00 4.3950771e-28 7.9767687e-13 8.5314897e-31], sum to 1.0000
[2019-03-26 10:04:01,952] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8449
[2019-03-26 10:04:01,960] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 53.0, 1.0, 2.0, 0.5386016581491159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752631.2448169832, 752631.2448169838, 189866.068643478], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5749200.0000, 
sim time next is 5749800.0000, 
raw observation next is [33.8, 53.0, 1.0, 2.0, 0.5415173487612382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756707.0241387336, 756707.0241387343, 190356.8480943336], 
processed observation next is [0.0, 0.5652173913043478, 0.800947867298578, 0.53, 1.0, 1.0, 0.4476112635677569, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21019639559409267, 0.21019639559409287, 0.2841146986482591], 
reward next is 0.7159, 
noisyNet noise sample is [array([-0.17806143], dtype=float32), 1.0002224]. 
=============================================
[2019-03-26 10:04:04,962] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.0274720e-21 1.0000000e+00 4.0231951e-19 4.5412274e-18 1.8748513e-21], sum to 1.0000
[2019-03-26 10:04:04,971] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3524
[2019-03-26 10:04:04,978] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 94.0, 1.0, 2.0, 0.8235487305750202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1151026.392597042, 1151026.392597041, 249772.1937324391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5806800.0000, 
sim time next is 5807400.0000, 
raw observation next is [26.08333333333333, 93.00000000000001, 1.0, 2.0, 0.7646290267128402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1068636.295664344, 1068636.295664344, 235412.9370440474], 
processed observation next is [1.0, 0.21739130434782608, 0.43522906793048954, 0.9300000000000002, 1.0, 1.0, 0.7164205141118556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2968434154623178, 0.2968434154623178, 0.3513625926030558], 
reward next is 0.6486, 
noisyNet noise sample is [array([-1.6013515], dtype=float32), -0.844759]. 
=============================================
[2019-03-26 10:04:05,717] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3939508e-18 1.0000000e+00 1.2673631e-15 2.7137023e-15 2.0072092e-18], sum to 1.0000
[2019-03-26 10:04:05,728] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7267
[2019-03-26 10:04:05,734] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.35, 77.33333333333333, 1.0, 2.0, 0.9998214274603627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1397554.689650868, 1397554.689650869, 298873.049129988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5818200.0000, 
sim time next is 5818800.0000, 
raw observation next is [29.5, 76.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 10.0739548631696, 6.9112, 168.8949309045779, 3698673.011771567, 1455145.302622348, 306049.6748903951], 
processed observation next is [1.0, 0.34782608695652173, 0.5971563981042655, 0.7666666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.31627548631696, 0.0, 0.8293514311754059, 1.0274091699365464, 0.4042070285062078, 0.4567905595379031], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.35318258], dtype=float32), 2.680721]. 
=============================================
[2019-03-26 10:04:09,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.5670593e-14 9.9999774e-01 1.1200278e-11 1.7229405e-08 2.2294148e-06], sum to 1.0000
[2019-03-26 10:04:09,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6124
[2019-03-26 10:04:09,165] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.91666666666666, 94.83333333333333, 1.0, 2.0, 0.817234511469615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104292, 1142196.62750742, 1142196.62750742, 248184.5119213736], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5885400.0000, 
sim time next is 5886000.0000, 
raw observation next is [25.9, 95.0, 1.0, 2.0, 0.7774507243969447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1086564.913465274, 1086564.913465275, 238450.4391852209], 
processed observation next is [1.0, 0.13043478260869565, 0.42654028436018954, 0.95, 1.0, 1.0, 0.7318683426469214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3018235870736872, 0.3018235870736875, 0.3558961778883894], 
reward next is 0.6441, 
noisyNet noise sample is [array([-1.6200774], dtype=float32), -0.28477788]. 
=============================================
[2019-03-26 10:04:09,174] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[53.402836]
 [53.505592]
 [53.465374]
 [53.628376]
 [54.327595]], R is [[53.5299263 ]
 [53.62420273]
 [53.70187759]
 [53.74467087]
 [53.56860733]].
[2019-03-26 10:04:09,438] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.9926798e-23 1.0000000e+00 6.8284091e-25 6.2146596e-17 4.9473710e-18], sum to 1.0000
[2019-03-26 10:04:09,449] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9301
[2019-03-26 10:04:09,454] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.31666666666667, 86.16666666666667, 1.0, 2.0, 0.5684665359561362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794379.4362924184, 794379.4362924184, 195018.3479474342], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5861400.0000, 
sim time next is 5862000.0000, 
raw observation next is [28.23333333333333, 86.33333333333334, 1.0, 2.0, 0.5677987464327612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793445.9136206879, 793445.9136206872, 194900.128771883], 
processed observation next is [1.0, 0.8695652173913043, 0.537124802527646, 0.8633333333333334, 1.0, 1.0, 0.47927559811176046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2204016426724133, 0.2204016426724131, 0.2908957145849], 
reward next is 0.7091, 
noisyNet noise sample is [array([0.5651001], dtype=float32), -0.9391598]. 
=============================================
[2019-03-26 10:04:09,467] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[65.19858 ]
 [65.127655]
 [65.032814]
 [65.57657 ]
 [66.22478 ]], R is [[65.32108307]
 [65.37680054]
 [65.43201447]
 [65.48719025]
 [65.54239655]].
[2019-03-26 10:04:10,799] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.027951e-14 2.461573e-04 7.189820e-14 9.997539e-01 1.832566e-10], sum to 1.0000
[2019-03-26 10:04:10,806] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3279
[2019-03-26 10:04:10,814] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.6, 72.0, 1.0, 2.0, 0.8283264945169192, 1.0, 2.0, 0.8283264945169192, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2316638.458722325, 2316638.458722325, 433905.6345204205], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5913000.0000, 
sim time next is 5913600.0000, 
raw observation next is [31.76666666666667, 71.33333333333333, 1.0, 2.0, 0.850342822501703, 1.0, 2.0, 0.850342822501703, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2378271.65016695, 2378271.650166949, 445132.3281221675], 
processed observation next is [1.0, 0.43478260869565216, 0.7045813586097948, 0.7133333333333333, 1.0, 1.0, 0.8196901475924132, 1.0, 1.0, 0.8196901475924132, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6606310139352639, 0.6606310139352636, 0.6643766091375635], 
reward next is 0.3356, 
noisyNet noise sample is [array([2.033191], dtype=float32), -0.8009194]. 
=============================================
[2019-03-26 10:04:11,713] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4085305e-13 1.0000000e+00 7.1462218e-14 2.0105470e-10 1.1795945e-11], sum to 1.0000
[2019-03-26 10:04:11,722] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1626
[2019-03-26 10:04:11,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2449828.870702262 W.
[2019-03-26 10:04:11,735] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.95, 74.66666666666667, 1.0, 2.0, 0.5839352031968084, 1.0, 1.0, 0.5839352031968084, 1.0, 2.0, 1.01410233763026, 6.9112, 6.9112, 170.5573041426782, 2449828.870702262, 2449828.870702262, 478040.8778832397], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5910600.0000, 
sim time next is 5911200.0000, 
raw observation next is [31.1, 74.0, 1.0, 2.0, 0.8906893682550208, 1.0, 2.0, 0.8906893682550208, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2491226.912148772, 2491226.912148772, 466443.2022759855], 
processed observation next is [1.0, 0.43478260869565216, 0.6729857819905214, 0.74, 1.0, 1.0, 0.8683004436807479, 1.0, 1.0, 0.8683004436807479, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.692007475596881, 0.692007475596881, 0.6961838839940082], 
reward next is 0.3038, 
noisyNet noise sample is [array([1.2291933], dtype=float32), -0.48353952]. 
=============================================
[2019-03-26 10:04:15,789] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 159622: loss -20.0575
[2019-03-26 10:04:15,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 159623: learning rate 0.0005
[2019-03-26 10:04:16,209] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 159814: loss -28.7147
[2019-03-26 10:04:16,212] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 159814: learning rate 0.0005
[2019-03-26 10:04:16,231] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159824: loss -49.0837
[2019-03-26 10:04:16,236] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159824: learning rate 0.0005
[2019-03-26 10:04:16,298] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 159851: loss 0.5025
[2019-03-26 10:04:16,303] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 159851: learning rate 0.0005
[2019-03-26 10:04:16,335] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 159868: loss 0.0432
[2019-03-26 10:04:16,336] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 159868: learning rate 0.0005
[2019-03-26 10:04:16,381] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 159889: loss -168.9778
[2019-03-26 10:04:16,383] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 159889: learning rate 0.0005
[2019-03-26 10:04:16,498] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 159943: loss 0.1607
[2019-03-26 10:04:16,500] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 159943: learning rate 0.0005
[2019-03-26 10:04:16,540] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159961: loss -27.9408
[2019-03-26 10:04:16,542] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159961: learning rate 0.0005
[2019-03-26 10:04:16,654] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 160010: loss -78.5895
[2019-03-26 10:04:16,660] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 160010: learning rate 0.0005
[2019-03-26 10:04:16,705] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 160035: loss 7.6352
[2019-03-26 10:04:16,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 160037: learning rate 0.0005
[2019-03-26 10:04:16,719] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 160039: loss -176.0209
[2019-03-26 10:04:16,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 160039: learning rate 0.0005
[2019-03-26 10:04:16,799] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 160072: loss -194.8409
[2019-03-26 10:04:16,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 160072: learning rate 0.0005
[2019-03-26 10:04:16,908] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 160123: loss 0.8158
[2019-03-26 10:04:16,909] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 160124: learning rate 0.0005
[2019-03-26 10:04:16,933] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 160134: loss -169.7104
[2019-03-26 10:04:16,936] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 160134: learning rate 0.0005
[2019-03-26 10:04:16,939] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 160135: loss -88.2833
[2019-03-26 10:04:16,941] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 160136: learning rate 0.0005
[2019-03-26 10:04:17,323] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 160310: loss -24.2752
[2019-03-26 10:04:17,324] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 160311: learning rate 0.0005
[2019-03-26 10:04:26,192] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.6919608e-14 9.7362715e-01 2.7720111e-13 2.6372805e-02 3.3431341e-10], sum to 1.0000
[2019-03-26 10:04:26,199] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1357
[2019-03-26 10:04:26,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2204759.815064982 W.
[2019-03-26 10:04:26,214] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.71666666666667, 80.66666666666667, 1.0, 2.0, 0.9355242709828484, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.996766895708799, 6.9112, 168.9124474167008, 2204759.815064982, 2144055.897222486, 444447.0949245748], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6169800.0000, 
sim time next is 6170400.0000, 
raw observation next is [28.8, 80.0, 1.0, 2.0, 0.523956683296033, 1.0, 1.0, 0.523956683296033, 1.0, 2.0, 0.9099394837622441, 6.9112, 6.9112, 170.5573041426782, 2197972.875556858, 2197972.875556858, 432107.5570220418], 
processed observation next is [1.0, 0.43478260869565216, 0.5639810426540285, 0.8, 1.0, 1.0, 0.4264538352964252, 1.0, 0.5, 0.4264538352964252, 1.0, 1.0, 0.8901701021490782, 0.0, 0.0, 0.8375144448122397, 0.6105480209880161, 0.6105480209880161, 0.6449366522717042], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.45751938], dtype=float32), -0.39605954]. 
=============================================
[2019-03-26 10:04:33,008] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 167486: loss 0.0724
[2019-03-26 10:04:33,011] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 167486: learning rate 0.0005
[2019-03-26 10:04:33,284] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5836665e-24 9.9994659e-01 5.3929219e-26 5.3382173e-05 1.6566409e-21], sum to 1.0000
[2019-03-26 10:04:33,297] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9947
[2019-03-26 10:04:33,303] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.43333333333333, 63.0, 1.0, 2.0, 0.5033983038012505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703422.489804685, 703422.4898046857, 184130.3470710506], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6281400.0000, 
sim time next is 6282000.0000, 
raw observation next is [30.4, 63.0, 1.0, 2.0, 0.5024094626308534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702040.278220611, 702040.278220611, 183974.5543643206], 
processed observation next is [0.0, 0.7391304347826086, 0.6398104265402843, 0.63, 1.0, 1.0, 0.4004933284709077, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19501118839461418, 0.19501118839461418, 0.27458888711092627], 
reward next is 0.7254, 
noisyNet noise sample is [array([1.8441842], dtype=float32), -0.000855982]. 
=============================================
[2019-03-26 10:04:33,321] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[84.56258]
 [84.49496]
 [84.41513]
 [84.33081]
 [84.24116]], R is [[84.57498932]
 [84.454422  ]
 [84.33481598]
 [84.21618652]
 [84.09843445]].
[2019-03-26 10:04:33,705] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 167811: loss 0.0127
[2019-03-26 10:04:33,708] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 167812: learning rate 0.0005
[2019-03-26 10:04:33,731] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167825: loss 0.0034
[2019-03-26 10:04:33,732] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167825: learning rate 0.0005
[2019-03-26 10:04:33,767] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 167839: loss 0.0166
[2019-03-26 10:04:33,769] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 167839: learning rate 0.0005
[2019-03-26 10:04:33,795] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 167854: loss 0.0182
[2019-03-26 10:04:33,797] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 167854: learning rate 0.0005
[2019-03-26 10:04:33,813] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 167863: loss 0.0355
[2019-03-26 10:04:33,815] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 167863: learning rate 0.0005
[2019-03-26 10:04:34,028] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 167959: loss 0.0151
[2019-03-26 10:04:34,034] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 167961: learning rate 0.0005
[2019-03-26 10:04:34,064] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 167976: loss 0.0543
[2019-03-26 10:04:34,067] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 167977: learning rate 0.0005
[2019-03-26 10:04:34,220] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168046: loss 0.0023
[2019-03-26 10:04:34,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168046: learning rate 0.0005
[2019-03-26 10:04:34,263] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168064: loss 0.0004
[2019-03-26 10:04:34,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168067: learning rate 0.0005
[2019-03-26 10:04:34,364] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 168112: loss 0.0130
[2019-03-26 10:04:34,366] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 168112: loss 0.0054
[2019-03-26 10:04:34,369] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 168112: learning rate 0.0005
[2019-03-26 10:04:34,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 168113: learning rate 0.0005
[2019-03-26 10:04:34,382] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 168118: loss 0.0059
[2019-03-26 10:04:34,388] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 168119: learning rate 0.0005
[2019-03-26 10:04:34,466] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 168159: loss 0.0061
[2019-03-26 10:04:34,471] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 168161: learning rate 0.0005
[2019-03-26 10:04:34,478] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 168163: loss 0.0017
[2019-03-26 10:04:34,479] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 168164: learning rate 0.0005
[2019-03-26 10:04:35,010] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 168408: loss 0.0040
[2019-03-26 10:04:35,012] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 168408: learning rate 0.0005
[2019-03-26 10:04:37,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6043894e-26 1.0000000e+00 1.8233802e-24 4.5910133e-16 8.3571318e-23], sum to 1.0000
[2019-03-26 10:04:37,099] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9627
[2019-03-26 10:04:37,105] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 65.5, 1.0, 2.0, 0.5468106131215998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764106.4011059444, 764106.4011059437, 191255.2296954122], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6347400.0000, 
sim time next is 6348000.0000, 
raw observation next is [31.33333333333334, 65.0, 1.0, 2.0, 0.544389221929007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760721.5673139654, 760721.5673139654, 190843.4633232435], 
processed observation next is [0.0, 0.4782608695652174, 0.6840442338072673, 0.65, 1.0, 1.0, 0.4510713517216951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2113115464761015, 0.2113115464761015, 0.28484099003469177], 
reward next is 0.7152, 
noisyNet noise sample is [array([0.63590616], dtype=float32), -1.8615739]. 
=============================================
[2019-03-26 10:04:37,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.874466]
 [80.966225]
 [80.97667 ]
 [80.9249  ]
 [80.8685  ]], R is [[80.7672348 ]
 [80.67411041]
 [80.58356476]
 [80.494133  ]
 [80.40573883]].
[2019-03-26 10:04:43,214] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.16076114e-35 1.00000000e+00 9.41541272e-37 0.00000000e+00
 1.12433165e-35], sum to 1.0000
[2019-03-26 10:04:43,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2266
[2019-03-26 10:04:43,225] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 70.0, 1.0, 2.0, 0.5023734663010554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701989.9622427048, 701989.9622427048, 183969.3246355904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6458400.0000, 
sim time next is 6459000.0000, 
raw observation next is [29.2, 70.5, 1.0, 2.0, 0.5075011301951279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709157.4787359153, 709157.4787359147, 184779.8942074165], 
processed observation next is [1.0, 0.782608695652174, 0.5829383886255924, 0.705, 1.0, 1.0, 0.40662786770497333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19698818853775424, 0.19698818853775407, 0.2757908868767411], 
reward next is 0.7242, 
noisyNet noise sample is [array([-0.93024516], dtype=float32), -0.66740626]. 
=============================================
[2019-03-26 10:04:43,245] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[53.866512]
 [53.03123 ]
 [52.188503]
 [51.072826]
 [49.43958 ]], R is [[54.59213257]
 [54.77162933]
 [54.95188141]
 [55.13306427]
 [55.31450653]].
[2019-03-26 10:04:45,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1937255e-25 1.0000000e+00 4.8270364e-25 1.3332052e-23 4.1165969e-24], sum to 1.0000
[2019-03-26 10:04:45,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4644
[2019-03-26 10:04:45,566] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.31666666666667, 91.0, 1.0, 2.0, 0.6992266518543517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 977188.671049151, 977188.671049151, 220677.4762813017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6490200.0000, 
sim time next is 6490800.0000, 
raw observation next is [26.3, 91.0, 1.0, 2.0, 0.6832307646945794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954823.9250331098, 954823.9250331093, 217266.3017586991], 
processed observation next is [1.0, 0.13043478260869565, 0.4454976303317536, 0.91, 1.0, 1.0, 0.6183503189091318, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2652288680647527, 0.2652288680647526, 0.32427806232641654], 
reward next is 0.6757, 
noisyNet noise sample is [array([-0.7028742], dtype=float32), -0.5080661]. 
=============================================
[2019-03-26 10:04:49,458] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 10:04:49,462] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:04:49,463] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:04:49,465] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:04:49,466] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:04:49,466] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:04:49,467] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:04:49,468] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:04:49,470] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:04:49,472] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:04:49,472] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:04:49,481] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run8
[2019-03-26 10:04:49,498] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run8
[2019-03-26 10:04:49,514] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run8
[2019-03-26 10:04:49,532] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run8
[2019-03-26 10:04:49,533] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run8
[2019-03-26 10:05:22,725] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.25898704], dtype=float32), 0.22052254]
[2019-03-26 10:05:22,727] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.1, 92.0, 1.0, 2.0, 0.3964194203871482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594588.1959578701, 594588.1959578701, 174128.7661441955]
[2019-03-26 10:05:22,727] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:05:22,729] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9049277e-29 1.0000000e+00 6.1832827e-31 1.4071452e-25 1.4667850e-28], sampled 0.4852478217566466
[2019-03-26 10:05:38,155] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.25898704], dtype=float32), 0.22052254]
[2019-03-26 10:05:38,158] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 67.66666666666667, 1.0, 2.0, 0.5811754934602593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812145.819240161, 812145.819240161, 197291.8184000549]
[2019-03-26 10:05:38,159] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:05:38,162] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.5732079e-29 1.0000000e+00 2.8046122e-30 6.6336635e-25 4.0945844e-28], sampled 0.7465232224383228
[2019-03-26 10:06:44,830] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164139523.4469 1778.0000
[2019-03-26 10:06:44,866] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-26 10:06:44,867] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 10:06:45,134] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-26 10:06:45,157] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0142 3007608811.4011 1766.0000
[2019-03-26 10:06:46,171] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 175000, evaluation results [175000.0, 7883.41881391125, 3164139523.4469013, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.014205657292, 3007608811.4011106, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 10:06:46,477] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8810710e-27 1.0000000e+00 1.1659624e-27 8.5358865e-23 1.7757327e-26], sum to 1.0000
[2019-03-26 10:06:46,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7766
[2019-03-26 10:06:46,497] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 89.5, 1.0, 2.0, 0.5175758127689657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723240.1443091163, 723240.1443091156, 186395.4929988685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6571800.0000, 
sim time next is 6572400.0000, 
raw observation next is [26.3, 89.66666666666667, 1.0, 2.0, 0.5165169162891372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721759.9812020903, 721759.9812020903, 186224.2413698407], 
processed observation next is [1.0, 0.043478260869565216, 0.4454976303317536, 0.8966666666666667, 1.0, 1.0, 0.4174902605893219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2004888836672473, 0.2004888836672473, 0.27794662891021], 
reward next is 0.7221, 
noisyNet noise sample is [array([1.3161917], dtype=float32), -1.5126632]. 
=============================================
[2019-03-26 10:06:47,357] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 175481: loss -214.6113
[2019-03-26 10:06:47,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 175482: learning rate 0.0005
[2019-03-26 10:06:47,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6483837e-27 1.0000000e+00 8.3080729e-26 1.2846313e-25 4.6758478e-25], sum to 1.0000
[2019-03-26 10:06:47,679] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9579
[2019-03-26 10:06:47,684] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 88.0, 1.0, 2.0, 0.6854201674262496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 957885.0238343929, 957885.0238343923, 217728.3920143394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6591600.0000, 
sim time next is 6592200.0000, 
raw observation next is [26.66666666666667, 87.50000000000001, 1.0, 2.0, 0.658464865065084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 920198.2623011803, 920198.262301181, 212134.1025921805], 
processed observation next is [1.0, 0.30434782608695654, 0.4628751974723541, 0.8750000000000001, 1.0, 1.0, 0.5885118856205831, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2556106284169945, 0.2556106284169947, 0.31661806357041866], 
reward next is 0.6834, 
noisyNet noise sample is [array([0.5116678], dtype=float32), 0.8857565]. 
=============================================
[2019-03-26 10:06:48,045] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175806: loss -256.9209
[2019-03-26 10:06:48,047] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175806: learning rate 0.0005
[2019-03-26 10:06:48,138] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 175849: loss -308.6794
[2019-03-26 10:06:48,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 175849: learning rate 0.0005
[2019-03-26 10:06:48,146] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 175852: loss -198.7897
[2019-03-26 10:06:48,149] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 175852: learning rate 0.0005
[2019-03-26 10:06:48,206] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 175877: loss -218.5089
[2019-03-26 10:06:48,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 175879: learning rate 0.0005
[2019-03-26 10:06:48,225] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 175888: loss -286.2228
[2019-03-26 10:06:48,226] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 175888: learning rate 0.0005
[2019-03-26 10:06:48,325] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 175933: loss -208.2518
[2019-03-26 10:06:48,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 175934: learning rate 0.0005
[2019-03-26 10:06:48,468] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 175996: loss -245.0107
[2019-03-26 10:06:48,471] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 175996: learning rate 0.0005
[2019-03-26 10:06:48,575] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 176047: loss -250.0041
[2019-03-26 10:06:48,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 176047: learning rate 0.0005
[2019-03-26 10:06:48,592] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1711295e-26 1.0000000e+00 7.3258381e-26 4.3800602e-22 6.9618464e-23], sum to 1.0000
[2019-03-26 10:06:48,604] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5474
[2019-03-26 10:06:48,609] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 176059: loss -270.0879
[2019-03-26 10:06:48,610] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 176059: learning rate 0.0005
[2019-03-26 10:06:48,612] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 90.0, 1.0, 2.0, 0.6755937162824901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 944146.3058865666, 944146.3058865673, 215663.5895416749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6588000.0000, 
sim time next is 6588600.0000, 
raw observation next is [26.18333333333334, 89.66666666666667, 1.0, 2.0, 0.6752231728762547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 943628.2391805528, 943628.2391805528, 215586.3919657916], 
processed observation next is [1.0, 0.2608695652173913, 0.4399684044233811, 0.8966666666666667, 1.0, 1.0, 0.6087026179231984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26211895532793134, 0.26211895532793134, 0.3217707342773009], 
reward next is 0.6782, 
noisyNet noise sample is [array([-1.6131431], dtype=float32), 0.6283131]. 
=============================================
[2019-03-26 10:06:48,630] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 176068: loss -254.1707
[2019-03-26 10:06:48,632] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 176068: learning rate 0.0005
[2019-03-26 10:06:48,669] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 176084: loss -194.6785
[2019-03-26 10:06:48,674] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 176088: learning rate 0.0005
[2019-03-26 10:06:48,681] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 176090: loss -220.6965
[2019-03-26 10:06:48,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 176091: learning rate 0.0005
[2019-03-26 10:06:48,739] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 176120: loss -125.0991
[2019-03-26 10:06:48,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 176120: learning rate 0.0005
[2019-03-26 10:06:48,778] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176137: loss -162.2077
[2019-03-26 10:06:48,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176137: learning rate 0.0005
[2019-03-26 10:06:49,190] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 176324: loss -245.5694
[2019-03-26 10:06:49,192] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 176324: learning rate 0.0005
[2019-03-26 10:06:50,191] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3523131e-29 1.0000000e+00 7.5546430e-32 2.5288594e-38 1.2472986e-33], sum to 1.0000
[2019-03-26 10:06:50,198] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4988
[2019-03-26 10:06:50,210] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2231690.050849135 W.
[2019-03-26 10:06:50,215] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.91666666666667, 84.16666666666667, 1.0, 2.0, 0.5319865542795568, 1.0, 2.0, 0.5319865542795568, 1.0, 1.0, 0.9210972644012091, 6.911199999999999, 6.9112, 170.5573041426782, 2231690.050849135, 2231690.050849136, 437431.5683097912], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6624600.0000, 
sim time next is 6625200.0000, 
raw observation next is [27.83333333333334, 84.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.102340092646784, 6.9112, 168.9118695558929, 2428942.484430566, 2293341.996767135, 476142.4250245894], 
processed observation next is [1.0, 0.6956521739130435, 0.5181674565560824, 0.8433333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.019114009264678433, 0.0, 0.8294346077079107, 0.6747062456751572, 0.6370394435464264, 0.7106603358575961], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.0188518], dtype=float32), -0.96956265]. 
=============================================
[2019-03-26 10:06:53,280] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.0290266e-20 1.0000000e+00 9.6653599e-20 2.2682927e-14 5.1859675e-18], sum to 1.0000
[2019-03-26 10:06:53,291] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8071
[2019-03-26 10:06:53,301] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1876083.197863409 W.
[2019-03-26 10:06:53,310] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.86666666666667, 80.0, 1.0, 2.0, 0.6709367699179095, 1.0, 2.0, 0.6709367699179095, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1876083.197863409, 1876083.197863409, 361926.2321342725], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6687600.0000, 
sim time next is 6688200.0000, 
raw observation next is [28.03333333333333, 79.0, 1.0, 2.0, 0.6745032314647159, 1.0, 2.0, 0.6745032314647159, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1886064.567212295, 1886064.567212295, 363395.888784241], 
processed observation next is [1.0, 0.391304347826087, 0.5276461295418641, 0.79, 1.0, 1.0, 0.6078352186321878, 1.0, 1.0, 0.6078352186321878, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5239068242256375, 0.5239068242256375, 0.5423819235585686], 
reward next is 0.4576, 
noisyNet noise sample is [array([-0.19108169], dtype=float32), -0.5520498]. 
=============================================
[2019-03-26 10:06:56,822] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.8775300e-25 1.0000000e+00 9.8512253e-26 1.0841484e-16 1.2957296e-25], sum to 1.0000
[2019-03-26 10:06:56,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4613
[2019-03-26 10:06:56,836] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.05, 84.16666666666667, 1.0, 2.0, 0.3684131104848796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584851.8768985935, 584851.8768985941, 173878.5036876616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6750600.0000, 
sim time next is 6751200.0000, 
raw observation next is [22.0, 84.33333333333334, 1.0, 2.0, 0.3452182871015645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548351.6535439709, 548351.6535439703, 170826.0941097662], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.8433333333333334, 1.0, 1.0, 0.21110637000188492, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15231990376221413, 0.15231990376221397, 0.25496431956681526], 
reward next is 0.7450, 
noisyNet noise sample is [array([0.5593134], dtype=float32), -1.8313321]. 
=============================================
[2019-03-26 10:07:04,521] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 183473: loss 0.2022
[2019-03-26 10:07:04,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 183473: learning rate 0.0005
[2019-03-26 10:07:05,100] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183741: loss 0.0286
[2019-03-26 10:07:05,102] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183742: learning rate 0.0005
[2019-03-26 10:07:05,229] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 183801: loss 0.0388
[2019-03-26 10:07:05,233] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 183802: learning rate 0.0005
[2019-03-26 10:07:05,297] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 183831: loss 0.0587
[2019-03-26 10:07:05,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 183833: learning rate 0.0005
[2019-03-26 10:07:05,356] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 183861: loss 0.0865
[2019-03-26 10:07:05,359] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 183863: learning rate 0.0005
[2019-03-26 10:07:05,426] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 183890: loss 0.0827
[2019-03-26 10:07:05,429] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 183890: learning rate 0.0005
[2019-03-26 10:07:05,499] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 183922: loss 0.0536
[2019-03-26 10:07:05,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 183923: learning rate 0.0005
[2019-03-26 10:07:05,614] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 183975: loss 0.0192
[2019-03-26 10:07:05,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 183975: learning rate 0.0005
[2019-03-26 10:07:05,760] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 184048: loss 0.0099
[2019-03-26 10:07:05,767] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 184049: learning rate 0.0005
[2019-03-26 10:07:05,829] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184075: loss 0.0015
[2019-03-26 10:07:05,832] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184077: learning rate 0.0005
[2019-03-26 10:07:05,914] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 184116: loss 0.0036
[2019-03-26 10:07:05,919] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 184119: learning rate 0.0005
[2019-03-26 10:07:05,927] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 184124: loss 0.0091
[2019-03-26 10:07:05,929] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 184124: learning rate 0.0005
[2019-03-26 10:07:06,029] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184165: loss 0.0137
[2019-03-26 10:07:06,032] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184165: learning rate 0.0005
[2019-03-26 10:07:06,045] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 184173: loss 0.0144
[2019-03-26 10:07:06,046] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 184173: learning rate 0.0005
[2019-03-26 10:07:06,101] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 184202: loss 0.0035
[2019-03-26 10:07:06,101] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 184202: learning rate 0.0005
[2019-03-26 10:07:06,178] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.3270620e-33 1.0000000e+00 9.7960189e-35 4.6681845e-25 4.9689707e-33], sum to 1.0000
[2019-03-26 10:07:06,192] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8035
[2019-03-26 10:07:06,200] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 59.0, 1.0, 2.0, 0.3639927159840001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 556564.8138261623, 556564.8138261623, 171082.7016008015], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6894000.0000, 
sim time next is 6894600.0000, 
raw observation next is [27.46666666666667, 59.83333333333334, 1.0, 2.0, 0.3651673809196034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557958.4867546587, 557958.4867546587, 171189.7000053049], 
processed observation next is [0.0, 0.8260869565217391, 0.500789889415482, 0.5983333333333334, 1.0, 1.0, 0.23514142279470288, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15498846854296075, 0.15498846854296075, 0.2555070149332909], 
reward next is 0.7445, 
noisyNet noise sample is [array([-0.34548935], dtype=float32), 2.5351844]. 
=============================================
[2019-03-26 10:07:06,503] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 184388: loss 0.0280
[2019-03-26 10:07:06,504] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 184389: learning rate 0.0005
[2019-03-26 10:07:12,753] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.5806882e-23 1.0000000e+00 6.2693889e-21 5.0758002e-09 1.6710924e-22], sum to 1.0000
[2019-03-26 10:07:12,760] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3485
[2019-03-26 10:07:12,765] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 85.66666666666667, 1.0, 2.0, 0.6654128531676959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 947290.1127677842, 947290.1127677842, 215851.9789133081], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7012200.0000, 
sim time next is 7012800.0000, 
raw observation next is [25.3, 86.0, 1.0, 2.0, 0.6550394125009066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 931995.9920461454, 931995.9920461461, 213622.7756947928], 
processed observation next is [1.0, 0.17391304347826086, 0.39810426540284366, 0.86, 1.0, 1.0, 0.5843848343384417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2588877755683737, 0.2588877755683739, 0.3188399637235713], 
reward next is 0.6812, 
noisyNet noise sample is [array([0.12977508], dtype=float32), -1.1852226]. 
=============================================
[2019-03-26 10:07:19,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.6872597e-29 1.0000000e+00 2.2649626e-30 1.6102424e-27 2.8116943e-29], sum to 1.0000
[2019-03-26 10:07:19,448] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7706
[2019-03-26 10:07:19,451] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.55, 73.5, 1.0, 2.0, 0.9750756980177285, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956503923, 1364332.979866603, 1364332.979866602, 291635.8903541163], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7129800.0000, 
sim time next is 7130400.0000, 
raw observation next is [27.43333333333333, 74.33333333333333, 1.0, 2.0, 0.9691412035308741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104294, 1357649.183645909, 1357649.183645909, 290126.6359008661], 
processed observation next is [1.0, 0.5217391304347826, 0.49921011058451803, 0.7433333333333333, 1.0, 1.0, 0.9628207271456314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451522949, 0.37712477323497473, 0.37712477323497473, 0.4330248297027853], 
reward next is 0.5670, 
noisyNet noise sample is [array([-0.67073166], dtype=float32), -0.69750315]. 
=============================================
[2019-03-26 10:07:19,564] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9483694e-24 1.0000000e+00 6.5420036e-28 1.0069628e-23 4.5118268e-23], sum to 1.0000
[2019-03-26 10:07:19,574] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4680
[2019-03-26 10:07:19,580] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1670480.397254095 W.
[2019-03-26 10:07:19,590] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.26666666666667, 83.66666666666667, 1.0, 2.0, 0.5974605322929204, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9993353003935839, 6.911199999999999, 6.9112, 168.91294960539, 1670480.397254095, 1670480.397254096, 357424.7386566325], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7141200.0000, 
sim time next is 7141800.0000, 
raw observation next is [26.23333333333333, 83.83333333333333, 1.0, 2.0, 0.4023203186807527, 1.0, 1.0, 0.4023203186807527, 1.0, 2.0, 0.6780513652983375, 6.9112, 6.9112, 170.5573041426782, 1687312.419947088, 1687312.419947088, 351200.2966930377], 
processed observation next is [1.0, 0.6521739130434783, 0.44233807266982617, 0.8383333333333333, 1.0, 1.0, 0.2799039984105454, 1.0, 0.5, 0.2799039984105454, 1.0, 1.0, 0.6073797137784603, 0.0, 0.0, 0.8375144448122397, 0.4686978944297467, 0.4686978944297467, 0.5241795473030413], 
reward next is 0.4758, 
noisyNet noise sample is [array([-0.13912678], dtype=float32), 0.76362264]. 
=============================================
[2019-03-26 10:07:19,816] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.771028e-22 1.000000e+00 8.971901e-26 6.331108e-28 3.365333e-24], sum to 1.0000
[2019-03-26 10:07:19,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3230
[2019-03-26 10:07:19,835] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1713559.949109075 W.
[2019-03-26 10:07:19,838] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.13333333333333, 85.33333333333333, 1.0, 2.0, 0.6128606123931729, 1.0, 2.0, 0.6128606123931729, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1713559.949109075, 1713559.949109074, 339040.5275589964], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7144800.0000, 
sim time next is 7145400.0000, 
raw observation next is [26.11666666666667, 85.66666666666667, 1.0, 2.0, 0.6040258978214207, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0131595867921, 6.911199999999999, 6.9112, 168.912956510431, 1688851.553278353, 1688851.553278354, 362180.4829150556], 
processed observation next is [1.0, 0.6956521739130435, 0.43680884676145365, 0.8566666666666667, 1.0, 1.0, 0.522922768459543, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0160482765757317, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.46912543146620916, 0.46912543146620944, 0.5405678849478441], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5402918], dtype=float32), 0.36768898]. 
=============================================
[2019-03-26 10:07:22,216] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 191489: loss 0.0358
[2019-03-26 10:07:22,217] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 191489: learning rate 0.0005
[2019-03-26 10:07:22,637] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191684: loss 0.0215
[2019-03-26 10:07:22,641] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191685: learning rate 0.0005
[2019-03-26 10:07:22,968] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 191848: loss 0.0870
[2019-03-26 10:07:22,971] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 191848: learning rate 0.0005
[2019-03-26 10:07:22,988] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 191856: loss 0.0820
[2019-03-26 10:07:22,993] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 191856: learning rate 0.0005
[2019-03-26 10:07:23,020] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 191870: loss 0.0373
[2019-03-26 10:07:23,022] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 191870: learning rate 0.0005
[2019-03-26 10:07:23,106] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 191911: loss 0.0502
[2019-03-26 10:07:23,110] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 191911: learning rate 0.0005
[2019-03-26 10:07:23,173] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191939: loss 0.0523
[2019-03-26 10:07:23,176] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191940: learning rate 0.0005
[2019-03-26 10:07:23,241] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 191976: loss 0.0290
[2019-03-26 10:07:23,243] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 191976: learning rate 0.0005
[2019-03-26 10:07:23,254] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 191979: loss 0.0364
[2019-03-26 10:07:23,256] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 191980: learning rate 0.0005
[2019-03-26 10:07:23,410] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192054: loss 0.0167
[2019-03-26 10:07:23,412] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192054: learning rate 0.0005
[2019-03-26 10:07:23,414] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 192054: loss 0.0163
[2019-03-26 10:07:23,417] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 192054: learning rate 0.0005
[2019-03-26 10:07:23,469] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.1388253e-22 1.0000000e+00 6.4317048e-24 8.3477961e-23 3.6600420e-21], sum to 1.0000
[2019-03-26 10:07:23,477] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9120
[2019-03-26 10:07:23,487] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 90.83333333333334, 1.0, 2.0, 0.493529391876577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689627.7115367845, 689627.7115367845, 182588.9488568095], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7188600.0000, 
sim time next is 7189200.0000, 
raw observation next is [25.8, 91.0, 1.0, 2.0, 0.4943361380950353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 690755.3757548734, 690755.375754874, 182713.9048441586], 
processed observation next is [1.0, 0.21739130434782608, 0.42180094786729866, 0.91, 1.0, 1.0, 0.39076643143980155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19187649326524261, 0.19187649326524278, 0.27270732066292325], 
reward next is 0.7273, 
noisyNet noise sample is [array([-0.5079782], dtype=float32), -0.94602054]. 
=============================================
[2019-03-26 10:07:23,556] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 192121: loss 0.0175
[2019-03-26 10:07:23,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 192121: learning rate 0.0005
[2019-03-26 10:07:23,572] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 192128: loss 0.0163
[2019-03-26 10:07:23,576] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 192129: learning rate 0.0005
[2019-03-26 10:07:23,592] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192133: loss 0.0197
[2019-03-26 10:07:23,596] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192133: learning rate 0.0005
[2019-03-26 10:07:23,686] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 192177: loss 0.0199
[2019-03-26 10:07:23,688] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 192177: learning rate 0.0005
[2019-03-26 10:07:23,956] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.5059615e-14 9.9921978e-01 4.9865122e-15 7.8024442e-04 3.0792997e-11], sum to 1.0000
[2019-03-26 10:07:23,965] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7111
[2019-03-26 10:07:23,972] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1914783.608200587 W.
[2019-03-26 10:07:23,976] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.6847646998016945, 1.0, 2.0, 0.6847646998016945, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1914783.608200587, 1914783.608200587, 367677.825894677], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7203600.0000, 
sim time next is 7204200.0000, 
raw observation next is [29.0, 83.16666666666667, 1.0, 2.0, 0.4886330277672241, 1.0, 2.0, 0.4886330277672241, 1.0, 1.0, 0.8485939758200934, 6.9112, 6.9112, 170.5573041426782, 2049650.086659309, 2049650.086659309, 407374.6637433796], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.8316666666666667, 1.0, 1.0, 0.3838952141773785, 1.0, 1.0, 0.3838952141773785, 1.0, 0.5, 0.8153585070976748, 0.0, 0.0, 0.8375144448122397, 0.5693472462942525, 0.5693472462942525, 0.6080218861841487], 
reward next is 0.3920, 
noisyNet noise sample is [array([-1.1655538], dtype=float32), -0.099515446]. 
=============================================
[2019-03-26 10:07:24,061] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 192342: loss 0.0166
[2019-03-26 10:07:24,063] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 192343: learning rate 0.0005
[2019-03-26 10:07:32,903] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5288971e-31 1.0000000e+00 4.9955077e-31 1.3400747e-21 1.9378973e-27], sum to 1.0000
[2019-03-26 10:07:32,913] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3425
[2019-03-26 10:07:32,917] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.85, 86.0, 1.0, 2.0, 0.425430782804455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 639351.2937903373, 639351.2937903379, 178353.7032204502], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7360200.0000, 
sim time next is 7360800.0000, 
raw observation next is [23.76666666666667, 87.0, 1.0, 2.0, 0.4187632833010237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 628245.6697787711, 628245.6697787705, 177263.7214131462], 
processed observation next is [1.0, 0.17391304347826086, 0.32543443917851517, 0.87, 1.0, 1.0, 0.29971479915785987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17451268604965864, 0.17451268604965847, 0.2645727185270839], 
reward next is 0.7354, 
noisyNet noise sample is [array([-0.6831768], dtype=float32), 0.889204]. 
=============================================
[2019-03-26 10:07:38,829] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6512729e-27 1.0000000e+00 1.0120005e-29 3.3014118e-17 2.0253461e-25], sum to 1.0000
[2019-03-26 10:07:38,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2044
[2019-03-26 10:07:38,844] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 86.33333333333334, 1.0, 2.0, 0.3743119112862442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 564499.4048139412, 564499.4048139419, 171526.7816347643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7471200.0000, 
sim time next is 7471800.0000, 
raw observation next is [23.8, 86.0, 1.0, 2.0, 0.3763085841236261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566504.5759045857, 566504.5759045857, 171669.7718851236], 
processed observation next is [0.0, 0.4782608695652174, 0.3270142180094788, 0.86, 1.0, 1.0, 0.2485645591850917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15736238219571824, 0.15736238219571824, 0.2562235401270501], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.4867809], dtype=float32), 0.5276553]. 
=============================================
[2019-03-26 10:07:39,554] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6918402e-32 1.0000000e+00 1.4784602e-33 4.9097667e-19 2.4872876e-32], sum to 1.0000
[2019-03-26 10:07:39,561] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3978
[2019-03-26 10:07:39,568] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 85.0, 1.0, 2.0, 0.3838705267360344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 574791.6780117674, 574791.6780117681, 172304.2200867445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7473600.0000, 
sim time next is 7474200.0000, 
raw observation next is [24.21666666666667, 84.50000000000001, 1.0, 2.0, 0.386239480210638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 577380.7410647481, 577380.7410647476, 172505.4138475132], 
processed observation next is [0.0, 0.5217391304347826, 0.34676145339652464, 0.8450000000000002, 1.0, 1.0, 0.2605294942296843, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16038353918465226, 0.1603835391846521, 0.2574707669365869], 
reward next is 0.7425, 
noisyNet noise sample is [array([-1.3868092], dtype=float32), -0.48632148]. 
=============================================
[2019-03-26 10:07:39,580] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 199464: loss 0.0025
[2019-03-26 10:07:39,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 199465: learning rate 0.0005
[2019-03-26 10:07:40,130] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199723: loss 0.0613
[2019-03-26 10:07:40,132] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199723: learning rate 0.0005
[2019-03-26 10:07:40,398] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 199841: loss 0.0151
[2019-03-26 10:07:40,399] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 199841: learning rate 0.0005
[2019-03-26 10:07:40,479] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 199881: loss 0.0171
[2019-03-26 10:07:40,482] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 199882: learning rate 0.0005
[2019-03-26 10:07:40,494] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 199885: loss 0.0231
[2019-03-26 10:07:40,496] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 199885: learning rate 0.0005
[2019-03-26 10:07:40,524] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 199898: loss 0.0114
[2019-03-26 10:07:40,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 199899: learning rate 0.0005
[2019-03-26 10:07:40,640] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 199953: loss 0.0041
[2019-03-26 10:07:40,644] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 199953: learning rate 0.0005
[2019-03-26 10:07:40,709] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199983: loss 0.0517
[2019-03-26 10:07:40,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199983: learning rate 0.0005
[2019-03-26 10:07:40,720] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 199986: loss 0.0451
[2019-03-26 10:07:40,722] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 199987: learning rate 0.0005
[2019-03-26 10:07:40,748] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 10:07:40,748] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:07:40,749] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:07:40,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:07:40,750] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:07:40,752] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:07:40,751] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:07:40,754] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:07:40,754] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:07:40,756] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:07:40,757] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:07:40,771] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run9
[2019-03-26 10:07:40,772] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run9
[2019-03-26 10:07:40,791] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run9
[2019-03-26 10:07:40,827] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run9
[2019-03-26 10:07:40,846] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run9
[2019-03-26 10:08:47,131] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.29497474], dtype=float32), 0.19277906]
[2019-03-26 10:08:47,134] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.77589833333333, 91.89567978, 1.0, 2.0, 0.5405627880897795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755372.6631985046, 755372.6631985052, 190195.6404391099]
[2019-03-26 10:08:47,135] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:08:47,138] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.00055973e-32 1.00000000e+00 1.84048494e-34 1.33370883e-23
 1.04687676e-29], sampled 0.9567033356512887
[2019-03-26 10:09:33,940] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.29497474], dtype=float32), 0.19277906]
[2019-03-26 10:09:33,943] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.88343863, 79.58276245333333, 1.0, 2.0, 0.4949133645681651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691562.2196139235, 691562.219613924, 182802.1147869486]
[2019-03-26 10:09:33,945] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:09:33,948] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3248466e-31 1.0000000e+00 6.1136175e-33 1.9086805e-22 1.4335286e-28], sampled 0.10898590629740246
[2019-03-26 10:09:35,413] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-26 10:09:36,016] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-26 10:09:36,228] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1685 3164067279.4129 1778.0000
[2019-03-26 10:09:36,298] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4596 3007660329.2749 1766.0000
[2019-03-26 10:09:36,523] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6764 2842629191.6753 1131.0000
[2019-03-26 10:09:37,538] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 200000, evaluation results [200000.0, 7884.168455680242, 3164067279.4128904, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7997.459627403906, 3007660329.2748713, 1766.0, 8496.676437139186, 2842629191.6752853, 1131.0]
[2019-03-26 10:09:37,699] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 200065: loss 0.0172
[2019-03-26 10:09:37,709] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 200069: learning rate 0.0005
[2019-03-26 10:09:37,748] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200083: loss 0.0068
[2019-03-26 10:09:37,749] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200084: learning rate 0.0005
[2019-03-26 10:09:37,829] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 200118: loss 0.0148
[2019-03-26 10:09:37,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 200118: learning rate 0.0005
[2019-03-26 10:09:37,866] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 200132: loss 0.0051
[2019-03-26 10:09:37,867] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 200132: learning rate 0.0005
[2019-03-26 10:09:37,905] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 200147: loss 0.0012
[2019-03-26 10:09:37,909] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 200147: learning rate 0.0005
[2019-03-26 10:09:38,118] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 200240: loss 0.0115
[2019-03-26 10:09:38,123] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 200240: learning rate 0.0005
[2019-03-26 10:09:38,508] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 200418: loss 0.2005
[2019-03-26 10:09:38,511] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 200420: learning rate 0.0005
[2019-03-26 10:09:41,562] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.3544373e-31 1.0000000e+00 2.6190543e-32 8.1303785e-23 2.8918303e-30], sum to 1.0000
[2019-03-26 10:09:41,569] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8009
[2019-03-26 10:09:41,573] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 64.0, 1.0, 2.0, 0.4519709058075205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 638945.6299543233, 638945.6299543233, 177370.9684343898], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7562400.0000, 
sim time next is 7563000.0000, 
raw observation next is [29.0, 63.5, 1.0, 2.0, 0.4477038366473571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635271.9331184266, 635271.9331184266, 177061.2354674151], 
processed observation next is [0.0, 0.5217391304347826, 0.5734597156398105, 0.635, 1.0, 1.0, 0.3345829357197074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1764644258662296, 0.1764644258662296, 0.26427050069763447], 
reward next is 0.7357, 
noisyNet noise sample is [array([-0.5937506], dtype=float32), -0.069302395]. 
=============================================
[2019-03-26 10:09:41,589] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.44395 ]
 [83.29465 ]
 [83.152885]
 [83.02117 ]
 [82.89981 ]], R is [[83.494133  ]
 [83.39446259]
 [83.29527283]
 [83.19657135]
 [83.09837341]].
[2019-03-26 10:09:53,539] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 207403: loss 0.0755
[2019-03-26 10:09:53,542] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 207403: learning rate 0.0005
[2019-03-26 10:09:54,310] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207769: loss 0.0234
[2019-03-26 10:09:54,314] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207769: learning rate 0.0005
[2019-03-26 10:09:54,442] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 207833: loss 0.0237
[2019-03-26 10:09:54,445] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 207833: learning rate 0.0005
[2019-03-26 10:09:54,497] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 207857: loss 0.0232
[2019-03-26 10:09:54,502] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 207857: learning rate 0.0005
[2019-03-26 10:09:54,532] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 207869: loss 0.0234
[2019-03-26 10:09:54,536] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 207871: learning rate 0.0005
[2019-03-26 10:09:54,551] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 207880: loss 0.0237
[2019-03-26 10:09:54,553] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 207880: learning rate 0.0005
[2019-03-26 10:09:54,680] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207938: loss 0.0261
[2019-03-26 10:09:54,682] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207938: learning rate 0.0005
[2019-03-26 10:09:54,699] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 207947: loss 0.0260
[2019-03-26 10:09:54,701] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 207948: learning rate 0.0005
[2019-03-26 10:09:54,750] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 207970: loss 0.0293
[2019-03-26 10:09:54,754] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 207971: learning rate 0.0005
[2019-03-26 10:09:54,840] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2784699e-28 1.0000000e+00 7.7423170e-27 5.8751727e-25 9.7226535e-25], sum to 1.0000
[2019-03-26 10:09:54,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8633
[2019-03-26 10:09:54,854] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 82.0, 1.0, 2.0, 0.7750000565356212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1083138.113945953, 1083138.113945952, 237866.0188770856], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7804800.0000, 
sim time next is 7805400.0000, 
raw observation next is [27.8, 81.50000000000001, 1.0, 2.0, 0.7127881500550555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 996150.1304022573, 996150.1304022567, 223629.0965376089], 
processed observation next is [1.0, 0.34782608695652173, 0.5165876777251186, 0.8150000000000002, 1.0, 1.0, 0.653961626572356, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2767083695561826, 0.27670836955618244, 0.33377477095165503], 
reward next is 0.6662, 
noisyNet noise sample is [array([0.2743685], dtype=float32), 0.14680794]. 
=============================================
[2019-03-26 10:09:54,976] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 208076: loss 0.0256
[2019-03-26 10:09:54,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 208079: learning rate 0.0005
[2019-03-26 10:09:55,058] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 208116: loss 0.0232
[2019-03-26 10:09:55,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 208119: learning rate 0.0005
[2019-03-26 10:09:55,101] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 208132: loss 0.0228
[2019-03-26 10:09:55,105] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 208134: learning rate 0.0005
[2019-03-26 10:09:55,118] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 208138: loss 0.0252
[2019-03-26 10:09:55,120] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 208139: learning rate 0.0005
[2019-03-26 10:09:55,139] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208148: loss 0.0256
[2019-03-26 10:09:55,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208148: learning rate 0.0005
[2019-03-26 10:09:55,292] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 208218: loss 0.0178
[2019-03-26 10:09:55,295] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 208218: learning rate 0.0005
[2019-03-26 10:09:55,320] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2019739e-34 1.0000000e+00 2.6981694e-36 0.0000000e+00 8.1841577e-36], sum to 1.0000
[2019-03-26 10:09:55,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9373
[2019-03-26 10:09:55,340] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2103702.589923292 W.
[2019-03-26 10:09:55,345] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.53333333333333, 68.66666666666667, 1.0, 2.0, 0.7522596156669322, 1.0, 2.0, 0.7522596156669322, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2103702.589923292, 2103702.589923292, 397298.0080369466], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7827600.0000, 
sim time next is 7828200.0000, 
raw observation next is [30.65, 69.0, 1.0, 2.0, 0.7492370712200662, 1.0, 2.0, 0.7492370712200662, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2095241.74352039, 2095241.74352039, 395915.1665330455], 
processed observation next is [1.0, 0.6086956521739131, 0.6516587677725119, 0.69, 1.0, 1.0, 0.6978759894217665, 1.0, 1.0, 0.6978759894217665, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5820115954223306, 0.5820115954223306, 0.5909181590045456], 
reward next is 0.4091, 
noisyNet noise sample is [array([0.76538384], dtype=float32), 0.08818675]. 
=============================================
[2019-03-26 10:09:55,609] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 208368: loss 0.1596
[2019-03-26 10:09:55,611] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 208368: learning rate 0.0005
[2019-03-26 10:09:56,103] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4390799e-22 1.0000000e+00 9.6897047e-24 8.7184381e-29 1.6895446e-24], sum to 1.0000
[2019-03-26 10:09:56,114] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1715
[2019-03-26 10:09:56,120] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2132953.622670835 W.
[2019-03-26 10:09:56,128] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.06666666666667, 71.33333333333334, 1.0, 2.0, 0.7627090418962753, 1.0, 2.0, 0.7627090418962753, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2132953.622670835, 2132953.622670836, 402124.1868329745], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7816800.0000, 
sim time next is 7817400.0000, 
raw observation next is [30.3, 71.0, 1.0, 2.0, 0.5244681735242198, 1.0, 2.0, 0.5244681735242198, 1.0, 1.0, 0.9080058021762747, 6.9112, 6.9112, 170.5573041426782, 2200120.755916861, 2200120.755916861, 431942.2029152848], 
processed observation next is [1.0, 0.4782608695652174, 0.6350710900473934, 0.71, 1.0, 1.0, 0.42707008858339734, 1.0, 1.0, 0.42707008858339734, 1.0, 0.5, 0.8878119538735058, 0.0, 0.0, 0.8375144448122397, 0.6111446544213502, 0.6111446544213502, 0.6446898550974399], 
reward next is 0.3553, 
noisyNet noise sample is [array([0.5642698], dtype=float32), 0.33596015]. 
=============================================
[2019-03-26 10:10:02,983] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.7440379e-34 1.0000000e+00 1.6574581e-34 6.6010223e-33 2.2702468e-31], sum to 1.0000
[2019-03-26 10:10:02,989] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1713
[2019-03-26 10:10:02,994] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.98333333333333, 87.5, 1.0, 2.0, 0.5277780746609507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737501.3339908647, 737501.3339908654, 188062.8238960693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7941000.0000, 
sim time next is 7941600.0000, 
raw observation next is [26.9, 88.0, 1.0, 2.0, 0.5264384626855924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735628.7519998525, 735628.7519998525, 187842.2282019271], 
processed observation next is [1.0, 0.9565217391304348, 0.4739336492890995, 0.88, 1.0, 1.0, 0.4294439309464968, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20434131999995903, 0.20434131999995903, 0.28036153462974195], 
reward next is 0.7196, 
noisyNet noise sample is [array([0.80610865], dtype=float32), 0.7051922]. 
=============================================
[2019-03-26 10:10:03,429] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:03,431] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:03,455] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-03-26 10:10:04,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-03-26 10:10:04,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,191] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-03-26 10:10:04,217] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,217] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,218] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-03-26 10:10:04,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,249] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,250] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-03-26 10:10:04,321] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,321] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,323] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-03-26 10:10:04,378] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,379] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,380] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-03-26 10:10:04,397] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,397] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,399] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-03-26 10:10:04,416] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,416] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,418] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-03-26 10:10:04,436] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,437] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-03-26 10:10:04,529] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,529] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,530] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-03-26 10:10:04,549] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,550] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,551] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-03-26 10:10:04,573] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,574] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,576] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-03-26 10:10:04,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,597] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-03-26 10:10:04,616] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,617] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,618] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-03-26 10:10:04,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:10:04,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:04,642] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-03-26 10:10:09,968] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0921217e-29 1.0000000e+00 1.4714604e-26 2.8448962e-26 2.5539343e-27], sum to 1.0000
[2019-03-26 10:10:09,969] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4231
[2019-03-26 10:10:09,974] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 75.0, 1.0, 2.0, 0.8198653639557903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1274433.207144546, 1274433.207144545, 265693.4114227602], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 34200.0000, 
sim time next is 34800.0000, 
raw observation next is [24.46666666666667, 74.0, 1.0, 2.0, 0.8061059699703014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1250934.472634344, 1250934.472634345, 261632.8874783517], 
processed observation next is [1.0, 0.391304347826087, 0.3586097946287521, 0.74, 1.0, 1.0, 0.7663927349039775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34748179795398443, 0.34748179795398476, 0.3904968469826145], 
reward next is 0.6095, 
noisyNet noise sample is [array([0.5834393], dtype=float32), -0.12040518]. 
=============================================
[2019-03-26 10:10:16,327] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7067329e-36 1.0000000e+00 1.2630307e-35 7.8278720e-28 1.0811011e-30], sum to 1.0000
[2019-03-26 10:10:16,337] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2962
[2019-03-26 10:10:16,341] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 96.0, 1.0, 2.0, 0.3750105758785816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 571163.1231693695, 571163.1231693688, 172279.1118634258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 160200.0000, 
sim time next is 160800.0000, 
raw observation next is [22.0, 96.0, 1.0, 2.0, 0.3711299105779521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 567008.1271868538, 567008.1271868545, 171966.7106981881], 
processed observation next is [1.0, 0.8695652173913043, 0.2417061611374408, 0.96, 1.0, 1.0, 0.24232519346741213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15750225755190383, 0.15750225755190403, 0.25666673238535537], 
reward next is 0.7433, 
noisyNet noise sample is [array([-0.9744092], dtype=float32), -0.30981612]. 
=============================================
[2019-03-26 10:10:27,654] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3208823e-34 1.0000000e+00 2.2224160e-31 5.9806740e-28 2.1810723e-28], sum to 1.0000
[2019-03-26 10:10:27,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5465
[2019-03-26 10:10:27,668] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.61666666666667, 86.0, 1.0, 2.0, 0.2667518460789443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 433298.7257239028, 433298.7257239028, 162486.9835625219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 345000.0000, 
sim time next is 345600.0000, 
raw observation next is [20.6, 86.0, 1.0, 2.0, 0.2668067466344873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 433505.2629856134, 433505.2629856128, 162498.1913553059], 
processed observation next is [1.0, 0.0, 0.17535545023696694, 0.86, 1.0, 1.0, 0.11663463449938226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12041812860711483, 0.12041812860711466, 0.24253461396314313], 
reward next is 0.7575, 
noisyNet noise sample is [array([-0.9898764], dtype=float32), -0.7563202]. 
=============================================
[2019-03-26 10:10:28,340] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0394126e-27 1.0000000e+00 1.7014332e-23 1.3425430e-18 1.6151796e-23], sum to 1.0000
[2019-03-26 10:10:28,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3186
[2019-03-26 10:10:28,354] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.18333333333333, 88.16666666666667, 1.0, 2.0, 0.2635817827153241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 429329.4209391063, 429329.4209391063, 162210.6649499334], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 357000.0000, 
sim time next is 357600.0000, 
raw observation next is [20.16666666666667, 88.33333333333334, 1.0, 2.0, 0.2568115347796366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418275.7382502115, 418275.7382502115, 161522.1478395511], 
processed observation next is [1.0, 0.13043478260869565, 0.15481832543443946, 0.8833333333333334, 1.0, 1.0, 0.10459221057787539, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1161877050695032, 0.1161877050695032, 0.2410778325963449], 
reward next is 0.7589, 
noisyNet noise sample is [array([0.5050384], dtype=float32), -0.5894285]. 
=============================================
[2019-03-26 10:10:32,932] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4046084e-25 1.0000000e+00 6.3016497e-25 4.9519871e-22 8.6802641e-24], sum to 1.0000
[2019-03-26 10:10:32,941] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6737
[2019-03-26 10:10:32,945] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.15, 81.16666666666666, 1.0, 2.0, 0.2382334274313591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 392838.4163206305, 392838.4163206298, 159743.9394355263], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 456600.0000, 
sim time next is 457200.0000, 
raw observation next is [20.2, 81.0, 1.0, 2.0, 0.2383487008260658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392901.3570151539, 392901.3570151546, 159759.5334614988], 
processed observation next is [1.0, 0.30434782608695654, 0.15639810426540288, 0.81, 1.0, 1.0, 0.08234783232056118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10913926583754276, 0.10913926583754295, 0.23844706486790865], 
reward next is 0.7616, 
noisyNet noise sample is [array([-0.11690259], dtype=float32), -0.31960493]. 
=============================================
[2019-03-26 10:10:34,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.7689065e-29 1.0000000e+00 1.0857564e-27 2.2407993e-20 5.1986679e-24], sum to 1.0000
[2019-03-26 10:10:34,845] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9427
[2019-03-26 10:10:34,850] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 64.33333333333334, 1.0, 2.0, 0.4487163907517874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736011.6630349646, 736011.6630349639, 187009.2207271415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 472800.0000, 
sim time next is 473400.0000, 
raw observation next is [23.15, 63.5, 1.0, 2.0, 0.459211428534984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753088.038394804, 753088.038394804, 188738.8580953126], 
processed observation next is [1.0, 0.4782608695652174, 0.2962085308056872, 0.635, 1.0, 1.0, 0.3484475042590168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20919112177633445, 0.20919112177633445, 0.2816997882019591], 
reward next is 0.7183, 
noisyNet noise sample is [array([-3.060059], dtype=float32), -0.5101205]. 
=============================================
[2019-03-26 10:10:35,021] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 10:10:35,024] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:10:35,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:35,025] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:10:35,026] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:10:35,027] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:10:35,028] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:10:35,028] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:35,030] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:35,030] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:35,032] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:10:35,050] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run10
[2019-03-26 10:10:35,051] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run10
[2019-03-26 10:10:35,084] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run10
[2019-03-26 10:10:35,103] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run10
[2019-03-26 10:10:35,103] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run10
[2019-03-26 10:11:33,288] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.4127963], dtype=float32), 0.13866071]
[2019-03-26 10:11:33,290] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5506057228366943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769411.5612939531, 769411.5612939531, 191904.7533094572]
[2019-03-26 10:11:33,291] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:11:33,295] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.073983e-38], sampled 0.4156193408497304
[2019-03-26 10:11:49,513] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.4127963], dtype=float32), 0.13866071]
[2019-03-26 10:11:49,514] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 0.5417049376440679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756969.2510182518, 756969.2510182523, 190388.4717676915]
[2019-03-26 10:11:49,514] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:11:49,517] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.2408791e-33 1.0000000e+00 2.1460111e-30 4.5200394e-25 1.0869043e-27], sampled 0.4555635621546399
[2019-03-26 10:12:00,879] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.4127963], dtype=float32), 0.13866071]
[2019-03-26 10:12:00,881] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.65278574, 92.62419472666667, 1.0, 2.0, 0.6585053095530214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 920254.8075681878, 920254.8075681878, 212141.6397240911]
[2019-03-26 10:12:00,882] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:12:00,885] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2908717e-30 1.0000000e+00 9.4979254e-27 1.5574342e-21 9.3533236e-25], sampled 0.5252113639882401
[2019-03-26 10:12:12,545] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.4127963], dtype=float32), 0.13866071]
[2019-03-26 10:12:12,546] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.77967856833333, 91.17239662833333, 1.0, 2.0, 0.6846026026594138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 956741.9500858025, 956741.9500858025, 217559.9425282037]
[2019-03-26 10:12:12,547] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:12:12,550] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5871312e-31 1.0000000e+00 1.1594160e-27 2.8992301e-23 1.1890691e-25], sampled 0.5721836863383394
[2019-03-26 10:12:29,899] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1685 3164067279.4129 1778.0000
[2019-03-26 10:12:30,588] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5883 2927327297.4404 1338.0000
[2019-03-26 10:12:30,608] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-26 10:12:30,722] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8893 2779265366.1414 933.0000
[2019-03-26 10:12:30,881] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-26 10:12:31,895] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 225000, evaluation results [225000.0, 7884.168455680242, 3164067279.4128904, 1778.0, 8253.588267098503, 2927327297.440425, 1338.0, 8659.88925272283, 2779265366.141443, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-26 10:12:32,189] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3661304e-28 1.0000000e+00 2.8225419e-26 7.3864975e-21 1.0892725e-23], sum to 1.0000
[2019-03-26 10:12:32,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5172
[2019-03-26 10:12:32,202] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 53.0, 1.0, 2.0, 0.5029257488295957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 823978.8810772432, 823978.8810772427, 196343.2298963897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 486600.0000, 
sim time next is 487200.0000, 
raw observation next is [25.03333333333334, 53.0, 1.0, 2.0, 0.5232164997916068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 857609.5708894643, 857609.5708894636, 200104.891505163], 
processed observation next is [1.0, 0.6521739130434783, 0.3854660347551346, 0.53, 1.0, 1.0, 0.4255620479416949, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.238224880802629, 0.2382248808026288, 0.29866401717188507], 
reward next is 0.7013, 
noisyNet noise sample is [array([-0.26113677], dtype=float32), 0.05496317]. 
=============================================
[2019-03-26 10:12:40,734] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2854790e-33 1.0000000e+00 4.3108345e-31 1.2977415e-27 3.3518369e-27], sum to 1.0000
[2019-03-26 10:12:40,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3041
[2019-03-26 10:12:40,745] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.38333333333334, 81.0, 1.0, 2.0, 0.2198474705210321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 365308.693004809, 365308.6930048096, 157792.595963085], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 629400.0000, 
sim time next is 630000.0000, 
raw observation next is [19.6, 80.0, 1.0, 2.0, 0.2208335492003436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 366683.8036039399, 366683.8036039399, 157923.2939653766], 
processed observation next is [1.0, 0.30434782608695654, 0.127962085308057, 0.8, 1.0, 1.0, 0.06124524000041395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10185661211220552, 0.10185661211220552, 0.23570640890354716], 
reward next is 0.7643, 
noisyNet noise sample is [array([0.97240186], dtype=float32), -0.76533246]. 
=============================================
[2019-03-26 10:12:40,763] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.51027 ]
 [76.52187 ]
 [76.53858 ]
 [76.59208 ]
 [76.656906]], R is [[76.55433655]
 [76.55328369]
 [76.55220795]
 [76.55063629]
 [76.54972839]].
[2019-03-26 10:12:46,163] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.3190525e-30 1.0000000e+00 1.7829218e-25 7.9938363e-22 9.4181592e-26], sum to 1.0000
[2019-03-26 10:12:46,173] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4238
[2019-03-26 10:12:46,177] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333333, 57.0, 1.0, 2.0, 0.5367944882323397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872424.6141691782, 872424.6141691782, 202478.6110542036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 732000.0000, 
sim time next is 732600.0000, 
raw observation next is [25.0, 56.5, 1.0, 2.0, 0.5711455446654399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928565.8693541674, 928565.8693541674, 209314.956994964], 
processed observation next is [1.0, 0.4782608695652174, 0.38388625592417064, 0.565, 1.0, 1.0, 0.4833078851390842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2579349637094909, 0.2579349637094909, 0.3124103835745732], 
reward next is 0.6876, 
noisyNet noise sample is [array([-0.39281327], dtype=float32), 0.59054583]. 
=============================================
[2019-03-26 10:12:46,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2883335e-36 1.0000000e+00 1.0829101e-28 5.1439798e-26 2.4876438e-28], sum to 1.0000
[2019-03-26 10:12:46,804] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8972
[2019-03-26 10:12:46,812] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 56.5, 1.0, 2.0, 0.5711455446654399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928565.8693541674, 928565.8693541674, 209314.956994964], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 732600.0000, 
sim time next is 733200.0000, 
raw observation next is [25.06666666666666, 56.0, 1.0, 2.0, 0.5410158222457473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879874.7795319294, 879874.7795319294, 203320.0198753377], 
processed observation next is [1.0, 0.4782608695652174, 0.38704581358609763, 0.56, 1.0, 1.0, 0.44700701475391236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24440966098109151, 0.24440966098109151, 0.3034627162318473], 
reward next is 0.6965, 
noisyNet noise sample is [array([-1.2925941], dtype=float32), -0.83469296]. 
=============================================
[2019-03-26 10:12:51,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2372990e-31 1.0000000e+00 3.2175331e-27 2.2137172e-24 1.0620600e-24], sum to 1.0000
[2019-03-26 10:12:51,135] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5817
[2019-03-26 10:12:51,142] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 65.0, 1.0, 2.0, 0.291832647351383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466766.9952571982, 466766.9952571976, 164717.0981647818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 814800.0000, 
sim time next is 815400.0000, 
raw observation next is [24.7, 64.0, 1.0, 2.0, 0.2917469936813316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466636.1393664615, 466636.1393664609, 164708.0811510054], 
processed observation next is [0.0, 0.43478260869565216, 0.3696682464454976, 0.64, 1.0, 1.0, 0.14668312491726698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12962114982401707, 0.12962114982401693, 0.2458329569417991], 
reward next is 0.7542, 
noisyNet noise sample is [array([-0.5283451], dtype=float32), 0.5495097]. 
=============================================
[2019-03-26 10:12:54,256] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4124819e-30 1.0000000e+00 2.9599498e-27 2.0602910e-22 3.2614194e-24], sum to 1.0000
[2019-03-26 10:12:54,264] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4121
[2019-03-26 10:12:54,270] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.36666666666667, 89.0, 1.0, 2.0, 0.3026761867764391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481203.8092262834, 481203.8092262834, 165695.3336316373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 865200.0000, 
sim time next is 865800.0000, 
raw observation next is [21.35, 89.0, 1.0, 2.0, 0.3020354382085937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480360.7102689046, 480360.710268904, 165637.8022522679], 
processed observation next is [0.0, 0.0, 0.2109004739336494, 0.89, 1.0, 1.0, 0.1590788412151731, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13343353063025126, 0.1334335306302511, 0.24722060037651925], 
reward next is 0.7528, 
noisyNet noise sample is [array([-0.6038116], dtype=float32), 0.5715914]. 
=============================================
[2019-03-26 10:13:05,816] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.5586753e-34 1.0000000e+00 5.7990729e-30 1.0766445e-29 1.3107855e-29], sum to 1.0000
[2019-03-26 10:13:05,827] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4847
[2019-03-26 10:13:05,832] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.55, 95.0, 1.0, 2.0, 0.2985181058265153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475780.7580911931, 475780.7580911931, 165327.4756442262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1061400.0000, 
sim time next is 1062000.0000, 
raw observation next is [20.6, 95.0, 1.0, 2.0, 0.3003555778695772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478186.0030671549, 478186.0030671549, 165490.4903227347], 
processed observation next is [1.0, 0.30434782608695654, 0.17535545023696694, 0.95, 1.0, 1.0, 0.1570549130958761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13282944529643192, 0.13282944529643192, 0.24700073182497714], 
reward next is 0.7530, 
noisyNet noise sample is [array([2.5585196], dtype=float32), 0.88214666]. 
=============================================
[2019-03-26 10:13:05,851] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[72.21041 ]
 [72.26049 ]
 [72.309456]
 [72.35551 ]
 [72.398895]], R is [[72.24059296]
 [72.27143097]
 [72.30218506]
 [72.3328476 ]
 [72.36326599]].
[2019-03-26 10:13:10,618] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5773965e-31 1.0000000e+00 1.0768972e-25 8.4004472e-29 4.7094253e-27], sum to 1.0000
[2019-03-26 10:13:10,629] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0674
[2019-03-26 10:13:10,632] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 89.66666666666667, 1.0, 2.0, 0.3079046795037447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494965.5690400408, 494965.5690400408, 166737.0260687673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1146000.0000, 
sim time next is 1146600.0000, 
raw observation next is [20.9, 89.0, 1.0, 2.0, 0.338034773189991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542871.0891272039, 542871.0891272033, 170388.1016173341], 
processed observation next is [1.0, 0.2608695652173913, 0.1895734597156398, 0.89, 1.0, 1.0, 0.20245153396384455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15079752475755664, 0.15079752475755648, 0.25431059942885686], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.884183], dtype=float32), -0.005655795]. 
=============================================
[2019-03-26 10:13:10,781] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1002894e-28 1.0000000e+00 3.0353055e-25 7.0787392e-23 3.5272961e-24], sum to 1.0000
[2019-03-26 10:13:10,788] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6132
[2019-03-26 10:13:10,792] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 83.0, 1.0, 2.0, 0.3354277079898254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532901.9206266302, 532901.9206266295, 169590.0594110843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1152000.0000, 
sim time next is 1152600.0000, 
raw observation next is [22.4, 82.16666666666667, 1.0, 2.0, 0.4185849907539139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 663846.6084781395, 663846.6084781402, 180970.7405445914], 
processed observation next is [1.0, 0.34782608695652173, 0.2606635071090047, 0.8216666666666668, 1.0, 1.0, 0.29949998886013723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1844018356883721, 0.1844018356883723, 0.2701055829023752], 
reward next is 0.7299, 
noisyNet noise sample is [array([-0.9197432], dtype=float32), 0.82810694]. 
=============================================
[2019-03-26 10:13:13,648] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7390294e-27 1.0000000e+00 8.1699132e-26 2.4909205e-20 1.5214999e-23], sum to 1.0000
[2019-03-26 10:13:13,658] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7659
[2019-03-26 10:13:13,662] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 74.66666666666667, 1.0, 2.0, 0.3513624789805923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541945.5373656523, 541945.5373656517, 169996.6903790539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1196400.0000, 
sim time next is 1197000.0000, 
raw observation next is [24.5, 75.5, 1.0, 2.0, 0.3516749710018386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542032.8963588405, 542032.8963588398, 169992.5359783983], 
processed observation next is [1.0, 0.8695652173913043, 0.3601895734597157, 0.755, 1.0, 1.0, 0.21888550723113082, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15056469343301127, 0.15056469343301107, 0.25372020295283326], 
reward next is 0.7463, 
noisyNet noise sample is [array([-1.8660853], dtype=float32), -2.8680463]. 
=============================================
[2019-03-26 10:13:13,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.463005]
 [72.38965 ]
 [72.34435 ]
 [72.14841 ]
 [72.08843 ]], R is [[72.54756927]
 [72.56837463]
 [72.58881378]
 [72.60906982]
 [72.62937927]].
[2019-03-26 10:13:14,095] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5729302e-33 1.0000000e+00 3.6200315e-32 1.9450690e-32 3.5673549e-29], sum to 1.0000
[2019-03-26 10:13:14,104] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6441
[2019-03-26 10:13:14,113] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 87.0, 1.0, 2.0, 0.3565731776975517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549146.8796949072, 549146.8796949066, 170571.0982558838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1206000.0000, 
sim time next is 1206600.0000, 
raw observation next is [22.85, 87.16666666666667, 1.0, 2.0, 0.3557153872458308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548072.2460944823, 548072.2460944816, 170488.2475067229], 
processed observation next is [1.0, 1.0, 0.28199052132701435, 0.8716666666666667, 1.0, 1.0, 0.22375347860943468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15224229058180064, 0.15224229058180044, 0.25446007090555656], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.49196103], dtype=float32), 0.24941276]. 
=============================================
[2019-03-26 10:13:14,785] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5770339e-33 1.0000000e+00 1.8216313e-28 6.5717253e-26 2.4760273e-28], sum to 1.0000
[2019-03-26 10:13:14,792] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2479
[2019-03-26 10:13:14,796] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 90.66666666666667, 1.0, 2.0, 0.3806363193510849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 592550.3243295372, 592550.3243295372, 174447.397134563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1218000.0000, 
sim time next is 1218600.0000, 
raw observation next is [22.0, 91.0, 1.0, 2.0, 0.343914896035247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535283.5384585808, 535283.5384585802, 169584.2315427783], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.91, 1.0, 1.0, 0.2095360193195747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14868987179405024, 0.14868987179405005, 0.2531107933474303], 
reward next is 0.7469, 
noisyNet noise sample is [array([-1.7620535], dtype=float32), 1.0777066]. 
=============================================
[2019-03-26 10:13:20,449] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5327733e-29 1.0000000e+00 6.1080414e-27 3.8656403e-22 2.4358207e-26], sum to 1.0000
[2019-03-26 10:13:20,458] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3132
[2019-03-26 10:13:20,465] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 91.66666666666667, 1.0, 2.0, 0.5137428404186826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730800.3274593552, 730800.3274593552, 187428.4019170274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1311600.0000, 
sim time next is 1312200.0000, 
raw observation next is [24.55, 91.5, 1.0, 2.0, 0.5153362958853872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733246.0981006918, 733246.0981006911, 187712.6272447009], 
processed observation next is [1.0, 0.17391304347826086, 0.3625592417061612, 0.915, 1.0, 1.0, 0.4160678263679364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2036794716946366, 0.2036794716946364, 0.2801681003652252], 
reward next is 0.7198, 
noisyNet noise sample is [array([-0.22906873], dtype=float32), 1.487356]. 
=============================================
[2019-03-26 10:13:26,125] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 10:13:26,126] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:13:26,128] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:13:26,129] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:13:26,130] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:13:26,131] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:13:26,133] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:13:26,134] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:13:26,136] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:13:26,134] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:13:26,138] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:13:26,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run11
[2019-03-26 10:13:26,169] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run11
[2019-03-26 10:13:26,171] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run11
[2019-03-26 10:13:26,200] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run11
[2019-03-26 10:13:26,220] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run11
[2019-03-26 10:13:46,153] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:13:46,154] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.93333333333334, 56.83333333333334, 1.0, 2.0, 0.2138303555427675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 357220.6767451257, 357220.6767451257, 156650.1846531912]
[2019-03-26 10:13:46,156] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:13:46,163] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0899222e-33 1.0000000e+00 1.6416904e-32 6.8483981e-23 1.5195056e-29], sampled 0.49451575297896844
[2019-03-26 10:14:03,487] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:03,490] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.8, 90.0, 1.0, 2.0, 0.4728245965627842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 671357.8801751691, 671357.8801751684, 180818.2907789623]
[2019-03-26 10:14:03,491] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:14:03,495] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.7054101e-34 1.0000000e+00 3.8150652e-32 4.4918911e-22 4.0713127e-29], sampled 0.8219646063491739
[2019-03-26 10:14:04,426] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:04,427] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.36666666666667, 63.0, 1.0, 2.0, 0.6847157379778789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 956900.1295449181, 956900.1295449181, 217578.596879991]
[2019-03-26 10:14:04,428] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:14:04,433] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1043373e-33 1.0000000e+00 1.3666607e-30 5.3202133e-21 6.8965139e-28], sampled 0.6678387731362698
[2019-03-26 10:14:09,891] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:09,892] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.31142186666667, 95.22585998, 1.0, 2.0, 0.5043802498146042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704795.0650524873, 704795.0650524867, 184285.286145757]
[2019-03-26 10:14:09,896] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:14:09,898] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2451023e-33 1.0000000e+00 6.6979468e-32 6.9065916e-22 7.6823805e-29], sampled 0.2554777051217262
[2019-03-26 10:14:18,011] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:18,012] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.16666666666666, 65.66666666666667, 1.0, 2.0, 0.8832164033051959, 1.0, 2.0, 0.8832164033051959, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2470304.630355133, 2470304.630355134, 462420.4017496986]
[2019-03-26 10:14:18,013] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:14:18,016] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 1.3709263e-38 6.1723825e-35 2.1624823e-35], sampled 0.22275857746945393
[2019-03-26 10:14:18,017] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2470304.630355133 W.
[2019-03-26 10:14:22,200] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:22,201] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.98010222166667, 65.19201034333332, 1.0, 2.0, 0.8032795880778753, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00598173306788, 6.9112, 168.9123160021019, 2019663.339499975, 1952422.170901783, 408668.4308820979]
[2019-03-26 10:14:22,202] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:14:22,205] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.1832909e-33 1.0000000e+00 7.7436899e-31 2.6114212e-18 1.5661576e-27], sampled 0.45537402742199107
[2019-03-26 10:14:22,208] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2019663.339499975 W.
[2019-03-26 10:14:22,289] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:22,291] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.00000000000001, 1.0, 2.0, 0.5175360039345714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723184.498069246, 723184.4980692466, 186389.2707194568]
[2019-03-26 10:14:22,293] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:14:22,295] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.2139658e-33 1.0000000e+00 3.0854853e-30 1.1035534e-19 3.7174697e-27], sampled 0.6077446704705353
[2019-03-26 10:14:47,419] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:47,420] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.83333333333334, 50.33333333333334, 1.0, 2.0, 0.848742989778388, 1.0, 1.0, 0.848742989778388, 0.0, 1.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2373813.159697578, 2373813.159697578, 444022.2855930166]
[2019-03-26 10:14:47,423] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:14:47,427] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6927681e-34 1.0000000e+00 7.3397419e-33 8.7567715e-21 9.3592604e-30], sampled 0.9724628072980006
[2019-03-26 10:14:47,429] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2373813.159697578 W.
[2019-03-26 10:14:50,111] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:50,112] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.46666666666667, 76.66666666666667, 1.0, 2.0, 0.5231095193629334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730975.3894732237, 730975.3894732231, 187295.9904514712]
[2019-03-26 10:14:50,113] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:14:50,116] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6492079e-33 1.0000000e+00 2.5472656e-30 4.3671493e-21 4.3063871e-28], sampled 0.44222597050037005
[2019-03-26 10:14:50,630] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:50,632] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.1, 68.33333333333333, 1.0, 2.0, 0.559072928625498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781247.91093396, 781247.91093396, 193369.1985763331]
[2019-03-26 10:14:50,634] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:14:50,639] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.2102910e-35 1.0000000e+00 5.3450591e-33 9.3032304e-23 5.2782630e-30], sampled 0.2668616545154392
[2019-03-26 10:14:54,068] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:54,069] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.98794766666666, 82.697313135, 1.0, 2.0, 0.551902142099959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771223.8237636526, 771223.8237636526, 192126.5373969791]
[2019-03-26 10:14:54,070] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:14:54,071] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1761301e-34 1.0000000e+00 1.4545460e-32 2.7762557e-22 1.8661315e-29], sampled 0.5261226211565594
[2019-03-26 10:14:56,121] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:14:56,122] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.46180643, 64.938914, 1.0, 2.0, 0.5963449669972016, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 833352.2494554971, 833352.2494554977, 200066.2312906479]
[2019-03-26 10:14:56,125] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:14:56,130] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1211183e-34 1.0000000e+00 1.4041863e-31 9.3273631e-22 6.7539673e-29], sampled 0.020819479574435418
[2019-03-26 10:15:18,302] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.36978394], dtype=float32), 0.11936608]
[2019-03-26 10:15:18,307] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.92093496333334, 91.61153514, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.412713720908581, 6.9112, 168.9101401790252, 1813905.29169084, 1458120.129509273, 312011.122688531]
[2019-03-26 10:15:18,308] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:15:18,310] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1327460e-31 1.0000000e+00 2.7568486e-29 1.4773004e-16 2.7018690e-27], sampled 0.7626757924943849
[2019-03-26 10:15:18,311] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1813905.29169084 W.
[2019-03-26 10:15:20,632] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-26 10:15:21,967] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-26 10:15:21,992] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-26 10:15:22,050] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-03-26 10:15:22,130] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 10:15:23,145] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 250000, evaluation results [250000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 10:15:24,415] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.20466557e-32 1.00000000e+00 1.90295210e-29 1.08185166e-23
 2.25103809e-27], sum to 1.0000
[2019-03-26 10:15:24,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7459
[2019-03-26 10:15:24,429] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 69.33333333333334, 1.0, 2.0, 0.4339430502299369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622305.1670256971, 622305.1670256971, 175954.0529619521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1437600.0000, 
sim time next is 1438200.0000, 
raw observation next is [27.8, 69.5, 1.0, 2.0, 0.4380729333670599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625911.7252979735, 625911.7252979735, 176244.3485751881], 
processed observation next is [0.0, 0.6521739130434783, 0.5165876777251186, 0.695, 1.0, 1.0, 0.3229794377916384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17386436813832598, 0.17386436813832598, 0.2630512665301315], 
reward next is 0.7369, 
noisyNet noise sample is [array([1.072319], dtype=float32), 0.65094244]. 
=============================================
[2019-03-26 10:15:24,488] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7682271e-31 1.0000000e+00 2.2496622e-29 1.0884018e-20 4.1569730e-27], sum to 1.0000
[2019-03-26 10:15:24,489] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5615
[2019-03-26 10:15:24,497] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 69.0, 1.0, 2.0, 0.4277090929814758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617149.4771918209, 617149.4771918209, 175561.0670608429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [27.66666666666667, 69.16666666666667, 1.0, 2.0, 0.4303329792422638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619344.9523717558, 619344.9523717551, 175728.4968300429], 
processed observation next is [0.0, 0.6521739130434783, 0.5102685624012641, 0.6916666666666668, 1.0, 1.0, 0.31365419185814924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17204026454770996, 0.17204026454770976, 0.26228133855230285], 
reward next is 0.7377, 
noisyNet noise sample is [array([0.37622675], dtype=float32), -0.6593066]. 
=============================================
[2019-03-26 10:15:24,526] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.67566 ]
 [76.592094]
 [76.56614 ]
 [76.54492 ]
 [76.5245  ]], R is [[76.68906403]
 [76.66014099]
 [76.6315155 ]
 [76.60316467]
 [76.57505798]].
[2019-03-26 10:15:28,777] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1431880e-35 1.0000000e+00 8.0014753e-34 3.1600496e-25 8.4155503e-30], sum to 1.0000
[2019-03-26 10:15:28,787] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3913
[2019-03-26 10:15:28,797] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.65, 51.5, 1.0, 2.0, 0.3685639424325112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 556260.8394629584, 556260.8394629578, 170827.1007411704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [29.7, 51.66666666666667, 1.0, 2.0, 0.3714434896909298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 559244.8657543177, 559244.8657543184, 171039.0920033883], 
processed observation next is [0.0, 0.5652173913043478, 0.6066350710900474, 0.5166666666666667, 1.0, 1.0, 0.24270299962762623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15534579604286602, 0.15534579604286622, 0.2552822268707288], 
reward next is 0.7447, 
noisyNet noise sample is [array([-1.4895785], dtype=float32), 1.1720098]. 
=============================================
[2019-03-26 10:15:28,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.690254]
 [79.58796 ]
 [79.493996]
 [79.3736  ]
 [79.22541 ]], R is [[79.74442291]
 [79.69200897]
 [79.64066315]
 [79.59101868]
 [79.54125977]].
[2019-03-26 10:15:32,656] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1221034e-28 1.0000000e+00 6.7025496e-24 5.4286146e-24 2.2671203e-18], sum to 1.0000
[2019-03-26 10:15:32,665] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1261
[2019-03-26 10:15:32,670] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 85.0, 1.0, 2.0, 0.8333633007763968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1262923.366575425, 1262923.366575424, 265653.1000151281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1594800.0000, 
sim time next is 1595400.0000, 
raw observation next is [23.73333333333333, 85.00000000000001, 1.0, 2.0, 0.8033237847524282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1216325.692660438, 1216325.692660439, 257306.5942499215], 
processed observation next is [1.0, 0.4782608695652174, 0.3238546603475513, 0.8500000000000001, 1.0, 1.0, 0.7630407045209978, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33786824796123277, 0.33786824796123305, 0.3840396929103306], 
reward next is 0.6160, 
noisyNet noise sample is [array([0.6619717], dtype=float32), -0.5542499]. 
=============================================
[2019-03-26 10:15:33,953] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.1552835e-29 1.0000000e+00 1.5188346e-20 3.9061385e-14 9.0082171e-19], sum to 1.0000
[2019-03-26 10:15:33,964] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2096
[2019-03-26 10:15:33,969] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 90.0, 1.0, 2.0, 0.7090873024859129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1054767.694420521, 1054767.69442052, 231158.8804639621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1610400.0000, 
sim time next is 1611000.0000, 
raw observation next is [23.55, 90.5, 1.0, 2.0, 0.7373119212329328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1096130.606329716, 1096130.606329715, 237805.394125598], 
processed observation next is [1.0, 0.6521739130434783, 0.3151658767772513, 0.905, 1.0, 1.0, 0.6835083388348587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30448072398047665, 0.30448072398047643, 0.3549334240680567], 
reward next is 0.6451, 
noisyNet noise sample is [array([0.87015694], dtype=float32), -3.1137016]. 
=============================================
[2019-03-26 10:15:33,993] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[62.90309 ]
 [62.942505]
 [62.75746 ]
 [62.31205 ]
 [62.360046]], R is [[62.8946228 ]
 [62.92066574]
 [62.9654007 ]
 [63.0082016 ]
 [63.00640869]].
[2019-03-26 10:15:41,205] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1168315e-32 1.0000000e+00 1.2931036e-31 4.2412954e-25 1.2944147e-26], sum to 1.0000
[2019-03-26 10:15:41,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9966
[2019-03-26 10:15:41,218] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 94.0, 1.0, 2.0, 0.5106239030525441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713522.5556837571, 713522.5556837571, 185277.5712869275], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1724400.0000, 
sim time next is 1725000.0000, 
raw observation next is [25.63333333333333, 94.00000000000001, 1.0, 2.0, 0.5088553574614962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711050.4436148852, 711050.4436148852, 184995.3250477609], 
processed observation next is [1.0, 1.0, 0.4139020537124801, 0.9400000000000002, 1.0, 1.0, 0.4082594668210798, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19751401211524588, 0.19751401211524588, 0.2761124254444193], 
reward next is 0.7239, 
noisyNet noise sample is [array([-1.5472475], dtype=float32), -1.7246908]. 
=============================================
[2019-03-26 10:15:41,233] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[66.84527 ]
 [66.817024]
 [66.83327 ]
 [66.85428 ]
 [66.87587 ]], R is [[66.8976593 ]
 [66.95214844]
 [67.00610352]
 [67.05954742]
 [67.11256409]].
[2019-03-26 10:15:41,748] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.0235966e-31 1.0000000e+00 1.0691837e-28 1.7076517e-28 3.0561260e-25], sum to 1.0000
[2019-03-26 10:15:41,757] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9948
[2019-03-26 10:15:41,765] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 93.16666666666666, 1.0, 2.0, 0.523230978863991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740943.7219133435, 740943.7219133441, 188570.2929640439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [24.5, 93.0, 1.0, 2.0, 0.4871256155342776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688954.6414912388, 688954.6414912394, 182666.2265049748], 
processed observation next is [1.0, 0.2608695652173913, 0.3601895734597157, 0.93, 1.0, 1.0, 0.38207905486057536, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1913762893031219, 0.19137628930312206, 0.2726361589626489], 
reward next is 0.7274, 
noisyNet noise sample is [array([0.912181], dtype=float32), -0.0146856755]. 
=============================================
[2019-03-26 10:15:45,764] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6311358e-38 1.0000000e+00 0.0000000e+00 1.7943559e-34 8.0332209e-34], sum to 1.0000
[2019-03-26 10:15:45,774] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3704
[2019-03-26 10:15:45,780] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.38333333333333, 92.66666666666666, 1.0, 2.0, 0.32651692131266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513805.7873566918, 513805.7873566911, 168023.048003581], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1803000.0000, 
sim time next is 1803600.0000, 
raw observation next is [21.4, 93.0, 1.0, 2.0, 0.3272048030727237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514163.28648065, 514163.28648065, 168034.7519484025], 
processed observation next is [1.0, 0.9130434782608695, 0.21327014218009477, 0.93, 1.0, 1.0, 0.18940337719605266, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14282313513351388, 0.14282313513351388, 0.25079813723642164], 
reward next is 0.7492, 
noisyNet noise sample is [array([-0.19917317], dtype=float32), -1.3118098]. 
=============================================
[2019-03-26 10:15:49,320] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8090553e-32 1.0000000e+00 1.6846420e-30 1.4249692e-20 7.1568412e-27], sum to 1.0000
[2019-03-26 10:15:49,328] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1489
[2019-03-26 10:15:49,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1876695.219370714 W.
[2019-03-26 10:15:49,338] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 85.66666666666666, 1.0, 2.0, 0.4474369689406877, 1.0, 1.0, 0.4474369689406877, 1.0, 1.0, 0.767744374906603, 6.9112, 6.9112, 170.5573041426782, 1876695.219370714, 1876695.219370714, 379212.0347704961], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1870800.0000, 
sim time next is 1871400.0000, 
raw observation next is [27.0, 85.83333333333334, 1.0, 2.0, 0.4387878578588854, 1.0, 2.0, 0.4387878578588854, 1.0, 2.0, 0.751606003753007, 6.9112, 6.9112, 170.5573041426782, 1840386.897135881, 1840386.897135881, 373765.0711010996], 
processed observation next is [1.0, 0.6521739130434783, 0.4786729857819906, 0.8583333333333334, 1.0, 1.0, 0.32384079260106674, 1.0, 1.0, 0.32384079260106674, 1.0, 1.0, 0.6970804923817159, 0.0, 0.0, 0.8375144448122397, 0.5112185825377448, 0.5112185825377448, 0.557858315076268], 
reward next is 0.4421, 
noisyNet noise sample is [array([1.4250014], dtype=float32), 0.44564384]. 
=============================================
[2019-03-26 10:15:49,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5064934e-33 1.0000000e+00 2.0428898e-33 1.6485343e-28 5.7371742e-31], sum to 1.0000
[2019-03-26 10:15:49,494] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1988
[2019-03-26 10:15:49,504] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1991980.055441069 W.
[2019-03-26 10:15:49,508] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.96666666666667, 86.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.669365519044552, 6.9112, 168.9091850560178, 1991980.055441069, 1454123.351611639, 311349.6997402179], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1873200.0000, 
sim time next is 1873800.0000, 
raw observation next is [26.95, 87.0, 1.0, 2.0, 0.6391849884428294, 1.0, 1.0, 0.6391849884428294, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1787224.356512405, 1787224.356512405, 349172.9943793897], 
processed observation next is [1.0, 0.6956521739130435, 0.476303317535545, 0.87, 1.0, 1.0, 0.5652831186058186, 1.0, 0.5, 0.5652831186058186, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.49645121014233473, 0.49645121014233473, 0.521153722954313], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0158678], dtype=float32), 0.04797713]. 
=============================================
[2019-03-26 10:15:52,216] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4108273e-31 1.0000000e+00 6.9660790e-28 1.9116797e-25 2.5318177e-27], sum to 1.0000
[2019-03-26 10:15:52,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7479
[2019-03-26 10:15:52,228] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 94.66666666666667, 1.0, 2.0, 0.4337006350218194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 630318.5950626275, 630318.5950626275, 176973.4965908111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1916400.0000, 
sim time next is 1917000.0000, 
raw observation next is [23.55, 94.5, 1.0, 2.0, 0.4325988030863935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 630478.0488462491, 630478.0488462498, 177035.4536174381], 
processed observation next is [1.0, 0.17391304347826086, 0.3151658767772513, 0.945, 1.0, 1.0, 0.3163841001040886, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17513279134618032, 0.1751327913461805, 0.26423202032453447], 
reward next is 0.7358, 
noisyNet noise sample is [array([1.3342482], dtype=float32), -0.06031094]. 
=============================================
[2019-03-26 10:15:52,246] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.740265]
 [64.80805 ]
 [64.97023 ]
 [64.993805]
 [65.032196]], R is [[64.74849701]
 [64.83687592]
 [64.91999054]
 [65.00875854]
 [65.09640503]].
[2019-03-26 10:15:52,763] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9531202e-33 1.0000000e+00 2.0007994e-29 4.5216331e-22 1.9718483e-28], sum to 1.0000
[2019-03-26 10:15:52,774] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1112
[2019-03-26 10:15:52,779] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 82.66666666666667, 1.0, 2.0, 0.8095534257718462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1160609.468294198, 1160609.468294198, 250365.9228827418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1930800.0000, 
sim time next is 1931400.0000, 
raw observation next is [25.65, 82.5, 1.0, 2.0, 0.9261565077601371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1326232.476951096, 1326232.476951096, 281979.1999425621], 
processed observation next is [1.0, 0.34782608695652173, 0.41469194312796204, 0.825, 1.0, 1.0, 0.9110319370604062, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36839791026419333, 0.36839791026419333, 0.4208644775262121], 
reward next is 0.5791, 
noisyNet noise sample is [array([-0.6051402], dtype=float32), -0.9387006]. 
=============================================
[2019-03-26 10:16:00,398] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.3827960e-31 1.0000000e+00 5.4187498e-30 3.8750617e-21 3.1695611e-26], sum to 1.0000
[2019-03-26 10:16:00,406] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7603
[2019-03-26 10:16:00,413] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 95.0, 1.0, 2.0, 0.4669897690462323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655613.5174679902, 655613.5174679902, 178984.1089243094], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2077200.0000, 
sim time next is 2077800.0000, 
raw observation next is [24.38333333333333, 95.16666666666667, 1.0, 2.0, 0.4671193770822873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655654.3432110912, 655654.3432110912, 178985.0317722916], 
processed observation next is [0.0, 0.043478260869565216, 0.3546603475513427, 0.9516666666666667, 1.0, 1.0, 0.35797515311118955, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18212620644752534, 0.18212620644752534, 0.2671418384661069], 
reward next is 0.7329, 
noisyNet noise sample is [array([-0.31820205], dtype=float32), -0.33476883]. 
=============================================
[2019-03-26 10:16:01,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1925411e-30 1.0000000e+00 5.5496278e-28 2.3976956e-22 3.3111224e-25], sum to 1.0000
[2019-03-26 10:16:01,473] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3138
[2019-03-26 10:16:01,479] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 96.0, 1.0, 2.0, 0.4668021613574072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655153.3928290574, 655153.3928290581, 178931.0186076815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2081400.0000, 
sim time next is 2082000.0000, 
raw observation next is [24.23333333333333, 96.0, 1.0, 2.0, 0.4652106779754475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653890.3115176935, 653890.3115176935, 178821.8851558273], 
processed observation next is [0.0, 0.08695652173913043, 0.3475513428120062, 0.96, 1.0, 1.0, 0.3556755156330693, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18163619764380373, 0.18163619764380373, 0.2668983360534736], 
reward next is 0.7331, 
noisyNet noise sample is [array([0.98438174], dtype=float32), 1.1456301]. 
=============================================
[2019-03-26 10:16:01,497] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[72.14232]
 [71.83185]
 [71.79233]
 [71.95031]
 [72.05931]], R is [[72.23825073]
 [72.24880981]
 [72.25914001]
 [72.26937866]
 [72.27952576]].
[2019-03-26 10:16:03,725] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.0120100e-36 1.0000000e+00 7.4031308e-31 6.4500944e-28 2.5659993e-31], sum to 1.0000
[2019-03-26 10:16:03,741] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2828
[2019-03-26 10:16:03,748] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.16666666666667, 1.0, 2.0, 0.5585308538496467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780490.1379214241, 780490.1379214241, 193275.1673035178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2117400.0000, 
sim time next is 2118000.0000, 
raw observation next is [30.0, 75.33333333333334, 1.0, 2.0, 0.5588145818713958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780886.7647537526, 780886.7647537532, 193324.6083505794], 
processed observation next is [0.0, 0.5217391304347826, 0.6208530805687204, 0.7533333333333334, 1.0, 1.0, 0.46845130345951297, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21691299020937574, 0.21691299020937588, 0.28854419156802896], 
reward next is 0.7115, 
noisyNet noise sample is [array([0.31994367], dtype=float32), -0.95235217]. 
=============================================
[2019-03-26 10:16:03,760] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.24308 ]
 [74.161194]
 [74.038345]
 [73.96218 ]
 [73.8855  ]], R is [[74.29564667]
 [74.26422119]
 [74.23361969]
 [74.2037735 ]
 [74.17463684]].
[2019-03-26 10:16:08,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1033902e-24 1.0000000e+00 1.5530091e-20 3.9556532e-21 5.0619276e-20], sum to 1.0000
[2019-03-26 10:16:08,507] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6324
[2019-03-26 10:16:08,519] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2254349.167030679 W.
[2019-03-26 10:16:08,525] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.2, 73.0, 1.0, 2.0, 0.9709551626916517, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.001510949450575, 6.9112, 168.9124192259149, 2254349.167030679, 2190279.675022678, 454660.5093359434], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2199600.0000, 
sim time next is 2200200.0000, 
raw observation next is [30.3, 72.5, 1.0, 2.0, 0.5022369928925043, 1.0, 1.0, 0.5022369928925043, 1.0, 2.0, 0.8722195643426911, 6.911199999999999, 6.9112, 170.5573041426782, 2106770.226767204, 2106770.226767205, 416696.1493696207], 
processed observation next is [1.0, 0.4782608695652174, 0.6350710900473934, 0.725, 1.0, 1.0, 0.4002855336054269, 1.0, 0.5, 0.4002855336054269, 1.0, 1.0, 0.844170200417916, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5852139518797789, 0.5852139518797792, 0.6219345512979414], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2824754], dtype=float32), -1.4786226]. 
=============================================
[2019-03-26 10:16:11,443] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0596788e-27 1.0000000e+00 3.9427705e-22 6.9087787e-22 5.7163214e-20], sum to 1.0000
[2019-03-26 10:16:11,453] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8273
[2019-03-26 10:16:11,457] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 85.66666666666666, 1.0, 2.0, 0.5158277893766025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720796.6962367847, 720796.6962367853, 186112.9971079416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2248800.0000, 
sim time next is 2249400.0000, 
raw observation next is [26.83333333333334, 85.83333333333334, 1.0, 2.0, 0.5147426238877348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719279.8168914057, 719279.8168914057, 185938.1056061781], 
processed observation next is [1.0, 0.0, 0.4707740916271725, 0.8583333333333334, 1.0, 1.0, 0.41535255890088535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1997999491365016, 0.1997999491365016, 0.277519560606236], 
reward next is 0.7225, 
noisyNet noise sample is [array([0.66921926], dtype=float32), 1.1250594]. 
=============================================
[2019-03-26 10:16:17,372] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 10:16:17,374] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:16:17,375] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:16:17,375] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:16:17,377] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:16:17,376] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:16:17,380] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:16:17,381] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:16:17,382] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:16:17,379] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:16:17,383] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:16:17,397] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run12
[2019-03-26 10:16:17,397] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run12
[2019-03-26 10:16:17,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run12
[2019-03-26 10:16:17,435] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run12
[2019-03-26 10:16:17,435] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run12
[2019-03-26 10:16:21,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:16:21,427] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.73333333333333, 72.66666666666667, 1.0, 2.0, 0.2600036051435105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 426605.2472818992, 426605.2472818986, 161913.0288085178]
[2019-03-26 10:16:21,429] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:16:21,432] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3892497e-34 1.0000000e+00 1.7484049e-31 7.2303762e-29 6.4798334e-29], sampled 0.9276300090812803
[2019-03-26 10:16:28,854] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:16:28,857] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.93333333333333, 90.0, 1.0, 2.0, 0.2147865918908541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 357865.0006242922, 357865.0006242922, 157127.2389302956]
[2019-03-26 10:16:28,859] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:16:28,861] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.6392845e-34 1.0000000e+00 1.1144577e-30 2.5172843e-28 2.4053286e-28], sampled 0.944869296176509
[2019-03-26 10:16:29,200] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:16:29,202] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [14.86758014166667, 91.62867720666667, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 267560.082625112, 267560.082625112, 110447.2367823175]
[2019-03-26 10:16:29,202] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:16:29,207] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.9418312e-34 1.0000000e+00 7.8804718e-31 2.1383728e-28 1.3561301e-28], sampled 0.3111509192913816
[2019-03-26 10:16:32,892] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:16:32,893] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.2, 89.0, 1.0, 2.0, 0.2755404509890333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447735.5607758646, 447735.5607758646, 163419.0095155097]
[2019-03-26 10:16:32,894] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:16:32,899] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.2173731e-35 1.0000000e+00 1.4313353e-31 3.1934898e-29 6.5076270e-29], sampled 0.44660382965613943
[2019-03-26 10:16:40,784] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:16:40,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.36429146, 50.35448053, 1.0, 2.0, 0.3083343568121628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492957.2333816982, 492957.2333816982, 166581.3607071312]
[2019-03-26 10:16:40,787] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:16:40,790] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3291105e-36 1.0000000e+00 5.8106453e-33 1.5070187e-30 7.8784618e-30], sampled 0.34651155785409227
[2019-03-26 10:16:41,404] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:16:41,405] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.28256612333334, 86.73427183333334, 1.0, 2.0, 0.344387393407694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538075.8351289459, 538075.8351289459, 169858.6927778916]
[2019-03-26 10:16:41,408] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:16:41,412] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1275260e-37 1.0000000e+00 6.6682362e-35 6.8855640e-32 3.4331232e-31], sampled 0.8293058605233778
[2019-03-26 10:16:53,323] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:16:53,324] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.8, 70.66666666666667, 1.0, 2.0, 0.526767664461989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773855.2089769847, 773855.2089769847, 192635.6912338451]
[2019-03-26 10:16:53,325] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:16:53,330] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.8321336e-35 1.0000000e+00 2.5467019e-30 1.1708960e-28 3.1166373e-27], sampled 0.2543072726001889
[2019-03-26 10:17:01,495] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:17:01,496] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.6, 78.0, 1.0, 2.0, 0.8231664288996497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1150491.782201654, 1150491.782201654, 249674.234736589]
[2019-03-26 10:17:01,498] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:17:01,501] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9089738e-34 1.0000000e+00 1.1192390e-28 6.0194039e-27 7.3409442e-26], sampled 0.6988960309664306
[2019-03-26 10:17:23,709] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:17:23,711] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.5090006578029237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711253.5474228067, 711253.5474228067, 185017.7406865331]
[2019-03-26 10:17:23,712] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:17:23,715] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.7687124e-36 1.0000000e+00 2.1206927e-31 3.8774731e-28 1.2543260e-27], sampled 0.7180287750178601
[2019-03-26 10:17:30,366] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:17:30,367] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.51498758833333, 73.79509748166667, 1.0, 2.0, 0.573009163414277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 800729.7309781812, 800729.7309781818, 195825.4666844672]
[2019-03-26 10:17:30,368] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:17:30,372] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2547844e-36 1.0000000e+00 9.4971409e-32 2.0796363e-30 3.1592131e-28], sampled 0.4980145198655481
[2019-03-26 10:17:33,469] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:17:33,469] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.00000000000001, 1.0, 2.0, 0.5226573742461251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730343.3601159235, 730343.3601159242, 187221.8609097044]
[2019-03-26 10:17:33,470] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:17:33,476] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.4644708e-38 1.0000000e+00 1.6137602e-34 6.1060455e-32 9.9218005e-31], sampled 0.25091712521688647
[2019-03-26 10:17:48,902] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:17:48,904] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.51666666666667, 90.83333333333334, 1.0, 2.0, 0.5251045605265967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733764.1547786708, 733764.1547786702, 187623.1721951491]
[2019-03-26 10:17:48,905] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:17:48,908] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5420761e-35 1.0000000e+00 4.9909746e-31 8.4815908e-29 3.4318451e-28], sampled 0.8013562111624307
[2019-03-26 10:17:59,784] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:17:59,785] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.26666666666667, 94.66666666666667, 1.0, 2.0, 0.4776057856813794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676346.7693060341, 676346.7693060341, 181315.2051857659]
[2019-03-26 10:17:59,785] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:17:59,790] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5537681e-35 1.0000000e+00 4.7357179e-31 7.1426512e-29 2.6034307e-28], sampled 0.7269776518038139
[2019-03-26 10:18:12,994] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4308 2779149198.4617 933.0000
[2019-03-26 10:18:13,363] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-26 10:18:13,499] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.311903], dtype=float32), 0.15646216]
[2019-03-26 10:18:13,501] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.74064367, 97.68297705333333, 1.0, 2.0, 0.6894743522386418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 963553.3785389233, 963553.378538924, 218592.3956370758]
[2019-03-26 10:18:13,501] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:18:13,503] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.3486674e-34 1.0000000e+00 3.2479897e-29 1.9478073e-26 2.6129350e-26], sampled 0.15693900802882255
[2019-03-26 10:18:13,503] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1701 3164000012.1395 1778.0000
[2019-03-26 10:18:13,839] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-26 10:18:13,862] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 10:18:14,877] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 275000, evaluation results [275000.0, 7884.170123390672, 3164000012.13951, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.430759135581, 2779149198.4616733, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 10:18:19,502] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6918848e-36 1.0000000e+00 3.0431671e-32 5.7429052e-33 3.3697210e-27], sum to 1.0000
[2019-03-26 10:18:19,514] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6692
[2019-03-26 10:18:19,522] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 86.33333333333334, 1.0, 2.0, 0.5327858104983352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744501.4469203365, 744501.4469203365, 188892.3380741523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2580000.0000, 
sim time next is 2580600.0000, 
raw observation next is [27.18333333333334, 86.66666666666666, 1.0, 2.0, 0.5332053691326285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745087.9331896056, 745087.9331896056, 188962.0568143222], 
processed observation next is [1.0, 0.8695652173913043, 0.4873617693522911, 0.8666666666666666, 1.0, 1.0, 0.4375968302802753, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.206968870330446, 0.206968870330446, 0.2820329206183913], 
reward next is 0.7180, 
noisyNet noise sample is [array([0.1905444], dtype=float32), -1.7337238]. 
=============================================
[2019-03-26 10:18:19,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.3949380e-33 1.0000000e+00 1.9337315e-28 4.0134450e-27 2.4781547e-27], sum to 1.0000
[2019-03-26 10:18:19,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5549
[2019-03-26 10:18:19,950] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 83.0, 1.0, 2.0, 0.7142453982520539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 998187.6505371014, 998187.650537102, 223949.9227639498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2435400.0000, 
sim time next is 2436000.0000, 
raw observation next is [27.7, 83.33333333333334, 1.0, 2.0, 0.72440626249048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1012394.649632712, 1012394.649632711, 226201.2772083527], 
processed observation next is [1.0, 0.17391304347826086, 0.5118483412322274, 0.8333333333333335, 1.0, 1.0, 0.6679593523981687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28122073600908665, 0.28122073600908637, 0.3376138465796309], 
reward next is 0.6624, 
noisyNet noise sample is [array([-0.11522792], dtype=float32), 0.14020658]. 
=============================================
[2019-03-26 10:18:19,962] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[59.157997]
 [59.08599 ]
 [59.034214]
 [59.00608 ]
 [59.342594]], R is [[59.19464493]
 [59.26844406]
 [59.3342514 ]
 [59.3816185 ]
 [59.41862106]].
[2019-03-26 10:18:24,465] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4486860e-30 1.0000000e+00 1.7213982e-22 4.3772874e-22 2.1627946e-21], sum to 1.0000
[2019-03-26 10:18:24,476] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1136
[2019-03-26 10:18:24,484] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.21666666666667, 96.83333333333334, 1.0, 2.0, 0.6362318134670158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889114.7828770584, 889114.7828770584, 207684.0529895017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2523000.0000, 
sim time next is 2523600.0000, 
raw observation next is [26.2, 97.0, 1.0, 2.0, 0.6396759181259455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893929.8408518926, 893929.8408518926, 208364.1872846041], 
processed observation next is [1.0, 0.21739130434782608, 0.44075829383886256, 0.97, 1.0, 1.0, 0.5658746001517415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24831384468108128, 0.24831384468108128, 0.31099132430537924], 
reward next is 0.6890, 
noisyNet noise sample is [array([0.00969908], dtype=float32), -0.28429818]. 
=============================================
[2019-03-26 10:18:37,947] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4459349e-36 4.0826537e-35], sum to 1.0000
[2019-03-26 10:18:37,953] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5117
[2019-03-26 10:18:37,959] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 95.0, 1.0, 2.0, 0.3585729966423505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549726.1945690847, 549726.1945690847, 170547.4506901155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2753400.0000, 
sim time next is 2754000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3533609519993398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 543689.7888835866, 543689.788883586, 170102.0400789446], 
processed observation next is [0.0, 0.9130434782608695, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2209168096377588, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15102494135655184, 0.15102494135655165, 0.2538836419088726], 
reward next is 0.7461, 
noisyNet noise sample is [array([-0.6361117], dtype=float32), -0.910508]. 
=============================================
[2019-03-26 10:18:37,974] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[83.263336]
 [83.23171 ]
 [83.179955]
 [83.129875]
 [83.07651 ]], R is [[83.24910736]
 [83.16207123]
 [83.07522583]
 [82.98858643]
 [82.90225983]].
[2019-03-26 10:18:38,744] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0604031e-35 1.0000000e+00 3.4328726e-32 2.1945069e-30 8.8295868e-30], sum to 1.0000
[2019-03-26 10:18:38,754] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0079
[2019-03-26 10:18:38,756] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3477619098459994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535718.1434407702, 535718.1434407702, 169465.6075517082], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2766000.0000, 
sim time next is 2766600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3476887940397863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535605.8759647226, 535605.8759647219, 169456.450300268], 
processed observation next is [1.0, 0.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2140828843852847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14877940999020073, 0.14877940999020053, 0.2529200750750269], 
reward next is 0.7471, 
noisyNet noise sample is [array([-1.250227], dtype=float32), 1.9969838]. 
=============================================
[2019-03-26 10:18:39,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6940153e-26 1.0000000e+00 2.7711782e-21 1.6746086e-22 4.2387181e-20], sum to 1.0000
[2019-03-26 10:18:39,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2170
[2019-03-26 10:18:39,410] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3484596300850965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536791.4933470496, 536791.4933470496, 169553.3087885754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2775600.0000, 
sim time next is 2776200.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.3493758065039241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538680.3763129507, 538680.37631295, 169722.2388710988], 
processed observation next is [1.0, 0.13043478260869565, 0.23380726698262277, 0.95, 1.0, 1.0, 0.21611542952280008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14963343786470854, 0.14963343786470834, 0.2533167744344758], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.09768887], dtype=float32), -0.2285795]. 
=============================================
[2019-03-26 10:18:40,343] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1568085e-36 1.0000000e+00 1.7419416e-30 1.6855586e-33 1.4749883e-28], sum to 1.0000
[2019-03-26 10:18:40,351] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1815
[2019-03-26 10:18:40,357] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3799617438694608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585306.2526241723, 585306.2526241723, 173689.40024381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2793000.0000, 
sim time next is 2793600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3661254196985073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563989.4557530059, 563989.4557530066, 171830.7491793997], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.94, 1.0, 1.0, 0.23629568638374374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1566637377091683, 0.1566637377091685, 0.25646380474537267], 
reward next is 0.7435, 
noisyNet noise sample is [array([-0.7588524], dtype=float32), 0.64714396]. 
=============================================
[2019-03-26 10:18:41,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7786696e-32 1.0000000e+00 2.7200195e-26 5.9555052e-32 2.0467827e-25], sum to 1.0000
[2019-03-26 10:18:41,245] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9394
[2019-03-26 10:18:41,249] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333333, 87.0, 1.0, 2.0, 0.6523596692798883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959223.0942226064, 959223.0942226064, 217033.8834302443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2812800.0000, 
sim time next is 2813400.0000, 
raw observation next is [24.5, 86.0, 1.0, 2.0, 0.6977168232560289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024736.695136607, 1024736.695136607, 226878.1001928396], 
processed observation next is [1.0, 0.5652173913043478, 0.3601895734597157, 0.86, 1.0, 1.0, 0.6358034015132878, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28464908198239086, 0.28464908198239086, 0.33862403013856657], 
reward next is 0.6614, 
noisyNet noise sample is [array([0.35777706], dtype=float32), 0.24339533]. 
=============================================
[2019-03-26 10:18:50,385] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5438784e-36 2.3676486e-36], sum to 1.0000
[2019-03-26 10:18:50,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2424
[2019-03-26 10:18:50,399] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3318349636986662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 515956.7768636285, 515956.7768636279, 168032.1716287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2962800.0000, 
sim time next is 2963400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3368404410383452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523209.2688497411, 523209.2688497411, 168588.2257081255], 
processed observation next is [1.0, 0.30434782608695654, 0.19431279620853087, 1.0, 1.0, 1.0, 0.20101257956427132, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14533590801381696, 0.14533590801381696, 0.25162421747481417], 
reward next is 0.7484, 
noisyNet noise sample is [array([0.29892862], dtype=float32), -0.3191857]. 
=============================================
[2019-03-26 10:18:52,612] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2550113e-33], sum to 1.0000
[2019-03-26 10:18:52,620] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0418
[2019-03-26 10:18:52,625] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3028372869783388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 482251.184449583, 482251.1844495836, 165783.3446633421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3026400.0000, 
sim time next is 3027000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.302087770492226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481058.0224425817, 481058.0224425824, 165697.6778390874], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15914189215930843, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1336272284562727, 0.1336272284562729, 0.24730996692401105], 
reward next is 0.7527, 
noisyNet noise sample is [array([-0.3704958], dtype=float32), -1.3955488]. 
=============================================
[2019-03-26 10:18:52,639] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[83.260765]
 [83.306915]
 [83.401924]
 [83.60506 ]
 [84.17021 ]], R is [[83.04039764]
 [82.96255493]
 [82.88534546]
 [82.80886078]
 [82.73323059]].
[2019-03-26 10:18:54,659] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7417900e-36 1.0000000e+00 1.0449651e-30 1.5481939e-28 2.3467709e-27], sum to 1.0000
[2019-03-26 10:18:54,668] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1558
[2019-03-26 10:18:54,673] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 97.0, 1.0, 2.0, 0.3158891045644832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495703.1224979213, 495703.1224979213, 166614.58172773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3040200.0000, 
sim time next is 3040800.0000, 
raw observation next is [21.0, 98.0, 1.0, 2.0, 0.3202592044005328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501052.8149874106, 501052.8149874106, 166975.7383874612], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.98, 1.0, 1.0, 0.1810351860247383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13918133749650294, 0.13918133749650294, 0.24921751998128538], 
reward next is 0.7508, 
noisyNet noise sample is [array([1.0535498], dtype=float32), -0.1694288]. 
=============================================
[2019-03-26 10:18:59,440] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 4.3360264e-35 1.4531296e-35 1.2962236e-33], sum to 1.0000
[2019-03-26 10:18:59,446] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6770
[2019-03-26 10:18:59,452] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 99.00000000000001, 1.0, 2.0, 0.4285375149836807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 647176.0835598018, 647176.0835598018, 179171.8585202544], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3121800.0000, 
sim time next is 3122400.0000, 
raw observation next is [22.0, 98.0, 1.0, 2.0, 0.3852086289591706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583957.1103113506, 583957.1103113499, 173337.6641927693], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.98, 1.0, 1.0, 0.25928750477008505, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1622103084198196, 0.1622103084198194, 0.25871293163099895], 
reward next is 0.7413, 
noisyNet noise sample is [array([-0.09505867], dtype=float32), 0.36369964]. 
=============================================
[2019-03-26 10:19:08,372] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 10:19:08,373] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:19:08,375] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:19:08,376] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:19:08,377] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:19:08,377] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:19:08,378] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:19:08,379] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:19:08,380] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:19:08,379] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:19:08,381] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:19:08,398] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run13
[2019-03-26 10:19:08,399] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run13
[2019-03-26 10:19:08,413] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run13
[2019-03-26 10:19:08,434] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run13
[2019-03-26 10:19:08,435] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run13
[2019-03-26 10:19:17,693] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.26646075], dtype=float32), 0.14500082]
[2019-03-26 10:19:17,699] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.55, 80.5, 1.0, 2.0, 0.3222722779123588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511563.5850527539, 511563.5850527532, 167929.9817333489]
[2019-03-26 10:19:17,699] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:19:17,703] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000742e-34 1.0000000e+00 3.8815604e-34 4.1115800e-29 1.8213237e-30], sampled 0.11109234588805961
[2019-03-26 10:19:44,862] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.26646075], dtype=float32), 0.14500082]
[2019-03-26 10:19:44,865] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.95, 89.0, 1.0, 2.0, 0.7701478120714231, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.968340867407591, 6.9112, 168.9125714502287, 1973294.95230907, 1932757.346744477, 401843.1216900989]
[2019-03-26 10:19:44,866] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:19:44,869] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.0252424e-33 1.0000000e+00 2.5095812e-31 9.8701507e-25 3.9151979e-27], sampled 0.4291865400116208
[2019-03-26 10:19:44,870] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1973294.95230907 W.
[2019-03-26 10:19:52,131] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.26646075], dtype=float32), 0.14500082]
[2019-03-26 10:19:52,134] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 89.0, 1.0, 2.0, 0.5395591959381556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844189.1259799454, 844189.1259799454, 200583.1075371584]
[2019-03-26 10:19:52,135] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:19:52,138] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.8525560e-33 1.0000000e+00 1.2648323e-30 1.2846551e-26 1.1806204e-27], sampled 0.813223595741409
[2019-03-26 10:20:13,978] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.26646075], dtype=float32), 0.14500082]
[2019-03-26 10:20:13,979] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.16666666666667, 79.0, 1.0, 2.0, 0.5698910863366542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796370.8600295669, 796370.8600295669, 195269.9276933215]
[2019-03-26 10:20:13,982] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:20:13,984] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1732976e-36 1.0000000e+00 2.6302453e-34 1.9812648e-29 3.4547361e-30], sampled 0.8451350178340177
[2019-03-26 10:20:51,610] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.26646075], dtype=float32), 0.14500082]
[2019-03-26 10:20:51,611] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.13333333333333, 78.66666666666666, 1.0, 2.0, 0.4810496341558501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672183.7460203886, 672183.746020388, 180680.3454985438]
[2019-03-26 10:20:51,612] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:20:51,615] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.9423391e-35 6.8986503e-34], sampled 0.3751869122060417
[2019-03-26 10:20:56,932] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.26646075], dtype=float32), 0.14500082]
[2019-03-26 10:20:56,935] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.95, 88.16666666666667, 1.0, 2.0, 0.5739715661886261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802075.1116900512, 802075.1116900512, 195996.4629180903]
[2019-03-26 10:20:56,935] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:20:56,937] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.9880614e-36 1.0000000e+00 6.6254645e-34 5.2119163e-30 9.7118418e-30], sampled 0.6382359121530802
[2019-03-26 10:21:04,056] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-26 10:21:04,301] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1685 3164067279.4129 1778.0000
[2019-03-26 10:21:04,376] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7746 2842563444.1824 1131.0000
[2019-03-26 10:21:04,397] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-26 10:21:04,502] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-26 10:21:05,518] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 300000, evaluation results [300000.0, 7884.168455680242, 3164067279.4128904, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8496.774567725599, 2842563444.1823883, 1131.0]
[2019-03-26 10:21:10,154] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.2166012e-34 1.0000000e+00 2.9494291e-30 4.2121942e-24 1.5550191e-27], sum to 1.0000
[2019-03-26 10:21:10,163] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5563
[2019-03-26 10:21:10,169] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5379667811787028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751743.7664364893, 751743.7664364887, 189758.3783296175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3369600.0000, 
sim time next is 3370200.0000, 
raw observation next is [26.95, 89.33333333333334, 1.0, 2.0, 0.5373291143985411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750852.3889982343, 750852.3889982337, 189651.3903222882], 
processed observation next is [1.0, 0.0, 0.476303317535545, 0.8933333333333334, 1.0, 1.0, 0.44256519807053146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2085701080550651, 0.20857010805506493, 0.28306177660043014], 
reward next is 0.7169, 
noisyNet noise sample is [array([0.48576534], dtype=float32), -0.17983806]. 
=============================================
[2019-03-26 10:21:10,588] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0867338e-28 1.0000000e+00 2.1454120e-24 1.2356437e-18 1.0767510e-22], sum to 1.0000
[2019-03-26 10:21:10,594] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1253
[2019-03-26 10:21:10,598] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7715458770249043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1078308.110315211, 1078308.110315211, 237045.5242425289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3384000.0000, 
sim time next is 3384600.0000, 
raw observation next is [26.0, 94.00000000000001, 1.0, 2.0, 0.8375912802156392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1170663.670720548, 1170663.670720547, 253346.4939416902], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.9400000000000002, 1.0, 1.0, 0.8043268436333002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32518435297792997, 0.3251843529779297, 0.3781290954353585], 
reward next is 0.6219, 
noisyNet noise sample is [array([-0.72496885], dtype=float32), -1.863981]. 
=============================================
[2019-03-26 10:21:10,667] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.1002579e-29 1.0000000e+00 1.6374121e-25 5.0056978e-22 2.3153584e-24], sum to 1.0000
[2019-03-26 10:21:10,677] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9834
[2019-03-26 10:21:10,685] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 90.66666666666666, 1.0, 2.0, 0.8117824857550356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1134572.607413059, 1134572.60741306, 246823.3154878099], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3390000.0000, 
sim time next is 3390600.0000, 
raw observation next is [26.83333333333333, 89.83333333333333, 1.0, 2.0, 0.8089821532016161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1130656.691245038, 1130656.691245037, 246127.4654552584], 
processed observation next is [1.0, 0.21739130434782608, 0.470774091627172, 0.8983333333333333, 1.0, 1.0, 0.7698580159055616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3140713031236217, 0.31407130312362136, 0.36735442605262447], 
reward next is 0.6326, 
noisyNet noise sample is [array([-1.3241386], dtype=float32), 0.45224324]. 
=============================================
[2019-03-26 10:21:10,964] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6220240e-24 1.0000000e+00 8.2238762e-20 3.5393099e-14 1.1304914e-18], sum to 1.0000
[2019-03-26 10:21:10,972] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3059
[2019-03-26 10:21:10,976] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7759260832202286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1084432.987340374, 1084432.987340373, 238086.6290497706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3382200.0000, 
sim time next is 3382800.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.7757666765312647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1084210.087084852, 1084210.087084853, 238048.6408188205], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.94, 1.0, 1.0, 0.7298393693147768, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3011694686346811, 0.3011694686346814, 0.35529647883406046], 
reward next is 0.6447, 
noisyNet noise sample is [array([-0.34188613], dtype=float32), 1.5729145]. 
=============================================
[2019-03-26 10:21:12,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0855225e-38], sum to 1.0000
[2019-03-26 10:21:12,464] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4452
[2019-03-26 10:21:12,473] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2289028.622875584 W.
[2019-03-26 10:21:12,478] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.9957323006044587, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005993088764956, 6.9112, 168.9123931302006, 2289028.622875584, 2221779.367478516, 462002.0044227137], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3415200.0000, 
sim time next is 3415800.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.5282603239839663, 1.0, 1.0, 0.5282603239839663, 1.0, 2.0, 0.9174134844014604, 6.9112, 6.9112, 170.5573041426782, 2216044.611126713, 2216044.611126713, 435240.3733163462], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.67, 1.0, 1.0, 0.4316389445589955, 1.0, 0.5, 0.4316389445589955, 1.0, 1.0, 0.8992847370749516, 0.0, 0.0, 0.8375144448122397, 0.6155679475351981, 0.6155679475351981, 0.6496124974870838], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.975339], dtype=float32), 0.47972584]. 
=============================================
[2019-03-26 10:21:17,320] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0816178e-25 1.0000000e+00 1.7590038e-21 1.1621409e-13 1.7529033e-19], sum to 1.0000
[2019-03-26 10:21:17,327] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5279
[2019-03-26 10:21:17,334] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2068317.66410907 W.
[2019-03-26 10:21:17,340] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.4930790538099703, 1.0, 1.0, 0.4930790538099703, 1.0, 1.0, 0.8379354335956267, 6.9112, 6.9112, 170.5573041426782, 2068317.66410907, 2068317.66410907, 407078.8995448349], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3486600.0000, 
sim time next is 3487200.0000, 
raw observation next is [29.33333333333334, 68.66666666666667, 1.0, 2.0, 0.7908429214580333, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.973869243134798, 6.9112, 168.9125827999119, 2002257.809876309, 1957798.190252248, 406686.5048703952], 
processed observation next is [1.0, 0.34782608695652173, 0.5892575039494474, 0.6866666666666668, 1.0, 1.0, 0.7480035198289557, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006266924313479816, 0.0, 0.8294381100625684, 0.5561827249656414, 0.5438328306256245, 0.6069947833886495], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8314416], dtype=float32), 1.046823]. 
=============================================
[2019-03-26 10:21:21,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.9798466e-32 1.0000000e+00 8.1448278e-27 3.3033812e-22 8.8123332e-28], sum to 1.0000
[2019-03-26 10:21:21,363] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5745
[2019-03-26 10:21:21,369] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 79.0, 1.0, 2.0, 0.7904241305869535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1104705.973821616, 1104705.973821616, 241571.4832407911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3565800.0000, 
sim time next is 3566400.0000, 
raw observation next is [27.66666666666666, 79.0, 1.0, 2.0, 0.765143346344006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1069355.464498483, 1069355.464498483, 235532.7141016354], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012636, 0.79, 1.0, 1.0, 0.7170401763180796, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29704318458291196, 0.29704318458291196, 0.35154136433079913], 
reward next is 0.6485, 
noisyNet noise sample is [array([-0.6756986], dtype=float32), -0.36880127]. 
=============================================
[2019-03-26 10:21:44,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2197455e-35 1.0000000e+00 2.4522220e-32 1.4240961e-25 1.1695059e-31], sum to 1.0000
[2019-03-26 10:21:44,164] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5094
[2019-03-26 10:21:44,171] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 60.5, 1.0, 2.0, 0.6015151681422277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840580.1191057914, 840580.1191057914, 201028.9345735524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3952200.0000, 
sim time next is 3952800.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.59795932202276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 835609.0901588723, 835609.0901588729, 200366.724353474], 
processed observation next is [0.0, 0.782608695652174, 0.8104265402843602, 0.6, 1.0, 1.0, 0.5156136409912772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2321136361552423, 0.23211363615524247, 0.29905481246787163], 
reward next is 0.7009, 
noisyNet noise sample is [array([0.47793704], dtype=float32), -1.1314588]. 
=============================================
[2019-03-26 10:21:55,058] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1745967e-34 1.0000000e+00 4.0206636e-34 3.7213219e-23 2.1166054e-29], sum to 1.0000
[2019-03-26 10:21:55,071] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0792
[2019-03-26 10:21:55,078] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5826754100228536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814242.634907639, 814242.634907639, 197562.6636269549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4140000.0000, 
sim time next is 4140600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5828694763048515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814513.9311146375, 814513.9311146375, 197597.8153263974], 
processed observation next is [1.0, 0.9565217391304348, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4974331039817488, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22625386975406597, 0.22625386975406597, 0.2949221124274588], 
reward next is 0.7051, 
noisyNet noise sample is [array([2.446976], dtype=float32), 0.47484243]. 
=============================================
[2019-03-26 10:21:59,625] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 10:21:59,625] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:21:59,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:21:59,627] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:21:59,628] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:21:59,629] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:21:59,629] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:21:59,630] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:21:59,630] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:21:59,630] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:21:59,631] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:21:59,650] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run14
[2019-03-26 10:21:59,667] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run14
[2019-03-26 10:21:59,669] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run14
[2019-03-26 10:21:59,669] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run14
[2019-03-26 10:21:59,669] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run14
[2019-03-26 10:22:49,030] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.26644373], dtype=float32), 0.147405]
[2019-03-26 10:22:49,033] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.64376429333333, 73.03514145666666, 1.0, 2.0, 0.5800531054367798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810576.7741412206, 810576.7741412206, 197088.3194027193]
[2019-03-26 10:22:49,034] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:22:49,038] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.8347281e-29 1.0000000e+00 4.2341921e-28 5.0092830e-17 1.9098802e-24], sampled 0.1696960964896299
[2019-03-26 10:22:53,867] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.26644373], dtype=float32), 0.147405]
[2019-03-26 10:22:53,870] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5250013921539649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733619.94084384, 733619.9408438393, 187605.9176718834]
[2019-03-26 10:22:53,871] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:22:53,873] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3099500e-30 1.0000000e+00 2.9313061e-28 1.4582486e-21 2.1286351e-24], sampled 0.2818271711299106
[2019-03-26 10:23:13,706] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.26644373], dtype=float32), 0.147405]
[2019-03-26 10:23:13,707] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.72605034333333, 62.50774855333334, 1.0, 2.0, 0.5398563908022735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754385.205882543, 754385.2058825436, 190076.7107929243]
[2019-03-26 10:23:13,708] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:23:13,711] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5780308e-28 1.0000000e+00 2.3082557e-27 1.3818989e-17 1.8629426e-24], sampled 0.5175336938010202
[2019-03-26 10:23:47,923] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.26644373], dtype=float32), 0.147405]
[2019-03-26 10:23:47,924] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.36303073666667, 87.10013771666668, 1.0, 2.0, 0.4715613547012553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766331.585066212, 766331.5850662114, 190624.961527714]
[2019-03-26 10:23:47,925] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:23:47,929] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3035876e-25 1.0000000e+00 8.5380031e-23 7.7519100e-15 2.2075069e-20], sampled 0.14605672910998735
[2019-03-26 10:23:55,442] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6674 3164196406.5138 1778.0000
[2019-03-26 10:23:55,534] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 10:23:55,698] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-03-26 10:23:55,831] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2300 2927457024.3359 1338.0000
[2019-03-26 10:23:55,894] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.1725 2779322230.8975 933.0000
[2019-03-26 10:23:56,909] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 325000, evaluation results [325000.0, 7882.667391253915, 3164196406.5138016, 1778.0, 8254.230042292205, 2927457024.3359056, 1338.0, 8661.172501768742, 2779322230.897455, 933.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 10:23:59,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.7287022e-15 1.0000000e+00 2.6583615e-09 2.9415209e-10 3.6792707e-09], sum to 1.0000
[2019-03-26 10:23:59,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7494
[2019-03-26 10:23:59,153] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2483890.377803407 W.
[2019-03-26 10:23:59,156] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.66666666666666, 76.5, 1.0, 2.0, 0.8880689386810842, 1.0, 2.0, 0.8880689386810842, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2483890.377803407, 2483890.377803406, 465043.4039362701], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4265400.0000, 
sim time next is 4266000.0000, 
raw observation next is [33.0, 75.0, 1.0, 2.0, 0.759821799279894, 1.0, 2.0, 0.7005009391542096, 1.0, 1.0, 1.03, 7.005102449523388, 6.9112, 170.5573041426782, 2939441.498933414, 2872175.354632382, 540891.4304377792], 
processed observation next is [1.0, 0.391304347826087, 0.7630331753554502, 0.75, 1.0, 1.0, 0.7106286738311975, 1.0, 1.0, 0.63915775801712, 1.0, 0.5, 1.0365853658536586, 0.009390244952338821, 0.0, 0.8375144448122397, 0.8165115274815039, 0.7978264873978839, 0.8073006424444465], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.07351957], dtype=float32), 0.80841833]. 
=============================================
[2019-03-26 10:23:59,167] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[18.66136 ]
 [18.100986]
 [17.622648]
 [18.820702]
 [20.270065]], R is [[17.8999691 ]
 [18.02687645]
 [18.13421059]
 [17.95286942]
 [17.77334023]].
[2019-03-26 10:24:01,857] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.65251387e-32 1.00000000e+00 1.09505295e-27 1.66589784e-23
 8.43038393e-25], sum to 1.0000
[2019-03-26 10:24:01,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3313
[2019-03-26 10:24:01,869] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 73.0, 1.0, 2.0, 0.6138371803709566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857806.3199818324, 857806.3199818324, 203352.9630802807], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4311000.0000, 
sim time next is 4311600.0000, 
raw observation next is [31.66666666666666, 75.0, 1.0, 2.0, 0.6159248780729145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 860724.9537543136, 860724.9537543142, 203751.033751722], 
processed observation next is [1.0, 0.9130434782608695, 0.6998420221169034, 0.75, 1.0, 1.0, 0.5372588892444753, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23909026493175378, 0.23909026493175395, 0.30410602052495816], 
reward next is 0.6959, 
noisyNet noise sample is [array([-0.7242633], dtype=float32), 1.0620612]. 
=============================================
[2019-03-26 10:24:03,134] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1965645e-25 1.0000000e+00 1.3302572e-14 9.8741632e-21 2.1307966e-16], sum to 1.0000
[2019-03-26 10:24:03,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6971
[2019-03-26 10:24:03,154] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.9945036003310147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1390116.541133479, 1390116.541133479, 297261.7540957158], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4345200.0000, 
sim time next is 4345800.0000, 
raw observation next is [30.16666666666666, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.528954585760246, 6.9112, 168.9040528137483, 2602131.334153201, 1454501.048982887, 310414.9103111009], 
processed observation next is [1.0, 0.30434782608695654, 0.6287519747235385, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.16177545857602463, 0.0, 0.8293962239254606, 0.7228142594870003, 0.4040280691619131, 0.4633058362852252], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46385813], dtype=float32), 0.18931545]. 
=============================================
[2019-03-26 10:24:03,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.8374776e-20 1.0000000e+00 2.4873631e-10 3.2593074e-15 6.3646094e-13], sum to 1.0000
[2019-03-26 10:24:03,774] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3174
[2019-03-26 10:24:03,780] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.9721585919429385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1358862.711681106, 1358862.711681107, 290563.1431600524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4338000.0000, 
sim time next is 4338600.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.754425147401236, 6.9112, 168.9086912352809, 2052362.533200363, 1454164.697730641, 311356.0814259161], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.89, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.08432251474012356, 0.0, 0.8294190006986644, 0.5701007036667675, 0.40393463825851145, 0.4647105692924121], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1534222], dtype=float32), 1.5312877]. 
=============================================
[2019-03-26 10:24:09,965] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7500917e-25 1.0000000e+00 6.1138179e-21 1.3118340e-17 4.0340263e-13], sum to 1.0000
[2019-03-26 10:24:09,974] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0356
[2019-03-26 10:24:09,982] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 71.0, 1.0, 2.0, 0.6423674400303991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 897692.7597030667, 897692.7597030667, 208905.309976897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4449600.0000, 
sim time next is 4450200.0000, 
raw observation next is [33.0, 70.33333333333334, 1.0, 2.0, 0.6710449508241483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 937786.5720021, 937786.5720021005, 214727.2930612], 
processed observation next is [0.0, 0.5217391304347826, 0.7630331753554502, 0.7033333333333335, 1.0, 1.0, 0.603668615450781, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26049627000058334, 0.26049627000058345, 0.32048849710626864], 
reward next is 0.6795, 
noisyNet noise sample is [array([-0.4254269], dtype=float32), 1.8299032]. 
=============================================
[2019-03-26 10:24:10,449] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.1544259e-29 1.0000000e+00 4.2767479e-24 5.6029472e-16 2.4339374e-13], sum to 1.0000
[2019-03-26 10:24:10,457] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8305
[2019-03-26 10:24:10,461] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 69.33333333333334, 1.0, 2.0, 0.5876119191998253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 821143.6811967702, 821143.6811967709, 198459.8766766262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4458000.0000, 
sim time next is 4458600.0000, 
raw observation next is [31.0, 68.5, 1.0, 2.0, 0.5716004523164921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 798760.4413700833, 798760.441370084, 195573.4632626218], 
processed observation next is [0.0, 0.6086956521739131, 0.6682464454976303, 0.685, 1.0, 1.0, 0.48385596664637603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22187790038057867, 0.22187790038057886, 0.29190069143674896], 
reward next is 0.7081, 
noisyNet noise sample is [array([0.36577773], dtype=float32), -0.79924524]. 
=============================================
[2019-03-26 10:24:15,894] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.5143508e-35 1.0000000e+00 3.9008446e-33 1.7429154e-26 1.3628578e-26], sum to 1.0000
[2019-03-26 10:24:15,908] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5494
[2019-03-26 10:24:15,914] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 64.5, 1.0, 2.0, 0.5295354357464656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739957.8732625517, 739957.8732625517, 188353.3893941975], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4555800.0000, 
sim time next is 4556400.0000, 
raw observation next is [30.66666666666667, 66.33333333333333, 1.0, 2.0, 0.5305104047202852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741320.7427937491, 741320.7427937497, 188514.8100468623], 
processed observation next is [0.0, 0.7391304347826086, 0.6524486571879939, 0.6633333333333333, 1.0, 1.0, 0.4343498852051629, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2059224285538192, 0.20592242855381934, 0.2813653881296452], 
reward next is 0.7186, 
noisyNet noise sample is [array([2.039611], dtype=float32), -0.1898256]. 
=============================================
[2019-03-26 10:24:26,888] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0300230e-28 1.0000000e+00 3.0243188e-22 7.1092412e-22 6.7154208e-17], sum to 1.0000
[2019-03-26 10:24:26,896] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7064
[2019-03-26 10:24:26,902] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5152865481673516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720040.1315345133, 720040.131534514, 186026.0277832874], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4741800.0000, 
sim time next is 4742400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5164198616967963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721624.3149491311, 721624.3149491318, 186208.848346045], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4173733273455377, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20045119859698085, 0.20045119859698105, 0.2779236542478284], 
reward next is 0.7221, 
noisyNet noise sample is [array([2.4865646], dtype=float32), -0.98470026]. 
=============================================
[2019-03-26 10:24:31,675] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.4491577e-23 1.0000000e+00 1.3659616e-16 9.4192460e-13 5.1478370e-11], sum to 1.0000
[2019-03-26 10:24:31,681] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6291
[2019-03-26 10:24:31,685] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 78.0, 1.0, 2.0, 0.489868158274409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684510.079874771, 684510.079874771, 182023.5516323719], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4836600.0000, 
sim time next is 4837200.0000, 
raw observation next is [27.16666666666667, 78.33333333333333, 1.0, 2.0, 0.4889720010529854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683257.4450206095, 683257.4450206102, 181885.9344141895], 
processed observation next is [1.0, 1.0, 0.4865718799368091, 0.7833333333333333, 1.0, 1.0, 0.38430361572648847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18979373472794708, 0.18979373472794728, 0.2714715439017754], 
reward next is 0.7285, 
noisyNet noise sample is [array([0.97800004], dtype=float32), -0.021282405]. 
=============================================
[2019-03-26 10:24:38,438] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.261213e-25 1.000000e+00 8.281677e-20 5.125022e-14 8.964315e-13], sum to 1.0000
[2019-03-26 10:24:38,452] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8086
[2019-03-26 10:24:38,458] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5064369021369601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707669.8826862844, 707669.882686285, 184610.3440527181], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5103600.0000, 
sim time next is 5104200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5059225387264951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706950.8975243263, 706950.8975243256, 184528.8429603633], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4047259502728856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19637524931231287, 0.19637524931231268, 0.2754161835229303], 
reward next is 0.7246, 
noisyNet noise sample is [array([-0.02324529], dtype=float32), 0.9516278]. 
=============================================
[2019-03-26 10:24:38,506] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.3035053e-15 2.9947615e-01 1.0357438e-07 9.2908259e-09 7.0052373e-01], sum to 1.0000
[2019-03-26 10:24:38,518] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9811
[2019-03-26 10:24:38,523] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 67.33333333333334, 1.0, 2.0, 0.4557716760987573, 1.0, 2.0, 0.4557716760987573, 1.0, 2.0, 0.7803993578500438, 6.911199999999999, 6.9112, 170.5573041426782, 1911684.861578313, 1911684.861578314, 384086.8347008958], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4958400.0000, 
sim time next is 4959000.0000, 
raw observation next is [30.0, 68.0, 1.0, 2.0, 0.4426774008934782, 1.0, 2.0, 0.4426774008934782, 1.0, 2.0, 0.7589973691836622, 6.9112, 6.9112, 170.5573041426782, 1856714.756006596, 1856714.756006596, 376222.4091863632], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.68, 1.0, 1.0, 0.3285269890282869, 1.0, 1.0, 0.3285269890282869, 1.0, 1.0, 0.7060943526630027, 0.0, 0.0, 0.8375144448122397, 0.5157540988907211, 0.5157540988907211, 0.5615259838602437], 
reward next is 0.4385, 
noisyNet noise sample is [array([-0.5632177], dtype=float32), 1.3939377]. 
=============================================
[2019-03-26 10:24:38,536] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[43.712322]
 [43.105312]
 [42.85039 ]
 [42.961452]
 [43.414288]], R is [[44.24719238]
 [44.23145676]
 [44.20491791]
 [44.15545273]
 [43.71389771]].
[2019-03-26 10:24:40,159] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6256360e-25 1.0000000e+00 6.1884253e-24 3.9495646e-27 2.4648987e-12], sum to 1.0000
[2019-03-26 10:24:40,165] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1243
[2019-03-26 10:24:40,172] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1792438.205777027 W.
[2019-03-26 10:24:40,176] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.4, 65.0, 1.0, 2.0, 0.4273654114787421, 1.0, 2.0, 0.4273654114787421, 1.0, 1.0, 0.7306984378499861, 6.911200000000001, 6.9112, 170.5573041426782, 1792438.205777027, 1792438.205777026, 366803.6698346287], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4971600.0000, 
sim time next is 4972200.0000, 
raw observation next is [30.43333333333333, 64.83333333333334, 1.0, 2.0, 0.5859674987946151, 1.0, 2.0, 0.5859674987946151, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1638309.252638139, 1638309.252638139, 329122.4714824094], 
processed observation next is [1.0, 0.5652173913043478, 0.6413902053712479, 0.6483333333333334, 1.0, 1.0, 0.5011656611983314, 1.0, 1.0, 0.5011656611983314, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4550859035105942, 0.4550859035105942, 0.49122756937673046], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.04372294], dtype=float32), 1.1989545]. 
=============================================
[2019-03-26 10:24:40,315] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4575160e-22 1.0000000e+00 1.8811269e-21 6.0569289e-26 1.1218257e-12], sum to 1.0000
[2019-03-26 10:24:40,324] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3255
[2019-03-26 10:24:40,329] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2194855.378293303 W.
[2019-03-26 10:24:40,332] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666667, 63.66666666666667, 1.0, 2.0, 0.7848214340739412, 1.0, 1.0, 0.7848214340739412, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2194855.378293303, 2194855.378293303, 412542.678554747], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4976400.0000, 
sim time next is 4977000.0000, 
raw observation next is [30.7, 63.5, 1.0, 2.0, 0.5285133328501294, 1.0, 2.0, 0.5285133328501294, 1.0, 1.0, 0.9056025970439899, 6.9112, 6.9112, 170.5573041426782, 2217106.921171057, 2217106.921171057, 433054.8097462632], 
processed observation next is [1.0, 0.6086956521739131, 0.6540284360189573, 0.635, 1.0, 1.0, 0.43194377451822813, 1.0, 1.0, 0.43194377451822813, 1.0, 0.5, 0.8848812159073046, 0.0, 0.0, 0.8375144448122397, 0.6158630336586269, 0.6158630336586269, 0.6463504623078555], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.68670297], dtype=float32), -0.7976106]. 
=============================================
[2019-03-26 10:24:40,354] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[43.032337]
 [43.246468]
 [44.178547]
 [44.626015]
 [45.33449 ]], R is [[42.2634697 ]
 [41.84083557]
 [41.42242813]
 [41.00820541]
 [40.66046906]].
[2019-03-26 10:24:40,940] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.5647464e-32 1.0000000e+00 1.3010868e-28 1.2772993e-36 2.5301359e-21], sum to 1.0000
[2019-03-26 10:24:40,946] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3742
[2019-03-26 10:24:40,949] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 72.66666666666666, 1.0, 2.0, 0.5178451930211949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723616.6939220176, 723616.693922017, 186440.0421537165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4992000.0000, 
sim time next is 4992600.0000, 
raw observation next is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.5169787342217955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722405.5264565716, 722405.5264565722, 186299.7380690597], 
processed observation next is [1.0, 0.782608695652174, 0.581358609794629, 0.7333333333333334, 1.0, 1.0, 0.41804666773710303, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20066820179349212, 0.2006682017934923, 0.2780593105508354], 
reward next is 0.7219, 
noisyNet noise sample is [array([0.03735419], dtype=float32), -1.2392212]. 
=============================================
[2019-03-26 10:24:41,901] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9815467e-33 1.0000000e+00 2.6603560e-27 1.3165769e-24 6.1291461e-18], sum to 1.0000
[2019-03-26 10:24:41,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9783
[2019-03-26 10:24:41,917] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5190113034278284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725246.7274278646, 725246.7274278653, 186628.4037743669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4996200.0000, 
sim time next is 4996800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5169605399671015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722380.0938837458, 722380.0938837464, 186296.2021665199], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.418024746948315, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2006611371899294, 0.20066113718992956, 0.27805403308435805], 
reward next is 0.7219, 
noisyNet noise sample is [array([0.6920432], dtype=float32), -0.7022305]. 
=============================================
[2019-03-26 10:24:42,185] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.7210338e-28 1.0000000e+00 4.3352735e-24 3.1638854e-25 1.2470483e-15], sum to 1.0000
[2019-03-26 10:24:42,195] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4275
[2019-03-26 10:24:42,200] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.41666666666667, 84.0, 1.0, 2.0, 0.4949626448588925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691631.1033505264, 691631.103350527, 182810.1856873518], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5011800.0000, 
sim time next is 5012400.0000, 
raw observation next is [26.33333333333334, 84.0, 1.0, 2.0, 0.4915962566191648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 686925.5934740766, 686925.5934740772, 182289.4524074868], 
processed observation next is [0.0, 0.0, 0.44707740916271754, 0.84, 1.0, 1.0, 0.3874653694206805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19081266485391016, 0.19081266485391032, 0.27207380956341315], 
reward next is 0.7279, 
noisyNet noise sample is [array([-0.12890495], dtype=float32), -0.11169583]. 
=============================================
[2019-03-26 10:24:47,589] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5620087e-31 1.0000000e+00 1.2492779e-28 1.1839190e-24 1.0064405e-23], sum to 1.0000
[2019-03-26 10:24:47,599] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1237
[2019-03-26 10:24:47,604] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5170087466958854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722447.478955866, 722447.478955866, 186303.4505701619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5095200.0000, 
sim time next is 5095800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5164354593231327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721646.1178474353, 721646.1178474353, 186210.8231642146], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4173921196664249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20045725495762093, 0.20045725495762093, 0.2779266017376337], 
reward next is 0.7221, 
noisyNet noise sample is [array([1.9051421], dtype=float32), 0.61108327]. 
=============================================
[2019-03-26 10:24:48,132] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.0708861e-31 1.0000000e+00 1.1901168e-26 3.7967814e-27 2.6083183e-23], sum to 1.0000
[2019-03-26 10:24:48,141] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4964
[2019-03-26 10:24:48,145] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4804699605607315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671499.0081530006, 671499.0081530006, 180608.3478709486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5119200.0000, 
sim time next is 5119800.0000, 
raw observation next is [26.16666666666667, 84.0, 1.0, 2.0, 0.4833168706571978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675352.8243392322, 675352.8243392328, 181023.1087109265], 
processed observation next is [0.0, 0.2608695652173913, 0.4391785150078992, 0.84, 1.0, 1.0, 0.3774902056110817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18759800676089783, 0.187598006760898, 0.2701837443446664], 
reward next is 0.7298, 
noisyNet noise sample is [array([-0.70553464], dtype=float32), -0.49631104]. 
=============================================
[2019-03-26 10:24:49,552] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.909735e-35 1.000000e+00 9.101760e-33 2.378047e-30 1.688747e-30], sum to 1.0000
[2019-03-26 10:24:49,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1601
[2019-03-26 10:24:49,562] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333333, 66.33333333333333, 1.0, 2.0, 0.5587821854353346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780841.4773765682, 780841.4773765688, 193319.4404316455], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5140200.0000, 
sim time next is 5140800.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5682425839325336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794066.3670615365, 794066.3670615365, 194979.8072214978], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.4798103420873898, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2205739908504268, 0.2205739908504268, 0.2910146376440266], 
reward next is 0.7090, 
noisyNet noise sample is [array([2.2675865], dtype=float32), -0.9201453]. 
=============================================
[2019-03-26 10:24:50,757] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 10:24:50,761] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:24:50,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:24:50,762] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:24:50,764] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:24:50,764] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:24:50,765] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:24:50,765] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:24:50,768] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:24:50,767] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:24:50,770] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:24:50,780] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run15
[2019-03-26 10:24:50,782] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run15
[2019-03-26 10:24:50,798] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run15
[2019-03-26 10:24:50,799] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run15
[2019-03-26 10:24:50,856] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run15
[2019-03-26 10:25:53,906] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.3131324], dtype=float32), 0.25171494]
[2019-03-26 10:25:53,907] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.65, 43.0, 1.0, 2.0, 0.7273100313530936, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.99320529894919, 6.9112, 168.9124026914794, 1913345.883981978, 1855168.693250426, 391166.9535452088]
[2019-03-26 10:25:53,907] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:25:53,910] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1677391e-26 1.0000000e+00 2.8802697e-23 5.8619819e-19 9.4538744e-17], sampled 0.37749495878684325
[2019-03-26 10:25:53,911] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1913345.883981978 W.
[2019-03-26 10:25:54,816] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.3131324], dtype=float32), 0.25171494]
[2019-03-26 10:25:54,819] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.79202285666667, 87.22741167000001, 1.0, 2.0, 0.9274564842513883, 1.0, 2.0, 0.9274564842513883, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 172.86236624, 2594134.089480628, 2594134.089480629, 487174.3562331138]
[2019-03-26 10:25:54,820] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:25:54,822] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8227529e-23 1.0000000e+00 7.2052359e-22 8.3124097e-20 1.2211754e-15], sampled 0.22717878075381193
[2019-03-26 10:25:54,823] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2594134.089480628 W.
[2019-03-26 10:25:58,366] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.3131324], dtype=float32), 0.25171494]
[2019-03-26 10:25:58,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.03333333333333, 67.0, 1.0, 2.0, 0.9131353452604261, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.977107073930657, 6.9112, 168.9113465312551, 2173422.524214172, 2126666.22327921, 438732.1897425741]
[2019-03-26 10:25:58,370] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:25:58,373] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7350073e-27 1.0000000e+00 2.8621852e-24 1.3195669e-19 6.1542082e-20], sampled 0.7006199820881087
[2019-03-26 10:25:58,373] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2173422.524214172 W.
[2019-03-26 10:26:46,845] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-26 10:26:47,233] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-26 10:26:47,256] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-26 10:26:47,274] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-26 10:26:47,422] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164139523.4469 1778.0000
[2019-03-26 10:26:48,436] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 350000, evaluation results [350000.0, 7883.41881391125, 3164139523.4469013, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-26 10:26:50,744] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0809794e-24 1.0000000e+00 2.1042413e-20 7.8318766e-20 1.0642660e-17], sum to 1.0000
[2019-03-26 10:26:50,750] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8966
[2019-03-26 10:26:50,754] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.9678544334632159, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129563937192, 1352842.616671975, 1352842.616671974, 289282.052854442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5194800.0000, 
sim time next is 5195400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.9637875567630984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104018, 1347154.431806704, 1347154.431806705, 288084.0860321596], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.9563705503169859, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451521594, 0.3742095643907511, 0.37420956439075137, 0.42997624780919347], 
reward next is 0.5700, 
noisyNet noise sample is [array([0.39669478], dtype=float32), -2.1756625]. 
=============================================
[2019-03-26 10:26:53,620] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.5295355e-34 1.0000000e+00 6.4344548e-31 1.0634381e-32 6.9535210e-28], sum to 1.0000
[2019-03-26 10:26:53,629] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6613
[2019-03-26 10:26:53,633] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.76666666666667, 79.16666666666667, 1.0, 2.0, 0.5478126691939763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765507.1664619223, 765507.1664619229, 191426.2368811697], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5256600.0000, 
sim time next is 5257200.0000, 
raw observation next is [28.73333333333333, 79.33333333333334, 1.0, 2.0, 0.5484807282593991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766441.0413705155, 766441.0413705155, 191540.3767640373], 
processed observation next is [1.0, 0.8695652173913043, 0.560821484992101, 0.7933333333333334, 1.0, 1.0, 0.45600087742096274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21290028926958762, 0.21290028926958762, 0.2858811593493094], 
reward next is 0.7141, 
noisyNet noise sample is [array([-1.1221792], dtype=float32), 0.9882886]. 
=============================================
[2019-03-26 10:27:00,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 3.8638161e-35 2.8128323e-36 8.8913640e-32], sum to 1.0000
[2019-03-26 10:27:00,325] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6706
[2019-03-26 10:27:00,332] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.1, 78.0, 1.0, 2.0, 0.5793145604886455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809544.3246782325, 809544.3246782325, 196956.0427698605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5515200.0000, 
sim time next is 5515800.0000, 
raw observation next is [29.96666666666667, 78.5, 1.0, 2.0, 0.5822848562401479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813696.6579454262, 813696.6579454262, 197492.1938883586], 
processed observation next is [1.0, 0.8695652173913043, 0.6192733017377569, 0.785, 1.0, 1.0, 0.4967287424580095, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22602684942928505, 0.22602684942928505, 0.2947644684900875], 
reward next is 0.7052, 
noisyNet noise sample is [array([0.8677095], dtype=float32), -1.6226816]. 
=============================================
[2019-03-26 10:27:11,135] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5715054e-19 9.9999821e-01 4.7267620e-14 1.8409713e-06 2.9342226e-08], sum to 1.0000
[2019-03-26 10:27:11,141] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0117
[2019-03-26 10:27:11,149] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 80.83333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.981440663791823, 6.9112, 168.9124306376941, 1503620.042500017, 1453789.053247153, 311352.4929811025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5559000.0000, 
sim time next is 5559600.0000, 
raw observation next is [29.2, 79.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.345313430236915, 6.9112, 168.9047713947167, 2471784.636362526, 1454424.511791837, 310804.4880376042], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.7966666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.14341134302369155, 0.0, 0.8293997524869833, 0.686606843434035, 0.4040068088310658, 0.4638872955785137], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.0794837], dtype=float32), -0.24136207]. 
=============================================
[2019-03-26 10:27:19,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7494101e-31 1.0000000e+00 3.5249566e-31 1.4365672e-20 2.7833165e-23], sum to 1.0000
[2019-03-26 10:27:19,150] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6704
[2019-03-26 10:27:19,165] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 87.0, 1.0, 2.0, 0.514571793229529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719041.0244809346, 719041.0244809346, 185910.3212599573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5701200.0000, 
sim time next is 5701800.0000, 
raw observation next is [26.51666666666667, 87.0, 1.0, 2.0, 0.5142109877555255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718536.6795371415, 718536.6795371415, 185852.2360275718], 
processed observation next is [0.0, 1.0, 0.45576619273301755, 0.87, 1.0, 1.0, 0.4147120334403922, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19959352209365042, 0.19959352209365042, 0.27739139705607735], 
reward next is 0.7226, 
noisyNet noise sample is [array([0.9621555], dtype=float32), -0.040373597]. 
=============================================
[2019-03-26 10:27:34,441] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.2127884e-24 1.0000000e+00 7.8481717e-21 6.1117417e-10 1.1729890e-08], sum to 1.0000
[2019-03-26 10:27:34,453] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2288
[2019-03-26 10:27:34,460] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 82.83333333333334, 1.0, 2.0, 0.5585933127316969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780577.4499351569, 780577.4499351569, 193285.6528329755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5946600.0000, 
sim time next is 5947200.0000, 
raw observation next is [28.5, 83.0, 1.0, 2.0, 0.5555106039364517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776268.1032273677, 776268.1032273677, 192750.558847794], 
processed observation next is [1.0, 0.8695652173913043, 0.5497630331753555, 0.83, 1.0, 1.0, 0.4644706071523514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2156300286742688, 0.2156300286742688, 0.2876874012653642], 
reward next is 0.7123, 
noisyNet noise sample is [array([0.1020552], dtype=float32), 0.019476777]. 
=============================================
[2019-03-26 10:27:38,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0277226e-22 9.9996698e-01 3.9242803e-19 9.8723851e-10 3.3005421e-05], sum to 1.0000
[2019-03-26 10:27:38,834] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0195
[2019-03-26 10:27:38,841] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 90.83333333333334, 1.0, 2.0, 0.5375517004343331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 751163.5360147606, 751163.5360147613, 189688.7101272195], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6047400.0000, 
sim time next is 6048000.0000, 
raw observation next is [26.7, 91.0, 1.0, 2.0, 0.5373788489273366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750921.9115468428, 750921.9115468435, 189659.692891892], 
processed observation next is [1.0, 0.0, 0.46445497630331756, 0.91, 1.0, 1.0, 0.4426251191895621, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.208589419874123, 0.2085894198741232, 0.28307416849536116], 
reward next is 0.7169, 
noisyNet noise sample is [array([-0.7230478], dtype=float32), -0.13144076]. 
=============================================
[2019-03-26 10:27:38,860] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.3403  ]
 [75.3589  ]
 [75.38067 ]
 [75.41392 ]
 [75.455414]], R is [[73.62416077]
 [73.60479736]
 [73.58553314]
 [73.56630707]
 [73.54715729]].
[2019-03-26 10:27:39,910] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2746850e-23 9.9999988e-01 2.8026397e-18 1.0223655e-12 1.0739163e-07], sum to 1.0000
[2019-03-26 10:27:39,917] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3214
[2019-03-26 10:27:39,922] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.58333333333334, 84.5, 1.0, 2.0, 0.530354557545269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741102.8902399419, 741102.8902399419, 188488.9550259797], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6205800.0000, 
sim time next is 6206400.0000, 
raw observation next is [27.5, 85.0, 1.0, 2.0, 0.5310323353892278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742050.3291968158, 742050.3291968158, 188601.2464162806], 
processed observation next is [1.0, 0.8695652173913043, 0.5023696682464456, 0.85, 1.0, 1.0, 0.43497871733641896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20612509144355995, 0.20612509144355995, 0.2814943976362397], 
reward next is 0.7185, 
noisyNet noise sample is [array([0.04174015], dtype=float32), -0.69595635]. 
=============================================
[2019-03-26 10:27:42,171] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 10:27:42,174] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:27:42,174] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:27:42,175] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:27:42,176] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:27:42,177] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:27:42,178] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:27:42,179] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:27:42,180] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:27:42,179] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:27:42,182] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:27:42,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run16
[2019-03-26 10:27:42,219] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run16
[2019-03-26 10:27:42,234] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run16
[2019-03-26 10:27:42,237] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run16
[2019-03-26 10:27:42,253] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run16
[2019-03-26 10:28:04,029] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.306441], dtype=float32), 0.28665203]
[2019-03-26 10:28:04,030] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 96.0, 1.0, 2.0, 0.3584876783872445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549730.4218779436, 549730.4218779436, 170551.8880829734]
[2019-03-26 10:28:04,031] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:28:04,034] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7777009e-23 1.0000000e+00 2.8674526e-24 6.9649488e-17 2.6919565e-16], sampled 0.9195970221637653
[2019-03-26 10:28:17,287] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.306441], dtype=float32), 0.28665203]
[2019-03-26 10:28:17,288] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.82885817333333, 93.22549245, 1.0, 2.0, 0.6337743432171941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 885679.1103483381, 885679.1103483376, 207206.7198401925]
[2019-03-26 10:28:17,288] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:28:17,291] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2111814e-20 1.0000000e+00 1.1582890e-19 5.5580600e-13 1.0103998e-10], sampled 0.06172776617049969
[2019-03-26 10:28:49,516] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.306441], dtype=float32), 0.28665203]
[2019-03-26 10:28:49,517] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.56666666666666, 42.66666666666667, 1.0, 2.0, 0.6852259701553602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 957613.507860989, 957613.5078609884, 217691.594827131]
[2019-03-26 10:28:49,518] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:28:49,524] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.33821086e-20 1.00000000e+00 1.55341288e-19 2.50893897e-13
 1.76664028e-09], sampled 0.8952471416547417
[2019-03-26 10:28:56,723] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.306441], dtype=float32), 0.28665203]
[2019-03-26 10:28:56,725] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.56666666666666, 82.66666666666667, 1.0, 2.0, 0.6034953434520491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843348.3898119255, 843348.3898119255, 201398.3682908799]
[2019-03-26 10:28:56,727] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:28:56,732] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.861854e-21 1.000000e+00 2.190112e-19 9.769311e-14 8.028647e-10], sampled 0.717217664909264
[2019-03-26 10:29:14,876] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.306441], dtype=float32), 0.28665203]
[2019-03-26 10:29:14,876] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 0.5402909341979382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754992.6444811684, 754992.6444811678, 190149.8576792844]
[2019-03-26 10:29:14,878] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:29:14,882] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2923409e-21 1.0000000e+00 6.5395070e-21 8.0117848e-15 9.0196826e-12], sampled 0.505190938919163
[2019-03-26 10:29:38,130] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8019.6783 3160590342.2633 1441.0000
[2019-03-26 10:29:38,275] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8748.9796 2780739030.7397 714.0000
[2019-03-26 10:29:38,686] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8353.7406 2930021091.9704 1129.0000
[2019-03-26 10:29:38,728] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8612.6480 2840882615.7557 854.0000
[2019-03-26 10:29:38,769] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8135.6295 3007108358.8754 1452.0000
[2019-03-26 10:29:39,784] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 375000, evaluation results [375000.0, 8019.678298241214, 3160590342.2633348, 1441.0, 8353.740645026266, 2930021091.9704003, 1129.0, 8748.979552995632, 2780739030.739665, 714.0, 8135.62954681801, 3007108358.8753777, 1452.0, 8612.647960839819, 2840882615.7556906, 854.0]
[2019-03-26 10:29:40,993] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2079208e-29 1.0000000e+00 7.7751366e-28 2.7783366e-19 1.0656407e-14], sum to 1.0000
[2019-03-26 10:29:40,999] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3180
[2019-03-26 10:29:41,007] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 86.0, 1.0, 2.0, 0.5259561978209946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734954.6169261284, 734954.616926129, 187762.9799009111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6120000.0000, 
sim time next is 6120600.0000, 
raw observation next is [27.2, 86.0, 1.0, 2.0, 0.5261270172167318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735193.3971707496, 735193.3971707496, 187791.0509652534], 
processed observation next is [1.0, 0.8695652173913043, 0.4881516587677725, 0.86, 1.0, 1.0, 0.42906869544184556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.204220388102986, 0.204220388102986, 0.28028515069440807], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.36565587], dtype=float32), -1.8625282]. 
=============================================
[2019-03-26 10:29:44,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6327167e-31 1.0000000e+00 4.4998088e-32 5.4252802e-32 8.0106883e-29], sum to 1.0000
[2019-03-26 10:29:44,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6098
[2019-03-26 10:29:44,842] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 84.0, 1.0, 2.0, 0.5296840628485503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740165.6329392651, 740165.6329392645, 188378.0031029248], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6205200.0000, 
sim time next is 6205800.0000, 
raw observation next is [27.58333333333334, 84.5, 1.0, 2.0, 0.530354557545269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741102.8902399419, 741102.8902399419, 188488.9550259797], 
processed observation next is [1.0, 0.8260869565217391, 0.506319115323855, 0.845, 1.0, 1.0, 0.4341621175244204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2058619139555394, 0.2058619139555394, 0.2813267985462384], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.8452926], dtype=float32), 0.123026624]. 
=============================================
[2019-03-26 10:29:44,945] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3811899e-21 1.0000000e+00 3.9259482e-21 8.9052358e-27 3.7395486e-17], sum to 1.0000
[2019-03-26 10:29:44,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8239
[2019-03-26 10:29:44,959] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2292894.214698514 W.
[2019-03-26 10:29:44,965] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.1, 71.33333333333334, 1.0, 2.0, 0.9984940750120048, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.993841001936248, 6.9112, 168.9124652099925, 2292894.214698514, 2234266.014167717, 463300.4575800509], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6189000.0000, 
sim time next is 6189600.0000, 
raw observation next is [30.0, 71.66666666666667, 1.0, 2.0, 0.8305502654721372, 1.0, 1.0, 0.8305502654721372, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2322863.614103404, 2322863.614103404, 435015.839368542], 
processed observation next is [1.0, 0.6521739130434783, 0.6208530805687204, 0.7166666666666667, 1.0, 1.0, 0.7958436933399243, 1.0, 0.5, 0.7958436933399243, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6452398928065011, 0.6452398928065011, 0.6492773721918538], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7100003], dtype=float32), 1.301244]. 
=============================================
[2019-03-26 10:29:48,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.6473417e-32 6.0302137e-32], sum to 1.0000
[2019-03-26 10:29:48,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5619
[2019-03-26 10:29:48,110] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666666, 67.66666666666667, 1.0, 2.0, 0.5385036327451984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752494.2175479198, 752494.2175479204, 189849.0417139439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6262800.0000, 
sim time next is 6263400.0000, 
raw observation next is [30.68333333333333, 67.33333333333333, 1.0, 2.0, 0.5371825052149364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750647.4480241525, 750647.448024152, 189627.2052148219], 
processed observation next is [0.0, 0.4782608695652174, 0.6532385466034754, 0.6733333333333333, 1.0, 1.0, 0.4423885604999233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20851318000670904, 0.20851318000670888, 0.2830256794251073], 
reward next is 0.7170, 
noisyNet noise sample is [array([-0.65895975], dtype=float32), -0.56105095]. 
=============================================
[2019-03-26 10:29:53,711] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1229407e-37 1.0000000e+00 9.0102359e-36 3.7341895e-24 7.3029149e-30], sum to 1.0000
[2019-03-26 10:29:53,724] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4468
[2019-03-26 10:29:53,731] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333333, 63.5, 1.0, 2.0, 0.547774198161312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765453.388110173, 765453.388110173, 191420.3098841449], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6349800.0000, 
sim time next is 6350400.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5494122160263172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 767743.162715467, 767743.1627154663, 191700.5889659435], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.63, 1.0, 1.0, 0.4571231518389363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2132619896431853, 0.2132619896431851, 0.28612028203872164], 
reward next is 0.7139, 
noisyNet noise sample is [array([-0.03153544], dtype=float32), 0.82250315]. 
=============================================
[2019-03-26 10:29:53,853] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.13789855e-33 1.00000000e+00 4.90711035e-33 1.81128972e-24
 5.77011422e-25], sum to 1.0000
[2019-03-26 10:29:53,854] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4085
[2019-03-26 10:29:53,861] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.96666666666667, 70.5, 1.0, 2.0, 0.5333704921153345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745318.7528925957, 745318.7528925957, 188990.0026327585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6342600.0000, 
sim time next is 6343200.0000, 
raw observation next is [30.1, 70.0, 1.0, 2.0, 0.5352173682689337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747900.4405693329, 747900.4405693322, 189298.1262683131], 
processed observation next is [0.0, 0.43478260869565216, 0.6255924170616115, 0.7, 1.0, 1.0, 0.4400209256252213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20775012238037024, 0.20775012238037005, 0.28253451681837777], 
reward next is 0.7175, 
noisyNet noise sample is [array([-0.8395182], dtype=float32), -0.10556919]. 
=============================================
[2019-03-26 10:30:00,130] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2499675e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9816509e-35], sum to 1.0000
[2019-03-26 10:30:00,141] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8797
[2019-03-26 10:30:00,146] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.58333333333334, 73.5, 1.0, 2.0, 0.5075902849713679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709282.1008905068, 709282.1008905062, 184793.8456243891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6462600.0000, 
sim time next is 6463200.0000, 
raw observation next is [28.46666666666667, 74.0, 1.0, 2.0, 0.5057182101791148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706665.2840431288, 706665.2840431288, 184496.881369619], 
processed observation next is [1.0, 0.8260869565217391, 0.5481832543443919, 0.74, 1.0, 1.0, 0.40447977130013824, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19629591223420245, 0.19629591223420245, 0.27536847965614775], 
reward next is 0.7246, 
noisyNet noise sample is [array([-0.0056074], dtype=float32), -0.40195936]. 
=============================================
[2019-03-26 10:30:00,924] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.3058037e-38 1.0000000e+00 0.0000000e+00 4.4161820e-37 1.2961672e-36], sum to 1.0000
[2019-03-26 10:30:00,935] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4268
[2019-03-26 10:30:00,942] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 79.33333333333334, 1.0, 2.0, 0.508538398243554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710607.3916309181, 710607.3916309188, 184944.8659699106], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6468600.0000, 
sim time next is 6469200.0000, 
raw observation next is [27.7, 80.0, 1.0, 2.0, 0.5109191848185395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713935.3076152625, 713935.3076152619, 185324.7263236895], 
processed observation next is [1.0, 0.9130434782608695, 0.5118483412322274, 0.8, 1.0, 1.0, 0.4107460058054692, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1983153632264618, 0.19831536322646162, 0.2766040691398351], 
reward next is 0.7234, 
noisyNet noise sample is [array([-0.35074493], dtype=float32), 0.5869395]. 
=============================================
[2019-03-26 10:30:11,419] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.8665574e-36], sum to 1.0000
[2019-03-26 10:30:11,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6496
[2019-03-26 10:30:11,434] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 95.0, 1.0, 2.0, 0.6174718364561248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862887.6307443506, 862887.6307443506, 204037.042439798], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6665400.0000, 
sim time next is 6666000.0000, 
raw observation next is [24.83333333333333, 95.0, 1.0, 2.0, 0.5671261544422777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 792505.6782678583, 792505.6782678589, 194775.735778482], 
processed observation next is [1.0, 0.13043478260869565, 0.3759873617693521, 0.95, 1.0, 1.0, 0.47846524631599713, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2201404661855162, 0.22014046618551636, 0.29071005340071937], 
reward next is 0.7093, 
noisyNet noise sample is [array([-0.21332207], dtype=float32), -0.5011481]. 
=============================================
[2019-03-26 10:30:11,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.307274]
 [69.42075 ]
 [69.53079 ]
 [69.49402 ]
 [69.34343 ]], R is [[69.44831085]
 [69.44929504]
 [69.45379639]
 [69.45573425]
 [69.46411133]].
[2019-03-26 10:30:13,599] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9222689e-29 1.0000000e+00 5.2919729e-30 1.6953954e-30 2.3371616e-24], sum to 1.0000
[2019-03-26 10:30:13,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9735
[2019-03-26 10:30:13,611] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 84.0, 1.0, 2.0, 1.023733988689424, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9127400751879, 1431002.300741326, 1431002.300741325, 306239.4205479156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6685200.0000, 
sim time next is 6685800.0000, 
raw observation next is [27.36666666666667, 83.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.420743729793116, 6.9112, 168.9104721418153, 1815485.057264405, 1454002.514964126, 311349.2609241601], 
processed observation next is [1.0, 0.391304347826087, 0.49605055292259104, 0.83, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.05095437297931156, 0.0, 0.8294277457650506, 0.5043014047956681, 0.403889587490035, 0.4647003894390449], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46236193], dtype=float32), -0.5181675]. 
=============================================
[2019-03-26 10:30:14,309] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.1879879e-36 0.0000000e+00 4.7965470e-33], sum to 1.0000
[2019-03-26 10:30:14,319] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7871
[2019-03-26 10:30:14,326] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2249113.076035518 W.
[2019-03-26 10:30:14,332] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 62.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.03159548825477, 6.9112, 168.9070445953937, 2249113.076035518, 1454293.775099188, 311347.5224892332], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6700800.0000, 
sim time next is 6701400.0000, 
raw observation next is [30.05, 62.0, 1.0, 2.0, 0.7290629550562188, 1.0, 1.0, 0.7290629550562188, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2038771.134679176, 2038771.134679176, 386803.0160921653], 
processed observation next is [1.0, 0.5652173913043478, 0.6232227488151659, 0.62, 1.0, 1.0, 0.6735698253689383, 1.0, 0.5, 0.6735698253689383, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.56632531518866, 0.56632531518866, 0.5773179344659184], 
reward next is 0.4227, 
noisyNet noise sample is [array([-0.02703567], dtype=float32), 1.0084797]. 
=============================================
[2019-03-26 10:30:22,980] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.141570e-38 1.000000e+00 0.000000e+00 2.548152e-36 4.404880e-35], sum to 1.0000
[2019-03-26 10:30:22,983] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5978
[2019-03-26 10:30:22,989] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 81.0, 1.0, 2.0, 0.3605851860763643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551974.9941587888, 551974.9941587888, 170712.0752082941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6849000.0000, 
sim time next is 6849600.0000, 
raw observation next is [24.06666666666667, 80.66666666666666, 1.0, 2.0, 0.3633083576297908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555019.5417307054, 555019.5417307054, 170936.3612040957], 
processed observation next is [0.0, 0.2608695652173913, 0.3396524486571882, 0.8066666666666665, 1.0, 1.0, 0.23290163569854314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15417209492519596, 0.15417209492519596, 0.25512889731954586], 
reward next is 0.7449, 
noisyNet noise sample is [array([-1.0725248], dtype=float32), -1.3171014]. 
=============================================
[2019-03-26 10:30:25,530] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3881140e-32 1.0000000e+00 1.6826933e-33 1.5277411e-37 4.5149662e-28], sum to 1.0000
[2019-03-26 10:30:25,538] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7175
[2019-03-26 10:30:25,547] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1978312.114872754 W.
[2019-03-26 10:30:25,550] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 55.0, 1.0, 2.0, 0.4716418672707368, 1.0, 1.0, 0.4716418672707368, 1.0, 2.0, 0.788319145934239, 6.9112, 6.9112, 170.5573041426782, 1978312.114872754, 1978312.114872754, 390719.6056358704], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7053600.0000, 
sim time next is 7054200.0000, 
raw observation next is [30.3, 56.5, 1.0, 2.0, 0.4835020379664037, 1.0, 2.0, 0.4835020379664037, 1.0, 2.0, 0.8101086723715926, 6.9112, 6.9112, 170.5573041426782, 2028106.948820502, 2028106.948820502, 398631.5898308376], 
processed observation next is [1.0, 0.6521739130434783, 0.6350710900473934, 0.565, 1.0, 1.0, 0.3777132987547032, 1.0, 1.0, 0.3777132987547032, 1.0, 1.0, 0.7684252102092591, 0.0, 0.0, 0.8375144448122397, 0.5633630413390284, 0.5633630413390284, 0.5949725221355785], 
reward next is 0.4050, 
noisyNet noise sample is [array([-0.28916457], dtype=float32), 0.050703585]. 
=============================================
[2019-03-26 10:30:32,908] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8793025e-38 1.0000000e+00 4.6090454e-34 0.0000000e+00 5.3648563e-32], sum to 1.0000
[2019-03-26 10:30:32,917] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3961
[2019-03-26 10:30:32,922] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 86.33333333333334, 1.0, 2.0, 0.6385519128081782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 908463.525086581, 908463.5250865803, 210248.3359778945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7013400.0000, 
sim time next is 7014000.0000, 
raw observation next is [25.2, 86.66666666666667, 1.0, 2.0, 0.6158495117229907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876285.895155288, 876285.895155288, 205765.4420471193], 
processed observation next is [1.0, 0.17391304347826086, 0.3933649289099526, 0.8666666666666667, 1.0, 1.0, 0.5371680864132419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24341274865424667, 0.24341274865424667, 0.3071126000703273], 
reward next is 0.6929, 
noisyNet noise sample is [array([-0.8435789], dtype=float32), 0.555593]. 
=============================================
[2019-03-26 10:30:32,935] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[66.20982 ]
 [66.09737 ]
 [66.10406 ]
 [66.011314]
 [65.86018 ]], R is [[66.12115479]
 [66.14614105]
 [66.16584015]
 [66.18201447]
 [66.1940155 ]].
[2019-03-26 10:30:33,772] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-26 10:30:33,773] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:30:33,775] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:30:33,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:30:33,776] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:30:33,777] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:30:33,775] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:30:33,779] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:30:33,777] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:30:33,782] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:30:33,782] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:30:33,792] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run17
[2019-03-26 10:30:33,793] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run17
[2019-03-26 10:30:33,809] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run17
[2019-03-26 10:30:33,859] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run17
[2019-03-26 10:30:33,876] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run17
[2019-03-26 10:30:35,058] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.244399], dtype=float32), 0.2646349]
[2019-03-26 10:30:35,060] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.1, 79.0, 1.0, 2.0, 0.3966604834615329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591116.8206663651, 591116.8206663651, 173697.4086328887]
[2019-03-26 10:30:35,060] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:30:35,063] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.8542878e-35 1.0000000e+00 3.1333478e-36 7.5382594e-33 2.3379659e-30], sampled 0.8085969907305293
[2019-03-26 10:31:07,529] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.244399], dtype=float32), 0.2646349]
[2019-03-26 10:31:07,531] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.2, 92.0, 1.0, 2.0, 0.388866770301778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 581283.4491554138, 581283.4491554132, 172856.0834764151]
[2019-03-26 10:31:07,533] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:31:07,537] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3949532e-34 1.0000000e+00 8.6694889e-36 2.5482155e-32 2.9294028e-30], sampled 0.11598692333873939
[2019-03-26 10:31:59,423] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.244399], dtype=float32), 0.2646349]
[2019-03-26 10:31:59,424] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.56666666666667, 65.33333333333334, 1.0, 2.0, 0.5808942874133992, 0.0, 2.0, 0.0, 1.0, 2.0, 1.008821272560695, 6.9112, 6.9112, 168.9129520471794, 1624126.269840612, 1624126.269840612, 355558.2578951463]
[2019-03-26 10:31:59,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:31:59,428] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.4463464e-26 1.0000000e+00 6.2844820e-26 7.9381274e-24 1.2872364e-17], sampled 0.8273421524273431
[2019-03-26 10:32:29,279] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9470 2927473933.7898 1338.0000
[2019-03-26 10:32:29,491] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1206 2842575967.7121 1131.0000
[2019-03-26 10:32:29,828] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1701 3164000012.1395 1778.0000
[2019-03-26 10:32:29,847] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2482 2779349048.6670 933.0000
[2019-03-26 10:32:29,862] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2178 3007719775.7786 1766.0000
[2019-03-26 10:32:30,876] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 400000, evaluation results [400000.0, 7884.170123390672, 3164000012.13951, 1778.0, 8252.946952103406, 2927473933.7898183, 1338.0, 8659.248225290772, 2779349048.6670237, 933.0, 7998.217808978373, 3007719775.7785764, 1766.0, 8496.120550445738, 2842575967.7121344, 1131.0]
[2019-03-26 10:32:39,274] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4861356e-30 1.0000000e+00 2.8877171e-31 4.8857100e-31 2.5839407e-27], sum to 1.0000
[2019-03-26 10:32:39,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6081
[2019-03-26 10:32:39,288] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 89.0, 1.0, 2.0, 0.5763088049091502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 805342.4367698091, 805342.4367698084, 196409.3213774699], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7182000.0000, 
sim time next is 7182600.0000, 
raw observation next is [25.8, 89.16666666666667, 1.0, 2.0, 0.6429331572530604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898483.67006563, 898483.67006563, 209006.9241447223], 
processed observation next is [1.0, 0.13043478260869565, 0.42180094786729866, 0.8916666666666667, 1.0, 1.0, 0.5697989846422414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2495787972404528, 0.2495787972404528, 0.3119506330518243], 
reward next is 0.6880, 
noisyNet noise sample is [array([1.8694309], dtype=float32), 0.20254132]. 
=============================================
[2019-03-26 10:32:43,844] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6284501e-38 1.0000000e+00 1.9870914e-38 0.0000000e+00 4.4875114e-35], sum to 1.0000
[2019-03-26 10:32:43,855] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2271
[2019-03-26 10:32:43,860] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.76666666666667, 88.66666666666666, 1.0, 2.0, 0.3197133659820682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504201.505626397, 504201.5056263963, 167313.5342132773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7267200.0000, 
sim time next is 7267800.0000, 
raw observation next is [21.73333333333333, 88.83333333333334, 1.0, 2.0, 0.3181189687245081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501836.8215141516, 501836.8215141523, 167138.3804371663], 
processed observation next is [1.0, 0.08695652173913043, 0.22906793048973137, 0.8883333333333334, 1.0, 1.0, 0.17845658882470858, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13939911708726432, 0.13939911708726452, 0.24946026930920345], 
reward next is 0.7505, 
noisyNet noise sample is [array([-0.07944981], dtype=float32), 1.169006]. 
=============================================
[2019-03-26 10:32:47,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.8204001e-38 1.0000000e+00 2.0259741e-38 0.0000000e+00 7.8796825e-33], sum to 1.0000
[2019-03-26 10:32:47,999] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6226
[2019-03-26 10:32:48,004] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 73.0, 1.0, 2.0, 0.3907919141474569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586274.669622638, 586274.669622638, 173372.4251087159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7329600.0000, 
sim time next is 7330200.0000, 
raw observation next is [25.73333333333333, 73.16666666666667, 1.0, 2.0, 0.3902320579632693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585958.1964709638, 585958.1964709645, 173359.1705183822], 
processed observation next is [1.0, 0.8695652173913043, 0.41864139020537117, 0.7316666666666667, 1.0, 1.0, 0.26533982887140883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16276616568637883, 0.16276616568637903, 0.25874503062445103], 
reward next is 0.7413, 
noisyNet noise sample is [array([1.2717854], dtype=float32), 0.70712733]. 
=============================================
[2019-03-26 10:32:52,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4866850e-37 1.0000000e+00 3.7184206e-34 3.5694285e-34 1.3585208e-30], sum to 1.0000
[2019-03-26 10:32:52,913] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1545
[2019-03-26 10:32:52,919] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 94.66666666666667, 1.0, 2.0, 0.3187621316488384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502701.3248446704, 502701.3248446711, 167200.2811768486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7428000.0000, 
sim time next is 7428600.0000, 
raw observation next is [21.05, 94.5, 1.0, 2.0, 0.3186843401584374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502615.6106929396, 502615.6106929396, 167194.6153053548], 
processed observation next is [1.0, 1.0, 0.1966824644549764, 0.945, 1.0, 1.0, 0.17913775922703298, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13961544741470544, 0.13961544741470544, 0.24954420194829072], 
reward next is 0.7505, 
noisyNet noise sample is [array([1.6887445], dtype=float32), 0.7532057]. 
=============================================
[2019-03-26 10:32:57,748] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6561312e-38], sum to 1.0000
[2019-03-26 10:32:57,757] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9034
[2019-03-26 10:32:57,763] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 77.0, 1.0, 2.0, 0.421005943769674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610719.8940036541, 610719.8940036541, 175038.407498414], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7494000.0000, 
sim time next is 7494600.0000, 
raw observation next is [26.01666666666667, 77.5, 1.0, 2.0, 0.4200347975032969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610208.4788215889, 610208.4788215889, 175016.526801844], 
processed observation next is [0.0, 0.7391304347826086, 0.43206951026856255, 0.775, 1.0, 1.0, 0.30124674397987583, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16950235522821913, 0.16950235522821913, 0.26121869671917014], 
reward next is 0.7388, 
noisyNet noise sample is [array([0.45215175], dtype=float32), 0.54022545]. 
=============================================
[2019-03-26 10:33:02,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.228819e-35], sum to 1.0000
[2019-03-26 10:33:02,577] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6488
[2019-03-26 10:33:02,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2257504.463689976 W.
[2019-03-26 10:33:02,585] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 62.33333333333334, 1.0, 2.0, 0.5381345982729018, 1.0, 2.0, 0.5381345982729018, 1.0, 2.0, 0.917941696476058, 6.9112, 6.9112, 170.5573041426782, 2257504.463689976, 2257504.463689976, 439227.0399961612], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7749600.0000, 
sim time next is 7750200.0000, 
raw observation next is [30.4, 62.66666666666666, 1.0, 2.0, 0.5406818925416296, 1.0, 2.0, 0.5406818925416296, 1.0, 2.0, 0.9220776021228096, 6.911199999999999, 6.9112, 170.5573041426782, 2268200.201465852, 2268200.201465853, 441051.6182823729], 
processed observation next is [1.0, 0.6956521739130435, 0.6398104265402843, 0.6266666666666666, 1.0, 1.0, 0.4466046898091922, 1.0, 1.0, 0.4466046898091922, 1.0, 1.0, 0.9049726855156215, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6300556115182923, 0.6300556115182925, 0.6582859974363774], 
reward next is 0.3417, 
noisyNet noise sample is [array([0.40258434], dtype=float32), -0.061321333]. 
=============================================
[2019-03-26 10:33:05,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6086470e-27 1.0000000e+00 6.8490075e-28 4.2486339e-31 7.4659265e-22], sum to 1.0000
[2019-03-26 10:33:05,653] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1113
[2019-03-26 10:33:05,663] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1683356.413464015 W.
[2019-03-26 10:33:05,672] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.93333333333334, 62.66666666666666, 1.0, 2.0, 0.6020666917899361, 1.0, 2.0, 0.6020666917899361, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1683356.413464015, 1683356.413464015, 335006.5775886615], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7645200.0000, 
sim time next is 7645800.0000, 
raw observation next is [30.01666666666667, 62.33333333333334, 1.0, 2.0, 0.5978809314312237, 0.0, 1.0, 0.0, 1.0, 1.0, 1.00288261450039, 6.911199999999999, 6.9112, 168.9129565104301, 1671656.745354409, 1671656.74535441, 358353.0071962058], 
processed observation next is [1.0, 0.4782608695652174, 0.6216429699842023, 0.6233333333333334, 1.0, 1.0, 0.5155191944954501, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.003515383537061, -8.881784197001253e-17, 0.0, 0.8294399451522982, 0.4643490959317803, 0.4643490959317806, 0.5348552346212027], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5381309], dtype=float32), -0.09416895]. 
=============================================
[2019-03-26 10:33:06,682] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.08371965e-34 1.00000000e+00 4.58542065e-35 0.00000000e+00
 1.11471764e-29], sum to 1.0000
[2019-03-26 10:33:06,693] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1490
[2019-03-26 10:33:06,703] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1893219.436594186 W.
[2019-03-26 10:33:06,712] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.36666666666667, 70.0, 1.0, 2.0, 0.4513731537568988, 1.0, 1.0, 0.4513731537568988, 1.0, 2.0, 0.7684167275091505, 6.9112, 6.9112, 170.5573041426782, 1893219.436594186, 1893219.436594186, 380629.6103066446], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7661400.0000, 
sim time next is 7662000.0000, 
raw observation next is [29.23333333333333, 71.0, 1.0, 2.0, 0.4389354158597649, 1.0, 2.0, 0.4389354158597649, 1.0, 2.0, 0.7505524878777459, 6.911199999999999, 6.9112, 170.5573041426782, 1841006.324284845, 1841006.324284846, 373645.6032527588], 
processed observation next is [1.0, 0.6956521739130435, 0.5845181674565559, 0.71, 1.0, 1.0, 0.32401857332501793, 1.0, 1.0, 0.32401857332501793, 1.0, 1.0, 0.6957957169240804, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5113906456346792, 0.5113906456346794, 0.5576800048548639], 
reward next is 0.4423, 
noisyNet noise sample is [array([0.7407407], dtype=float32), 0.8966574]. 
=============================================
[2019-03-26 10:33:06,728] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[49.660458]
 [48.39712 ]
 [48.593872]
 [47.50232 ]
 [46.87256 ]], R is [[50.61631775]
 [50.1101532 ]
 [49.60905075]
 [49.52423859]
 [49.42131805]].
[2019-03-26 10:33:08,523] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0252652e-36 1.0000000e+00 4.8190269e-35 4.8121823e-32 5.5041720e-28], sum to 1.0000
[2019-03-26 10:33:08,530] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2951
[2019-03-26 10:33:08,536] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 94.5, 1.0, 2.0, 0.47821504555122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668221.6524615595, 668221.6524615588, 180252.9526438478], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7696200.0000, 
sim time next is 7696800.0000, 
raw observation next is [24.6, 95.0, 1.0, 2.0, 0.4779032091172766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 667785.7787807185, 667785.7787807178, 180206.1727179014], 
processed observation next is [1.0, 0.08695652173913043, 0.36492890995260674, 0.95, 1.0, 1.0, 0.37096772182804405, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18549604966131067, 0.18549604966131047, 0.2689644368923902], 
reward next is 0.7310, 
noisyNet noise sample is [array([0.02381445], dtype=float32), 0.5783114]. 
=============================================
[2019-03-26 10:33:15,217] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:15,218] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:15,244] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-03-26 10:33:19,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1698813e-32 1.0000000e+00 5.2407394e-32 3.1575159e-27 3.9662129e-22], sum to 1.0000
[2019-03-26 10:33:19,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4336
[2019-03-26 10:33:19,578] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 86.5, 1.0, 2.0, 0.6033662351347145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 843167.8971018008, 843167.8971018002, 201368.0948255169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7884600.0000, 
sim time next is 7885200.0000, 
raw observation next is [26.73333333333334, 86.0, 1.0, 2.0, 0.6243938726880052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872564.8218607658, 872564.8218607658, 205370.5781021543], 
processed observation next is [1.0, 0.2608695652173913, 0.4660347551342816, 0.86, 1.0, 1.0, 0.547462497214464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24237911718354604, 0.24237911718354604, 0.3065232508987378], 
reward next is 0.6935, 
noisyNet noise sample is [array([-0.5525794], dtype=float32), 1.2392768]. 
=============================================
[2019-03-26 10:33:22,207] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0686980e-29 1.0000000e+00 4.1544638e-29 2.8617142e-25 1.1939765e-20], sum to 1.0000
[2019-03-26 10:33:22,207] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1749
[2019-03-26 10:33:22,209] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 95.83333333333333, 1.0, 2.0, 0.8993225992315843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1340447.958229606, 1340447.958229607, 281729.6384257205], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 129000.0000, 
sim time next is 129600.0000, 
raw observation next is [22.8, 96.0, 1.0, 2.0, 0.9100219829494143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1355436.174520329, 1355436.174520329, 284768.7342746886], 
processed observation next is [1.0, 0.5217391304347826, 0.2796208530805688, 0.96, 1.0, 1.0, 0.891592750541463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3765100484778692, 0.3765100484778692, 0.4250279616040128], 
reward next is 0.5750, 
noisyNet noise sample is [array([-0.94075817], dtype=float32), -0.53423667]. 
=============================================
[2019-03-26 10:33:22,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:22,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:22,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-03-26 10:33:22,528] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:22,529] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:22,531] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-03-26 10:33:22,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:22,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:22,726] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-03-26 10:33:22,909] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0012561e-35 1.0000000e+00 2.8279039e-37 2.2626357e-26 5.6177518e-33], sum to 1.0000
[2019-03-26 10:33:22,910] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6330
[2019-03-26 10:33:22,911] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 86.0, 1.0, 2.0, 0.2820151670907188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 453632.9606308299, 453632.9606308305, 163840.9071685976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 333600.0000, 
sim time next is 334200.0000, 
raw observation next is [21.13333333333334, 86.0, 1.0, 2.0, 0.2809594953350604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 452218.6943056459, 452218.6943056459, 163747.0519832825], 
processed observation next is [0.0, 0.8695652173913043, 0.20063191153238583, 0.86, 1.0, 1.0, 0.13368613895790413, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12561630397379053, 0.12561630397379053, 0.24439858504967538], 
reward next is 0.7556, 
noisyNet noise sample is [array([-0.6915215], dtype=float32), 0.30177763]. 
=============================================
[2019-03-26 10:33:22,929] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:22,929] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:22,931] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-03-26 10:33:22,953] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:22,953] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:22,957] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-03-26 10:33:22,957] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:22,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:22,982] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-03-26 10:33:22,996] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:22,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:23,004] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-03-26 10:33:23,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:23,018] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:23,020] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-03-26 10:33:23,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:23,075] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:23,076] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-03-26 10:33:23,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:23,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:23,114] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-03-26 10:33:23,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:23,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:23,155] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-03-26 10:33:23,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:23,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:23,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:23,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:23,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:23,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:23,227] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-03-26 10:33:23,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:33:23,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:23,266] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-03-26 10:33:23,299] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-03-26 10:33:23,366] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-03-26 10:33:23,728] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1775176e-09 9.9999976e-01 7.2712321e-09 2.3156064e-08 2.6593779e-07], sum to 1.0000
[2019-03-26 10:33:23,729] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6987
[2019-03-26 10:33:23,731] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.55, 84.83333333333333, 1.0, 2.0, 0.2821758872622865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458816.9973071057, 458816.9973071057, 164148.413285434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3000.0000, 
sim time next is 3600.0000, 
raw observation next is [20.2, 84.0, 1.0, 2.0, 0.2705443458178718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 442899.9157403773, 442899.9157403773, 162995.9003588694], 
processed observation next is [1.0, 0.043478260869565216, 0.15639810426540288, 0.84, 1.0, 1.0, 0.12113776604562865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12302775437232703, 0.12302775437232703, 0.24327746322219315], 
reward next is 0.7567, 
noisyNet noise sample is [array([0.91544014], dtype=float32), -1.2994163]. 
=============================================
[2019-03-26 10:33:24,875] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 10:33:24,878] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:33:24,880] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:24,880] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:33:24,880] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:33:24,882] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:24,882] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:33:24,884] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:24,883] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:33:24,885] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:24,886] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:33:24,890] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run18
[2019-03-26 10:33:24,903] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run18
[2019-03-26 10:33:24,904] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run18
[2019-03-26 10:33:24,904] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run18
[2019-03-26 10:33:24,904] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run18
[2019-03-26 10:33:47,786] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.17734292], dtype=float32), 0.20505299]
[2019-03-26 10:33:47,787] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.2, 61.0, 1.0, 2.0, 0.3007723180802986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 481330.7307996119, 481330.7307996113, 165745.4256637316]
[2019-03-26 10:33:47,788] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:33:47,791] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.7475053e-30 4.9853484e-36], sampled 0.85856882791562
[2019-03-26 10:34:22,278] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.17734292], dtype=float32), 0.20505299]
[2019-03-26 10:34:22,279] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.98912121, 78.54165272, 1.0, 2.0, 0.6695438949724482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 935687.9195578637, 935687.9195578643, 214417.7448618987]
[2019-03-26 10:34:22,281] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:34:22,284] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1831024e-38 1.0000000e+00 0.0000000e+00 4.7477539e-28 8.6323199e-31], sampled 0.3015974960039275
[2019-03-26 10:34:35,180] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17734292], dtype=float32), 0.20505299]
[2019-03-26 10:34:35,182] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.838319005, 64.71235371666667, 1.0, 2.0, 0.8333487939473406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1164730.886250577, 1164730.886250577, 252263.3718589884]
[2019-03-26 10:34:35,184] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:34:35,187] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.7435067e-37 1.0000000e+00 3.2115326e-37 6.9441407e-27 6.4284261e-29], sampled 0.7724025639063216
[2019-03-26 10:34:37,178] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17734292], dtype=float32), 0.20505299]
[2019-03-26 10:34:37,179] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.5940333328374745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830120.6328028388, 830120.6328028388, 199632.858922759]
[2019-03-26 10:34:37,180] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:34:37,182] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.5745330e-37 1.0000000e+00 1.4645202e-36 4.5380375e-27 2.7753366e-28], sampled 0.7300661710713857
[2019-03-26 10:34:40,596] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17734292], dtype=float32), 0.20505299]
[2019-03-26 10:34:40,598] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.83333333333334, 84.83333333333334, 1.0, 2.0, 0.6179650533153994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863577.1581398257, 863577.1581398257, 204140.7094082737]
[2019-03-26 10:34:40,599] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:34:40,600] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7839255e-28 7.2586600e-33], sampled 0.7431983263687341
[2019-03-26 10:34:43,493] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17734292], dtype=float32), 0.20505299]
[2019-03-26 10:34:43,494] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.69033195, 98.83528768, 1.0, 2.0, 0.7293251371787216, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977114582354, 6.9112, 168.9123160316457, 1916165.836471519, 1848927.94436173, 391122.6585770366]
[2019-03-26 10:34:43,495] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:34:43,499] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.43488288e-36 1.00000000e+00 1.18791144e-35 2.58572739e-33
 2.20893837e-28], sampled 0.34237562034230606
[2019-03-26 10:34:43,501] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1916165.836471519 W.
[2019-03-26 10:34:49,943] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.17734292], dtype=float32), 0.20505299]
[2019-03-26 10:34:49,943] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.90360206666667, 57.51952030666667, 1.0, 2.0, 0.7453917153247407, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005978117875388, 6.9112, 168.9123160158406, 1938649.773787565, 1871411.169916158, 394807.1021414134]
[2019-03-26 10:34:49,944] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:34:49,947] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9374736e-37 1.0000000e+00 2.0550810e-35 5.3617288e-31 6.0013340e-28], sampled 0.9342392815973679
[2019-03-26 10:34:49,951] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1938649.773787565 W.
[2019-03-26 10:35:16,288] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.17734292], dtype=float32), 0.20505299]
[2019-03-26 10:35:16,289] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.9, 61.5, 1.0, 2.0, 0.7328939185524433, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977337291615, 6.9112, 168.9123162828951, 1921160.029672405, 1853921.979465587, 391931.0583745669]
[2019-03-26 10:35:16,290] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:35:16,292] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.7298107e-31 1.0000000e+00 3.5410908e-30 4.9180661e-21 1.1765557e-20], sampled 0.5127978377522596
[2019-03-26 10:35:16,293] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1921160.029672405 W.
[2019-03-26 10:35:20,499] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-26 10:35:20,531] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8724 2842497929.8887 1131.0000
[2019-03-26 10:35:20,571] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-26 10:35:20,618] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-26 10:35:20,625] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-26 10:35:21,639] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 425000, evaluation results [425000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8496.87235025343, 2842497929.888741, 1131.0]
[2019-03-26 10:35:21,655] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4616882e-32 1.0000000e+00 1.1933967e-30 2.3407507e-22 2.8051692e-22], sum to 1.0000
[2019-03-26 10:35:21,657] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1527
[2019-03-26 10:35:21,666] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.76666666666667, 68.66666666666667, 1.0, 2.0, 0.8336674078865801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1278676.334116864, 1278676.334116864, 267608.4755021951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 38400.0000, 
sim time next is 39000.0000, 
raw observation next is [25.98333333333333, 67.83333333333333, 1.0, 2.0, 0.8706346101013434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1332474.388643566, 1332474.388643566, 277886.5639423959], 
processed observation next is [1.0, 0.43478260869565216, 0.43048973143759867, 0.6783333333333332, 1.0, 1.0, 0.8441380844594498, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3701317746232128, 0.3701317746232128, 0.4147560655856655], 
reward next is 0.5852, 
noisyNet noise sample is [array([0.7202345], dtype=float32), -0.53238064]. 
=============================================
[2019-03-26 10:35:21,703] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.89878]
 [69.53513]
 [69.57862]
 [69.39643]
 [69.28024]], R is [[68.32723236]
 [68.24454498]
 [68.19829559]
 [68.15390778]
 [68.09226227]].
[2019-03-26 10:35:35,035] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6158177e-34 1.0000000e+00 6.2671441e-35 1.2054771e-28 8.7596711e-30], sum to 1.0000
[2019-03-26 10:35:35,044] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9698
[2019-03-26 10:35:35,052] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.45, 77.5, 1.0, 2.0, 0.2442870918465227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 403544.4234412894, 403544.4234412888, 160288.2708854308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 678600.0000, 
sim time next is 679200.0000, 
raw observation next is [20.33333333333334, 78.33333333333333, 1.0, 2.0, 0.2456158105228543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 405761.9823307951, 405761.9823307945, 160415.6371487835], 
processed observation next is [1.0, 0.8695652173913043, 0.16271721958925783, 0.7833333333333333, 1.0, 1.0, 0.09110338617211361, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1127116617585542, 0.11271166175855403, 0.23942632410266196], 
reward next is 0.7606, 
noisyNet noise sample is [array([-0.6099863], dtype=float32), 0.18442665]. 
=============================================
[2019-03-26 10:35:43,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1434470e-35 1.0000000e+00 4.0550337e-31 2.4408793e-27 9.2319307e-29], sum to 1.0000
[2019-03-26 10:35:43,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7472
[2019-03-26 10:35:43,400] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333333, 73.0, 1.0, 2.0, 0.5187384732909831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836339.7480974811, 836339.7480974811, 198703.6201582337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 394800.0000, 
sim time next is 395400.0000, 
raw observation next is [22.86666666666666, 73.0, 1.0, 2.0, 0.5312156999476261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855929.6869163697, 855929.6869163697, 201017.8298843545], 
processed observation next is [1.0, 0.5652173913043478, 0.28278041074249577, 0.73, 1.0, 1.0, 0.4351996384911157, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23775824636565826, 0.23775824636565826, 0.3000266117676933], 
reward next is 0.7000, 
noisyNet noise sample is [array([-0.83815676], dtype=float32), 0.30952063]. 
=============================================
[2019-03-26 10:35:55,860] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9408739e-32 1.0000000e+00 6.4935352e-31 3.6286182e-24 6.3522789e-27], sum to 1.0000
[2019-03-26 10:35:55,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3246
[2019-03-26 10:35:55,873] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.36666666666667, 97.0, 1.0, 2.0, 0.3802825846655978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572803.730547473, 572803.730547473, 172234.9270182879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1039800.0000, 
sim time next is 1040400.0000, 
raw observation next is [22.4, 97.0, 1.0, 2.0, 0.3809588480346077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573181.2049272379, 573181.2049272379, 172248.3978779776], 
processed observation next is [1.0, 0.043478260869565216, 0.2606635071090047, 0.97, 1.0, 1.0, 0.254167286788684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1592170013686772, 0.1592170013686772, 0.2570871610119069], 
reward next is 0.7429, 
noisyNet noise sample is [array([0.7803718], dtype=float32), 1.3009675]. 
=============================================
[2019-03-26 10:35:57,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0134316e-36 1.0000000e+00 1.6711604e-32 3.5831145e-25 2.2125320e-27], sum to 1.0000
[2019-03-26 10:35:57,892] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8602
[2019-03-26 10:35:57,899] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 81.5, 1.0, 2.0, 0.54169015138309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 861854.4169165186, 861854.416916518, 202247.3206357979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1074600.0000, 
sim time next is 1075200.0000, 
raw observation next is [22.4, 80.66666666666667, 1.0, 2.0, 0.5576904688059529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 887560.08459213, 887560.0845921306, 205384.0907203791], 
processed observation next is [1.0, 0.43478260869565216, 0.2606635071090047, 0.8066666666666668, 1.0, 1.0, 0.46709695036861787, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24654446794225834, 0.2465444679422585, 0.30654341898564047], 
reward next is 0.6935, 
noisyNet noise sample is [array([1.8670177], dtype=float32), -0.8009903]. 
=============================================
[2019-03-26 10:36:05,673] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.8966715e-35 2.4419349e-32], sum to 1.0000
[2019-03-26 10:36:05,680] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9825
[2019-03-26 10:36:05,685] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 77.16666666666667, 1.0, 2.0, 0.2526953037791749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 415038.7934701332, 415038.7934701332, 161177.4105638153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 762600.0000, 
sim time next is 763200.0000, 
raw observation next is [20.8, 79.0, 1.0, 2.0, 0.2538892108566228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 416831.1548951771, 416831.1548951764, 161296.7499757389], 
processed observation next is [1.0, 0.8695652173913043, 0.1848341232227489, 0.79, 1.0, 1.0, 0.10107133838147325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11578643191532698, 0.11578643191532678, 0.24074141787423717], 
reward next is 0.7593, 
noisyNet noise sample is [array([1.7649379], dtype=float32), -0.20400178]. 
=============================================
[2019-03-26 10:36:07,292] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1797598e-34 1.0000000e+00 4.3325113e-32 4.4303423e-28 1.0583538e-27], sum to 1.0000
[2019-03-26 10:36:07,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4796
[2019-03-26 10:36:07,306] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.76666666666667, 75.33333333333334, 1.0, 2.0, 0.287771708005965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 461659.387396554, 461659.3873965534, 164378.4425637016], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 808800.0000, 
sim time next is 809400.0000, 
raw observation next is [22.93333333333334, 74.16666666666667, 1.0, 2.0, 0.2876578747443136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461500.5522880002, 461500.5522880002, 164367.7413135518], 
processed observation next is [0.0, 0.34782608695652173, 0.28593996840442376, 0.7416666666666667, 1.0, 1.0, 0.14175647559555854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12819459785777781, 0.12819459785777781, 0.24532498703515196], 
reward next is 0.7547, 
noisyNet noise sample is [array([0.03895679], dtype=float32), -0.83919543]. 
=============================================
[2019-03-26 10:36:10,901] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6381448e-38 1.0000000e+00 0.0000000e+00 3.5606079e-31 4.0744347e-37], sum to 1.0000
[2019-03-26 10:36:10,910] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3420
[2019-03-26 10:36:10,916] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.73333333333333, 87.0, 1.0, 2.0, 0.3062337674882957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 485812.4309306119, 485812.4309306126, 166009.607308925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 858000.0000, 
sim time next is 858600.0000, 
raw observation next is [21.7, 87.5, 1.0, 2.0, 0.306765042468953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486322.4256024822, 486322.4256024816, 166040.3258112925], 
processed observation next is [0.0, 0.9565217391304348, 0.2274881516587678, 0.875, 1.0, 1.0, 0.16477715960114817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13508956266735617, 0.135089562667356, 0.24782138180789928], 
reward next is 0.7522, 
noisyNet noise sample is [array([1.1791109], dtype=float32), -0.7167939]. 
=============================================
[2019-03-26 10:36:15,460] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 10:36:15,464] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:36:15,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:36:15,466] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:36:15,467] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:36:15,468] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:36:15,467] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:36:15,469] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:36:15,469] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:36:15,471] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:36:15,470] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:36:15,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run19
[2019-03-26 10:36:15,491] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run19
[2019-03-26 10:36:15,492] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run19
[2019-03-26 10:36:15,492] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run19
[2019-03-26 10:36:15,530] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run19
[2019-03-26 10:37:15,782] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14754882], dtype=float32), 0.17809229]
[2019-03-26 10:37:15,784] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.91666666666667, 73.33333333333333, 1.0, 2.0, 0.8947006742164791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1250529.908129236, 1250529.908129237, 268487.9305666335]
[2019-03-26 10:37:15,785] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:37:15,787] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.8097346e-36 1.0000000e+00 1.7659786e-33 4.6852167e-27 7.3079502e-25], sampled 0.8235832502963889
[2019-03-26 10:37:59,830] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14754882], dtype=float32), 0.17809229]
[2019-03-26 10:37:59,831] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.46666666666667, 73.0, 1.0, 2.0, 0.5380627286945151, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9344370196789098, 6.911199999999999, 6.9112, 168.9126032527224, 1504288.366254905, 1504288.366254906, 329590.850651501]
[2019-03-26 10:37:59,831] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:37:59,834] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7577649e-33 1.0000000e+00 9.2719744e-32 3.4252064e-25 5.1872404e-23], sampled 0.27356690232723246
[2019-03-26 10:38:05,459] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14754882], dtype=float32), 0.17809229]
[2019-03-26 10:38:05,461] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.3525815, 94.90745888999999, 1.0, 2.0, 0.4584302014151915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 649926.0815683608, 649926.0815683601, 178546.2295280452]
[2019-03-26 10:38:05,464] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:38:05,467] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6165241e-37 1.0000000e+00 0.0000000e+00 5.9239705e-30 5.1300678e-33], sampled 0.6386091105325572
[2019-03-26 10:38:11,511] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-26 10:38:11,608] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-26 10:38:11,675] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6857 2927400807.7850 1338.0000
[2019-03-26 10:38:11,799] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-26 10:38:11,859] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-26 10:38:12,877] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 450000, evaluation results [450000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8253.68573338827, 2927400807.784972, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-26 10:38:32,390] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0411517e-37], sum to 1.0000
[2019-03-26 10:38:32,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2167
[2019-03-26 10:38:32,406] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.81666666666667, 85.33333333333334, 1.0, 2.0, 0.5083002362419704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710274.4841059294, 710274.4841059294, 184906.9153788787], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1713000.0000, 
sim time next is 1713600.0000, 
raw observation next is [26.7, 86.0, 1.0, 2.0, 0.5067137639541339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708056.8846347855, 708056.8846347849, 184654.8703867525], 
processed observation next is [1.0, 0.8695652173913043, 0.46445497630331756, 0.86, 1.0, 1.0, 0.4056792336796794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19668246795410707, 0.1966824679541069, 0.2756042841593321], 
reward next is 0.7244, 
noisyNet noise sample is [array([-1.9195261], dtype=float32), -0.1241657]. 
=============================================
[2019-03-26 10:38:33,123] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 10:38:33,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1671
[2019-03-26 10:38:33,132] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.55, 86.5, 1.0, 2.0, 0.4692381634320026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658222.2365848398, 658222.2365848392, 179245.9695006397], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1279800.0000, 
sim time next is 1280400.0000, 
raw observation next is [25.4, 87.33333333333333, 1.0, 2.0, 0.4677127682766395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657033.2627142441, 657033.2627142441, 179143.0367438367], 
processed observation next is [1.0, 0.8260869565217391, 0.4028436018957346, 0.8733333333333333, 1.0, 1.0, 0.35869008226101146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18250923964284557, 0.18250923964284557, 0.2673776667818458], 
reward next is 0.7326, 
noisyNet noise sample is [array([0.8021073], dtype=float32), -1.0361732]. 
=============================================
[2019-03-26 10:38:33,848] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4467998e-34 1.0000000e+00 2.6821311e-35 3.9714955e-30 2.0571788e-24], sum to 1.0000
[2019-03-26 10:38:33,850] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2016
[2019-03-26 10:38:33,857] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.58333333333334, 91.16666666666667, 1.0, 2.0, 0.4932840924185918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702203.8162529498, 702203.8162529498, 184203.1094174622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1313400.0000, 
sim time next is 1314000.0000, 
raw observation next is [24.6, 91.0, 1.0, 2.0, 0.4899368075859202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 697606.7093396898, 697606.7093396892, 183697.8037423817], 
processed observation next is [1.0, 0.21739130434782608, 0.36492890995260674, 0.91, 1.0, 1.0, 0.3854660332360484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19377964148324717, 0.193779641483247, 0.2741758264811667], 
reward next is 0.7258, 
noisyNet noise sample is [array([-0.7748076], dtype=float32), -2.2632043]. 
=============================================
[2019-03-26 10:38:33,890] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[67.9535 ]
 [67.93842]
 [67.89956]
 [67.91427]
 [67.84338]], R is [[67.98240662]
 [68.02764893]
 [68.07078552]
 [68.10990906]
 [68.14906311]].
[2019-03-26 10:38:36,902] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1555748e-34 1.0000000e+00 2.5622882e-34 2.2459729e-31 1.2856726e-22], sum to 1.0000
[2019-03-26 10:38:36,910] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2076
[2019-03-26 10:38:36,914] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.85, 92.5, 1.0, 2.0, 0.298805755712317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476170.2542246285, 476170.2542246285, 165354.0656031102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1359000.0000, 
sim time next is 1359600.0000, 
raw observation next is [20.86666666666667, 92.66666666666667, 1.0, 2.0, 0.3025808591077417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481811.8164296245, 481811.8164296245, 165751.3338185707], 
processed observation next is [1.0, 0.7391304347826086, 0.18799368088467638, 0.9266666666666667, 1.0, 1.0, 0.15973597482860444, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1338366156748957, 0.1338366156748957, 0.24739005047547868], 
reward next is 0.7526, 
noisyNet noise sample is [array([-1.7786342], dtype=float32), -1.0264637]. 
=============================================
[2019-03-26 10:38:37,655] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 2.8131941e-38 2.8749861e-33 2.8670225e-34], sum to 1.0000
[2019-03-26 10:38:37,662] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1015
[2019-03-26 10:38:37,665] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 94.16666666666667, 1.0, 2.0, 0.3237531098649404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 509624.6295686291, 509624.6295686285, 167705.5916745497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1368600.0000, 
sim time next is 1369200.0000, 
raw observation next is [21.13333333333333, 94.33333333333334, 1.0, 2.0, 0.3234996431887225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 509385.1160155715, 509385.1160155709, 167690.7516526353], 
processed observation next is [1.0, 0.8695652173913043, 0.20063191153238533, 0.9433333333333335, 1.0, 1.0, 0.18493932914303912, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14149586555988097, 0.1414958655598808, 0.2502847039591572], 
reward next is 0.7497, 
noisyNet noise sample is [array([-0.4534648], dtype=float32), -0.7420168]. 
=============================================
[2019-03-26 10:38:45,964] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.6805406e-35 9.6990122e-34], sum to 1.0000
[2019-03-26 10:38:45,975] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8193
[2019-03-26 10:38:45,984] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.15, 56.5, 1.0, 2.0, 0.3618000962645297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 552033.1282474456, 552033.128247445, 170661.9828688587], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1525800.0000, 
sim time next is 1526400.0000, 
raw observation next is [28.0, 57.0, 1.0, 2.0, 0.3597016180012442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549558.4744311062, 549558.4744311069, 170475.6977692278], 
processed observation next is [0.0, 0.6956521739130435, 0.5260663507109005, 0.57, 1.0, 1.0, 0.2285561662665593, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1526551317864184, 0.1526551317864186, 0.2544413399540713], 
reward next is 0.7456, 
noisyNet noise sample is [array([-1.3802191], dtype=float32), 0.21256855]. 
=============================================
[2019-03-26 10:38:51,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.1860872e-32 4.6637867e-33], sum to 1.0000
[2019-03-26 10:38:51,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2675
[2019-03-26 10:38:51,062] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 88.0, 1.0, 2.0, 0.5122492992232585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715794.5754063062, 715794.5754063068, 185537.7050539116], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2032200.0000, 
sim time next is 2032800.0000, 
raw observation next is [26.66666666666666, 87.33333333333333, 1.0, 2.0, 0.5121627211644103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715673.554291611, 715673.5542916103, 185523.8814697156], 
processed observation next is [0.0, 0.5217391304347826, 0.4628751974723536, 0.8733333333333333, 1.0, 1.0, 0.4122442423667594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1987982095254475, 0.1987982095254473, 0.27690131562644116], 
reward next is 0.7231, 
noisyNet noise sample is [array([-1.1187594], dtype=float32), -1.8208094]. 
=============================================
[2019-03-26 10:39:00,762] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7623521e-32 1.0000000e+00 2.2106535e-32 6.5920091e-27 2.2896323e-24], sum to 1.0000
[2019-03-26 10:39:00,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9313
[2019-03-26 10:39:00,771] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.21666666666667, 87.00000000000001, 1.0, 2.0, 0.8346412022571453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1228578.621944959, 1228578.621944959, 261279.0360663521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1764600.0000, 
sim time next is 1765200.0000, 
raw observation next is [24.13333333333334, 86.0, 1.0, 2.0, 0.8258029945387197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1225044.500419657, 1225044.500419656, 260161.9662070169], 
processed observation next is [1.0, 0.43478260869565216, 0.3428120063191157, 0.86, 1.0, 1.0, 0.7901240898056864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3402901390054603, 0.34029013900546, 0.3883014421000252], 
reward next is 0.6117, 
noisyNet noise sample is [array([0.31122774], dtype=float32), -1.9497879]. 
=============================================
[2019-03-26 10:39:06,822] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 10:39:06,824] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:39:06,824] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:39:06,825] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:39:06,826] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:39:06,827] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:39:06,828] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:39:06,828] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:39:06,830] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:39:06,832] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:39:06,833] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:39:06,850] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run20
[2019-03-26 10:39:06,867] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run20
[2019-03-26 10:39:06,868] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run20
[2019-03-26 10:39:06,907] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run20
[2019-03-26 10:39:06,933] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run20
[2019-03-26 10:39:12,830] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.07951315], dtype=float32), 0.19476765]
[2019-03-26 10:39:12,831] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.53333333333333, 75.0, 1.0, 2.0, 0.2624565554253477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 429838.5439165451, 429838.5439165451, 162157.2088068676]
[2019-03-26 10:39:12,832] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:39:12,834] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3361583e-38 1.0000000e+00 0.0000000e+00 2.0451130e-32 1.3827511e-35], sampled 0.12008326365339073
[2019-03-26 10:40:05,805] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.07951315], dtype=float32), 0.19476765]
[2019-03-26 10:40:05,807] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.76666666666667, 59.33333333333333, 1.0, 2.0, 0.9625071422796457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1345363.571358407, 1345363.571358407, 287709.5880551712]
[2019-03-26 10:40:05,810] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:40:05,812] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4421242e-37 1.0000000e+00 4.6003781e-37 7.6360455e-28 1.4443200e-28], sampled 0.36567526304915277
[2019-03-26 10:40:09,675] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.07951315], dtype=float32), 0.19476765]
[2019-03-26 10:40:09,676] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.92809486, 78.16462442, 1.0, 2.0, 0.547238030294785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764703.8835810944, 764703.8835810951, 191325.6116292212]
[2019-03-26 10:40:09,677] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:40:09,680] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.6447216e-30 1.9900017e-33], sampled 0.2866538943178175
[2019-03-26 10:40:12,343] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.07951315], dtype=float32), 0.19476765]
[2019-03-26 10:40:12,345] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.88488759166667, 83.32146975666667, 1.0, 2.0, 0.5478324985912182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765534.8858202795, 765534.8858202795, 191429.0144438099]
[2019-03-26 10:40:12,347] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:40:12,351] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4718322e-32 2.2076007e-34], sampled 0.3170835264296006
[2019-03-26 10:40:26,578] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.07951315], dtype=float32), 0.19476765]
[2019-03-26 10:40:26,580] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.74786540333334, 67.45103648666667, 1.0, 2.0, 0.7865663196384646, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005980689240722, 6.9112, 168.9123160051759, 1996272.633018188, 1929032.204942876, 404552.4978332365]
[2019-03-26 10:40:26,580] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:40:26,585] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8492862e-37], sampled 0.0687466236348927
[2019-03-26 10:40:26,587] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1996272.633018188 W.
[2019-03-26 10:40:29,084] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.07951315], dtype=float32), 0.19476765]
[2019-03-26 10:40:29,086] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.99645866, 63.51306797666666, 1.0, 2.0, 0.6953324513062177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 971743.9277148304, 971743.9277148297, 219840.4894275318]
[2019-03-26 10:40:29,087] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:40:29,090] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.8334693e-31 1.4500902e-34], sampled 0.5278434905495298
[2019-03-26 10:40:34,732] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.07951315], dtype=float32), 0.19476765]
[2019-03-26 10:40:34,732] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.264144935, 94.83964750166666, 1.0, 2.0, 0.5444296865480065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 760778.1322323508, 760778.1322323515, 190849.6426732705]
[2019-03-26 10:40:34,734] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:40:34,736] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.8325113e-33 1.7919384e-37], sampled 0.006040838478708288
[2019-03-26 10:40:41,433] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.07951315], dtype=float32), 0.19476765]
[2019-03-26 10:40:41,436] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.33333333333334, 63.33333333333334, 1.0, 2.0, 0.7123406295098956, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.003949206944893, 6.9112, 168.9123278736005, 1892397.845485247, 1826598.61078058, 387417.5124740928]
[2019-03-26 10:40:41,437] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:40:41,439] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2478312e-35 1.2632318e-37], sampled 0.997412288943563
[2019-03-26 10:40:41,440] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1892397.845485247 W.
[2019-03-26 10:40:45,405] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.07951315], dtype=float32), 0.19476765]
[2019-03-26 10:40:45,407] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.0, 85.5, 1.0, 2.0, 0.9013080407843971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1259770.553398035, 1259770.553398035, 270294.1710406382]
[2019-03-26 10:40:45,408] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:40:45,410] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.5785320e-37 1.0000000e+00 4.3132743e-36 6.9659499e-27 1.0837164e-27], sampled 0.981834730678171
[2019-03-26 10:41:02,776] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-26 10:41:02,813] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-26 10:41:02,942] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-26 10:41:03,109] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-26 10:41:03,127] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-26 10:41:04,142] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 475000, evaluation results [475000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-26 10:41:06,893] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3564912e-37 4.7110390e-36], sum to 1.0000
[2019-03-26 10:41:06,906] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8922
[2019-03-26 10:41:06,911] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.38333333333333, 83.66666666666667, 1.0, 2.0, 0.4781189451026186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687416.3430143271, 687416.3430143271, 182702.9621575311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1929000.0000, 
sim time next is 1929600.0000, 
raw observation next is [25.5, 83.0, 1.0, 2.0, 0.4865390156703512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698938.4954897553, 698938.4954897553, 183945.1048319371], 
processed observation next is [1.0, 0.34782608695652173, 0.40758293838862564, 0.83, 1.0, 1.0, 0.3813723080365677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19414958208048758, 0.19414958208048758, 0.27454493258498075], 
reward next is 0.7255, 
noisyNet noise sample is [array([-1.4396932], dtype=float32), 0.8812975]. 
=============================================
[2019-03-26 10:41:07,626] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0183484e-35 5.0962459e-33], sum to 1.0000
[2019-03-26 10:41:07,634] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2306
[2019-03-26 10:41:07,639] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.36666666666667, 72.5, 1.0, 2.0, 0.8230301068088046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1150301.149550287, 1150301.149550286, 249641.8106628332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2361000.0000, 
sim time next is 2361600.0000, 
raw observation next is [29.5, 72.0, 1.0, 2.0, 0.855149821951676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1195218.267409612, 1195218.267409612, 257897.9041275255], 
processed observation next is [1.0, 0.34782608695652173, 0.5971563981042655, 0.72, 1.0, 1.0, 0.8254817131947904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3320050742804478, 0.3320050742804478, 0.38492224496645594], 
reward next is 0.6151, 
noisyNet noise sample is [array([0.07104047], dtype=float32), -0.23503189]. 
=============================================
[2019-03-26 10:41:08,326] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.4536939e-36 1.0000000e+00 1.5464251e-33 1.2828184e-34 1.1116140e-28], sum to 1.0000
[2019-03-26 10:41:08,334] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5567
[2019-03-26 10:41:08,340] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2165415.031692195 W.
[2019-03-26 10:41:08,343] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.1, 80.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.89413286559147, 6.9112, 168.9074644089985, 2165415.031692195, 1468111.300044626, 313576.9897977772], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1956600.0000, 
sim time next is 1957200.0000, 
raw observation next is [25.93333333333334, 81.33333333333333, 1.0, 2.0, 0.4285326961979157, 1.0, 1.0, 0.4285326961979157, 1.0, 1.0, 0.7166793583295402, 6.9112, 6.9112, 170.5573041426782, 1797338.088740721, 1797338.088740721, 364935.432035615], 
processed observation next is [1.0, 0.6521739130434783, 0.42812006319115364, 0.8133333333333332, 1.0, 1.0, 0.31148517614206706, 1.0, 0.5, 0.31148517614206706, 1.0, 0.5, 0.6544870223530977, 0.0, 0.0, 0.8375144448122397, 0.49926058020575587, 0.49926058020575587, 0.5446797493068881], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.855353], dtype=float32), 0.32827213]. 
=============================================
[2019-03-26 10:41:15,838] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.4023252e-36 2.9025853e-29 2.0506106e-34], sum to 1.0000
[2019-03-26 10:41:15,846] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6886
[2019-03-26 10:41:15,849] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.35, 92.5, 1.0, 2.0, 0.4872078068111014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680791.4856151243, 680791.4856151249, 181616.374062355], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2098200.0000, 
sim time next is 2098800.0000, 
raw observation next is [25.5, 92.0, 1.0, 2.0, 0.4897628184162399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684362.8373559648, 684362.8373559642, 182008.0208962008], 
processed observation next is [0.0, 0.30434782608695654, 0.40758293838862564, 0.92, 1.0, 1.0, 0.38525640773040953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19010078815443468, 0.1901007881544345, 0.271653762531643], 
reward next is 0.7283, 
noisyNet noise sample is [array([1.9483032], dtype=float32), -0.44095588]. 
=============================================
[2019-03-26 10:41:19,157] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1131287e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 10:41:19,166] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1485
[2019-03-26 10:41:19,171] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 91.0, 1.0, 2.0, 0.5327190006761872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744408.0558479098, 744408.0558479098, 188881.2250719339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2152800.0000, 
sim time next is 2153400.0000, 
raw observation next is [26.51666666666667, 91.33333333333334, 1.0, 2.0, 0.5313139170014843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742443.9412922456, 742443.9412922449, 188647.7379852303], 
processed observation next is [0.0, 0.9565217391304348, 0.45576619273301755, 0.9133333333333334, 1.0, 1.0, 0.43531797229094493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2062344281367349, 0.2062344281367347, 0.2815637880376572], 
reward next is 0.7184, 
noisyNet noise sample is [array([-0.43660584], dtype=float32), -0.8141395]. 
=============================================
[2019-03-26 10:41:23,277] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 10:41:23,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5663
[2019-03-26 10:41:23,293] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.46666666666667, 69.33333333333334, 1.0, 2.0, 0.5372240362911849, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564456753, 750705.5031775748, 750705.5031775754, 189637.0509683868], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2222400.0000, 
sim time next is 2223000.0000, 
raw observation next is [31.3, 70.0, 1.0, 2.0, 0.5267534097252018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104148, 736069.0016898519, 736069.0016898524, 187897.8433761199], 
processed observation next is [1.0, 0.7391304347826086, 0.6824644549763034, 0.7, 1.0, 1.0, 0.4298233852110865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451522232, 0.2044636115805144, 0.20446361158051457, 0.2804445423524178], 
reward next is 0.7196, 
noisyNet noise sample is [array([-1.0423806], dtype=float32), -0.20757872]. 
=============================================
[2019-03-26 10:41:23,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[57.377155]
 [55.19385 ]
 [51.792545]
 [52.56682 ]
 [52.482853]], R is [[58.85747147]
 [58.26889801]
 [58.24602509]
 [57.66356659]
 [57.08693314]].
[2019-03-26 10:41:32,079] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.9057751e-37 1.0000000e+00 5.8245571e-34 1.2304368e-33 3.7610155e-28], sum to 1.0000
[2019-03-26 10:41:32,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6776
[2019-03-26 10:41:32,093] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.43333333333333, 76.33333333333334, 1.0, 2.0, 0.7052609130727916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 985625.6320818511, 985625.6320818511, 221983.717802521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2356800.0000, 
sim time next is 2357400.0000, 
raw observation next is [28.56666666666667, 75.66666666666666, 1.0, 2.0, 0.7034871674576918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 983145.6160424062, 983145.6160424056, 221598.6202840613], 
processed observation next is [1.0, 0.2608695652173913, 0.552922590837283, 0.7566666666666666, 1.0, 1.0, 0.6427556234430022, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27309600445622395, 0.2730960044562238, 0.33074420937919596], 
reward next is 0.6693, 
noisyNet noise sample is [array([0.13389713], dtype=float32), 0.6850784]. 
=============================================
[2019-03-26 10:41:34,071] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4490064e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 10:41:34,082] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2894
[2019-03-26 10:41:34,089] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 78.0, 1.0, 2.0, 0.5741745914193467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802358.9288674291, 802358.9288674291, 196033.3951373182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2412000.0000, 
sim time next is 2412600.0000, 
raw observation next is [29.71666666666667, 78.33333333333333, 1.0, 2.0, 0.5741466427085284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802319.8582302403, 802319.8582302403, 196028.3315380805], 
processed observation next is [1.0, 0.9565217391304348, 0.6074249605055293, 0.7833333333333333, 1.0, 1.0, 0.4869236659138897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22286662728617787, 0.22286662728617787, 0.29257959931056793], 
reward next is 0.7074, 
noisyNet noise sample is [array([-1.5377247], dtype=float32), -1.4598963]. 
=============================================
[2019-03-26 10:41:38,718] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 10:41:38,726] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4816
[2019-03-26 10:41:38,731] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.98333333333333, 84.5, 1.0, 2.0, 0.5292023043242295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739492.2027779863, 739492.2027779863, 188299.3714882909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2483400.0000, 
sim time next is 2484000.0000, 
raw observation next is [27.9, 85.0, 1.0, 2.0, 0.5352298734017962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747917.9211063155, 747917.9211063149, 189300.8957934308], 
processed observation next is [1.0, 0.782608695652174, 0.5213270142180094, 0.85, 1.0, 1.0, 0.4400359920503568, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20775497808508764, 0.20775497808508747, 0.2825386504379564], 
reward next is 0.7175, 
noisyNet noise sample is [array([1.0072293], dtype=float32), 0.9773704]. 
=============================================
[2019-03-26 10:41:38,748] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[58.235664]
 [57.608315]
 [56.676598]
 [54.88652 ]
 [52.872993]], R is [[58.97954941]
 [59.10871124]
 [59.23917007]
 [59.37095261]
 [59.50087738]].
[2019-03-26 10:41:40,763] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1883045e-37 1.0000000e+00 4.6926722e-36 2.3180226e-28 2.6204047e-31], sum to 1.0000
[2019-03-26 10:41:40,775] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5046
[2019-03-26 10:41:40,780] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.31666666666667, 95.83333333333333, 1.0, 2.0, 0.8263701618173276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1154971.888396061, 1154971.888396061, 250487.3977074453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2515800.0000, 
sim time next is 2516400.0000, 
raw observation next is [26.3, 96.0, 1.0, 2.0, 0.8061811869070494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1126739.903315229, 1126739.90331523, 245434.4850796049], 
processed observation next is [1.0, 0.13043478260869565, 0.4454976303317536, 0.96, 1.0, 1.0, 0.7664833577193366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3129833064764525, 0.3129833064764528, 0.3663201269844849], 
reward next is 0.6337, 
noisyNet noise sample is [array([-1.6570644], dtype=float32), -0.35502282]. 
=============================================
[2019-03-26 10:41:42,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3638601e-33 1.0000000e+00 2.6323540e-26 0.0000000e+00 5.6391725e-28], sum to 1.0000
[2019-03-26 10:41:42,873] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6854
[2019-03-26 10:41:42,880] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1859589.145747411 W.
[2019-03-26 10:41:42,885] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.51666666666667, 79.33333333333334, 1.0, 2.0, 0.4433621178855864, 1.0, 1.0, 0.4433621178855864, 1.0, 2.0, 0.7646112334466273, 6.9112, 6.9112, 170.5573041426782, 1859589.145747411, 1859589.145747411, 377345.0865979478], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2545800.0000, 
sim time next is 2546400.0000, 
raw observation next is [28.63333333333333, 78.66666666666667, 1.0, 2.0, 0.4355925361799806, 1.0, 2.0, 0.4355925361799806, 1.0, 2.0, 0.7518221369183145, 6.9112, 6.9112, 170.5573041426782, 1826973.486647314, 1826973.486647314, 372746.9165044011], 
processed observation next is [1.0, 0.4782608695652174, 0.55608214849921, 0.7866666666666667, 1.0, 1.0, 0.31999100744575976, 1.0, 1.0, 0.31999100744575976, 1.0, 1.0, 0.6973440694125786, 0.0, 0.0, 0.8375144448122397, 0.5074926351798095, 0.5074926351798095, 0.5563386813498523], 
reward next is 0.4437, 
noisyNet noise sample is [array([1.1782987], dtype=float32), 0.8947602]. 
=============================================
[2019-03-26 10:41:46,279] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6592746e-33 1.0000000e+00 1.7976503e-35 1.2423094e-23 5.1450852e-31], sum to 1.0000
[2019-03-26 10:41:46,289] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0044
[2019-03-26 10:41:46,294] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 92.0, 1.0, 2.0, 0.443301228954696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 637989.3737035977, 637989.3737035977, 177573.5516215751], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2601000.0000, 
sim time next is 2601600.0000, 
raw observation next is [24.16666666666667, 92.0, 1.0, 2.0, 0.4417324161473611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636596.4127780722, 636596.4127780729, 177456.4061942771], 
processed observation next is [0.0, 0.08695652173913043, 0.34439178515007923, 0.92, 1.0, 1.0, 0.3273884531895917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1768323368827978, 0.176832336882798, 0.2648603077526524], 
reward next is 0.7351, 
noisyNet noise sample is [array([0.07021765], dtype=float32), 0.12582618]. 
=============================================
[2019-03-26 10:41:47,813] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9757421e-35 1.0000000e+00 5.2145993e-38 4.8960772e-26 6.3552098e-35], sum to 1.0000
[2019-03-26 10:41:47,823] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0771
[2019-03-26 10:41:47,827] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4981254129902304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696052.0137000309, 696052.0137000309, 183302.9807375376], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2650200.0000, 
sim time next is 2650800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4977197739144745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695485.011262063, 695485.0112620625, 183239.6990366384], 
processed observation next is [0.0, 0.6956521739130435, 0.4312796208530806, 0.89, 1.0, 1.0, 0.3948431011017765, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19319028090612864, 0.19319028090612847, 0.2734920881143857], 
reward next is 0.7265, 
noisyNet noise sample is [array([0.28157097], dtype=float32), -1.8292897]. 
=============================================
[2019-03-26 10:41:48,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2506461e-36 1.0000000e+00 9.0190392e-36 1.1659192e-25 3.7947118e-34], sum to 1.0000
[2019-03-26 10:41:48,075] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0392
[2019-03-26 10:41:48,080] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5036098937985236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703718.2525470251, 703718.2525470251, 184164.0102108445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2638800.0000, 
sim time next is 2639400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5050180542036596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705686.5961941202, 705686.5961941202, 184386.3563279679], 
processed observation next is [0.0, 0.5652173913043478, 0.4786729857819906, 0.84, 1.0, 1.0, 0.40363620988392723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19602405449836674, 0.19602405449836674, 0.2752035169074148], 
reward next is 0.7248, 
noisyNet noise sample is [array([0.7492534], dtype=float32), 0.62342775]. 
=============================================
[2019-03-26 10:41:48,245] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0141848e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 10:41:48,253] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5356
[2019-03-26 10:41:48,259] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4852130322956492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678003.2320904683, 678003.2320904683, 181311.4334101371], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2646000.0000, 
sim time next is 2646600.0000, 
raw observation next is [26.83333333333333, 80.66666666666667, 1.0, 2.0, 0.485298489297006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678122.6819250647, 678122.6819250641, 181324.6432164886], 
processed observation next is [0.0, 0.6521739130434783, 0.470774091627172, 0.8066666666666668, 1.0, 1.0, 0.37987769794820003, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1883674116458513, 0.18836741164585113, 0.27063379584550534], 
reward next is 0.7294, 
noisyNet noise sample is [array([0.82512033], dtype=float32), -0.5059501]. 
=============================================
[2019-03-26 10:41:52,801] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.744418e-29 0.000000e+00], sum to 1.0000
[2019-03-26 10:41:52,811] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5381
[2019-03-26 10:41:52,818] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3841903961835303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578664.1240266134, 578664.1240266134, 172755.6668044197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2725200.0000, 
sim time next is 2725800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3840836943958654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578503.4916854348, 578503.4916854348, 172741.3072146917], 
processed observation next is [0.0, 0.5652173913043478, 0.2417061611374408, 1.0, 1.0, 1.0, 0.25793216192272944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16069541435706522, 0.16069541435706522, 0.2578228465890921], 
reward next is 0.7422, 
noisyNet noise sample is [array([1.2804978], dtype=float32), 0.255731]. 
=============================================
[2019-03-26 10:41:53,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7648402e-30 3.6380449e-38], sum to 1.0000
[2019-03-26 10:41:53,728] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3001
[2019-03-26 10:41:53,731] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.3911062162407326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584174.1603886123, 584174.1603886123, 173103.7404002603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2747400.0000, 
sim time next is 2748000.0000, 
raw observation next is [22.66666666666667, 96.0, 1.0, 2.0, 0.3899526964879866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583306.3079049023, 583306.3079049016, 173051.4434765129], 
processed observation next is [0.0, 0.8260869565217391, 0.27330173775671435, 0.96, 1.0, 1.0, 0.26500324878070675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16202952997358397, 0.16202952997358377, 0.25828573653210884], 
reward next is 0.7417, 
noisyNet noise sample is [array([0.54390895], dtype=float32), 1.2899468]. 
=============================================
[2019-03-26 10:41:53,745] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.05578]
 [77.0186 ]
 [76.93045]
 [76.88599]
 [76.82957]], R is [[77.07475281]
 [77.04564667]
 [77.01669312]
 [76.98781586]
 [76.9591217 ]].
[2019-03-26 10:41:56,855] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.38982993e-36 1.00000000e+00 1.19433915e-38 1.20613424e-29
 7.14639807e-33], sum to 1.0000
[2019-03-26 10:41:56,860] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9184
[2019-03-26 10:41:56,868] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.580029367333351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893389.2896647172, 893389.2896647172, 207076.6346491899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2795400.0000, 
sim time next is 2796000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6041777446004031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 930524.5553125858, 930524.5553125851, 211986.2257014145], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.94, 1.0, 1.0, 0.5231057163860278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25847904314238496, 0.25847904314238473, 0.316397351793156], 
reward next is 0.6836, 
noisyNet noise sample is [array([1.4499539], dtype=float32), -0.24555501]. 
=============================================
[2019-03-26 10:41:56,889] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[67.11247 ]
 [67.73788 ]
 [68.569725]
 [68.660576]
 [68.5597  ]], R is [[66.68228149]
 [66.70639038]
 [66.74701691]
 [66.81874084]
 [66.89408875]].
[2019-03-26 10:41:57,684] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 10:41:57,685] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:41:57,687] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:41:57,687] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:41:57,688] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:41:57,689] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:41:57,688] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:41:57,690] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:41:57,690] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:41:57,690] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:41:57,691] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:41:57,698] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run21
[2019-03-26 10:41:57,699] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run21
[2019-03-26 10:41:57,740] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run21
[2019-03-26 10:41:57,741] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run21
[2019-03-26 10:41:57,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run21
[2019-03-26 10:42:15,021] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.090869], dtype=float32), 0.20445609]
[2019-03-26 10:42:15,025] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.74247703, 88.86308790166666, 1.0, 2.0, 0.3276223455863551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 516000.5316810466, 516000.5316810473, 168201.7896097124]
[2019-03-26 10:42:15,025] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:42:15,028] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1304347e-35 1.0000000e+00 1.6057564e-37 2.9079745e-27 5.0365271e-33], sampled 0.6422788197688714
[2019-03-26 10:42:16,798] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.090869], dtype=float32), 0.20445609]
[2019-03-26 10:42:16,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.3, 62.0, 1.0, 2.0, 0.8520371013654272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1293168.079774459, 1293168.07977446, 271151.0565299764]
[2019-03-26 10:42:16,801] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:42:16,806] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4184991e-32 1.0000000e+00 2.0070264e-32 5.1379278e-23 5.9990938e-27], sampled 0.15179049677691447
[2019-03-26 10:42:41,476] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.090869], dtype=float32), 0.20445609]
[2019-03-26 10:42:41,477] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3494694749731769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538347.7601012872, 538347.7601012879, 169680.8413601649]
[2019-03-26 10:42:41,479] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:42:41,481] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5844440e-34 1.0000000e+00 2.4674368e-36 2.2150447e-23 1.3612540e-31], sampled 0.20586257937303865
[2019-03-26 10:42:43,497] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.090869], dtype=float32), 0.20445609]
[2019-03-26 10:42:43,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.03333333333333, 88.0, 1.0, 2.0, 0.5069535579094164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708392.0726373285, 708392.0726373292, 184692.048496478]
[2019-03-26 10:42:43,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:42:43,504] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6410126e-34 1.0000000e+00 3.8090534e-35 8.5592330e-23 3.8120138e-30], sampled 0.5251274133430437
[2019-03-26 10:42:56,289] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.090869], dtype=float32), 0.20445609]
[2019-03-26 10:42:56,290] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.4, 71.0, 1.0, 2.0, 0.5311000579389946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742144.9959286272, 742144.9959286272, 188611.5337362468]
[2019-03-26 10:42:56,291] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:42:56,297] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.3138879e-37 1.0000000e+00 3.1885451e-38 8.4372555e-28 5.4449534e-33], sampled 0.6308510376879345
[2019-03-26 10:43:13,233] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.090869], dtype=float32), 0.20445609]
[2019-03-26 10:43:13,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.32557324, 52.78354366000001, 1.0, 2.0, 0.546099202086286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763111.9270081007, 763111.9270081001, 191134.9629434195]
[2019-03-26 10:43:13,235] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:43:13,239] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.0726233e-37 1.0000000e+00 0.0000000e+00 2.4416058e-27 3.5209598e-34], sampled 0.21707043725357555
[2019-03-26 10:43:30,699] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.090869], dtype=float32), 0.20445609]
[2019-03-26 10:43:30,701] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 89.0, 1.0, 2.0, 0.5298350651994064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740376.7129195761, 740376.7129195761, 188403.1556072866]
[2019-03-26 10:43:30,702] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:43:30,703] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6162351e-36 1.0000000e+00 8.6439869e-38 2.3126802e-27 2.1064932e-32], sampled 0.7592665457949603
[2019-03-26 10:43:50,330] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.090869], dtype=float32), 0.20445609]
[2019-03-26 10:43:50,333] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.73333333333334, 78.0, 1.0, 2.0, 0.5608746490812074, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9586560051252685, 6.911199999999999, 6.9112, 168.9126846917552, 1568111.880951334, 1568111.880951334, 339937.3077954011]
[2019-03-26 10:43:50,335] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:43:50,338] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.7372207e-35 1.0000000e+00 7.9035847e-33 1.1953309e-26 4.5733111e-29], sampled 0.6897449841296134
[2019-03-26 10:43:54,913] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-26 10:43:54,949] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-26 10:43:54,979] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-26 10:43:55,119] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1701 3164000012.1395 1778.0000
[2019-03-26 10:43:55,292] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 10:43:56,307] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 500000, evaluation results [500000.0, 7884.170123390672, 3164000012.13951, 1778.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 10:44:02,617] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4894827e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 10:44:02,625] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8104
[2019-03-26 10:44:02,631] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3159174402727212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499659.2191652672, 499659.2191652672, 167001.6353500614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2930400.0000, 
sim time next is 2931000.0000, 
raw observation next is [21.0, 94.00000000000001, 1.0, 2.0, 0.3162247988953518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 500056.7546586314, 500056.754658632, 167029.6317560708], 
processed observation next is [1.0, 0.9565217391304348, 0.19431279620853087, 0.9400000000000002, 1.0, 1.0, 0.17617445650042388, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13890465407184205, 0.13890465407184222, 0.2492979578448818], 
reward next is 0.7507, 
noisyNet noise sample is [array([-1.4549102], dtype=float32), 0.3317905]. 
=============================================
[2019-03-26 10:44:02,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.189514]
 [83.15522 ]
 [83.17936 ]
 [83.169205]
 [83.209915]], R is [[83.09561157]
 [83.01539612]
 [82.93590546]
 [82.85720825]
 [82.77947998]].
[2019-03-26 10:44:05,953] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1167704e-33 4.9743635e-38], sum to 1.0000
[2019-03-26 10:44:05,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5012
[2019-03-26 10:44:05,972] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 98.0, 1.0, 2.0, 0.3118763279392696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495249.3745411828, 495249.3745411834, 166709.224319104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3001200.0000, 
sim time next is 3001800.0000, 
raw observation next is [20.16666666666667, 99.0, 1.0, 2.0, 0.3094566324914813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491993.0004156154, 491993.0004156154, 166478.719867586], 
processed observation next is [1.0, 0.7391304347826086, 0.15481832543443946, 0.99, 1.0, 1.0, 0.16802003914636301, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13666472233767094, 0.13666472233767094, 0.24847570129490448], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.14289501], dtype=float32), 1.4995475]. 
=============================================
[2019-03-26 10:44:10,653] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7041121e-35 1.0000000e+00 0.0000000e+00 5.9931892e-28 7.1097365e-35], sum to 1.0000
[2019-03-26 10:44:10,662] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1751
[2019-03-26 10:44:10,666] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.5710010803508623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859872.9306723499, 859872.9306723499, 203122.4649665443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3057600.0000, 
sim time next is 3058200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.5764305647285458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868067.4489496684, 868067.4489496684, 204174.1835280488], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 1.0, 1.0, 1.0, 0.489675379191019, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24112984693046347, 0.24112984693046347, 0.3047375873552967], 
reward next is 0.6953, 
noisyNet noise sample is [array([-0.80089927], dtype=float32), -0.6302619]. 
=============================================
[2019-03-26 10:44:18,662] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2221248e-32 3.3324842e-34], sum to 1.0000
[2019-03-26 10:44:18,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9362
[2019-03-26 10:44:18,680] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 88.16666666666667, 1.0, 2.0, 0.4575084408629382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646776.5353416, 646776.5353416, 178174.6390386999], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3219000.0000, 
sim time next is 3219600.0000, 
raw observation next is [25.33333333333334, 87.33333333333334, 1.0, 2.0, 0.4599948506132508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648857.8977596752, 648857.8977596757, 178354.138512402], 
processed observation next is [0.0, 0.2608695652173913, 0.3996840442338076, 0.8733333333333334, 1.0, 1.0, 0.34939138628102506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1802383049332431, 0.18023830493324328, 0.26620020673492834], 
reward next is 0.7338, 
noisyNet noise sample is [array([0.0684936], dtype=float32), 0.3696978]. 
=============================================
[2019-03-26 10:44:20,169] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0746188e-34 3.8366823e-38], sum to 1.0000
[2019-03-26 10:44:20,178] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5704
[2019-03-26 10:44:20,185] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 77.33333333333333, 1.0, 2.0, 0.5156798263094226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720589.8685582536, 720589.8685582536, 186090.4566880261], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3231600.0000, 
sim time next is 3232200.0000, 
raw observation next is [28.83333333333334, 78.16666666666667, 1.0, 2.0, 0.5251002316093973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733758.1036003145, 733758.1036003145, 187623.5510522571], 
processed observation next is [0.0, 0.391304347826087, 0.5655608214849924, 0.7816666666666667, 1.0, 1.0, 0.42783160434867146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2038216954445318, 0.2038216954445318, 0.28003515082426433], 
reward next is 0.7200, 
noisyNet noise sample is [array([1.0266267], dtype=float32), 0.054228086]. 
=============================================
[2019-03-26 10:44:22,513] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.4169334e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 10:44:22,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3439
[2019-03-26 10:44:22,524] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5206777220900716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727576.1151952763, 727576.1151952758, 186899.1046222705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3273000.0000, 
sim time next is 3273600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5199735503351353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726591.7946593971, 726591.7946593978, 186784.5782920096], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4216548799218497, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20183105407205476, 0.20183105407205496, 0.2787829526746412], 
reward next is 0.7212, 
noisyNet noise sample is [array([0.48620498], dtype=float32), 2.2175515]. 
=============================================
[2019-03-26 10:44:25,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.851062e-31 2.536976e-38], sum to 1.0000
[2019-03-26 10:44:25,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3895
[2019-03-26 10:44:25,726] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 68.33333333333334, 1.0, 2.0, 0.5819497667872816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813228.2178936779, 813228.2178936779, 197432.10957361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3331200.0000, 
sim time next is 3331800.0000, 
raw observation next is [32.0, 69.0, 1.0, 2.0, 0.5852834853996871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817888.6164457739, 817888.6164457739, 198037.1086386241], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.69, 1.0, 1.0, 0.5003415486743218, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2271912823460483, 0.2271912823460483, 0.2955777740874987], 
reward next is 0.7044, 
noisyNet noise sample is [array([1.4441838], dtype=float32), -0.09130558]. 
=============================================
[2019-03-26 10:44:35,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0434550e-26 1.0000000e+00 1.3541774e-26 1.1456150e-28 9.5790086e-24], sum to 1.0000
[2019-03-26 10:44:35,469] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3631
[2019-03-26 10:44:35,477] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2480369.767229521 W.
[2019-03-26 10:44:35,485] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.5912076384902966, 1.0, 2.0, 0.5912076384902966, 1.0, 1.0, 1.026732152703947, 6.9112, 6.9112, 170.5573041426782, 2480369.767229521, 2480369.767229521, 483946.3556321007], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3508800.0000, 
sim time next is 3509400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.5940647339243116, 1.0, 2.0, 0.5940647339243116, 1.0, 2.0, 1.03, 6.913104273971468, 6.9112, 170.5573041426782, 2492368.463378501, 2491004.354471826, 485836.3730836774], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.63, 1.0, 1.0, 0.5109213661738694, 1.0, 1.0, 0.5109213661738694, 1.0, 1.0, 1.0365853658536586, 0.00019042739714683065, 0.0, 0.8375144448122397, 0.6923245731606947, 0.6919456540199517, 0.7251289150502648], 
reward next is 0.2653, 
noisyNet noise sample is [array([-1.7950852], dtype=float32), 0.14221464]. 
=============================================
[2019-03-26 10:44:35,609] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3631838e-30 1.0000000e+00 1.1195282e-30 7.2479174e-35 9.9052796e-30], sum to 1.0000
[2019-03-26 10:44:35,620] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4528
[2019-03-26 10:44:35,630] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2992359.253552138 W.
[2019-03-26 10:44:35,637] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 0.0, 1.0, 0.0, 7.02697298626845, 6.9112, 170.5573041426782, 2992359.253552138, 2909426.354144062, 553041.792984762], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3508800.0000, 
sim time next is 3509400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.6214611419834252, 1.0, 2.0, 0.6214611419834252, 1.0, 1.0, 1.03, 6.966591727395378, 6.9112, 170.5573041426782, 2607428.634583469, 2567749.28705875, 495891.6825356976], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.63, 1.0, 1.0, 0.5439290867270182, 1.0, 1.0, 0.5439290867270182, 1.0, 0.5, 1.0365853658536586, 0.00553917273953779, 0.0, 0.8375144448122397, 0.7242857318287413, 0.7132636908496528, 0.7401368396055188], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2980732], dtype=float32), -1.587153]. 
=============================================
[2019-03-26 10:44:36,126] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3952721e-27 1.0000000e+00 1.0313560e-26 3.3750171e-22 5.3353490e-23], sum to 1.0000
[2019-03-26 10:44:36,133] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7590
[2019-03-26 10:44:36,140] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6021312725522473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 841441.4278245873, 841441.4278245867, 201135.2269374539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3563400.0000, 
sim time next is 3564000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6083406038433897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850122.0590868837, 850122.0590868837, 202301.3287638593], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5281212094498671, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23614501641302324, 0.23614501641302324, 0.3019422817371034], 
reward next is 0.6981, 
noisyNet noise sample is [array([0.8920063], dtype=float32), 0.108649984]. 
=============================================
[2019-03-26 10:44:36,151] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[49.465424]
 [49.47729 ]
 [49.3902  ]
 [49.265305]
 [49.078876]], R is [[49.49213409]
 [49.69701004]
 [49.90155792]
 [50.10662842]
 [50.30938339]].
[2019-03-26 10:44:39,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8347606e-31 1.0000000e+00 2.0930068e-30 2.7938634e-27 8.9616355e-27], sum to 1.0000
[2019-03-26 10:44:39,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2938
[2019-03-26 10:44:39,410] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 75.66666666666666, 1.0, 2.0, 0.8210850515792721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1147581.190334823, 1147581.190334823, 249151.2763759262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3570000.0000, 
sim time next is 3570600.0000, 
raw observation next is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.7900727741863117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1104214.658699758, 1104214.658699759, 241487.7834728067], 
processed observation next is [1.0, 0.30434782608695654, 0.5655608214849924, 0.7483333333333334, 1.0, 1.0, 0.7470756315497732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30672629408326607, 0.3067262940832664, 0.3604295275713533], 
reward next is 0.6396, 
noisyNet noise sample is [array([0.4455236], dtype=float32), 1.1383029]. 
=============================================
[2019-03-26 10:44:45,255] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.9327710e-31 1.0000000e+00 1.7755065e-29 3.6054709e-36 2.7529210e-27], sum to 1.0000
[2019-03-26 10:44:45,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4919
[2019-03-26 10:44:45,274] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2184218.30286812 W.
[2019-03-26 10:44:45,277] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5206811834685612, 1.0, 2.0, 0.5206811834685612, 1.0, 1.0, 0.9042510237862712, 6.9112, 6.9112, 170.5573041426782, 2184218.30286812, 2184218.30286812, 429741.8593626583], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3671400.0000, 
sim time next is 3672000.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5539396938239769, 1.0, 2.0, 0.5539396938239769, 1.0, 2.0, 0.962010057516183, 6.9112, 6.9112, 170.5573041426782, 2323869.361033925, 2323869.361033925, 454450.072694027], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.63, 1.0, 1.0, 0.4625779443662372, 1.0, 1.0, 0.4625779443662372, 1.0, 1.0, 0.9536708018490037, 0.0, 0.0, 0.8375144448122397, 0.645519266953868, 0.645519266953868, 0.6782836905881], 
reward next is 0.3217, 
noisyNet noise sample is [array([0.10083103], dtype=float32), 0.6822704]. 
=============================================
[2019-03-26 10:44:45,294] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[41.820187]
 [41.128166]
 [39.623978]
 [39.091644]
 [38.51436 ]], R is [[42.08281326]
 [42.02058029]
 [41.97703552]
 [41.89595413]
 [41.79022598]].
[2019-03-26 10:44:50,184] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 10:44:50,186] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:44:50,187] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:44:50,187] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:44:50,189] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:44:50,189] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:44:50,191] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:44:50,191] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:44:50,192] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:44:50,192] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:44:50,193] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:44:50,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run22
[2019-03-26 10:44:50,232] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run22
[2019-03-26 10:44:50,254] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run22
[2019-03-26 10:44:50,271] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run22
[2019-03-26 10:44:50,299] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run22
[2019-03-26 10:44:52,023] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.09634735], dtype=float32), 0.185063]
[2019-03-26 10:44:52,024] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.43333333333333, 67.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.912927720537499, 6.9112, 168.9124462614465, 1454981.470200549, 1453755.76949028, 311347.6563773376]
[2019-03-26 10:44:52,026] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:44:52,028] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3008115e-24 1.0000000e+00 7.0187972e-25 8.0305996e-12 2.1699464e-18], sampled 0.6262180483258504
[2019-03-26 10:44:55,547] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.09634735], dtype=float32), 0.185063]
[2019-03-26 10:44:55,549] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.57508689833334, 86.65978255, 1.0, 2.0, 0.2880653954184605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461088.2162001599, 461088.2162001599, 164329.4509192845]
[2019-03-26 10:44:55,549] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:44:55,552] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0404551e-30 1.0000000e+00 3.5331767e-34 5.1660395e-20 4.8498975e-28], sampled 0.9526883294376144
[2019-03-26 10:45:09,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.09634735], dtype=float32), 0.185063]
[2019-03-26 10:45:09,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.68333333333333, 45.33333333333334, 1.0, 2.0, 0.2198576090275146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 366230.3051335138, 366230.3051335145, 157592.1351600226]
[2019-03-26 10:45:09,503] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:45:09,506] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5778554e-29 1.0000000e+00 1.5523762e-34 7.0629747e-19 6.5907700e-29], sampled 0.6413790617521314
[2019-03-26 10:45:18,421] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.09634735], dtype=float32), 0.185063]
[2019-03-26 10:45:18,422] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.41296882666667, 83.25509447833335, 1.0, 2.0, 0.31739311793994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 514178.9156248581, 514178.9156248588, 168115.9939729008]
[2019-03-26 10:45:18,423] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:45:18,427] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8563861e-29 1.0000000e+00 3.5877792e-32 1.4059650e-17 2.1236621e-25], sampled 0.2071041375068926
[2019-03-26 10:45:56,259] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.09634735], dtype=float32), 0.185063]
[2019-03-26 10:45:56,260] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.65165876, 66.8011921, 1.0, 2.0, 0.6810496568129507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 951774.4323940699, 951774.4323940706, 216818.2293686434]
[2019-03-26 10:45:56,263] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:45:56,266] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1759605e-29 1.0000000e+00 3.2701515e-32 6.3049453e-17 7.9946472e-25], sampled 0.2799190821649943
[2019-03-26 10:46:04,989] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.09634735], dtype=float32), 0.185063]
[2019-03-26 10:46:04,990] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.5064369021369601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707669.8826862844, 707669.882686285, 184610.3440527181]
[2019-03-26 10:46:04,992] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:46:04,996] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.7227528e-29 1.0000000e+00 5.4216625e-31 1.9622203e-14 4.1069037e-24], sampled 0.6998373023241773
[2019-03-26 10:46:45,729] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-26 10:46:46,040] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-26 10:46:46,192] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-26 10:46:46,388] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-26 10:46:46,418] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-26 10:46:47,433] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 525000, evaluation results [525000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-26 10:46:48,065] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6165701e-26 1.0000000e+00 5.3650950e-27 1.5479541e-25 1.6509849e-25], sum to 1.0000
[2019-03-26 10:46:48,073] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0124
[2019-03-26 10:46:48,079] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2847743.53337172 W.
[2019-03-26 10:46:48,084] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 1.018009794229883, 1.0, 2.0, 1.018009794229883, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2847743.53337172, 2847743.53337172, 539964.5947853555], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3773400.0000, 
sim time next is 3774000.0000, 
raw observation next is [33.0, 63.00000000000001, 1.0, 2.0, 0.7190557985555792, 1.0, 2.0, 0.6801179387920522, 1.0, 1.0, 1.03, 7.005099234736068, 6.9112, 170.5573041426782, 2853812.663838333, 2786548.822420226, 527415.3387742961], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.6300000000000001, 1.0, 1.0, 0.6615130103079268, 1.0, 1.0, 0.6145999262554845, 1.0, 0.5, 1.0365853658536586, 0.009389923473606832, 0.0, 0.8375144448122397, 0.7927257399550925, 0.7740413395611738, 0.7871870727974569], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07361368], dtype=float32), -1.3777786]. 
=============================================
[2019-03-26 10:46:48,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[35.207573]
 [35.582664]
 [35.324226]
 [34.403587]
 [34.53157 ]], R is [[35.07456589]
 [34.72381973]
 [34.3765831 ]
 [34.2389946 ]
 [34.09566498]].
[2019-03-26 10:46:50,539] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0528395e-30 1.0000000e+00 1.2511573e-35 1.1410277e-19 5.4066145e-28], sum to 1.0000
[2019-03-26 10:46:50,547] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3074
[2019-03-26 10:46:50,554] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5202722969842882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727009.395026469, 727009.395026469, 186833.1483374799], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3802200.0000, 
sim time next is 3802800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5210283407119982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728066.2248126034, 728066.2248126034, 186956.1836537216], 
processed observation next is [0.0, 0.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42292571170120263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20224061800350096, 0.20224061800350096, 0.27903908008018147], 
reward next is 0.7210, 
noisyNet noise sample is [array([-1.0196295], dtype=float32), 0.14643365]. 
=============================================
[2019-03-26 10:46:53,470] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.648676e-34 0.000000e+00], sum to 1.0000
[2019-03-26 10:46:53,484] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0892
[2019-03-26 10:46:53,488] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 57.33333333333333, 1.0, 2.0, 0.6093395870336394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851518.6419786054, 851518.6419786054, 202499.3785488599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3854400.0000, 
sim time next is 3855000.0000, 
raw observation next is [35.0, 56.66666666666667, 1.0, 2.0, 0.6045622374870508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844839.9029176588, 844839.9029176594, 201599.4660853402], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5666666666666668, 1.0, 1.0, 0.523568960827772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23467775081046077, 0.23467775081046094, 0.3008947255005077], 
reward next is 0.6991, 
noisyNet noise sample is [array([0.9935554], dtype=float32), 1.6212001]. 
=============================================
[2019-03-26 10:46:53,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.643486]
 [83.5081  ]
 [83.38002 ]
 [83.26666 ]
 [83.18794 ]], R is [[83.61588287]
 [83.47748566]
 [83.33918762]
 [83.20069122]
 [83.06167603]].
[2019-03-26 10:46:53,827] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3139799e-36 1.0000000e+00 0.0000000e+00 1.1503594e-25 7.6177882e-32], sum to 1.0000
[2019-03-26 10:46:53,838] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1734
[2019-03-26 10:46:53,846] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.83333333333334, 57.83333333333334, 1.0, 2.0, 0.5499505337870324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768495.674717689, 768495.674717689, 191792.1702312551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3867000.0000, 
sim time next is 3867600.0000, 
raw observation next is [32.66666666666667, 59.66666666666667, 1.0, 2.0, 0.5455580731291005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 762355.4890822403, 762355.4890822397, 191042.6626692287], 
processed observation next is [0.0, 0.782608695652174, 0.7472353870458138, 0.5966666666666667, 1.0, 1.0, 0.45247960617963917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21176541363395562, 0.21176541363395546, 0.2851383024913861], 
reward next is 0.7149, 
noisyNet noise sample is [array([-0.10221961], dtype=float32), 0.12287229]. 
=============================================
[2019-03-26 10:46:57,569] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.7592788e-37 1.0000000e+00 0.0000000e+00 1.0744945e-29 4.7908148e-29], sum to 1.0000
[2019-03-26 10:46:57,577] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8323
[2019-03-26 10:46:57,582] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.66666666666667, 62.5, 1.0, 2.0, 0.615529330686184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 860171.9717559528, 860171.9717559528, 203674.4557406329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3953400.0000, 
sim time next is 3954000.0000, 
raw observation next is [33.33333333333334, 65.0, 1.0, 2.0, 0.6086731510854085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850586.961389273, 850586.961389273, 202373.494847292], 
processed observation next is [0.0, 0.782608695652174, 0.7788309636650873, 0.65, 1.0, 1.0, 0.5285218687776007, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23627415594146473, 0.23627415594146473, 0.30204999230939106], 
reward next is 0.6980, 
noisyNet noise sample is [array([0.20129524], dtype=float32), -1.5869294]. 
=============================================
[2019-03-26 10:46:57,605] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[83.27965]
 [83.31666]
 [83.24092]
 [83.20193]
 [83.12843]], R is [[83.19325256]
 [83.05732727]
 [82.92769623]
 [82.79837799]
 [82.66937256]].
[2019-03-26 10:47:03,913] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0966450e-29 1.0000000e+00 4.6366857e-31 5.1513565e-21 2.1857917e-16], sum to 1.0000
[2019-03-26 10:47:03,923] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4913
[2019-03-26 10:47:03,927] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.96666666666667, 84.16666666666667, 1.0, 2.0, 0.5423837146603072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757918.1008522207, 757918.1008522207, 190503.3959678632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4061400.0000, 
sim time next is 4062000.0000, 
raw observation next is [27.93333333333333, 84.33333333333334, 1.0, 2.0, 0.542590815199551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758207.6030335285, 758207.6030335291, 190538.4164049662], 
processed observation next is [1.0, 0.0, 0.522906793048973, 0.8433333333333334, 1.0, 1.0, 0.44890459662596505, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21061322306486904, 0.2106132230648692, 0.28438569612681525], 
reward next is 0.7156, 
noisyNet noise sample is [array([0.02521082], dtype=float32), 0.71331483]. 
=============================================
[2019-03-26 10:47:03,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[60.98693 ]
 [61.84606 ]
 [63.481743]
 [63.490593]
 [63.479572]], R is [[60.59229279]
 [60.70203781]
 [60.81079102]
 [60.91872025]
 [61.0257988 ]].
[2019-03-26 10:47:04,008] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4640998e-23 1.0000000e+00 4.0264723e-23 3.5004315e-18 6.9385906e-09], sum to 1.0000
[2019-03-26 10:47:04,018] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6709
[2019-03-26 10:47:04,023] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 85.5, 1.0, 2.0, 0.5400614900624227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754671.9095572466, 754671.909557246, 190111.1631032673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4066200.0000, 
sim time next is 4066800.0000, 
raw observation next is [27.66666666666666, 85.66666666666666, 1.0, 2.0, 0.5398106426584461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754321.2555846643, 754321.255584665, 190068.885851162], 
processed observation next is [1.0, 0.043478260869565216, 0.5102685624012636, 0.8566666666666666, 1.0, 1.0, 0.44555499115475433, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2095336821068512, 0.2095336821068514, 0.28368490425546566], 
reward next is 0.7163, 
noisyNet noise sample is [array([-0.31739148], dtype=float32), -0.2087336]. 
=============================================
[2019-03-26 10:47:04,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0177727e-23 1.0000000e+00 2.7059696e-23 9.6041898e-15 2.3388562e-09], sum to 1.0000
[2019-03-26 10:47:04,214] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8043
[2019-03-26 10:47:04,222] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 84.83333333333334, 1.0, 2.0, 0.5416925017101607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756951.8670642086, 756951.867064208, 190386.4460029551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4063800.0000, 
sim time next is 4064400.0000, 
raw observation next is [27.8, 85.0, 1.0, 2.0, 0.5417378231236277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757015.2210022269, 757015.2210022275, 190394.0819614282], 
processed observation next is [1.0, 0.043478260869565216, 0.5165876777251186, 0.85, 1.0, 1.0, 0.44787689532967195, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21028200583395193, 0.2102820058339521, 0.2841702715842212], 
reward next is 0.7158, 
noisyNet noise sample is [array([-2.0449128], dtype=float32), -0.5995741]. 
=============================================
[2019-03-26 10:47:17,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3362185e-30 1.0000000e+00 9.3313915e-28 2.3579658e-38 7.1041315e-18], sum to 1.0000
[2019-03-26 10:47:17,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3245
[2019-03-26 10:47:17,739] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.66666666666667, 48.66666666666666, 1.0, 2.0, 0.5627929715880925, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564928155, 786448.2194416778, 786448.2194416778, 194021.6928221727], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4296000.0000, 
sim time next is 4296600.0000, 
raw observation next is [36.5, 49.0, 1.0, 2.0, 0.5515995775436869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104266, 770800.8689649045, 770800.8689649045, 192078.8100409906], 
processed observation next is [1.0, 0.7391304347826086, 0.9289099526066351, 0.49, 1.0, 1.0, 0.4597585271610686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.829439945152281, 0.21411135249025123, 0.21411135249025123, 0.2866847911059561], 
reward next is 0.7133, 
noisyNet noise sample is [array([2.9570887], dtype=float32), -0.76631546]. 
=============================================
[2019-03-26 10:47:19,635] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4254364e-28 1.0000000e+00 5.4878119e-23 3.3572780e-33 1.1011247e-12], sum to 1.0000
[2019-03-26 10:47:19,641] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7948
[2019-03-26 10:47:19,650] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3111628.087882918 W.
[2019-03-26 10:47:19,653] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.33333333333334, 56.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.193276656629713, 6.9112, 170.5573041426782, 3111628.087882918, 2909565.104594273, 552169.6409824602], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4376400.0000, 
sim time next is 4377000.0000, 
raw observation next is [35.66666666666666, 56.83333333333333, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.881559847966543, 6.9112, 170.5573041426782, 3605247.72464378, 2910139.493830395, 548077.5882255172], 
processed observation next is [1.0, 0.6521739130434783, 0.889415481832543, 0.5683333333333332, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.09703598479665435, 0.0, 0.8375144448122397, 1.0014577012899388, 0.8083720816195541, 0.8180262510828614], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.423702], dtype=float32), 0.12525025]. 
=============================================
[2019-03-26 10:47:19,666] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[35.395214]
 [36.12475 ]
 [36.305477]
 [36.01636 ]
 [36.400223]], R is [[33.80469894]
 [33.46665192]
 [33.34386826]
 [33.01042938]
 [32.68032455]].
[2019-03-26 10:47:24,487] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4634116e-34 1.0000000e+00 1.2754559e-33 4.0934462e-27 1.7788443e-19], sum to 1.0000
[2019-03-26 10:47:24,498] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4132
[2019-03-26 10:47:24,504] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 85.66666666666667, 1.0, 2.0, 0.6171029704802917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862371.9487547625, 862371.9487547625, 203975.584667055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4414800.0000, 
sim time next is 4415400.0000, 
raw observation next is [29.5, 86.5, 1.0, 2.0, 0.615433754593709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 860038.3547904453, 860038.3547904453, 203656.5748547878], 
processed observation next is [0.0, 0.08695652173913043, 0.5971563981042655, 0.865, 1.0, 1.0, 0.5366671742092879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23889954299734592, 0.23889954299734592, 0.3039650370966982], 
reward next is 0.6960, 
noisyNet noise sample is [array([-0.91758394], dtype=float32), 1.1581484]. 
=============================================
[2019-03-26 10:47:27,715] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9210411e-35 1.0000000e+00 1.8271000e-37 2.1402551e-23 1.8008588e-24], sum to 1.0000
[2019-03-26 10:47:27,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2535
[2019-03-26 10:47:27,731] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 79.00000000000001, 1.0, 2.0, 0.5937601008905806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 829738.660852878, 829738.6608528786, 199588.4261845828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4471800.0000, 
sim time next is 4472400.0000, 
raw observation next is [29.66666666666667, 79.0, 1.0, 2.0, 0.5847947394891898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 817205.3689186447, 817205.368918644, 197946.8030978707], 
processed observation next is [0.0, 0.782608695652174, 0.6050552922590839, 0.79, 1.0, 1.0, 0.4997526981797467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2270014913662902, 0.22700149136629, 0.29544298969831445], 
reward next is 0.7046, 
noisyNet noise sample is [array([0.1245859], dtype=float32), 0.24793875]. 
=============================================
[2019-03-26 10:47:29,345] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5352522e-32 1.0000000e+00 2.5193131e-34 1.1141380e-24 5.4552280e-24], sum to 1.0000
[2019-03-26 10:47:29,356] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6395
[2019-03-26 10:47:29,361] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 84.0, 1.0, 2.0, 0.5266037821918546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735859.8443584514, 735859.844358452, 187868.5678555646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4488600.0000, 
sim time next is 4489200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.521256030414582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728384.499280889, 728384.4992808898, 186992.7269796804], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4232000366440747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20232902757802473, 0.20232902757802493, 0.27909362235773194], 
reward next is 0.7209, 
noisyNet noise sample is [array([-0.6723503], dtype=float32), -0.69385844]. 
=============================================
[2019-03-26 10:47:32,911] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4803084e-29 1.0000000e+00 1.4533590e-32 8.1532605e-25 1.9702218e-23], sum to 1.0000
[2019-03-26 10:47:32,914] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4930
[2019-03-26 10:47:32,927] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5256229792935879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734488.8268748581, 734488.8268748581, 187707.9301477585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4561800.0000, 
sim time next is 4562400.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5263370743355337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735487.0260210639, 735487.0260210639, 187825.1955856867], 
processed observation next is [0.0, 0.8260869565217391, 0.5576619273301741, 0.7566666666666667, 1.0, 1.0, 0.4293217763078719, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20430195167251775, 0.20430195167251775, 0.2803361128144578], 
reward next is 0.7197, 
noisyNet noise sample is [array([-1.144336], dtype=float32), -1.3319336]. 
=============================================
[2019-03-26 10:47:40,826] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-26 10:47:40,828] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:47:40,829] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:47:40,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:47:40,832] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:47:40,830] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:47:40,831] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:47:40,833] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:47:40,835] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:47:40,836] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:47:40,838] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:47:40,858] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run23
[2019-03-26 10:47:40,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run23
[2019-03-26 10:47:40,899] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run23
[2019-03-26 10:47:40,902] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run23
[2019-03-26 10:47:40,920] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run23
[2019-03-26 10:47:42,112] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15893596], dtype=float32), 0.21465401]
[2019-03-26 10:47:42,114] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.9, 61.5, 1.0, 2.0, 0.6954945418707688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 971970.5568629194, 971970.5568629188, 219871.9829838784]
[2019-03-26 10:47:42,116] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:47:42,119] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.3763514e-28 1.0000000e+00 8.1219576e-28 1.9009322e-17 3.6599803e-13], sampled 0.3432444222965576
[2019-03-26 10:47:54,786] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15893596], dtype=float32), 0.21465401]
[2019-03-26 10:47:54,787] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.33333333333334, 89.0, 1.0, 2.0, 0.3431394154463024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532919.8848588557, 532919.8848588563, 169363.1916564574]
[2019-03-26 10:47:54,790] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:47:54,793] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7241636e-31 1.0000000e+00 3.8036268e-33 1.1142412e-22 4.0750762e-20], sampled 0.9080354025577928
[2019-03-26 10:48:30,313] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15893596], dtype=float32), 0.21465401]
[2019-03-26 10:48:30,314] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 79.0, 1.0, 2.0, 0.5853135828045638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 817930.6914463717, 817930.691446371, 198042.0032651483]
[2019-03-26 10:48:30,317] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:48:30,321] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4774610e-30 1.0000000e+00 5.1850204e-32 1.2112353e-19 1.1829706e-17], sampled 0.9526592263483664
[2019-03-26 10:48:40,261] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15893596], dtype=float32), 0.21465401]
[2019-03-26 10:48:40,262] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.2, 47.0, 1.0, 2.0, 0.7287209033220824, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988899858978005, 6.9112, 168.9124305444076, 1915320.265902097, 1860197.48336378, 391636.6199359259]
[2019-03-26 10:48:40,263] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:48:40,266] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5576296e-24 9.9999857e-01 3.9811227e-22 2.2362238e-12 1.4463742e-06], sampled 0.957171340455004
[2019-03-26 10:48:40,267] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1915320.265902097 W.
[2019-03-26 10:48:41,324] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.15893596], dtype=float32), 0.21465401]
[2019-03-26 10:48:41,326] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.6754614, 75.72313923, 1.0, 2.0, 0.8615030003958903, 1.0, 2.0, 0.8615030003958903, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 2409501.903602191, 2409501.903602191, 451153.2965379093]
[2019-03-26 10:48:41,328] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:48:41,331] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.8158517e-22 9.9996364e-01 2.2935838e-20 8.8823473e-09 3.6356985e-05], sampled 0.4743354448864253
[2019-03-26 10:48:41,332] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2409501.903602191 W.
[2019-03-26 10:48:58,225] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.15893596], dtype=float32), 0.21465401]
[2019-03-26 10:48:58,226] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.705808385, 86.19126576, 1.0, 2.0, 0.5958116735215593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832606.7153551618, 832606.7153551618, 199970.0274401345]
[2019-03-26 10:48:58,227] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:48:58,230] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.4985312e-28 1.0000000e+00 1.1538781e-28 8.1674265e-17 3.5006771e-15], sampled 0.5169901991949113
[2019-03-26 10:49:06,612] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15893596], dtype=float32), 0.21465401]
[2019-03-26 10:49:06,613] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.83439667, 58.991940125, 1.0, 2.0, 0.9478492643593689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1324862.445696673, 1324862.445696673, 283445.2635932989]
[2019-03-26 10:49:06,615] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:49:06,617] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2206795e-26 1.0000000e+00 3.4077687e-25 8.6789749e-16 1.1540696e-09], sampled 0.15348733318148566
[2019-03-26 10:49:17,000] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15893596], dtype=float32), 0.21465401]
[2019-03-26 10:49:17,003] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.2, 71.0, 1.0, 2.0, 0.550528076887877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769303.0202411287, 769303.0202411287, 191891.1847019295]
[2019-03-26 10:49:17,004] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:49:17,007] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5586300e-27 1.0000000e+00 8.0378972e-30 1.4479721e-13 2.5491112e-16], sampled 0.2664556888249301
[2019-03-26 10:49:36,212] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.3374 2927742795.9574 1347.0000
[2019-03-26 10:49:36,700] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6350 2842491926.9726 1128.0000
[2019-03-26 10:49:36,832] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.4485 2779553741.6287 921.0000
[2019-03-26 10:49:36,885] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.3316 3007752738.1584 1758.0000
[2019-03-26 10:49:36,901] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.3404 3163480037.2387 1786.0000
[2019-03-26 10:49:37,919] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 550000, evaluation results [550000.0, 7885.340365555697, 3163480037.2387056, 1786.0, 8255.337435559108, 2927742795.9573936, 1347.0, 8664.448481247233, 2779553741.6286883, 921.0, 7997.33160091952, 3007752738.158432, 1758.0, 8496.634982090163, 2842491926.9726377, 1128.0]
[2019-03-26 10:49:38,353] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.9468898e-21 8.9612055e-01 8.4547917e-18 1.2505469e-19 1.0387940e-01], sum to 1.0000
[2019-03-26 10:49:38,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9286
[2019-03-26 10:49:38,363] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666666, 71.66666666666667, 1.0, 2.0, 0.5961440004455558, 1.0, 2.0, 0.5961440004455558, 1.0, 1.0, 1.03, 6.917163563195301, 6.9112, 170.5573041426782, 2501100.65127302, 2496828.708373984, 486583.9202206482], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4704000.0000, 
sim time next is 4704600.0000, 
raw observation next is [30.83333333333334, 70.83333333333333, 1.0, 2.0, 0.596094627792049, 1.0, 2.0, 0.596094627792049, 1.0, 2.0, 1.03, 6.917067174126713, 6.9112, 170.5573041426782, 2500893.302790482, 2496690.407302132, 486566.1319699459], 
processed observation next is [1.0, 0.43478260869565216, 0.6603475513428123, 0.7083333333333333, 1.0, 1.0, 0.5133670214362036, 1.0, 1.0, 0.5133670214362036, 1.0, 1.0, 1.0365853658536586, 0.0005867174126713337, 0.0, 0.8375144448122397, 0.6946925841084672, 0.6935251131394811, 0.7262181074178298], 
reward next is 0.2444, 
noisyNet noise sample is [array([0.11880226], dtype=float32), 0.681637]. 
=============================================
[2019-03-26 10:49:39,389] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.9785011e-30 1.0000000e+00 1.0241908e-30 1.3358552e-34 2.8696500e-18], sum to 1.0000
[2019-03-26 10:49:39,398] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2057
[2019-03-26 10:49:39,402] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 70.0, 1.0, 2.0, 0.5069605332519705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708401.8228894348, 708401.8228894342, 184697.056501516], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4728600.0000, 
sim time next is 4729200.0000, 
raw observation next is [30.33333333333334, 70.0, 1.0, 2.0, 0.5135358392008693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717592.936849491, 717592.9368494903, 185746.4177643541], 
processed observation next is [1.0, 0.7391304347826086, 0.6366508688783573, 0.7, 1.0, 1.0, 0.4138986014468305, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19933137134708082, 0.19933137134708062, 0.2772334593497822], 
reward next is 0.7228, 
noisyNet noise sample is [array([-0.5463806], dtype=float32), -1.0692096]. 
=============================================
[2019-03-26 10:49:40,959] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3714145e-28 1.0000000e+00 8.8816486e-29 2.3046226e-19 1.3984603e-17], sum to 1.0000
[2019-03-26 10:49:40,969] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1560
[2019-03-26 10:49:40,975] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5156445368819844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720540.5398331, 720540.5398331007, 186083.7356781453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4741200.0000, 
sim time next is 4741800.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.5152865481673516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720040.1315345133, 720040.131534514, 186026.0277832874], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.41600788935825495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2000111476484759, 0.2000111476484761, 0.27765078773624985], 
reward next is 0.7223, 
noisyNet noise sample is [array([-0.9860779], dtype=float32), 2.2627547]. 
=============================================
[2019-03-26 10:49:43,090] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7985210e-22 9.9992752e-01 2.4018946e-17 1.3636767e-15 7.2491617e-05], sum to 1.0000
[2019-03-26 10:49:43,097] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6266
[2019-03-26 10:49:43,102] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.8040646589605409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1123780.223190856, 1123780.223190856, 244908.5317874175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5199000.0000, 
sim time next is 5199600.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.7720453625760281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079006.543133964, 1079006.543133964, 237162.2853708778], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.89, 1.0, 1.0, 0.725355858525335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29972403975943446, 0.29972403975943446, 0.3539735602550415], 
reward next is 0.6460, 
noisyNet noise sample is [array([-0.1022715], dtype=float32), 1.3518947]. 
=============================================
[2019-03-26 10:49:43,751] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0811447e-24 9.9639314e-01 3.0557316e-22 6.3765744e-20 3.6069041e-03], sum to 1.0000
[2019-03-26 10:49:43,759] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2480
[2019-03-26 10:49:43,768] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2417336.279278472 W.
[2019-03-26 10:49:43,773] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.5, 65.0, 1.0, 2.0, 0.8642967491981542, 1.0, 2.0, 0.8642967491981542, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2417336.279278472, 2417336.279278472, 452385.4188848557], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4802400.0000, 
sim time next is 4803000.0000, 
raw observation next is [31.41666666666666, 65.16666666666667, 1.0, 2.0, 0.8113799814923054, 1.0, 2.0, 0.8113799814923054, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2269199.934757526, 2269199.934757527, 425448.6408294724], 
processed observation next is [1.0, 0.6086956521739131, 0.6879936808846759, 0.6516666666666667, 1.0, 1.0, 0.77274696565338, 1.0, 1.0, 0.77274696565338, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6303333152104239, 0.6303333152104241, 0.6349979713872722], 
reward next is 0.3650, 
noisyNet noise sample is [array([0.59218585], dtype=float32), -1.2309686]. 
=============================================
[2019-03-26 10:49:43,783] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[68.20856 ]
 [68.672485]
 [68.624855]
 [71.27786 ]
 [72.72913 ]], R is [[68.32041931]
 [67.96201324]
 [67.28239441]
 [66.60957336]
 [66.24378204]].
[2019-03-26 10:49:46,624] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4020430e-23 1.0000000e+00 3.1445869e-24 6.4655583e-15 2.0584988e-08], sum to 1.0000
[2019-03-26 10:49:46,636] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9011
[2019-03-26 10:49:46,643] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.756665916836954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1057501.607943382, 1057501.607943382, 233548.500064555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4849200.0000, 
sim time next is 4849800.0000, 
raw observation next is [27.0, 79.83333333333334, 1.0, 2.0, 0.6573587196675788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 918651.7662806674, 918651.7662806674, 211907.3129420844], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.7983333333333335, 1.0, 1.0, 0.587179180322384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2551810461890743, 0.2551810461890743, 0.31627957155534986], 
reward next is 0.6837, 
noisyNet noise sample is [array([-0.15178223], dtype=float32), 1.5193127]. 
=============================================
[2019-03-26 10:49:48,044] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2314853e-24 9.9999857e-01 2.8433167e-20 4.6412850e-29 1.3820076e-06], sum to 1.0000
[2019-03-26 10:49:48,052] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8529
[2019-03-26 10:49:48,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2191060.983634938 W.
[2019-03-26 10:49:48,065] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5223106977031231, 1.0, 2.0, 0.5223106977031231, 1.0, 1.0, 0.9043171406207356, 6.9112, 6.9112, 170.5573041426782, 2191060.983634938, 2191060.983634938, 430393.9688172733], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5471744727580515, 1.0, 2.0, 0.5471744727580515, 1.0, 2.0, 0.9482683008015323, 6.911200000000001, 6.9112, 170.5573041426782, 2295462.06552107, 2295462.06552107, 448903.8082945814], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.45442707561211027, 1.0, 1.0, 0.45442707561211027, 1.0, 1.0, 0.9369125619530881, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6376283515336305, 0.6376283515336305, 0.6700056840217633], 
reward next is 0.3300, 
noisyNet noise sample is [array([-0.343469], dtype=float32), -0.24725622]. 
=============================================
[2019-03-26 10:49:48,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6442409e-18 9.9199486e-01 1.0459488e-18 5.6084343e-12 8.0052130e-03], sum to 1.0000
[2019-03-26 10:49:48,187] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6472
[2019-03-26 10:49:48,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1841556.717754675 W.
[2019-03-26 10:49:48,197] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.6760084946614636, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98236626229794, 6.9112, 168.9125386574504, 1841556.717754675, 1791069.046822346, 380385.979042026], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4870800.0000, 
sim time next is 4871400.0000, 
raw observation next is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.7678779784838424, 1.0, 1.0, 0.7678779784838424, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2147423.29934708, 2147423.29934708, 404531.3601591509], 
processed observation next is [1.0, 0.391304347826087, 0.581358609794629, 0.7333333333333334, 1.0, 1.0, 0.7203349138359547, 1.0, 0.5, 0.7203349138359547, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5965064720408555, 0.5965064720408555, 0.60377814949127], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29970458], dtype=float32), 0.44724035]. 
=============================================
[2019-03-26 10:50:00,926] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1913659e-26 1.0000000e+00 7.1772220e-31 3.7978091e-20 1.5214665e-21], sum to 1.0000
[2019-03-26 10:50:00,940] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0904
[2019-03-26 10:50:00,948] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5183896779389829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724377.795350636, 724377.7953506367, 186526.8721398202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5098200.0000, 
sim time next is 5098800.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5162498980852747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721386.7339989959, 721386.7339989964, 186180.6622535018], 
processed observation next is [0.0, 0.0, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.41716855190996943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20038520388860995, 0.20038520388861011, 0.27788158545298774], 
reward next is 0.7221, 
noisyNet noise sample is [array([1.3516791], dtype=float32), 1.0701007]. 
=============================================
[2019-03-26 10:50:08,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1911024e-31 1.0000000e+00 5.5302770e-33 6.4891938e-26 5.2522236e-26], sum to 1.0000
[2019-03-26 10:50:08,163] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0757
[2019-03-26 10:50:08,168] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.85, 92.5, 1.0, 2.0, 0.5159975761444457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721034.0298569204, 721034.0298569197, 186140.2664358656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5621400.0000, 
sim time next is 5622000.0000, 
raw observation next is [25.8, 92.66666666666667, 1.0, 2.0, 0.5148374441949914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719412.359690964, 719412.3596909647, 185953.2053432669], 
processed observation next is [0.0, 0.043478260869565216, 0.42180094786729866, 0.9266666666666667, 1.0, 1.0, 0.41546680023492943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19983676658082333, 0.19983676658082353, 0.277542097527264], 
reward next is 0.7225, 
noisyNet noise sample is [array([-1.0818366], dtype=float32), 1.4737033]. 
=============================================
[2019-03-26 10:50:08,185] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[61.010403]
 [60.597637]
 [60.1854  ]
 [59.733086]
 [59.152794]], R is [[61.51302719]
 [61.62007523]
 [61.7258606 ]
 [61.83045959]
 [61.93370438]].
[2019-03-26 10:50:08,380] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.25703255e-17 9.98946607e-01 3.64434782e-15 1.48413303e-24
 1.05347158e-03], sum to 1.0000
[2019-03-26 10:50:08,388] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6254
[2019-03-26 10:50:08,395] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2719465.766529178 W.
[2019-03-26 10:50:08,402] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 69.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.521190089976887, 6.9112, 168.9095828435121, 2719465.766529178, 2286726.445655154, 474880.1615603779], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5223000.0000, 
sim time next is 5223600.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.9094455991023243, 1.0, 1.0, 0.9094455991023243, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2543740.831544775, 2543740.831544775, 476670.899871411], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.890898312171475, 1.0, 0.5, 0.890898312171475, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7065946754291041, 0.7065946754291041, 0.711449104285688], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6932076], dtype=float32), 0.20134692]. 
=============================================
[2019-03-26 10:50:09,472] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5046257e-24 9.9999774e-01 2.7359198e-22 3.6700676e-31 2.2078586e-06], sum to 1.0000
[2019-03-26 10:50:09,483] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1504
[2019-03-26 10:50:09,489] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 70.83333333333334, 1.0, 2.0, 0.2780649003698497, 1.0, 2.0, 0.2780649003698497, 1.0, 1.0, 0.4829067744746841, 6.9112, 6.9112, 170.5573041426782, 1165907.647133067, 1165907.647133067, 295667.7168079271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5245800.0000, 
sim time next is 5246400.0000, 
raw observation next is [30.66666666666667, 71.66666666666667, 1.0, 2.0, 0.538301949564169, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129565104302, 752212.2897112775, 752212.2897112775, 189816.996335308], 
processed observation next is [1.0, 0.7391304347826086, 0.6524486571879939, 0.7166666666666667, 1.0, 1.0, 0.4437372886315289, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451522988, 0.20894785825313264, 0.20894785825313264, 0.2833089497541911], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33191696], dtype=float32), 0.3341551]. 
=============================================
[2019-03-26 10:50:11,484] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.8634049e-26 1.0000000e+00 3.4465628e-22 5.6595535e-30 2.9809243e-08], sum to 1.0000
[2019-03-26 10:50:11,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1455
[2019-03-26 10:50:11,500] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2786018.021420646 W.
[2019-03-26 10:50:11,508] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.73333333333333, 54.0, 1.0, 2.0, 0.9959687705381912, 1.0, 2.0, 0.9959687705381912, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2786018.021420646, 2786018.021420646, 526559.801922099], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5308800.0000, 
sim time next is 5309400.0000, 
raw observation next is [36.06666666666666, 52.5, 1.0, 2.0, 1.012121960283354, 1.0, 2.0, 1.012121960283354, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2831254.455997037, 2831254.455997037, 536358.8950547964], 
processed observation next is [1.0, 0.43478260869565216, 0.9083728278041072, 0.525, 1.0, 1.0, 1.0146047714257278, 1.0, 1.0, 1.0146047714257278, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.786459571110288, 0.786459571110288, 0.8005356642608901], 
reward next is 0.1995, 
noisyNet noise sample is [array([-1.0288273], dtype=float32), -0.10287856]. 
=============================================
[2019-03-26 10:50:13,331] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5567326e-23 9.9921227e-01 6.2371506e-20 4.2661504e-30 7.8770908e-04], sum to 1.0000
[2019-03-26 10:50:13,340] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2968
[2019-03-26 10:50:13,350] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3049927.254884924 W.
[2019-03-26 10:50:13,354] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.3, 52.0, 1.0, 2.0, 0.8124175029137466, 1.0, 2.0, 0.7267987909711359, 1.0, 2.0, 1.03, 7.005106597846311, 6.9112, 170.5573041426782, 3049927.254884924, 2982658.138971439, 559187.3678730129], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5313600.0000, 
sim time next is 5314200.0000, 
raw observation next is [36.28333333333333, 52.0, 1.0, 2.0, 0.7228594565918539, 1.0, 2.0, 0.6820197678101895, 1.0, 2.0, 1.03, 7.005099534672089, 6.9112, 170.5573041426782, 2861801.985311664, 2794537.929037185, 528647.1032678863], 
processed observation next is [1.0, 0.5217391304347826, 0.9186413902053712, 0.52, 1.0, 1.0, 0.6660957308335589, 1.0, 1.0, 0.6168912865183005, 1.0, 1.0, 1.0365853658536586, 0.009389953467208922, 0.0, 0.8375144448122397, 0.7949449959199067, 0.7762605358436625, 0.789025527265502], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.86496025], dtype=float32), 0.92550737]. 
=============================================
[2019-03-26 10:50:13,446] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9768531e-27 1.0000000e+00 5.8827434e-28 1.5013557e-22 1.5335298e-19], sum to 1.0000
[2019-03-26 10:50:13,452] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7505
[2019-03-26 10:50:13,456] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333333, 85.83333333333334, 1.0, 2.0, 0.601075123746598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839964.9412154674, 839964.9412154674, 200946.6987901429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5363400.0000, 
sim time next is 5364000.0000, 
raw observation next is [29.3, 86.0, 1.0, 2.0, 0.6006835132853975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 839417.4738056152, 839417.4738056146, 200873.6446078068], 
processed observation next is [1.0, 0.08695652173913043, 0.5876777251184835, 0.86, 1.0, 1.0, 0.518895799139033, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2331715205015598, 0.23317152050155962, 0.29981140986239824], 
reward next is 0.7002, 
noisyNet noise sample is [array([2.5220373], dtype=float32), 0.31904513]. 
=============================================
[2019-03-26 10:50:13,471] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[49.53113 ]
 [49.49376 ]
 [49.486244]
 [49.484592]
 [49.436737]], R is [[49.99414062]
 [50.19427872]
 [50.39225006]
 [50.5880394 ]
 [50.7817421 ]].
[2019-03-26 10:50:15,413] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.446206e-32 1.000000e+00 1.031793e-33 1.752577e-25 7.395440e-27], sum to 1.0000
[2019-03-26 10:50:15,420] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3814
[2019-03-26 10:50:15,429] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.4, 81.0, 1.0, 2.0, 0.6137134874838422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 857633.3956585486, 857633.3956585493, 203328.8708518688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5349600.0000, 
sim time next is 5350200.0000, 
raw observation next is [30.31666666666666, 81.5, 1.0, 2.0, 0.6153620116768689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859938.0570115658, 859938.0570115658, 203642.9743526781], 
processed observation next is [1.0, 0.9565217391304348, 0.6358609794628749, 0.815, 1.0, 1.0, 0.536580736960083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23887168250321272, 0.23887168250321272, 0.30394473783981807], 
reward next is 0.6961, 
noisyNet noise sample is [array([0.08793932], dtype=float32), -0.82502586]. 
=============================================
[2019-03-26 10:50:19,445] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.6282953e-34 1.0000000e+00 7.7387209e-33 3.6989623e-37 1.4076511e-23], sum to 1.0000
[2019-03-26 10:50:19,454] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8930
[2019-03-26 10:50:19,464] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.9, 74.33333333333333, 1.0, 2.0, 0.560677956927181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783491.6029557209, 783491.6029557209, 193651.3025116284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5421000.0000, 
sim time next is 5421600.0000, 
raw observation next is [30.9, 75.0, 1.0, 2.0, 0.5725982609448191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800155.3147649188, 800155.3147649188, 195753.8019869927], 
processed observation next is [1.0, 0.782608695652174, 0.6635071090047393, 0.75, 1.0, 1.0, 0.4850581457166495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22226536521247744, 0.22226536521247744, 0.29216985371192944], 
reward next is 0.7078, 
noisyNet noise sample is [array([-0.11901748], dtype=float32), -1.0056907]. 
=============================================
[2019-03-26 10:50:27,868] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1210897e-23 1.0000000e+00 2.2114987e-23 1.7762345e-18 1.6815081e-12], sum to 1.0000
[2019-03-26 10:50:27,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5988
[2019-03-26 10:50:27,885] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.25, 84.0, 1.0, 2.0, 0.9526288836298518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1331547.379096038, 1331547.379096038, 284826.1462349914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5556600.0000, 
sim time next is 5557200.0000, 
raw observation next is [28.43333333333333, 83.33333333333334, 1.0, 2.0, 0.8754328606615432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1223583.621110263, 1223583.621110263, 263270.877240783], 
processed observation next is [1.0, 0.30434782608695654, 0.546603475513428, 0.8333333333333335, 1.0, 1.0, 0.849919109230775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33988433919729527, 0.33988433919729527, 0.3929416078220642], 
reward next is 0.6071, 
noisyNet noise sample is [array([-1.3898346], dtype=float32), 1.616932]. 
=============================================
[2019-03-26 10:50:28,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8635272e-23 1.0000000e+00 2.9773469e-25 3.4581671e-21 1.2702669e-08], sum to 1.0000
[2019-03-26 10:50:28,802] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6201
[2019-03-26 10:50:28,814] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2405321.110373792 W.
[2019-03-26 10:50:28,820] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.93333333333334, 49.33333333333333, 1.0, 2.0, 0.8600049646511422, 1.0, 1.0, 0.8600049646511422, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2405321.110373792, 2405321.110373791, 450133.2783603748], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5582400.0000, 
sim time next is 5583000.0000, 
raw observation next is [33.86666666666666, 49.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.66285355707741, 6.9112, 168.9088633514949, 2834887.255560663, 2301651.274128438, 475231.2236452129], 
processed observation next is [1.0, 0.6086956521739131, 0.8041074249605052, 0.4966666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.07516535570774101, 0.0, 0.82941984586807, 0.7874686821001842, 0.6393475761467883, 0.7093003337988252], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.07099888], dtype=float32), 1.1557411]. 
=============================================
[2019-03-26 10:50:28,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[59.037907]
 [62.27271 ]
 [61.54169 ]
 [61.702362]
 [61.479244]], R is [[55.0768013 ]
 [54.85419464]
 [54.30565262]
 [53.76259613]
 [53.53285599]].
[2019-03-26 10:50:31,775] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 10:50:31,778] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:50:31,780] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:50:31,780] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:50:31,781] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:50:31,781] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:50:31,782] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:50:31,783] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:50:31,782] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:50:31,784] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:50:31,785] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:50:31,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run24
[2019-03-26 10:50:31,835] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run24
[2019-03-26 10:50:31,836] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run24
[2019-03-26 10:50:31,836] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run24
[2019-03-26 10:50:31,876] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run24
[2019-03-26 10:50:53,871] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.24058925], dtype=float32), 0.21913181]
[2019-03-26 10:50:53,872] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.621883265, 96.52610383666666, 1.0, 2.0, 0.4451561786973749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645471.3250402857, 645471.3250402863, 178446.316744783]
[2019-03-26 10:50:53,877] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:50:53,880] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2927373e-33 1.0000000e+00 0.0000000e+00 6.1999436e-28 2.6467338e-25], sampled 0.5748876151568769
[2019-03-26 10:51:01,541] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.24058925], dtype=float32), 0.21913181]
[2019-03-26 10:51:01,543] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.35291634, 88.70441376666668, 1.0, 2.0, 0.3504803190371137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543712.1261765499, 543712.1261765499, 170226.8690034919]
[2019-03-26 10:51:01,547] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:51:01,549] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5457256e-30 1.0000000e+00 4.6420720e-36 5.9130583e-25 3.8922935e-20], sampled 0.7324848801714606
[2019-03-26 10:51:03,976] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.24058925], dtype=float32), 0.21913181]
[2019-03-26 10:51:03,977] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.0, 90.33333333333334, 1.0, 2.0, 0.4241278887929203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 620977.4660687854, 620977.466068786, 176187.7147330838]
[2019-03-26 10:51:03,979] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:51:03,981] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0935226e-33 1.0000000e+00 0.0000000e+00 6.7291535e-28 3.9216593e-25], sampled 0.329242467678588
[2019-03-26 10:51:08,403] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.24058925], dtype=float32), 0.21913181]
[2019-03-26 10:51:08,404] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.01483734666667, 93.32930605333334, 1.0, 2.0, 0.8481019523711724, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1185362.164578738, 1185362.164578737, 256067.7964710389]
[2019-03-26 10:51:08,406] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:51:08,411] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5429112e-28 1.0000000e+00 6.2191656e-32 4.0352678e-23 3.6043679e-15], sampled 0.012890477761045127
[2019-03-26 10:51:16,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.24058925], dtype=float32), 0.21913181]
[2019-03-26 10:51:16,574] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [16.95301631, 100.0, 1.0, 2.0, 0.2377262736899428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395014.0346640667, 395014.0346640667, 159421.8609442622]
[2019-03-26 10:51:16,575] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:51:16,579] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6226450e-31 1.0000000e+00 5.2140080e-36 1.3721342e-24 8.1566016e-22], sampled 0.08851499312156108
[2019-03-26 10:51:59,050] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.24058925], dtype=float32), 0.21913181]
[2019-03-26 10:51:59,052] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.45222927333333, 74.73168851666667, 1.0, 2.0, 0.7689472526599852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1074674.449974451, 1074674.449974451, 236432.0170436186]
[2019-03-26 10:51:59,053] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:51:59,058] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.6708878e-29 1.0000000e+00 1.8227458e-33 3.2709133e-24 1.6630966e-17], sampled 0.01443048111559353
[2019-03-26 10:52:10,471] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.24058925], dtype=float32), 0.21913181]
[2019-03-26 10:52:10,473] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.76666666666667, 55.0, 1.0, 2.0, 0.911725480399462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1415360.450796674, 1415360.450796674, 292633.9081865932]
[2019-03-26 10:52:10,474] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:52:10,478] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.5910747e-27 1.0000000e+00 1.6970777e-30 2.5016690e-22 3.3929290e-14], sampled 0.7832467479443076
[2019-03-26 10:52:27,346] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7901.0795 3163410891.1063 1800.0000
[2019-03-26 10:52:27,468] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.0970 2779331942.8466 930.0000
[2019-03-26 10:52:28,031] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8262.4489 2927270135.1114 1319.0000
[2019-03-26 10:52:28,047] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8010.8248 3007117592.2378 1721.0000
[2019-03-26 10:52:28,056] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8502.4149 2842221932.5828 1117.0000
[2019-03-26 10:52:29,069] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 575000, evaluation results [575000.0, 7901.079504224224, 3163410891.106313, 1800.0, 8262.448876705905, 2927270135.1113877, 1319.0, 8664.097010407286, 2779331942.8465877, 930.0, 8010.824844696686, 3007117592.2378454, 1721.0, 8502.414941074587, 2842221932.5827913, 1117.0]
[2019-03-26 10:52:33,086] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.0051934e-24 1.0000000e+00 3.4479646e-25 5.5993665e-13 8.3233309e-11], sum to 1.0000
[2019-03-26 10:52:33,094] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6932
[2019-03-26 10:52:33,098] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 87.33333333333333, 1.0, 2.0, 0.5096567978102957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712170.713913877, 712170.713913877, 185122.6269993627], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5710200.0000, 
sim time next is 5710800.0000, 
raw observation next is [26.33333333333334, 87.66666666666667, 1.0, 2.0, 0.5097269183795204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712268.7299873319, 712268.7299873313, 185133.8424928221], 
processed observation next is [0.0, 0.08695652173913043, 0.44707740916271754, 0.8766666666666667, 1.0, 1.0, 0.4093095402162897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1978524249964811, 0.19785242499648092, 0.2763191678997345], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.04360581], dtype=float32), -0.07839319]. 
=============================================
[2019-03-26 10:52:43,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1735817e-31 1.0000000e+00 0.0000000e+00 2.5009057e-22 1.5818688e-22], sum to 1.0000
[2019-03-26 10:52:43,134] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7896
[2019-03-26 10:52:43,139] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 80.66666666666666, 1.0, 2.0, 0.5301681045499398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740842.2550885848, 740842.2550885843, 188457.8951091644], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6291600.0000, 
sim time next is 6292200.0000, 
raw observation next is [27.95, 81.83333333333334, 1.0, 2.0, 0.5317184949596887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743009.485499889, 743009.4854998883, 188715.0068130046], 
processed observation next is [0.0, 0.8260869565217391, 0.523696682464455, 0.8183333333333335, 1.0, 1.0, 0.43580541561408276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20639152374996916, 0.20639152374996897, 0.2816641892731412], 
reward next is 0.7183, 
noisyNet noise sample is [array([0.03084913], dtype=float32), 1.3260303]. 
=============================================
[2019-03-26 10:52:53,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2886874e-10 1.6486718e-03 6.1838149e-15 1.0652439e-07 9.9835122e-01], sum to 1.0000
[2019-03-26 10:52:53,268] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7077
[2019-03-26 10:52:53,275] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.83333333333333, 90.33333333333334, 1.0, 2.0, 0.1797510528794759, 1.0, 2.0, 0.1797510528794759, 1.0, 2.0, 0.3081939541799341, 6.9112, 6.9112, 170.5573041426782, 753539.3846873104, 753539.3846873104, 264355.6753599254], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6045600.0000, 
sim time next is 6046200.0000, 
raw observation next is [26.8, 90.5, 1.0, 2.0, 0.1795917229483104, 1.0, 2.0, 0.1795917229483104, 1.0, 2.0, 0.3078733853034117, 6.911199999999999, 6.9112, 170.5573041426782, 752871.2188314721, 752871.2188314727, 264314.4586459503], 
processed observation next is [1.0, 1.0, 0.4691943127962086, 0.905, 1.0, 1.0, 0.011556292708807711, 1.0, 1.0, 0.011556292708807711, 1.0, 1.0, 0.15594315280903864, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.20913089411985336, 0.20913089411985353, 0.39449919200888106], 
reward next is 0.6055, 
noisyNet noise sample is [array([-0.9039251], dtype=float32), 0.088486984]. 
=============================================
[2019-03-26 10:53:14,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.96990111e-25 1.00000000e+00 2.82406966e-27 1.04008327e-19
 1.79252514e-16], sum to 1.0000
[2019-03-26 10:53:14,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3612
[2019-03-26 10:53:14,262] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333333, 84.33333333333334, 1.0, 2.0, 0.521590178621047, 0.0, 2.0, 0.0, 1.0, 1.0, 0.880909918004268, 6.911200000000001, 6.9112, 168.9127241827837, 1458203.616674439, 1458203.616674438, 315261.710837274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6403200.0000, 
sim time next is 6403800.0000, 
raw observation next is [26.91666666666667, 84.66666666666667, 1.0, 2.0, 0.964747707836527, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564524179, 1348497.355635101, 1348497.355635101, 288367.1482246378], 
processed observation next is [1.0, 0.08695652173913043, 0.4747235387045816, 0.8466666666666667, 1.0, 1.0, 0.9575273588391892, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399448674318, 0.37458259878752803, 0.37458259878752803, 0.4303987286934892], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5098829], dtype=float32), 0.59347546]. 
=============================================
[2019-03-26 10:53:17,184] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7439742e-37 1.0000000e+00 0.0000000e+00 5.1915413e-36 5.8076568e-32], sum to 1.0000
[2019-03-26 10:53:17,190] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3174
[2019-03-26 10:53:17,197] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 81.5, 1.0, 2.0, 0.5176470827866798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723339.7681451675, 723339.7681451675, 186407.225713669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6471000.0000, 
sim time next is 6471600.0000, 
raw observation next is [27.56666666666667, 82.0, 1.0, 2.0, 0.5186913375641127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724799.4667173998, 724799.4667173992, 186576.5096105392], 
processed observation next is [1.0, 0.9130434782608695, 0.505529225908373, 0.82, 1.0, 1.0, 0.420110045257967, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2013331851992777, 0.20133318519927756, 0.27847240240378984], 
reward next is 0.7215, 
noisyNet noise sample is [array([-0.8779084], dtype=float32), 0.17845951]. 
=============================================
[2019-03-26 10:53:20,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2464051e-34 1.0000000e+00 7.0228254e-37 3.3968559e-31 6.3880636e-30], sum to 1.0000
[2019-03-26 10:53:20,959] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4972
[2019-03-26 10:53:20,966] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 58.16666666666667, 1.0, 2.0, 0.4558272605659852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 644549.2557413027, 644549.2557413033, 177948.9500027102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6952200.0000, 
sim time next is 6952800.0000, 
raw observation next is [30.33333333333334, 57.33333333333334, 1.0, 2.0, 0.4548126647770154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643250.0425234216, 643250.0425234216, 177818.9539617562], 
processed observation next is [0.0, 0.4782608695652174, 0.6366508688783573, 0.5733333333333335, 1.0, 1.0, 0.3431477888879704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17868056736761712, 0.17868056736761712, 0.2654014238235167], 
reward next is 0.7346, 
noisyNet noise sample is [array([-0.20226856], dtype=float32), -0.07793512]. 
=============================================
[2019-03-26 10:53:22,927] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 10:53:22,930] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:53:22,932] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:53:22,934] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:53:22,935] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:53:22,936] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:53:22,934] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:53:22,938] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:53:22,937] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:53:22,938] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:53:22,941] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:53:22,957] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run25
[2019-03-26 10:53:22,980] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run25
[2019-03-26 10:53:22,998] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run25
[2019-03-26 10:53:23,027] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run25
[2019-03-26 10:53:23,029] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run25
[2019-03-26 10:53:53,731] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06189635], dtype=float32), 0.22585897]
[2019-03-26 10:53:53,733] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.36666666666667, 89.33333333333334, 1.0, 2.0, 0.4875365960060733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681251.0608834457, 681251.0608834464, 181665.6871409693]
[2019-03-26 10:53:53,734] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:53:53,738] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4923299e-36 1.0000000e+00 0.0000000e+00 3.6239988e-31 3.7735412e-31], sampled 0.8155237415366342
[2019-03-26 10:54:13,556] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06189635], dtype=float32), 0.22585897]
[2019-03-26 10:54:13,557] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.13333333333333, 63.00000000000001, 1.0, 2.0, 0.6233997445446358, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.947974812609036, 6.9112, 168.9127047876935, 1743065.160441705, 1716975.881565123, 369804.3076066772]
[2019-03-26 10:54:13,558] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:54:13,562] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5842139e-27 1.0000000e+00 3.1464132e-30 5.4205787e-19 2.1638235e-16], sampled 0.8401382289699798
[2019-03-26 10:54:13,563] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1743065.160441705 W.
[2019-03-26 10:55:04,873] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06189635], dtype=float32), 0.22585897]
[2019-03-26 10:55:04,873] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.642286825, 31.10357846, 1.0, 2.0, 0.2627643515248514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 429935.7443608632, 429935.7443608625, 162182.4275228721]
[2019-03-26 10:55:04,874] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:55:04,878] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9179864e-37 1.0000000e+00 0.0000000e+00 1.2548103e-33 4.2779174e-34], sampled 0.5430831719729413
[2019-03-26 10:55:12,410] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.06189635], dtype=float32), 0.22585897]
[2019-03-26 10:55:12,411] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.23857636666667, 86.02016390833333, 1.0, 2.0, 0.5665323003030956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 886844.0681365145, 886844.0681365139, 205842.9614367128]
[2019-03-26 10:55:12,411] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:55:12,415] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.9125168e-34 1.0000000e+00 6.3204099e-38 2.2778830e-29 1.3021158e-26], sampled 0.35202670159776217
[2019-03-26 10:55:14,877] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06189635], dtype=float32), 0.22585897]
[2019-03-26 10:55:14,879] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.26666666666667, 88.33333333333333, 1.0, 2.0, 0.9169966694656004, 1.0, 2.0, 0.9169966694656004, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2564847.927830988, 2564847.927830988, 481335.985248504]
[2019-03-26 10:55:14,881] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:55:14,885] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.8397954e-27 9.9993908e-01 4.1965505e-27 8.0126378e-31 6.0956572e-05], sampled 0.13622074040604404
[2019-03-26 10:55:14,887] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2564847.927830988 W.
[2019-03-26 10:55:16,143] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06189635], dtype=float32), 0.22585897]
[2019-03-26 10:55:16,145] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.1, 64.66666666666667, 1.0, 2.0, 0.3506556103005052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555281.44989107, 555281.44989107, 171379.5822294448]
[2019-03-26 10:55:16,146] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:55:16,150] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.5482194e-36 1.0000000e+00 0.0000000e+00 3.5709965e-33 1.9558296e-30], sampled 0.925009797345623
[2019-03-26 10:55:18,292] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.2017 2927430591.7856 1340.0000
[2019-03-26 10:55:18,517] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7880.0433 3164445661.4674 1808.0000
[2019-03-26 10:55:18,858] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7994.6151 3008006107.9315 1766.0000
[2019-03-26 10:55:19,119] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.8251 2779402117.5128 942.0000
[2019-03-26 10:55:19,195] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.7687 2842285846.0397 1133.0000
[2019-03-26 10:55:20,208] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 600000, evaluation results [600000.0, 7880.043321003066, 3164445661.4673786, 1808.0, 8253.20174979031, 2927430591.7855587, 1340.0, 8657.825080872079, 2779402117.5128326, 942.0, 7994.615069066108, 3008006107.9315424, 1766.0, 8497.768669514962, 2842285846.0397353, 1133.0]
[2019-03-26 10:55:27,779] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7369636e-34 1.0000000e+00 0.0000000e+00 1.0499162e-29 2.1032055e-28], sum to 1.0000
[2019-03-26 10:55:27,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1906
[2019-03-26 10:55:27,792] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 67.0, 1.0, 2.0, 0.4436049069469582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 637637.6749049134, 637637.6749049134, 177517.7503546632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6721200.0000, 
sim time next is 6721800.0000, 
raw observation next is [27.86666666666667, 67.0, 1.0, 2.0, 0.4370666914735018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 631364.9630658949, 631364.9630658942, 176975.0070612533], 
processed observation next is [1.0, 0.8260869565217391, 0.519747235387046, 0.67, 1.0, 1.0, 0.3217670981608456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17537915640719304, 0.17537915640719284, 0.26414180158396017], 
reward next is 0.7359, 
noisyNet noise sample is [array([0.29573584], dtype=float32), 0.7109328]. 
=============================================
[2019-03-26 10:55:30,556] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8733900e-36 1.0000000e+00 0.0000000e+00 1.8061145e-32 6.7024812e-29], sum to 1.0000
[2019-03-26 10:55:30,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1688
[2019-03-26 10:55:30,572] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 72.0, 1.0, 2.0, 0.3550359818122086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547326.3541794426, 547326.3541794426, 170434.4798124389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6735600.0000, 
sim time next is 6736200.0000, 
raw observation next is [24.85, 72.66666666666667, 1.0, 2.0, 0.3519590797714953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543323.6003411306, 543323.6003411306, 170123.3270526506], 
processed observation next is [1.0, 1.0, 0.37677725118483424, 0.7266666666666667, 1.0, 1.0, 0.21922780695360883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15092322231698074, 0.15092322231698074, 0.2539154135114188], 
reward next is 0.7461, 
noisyNet noise sample is [array([0.35468355], dtype=float32), 0.3787267]. 
=============================================
[2019-03-26 10:55:32,665] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5893017e-23 1.0000000e+00 6.0945017e-25 1.0320551e-21 1.3805452e-09], sum to 1.0000
[2019-03-26 10:55:32,678] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2300
[2019-03-26 10:55:32,685] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 51.0, 1.0, 2.0, 0.8357807707314325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1299911.58834601, 1299911.58834601, 270281.1533023429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6782400.0000, 
sim time next is 6783000.0000, 
raw observation next is [28.56666666666667, 50.5, 1.0, 2.0, 0.9403810267632208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1463864.095074482, 1463864.095074482, 302138.8370573138], 
processed observation next is [1.0, 0.5217391304347826, 0.552922590837283, 0.505, 1.0, 1.0, 0.9281699117629166, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.40662891529846723, 0.40662891529846723, 0.4509534881452445], 
reward next is 0.5490, 
noisyNet noise sample is [array([0.06790423], dtype=float32), -0.70894384]. 
=============================================
[2019-03-26 10:55:32,710] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[52.98363 ]
 [52.903397]
 [53.340206]
 [53.75749 ]
 [53.99296 ]], R is [[51.89171219]
 [51.96939087]
 [52.02445602]
 [52.08198166]
 [52.14503479]].
[2019-03-26 10:55:33,456] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8431207e-31 1.0000000e+00 1.6549978e-37 2.5488426e-28 8.3163090e-28], sum to 1.0000
[2019-03-26 10:55:33,463] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7768
[2019-03-26 10:55:33,472] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 73.5, 1.0, 2.0, 0.3324294445221038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 520206.5995089056, 520206.599508905, 168454.4481746392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6823800.0000, 
sim time next is 6824400.0000, 
raw observation next is [24.13333333333333, 74.0, 1.0, 2.0, 0.3322379970240978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519787.4987548686, 519787.4987548686, 168418.6717349846], 
processed observation next is [1.0, 1.0, 0.3428120063191152, 0.74, 1.0, 1.0, 0.1954674662940937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14438541632079682, 0.14438541632079682, 0.2513711518432606], 
reward next is 0.7486, 
noisyNet noise sample is [array([0.6248203], dtype=float32), -0.5395012]. 
=============================================
[2019-03-26 10:55:40,319] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9867968e-37 1.0000000e+00 0.0000000e+00 1.5874602e-30 3.3306830e-32], sum to 1.0000
[2019-03-26 10:55:40,328] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8574
[2019-03-26 10:55:40,333] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.421909687787127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617839.8271647256, 617839.8271647262, 175888.9389407376], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6914400.0000, 
sim time next is 6915000.0000, 
raw observation next is [24.95, 83.5, 1.0, 2.0, 0.4227753344880937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618615.2883998014, 618615.2883998014, 175949.7417992525], 
processed observation next is [0.0, 0.0, 0.3815165876777251, 0.835, 1.0, 1.0, 0.3045485957687875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17183758011105593, 0.17183758011105593, 0.2626115549242574], 
reward next is 0.7374, 
noisyNet noise sample is [array([-0.497862], dtype=float32), 0.3695996]. 
=============================================
[2019-03-26 10:55:40,343] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[67.63724 ]
 [67.68949 ]
 [67.858406]
 [68.18381 ]
 [68.99661 ]], R is [[67.74367523]
 [67.80371857]
 [67.86333466]
 [67.92253113]
 [67.9812851 ]].
[2019-03-26 10:55:44,943] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9199513e-34 1.0000000e+00 2.1550729e-33 3.8459260e-32 2.8696428e-23], sum to 1.0000
[2019-03-26 10:55:44,951] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8450
[2019-03-26 10:55:44,956] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 71.0, 1.0, 2.0, 0.705546750514573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1011798.27218437, 1011798.27218437, 225494.4398628975], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7026000.0000, 
sim time next is 7026600.0000, 
raw observation next is [27.55, 70.0, 1.0, 2.0, 0.6918252489627995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 992878.5556813459, 992878.5556813465, 222552.263091632], 
processed observation next is [1.0, 0.30434782608695654, 0.504739336492891, 0.7, 1.0, 1.0, 0.6287051192322886, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27579959880037386, 0.275799598800374, 0.33216755685318206], 
reward next is 0.6678, 
noisyNet noise sample is [array([1.7036313], dtype=float32), -0.48839694]. 
=============================================
[2019-03-26 10:55:54,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6125431e-35 1.0000000e+00 2.3756722e-37 7.7052065e-31 1.6084408e-23], sum to 1.0000
[2019-03-26 10:55:54,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9859
[2019-03-26 10:55:54,470] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 85.83333333333334, 1.0, 2.0, 0.4704582394378091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658808.0729775097, 658808.0729775097, 179281.5992119209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7170600.0000, 
sim time next is 7171200.0000, 
raw observation next is [25.7, 86.0, 1.0, 2.0, 0.4720728632724427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660374.3298207412, 660374.3298207412, 179431.4498755454], 
processed observation next is [1.0, 0.0, 0.4170616113744076, 0.86, 1.0, 1.0, 0.3639432087619791, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18343731383909478, 0.18343731383909478, 0.26780813414260507], 
reward next is 0.7322, 
noisyNet noise sample is [array([2.4744828], dtype=float32), 1.3090281]. 
=============================================
[2019-03-26 10:55:54,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.4637893e-26 1.0000000e+00 7.7559291e-31 2.5772015e-24 5.6655184e-14], sum to 1.0000
[2019-03-26 10:55:54,637] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9197
[2019-03-26 10:55:54,644] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.75, 84.5, 1.0, 2.0, 0.4685609498154886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660048.8260676397, 660048.8260676397, 179503.4029037608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7165800.0000, 
sim time next is 7166400.0000, 
raw observation next is [25.73333333333333, 84.66666666666667, 1.0, 2.0, 0.4683753990956585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659578.6357287432, 659578.6357287432, 179448.9864789623], 
processed observation next is [1.0, 0.9565217391304348, 0.41864139020537117, 0.8466666666666667, 1.0, 1.0, 0.35948843264537167, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18321628770242865, 0.18321628770242865, 0.26783430817755566], 
reward next is 0.7322, 
noisyNet noise sample is [array([0.39253867], dtype=float32), -0.7728051]. 
=============================================
[2019-03-26 10:55:56,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2203027e-34 1.0000000e+00 1.7064522e-37 8.5200977e-33 2.1662863e-22], sum to 1.0000
[2019-03-26 10:55:56,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0817
[2019-03-26 10:55:56,408] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 90.5, 1.0, 2.0, 0.4939772251370141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690253.6895862136, 690253.6895862136, 182658.1550271027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7187400.0000, 
sim time next is 7188000.0000, 
raw observation next is [25.8, 90.66666666666667, 1.0, 2.0, 0.4922457907611658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687833.5053485857, 687833.5053485857, 182390.561768061], 
processed observation next is [1.0, 0.17391304347826086, 0.42180094786729866, 0.9066666666666667, 1.0, 1.0, 0.38824794067610335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19106486259682934, 0.19106486259682934, 0.27222471905680745], 
reward next is 0.7278, 
noisyNet noise sample is [array([-1.2298979], dtype=float32), 0.61909294]. 
=============================================
[2019-03-26 10:55:56,421] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[64.11672 ]
 [64.075645]
 [64.03945 ]
 [63.86982 ]
 [63.985176]], R is [[64.23923492]
 [64.32421875]
 [64.40690613]
 [64.4812088 ]
 [64.54322815]].
[2019-03-26 10:56:01,827] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6304769e-32 1.0000000e+00 3.0392326e-33 2.3830106e-33 1.5915080e-18], sum to 1.0000
[2019-03-26 10:56:01,837] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7460
[2019-03-26 10:56:01,844] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.45, 90.66666666666667, 1.0, 2.0, 0.3474194645464314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548833.0656638375, 548833.0656638375, 170837.2225097524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7276200.0000, 
sim time next is 7276800.0000, 
raw observation next is [21.5, 90.33333333333334, 1.0, 2.0, 0.3234665662315713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 510886.6264841023, 510886.6264841029, 167836.885513008], 
processed observation next is [1.0, 0.21739130434782608, 0.21800947867298584, 0.9033333333333334, 1.0, 1.0, 0.18489947738743526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14191295180113953, 0.1419129518011397, 0.2505028141985194], 
reward next is 0.7495, 
noisyNet noise sample is [array([-1.1802869], dtype=float32), 0.95867044]. 
=============================================
[2019-03-26 10:56:03,071] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2976277e-27 1.0000000e+00 4.9701069e-26 7.6406282e-26 7.0254101e-11], sum to 1.0000
[2019-03-26 10:56:03,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0834
[2019-03-26 10:56:03,091] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 62.5, 1.0, 2.0, 0.8977900800573273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1370208.005544685, 1370208.005544685, 285484.8455427904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7301400.0000, 
sim time next is 7302000.0000, 
raw observation next is [27.1, 62.00000000000001, 1.0, 2.0, 0.7014495561269493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1070277.931118156, 1070277.931118156, 232592.0384331172], 
processed observation next is [1.0, 0.5217391304347826, 0.4834123222748816, 0.6200000000000001, 1.0, 1.0, 0.6403006700324689, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2972994253105989, 0.2972994253105989, 0.34715229616883164], 
reward next is 0.6528, 
noisyNet noise sample is [array([0.43855232], dtype=float32), -0.22834781]. 
=============================================
[2019-03-26 10:56:03,104] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[62.330162]
 [62.88314 ]
 [63.353706]
 [63.39419 ]
 [63.36609 ]], R is [[63.45570755]
 [63.39505386]
 [63.35390472]
 [63.34147644]
 [63.33417511]].
[2019-03-26 10:56:03,719] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6949297e-30 1.0000000e+00 3.0176327e-31 6.4110808e-31 3.7659015e-18], sum to 1.0000
[2019-03-26 10:56:03,727] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5317
[2019-03-26 10:56:03,731] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.63333333333333, 92.33333333333334, 1.0, 2.0, 0.5817624780692395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 931735.4583651241, 931735.4583651234, 210682.2938787546], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7377600.0000, 
sim time next is 7378200.0000, 
raw observation next is [20.7, 92.5, 1.0, 2.0, 0.5993035104420347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958112.2415123684, 958112.2415123684, 214230.8905918325], 
processed observation next is [1.0, 0.391304347826087, 0.18009478672985785, 0.925, 1.0, 1.0, 0.5172331451108851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26614228930899125, 0.26614228930899125, 0.31974759789825746], 
reward next is 0.6803, 
noisyNet noise sample is [array([-1.8015316], dtype=float32), 0.5894417]. 
=============================================
[2019-03-26 10:56:06,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2433263e-27 1.0000000e+00 1.5004793e-30 1.3864738e-29 8.7404075e-16], sum to 1.0000
[2019-03-26 10:56:06,541] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0060
[2019-03-26 10:56:06,544] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 93.0, 1.0, 2.0, 0.6694935311575282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1061029.746205035, 1061029.746205035, 229195.9729067073], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7381200.0000, 
sim time next is 7381800.0000, 
raw observation next is [21.1, 93.0, 1.0, 2.0, 0.6958354211556949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1101032.916288314, 1101032.916288314, 235333.9584616101], 
processed observation next is [1.0, 0.43478260869565216, 0.1990521327014219, 0.93, 1.0, 1.0, 0.633536651994813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30584247674675386, 0.30584247674675386, 0.35124471412180613], 
reward next is 0.6488, 
noisyNet noise sample is [array([-0.992343], dtype=float32), -0.6924202]. 
=============================================
[2019-03-26 10:56:14,286] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 10:56:14,289] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:56:14,290] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:56:14,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:56:14,290] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:56:14,291] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:56:14,292] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:56:14,293] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:56:14,294] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:56:14,294] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:56:14,296] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:56:14,312] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run26
[2019-03-26 10:56:14,312] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run26
[2019-03-26 10:56:14,349] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run26
[2019-03-26 10:56:14,367] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run26
[2019-03-26 10:56:14,388] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run26
[2019-03-26 10:56:43,087] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04563272], dtype=float32), 0.21240298]
[2019-03-26 10:56:43,090] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.49513532333333, 90.51781518666667, 1.0, 2.0, 0.4138264816820439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 616147.8797888999, 616147.8797888992, 176003.3011371761]
[2019-03-26 10:56:43,092] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 10:56:43,095] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1594557e-36 1.0000000e+00 0.0000000e+00 2.9274115e-32 1.3847915e-31], sampled 0.2309469472155048
[2019-03-26 10:56:52,298] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04563272], dtype=float32), 0.21240298]
[2019-03-26 10:56:52,298] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.53333333333333, 94.16666666666667, 1.0, 2.0, 0.4867050502237183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682958.858000932, 682958.8580009327, 181908.5166942618]
[2019-03-26 10:56:52,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:56:52,301] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.4165093e-36 1.0000000e+00 0.0000000e+00 4.2307605e-32 4.3132362e-31], sampled 0.8562284881246771
[2019-03-26 10:57:22,766] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04563272], dtype=float32), 0.21240298]
[2019-03-26 10:57:22,767] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.3, 65.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.041432458238363, 6.9112, 168.9119129952585, 2376153.649014539, 2283762.831663047, 475795.3353220139]
[2019-03-26 10:57:22,769] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:57:22,770] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9543689e-25 1.0000000e+00 4.3927537e-25 1.0357360e-19 7.0652706e-10], sampled 0.8181652679721522
[2019-03-26 10:57:22,775] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2376153.649014539 W.
[2019-03-26 10:58:02,346] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04563272], dtype=float32), 0.21240298]
[2019-03-26 10:58:02,347] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.94494931, 90.80021917, 1.0, 2.0, 0.2556570641388859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 421245.7913547989, 421245.7913547995, 161452.4214989915]
[2019-03-26 10:58:02,348] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:58:02,352] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1369466e-36 1.0000000e+00 0.0000000e+00 7.4651403e-34 6.7172176e-31], sampled 0.457953792682103
[2019-03-26 10:58:09,703] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8261.7863 2927132542.4338 1321.0000
[2019-03-26 10:58:10,099] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.2099 3164039813.4985 1813.0000
[2019-03-26 10:58:10,309] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.5589 2842365088.3538 1120.0000
[2019-03-26 10:58:10,502] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8656.8617 2779624483.8325 938.0000
[2019-03-26 10:58:10,696] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.4006 3007328809.7663 1742.0000
[2019-03-26 10:58:11,712] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 625000, evaluation results [625000.0, 7883.209889190816, 3164039813.49852, 1813.0, 8261.786278611256, 2927132542.4338303, 1321.0, 8656.86171149697, 2779624483.8325267, 938.0, 8001.400551903808, 3007328809.7663474, 1742.0, 8500.55885913162, 2842365088.353775, 1120.0]
[2019-03-26 10:58:14,268] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:14,268] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:14,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-03-26 10:58:23,000] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2354997e-23 9.9998927e-01 6.6345324e-24 5.5616528e-19 1.0741375e-05], sum to 1.0000
[2019-03-26 10:58:23,013] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1620
[2019-03-26 10:58:23,021] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 96.0, 1.0, 2.0, 0.7233190762209774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1081954.492002188, 1081954.492002187, 235264.6045306042], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 141600.0000, 
sim time next is 142200.0000, 
raw observation next is [22.65, 96.0, 1.0, 2.0, 0.7174834026803589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1073897.236046703, 1073897.236046703, 233953.9649880056], 
processed observation next is [1.0, 0.6521739130434783, 0.2725118483412322, 0.96, 1.0, 1.0, 0.6596185574462156, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29830478779075087, 0.29830478779075087, 0.34918502237015764], 
reward next is 0.6508, 
noisyNet noise sample is [array([0.31906098], dtype=float32), -0.5393714]. 
=============================================
[2019-03-26 10:58:27,206] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.9923586e-24 9.9986315e-01 2.3019379e-24 5.7173686e-21 1.3683109e-04], sum to 1.0000
[2019-03-26 10:58:27,213] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8880
[2019-03-26 10:58:27,221] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2061099.477300504 W.
[2019-03-26 10:58:27,225] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.3, 68.0, 1.0, 2.0, 0.7370398790767353, 1.0, 1.0, 0.7370398790767353, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2061099.477300504, 2061099.477300504, 390381.4846641779], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7826400.0000, 
sim time next is 7827000.0000, 
raw observation next is [30.41666666666666, 68.33333333333334, 1.0, 2.0, 0.9561626604107957, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98911099611312, 6.9112, 168.9124934099799, 2233645.223296745, 2178372.632703151, 450735.4443939655], 
processed observation next is [1.0, 0.6086956521739131, 0.6406003159557659, 0.6833333333333335, 1.0, 1.0, 0.9471839282057779, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007791099611312013, 0.0, 0.8294376711170794, 0.6204570064713181, 0.6051035090842086, 0.6727394692447246], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1333861], dtype=float32), 2.0453076]. 
=============================================
[2019-03-26 10:58:27,237] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[62.619396]
 [63.573742]
 [63.788994]
 [62.730736]
 [63.2495  ]], R is [[60.35046768]
 [59.7469635 ]
 [59.14949417]
 [58.92502975]
 [58.3357811 ]].
[2019-03-26 10:58:27,326] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9500659e-25 1.0000000e+00 7.5758663e-29 7.8382245e-32 2.3417988e-12], sum to 1.0000
[2019-03-26 10:58:27,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3471
[2019-03-26 10:58:27,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1699878.736359204 W.
[2019-03-26 10:58:27,343] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.53333333333334, 77.66666666666667, 1.0, 2.0, 0.6079713574567502, 1.0, 2.0, 0.6079713574567502, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1699878.736359204, 1699878.736359204, 337212.9152022224], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7809600.0000, 
sim time next is 7810200.0000, 
raw observation next is [28.65, 77.0, 1.0, 2.0, 0.4102292233479035, 1.0, 2.0, 0.4102292233479035, 1.0, 1.0, 0.7047657904349047, 6.911200000000001, 6.9112, 170.5573041426782, 1720508.620133754, 1720508.620133754, 357515.42575339], 
processed observation next is [1.0, 0.391304347826087, 0.5568720379146919, 0.77, 1.0, 1.0, 0.28943279921434156, 1.0, 1.0, 0.28943279921434156, 1.0, 0.5, 0.6399582810181765, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.47791906114826505, 0.47791906114826505, 0.5336051130647612], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1994038], dtype=float32), 0.12220278]. 
=============================================
[2019-03-26 10:58:31,544] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.8152447e-24 5.0861921e-08 4.0885832e-24 1.2132061e-25 1.0000000e+00], sum to 1.0000
[2019-03-26 10:58:31,553] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7739
[2019-03-26 10:58:31,558] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.15, 90.5, 1.0, 2.0, 0.2479636128849474, 1.0, 2.0, 0.2479636128849474, 1.0, 2.0, 0.4216999564544296, 6.9112, 6.9112, 170.5573041426782, 1039633.760317645, 1039633.760317645, 284028.7356804644], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7875000.0000, 
sim time next is 7875600.0000, 
raw observation next is [26.16666666666666, 90.33333333333334, 1.0, 2.0, 0.2550516802965569, 1.0, 2.0, 0.2550516802965569, 1.0, 2.0, 0.4338085377193091, 6.911199999999999, 6.9112, 170.5573041426782, 1069366.625735129, 1069366.625735129, 286451.7606460318], 
processed observation next is [1.0, 0.13043478260869565, 0.4391785150078987, 0.9033333333333334, 1.0, 1.0, 0.10247190397175528, 1.0, 1.0, 0.10247190397175528, 1.0, 1.0, 0.3095226069747672, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2970462849264247, 0.2970462849264247, 0.42753994126273404], 
reward next is 0.5725, 
noisyNet noise sample is [array([-0.67901415], dtype=float32), 0.693894]. 
=============================================
[2019-03-26 10:58:32,107] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.6605199e-24 1.9407227e-09 3.1636093e-24 2.7196779e-33 1.0000000e+00], sum to 1.0000
[2019-03-26 10:58:32,116] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7391
[2019-03-26 10:58:32,120] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.68333333333333, 76.5, 1.0, 2.0, 0.4418964835611704, 1.0, 2.0, 0.4418964835611704, 1.0, 2.0, 0.759567160074914, 6.9112, 6.9112, 170.5573041426782, 1853436.53398267, 1853436.53398267, 376054.6023347296], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7897800.0000, 
sim time next is 7898400.0000, 
raw observation next is [28.8, 76.0, 1.0, 2.0, 0.4885928514839869, 1.0, 2.0, 0.4885928514839869, 1.0, 2.0, 0.8407587637755707, 6.9112, 6.9112, 170.5573041426782, 2049481.399549041, 2049481.399549041, 405976.8103414082], 
processed observation next is [1.0, 0.43478260869565216, 0.5639810426540285, 0.76, 1.0, 1.0, 0.38384680901685164, 1.0, 1.0, 0.38384680901685164, 1.0, 1.0, 0.8058033704580131, 0.0, 0.0, 0.8375144448122397, 0.5693003887636224, 0.5693003887636224, 0.6059355378229972], 
reward next is 0.3941, 
noisyNet noise sample is [array([-0.44331974], dtype=float32), 1.5570714]. 
=============================================
[2019-03-26 10:58:33,101] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8825104e-26 2.8451016e-06 8.1728744e-28 1.2854442e-28 9.9999714e-01], sum to 1.0000
[2019-03-26 10:58:33,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0282
[2019-03-26 10:58:33,116] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.3, 68.5, 1.0, 2.0, 0.5237263628235239, 1.0, 2.0, 0.5237263628235239, 1.0, 2.0, 0.9047850882439705, 6.9112, 6.9112, 170.5573041426782, 2197005.701458907, 2197005.701458907, 431036.9038712811], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7914600.0000, 
sim time next is 7915200.0000, 
raw observation next is [30.33333333333334, 68.0, 1.0, 2.0, 0.5245934105163272, 1.0, 2.0, 0.5245934105163272, 1.0, 2.0, 0.9056112451838298, 6.911200000000001, 6.9112, 170.5573041426782, 2200646.659241324, 2200646.659241323, 431533.6344114076], 
processed observation next is [1.0, 0.6086956521739131, 0.6366508688783573, 0.68, 1.0, 1.0, 0.42722097652569546, 1.0, 1.0, 0.42722097652569546, 1.0, 1.0, 0.8848917624193047, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6112907386781455, 0.6112907386781452, 0.6440800513603098], 
reward next is 0.3559, 
noisyNet noise sample is [array([-2.0265977], dtype=float32), -0.7724305]. 
=============================================
[2019-03-26 10:58:33,249] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:33,251] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:33,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-03-26 10:58:34,669] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:34,670] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:34,690] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-03-26 10:58:35,315] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,316] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,322] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-03-26 10:58:35,487] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,487] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,497] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-03-26 10:58:35,549] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,549] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-03-26 10:58:35,569] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,573] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-03-26 10:58:35,623] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4160944e-31 1.0000000e+00 4.9431127e-31 2.7742878e-21 9.4068203e-17], sum to 1.0000
[2019-03-26 10:58:35,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4363
[2019-03-26 10:58:35,626] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.63333333333333, 87.0, 1.0, 2.0, 0.2240645058779399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 372263.3600338448, 372263.3600338448, 158175.7977826523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 524400.0000, 
sim time next is 525000.0000, 
raw observation next is [18.61666666666667, 87.0, 1.0, 2.0, 0.2235750763371721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 371500.0341577611, 371500.0341577617, 158123.462648002], 
processed observation next is [1.0, 0.043478260869565216, 0.081358609794629, 0.87, 1.0, 1.0, 0.06454828474358085, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10319445393271141, 0.1031944539327116, 0.23600516813134625], 
reward next is 0.7640, 
noisyNet noise sample is [array([-0.8385946], dtype=float32), 0.9311319]. 
=============================================
[2019-03-26 10:58:35,628] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,630] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-03-26 10:58:35,653] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.1283  ]
 [75.35946 ]
 [75.57804 ]
 [75.783264]
 [76.08387 ]], R is [[74.89461517]
 [74.90958405]
 [74.92433929]
 [74.93887329]
 [74.95319366]].
[2019-03-26 10:58:35,663] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,663] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,664] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-03-26 10:58:35,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,687] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-03-26 10:58:35,761] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-03-26 10:58:35,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,792] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-03-26 10:58:35,829] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,829] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,831] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-03-26 10:58:35,875] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,875] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-03-26 10:58:35,905] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,910] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 10:58:35,915] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:58:35,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-03-26 10:58:35,914] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-03-26 10:58:37,263] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0799657e-23 5.8233604e-02 2.7704369e-21 1.1758924e-18 9.4176638e-01], sum to 1.0000
[2019-03-26 10:58:37,265] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0181
[2019-03-26 10:58:37,276] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 71.0, 1.0, 2.0, 0.3788678878627092, 1.0, 1.0, 0.3788678878627092, 1.0, 1.0, 0.637181910895684, 6.9112, 6.9112, 170.5573041426782, 1613221.646051893, 1613221.646051893, 340805.9713250757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 61200.0000, 
sim time next is 61800.0000, 
raw observation next is [26.43333333333333, 71.66666666666667, 1.0, 2.0, 0.5302053947007495, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564691643, 775333.9952994147, 775333.9952994147, 192799.6980259943], 
processed observation next is [1.0, 0.7391304347826086, 0.4518167456556081, 0.7166666666666667, 1.0, 1.0, 0.43398240325391496, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399449496643, 0.21537055424983742, 0.21537055424983742, 0.28776074332237955], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.20467523], dtype=float32), 0.5340494]. 
=============================================
[2019-03-26 10:58:37,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1654490e-36 1.0000000e+00 0.0000000e+00 2.3721531e-34 3.2867105e-24], sum to 1.0000
[2019-03-26 10:58:37,735] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7769
[2019-03-26 10:58:37,741] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 85.5, 1.0, 2.0, 0.284368687725583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457095.9576430458, 457095.9576430458, 164073.2743378538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 16200.0000, 
sim time next is 16800.0000, 
raw observation next is [21.33333333333333, 85.66666666666666, 1.0, 2.0, 0.2853438512512497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458184.2511078147, 458184.2511078147, 164144.2919661589], 
processed observation next is [1.0, 0.17391304347826086, 0.21011058451816728, 0.8566666666666666, 1.0, 1.0, 0.1389684954834334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12727340308550408, 0.12727340308550408, 0.24499148054650582], 
reward next is 0.7550, 
noisyNet noise sample is [array([1.8048813], dtype=float32), -0.6644902]. 
=============================================
[2019-03-26 10:58:42,365] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.3668009e-29 9.9999988e-01 4.3548059e-28 9.2408466e-21 1.4244377e-07], sum to 1.0000
[2019-03-26 10:58:42,373] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1001
[2019-03-26 10:58:42,378] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.48333333333333, 89.83333333333333, 1.0, 2.0, 0.3817879508531122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 588945.2340165173, 588945.2340165167, 174030.2213120737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 96600.0000, 
sim time next is 97200.0000, 
raw observation next is [22.5, 90.0, 1.0, 2.0, 0.3891848569470093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599712.4730124634, 599712.4730124628, 174978.7199841454], 
processed observation next is [1.0, 0.13043478260869565, 0.2654028436018958, 0.9, 1.0, 1.0, 0.26407814090001125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1665867980590176, 0.16658679805901747, 0.2611622686330528], 
reward next is 0.7388, 
noisyNet noise sample is [array([0.55859405], dtype=float32), -0.5633398]. 
=============================================
[2019-03-26 10:58:50,035] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9286304e-31 1.0000000e+00 4.6178751e-36 1.1834906e-23 1.1345810e-21], sum to 1.0000
[2019-03-26 10:58:50,044] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4631
[2019-03-26 10:58:50,050] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 88.0, 1.0, 2.0, 0.3012355422874743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479912.221147465, 479912.2211474656, 165618.7734097744], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 237600.0000, 
sim time next is 238200.0000, 
raw observation next is [21.38333333333333, 88.16666666666667, 1.0, 2.0, 0.3010137077222932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479568.2306969896, 479568.2306969896, 165594.3266130765], 
processed observation next is [0.0, 0.782608695652174, 0.21248025276461283, 0.8816666666666667, 1.0, 1.0, 0.1578478406292689, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13321339741583044, 0.13321339741583044, 0.24715571136280073], 
reward next is 0.7528, 
noisyNet noise sample is [array([0.62021804], dtype=float32), -0.25462908]. 
=============================================
[2019-03-26 10:58:51,450] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1907283e-36 1.0000000e+00 0.0000000e+00 1.5609474e-31 6.6466548e-26], sum to 1.0000
[2019-03-26 10:58:51,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8825
[2019-03-26 10:58:51,466] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.13333333333334, 89.0, 1.0, 2.0, 0.2970557821000253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474587.9209543765, 474587.9209543765, 165258.5003072337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 244200.0000, 
sim time next is 244800.0000, 
raw observation next is [21.1, 89.0, 1.0, 2.0, 0.2975081561231734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475638.0772194005, 475638.0772194005, 165336.4330564601], 
processed observation next is [0.0, 0.8695652173913043, 0.1990521327014219, 0.89, 1.0, 1.0, 0.15362428448575105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13212168811650013, 0.13212168811650013, 0.24677079560665688], 
reward next is 0.7532, 
noisyNet noise sample is [array([-1.0037968], dtype=float32), 0.47219747]. 
=============================================
[2019-03-26 10:58:59,061] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8061289e-32 1.0000000e+00 1.4525199e-35 6.4631274e-27 1.2346684e-18], sum to 1.0000
[2019-03-26 10:58:59,072] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0934
[2019-03-26 10:58:59,077] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 80.0, 1.0, 2.0, 0.5961380829508031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954474.4791067817, 954474.479106781, 213665.3531353726], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 404400.0000, 
sim time next is 405000.0000, 
raw observation next is [22.15, 80.5, 1.0, 2.0, 0.5874190559732858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 940185.5759398754, 940185.5759398754, 211811.7673065752], 
processed observation next is [1.0, 0.6956521739130435, 0.24881516587677724, 0.805, 1.0, 1.0, 0.502914525269019, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26116265998329874, 0.26116265998329874, 0.3161369661292167], 
reward next is 0.6839, 
noisyNet noise sample is [array([-0.1303229], dtype=float32), -0.1798701]. 
=============================================
[2019-03-26 10:58:59,087] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[64.72469 ]
 [64.52985 ]
 [65.37905 ]
 [65.361244]
 [65.39821 ]], R is [[64.85457611]
 [64.88713074]
 [64.90138245]
 [64.95610046]
 [65.01208496]].
[2019-03-26 10:59:03,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6400380e-32 1.0000000e+00 2.2793613e-36 4.0352381e-30 8.4896617e-19], sum to 1.0000
[2019-03-26 10:59:03,152] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5009
[2019-03-26 10:59:03,161] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 81.0, 1.0, 2.0, 0.2383487008260658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392901.3570151539, 392901.3570151546, 159759.5334614988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 457200.0000, 
sim time next is 457800.0000, 
raw observation next is [20.23333333333333, 80.83333333333333, 1.0, 2.0, 0.2636482956071631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 434520.2358408161, 434520.2358408161, 162264.0863842669], 
processed observation next is [1.0, 0.30434782608695654, 0.15797788309636643, 0.8083333333333332, 1.0, 1.0, 0.11282927181585911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12070006551133781, 0.12070006551133781, 0.24218520355860731], 
reward next is 0.7578, 
noisyNet noise sample is [array([0.6091466], dtype=float32), -0.35587484]. 
=============================================
[2019-03-26 10:59:03,754] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 10:59:03,761] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 10:59:03,763] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 10:59:03,764] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 10:59:03,764] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:59:03,765] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:59:03,765] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 10:59:03,764] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:59:03,767] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 10:59:03,767] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:59:03,771] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 10:59:03,789] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run27
[2019-03-26 10:59:03,790] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run27
[2019-03-26 10:59:03,838] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run27
[2019-03-26 10:59:03,839] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run27
[2019-03-26 10:59:03,858] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run27
[2019-03-26 10:59:10,924] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 10:59:10,925] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.09305163666667, 89.57721906666667, 1.0, 2.0, 0.2978218643762356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476653.6365046832, 476653.6365046832, 165413.5618288125]
[2019-03-26 10:59:10,926] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:59:10,929] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.1952242e-34 1.0000000e+00 0.0000000e+00 2.1527120e-28 1.9694728e-28], sampled 0.9858393574586984
[2019-03-26 10:59:16,579] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 10:59:16,580] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.23648737666667, 87.70319796000001, 1.0, 2.0, 0.2355721849971357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 390293.0081652263, 390293.0081652269, 159370.3232521318]
[2019-03-26 10:59:16,581] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:59:16,584] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.37299190e-35 1.00000000e+00 0.00000000e+00 1.05474140e-29
 1.07207645e-26], sampled 0.3166350413024651
[2019-03-26 10:59:20,678] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 10:59:20,678] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.9, 90.33333333333334, 1.0, 2.0, 0.3062226382590891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490072.8575880549, 490072.8575880549, 166374.8759547537]
[2019-03-26 10:59:20,679] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:59:20,683] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0890619e-34 1.0000000e+00 0.0000000e+00 2.9818547e-28 3.1110408e-28], sampled 0.7028685469008462
[2019-03-26 10:59:39,614] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 10:59:39,615] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.51666666666667, 78.83333333333333, 1.0, 2.0, 0.3480994007425072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543431.4005276918, 543431.4005276918, 170285.1186760031]
[2019-03-26 10:59:39,615] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 10:59:39,618] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.4775443e-34 1.0000000e+00 0.0000000e+00 3.1409120e-28 9.4651522e-28], sampled 0.8070704677737087
[2019-03-26 10:59:41,222] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 10:59:41,223] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.96666666666667, 94.33333333333334, 1.0, 2.0, 0.4908382629253306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704692.7066271114, 704692.7066271114, 184571.4983395751]
[2019-03-26 10:59:41,225] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 10:59:41,227] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.0176773e-32 1.0000000e+00 9.6449132e-36 5.1798421e-24 8.5730026e-20], sampled 0.04266688813272124
[2019-03-26 10:59:57,236] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 10:59:57,237] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.92055504, 59.40056986666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.058071631458127, 6.9112, 168.9116387171747, 2387968.70381761, 2283773.72642485, 475750.2814353217]
[2019-03-26 10:59:57,238] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 10:59:57,240] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.4612798e-30 1.0000000e+00 6.8568817e-34 1.9291933e-20 5.6946138e-08], sampled 0.2651066413495593
[2019-03-26 10:59:57,240] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2387968.70381761 W.
[2019-03-26 10:59:59,939] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 10:59:59,941] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.518497269532667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724528.1909779884, 724528.1909779878, 186544.9532383817]
[2019-03-26 10:59:59,944] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 10:59:59,946] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7286982e-33 1.0000000e+00 0.0000000e+00 1.3872430e-26 1.0280261e-24], sampled 0.5340860876002828
[2019-03-26 11:00:12,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 11:00:12,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.933651975, 88.83968734000001, 1.0, 2.0, 0.4535069972036486, 1.0, 2.0, 0.4535069972036486, 1.0, 2.0, 0.7875916770869786, 6.911199999999999, 6.9112, 184.5923449428631, 1902049.11139986, 1902049.11139986, 388604.3441974349]
[2019-03-26 11:00:12,137] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:00:12,143] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.5183945e-32 2.5602396e-09 3.8015381e-33 1.2060843e-28 1.0000000e+00], sampled 0.22370243420795222
[2019-03-26 11:00:19,830] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 11:00:19,831] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.10524231, 60.77554976, 1.0, 2.0, 0.5045813625361564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 705076.1829090016, 705076.1829090009, 184315.4232727447]
[2019-03-26 11:00:19,833] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:00:19,835] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1695850e-33 1.0000000e+00 0.0000000e+00 2.4411308e-27 4.5145645e-23], sampled 0.6147563397631035
[2019-03-26 11:00:57,890] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 11:00:57,892] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.15482137333333, 87.23793314666666, 1.0, 2.0, 0.5192585040455232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725592.2741804882, 725592.2741804875, 186667.0469204414]
[2019-03-26 11:00:57,894] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:00:57,897] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.7822862e-33 1.0000000e+00 1.7908784e-38 2.2625494e-26 9.7008422e-23], sampled 0.44988160197957583
[2019-03-26 11:00:59,449] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02119884], dtype=float32), 0.19852518]
[2019-03-26 11:00:59,451] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.92005299, 89.43433716999999, 1.0, 2.0, 0.508024311344212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709888.7909931964, 709888.7909931964, 184862.3272284695]
[2019-03-26 11:00:59,453] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:00:59,456] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.4602592e-32 1.0000000e+00 3.5368834e-35 5.2998503e-23 1.5378707e-19], sampled 0.7729648655521444
[2019-03-26 11:01:00,010] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8517.4994 2924012093.6181 748.0000
[2019-03-26 11:01:00,239] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8073.6182 3161640168.4739 1314.0000
[2019-03-26 11:01:00,281] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8275.3719 3001726532.9791 1095.0000
[2019-03-26 11:01:00,316] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8650.5985 2841612605.0158 751.0000
[2019-03-26 11:01:00,328] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8810.9899 2782499568.5840 552.0000
[2019-03-26 11:01:01,343] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 650000, evaluation results [650000.0, 8073.618151325213, 3161640168.4739275, 1314.0, 8517.499423106921, 2924012093.6181006, 748.0, 8810.989877818278, 2782499568.5840497, 552.0, 8275.371907834566, 3001726532.979109, 1095.0, 8650.598471288795, 2841612605.01579, 751.0]
[2019-03-26 11:01:02,545] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.1305107e-36 1.0000000e+00 0.0000000e+00 1.1602554e-29 1.5089197e-19], sum to 1.0000
[2019-03-26 11:01:02,554] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0099
[2019-03-26 11:01:02,562] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 59.83333333333334, 1.0, 2.0, 0.244318525826496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 402209.6093739917, 402209.6093739923, 160344.6921515403], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 496200.0000, 
sim time next is 496800.0000, 
raw observation next is [23.2, 61.0, 1.0, 2.0, 0.2450129090338873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403502.7665618007, 403502.7665618007, 160408.1908090874], 
processed observation next is [1.0, 0.782608695652174, 0.29857819905213273, 0.61, 1.0, 1.0, 0.0903769988360088, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11208410182272241, 0.11208410182272241, 0.239415210162817], 
reward next is 0.7606, 
noisyNet noise sample is [array([-0.2232082], dtype=float32), 1.2476716]. 
=============================================
[2019-03-26 11:01:03,171] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.5064923e-28 9.9999940e-01 9.5069945e-32 2.7631135e-20 5.9317307e-07], sum to 1.0000
[2019-03-26 11:01:03,180] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3137
[2019-03-26 11:01:03,189] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 57.5, 1.0, 2.0, 0.6350003701382125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1041487.492166851, 1041487.492166852, 223328.2821097738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 567000.0000, 
sim time next is 567600.0000, 
raw observation next is [24.06666666666667, 58.0, 1.0, 2.0, 0.6372104790311179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044985.718161123, 1044985.718161123, 223828.8376568352], 
processed observation next is [1.0, 0.5652173913043478, 0.3396524486571882, 0.58, 1.0, 1.0, 0.5629041916037565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29027381060031193, 0.29027381060031193, 0.3340728920251272], 
reward next is 0.6659, 
noisyNet noise sample is [array([0.28567767], dtype=float32), 1.2222501]. 
=============================================
[2019-03-26 11:01:04,584] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1420297e-37 1.0000000e+00 0.0000000e+00 4.8997830e-31 2.4503886e-25], sum to 1.0000
[2019-03-26 11:01:04,594] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5045
[2019-03-26 11:01:04,600] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.61666666666667, 87.0, 1.0, 2.0, 0.2235750763371721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 371500.0341577611, 371500.0341577617, 158123.462648002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 525000.0000, 
sim time next is 525600.0000, 
raw observation next is [18.6, 87.0, 1.0, 2.0, 0.2230419871355725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 370663.4714237471, 370663.4714237471, 158067.2075455481], 
processed observation next is [1.0, 0.08695652173913043, 0.08056872037914704, 0.87, 1.0, 1.0, 0.06390600859707528, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1029620753954853, 0.1029620753954853, 0.23592120529186283], 
reward next is 0.7641, 
noisyNet noise sample is [array([-0.05987835], dtype=float32), 1.1566195]. 
=============================================
[2019-03-26 11:01:06,074] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.11492345e-33 1.00000000e+00 0.00000000e+00 2.33695532e-25
 1.02372965e-23], sum to 1.0000
[2019-03-26 11:01:06,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7890
[2019-03-26 11:01:06,087] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 92.5, 1.0, 2.0, 0.2604571569572372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 425989.4158092547, 425989.4158092541, 161942.9033441317], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 790200.0000, 
sim time next is 790800.0000, 
raw observation next is [19.4, 92.66666666666667, 1.0, 2.0, 0.2605737586324761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 426060.2891390176, 426060.2891390182, 161952.2698383595], 
processed observation next is [0.0, 0.13043478260869565, 0.11848341232227487, 0.9266666666666667, 1.0, 1.0, 0.10912501040057362, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11835008031639377, 0.11835008031639395, 0.24171980572889476], 
reward next is 0.7583, 
noisyNet noise sample is [array([-0.7936695], dtype=float32), 0.7745337]. 
=============================================
[2019-03-26 11:01:09,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9388816e-31 1.0000000e+00 1.8084567e-34 5.7971385e-26 2.6033519e-20], sum to 1.0000
[2019-03-26 11:01:09,554] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9133
[2019-03-26 11:01:09,560] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.36666666666667, 90.33333333333334, 1.0, 2.0, 0.2064099535450901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 344783.9959661341, 344783.9959661347, 156049.3884382827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 610800.0000, 
sim time next is 611400.0000, 
raw observation next is [17.28333333333333, 90.66666666666667, 1.0, 2.0, 0.2055258604055006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 343373.9307243232, 343373.9307243232, 155935.3899002669], 
processed observation next is [1.0, 0.043478260869565216, 0.018167456556082123, 0.9066666666666667, 1.0, 1.0, 0.042802241452410364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09538164742342312, 0.09538164742342312, 0.2327393879108461], 
reward next is 0.7673, 
noisyNet noise sample is [array([0.13822564], dtype=float32), -0.45849302]. 
=============================================
[2019-03-26 11:01:15,869] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.22257246e-32 1.00000000e+00 1.37679288e-35 1.29774576e-28
 4.73924198e-16], sum to 1.0000
[2019-03-26 11:01:15,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3208
[2019-03-26 11:01:15,882] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 50.0, 1.0, 2.0, 0.6247459135902054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029735.949621896, 1029735.949621896, 221012.1660798875], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 748800.0000, 
sim time next is 749400.0000, 
raw observation next is [25.01666666666667, 50.0, 1.0, 2.0, 0.6101124164259905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1006275.576645778, 1006275.576645778, 217768.6788588425], 
processed observation next is [1.0, 0.6956521739130435, 0.3846761453396526, 0.5, 1.0, 1.0, 0.5302559234048078, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27952099351271614, 0.27952099351271614, 0.32502787889379475], 
reward next is 0.6750, 
noisyNet noise sample is [array([-0.82760125], dtype=float32), -2.1104393]. 
=============================================
[2019-03-26 11:01:18,588] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.6062574e-31 1.0000000e+00 6.1377405e-38 3.5858046e-27 1.2687179e-22], sum to 1.0000
[2019-03-26 11:01:18,597] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0610
[2019-03-26 11:01:18,600] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.53333333333333, 88.66666666666666, 1.0, 2.0, 0.2538946702498717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417037.6318998868, 417037.6318998868, 161296.6966982902], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 776400.0000, 
sim time next is 777000.0000, 
raw observation next is [19.51666666666667, 88.83333333333334, 1.0, 2.0, 0.2536828856595335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 416672.4523089746, 416672.4523089739, 161275.6469874782], 
processed observation next is [1.0, 1.0, 0.1240126382306479, 0.8883333333333334, 1.0, 1.0, 0.10082275380666687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11574234786360404, 0.11574234786360385, 0.24070992087683313], 
reward next is 0.7593, 
noisyNet noise sample is [array([1.4065393], dtype=float32), 0.3541068]. 
=============================================
[2019-03-26 11:01:18,612] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.8242  ]
 [74.83025 ]
 [74.83744 ]
 [74.851814]
 [74.885544]], R is [[74.82518005]
 [74.83618927]
 [74.84701538]
 [74.85761261]
 [74.8679657 ]].
[2019-03-26 11:01:23,507] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3730166e-35 1.0000000e+00 0.0000000e+00 5.2759225e-32 1.9252136e-30], sum to 1.0000
[2019-03-26 11:01:23,519] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4370
[2019-03-26 11:01:23,522] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.56666666666667, 89.0, 1.0, 2.0, 0.3091973448710491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 489324.0105647654, 489324.0105647654, 166242.3931793907], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 861000.0000, 
sim time next is 861600.0000, 
raw observation next is [21.53333333333333, 89.0, 1.0, 2.0, 0.3091584772471531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489622.0235564462, 489622.0235564456, 166271.5381219746], 
processed observation next is [0.0, 1.0, 0.21958925750394942, 0.89, 1.0, 1.0, 0.16766081596042542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13600611765456838, 0.13600611765456821, 0.2481664748089173], 
reward next is 0.7518, 
noisyNet noise sample is [array([-1.5512967], dtype=float32), -0.80548304]. 
=============================================
[2019-03-26 11:01:23,849] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.3656100e-31 3.6098307e-30], sum to 1.0000
[2019-03-26 11:01:23,859] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8764
[2019-03-26 11:01:23,864] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.35, 89.0, 1.0, 2.0, 0.3020354382085937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480360.7102689046, 480360.710268904, 165637.8022522679], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 865800.0000, 
sim time next is 866400.0000, 
raw observation next is [21.33333333333333, 89.0, 1.0, 2.0, 0.3015227486900099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479719.5905491434, 479719.5905491434, 165594.7765265664], 
processed observation next is [0.0, 0.0, 0.21011058451816728, 0.89, 1.0, 1.0, 0.1584611430000119, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13325544181920648, 0.13325544181920648, 0.24715638287547223], 
reward next is 0.7528, 
noisyNet noise sample is [array([-0.52642804], dtype=float32), -0.07571994]. 
=============================================
[2019-03-26 11:01:34,234] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0457585e-31 1.0000000e+00 7.5775692e-37 2.8558130e-26 1.6013135e-17], sum to 1.0000
[2019-03-26 11:01:34,249] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9779
[2019-03-26 11:01:34,252] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 96.5, 1.0, 2.0, 0.3572862484987616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547192.5366920851, 547192.5366920851, 170318.1159836626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1027800.0000, 
sim time next is 1028400.0000, 
raw observation next is [21.9, 96.66666666666666, 1.0, 2.0, 0.3583175151893303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548439.6797280892, 548439.6797280892, 170412.4880329457], 
processed observation next is [1.0, 0.9130434782608695, 0.23696682464454974, 0.9666666666666666, 1.0, 1.0, 0.22688857251726544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15234435548002478, 0.15234435548002478, 0.254346997064098], 
reward next is 0.7457, 
noisyNet noise sample is [array([-0.59135133], dtype=float32), 0.57691747]. 
=============================================
[2019-03-26 11:01:37,288] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9148416e-30 1.0000000e+00 7.3480657e-32 4.4768011e-23 2.6055208e-14], sum to 1.0000
[2019-03-26 11:01:37,296] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0586
[2019-03-26 11:01:37,300] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.75, 67.0, 1.0, 2.0, 0.7502723978122694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1159155.047152493, 1159155.047152493, 246202.704848598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1096200.0000, 
sim time next is 1096800.0000, 
raw observation next is [25.76666666666667, 67.0, 1.0, 2.0, 0.7641579954003923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1180096.443253924, 1180096.443253925, 249733.5063198174], 
processed observation next is [1.0, 0.6956521739130435, 0.42022116903633505, 0.67, 1.0, 1.0, 0.7158530065064967, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32780456757053444, 0.3278045675705347, 0.37273657659674236], 
reward next is 0.6273, 
noisyNet noise sample is [array([1.2481673], dtype=float32), -0.78060055]. 
=============================================
[2019-03-26 11:01:41,740] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8111006e-36 1.0000000e+00 0.0000000e+00 2.6206133e-32 8.7760539e-24], sum to 1.0000
[2019-03-26 11:01:41,756] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6035
[2019-03-26 11:01:41,762] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 73.0, 1.0, 2.0, 0.3514016201417813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542729.9341087812, 542729.9341087817, 170081.8251625188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1195200.0000, 
sim time next is 1195800.0000, 
raw observation next is [24.7, 73.83333333333334, 1.0, 2.0, 0.3517874959090655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543006.3234376422, 543006.3234376422, 170095.7090169036], 
processed observation next is [1.0, 0.8695652173913043, 0.3696682464454976, 0.7383333333333334, 1.0, 1.0, 0.21902107940851265, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1508350898437895, 0.1508350898437895, 0.2538741925625427], 
reward next is 0.7461, 
noisyNet noise sample is [array([-1.821846], dtype=float32), -1.4328945]. 
=============================================
[2019-03-26 11:01:41,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7413533e-34 1.0000000e+00 9.4469690e-36 1.3376439e-28 4.3060026e-16], sum to 1.0000
[2019-03-26 11:01:41,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7820
[2019-03-26 11:01:41,820] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 63.0, 1.0, 2.0, 0.5250589993288421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799527.1029328268, 799527.1029328268, 195610.9273368673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1170000.0000, 
sim time next is 1170600.0000, 
raw observation next is [27.1, 62.66666666666666, 1.0, 2.0, 0.5804043411100137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 882944.53232773, 882944.53232773, 205955.4380666619], 
processed observation next is [1.0, 0.5652173913043478, 0.4834123222748816, 0.6266666666666666, 1.0, 1.0, 0.49446306157832975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2452623700910361, 0.2452623700910361, 0.30739617621889836], 
reward next is 0.6926, 
noisyNet noise sample is [array([-0.4275239], dtype=float32), -0.85716575]. 
=============================================
[2019-03-26 11:01:41,962] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.1819131e-28 9.9999952e-01 6.0107012e-28 1.0535836e-20 4.4016781e-07], sum to 1.0000
[2019-03-26 11:01:41,971] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0745
[2019-03-26 11:01:41,976] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 58.83333333333334, 1.0, 2.0, 0.9342209316084197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1428344.877944785, 1428344.877944785, 296995.1814569918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1177800.0000, 
sim time next is 1178400.0000, 
raw observation next is [27.6, 58.66666666666667, 1.0, 2.0, 0.9488908089846819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1452102.698456047, 1452102.698456047, 301799.5554521694], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.5866666666666667, 1.0, 1.0, 0.9384226614273277, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.4033618606822353, 0.4033618606822353, 0.45044709768980506], 
reward next is 0.5496, 
noisyNet noise sample is [array([0.8578299], dtype=float32), 1.0380653]. 
=============================================
[2019-03-26 11:01:43,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.4682948e-30 3.6097490e-04 2.2901580e-33 5.0145578e-28 9.9963903e-01], sum to 1.0000
[2019-03-26 11:01:43,176] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8964
[2019-03-26 11:01:43,179] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.43333333333334, 73.33333333333334, 1.0, 2.0, 0.4611822442489845, 1.0, 2.0, 0.4611822442489845, 1.0, 2.0, 0.7846476868949167, 6.9112, 6.9112, 170.5573041426782, 1934399.389906681, 1934399.389906681, 386622.8761691388], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1261200.0000, 
sim time next is 1261800.0000, 
raw observation next is [28.4, 73.5, 1.0, 2.0, 0.462978532220803, 1.0, 2.0, 0.462978532220803, 1.0, 2.0, 0.7877086745875379, 6.9112, 6.9112, 170.5573041426782, 1941940.633083477, 1941940.633083477, 387749.3148425824], 
processed observation next is [1.0, 0.6086956521739131, 0.5450236966824644, 0.735, 1.0, 1.0, 0.3529861833985578, 1.0, 1.0, 0.3529861833985578, 1.0, 1.0, 0.7411081397409, 0.0, 0.0, 0.8375144448122397, 0.5394279536342992, 0.5394279536342992, 0.5787303206605707], 
reward next is 0.4213, 
noisyNet noise sample is [array([-1.1882102], dtype=float32), -1.8234318]. 
=============================================
[2019-03-26 11:01:47,345] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4507525e-33 9.2791220e-32], sum to 1.0000
[2019-03-26 11:01:47,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2904
[2019-03-26 11:01:47,360] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 61.0, 1.0, 2.0, 0.357913139627097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547127.1069043688, 547127.1069043695, 170280.8052184762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1504800.0000, 
sim time next is 1505400.0000, 
raw observation next is [27.46666666666667, 59.33333333333334, 1.0, 2.0, 0.3557121950948179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544564.9627604837, 544564.9627604837, 170091.8296220152], 
processed observation next is [0.0, 0.43478260869565216, 0.500789889415482, 0.5933333333333334, 1.0, 1.0, 0.2237496326443589, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15126804521124548, 0.15126804521124548, 0.25386840242091824], 
reward next is 0.7461, 
noisyNet noise sample is [array([-0.8829374], dtype=float32), -0.15096404]. 
=============================================
[2019-03-26 11:01:47,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7104685e-34 1.0000000e+00 0.0000000e+00 1.7078957e-32 4.3539293e-19], sum to 1.0000
[2019-03-26 11:01:47,474] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2363
[2019-03-26 11:01:47,481] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2919217.611688308 W.
[2019-03-26 11:01:47,485] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.8, 76.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 8.975704467479407, 6.9112, 168.9014013265779, 2919217.611688308, 1454687.281758849, 309336.8026352935], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1270800.0000, 
sim time next is 1271400.0000, 
raw observation next is [27.65, 76.66666666666667, 1.0, 2.0, 0.4308690306055575, 1.0, 1.0, 0.4308690306055575, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1204425.096269869, 1204425.096269869, 280174.8382841963], 
processed observation next is [1.0, 0.7391304347826086, 0.509478672985782, 0.7666666666666667, 1.0, 1.0, 0.3143000368741656, 1.0, 0.5, 0.3143000368741656, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3345625267416303, 0.3345625267416303, 0.41817140042417356], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.019382], dtype=float32), -0.35199264]. 
=============================================
[2019-03-26 11:01:48,125] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5240406e-35 1.0000000e+00 2.2421896e-38 1.1401913e-28 8.4266110e-20], sum to 1.0000
[2019-03-26 11:01:48,132] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6404
[2019-03-26 11:01:48,137] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4597026772879609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651966.8511016708, 651966.8511016708, 178762.9624516959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1299000.0000, 
sim time next is 1299600.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.45971656316523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651988.9023952944, 651988.902395295, 178765.3062099393], 
processed observation next is [1.0, 0.043478260869565216, 0.3507109004739337, 0.94, 1.0, 1.0, 0.3490561001990723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18110802844313734, 0.1811080284431375, 0.266813889865581], 
reward next is 0.7332, 
noisyNet noise sample is [array([-2.0375729], dtype=float32), -0.15933257]. 
=============================================
[2019-03-26 11:01:50,168] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6224722e-31 1.0000000e+00 3.5180481e-33 3.3221263e-27 6.1674851e-13], sum to 1.0000
[2019-03-26 11:01:50,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2859
[2019-03-26 11:01:50,185] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666666, 89.0, 1.0, 2.0, 0.6739470771927084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1062920.317937728, 1062920.317937729, 229780.7504728547], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1345200.0000, 
sim time next is 1345800.0000, 
raw observation next is [21.58333333333334, 88.5, 1.0, 2.0, 0.681101081227123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1077826.909542685, 1077826.909542686, 231799.145224729], 
processed observation next is [1.0, 0.5652173913043478, 0.22195892575039528, 0.885, 1.0, 1.0, 0.6157844352134012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2993963637618569, 0.29939636376185724, 0.34596887346974475], 
reward next is 0.6540, 
noisyNet noise sample is [array([1.7197424], dtype=float32), -0.7354634]. 
=============================================
[2019-03-26 11:01:55,194] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 11:01:55,196] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:01:55,197] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:01:55,198] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:01:55,200] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:01:55,199] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:01:55,201] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:01:55,203] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:01:55,202] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:01:55,203] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:01:55,205] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:01:55,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run28
[2019-03-26 11:01:55,221] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run28
[2019-03-26 11:01:55,261] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run28
[2019-03-26 11:01:55,262] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run28
[2019-03-26 11:01:55,299] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run28
[2019-03-26 11:02:30,939] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06414872], dtype=float32), 0.23712288]
[2019-03-26 11:02:30,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.2, 80.0, 1.0, 2.0, 0.5678562392396902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793526.2844790141, 793526.2844790141, 194910.2265544807]
[2019-03-26 11:02:30,943] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:02:30,946] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.1702450e-37 1.0000000e+00 0.0000000e+00 7.3829946e-31 5.4375655e-25], sampled 0.24079885729057104
[2019-03-26 11:02:41,857] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06414872], dtype=float32), 0.23712288]
[2019-03-26 11:02:41,858] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 76.66666666666667, 1.0, 2.0, 0.7556956873259112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1056144.959692633, 1056144.959692633, 233327.310388376]
[2019-03-26 11:02:41,858] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:02:41,862] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6810495e-33 1.0000000e+00 2.4701304e-36 9.6815940e-28 2.1327690e-12], sampled 0.18239692119977002
[2019-03-26 11:02:57,261] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06414872], dtype=float32), 0.23712288]
[2019-03-26 11:02:57,262] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.4, 43.0, 1.0, 2.0, 0.8310089749142419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1161458.846938262, 1161458.846938262, 251665.2352505539]
[2019-03-26 11:02:57,263] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:02:57,267] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.4992263e-33 1.0000000e+00 1.5468181e-36 7.5199291e-27 4.6475684e-12], sampled 0.9869946343374064
[2019-03-26 11:03:36,384] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06414872], dtype=float32), 0.23712288]
[2019-03-26 11:03:36,385] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.5, 86.66666666666667, 1.0, 2.0, 0.4226153594569511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618355.9671132707, 618355.9671132707, 175924.0946681097]
[2019-03-26 11:03:36,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:03:36,389] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.8922064e-36 1.0000000e+00 0.0000000e+00 1.1880833e-28 1.3742572e-22], sampled 0.564231407822037
[2019-03-26 11:03:47,449] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.06414872], dtype=float32), 0.23712288]
[2019-03-26 11:03:47,450] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.6, 74.0, 1.0, 2.0, 0.8168804372091937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1141701.494417533, 1141701.494417532, 248097.2569354888]
[2019-03-26 11:03:47,451] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:03:47,454] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.76964121e-32 1.00000000e+00 2.78007370e-35 4.37442416e-26
 1.09797735e-11], sampled 0.3884656663912218
[2019-03-26 11:03:50,052] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.06414872], dtype=float32), 0.23712288]
[2019-03-26 11:03:50,053] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.10775043, 95.61880149666666, 1.0, 2.0, 0.716229409779706, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.984277025383347, 6.9112, 168.9119201349339, 1897839.732122941, 1845996.693286889, 389000.9513379208]
[2019-03-26 11:03:50,054] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:03:50,057] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4322100e-29 1.0000000e+00 2.2130835e-32 1.6530356e-22 2.8494417e-11], sampled 0.6145570426023439
[2019-03-26 11:03:50,059] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1897839.732122941 W.
[2019-03-26 11:03:50,435] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8416.0616 2923886020.9504 951.0000
[2019-03-26 11:03:50,935] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8046.5395 3159046385.5981 1393.0000
[2019-03-26 11:03:50,974] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8778.5701 2779402561.8047 645.0000
[2019-03-26 11:03:51,159] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8608.8603 2840852848.2934 860.0000
[2019-03-26 11:03:51,192] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8202.2119 3002121809.0337 1274.0000
[2019-03-26 11:03:52,210] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 675000, evaluation results [675000.0, 8046.539507278642, 3159046385.598139, 1393.0, 8416.061607570391, 2923886020.950437, 951.0, 8778.570147676774, 2779402561.804659, 645.0, 8202.211855611635, 3002121809.0337186, 1274.0, 8608.86027515611, 2840852848.2933836, 860.0]
[2019-03-26 11:03:52,977] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.0453841e-37 2.5457197e-31], sum to 1.0000
[2019-03-26 11:03:52,986] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1299
[2019-03-26 11:03:52,989] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.68333333333334, 92.5, 1.0, 2.0, 0.3308154636927673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516777.1966169412, 516777.1966169412, 168163.2932813919], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1408200.0000, 
sim time next is 1408800.0000, 
raw observation next is [21.76666666666667, 92.0, 1.0, 2.0, 0.3310319007821896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516816.3737775118, 516816.3737775118, 168158.4414166933], 
processed observation next is [0.0, 0.30434782608695654, 0.23064770932069528, 0.92, 1.0, 1.0, 0.19401433829179468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14356010382708662, 0.14356010382708662, 0.25098274838312434], 
reward next is 0.7490, 
noisyNet noise sample is [array([-1.230484], dtype=float32), -1.3349754]. 
=============================================
[2019-03-26 11:03:53,282] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.3161547e-38 1.0000000e+00 0.0000000e+00 7.4822619e-30 5.6977240e-22], sum to 1.0000
[2019-03-26 11:03:53,288] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3406
[2019-03-26 11:03:53,300] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 80.66666666666667, 1.0, 2.0, 0.4230837763388082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612587.0450986268, 612587.0450986268, 175183.2670321294], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1428600.0000, 
sim time next is 1429200.0000, 
raw observation next is [26.0, 79.0, 1.0, 2.0, 0.4261282500735583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 614974.7235593331, 614974.7235593325, 175353.5020542791], 
processed observation next is [0.0, 0.5652173913043478, 0.4312796208530806, 0.79, 1.0, 1.0, 0.30858825310067267, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17082631209981475, 0.1708263120998146, 0.26172164485713295], 
reward next is 0.7383, 
noisyNet noise sample is [array([-1.5524533], dtype=float32), -1.3981693]. 
=============================================
[2019-03-26 11:03:54,881] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.3403274e-32 2.9666568e-17 1.4975893e-33 1.8296445e-31 1.0000000e+00], sum to 1.0000
[2019-03-26 11:03:54,891] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7059
[2019-03-26 11:03:54,900] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.86666666666667, 81.50000000000001, 1.0, 2.0, 0.4105178320479846, 1.0, 2.0, 0.4105178320479846, 1.0, 2.0, 0.7049594022741437, 6.9112, 6.9112, 170.5573041426782, 1721720.022521236, 1721720.022521236, 357633.3906831997], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1692600.0000, 
sim time next is 1693200.0000, 
raw observation next is [27.93333333333333, 81.0, 1.0, 2.0, 0.3605899306703886, 1.0, 2.0, 0.3605899306703886, 1.0, 2.0, 0.6186144094028279, 6.9112, 6.9112, 170.5573041426782, 1512173.67109241, 1512173.67109241, 331201.9849481297], 
processed observation next is [1.0, 0.6086956521739131, 0.522906793048973, 0.81, 1.0, 1.0, 0.22962642249444407, 1.0, 1.0, 0.22962642249444407, 1.0, 1.0, 0.5348956212229607, 0.0, 0.0, 0.8375144448122397, 0.4200482419701139, 0.4200482419701139, 0.494331320818104], 
reward next is 0.5057, 
noisyNet noise sample is [array([2.3600402], dtype=float32), 0.6762568]. 
=============================================
[2019-03-26 11:03:59,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7678746e-38 1.0000000e+00 0.0000000e+00 6.2009780e-30 3.5069607e-29], sum to 1.0000
[2019-03-26 11:03:59,965] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7909
[2019-03-26 11:03:59,972] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 77.0, 1.0, 2.0, 0.3549531454996852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 545185.9988718478, 545185.9988718478, 170198.2112257469], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1538400.0000, 
sim time next is 1539000.0000, 
raw observation next is [24.2, 78.5, 1.0, 2.0, 0.3560926938630097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 546592.6715774988, 546592.6715774994, 170305.2003777306], 
processed observation next is [0.0, 0.8260869565217391, 0.3459715639810427, 0.785, 1.0, 1.0, 0.22420806489519238, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15183129766041634, 0.1518312976604165, 0.2541868662354188], 
reward next is 0.7458, 
noisyNet noise sample is [array([1.8924934], dtype=float32), -0.05682435]. 
=============================================
[2019-03-26 11:03:59,987] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.54524 ]
 [73.552505]
 [73.58167 ]
 [73.54976 ]
 [73.56599 ]], R is [[73.55099487]
 [73.56145477]
 [73.57187653]
 [73.5823288 ]
 [73.5928421 ]].
[2019-03-26 11:04:06,974] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.8331279e-38 1.0000000e+00 0.0000000e+00 2.4659365e-32 7.2350167e-20], sum to 1.0000
[2019-03-26 11:04:06,984] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5491
[2019-03-26 11:04:06,990] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.58333333333333, 98.16666666666667, 1.0, 2.0, 0.4773386055273855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682501.2686907761, 682501.2686907767, 182104.4983785113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1666200.0000, 
sim time next is 1666800.0000, 
raw observation next is [23.6, 98.0, 1.0, 2.0, 0.4648490107739431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664753.0375699161, 664753.0375699154, 180221.710398359], 
processed observation next is [1.0, 0.30434782608695654, 0.3175355450236968, 0.98, 1.0, 1.0, 0.3552397720167989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18465362154719892, 0.18465362154719872, 0.2689876274602373], 
reward next is 0.7310, 
noisyNet noise sample is [array([-0.6953567], dtype=float32), -1.5003417]. 
=============================================
[2019-03-26 11:04:07,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1329885e-20 2.4662849e-01 5.6446220e-24 1.6022561e-20 7.5337154e-01], sum to 1.0000
[2019-03-26 11:04:07,863] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3468
[2019-03-26 11:04:07,868] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333333, 74.5, 1.0, 2.0, 0.42466934771628, 1.0, 2.0, 0.42466934771628, 1.0, 2.0, 0.7278398252862184, 6.911200000000001, 6.9112, 170.5573041426782, 1781121.089468031, 1781121.089468031, 365506.2476028267], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1702200.0000, 
sim time next is 1702800.0000, 
raw observation next is [28.9, 74.0, 1.0, 2.0, 0.4544956675748196, 1.0, 2.0, 0.4544956675748196, 1.0, 2.0, 0.7790344365129739, 6.9112, 6.9112, 170.5573041426782, 1906328.020806197, 1906328.020806197, 383429.7790861395], 
processed observation next is [1.0, 0.7391304347826086, 0.5687203791469194, 0.74, 1.0, 1.0, 0.3427658645479754, 1.0, 1.0, 0.3427658645479754, 1.0, 1.0, 0.7305298006255778, 0.0, 0.0, 0.8375144448122397, 0.5295355613350547, 0.5295355613350547, 0.5722832523673724], 
reward next is 0.4277, 
noisyNet noise sample is [array([-0.39704868], dtype=float32), 1.2025336]. 
=============================================
[2019-03-26 11:04:17,590] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.3103326e-29 1.0000000e+00 2.9623640e-29 5.1919068e-25 2.2853237e-11], sum to 1.0000
[2019-03-26 11:04:17,596] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0371
[2019-03-26 11:04:17,602] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 91.0, 1.0, 2.0, 0.8637238646207229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1248636.462475907, 1248636.462475906, 266115.14753887], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1848600.0000, 
sim time next is 1849200.0000, 
raw observation next is [24.36666666666667, 90.66666666666667, 1.0, 2.0, 0.8773852283162079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1264676.635659685, 1264676.635659684, 269362.8476898289], 
processed observation next is [1.0, 0.391304347826087, 0.3538704581358612, 0.9066666666666667, 1.0, 1.0, 0.852271359417118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3512990654610236, 0.3512990654610233, 0.40203410102959536], 
reward next is 0.5980, 
noisyNet noise sample is [array([-1.1924275], dtype=float32), 0.4510894]. 
=============================================
[2019-03-26 11:04:21,342] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.91975902e-28 9.99999881e-01 6.65869119e-31 1.41304865e-21
 1.00756644e-07], sum to 1.0000
[2019-03-26 11:04:21,350] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8112
[2019-03-26 11:04:21,355] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.23333333333333, 95.66666666666667, 1.0, 2.0, 0.4659344309157669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656519.0869793519, 656519.0869793525, 179136.0009251675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1906800.0000, 
sim time next is 1907400.0000, 
raw observation next is [24.21666666666667, 95.83333333333333, 1.0, 2.0, 0.466590250540021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657309.3593871044, 657309.3593871044, 179215.8329127621], 
processed observation next is [1.0, 0.043478260869565216, 0.34676145339652464, 0.9583333333333333, 1.0, 1.0, 0.35733765125303735, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18258593316308455, 0.18258593316308455, 0.26748631778024196], 
reward next is 0.7325, 
noisyNet noise sample is [array([-0.42572287], dtype=float32), -0.3813219]. 
=============================================
[2019-03-26 11:04:24,621] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5628126e-26 9.9999774e-01 1.8164645e-28 1.4844328e-22 2.3108876e-06], sum to 1.0000
[2019-03-26 11:04:24,629] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1997
[2019-03-26 11:04:24,635] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.45, 97.83333333333334, 1.0, 2.0, 0.4396532603462254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633903.1751191174, 633903.1751191174, 177195.574837244], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1983000.0000, 
sim time next is 1983600.0000, 
raw observation next is [23.5, 98.0, 1.0, 2.0, 0.4427695656611675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636530.7489122464, 636530.7489122457, 177409.2802272631], 
processed observation next is [1.0, 1.0, 0.31279620853080575, 0.98, 1.0, 1.0, 0.3286380309170693, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17681409692006844, 0.17681409692006825, 0.2647899704884524], 
reward next is 0.7352, 
noisyNet noise sample is [array([-0.9875519], dtype=float32), 0.20397754]. 
=============================================
[2019-03-26 11:04:25,606] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4970074e-37 1.0000000e+00 0.0000000e+00 2.1745863e-26 7.6186014e-17], sum to 1.0000
[2019-03-26 11:04:25,610] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5256
[2019-03-26 11:04:25,618] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.85, 95.5, 1.0, 2.0, 0.3992332777631029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595434.7233784811, 595434.7233784818, 174110.2442602914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1974600.0000, 
sim time next is 1975200.0000, 
raw observation next is [22.9, 95.66666666666667, 1.0, 2.0, 0.4016734466723614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 597582.7548619639, 597582.7548619632, 174263.4205400444], 
processed observation next is [1.0, 0.8695652173913043, 0.2843601895734597, 0.9566666666666667, 1.0, 1.0, 0.2791246345450137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16599520968387887, 0.16599520968387868, 0.2600946575224543], 
reward next is 0.7399, 
noisyNet noise sample is [array([0.565333], dtype=float32), -0.05894718]. 
=============================================
[2019-03-26 11:04:27,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9181826e-32 1.0000000e+00 1.3024589e-33 1.0318612e-23 1.9547996e-12], sum to 1.0000
[2019-03-26 11:04:27,852] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4430
[2019-03-26 11:04:27,859] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.08333333333333, 90.66666666666666, 1.0, 2.0, 0.5076028214960224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709299.624672002, 709299.624672002, 184796.0986747059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2029800.0000, 
sim time next is 2030400.0000, 
raw observation next is [26.2, 90.0, 1.0, 2.0, 0.5083006152379825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710275.0138739177, 710275.0138739183, 184907.0869315164], 
processed observation next is [0.0, 0.5217391304347826, 0.44075829383886256, 0.9, 1.0, 1.0, 0.40759110269636445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19729861496497716, 0.1972986149649773, 0.2759807267634573], 
reward next is 0.7240, 
noisyNet noise sample is [array([0.7095397], dtype=float32), -1.9753817]. 
=============================================
[2019-03-26 11:04:33,008] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.0922896e-35 1.0000000e+00 0.0000000e+00 2.6951505e-32 1.8912463e-18], sum to 1.0000
[2019-03-26 11:04:33,026] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7391
[2019-03-26 11:04:33,031] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 74.16666666666667, 1.0, 2.0, 0.5504094790552002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769137.2326867845, 769137.2326867852, 191871.7506261404], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2113800.0000, 
sim time next is 2114400.0000, 
raw observation next is [30.0, 74.33333333333334, 1.0, 2.0, 0.5516298219033103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770843.1475282075, 770843.1475282075, 192081.3927106046], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.7433333333333334, 1.0, 1.0, 0.45979496614856663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2141230965356132, 0.2141230965356132, 0.28668864583672327], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.5486768], dtype=float32), -0.99693984]. 
=============================================
[2019-03-26 11:04:35,528] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00451586e-29
 3.19177412e-28], sum to 1.0000
[2019-03-26 11:04:35,537] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0452
[2019-03-26 11:04:35,543] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 93.0, 1.0, 2.0, 0.5228074776523001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730553.1815580953, 730553.1815580946, 187246.6412671545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2157000.0000, 
sim time next is 2157600.0000, 
raw observation next is [26.0, 93.0, 1.0, 2.0, 0.5209833541982788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728003.3407383447, 728003.3407383454, 186948.9830338373], 
processed observation next is [0.0, 1.0, 0.4312796208530806, 0.93, 1.0, 1.0, 0.42287151108226356, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20222315020509576, 0.20222315020509596, 0.27902833288632434], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.8900863], dtype=float32), 0.10510284]. 
=============================================
[2019-03-26 11:04:39,078] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6730645e-28 1.3882594e-07 1.2270121e-30 3.8317130e-21 9.9999988e-01], sum to 1.0000
[2019-03-26 11:04:39,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0382
[2019-03-26 11:04:39,089] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.5, 83.0, 1.0, 2.0, 0.2073295642493069, 1.0, 2.0, 0.2073295642493069, 1.0, 2.0, 0.3637603677666042, 6.911199999999999, 6.9112, 170.5573041426782, 939481.3913043678, 939481.3913043684, 279894.9528897697], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2806200.0000, 
sim time next is 2806800.0000, 
raw observation next is [23.66666666666667, 83.0, 1.0, 2.0, 0.2280808676146826, 1.0, 2.0, 0.2280808676146826, 1.0, 2.0, 0.3985824126026503, 6.9112, 6.9112, 170.5573041426782, 1027295.11910766, 1027295.11910766, 285772.459076412], 
processed observation next is [1.0, 0.4782608695652174, 0.3206951026856243, 0.83, 1.0, 1.0, 0.06997694893335252, 1.0, 1.0, 0.06997694893335252, 1.0, 1.0, 0.2665639178081101, 0.0, 0.0, 0.8375144448122397, 0.2853597553076833, 0.2853597553076833, 0.426526058323003], 
reward next is 0.5735, 
noisyNet noise sample is [array([-1.3363314], dtype=float32), -0.46998435]. 
=============================================
[2019-03-26 11:04:43,390] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0496317e-27 1.4881335e-10 1.9635575e-32 4.1911514e-23 1.0000000e+00], sum to 1.0000
[2019-03-26 11:04:43,394] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7279
[2019-03-26 11:04:43,399] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 65.0, 1.0, 2.0, 0.5676983843073065, 1.0, 2.0, 0.5676983843073065, 1.0, 2.0, 0.9859043528172545, 6.9112, 6.9112, 170.5573041426782, 2381644.363490299, 2381644.363490299, 465117.1614710677], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2300400.0000, 
sim time next is 2301000.0000, 
raw observation next is [32.03333333333333, 64.83333333333334, 1.0, 2.0, 0.5737388392259213, 1.0, 2.0, 0.5737388392259213, 1.0, 2.0, 0.9963946254019216, 6.911199999999999, 6.9112, 170.5573041426782, 2407010.070184238, 2407010.070184239, 469882.4103330358], 
processed observation next is [1.0, 0.6521739130434783, 0.7172195892575038, 0.6483333333333334, 1.0, 1.0, 0.4864323364167726, 1.0, 1.0, 0.4864323364167726, 1.0, 1.0, 0.9956032017096604, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6686139083845105, 0.6686139083845108, 0.7013170303478147], 
reward next is 0.2987, 
noisyNet noise sample is [array([-0.1551326], dtype=float32), -1.8881077]. 
=============================================
[2019-03-26 11:04:43,421] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.14788 ]
 [63.8749  ]
 [63.797855]
 [63.655697]
 [63.418575]], R is [[64.09951782]
 [63.76431656]
 [63.44430542]
 [63.12680435]
 [62.80424881]].
[2019-03-26 11:04:43,593] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7937444e-27 2.3440456e-09 2.0096880e-29 1.7691278e-25 1.0000000e+00], sum to 1.0000
[2019-03-26 11:04:43,602] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2811
[2019-03-26 11:04:43,609] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.16666666666667, 64.16666666666667, 1.0, 2.0, 0.5578300081096207, 1.0, 2.0, 0.5578300081096207, 1.0, 2.0, 0.9687662468837523, 6.9112, 6.9112, 170.5573041426782, 2340205.159207549, 2340205.159207549, 457440.0980063113], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2303400.0000, 
sim time next is 2304000.0000, 
raw observation next is [32.2, 64.0, 1.0, 2.0, 0.5699480319969048, 1.0, 2.0, 0.5699480319969048, 1.0, 2.0, 0.9898112468842273, 6.911200000000001, 6.9112, 170.5573041426782, 2391091.257931932, 2391091.257931932, 466886.018409492], 
processed observation next is [1.0, 0.6956521739130435, 0.7251184834123224, 0.64, 1.0, 1.0, 0.48186509879145156, 1.0, 1.0, 0.48186509879145156, 1.0, 1.0, 0.9875746913222284, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6641920160922034, 0.6641920160922034, 0.6968448035962567], 
reward next is 0.3032, 
noisyNet noise sample is [array([-1.0398729], dtype=float32), -0.7377569]. 
=============================================
[2019-03-26 11:04:43,623] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[60.85404 ]
 [60.71496 ]
 [60.66437 ]
 [60.431454]
 [59.895496]], R is [[60.51452637]
 [60.22663498]
 [59.94670486]
 [59.68552399]
 [59.42256546]].
[2019-03-26 11:04:46,191] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 11:04:46,192] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:04:46,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:04:46,194] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:04:46,194] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:04:46,198] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:04:46,198] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:04:46,200] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:04:46,201] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:04:46,202] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:04:46,203] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:04:46,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run29
[2019-03-26 11:04:46,243] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run29
[2019-03-26 11:04:46,246] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run29
[2019-03-26 11:04:46,285] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run29
[2019-03-26 11:04:46,307] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run29
[2019-03-26 11:04:50,177] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14237857], dtype=float32), 0.25023794]
[2019-03-26 11:04:50,178] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.35, 64.0, 1.0, 2.0, 0.2658799678509485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 434603.2138462873, 434603.2138462866, 162494.9646049418]
[2019-03-26 11:04:50,179] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:04:50,181] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1272769e-30 1.0000000e+00 1.3875549e-34 1.5614115e-24 4.2934571e-15], sampled 0.8594149190935859
[2019-03-26 11:05:10,973] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14237857], dtype=float32), 0.25023794]
[2019-03-26 11:05:10,974] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.53690023166667, 98.42931041833334, 1.0, 2.0, 0.3745750238830812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565632.386206318, 565632.386206318, 171650.2281515404]
[2019-03-26 11:05:10,975] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:05:10,977] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0847374e-28 1.0000000e+00 2.7194911e-33 2.2160358e-21 1.2022097e-13], sampled 0.8915815348179815
[2019-03-26 11:05:57,168] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14237857], dtype=float32), 0.25023794]
[2019-03-26 11:05:57,169] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.53333333333333, 81.83333333333334, 1.0, 2.0, 0.6342876114133726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 886396.6853363215, 886396.6853363215, 207307.406274602]
[2019-03-26 11:05:57,170] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:05:57,173] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2746521e-25 9.9999762e-01 5.9847647e-29 1.4900987e-17 2.4230642e-06], sampled 0.2360409572933544
[2019-03-26 11:05:58,747] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14237857], dtype=float32), 0.25023794]
[2019-03-26 11:05:58,749] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.4, 79.66666666666667, 1.0, 2.0, 0.4190485178314156, 1.0, 2.0, 0.4190485178314156, 1.0, 2.0, 0.7277486940106731, 6.911200000000001, 6.9112, 178.6582176852504, 1757461.881303349, 1757461.881303348, 365719.8862064682]
[2019-03-26 11:05:58,750] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:05:58,754] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5199930e-36 1.2594655e-20 1.4832588e-37 4.6074282e-34 1.0000000e+00], sampled 0.4681396720774601
[2019-03-26 11:06:40,088] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14237857], dtype=float32), 0.25023794]
[2019-03-26 11:06:40,089] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.23250483166667, 91.78254591333334, 1.0, 2.0, 0.5691416121131967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795323.1459982364, 795323.1459982364, 195137.0989603291]
[2019-03-26 11:06:40,090] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:06:40,093] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.9619429e-27 1.0000000e+00 7.7443397e-31 4.1616227e-19 5.0345332e-09], sampled 0.9443374538034786
[2019-03-26 11:06:40,929] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8205.5455 3225013744.9148 669.0000
[2019-03-26 11:06:41,649] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8623.9560 2924239840.2090 215.0000
[2019-03-26 11:06:41,791] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8595.6406 3055646409.6780 144.0000
[2019-03-26 11:06:41,807] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8697.5339 2982563626.7345 185.0000
[2019-03-26 11:06:41,851] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8852.2878 2849433535.8526 148.0000
[2019-03-26 11:06:42,865] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 700000, evaluation results [700000.0, 8205.5455102513, 3225013744.9148264, 669.0, 8697.533869515433, 2982563626.7344756, 185.0, 8852.28784701958, 2849433535.8525944, 148.0, 8595.640589513065, 3055646409.677977, 144.0, 8623.95603743872, 2924239840.2090344, 215.0]
[2019-03-26 11:06:51,658] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.6880063e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 11:06:51,674] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3149
[2019-03-26 11:06:51,678] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.08333333333333, 92.33333333333333, 1.0, 2.0, 0.55378696632678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773858.6221313174, 773858.6221313174, 192452.5599444059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2490600.0000, 
sim time next is 2491200.0000, 
raw observation next is [27.0, 93.0, 1.0, 2.0, 0.5543193037997713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774602.7785973914, 774602.7785973914, 192544.4804045955], 
processed observation next is [1.0, 0.8695652173913043, 0.4786729857819906, 0.93, 1.0, 1.0, 0.463035305782857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2151674384992754, 0.2151674384992754, 0.2873798214993963], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.39927718], dtype=float32), -0.5382174]. 
=============================================
[2019-03-26 11:06:55,380] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5493303e-38 1.0000000e+00 0.0000000e+00 4.3599714e-30 7.4928567e-38], sum to 1.0000
[2019-03-26 11:06:55,388] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1575
[2019-03-26 11:06:55,408] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 91.16666666666667, 1.0, 2.0, 0.5060612323874153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707144.765629216, 707144.7656292167, 184550.80795788], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2589000.0000, 
sim time next is 2589600.0000, 
raw observation next is [25.6, 91.33333333333334, 1.0, 2.0, 0.5035242918282284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703598.5971951223, 703598.5971951229, 184149.6919935926], 
processed observation next is [1.0, 1.0, 0.4123222748815167, 0.9133333333333334, 1.0, 1.0, 0.40183649617858846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19544405477642285, 0.19544405477642302, 0.2748502865576009], 
reward next is 0.7251, 
noisyNet noise sample is [array([0.2540727], dtype=float32), 0.73439294]. 
=============================================
[2019-03-26 11:06:56,605] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7305518e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 11:06:56,612] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6572
[2019-03-26 11:06:56,616] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 88.0, 1.0, 2.0, 0.527408824582229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736985.1759527229, 736985.1759527235, 188001.5331280662], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2583000.0000, 
sim time next is 2583600.0000, 
raw observation next is [26.63333333333333, 88.33333333333333, 1.0, 2.0, 0.524319538201854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 732666.811622995, 732666.8116229944, 187493.8411431129], 
processed observation next is [1.0, 0.9130434782608695, 0.46129541864139006, 0.8833333333333333, 1.0, 1.0, 0.4268910098817517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20351855878416528, 0.2035185587841651, 0.2798415539449446], 
reward next is 0.7202, 
noisyNet noise sample is [array([0.25275642], dtype=float32), 1.8838474]. 
=============================================
[2019-03-26 11:07:00,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.3457557e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 11:07:00,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8850
[2019-03-26 11:07:00,198] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3958583586890215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590679.0946986069, 590679.0946986069, 173679.9431152444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2669400.0000, 
sim time next is 2670000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3957938915681671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590582.9688820627, 590582.9688820634, 173671.1171624665], 
processed observation next is [0.0, 0.9130434782608695, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2720408332146591, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16405082468946186, 0.16405082468946205, 0.259210622630547], 
reward next is 0.7408, 
noisyNet noise sample is [array([-1.8517113], dtype=float32), 0.20118737]. 
=============================================
[2019-03-26 11:07:00,210] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.858955]
 [74.813774]
 [74.773705]
 [74.75017 ]
 [74.65822 ]], R is [[74.90821075]
 [74.89990234]
 [74.89165497]
 [74.88345337]
 [74.87528229]].
[2019-03-26 11:07:21,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8448818e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 11:07:21,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6860
[2019-03-26 11:07:21,814] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 79.0, 1.0, 2.0, 0.5115656084541126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714838.8946300927, 714838.8946300927, 185427.6951711184], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3277200.0000, 
sim time next is 3277800.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.5074617159312806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709102.3847715318, 709102.3847715311, 184773.0094002295], 
processed observation next is [0.0, 0.9565217391304348, 0.5023696682464456, 0.79, 1.0, 1.0, 0.4065803806400971, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19697288465875884, 0.19697288465875865, 0.27578061104511864], 
reward next is 0.7242, 
noisyNet noise sample is [array([2.3728364], dtype=float32), -0.8296968]. 
=============================================
[2019-03-26 11:07:25,582] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3264349e-35 1.0000000e+00 0.0000000e+00 9.9156256e-25 3.8610403e-23], sum to 1.0000
[2019-03-26 11:07:25,593] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2365
[2019-03-26 11:07:25,597] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4329084117847707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628357.4160136356, 628357.4160136356, 176759.1349589133], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3090600.0000, 
sim time next is 3091200.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4296902682384305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 623687.5206354492, 623687.5206354499, 176302.0155777762], 
processed observation next is [1.0, 0.782608695652174, 0.28909952606635075, 1.0, 1.0, 1.0, 0.312879841251121, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.173246533509847, 0.1732465335098472, 0.26313733668324807], 
reward next is 0.7369, 
noisyNet noise sample is [array([-2.8395412], dtype=float32), -0.6577867]. 
=============================================
[2019-03-26 11:07:36,008] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 11:07:36,010] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:07:36,011] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:07:36,014] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:07:36,014] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:07:36,015] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:07:36,015] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:07:36,013] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:07:36,022] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:07:36,025] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:07:36,028] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:07:36,035] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run30
[2019-03-26 11:07:36,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run30
[2019-03-26 11:07:36,059] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run30
[2019-03-26 11:07:36,059] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run30
[2019-03-26 11:07:36,117] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run30
[2019-03-26 11:07:51,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.21966709], dtype=float32), 0.25104585]
[2019-03-26 11:07:51,178] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.35, 89.16666666666666, 1.0, 2.0, 0.3034979173885851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482716.3050803329, 482716.3050803329, 165807.6627126154]
[2019-03-26 11:07:51,179] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:07:51,183] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3558463e-36 1.0000000e+00 0.0000000e+00 4.7586075e-31 3.7634503e-23], sampled 0.5511220227957634
[2019-03-26 11:08:23,620] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.21966709], dtype=float32), 0.25104585]
[2019-03-26 11:08:23,620] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.6, 52.0, 1.0, 2.0, 0.5678754331523099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793553.1162061839, 793553.1162061839, 194913.2755308324]
[2019-03-26 11:08:23,622] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:08:23,624] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3117811e-34 1.0000000e+00 1.7647956e-38 8.5787583e-28 2.0082717e-20], sampled 0.6557006323680435
[2019-03-26 11:08:45,144] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.21966709], dtype=float32), 0.25104585]
[2019-03-26 11:08:45,144] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.877344875, 74.91632408333334, 1.0, 2.0, 0.5699007322600192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 804766.8139145821, 804766.8139145827, 196348.4700395095]
[2019-03-26 11:08:45,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:08:45,148] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.71507833e-31 1.00000000e+00 3.42636877e-33 1.76396674e-24
 1.05822365e-10], sampled 0.5654988431152744
[2019-03-26 11:08:52,152] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21966709], dtype=float32), 0.25104585]
[2019-03-26 11:08:52,152] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.51331567833333, 75.18405805666667, 1.0, 2.0, 0.7923371264242712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1107380.993210807, 1107380.993210807, 242034.8567191482]
[2019-03-26 11:08:52,153] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:08:52,156] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.7481173e-35 1.0000000e+00 1.3837266e-36 5.2332106e-31 5.9787550e-14], sampled 0.7980029478162342
[2019-03-26 11:09:14,051] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.21966709], dtype=float32), 0.25104585]
[2019-03-26 11:09:14,056] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.03333333333333, 73.66666666666666, 1.0, 2.0, 0.5707414626166104, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9841887153614332, 6.911200000000001, 6.9112, 168.9129008880007, 1595718.579621056, 1595718.579621055, 347719.2235393965]
[2019-03-26 11:09:14,057] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:09:14,060] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8102603e-25 9.4303852e-01 4.5541610e-27 7.1095972e-19 5.6961410e-02], sampled 0.897159671070417
[2019-03-26 11:09:26,177] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.21966709], dtype=float32), 0.25104585]
[2019-03-26 11:09:26,179] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.15, 65.5, 1.0, 2.0, 0.4009346904171443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 599465.0468951699, 599465.0468951705, 174521.4094605733]
[2019-03-26 11:09:26,182] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:09:26,185] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.8269703e-37 1.0000000e+00 0.0000000e+00 1.3213987e-29 4.7892481e-27], sampled 0.665076762263838
[2019-03-26 11:09:31,018] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8114.8677 3159375648.8070 1246.0000
[2019-03-26 11:09:31,132] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8538.6490 2926054361.3933 712.0000
[2019-03-26 11:09:31,289] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8839.2408 2782444819.4923 512.0000
[2019-03-26 11:09:31,580] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8660.9933 2843933387.7953 726.0000
[2019-03-26 11:09:31,587] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8277.5368 3002495404.0413 1116.0000
[2019-03-26 11:09:32,602] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 725000, evaluation results [725000.0, 8114.867728416282, 3159375648.8069663, 1246.0, 8538.648967676761, 2926054361.393299, 712.0, 8839.240805268008, 2782444819.4923334, 512.0, 8277.536798705505, 3002495404.0413074, 1116.0, 8660.993279612827, 2843933387.7952538, 726.0]
[2019-03-26 11:09:33,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3195680e-34 1.0000000e+00 1.0726718e-37 4.1970766e-28 5.3818873e-18], sum to 1.0000
[2019-03-26 11:09:33,612] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3988
[2019-03-26 11:09:33,616] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4214444541305093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617074.541790927, 617074.541790927, 175813.1796244131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3302400.0000, 
sim time next is 3303000.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4214256183479789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617046.8286657725, 617046.8286657725, 175810.5194136259], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.83, 1.0, 1.0, 0.30292243174455297, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17140189685160345, 0.17140189685160345, 0.26240376031884466], 
reward next is 0.7376, 
noisyNet noise sample is [array([0.8824294], dtype=float32), 0.4716249]. 
=============================================
[2019-03-26 11:09:33,628] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.57691 ]
 [72.565506]
 [72.55358 ]
 [72.468216]
 [72.45691 ]], R is [[72.59674072]
 [72.60836792]
 [72.6199646 ]
 [72.63142395]
 [72.64260864]].
[2019-03-26 11:09:34,138] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.5248221e-31 1.0000000e+00 1.5810855e-36 4.4975425e-25 9.1376187e-20], sum to 1.0000
[2019-03-26 11:09:34,147] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0974
[2019-03-26 11:09:34,150] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.5209711584512375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727986.2930018047, 727986.2930018047, 186947.8906946398], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3319800.0000, 
sim time next is 3320400.0000, 
raw observation next is [29.66666666666667, 72.66666666666667, 1.0, 2.0, 0.5287594422156511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738873.1446716838, 738873.1446716838, 188225.5981751533], 
processed observation next is [0.0, 0.43478260869565216, 0.6050552922590839, 0.7266666666666667, 1.0, 1.0, 0.4322402918260856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20524254018657884, 0.20524254018657884, 0.2809337286196318], 
reward next is 0.7191, 
noisyNet noise sample is [array([-0.22126608], dtype=float32), 0.36924174]. 
=============================================
[2019-03-26 11:09:34,354] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2522554e-36 1.0000000e+00 0.0000000e+00 8.4149166e-28 1.3079394e-20], sum to 1.0000
[2019-03-26 11:09:34,361] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8553
[2019-03-26 11:09:34,368] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 68.0, 1.0, 2.0, 0.5704442115171784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 797144.0921052421, 797144.0921052414, 195369.4869310706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3325200.0000, 
sim time next is 3325800.0000, 
raw observation next is [31.83333333333333, 67.5, 1.0, 2.0, 0.5728011104296235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800438.8857474062, 800438.8857474062, 195788.7925139981], 
processed observation next is [0.0, 0.4782608695652174, 0.7077409162717218, 0.675, 1.0, 1.0, 0.4853025426862933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22234413492983504, 0.22234413492983504, 0.29222207837910164], 
reward next is 0.7078, 
noisyNet noise sample is [array([0.7068934], dtype=float32), 0.5776808]. 
=============================================
[2019-03-26 11:09:35,684] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9726439e-30 4.0322457e-11 2.3453856e-33 1.6196915e-36 1.0000000e+00], sum to 1.0000
[2019-03-26 11:09:35,692] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5660
[2019-03-26 11:09:35,697] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 67.33333333333334, 1.0, 2.0, 0.503586584917563, 1.0, 2.0, 0.503586584917563, 1.0, 2.0, 0.8745354572891125, 6.911199999999999, 6.9112, 170.5573041426782, 2112437.04111654, 2112437.041116541, 417629.471492136], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3579600.0000, 
sim time next is 3580200.0000, 
raw observation next is [31.0, 68.0, 1.0, 2.0, 0.5248501864635706, 1.0, 2.0, 0.5248501864635706, 1.0, 2.0, 0.9114912032782444, 6.9112, 6.9112, 170.5573041426782, 2201724.930310814, 2201724.930310814, 432755.5415567602], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.68, 1.0, 1.0, 0.427530345136832, 1.0, 1.0, 0.427530345136832, 1.0, 1.0, 0.8920624430222491, 0.0, 0.0, 0.8375144448122397, 0.6115902584196706, 0.6115902584196706, 0.6459037933682988], 
reward next is 0.3541, 
noisyNet noise sample is [array([1.2288611], dtype=float32), 0.22460784]. 
=============================================
[2019-03-26 11:09:39,768] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8060081e-27 9.9999952e-01 2.7992329e-31 6.4399080e-22 5.0472806e-07], sum to 1.0000
[2019-03-26 11:09:39,776] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8750
[2019-03-26 11:09:39,784] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2902116.741252822 W.
[2019-03-26 11:09:39,790] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.7420526485399288, 1.0, 2.0, 0.6916163637842269, 1.0, 2.0, 1.03, 7.005101048202501, 6.9112, 170.5573041426782, 2902116.741252822, 2834851.600774943, 534940.9183119254], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3412800.0000, 
sim time next is 3413400.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.770746033644882, 1.0, 2.0, 0.7059630563367036, 1.0, 2.0, 1.03, 7.005103311078128, 6.9112, 170.5573041426782, 2962388.810489372, 2895122.049021633, 544605.5399610896], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.67, 1.0, 1.0, 0.7237904019817856, 1.0, 1.0, 0.6457386220924141, 1.0, 1.0, 1.0365853658536586, 0.009390331107812778, 0.0, 0.8375144448122397, 0.8228857806914922, 0.8042005691726758, 0.8128440894941636], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.853816], dtype=float32), -0.10409863]. 
=============================================
[2019-03-26 11:09:42,043] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.0946280e-24 2.0381594e-02 4.5148312e-26 2.0406159e-17 9.7961843e-01], sum to 1.0000
[2019-03-26 11:09:42,052] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4788
[2019-03-26 11:09:42,063] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 66.5, 1.0, 2.0, 0.5909227156245314, 1.0, 2.0, 0.5909227156245314, 1.0, 2.0, 1.026237335911544, 6.911200000000001, 6.9112, 170.5573041426782, 2479173.207674473, 2479173.207674472, 483713.3121149794], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3429000.0000, 
sim time next is 3429600.0000, 
raw observation next is [31.66666666666666, 67.66666666666667, 1.0, 2.0, 0.5849094388005477, 1.0, 2.0, 0.5849094388005477, 1.0, 2.0, 1.015794262689318, 6.9112, 6.9112, 170.5573041426782, 2453920.17061263, 2453920.17061263, 478827.1845196593], 
processed observation next is [1.0, 0.6956521739130435, 0.6998420221169034, 0.6766666666666667, 1.0, 1.0, 0.4998908901211418, 1.0, 1.0, 0.4998908901211418, 1.0, 1.0, 1.0192612959625829, 0.0, 0.0, 0.8375144448122397, 0.6816444918368416, 0.6816444918368416, 0.7146674395815811], 
reward next is 0.2853, 
noisyNet noise sample is [array([-0.18037331], dtype=float32), -0.12890328]. 
=============================================
[2019-03-26 11:09:43,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2277924e-33 1.0000000e+00 2.7560711e-36 2.2073570e-26 5.0549395e-21], sum to 1.0000
[2019-03-26 11:09:43,599] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8166
[2019-03-26 11:09:43,606] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5074331952403807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709062.5180404293, 709062.5180404286, 184768.9709951507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3460200.0000, 
sim time next is 3460800.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5068390298243134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708231.9833506766, 708231.983350676, 184674.5551597565], 
processed observation next is [1.0, 0.043478260869565216, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.4058301564148354, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19673110648629905, 0.1967311064862989, 0.275633664417547], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.736691], dtype=float32), 0.08229072]. 
=============================================
[2019-03-26 11:09:45,183] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.5493922e-19 7.4967629e-01 4.4915089e-22 2.3809139e-14 2.5032368e-01], sum to 1.0000
[2019-03-26 11:09:45,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8817
[2019-03-26 11:09:45,198] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666666, 67.33333333333333, 1.0, 2.0, 0.4762285289061181, 1.0, 2.0, 0.4762285289061181, 1.0, 2.0, 0.8122029344673318, 6.911199999999999, 6.9112, 170.5573041426782, 1997568.903432091, 1997568.903432091, 396530.7434850005], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3487800.0000, 
sim time next is 3488400.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.4378726285034936, 1.0, 2.0, 0.4378726285034936, 1.0, 2.0, 0.747244497146847, 6.9112, 6.9112, 170.5573041426782, 1836544.904654201, 1836544.904654201, 372773.0331065361], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.32273810663071517, 1.0, 1.0, 0.32273810663071517, 1.0, 1.0, 0.6917615818863988, 0.0, 0.0, 0.8375144448122397, 0.5101513624039447, 0.5101513624039447, 0.5563776613530389], 
reward next is 0.4436, 
noisyNet noise sample is [array([0.13415663], dtype=float32), 1.29845]. 
=============================================
[2019-03-26 11:09:48,232] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2838729e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 11:09:48,238] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9911
[2019-03-26 11:09:48,243] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 73.66666666666667, 1.0, 2.0, 0.6262054064710979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 875097.4089219802, 875097.4089219796, 205730.8517801858], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4126800.0000, 
sim time next is 4127400.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.6240140405236253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872033.8040864693, 872033.8040864693, 205306.211541644], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.75, 1.0, 1.0, 0.5470048681007533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24223161224624148, 0.24223161224624148, 0.30642718140543884], 
reward next is 0.6936, 
noisyNet noise sample is [array([-1.5047724], dtype=float32), 1.2460192]. 
=============================================
[2019-03-26 11:09:50,513] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.8068111e-38 1.0000000e+00 0.0000000e+00 1.7648396e-34 1.0392647e-37], sum to 1.0000
[2019-03-26 11:09:50,519] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9926
[2019-03-26 11:09:50,523] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 70.83333333333333, 1.0, 2.0, 0.5633243643455713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787191.0640905906, 787191.0640905906, 194112.5849742868], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3833400.0000, 
sim time next is 3834000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.563359603511998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787240.3256501538, 787240.3256501532, 194118.7997809594], 
processed observation next is [0.0, 0.391304347826087, 0.6682464454976303, 0.7, 1.0, 1.0, 0.4739272331469855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21867786823615384, 0.21867786823615368, 0.2897295519118797], 
reward next is 0.7103, 
noisyNet noise sample is [array([0.35168964], dtype=float32), -0.7883884]. 
=============================================
[2019-03-26 11:09:50,538] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[55.41437 ]
 [55.42881 ]
 [55.434723]
 [55.449066]
 [55.47193 ]], R is [[55.6320343 ]
 [55.78599548]
 [55.9384079 ]
 [56.08927155]
 [56.23838043]].
[2019-03-26 11:09:52,447] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0543635e-36 1.0000000e+00 0.0000000e+00 5.1924748e-35 7.7396657e-33], sum to 1.0000
[2019-03-26 11:09:52,458] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2554
[2019-03-26 11:09:52,461] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 71.0, 1.0, 2.0, 0.5411160302702348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565103729, 756146.0289815468, 756146.0289815468, 190291.370528873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3691800.0000, 
sim time next is 3692400.0000, 
raw observation next is [30.66666666666667, 72.33333333333334, 1.0, 2.0, 0.5447880596033268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761279.0969874092, 761279.0969874092, 190912.9575939931], 
processed observation next is [1.0, 0.7391304347826086, 0.6524486571879939, 0.7233333333333334, 1.0, 1.0, 0.4515518790401527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21146641582983589, 0.21146641582983589, 0.2849447128268554], 
reward next is 0.7151, 
noisyNet noise sample is [array([1.533206], dtype=float32), 0.041641768]. 
=============================================
[2019-03-26 11:09:57,847] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8671840e-38 1.0000000e+00 0.0000000e+00 1.2930967e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 11:09:57,857] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5132
[2019-03-26 11:09:57,861] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 76.5, 1.0, 2.0, 0.4905703973505663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 685491.6599443932, 685491.6599443926, 182131.5274935262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3713400.0000, 
sim time next is 3714000.0000, 
raw observation next is [27.33333333333334, 77.33333333333333, 1.0, 2.0, 0.490936171609408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686002.9345044851, 686002.9345044851, 182187.7397193728], 
processed observation next is [1.0, 1.0, 0.4944707740916275, 0.7733333333333333, 1.0, 1.0, 0.3866700862763952, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1905563706956903, 0.1905563706956903, 0.27192199958115343], 
reward next is 0.7281, 
noisyNet noise sample is [array([0.91904014], dtype=float32), -1.0169727]. 
=============================================
[2019-03-26 11:09:57,876] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[67.60595 ]
 [67.557045]
 [67.50683 ]
 [67.47251 ]
 [67.38259 ]], R is [[67.70743561]
 [67.75852966]
 [67.80914307]
 [67.85914612]
 [67.90856171]].
[2019-03-26 11:10:16,239] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0520188e-29 1.0000000e+00 1.9451011e-37 2.2002460e-10 9.3654131e-28], sum to 1.0000
[2019-03-26 11:10:16,250] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0001
[2019-03-26 11:10:16,254] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5542029551291414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 774440.13432818, 774440.1343281806, 192524.1516699629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4042800.0000, 
sim time next is 4043400.0000, 
raw observation next is [28.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5520684506241822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771456.3064145017, 771456.3064145017, 192156.1725162305], 
processed observation next is [1.0, 0.8260869565217391, 0.5655608214849924, 0.7983333333333335, 1.0, 1.0, 0.4603234344869665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21429341844847272, 0.21429341844847272, 0.2868002574869112], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.7733825], dtype=float32), -0.8459533]. 
=============================================
[2019-03-26 11:10:19,098] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8837897e-30 7.4015266e-11 4.6433720e-33 3.4163514e-22 1.0000000e+00], sum to 1.0000
[2019-03-26 11:10:19,103] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9845
[2019-03-26 11:10:19,107] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 71.0, 1.0, 2.0, 0.6440332718533647, 1.0, 2.0, 0.6426066754409449, 1.0, 2.0, 1.03, 7.005093319646533, 6.9112, 170.5573041426782, 2696243.460504836, 2628983.856305975, 504234.7167507255], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4104000.0000, 
sim time next is 4104600.0000, 
raw observation next is [33.16666666666666, 71.0, 1.0, 2.0, 0.6678707567543187, 1.0, 2.0, 0.6545254178914218, 1.0, 2.0, 1.03, 7.005095198931822, 6.9112, 170.5573041426782, 2746306.982228575, 2679046.031821216, 511374.3124838358], 
processed observation next is [1.0, 0.5217391304347826, 0.7709320695102682, 0.71, 1.0, 1.0, 0.5998442852461672, 1.0, 1.0, 0.5837655637246046, 1.0, 1.0, 1.0365853658536586, 0.009389519893182197, 0.0, 0.8375144448122397, 0.7628630506190486, 0.7441794532836711, 0.7632452425131877], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.25591666], dtype=float32), 1.2508006]. 
=============================================
[2019-03-26 11:10:24,104] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2671684e-21 9.9994636e-01 4.8135594e-24 4.6543384e-22 5.3634143e-05], sum to 1.0000
[2019-03-26 11:10:24,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1662
[2019-03-26 11:10:24,122] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2608080.562599769 W.
[2019-03-26 11:10:24,129] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.33333333333334, 55.0, 1.0, 2.0, 0.6216163621499682, 1.0, 2.0, 0.6216163621499682, 1.0, 2.0, 1.03, 6.966894785938018, 6.9112, 170.5573041426782, 2608080.562599769, 2568184.121913323, 495950.3667987745], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4188000.0000, 
sim time next is 4188600.0000, 
raw observation next is [35.5, 54.5, 1.0, 2.0, 0.932334400588041, 1.0, 2.0, 0.932334400588041, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2607828.161891921, 2607828.161891921, 489446.160174203], 
processed observation next is [1.0, 0.4782608695652174, 0.8815165876777251, 0.545, 1.0, 1.0, 0.9184751814313746, 1.0, 1.0, 0.9184751814313746, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7243967116366448, 0.7243967116366448, 0.7305166569764223], 
reward next is 0.2695, 
noisyNet noise sample is [array([-0.15358242], dtype=float32), -0.91695416]. 
=============================================
[2019-03-26 11:10:26,586] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7238762e-19 9.9968231e-01 1.5138344e-21 4.4328777e-11 3.1772652e-04], sum to 1.0000
[2019-03-26 11:10:26,594] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 11:10:26,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5437
[2019-03-26 11:10:26,601] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:10:26,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:10:26,604] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:10:26,606] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:10:26,607] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:10:26,607] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2458425.883110345 W.
[2019-03-26 11:10:26,608] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:10:26,610] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:10:26,611] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:10:26,612] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.58333333333333, 64.66666666666667, 1.0, 2.0, 0.8789735282780564, 1.0, 2.0, 0.8789735282780564, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2458425.883110345, 2458425.883110345, 460147.8535635392], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4801800.0000, 
sim time next is 4802400.0000, 
raw observation next is [31.5, 65.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.967624880453835, 6.9112, 168.9059748664972, 3040233.472132774, 2290800.280046408, 473946.6654063478], 
processed observation next is [1.0, 0.6086956521739131, 0.6919431279620853, 0.65, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.1056424880453835, 0.0, 0.8294056620843773, 0.8445092978146594, 0.6363334111240022, 0.7073830826960414], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.13357006], dtype=float32), 0.9493161]. 
=============================================
[2019-03-26 11:10:26,612] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:10:26,613] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:10:26,635] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run31
[2019-03-26 11:10:26,636] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run31
[2019-03-26 11:10:26,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run31
[2019-03-26 11:10:26,657] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run31
[2019-03-26 11:10:26,704] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run31
[2019-03-26 11:10:40,648] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.23864733], dtype=float32), 0.3024165]
[2019-03-26 11:10:40,649] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.33333333333334, 89.0, 1.0, 2.0, 0.3431394154463024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532919.8848588557, 532919.8848588563, 169363.1916564574]
[2019-03-26 11:10:40,650] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:10:40,654] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.41282900066862815
[2019-03-26 11:10:47,143] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.23864733], dtype=float32), 0.3024165]
[2019-03-26 11:10:47,146] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.73333333333333, 94.33333333333334, 1.0, 2.0, 0.8214807975876162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1233206.93885798, 1233206.93885798, 260891.7385488646]
[2019-03-26 11:10:47,147] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:10:47,150] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4132676e-37 1.0000000e+00 0.0000000e+00 8.8433310e-32 4.7106113e-27], sampled 0.07674074664630437
[2019-03-26 11:10:55,363] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.23864733], dtype=float32), 0.3024165]
[2019-03-26 11:10:55,365] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.693483925, 94.35169574, 1.0, 2.0, 0.4727981104281827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663260.6056722715, 663260.6056722715, 179781.3241656203]
[2019-03-26 11:10:55,367] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:10:55,370] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4241021e-35 2.2912848e-38], sampled 0.8458569064217373
[2019-03-26 11:11:17,592] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.23864733], dtype=float32), 0.3024165]
[2019-03-26 11:11:17,593] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.71857388, 58.71101154, 1.0, 2.0, 0.8001343808028004, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981536629752, 6.9112, 168.9123160039225, 2015261.490823271, 1948020.461583794, 407878.4651591429]
[2019-03-26 11:11:17,594] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:11:17,597] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.9725550e-28 1.0000000e+00 4.4280712e-31 1.0350458e-17 5.5616387e-15], sampled 0.32739805629863583
[2019-03-26 11:11:17,599] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2015261.490823271 W.
[2019-03-26 11:11:44,168] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.23864733], dtype=float32), 0.3024165]
[2019-03-26 11:11:44,169] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.5, 55.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.416285087538952, 6.9112, 168.909918942272, 2642326.546387881, 2284008.24536823, 474999.4985062372]
[2019-03-26 11:11:44,171] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:11:44,173] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5445320e-26 1.0000000e+00 9.0735209e-30 2.2016845e-15 1.1137952e-14], sampled 0.44881176519437715
[2019-03-26 11:11:44,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2642326.546387881 W.
[2019-03-26 11:12:00,743] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.23864733], dtype=float32), 0.3024165]
[2019-03-26 11:12:00,745] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.06693066166666, 53.593105675, 1.0, 2.0, 0.4843924190842196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680671.0765363654, 680671.0765363661, 181677.3078528302]
[2019-03-26 11:12:00,747] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:12:00,750] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 1.47862e-38 0.00000e+00], sampled 0.7627666593052254
[2019-03-26 11:12:11,276] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.23864733], dtype=float32), 0.3024165]
[2019-03-26 11:12:11,277] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.8, 67.5, 1.0, 2.0, 0.5894884827403138, 0.0, 2.0, 0.0, 1.0, 1.0, 1.023746547699721, 6.911200000000001, 6.9112, 168.9122477858528, 1648173.501414149, 1648173.501414148, 361007.1740340238]
[2019-03-26 11:12:11,280] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:12:11,283] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.3777012e-30 1.0000000e+00 5.0047726e-33 2.5579065e-24 9.9903102e-17], sampled 0.7919423119226829
[2019-03-26 11:12:20,955] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.23864733], dtype=float32), 0.3024165]
[2019-03-26 11:12:20,957] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.0, 64.0, 1.0, 2.0, 0.3525881263262725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547600.5760889766, 547600.5760889766, 170563.1005559903]
[2019-03-26 11:12:20,957] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:12:20,960] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.7158866e-38 0.0000000e+00], sampled 0.6418625645458442
[2019-03-26 11:12:22,494] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.7456 3164502197.1393 1819.0000
[2019-03-26 11:12:22,859] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8250.0251 2927666655.1061 1344.0000
[2019-03-26 11:12:23,193] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.3563 3007937057.5632 1766.0000
[2019-03-26 11:12:23,276] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.5288 2842718048.4231 1132.0000
[2019-03-26 11:12:23,276] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8654.5566 2779822752.9240 945.0000
[2019-03-26 11:12:24,292] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 750000, evaluation results [750000.0, 7876.745616792593, 3164502197.1393394, 1819.0, 8250.025145903204, 2927666655.1061177, 1344.0, 8654.556646411913, 2779822752.924012, 945.0, 7995.356252368326, 3007937057.5631742, 1766.0, 8494.52877625375, 2842718048.4231496, 1132.0]
[2019-03-26 11:12:24,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.8990823e-23 1.0000000e+00 5.5859499e-27 3.1222191e-19 2.1818766e-12], sum to 1.0000
[2019-03-26 11:12:24,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9798
[2019-03-26 11:12:24,621] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2572331.419681767 W.
[2019-03-26 11:12:24,627] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 77.66666666666667, 1.0, 2.0, 0.6131045852094901, 1.0, 1.0, 0.6131045852094901, 1.0, 1.0, 1.03, 6.950276258993491, 6.9112, 170.5573041426782, 2572331.419681767, 2544339.505844141, 492779.105821643], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4242000.0000, 
sim time next is 4242600.0000, 
raw observation next is [30.0, 77.0, 1.0, 2.0, 0.7208893346160574, 1.0, 2.0, 0.7208893346160574, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2015892.698519349, 2015892.698519349, 383201.34248528], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.77, 1.0, 1.0, 0.6637220898988644, 1.0, 1.0, 0.6637220898988644, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5599701940331525, 0.5599701940331525, 0.5719423022168358], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.89246184], dtype=float32), 0.7647966]. 
=============================================
[2019-03-26 11:12:25,417] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.4952566e-35 1.0000000e+00 0.0000000e+00 1.0639729e-26 1.2704067e-24], sum to 1.0000
[2019-03-26 11:12:25,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6507
[2019-03-26 11:12:25,431] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5952278981837773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831790.6091750228, 831790.6091750228, 199860.3863433832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4238400.0000, 
sim time next is 4239000.0000, 
raw observation next is [30.5, 77.0, 1.0, 2.0, 0.593500489155033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 829375.7297710113, 829375.7297710113, 199541.4305315053], 
processed observation next is [1.0, 0.043478260869565216, 0.6445497630331753, 0.77, 1.0, 1.0, 0.5102415531988349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23038214715861424, 0.23038214715861424, 0.29782303064403776], 
reward next is 0.7022, 
noisyNet noise sample is [array([-0.49832475], dtype=float32), 0.041741632]. 
=============================================
[2019-03-26 11:12:25,449] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[62.45389 ]
 [62.486046]
 [62.61493 ]
 [62.623844]
 [62.783207]], R is [[62.47653961]
 [62.55347824]
 [62.62869644]
 [62.70332336]
 [62.77712631]].
[2019-03-26 11:12:29,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9718679e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 11:12:29,813] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5394
[2019-03-26 11:12:29,818] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.66666666666666, 58.66666666666667, 1.0, 2.0, 0.5933207032444365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 829124.3933819556, 829124.393381955, 199509.7389313164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4304400.0000, 
sim time next is 4305000.0000, 
raw observation next is [34.33333333333334, 60.83333333333333, 1.0, 2.0, 0.6001270733701467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838639.5769942261, 838639.5769942261, 200771.0780567969], 
processed observation next is [1.0, 0.8260869565217391, 0.8262243285939973, 0.6083333333333333, 1.0, 1.0, 0.5182253896025864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2329554380539517, 0.2329554380539517, 0.2996583254579058], 
reward next is 0.7003, 
noisyNet noise sample is [array([-1.8839344], dtype=float32), 0.16870093]. 
=============================================
[2019-03-26 11:12:29,831] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[62.672832]
 [62.73789 ]
 [62.811886]
 [62.36238 ]
 [62.551586]], R is [[62.47646713]
 [62.55392838]
 [62.63202286]
 [62.71058273]
 [62.78988647]].
[2019-03-26 11:12:56,215] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.10926102e-32 1.00000000e+00 1.01942114e-35 3.56477024e-29
 2.10800610e-23], sum to 1.0000
[2019-03-26 11:12:56,224] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1837
[2019-03-26 11:12:56,230] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6332765027489061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 884983.1045701196, 884983.1045701191, 207098.9061980101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4766400.0000, 
sim time next is 4767000.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.658356801585593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 920047.1792237106, 920047.1792237106, 212110.0031102683], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.5883816886573409, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25556866089547514, 0.25556866089547514, 0.3165820941944303], 
reward next is 0.6834, 
noisyNet noise sample is [array([0.08266291], dtype=float32), 1.0180218]. 
=============================================
[2019-03-26 11:12:56,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[66.50887 ]
 [66.13821 ]
 [66.168015]
 [66.28191 ]
 [66.073555]], R is [[66.49446869]
 [66.52041626]
 [66.52297211]
 [66.52986908]
 [66.54911804]].
[2019-03-26 11:12:56,432] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.9608508e-36 1.0000000e+00 0.0000000e+00 4.7192094e-34 3.0848593e-24], sum to 1.0000
[2019-03-26 11:12:56,444] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8837
[2019-03-26 11:12:56,449] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6636247599018241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 927412.3136915882, 927412.3136915877, 213186.2250186434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4771800.0000, 
sim time next is 4772400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6051795553587257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845702.911507406, 845702.911507406, 201706.3067884783], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5243127172996694, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2349174754187239, 0.2349174754187239, 0.30105418923653476], 
reward next is 0.6989, 
noisyNet noise sample is [array([0.45208314], dtype=float32), 0.7075937]. 
=============================================
[2019-03-26 11:12:57,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3719764e-24 1.0000000e+00 4.9942171e-29 2.4492503e-16 4.9313966e-09], sum to 1.0000
[2019-03-26 11:12:57,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1786
[2019-03-26 11:12:57,055] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2268516.453965872 W.
[2019-03-26 11:12:57,060] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.41666666666666, 65.16666666666667, 1.0, 2.0, 0.5407572108271466, 1.0, 2.0, 0.5407572108271466, 1.0, 2.0, 0.939116444064433, 6.911199999999999, 6.9112, 170.5573041426782, 2268516.453965872, 2268516.453965872, 444474.3172446498], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4803000.0000, 
sim time next is 4803600.0000, 
raw observation next is [31.33333333333334, 65.33333333333334, 1.0, 2.0, 0.5387231118166721, 1.0, 2.0, 0.5387231118166721, 1.0, 2.0, 0.9355838867700609, 6.9112, 6.9112, 170.5573041426782, 2259975.542868635, 2259975.542868635, 442956.2919719276], 
processed observation next is [1.0, 0.6086956521739131, 0.6840442338072673, 0.6533333333333334, 1.0, 1.0, 0.44424471303213503, 1.0, 1.0, 0.44424471303213503, 1.0, 1.0, 0.9214437643537327, 0.0, 0.0, 0.8375144448122397, 0.6277709841301764, 0.6277709841301764, 0.6611287939879517], 
reward next is 0.3389, 
noisyNet noise sample is [array([-0.24749047], dtype=float32), -0.036503233]. 
=============================================
[2019-03-26 11:12:59,294] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9637633e-36 1.0000000e+00 0.0000000e+00 4.8227567e-29 3.1318484e-32], sum to 1.0000
[2019-03-26 11:12:59,299] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7958
[2019-03-26 11:12:59,304] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4917798306321272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687182.1911867269, 687182.1911867263, 182317.988432442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4825200.0000, 
sim time next is 4825800.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4921104465037565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687644.3225295888, 687644.3225295888, 182368.9809710474], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.74, 1.0, 1.0, 0.38808487530573077, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19101231181377468, 0.19101231181377468, 0.272192508912011], 
reward next is 0.7278, 
noisyNet noise sample is [array([-1.5727756], dtype=float32), 0.2664072]. 
=============================================
[2019-03-26 11:13:00,076] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0267494e-37 1.0000000e+00 1.9967567e-38 2.0430566e-30 9.6485318e-26], sum to 1.0000
[2019-03-26 11:13:00,087] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9028
[2019-03-26 11:13:00,091] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4846459937440183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 677210.6389611713, 677210.6389611718, 181225.1533936819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4845000.0000, 
sim time next is 4845600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4848186778890622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677452.0127284338, 677452.0127284338, 181251.4184775893], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3792996119145327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18818111464678716, 0.18818111464678716, 0.2705245051904318], 
reward next is 0.7295, 
noisyNet noise sample is [array([-0.24830392], dtype=float32), -0.8147852]. 
=============================================
[2019-03-26 11:13:15,992] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.7391142e-37 6.7072916e-36], sum to 1.0000
[2019-03-26 11:13:16,000] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7366
[2019-03-26 11:13:16,005] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 68.66666666666667, 1.0, 2.0, 0.5190657316220347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725322.8092902369, 725322.8092902362, 186636.9319086387], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5128800.0000, 
sim time next is 5129400.0000, 
raw observation next is [29.83333333333333, 67.33333333333333, 1.0, 2.0, 0.5173825377020261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722969.9774023629, 722969.9774023634, 186364.0583182439], 
processed observation next is [0.0, 0.34782608695652173, 0.6129541864139019, 0.6733333333333333, 1.0, 1.0, 0.4185331779542483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20082499372287857, 0.20082499372287874, 0.2781553109227521], 
reward next is 0.7218, 
noisyNet noise sample is [array([-0.41235974], dtype=float32), 0.0460566]. 
=============================================
[2019-03-26 11:13:18,253] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 11:13:18,256] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:13:18,257] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:13:18,257] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:13:18,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:13:18,258] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:13:18,258] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:13:18,261] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:13:18,259] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:13:18,262] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:13:18,265] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:13:18,275] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run32
[2019-03-26 11:13:18,298] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run32
[2019-03-26 11:13:18,299] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run32
[2019-03-26 11:13:18,319] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run32
[2019-03-26 11:13:18,360] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run32
[2019-03-26 11:13:20,517] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.18834898], dtype=float32), 0.30876464]
[2019-03-26 11:13:20,518] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.03333333333333, 77.33333333333334, 1.0, 2.0, 0.4379576091629312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 636085.0009282514, 636085.0009282514, 177532.737330833]
[2019-03-26 11:13:20,519] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:13:20,524] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6776216343985474
[2019-03-26 11:13:34,494] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.18834898], dtype=float32), 0.30876464]
[2019-03-26 11:13:34,496] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.7, 84.16666666666666, 1.0, 2.0, 0.2981111564233166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476756.1514618054, 476756.1514618054, 165417.1690421098]
[2019-03-26 11:13:34,497] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:13:34,500] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.030516921150406495
[2019-03-26 11:13:46,630] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.18834898], dtype=float32), 0.30876464]
[2019-03-26 11:13:46,630] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.93333333333334, 90.0, 1.0, 2.0, 0.6556735457417032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 997157.5281622157, 997157.5281622151, 221699.211939199]
[2019-03-26 11:13:46,631] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:13:46,634] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.4086725e-36 2.1192376e-38], sampled 0.4913240883944313
[2019-03-26 11:13:55,864] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18834898], dtype=float32), 0.30876464]
[2019-03-26 11:13:55,865] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.19936849666667, 96.57196537166666, 1.0, 2.0, 0.5260596293400593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735099.1988819549, 735099.1988819549, 187778.5784766208]
[2019-03-26 11:13:55,866] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:13:55,870] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.0537556e-35 3.9761788e-38], sampled 0.6148387902616888
[2019-03-26 11:14:18,744] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.18834898], dtype=float32), 0.30876464]
[2019-03-26 11:14:18,746] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.77053130500001, 77.83337469833334, 1.0, 2.0, 0.983134284266118, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005992457492084, 6.9112, 168.9123159307412, 2271395.620469781, 2204146.843653213, 458198.4744058904]
[2019-03-26 11:14:18,748] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:14:18,752] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0913821e-29 1.0000000e+00 6.3928349e-32 4.1097164e-28 5.1180881e-17], sampled 0.9978304094210364
[2019-03-26 11:14:18,753] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2271395.620469781 W.
[2019-03-26 11:14:19,267] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.18834898], dtype=float32), 0.30876464]
[2019-03-26 11:14:19,269] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.43333333333333, 57.66666666666666, 1.0, 2.0, 0.9726468304513298, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991895891498, 6.9112, 168.9123159326032, 2256716.889340518, 2189468.510940482, 455017.0998580169]
[2019-03-26 11:14:19,270] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:14:19,273] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1584298e-28 1.0000000e+00 5.3579081e-30 1.9329886e-26 1.6551237e-15], sampled 0.5487720777108892
[2019-03-26 11:14:19,274] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2256716.889340518 W.
[2019-03-26 11:14:49,448] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.18834898], dtype=float32), 0.30876464]
[2019-03-26 11:14:49,448] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.74600040666667, 85.33365031666668, 1.0, 2.0, 0.5644542340418484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788770.5334446575, 788770.5334446569, 194309.1224253619]
[2019-03-26 11:14:49,450] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:14:49,452] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5116598e-37 0.0000000e+00], sampled 0.8153838834299337
[2019-03-26 11:14:51,163] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18834898], dtype=float32), 0.30876464]
[2019-03-26 11:14:51,164] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.15, 73.66666666666667, 1.0, 2.0, 0.5251241461963687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733791.5326141464, 733791.5326141464, 187626.3691541659]
[2019-03-26 11:14:51,164] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:14:51,166] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.35108134998129237
[2019-03-26 11:15:08,382] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18834898], dtype=float32), 0.30876464]
[2019-03-26 11:15:08,383] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.3, 92.0, 1.0, 2.0, 0.394226100906987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586920.5135751214, 586920.513575122, 173294.9515662149]
[2019-03-26 11:15:08,384] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:15:08,388] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5709940581988413
[2019-03-26 11:15:14,342] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.0951 2842779202.8145 1134.0000
[2019-03-26 11:15:14,352] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1367 3164188877.1900 1793.0000
[2019-03-26 11:15:14,364] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8250.0363 2927726025.7856 1345.0000
[2019-03-26 11:15:14,553] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8656.7466 2779648021.3986 935.0000
[2019-03-26 11:15:14,707] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.3563 3007937057.5632 1766.0000
[2019-03-26 11:15:15,721] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 775000, evaluation results [775000.0, 7884.136740724249, 3164188877.1900196, 1793.0, 8250.036279343318, 2927726025.7856336, 1345.0, 8656.746609681193, 2779648021.398621, 935.0, 7995.356252368326, 3007937057.5631742, 1766.0, 8493.095054864645, 2842779202.8144827, 1134.0]
[2019-03-26 11:15:15,884] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:15:15,891] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8444
[2019-03-26 11:15:15,895] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5226573742461251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730343.3601159235, 730343.3601159242, 187221.8609097044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5170200.0000, 
sim time next is 5170800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.522900971675224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730683.8718226827, 730683.8718226827, 187261.656465971], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4251818935846072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2029677421729674, 0.2029677421729674, 0.279495009650703], 
reward next is 0.7205, 
noisyNet noise sample is [array([1.2379528], dtype=float32), -0.23674935]. 
=============================================
[2019-03-26 11:15:20,242] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.19559880e-13 1.51975375e-08 1.32232456e-17 1.94249896e-14
 1.00000000e+00], sum to 1.0000
[2019-03-26 11:15:20,251] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9575
[2019-03-26 11:15:20,258] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333333, 69.0, 1.0, 2.0, 0.6319099411791593, 1.0, 2.0, 0.6319099411791593, 1.0, 2.0, 1.03, 6.986992765877686, 6.9112, 170.5573041426782, 2651314.573785921, 2597021.132535137, 499838.4250038426], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5244000.0000, 
sim time next is 5244600.0000, 
raw observation next is [31.16666666666667, 69.5, 1.0, 2.0, 0.6021220675659845, 1.0, 2.0, 0.6021220675659845, 1.0, 2.0, 1.03, 6.928834520936862, 6.9112, 170.5573041426782, 2526206.770019764, 2513574.445346241, 488748.8475243466], 
processed observation next is [1.0, 0.6956521739130435, 0.6761453396524489, 0.695, 1.0, 1.0, 0.5206289970674512, 1.0, 1.0, 0.5206289970674512, 1.0, 1.0, 1.0365853658536586, 0.0017634520936861797, 0.0, 0.8375144448122397, 0.7017241027832678, 0.6982151237072891, 0.729475891827383], 
reward next is 0.1824, 
noisyNet noise sample is [array([0.18039781], dtype=float32), 2.1514525]. 
=============================================
[2019-03-26 11:15:22,917] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.370549e-36 1.000000e+00 0.000000e+00 7.990817e-37 7.404172e-32], sum to 1.0000
[2019-03-26 11:15:22,923] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7683
[2019-03-26 11:15:22,929] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 87.5, 1.0, 2.0, 1.005528955076386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564787521, 1405537.977825804, 1405537.977825805, 300618.8163392667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5283000.0000, 
sim time next is 5283600.0000, 
raw observation next is [28.6, 87.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.961236593925388, 6.9112, 168.9125063524442, 1489276.824737481, 1453779.237482518, 311354.2018498838], 
processed observation next is [1.0, 0.13043478260869565, 0.5545023696682465, 0.8766666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.00500365939253884, 0.0, 0.8294377346705043, 0.4136880068715225, 0.4038275659673661, 0.4647077639550504], 
reward next is 0.2851, 
noisyNet noise sample is [array([-0.6052051], dtype=float32), -1.045522]. 
=============================================
[2019-03-26 11:15:23,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7355251e-26 1.0000000e+00 2.5771542e-30 4.9721269e-22 1.3293192e-17], sum to 1.0000
[2019-03-26 11:15:23,172] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3700
[2019-03-26 11:15:23,176] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.9495685690163033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1327267.117137036, 1327267.117137035, 283940.5516541216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5292000.0000, 
sim time next is 5292600.0000, 
raw observation next is [28.78333333333333, 87.33333333333334, 1.0, 2.0, 1.036322603862964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9128935384609, 1448611.003966653, 1448611.003966652, 310192.5487914634], 
processed observation next is [1.0, 0.2608695652173913, 0.5631911532385465, 0.8733333333333334, 1.0, 1.0, 1.0437621733288724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294396359311008, 0.4023919455462925, 0.40239194554629226, 0.4629739534200947], 
reward next is 0.5370, 
noisyNet noise sample is [array([-0.05823726], dtype=float32), 0.5437335]. 
=============================================
[2019-03-26 11:15:30,152] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3228264e-19 1.8360115e-05 1.8299753e-22 1.5607996e-16 9.9998164e-01], sum to 1.0000
[2019-03-26 11:15:30,161] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6347
[2019-03-26 11:15:30,165] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 68.16666666666667, 1.0, 2.0, 0.6100057688494053, 1.0, 2.0, 0.6100057688494053, 1.0, 2.0, 1.03, 6.94422619922296, 6.9112, 170.5573041426782, 2559316.761528086, 2535658.751604788, 491635.4084450895], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6005400.0000, 
sim time next is 6006000.0000, 
raw observation next is [31.0, 69.33333333333334, 1.0, 2.0, 0.5004928690939008, 1.0, 2.0, 0.5004928690939008, 1.0, 2.0, 0.8691905980950708, 6.9112, 6.9112, 170.5573041426782, 2099446.853734542, 2099446.853734542, 415486.9063965866], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.6933333333333335, 1.0, 1.0, 0.3981841796312057, 1.0, 1.0, 0.3981841796312057, 1.0, 1.0, 0.8404763391403302, 0.0, 0.0, 0.8375144448122397, 0.5831796815929283, 0.5831796815929283, 0.6201297110396815], 
reward next is 0.3799, 
noisyNet noise sample is [array([0.37311825], dtype=float32), 0.60342634]. 
=============================================
[2019-03-26 11:15:30,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[36.137196]
 [36.04527 ]
 [35.354416]
 [34.864323]
 [34.606068]], R is [[38.51910782]
 [38.23500443]
 [38.1310463 ]
 [37.84783936]
 [37.46936035]].
[2019-03-26 11:15:32,720] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3169536e-38 1.0000000e+00 0.0000000e+00 1.5340364e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 11:15:32,734] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6781
[2019-03-26 11:15:32,740] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 85.0, 1.0, 2.0, 0.5875141679837166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 821007.0283939315, 821007.0283939309, 198443.1342128485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5443200.0000, 
sim time next is 5443800.0000, 
raw observation next is [29.0, 85.66666666666667, 1.0, 2.0, 0.5906428527188955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825380.8296423217, 825380.8296423217, 199015.6882775268], 
processed observation next is [1.0, 0.0, 0.5734597156398105, 0.8566666666666667, 1.0, 1.0, 0.506798617733609, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2292724526784227, 0.2292724526784227, 0.29703834071272656], 
reward next is 0.7030, 
noisyNet noise sample is [array([-0.85853195], dtype=float32), -1.1355654]. 
=============================================
[2019-03-26 11:15:35,401] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.06648496e-32 1.00000000e+00 0.00000000e+00 8.46795651e-27
 1.65986966e-33], sum to 1.0000
[2019-03-26 11:15:35,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7020
[2019-03-26 11:15:35,423] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.2, 59.0, 1.0, 2.0, 0.5421744223301029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757625.53488525, 757625.5348852506, 190469.7265457408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5508000.0000, 
sim time next is 5508600.0000, 
raw observation next is [32.93333333333334, 60.5, 1.0, 2.0, 0.5506815927695967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769517.6196698534, 769517.6196698528, 191919.2281587855], 
processed observation next is [1.0, 0.782608695652174, 0.7598736176935231, 0.605, 1.0, 1.0, 0.45865252140915264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21375489435273703, 0.2137548943527369, 0.28644660919221715], 
reward next is 0.7136, 
noisyNet noise sample is [array([-0.08707894], dtype=float32), 0.6902033]. 
=============================================
[2019-03-26 11:15:41,266] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8363291e-31 1.0000000e+00 0.0000000e+00 6.2742742e-27 2.2372962e-21], sum to 1.0000
[2019-03-26 11:15:41,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3941
[2019-03-26 11:15:41,279] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 90.0, 1.0, 2.0, 0.5464752681517986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763637.6257394119, 763637.6257394119, 191197.798214859], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5609400.0000, 
sim time next is 5610000.0000, 
raw observation next is [26.93333333333334, 90.0, 1.0, 2.0, 0.5440535270929651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760252.3043683963, 760252.3043683963, 190785.8175681132], 
processed observation next is [1.0, 0.9565217391304348, 0.4755134281200636, 0.9, 1.0, 1.0, 0.45066690011200605, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21118119565788787, 0.21118119565788787, 0.28475495159419884], 
reward next is 0.7152, 
noisyNet noise sample is [array([-1.259901], dtype=float32), -1.4281559]. 
=============================================
[2019-03-26 11:15:41,295] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.95358]
 [78.97431]
 [78.61775]
 [78.64688]
 [78.73763]], R is [[78.8087616 ]
 [78.73530579]
 [78.66188049]
 [78.58843994]
 [78.5151825 ]].
[2019-03-26 11:15:47,060] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.7160337e-29 1.0000000e+00 3.4585946e-33 2.3962796e-21 4.7108318e-25], sum to 1.0000
[2019-03-26 11:15:47,067] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9636
[2019-03-26 11:15:47,073] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 86.33333333333334, 1.0, 2.0, 0.5247479654636504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733265.6883661287, 733265.6883661287, 187563.9209040839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5696400.0000, 
sim time next is 5697000.0000, 
raw observation next is [26.8, 86.5, 1.0, 2.0, 0.52292946623043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730723.7028315716, 730723.7028315709, 187266.1559191874], 
processed observation next is [0.0, 0.9565217391304348, 0.4691943127962086, 0.865, 1.0, 1.0, 0.425216224374012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20297880634210322, 0.20297880634210302, 0.27950172525251854], 
reward next is 0.7205, 
noisyNet noise sample is [array([1.2946202], dtype=float32), 0.11757672]. 
=============================================
[2019-03-26 11:15:47,100] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.51505 ]
 [68.54826 ]
 [68.591286]
 [68.540054]
 [68.594345]], R is [[68.52157593]
 [68.55641174]
 [68.59030151]
 [68.62382507]
 [68.65634155]].
[2019-03-26 11:15:48,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.131187e-35 3.993589e-28], sum to 1.0000
[2019-03-26 11:15:48,337] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4595
[2019-03-26 11:15:48,344] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 90.33333333333333, 1.0, 2.0, 0.5101268170908337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712827.7173717354, 712827.7173717354, 185197.4327948444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5717400.0000, 
sim time next is 5718000.0000, 
raw observation next is [25.83333333333334, 90.66666666666667, 1.0, 2.0, 0.5101049899327499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712797.2068732105, 712797.2068732111, 185193.977232375], 
processed observation next is [0.0, 0.17391304347826086, 0.42338072669826254, 0.9066666666666667, 1.0, 1.0, 0.4097650481117468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19799922413144735, 0.19799922413144752, 0.2764089212423508], 
reward next is 0.7236, 
noisyNet noise sample is [array([-1.0933948], dtype=float32), 0.4591858]. 
=============================================
[2019-03-26 11:15:48,361] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.0431  ]
 [76.99297 ]
 [76.8652  ]
 [76.895454]
 [76.92299 ]], R is [[76.89118195]
 [76.84585571]
 [76.80103302]
 [76.75623322]
 [76.7116394 ]].
[2019-03-26 11:15:59,072] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3513130e-28 2.2175658e-14 2.0125138e-34 4.8839881e-19 1.0000000e+00], sum to 1.0000
[2019-03-26 11:15:59,081] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8348
[2019-03-26 11:15:59,086] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.9, 72.5, 1.0, 2.0, 0.5018408172981422, 1.0, 2.0, 0.5018408172981422, 1.0, 2.0, 0.8715315383525554, 6.9112, 6.9112, 170.5573041426782, 2105106.727117074, 2105106.727117074, 416421.6614340129], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5920200.0000, 
sim time next is 5920800.0000, 
raw observation next is [30.76666666666667, 72.66666666666667, 1.0, 2.0, 0.5008611469403116, 1.0, 2.0, 0.5008611469403116, 1.0, 2.0, 0.8698301749229416, 6.911200000000001, 6.9112, 170.5573041426782, 2100993.205420461, 2100993.205420461, 415742.2549095104], 
processed observation next is [1.0, 0.5217391304347826, 0.6571879936808849, 0.7266666666666667, 1.0, 1.0, 0.39862788787989345, 1.0, 1.0, 0.39862788787989345, 1.0, 1.0, 0.8412563108816361, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5836092237279058, 0.5836092237279058, 0.6205108282231498], 
reward next is 0.3795, 
noisyNet noise sample is [array([1.2523686], dtype=float32), 0.8426304]. 
=============================================
[2019-03-26 11:16:00,679] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1932051e-23 1.0000000e+00 2.4221763e-30 7.5697315e-13 2.3770723e-11], sum to 1.0000
[2019-03-26 11:16:00,689] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3171
[2019-03-26 11:16:00,693] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.9, 79.83333333333334, 1.0, 2.0, 0.5642786092716867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788525.0236151748, 788525.0236151741, 194281.6827352617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5939400.0000, 
sim time next is 5940000.0000, 
raw observation next is [29.8, 80.0, 1.0, 2.0, 0.5675700002411044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793126.1427609666, 793126.1427609659, 194861.2474464809], 
processed observation next is [1.0, 0.782608695652174, 0.6113744075829385, 0.8, 1.0, 1.0, 0.47900000029048717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22031281743360182, 0.22031281743360162, 0.29083768275594163], 
reward next is 0.7092, 
noisyNet noise sample is [array([-0.1817537], dtype=float32), 0.4076263]. 
=============================================
[2019-03-26 11:16:00,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[53.823174]
 [54.160072]
 [56.225597]
 [56.396927]
 [55.560207]], R is [[53.90328598]
 [54.07427979]
 [54.24445724]
 [54.30533218]
 [54.3649292 ]].
[2019-03-26 11:16:01,549] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.5970352e-21 2.3612541e-13 1.2841204e-27 6.0815714e-12 1.0000000e+00], sum to 1.0000
[2019-03-26 11:16:01,557] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4311
[2019-03-26 11:16:01,561] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.6, 89.0, 1.0, 2.0, 0.1856387988602783, 1.0, 2.0, 0.1856387988602783, 1.0, 2.0, 0.3211749036174797, 6.911199999999999, 6.9112, 170.5573041426782, 778230.5228244116, 778230.5228244122, 265958.2694731264], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5954400.0000, 
sim time next is 5955000.0000, 
raw observation next is [27.53333333333334, 89.5, 1.0, 2.0, 0.1857550951688725, 1.0, 2.0, 0.1857550951688725, 1.0, 2.0, 0.3213856529210418, 6.9112, 6.9112, 170.5573041426782, 778718.2344096614, 778718.2344096614, 265988.4639049313], 
processed observation next is [1.0, 0.9565217391304348, 0.5039494470774094, 0.895, 1.0, 1.0, 0.01898204237213554, 1.0, 1.0, 0.01898204237213554, 1.0, 1.0, 0.17242152795249002, 0.0, 0.0, 0.8375144448122397, 0.2163106206693504, 0.2163106206693504, 0.39699770732079304], 
reward next is 0.6030, 
noisyNet noise sample is [array([0.6511653], dtype=float32), -1.3054955]. 
=============================================
[2019-03-26 11:16:01,575] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[53.65361 ]
 [53.42813 ]
 [53.59436 ]
 [53.511574]
 [55.01697 ]], R is [[53.80967331]
 [53.87462234]
 [53.93886948]
 [54.00235748]
 [54.06494904]].
[2019-03-26 11:16:07,431] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2307800e-23 1.0000000e+00 1.4794384e-30 5.2742733e-13 6.0325264e-13], sum to 1.0000
[2019-03-26 11:16:07,441] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4690
[2019-03-26 11:16:07,445] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 86.0, 1.0, 2.0, 0.5259628202564145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 734963.8741101392, 734963.8741101385, 187764.0679277733], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6123000.0000, 
sim time next is 6123600.0000, 
raw observation next is [27.2, 86.0, 1.0, 2.0, 0.5269159403329874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736296.1956722493, 736296.1956722493, 187920.8085724144], 
processed observation next is [1.0, 0.9130434782608695, 0.4881516587677725, 0.86, 1.0, 1.0, 0.4300192052204667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20452672102006925, 0.20452672102006925, 0.2804788187647976], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.3719109], dtype=float32), -1.6829575]. 
=============================================
[2019-03-26 11:16:08,932] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0232334e-24 8.6451324e-10 1.5209009e-32 1.8663198e-18 1.0000000e+00], sum to 1.0000
[2019-03-26 11:16:08,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2857
[2019-03-26 11:16:08,945] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.23333333333333, 92.33333333333333, 1.0, 2.0, 0.2363870595528657, 1.0, 2.0, 0.2363870595528657, 1.0, 2.0, 0.4037887604580198, 6.9112, 6.9112, 170.5573041426782, 991074.4681313266, 991074.4681313266, 280355.4457076637], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6066600.0000, 
sim time next is 6067200.0000, 
raw observation next is [26.36666666666667, 91.66666666666667, 1.0, 2.0, 0.2255686082351476, 1.0, 2.0, 0.2255686082351476, 1.0, 2.0, 0.3855223331940292, 6.911200000000001, 6.9112, 170.5573041426782, 945697.1166899157, 945697.116689915, 276965.5636378792], 
processed observation next is [1.0, 0.21739130434782608, 0.4486571879936811, 0.9166666666666667, 1.0, 1.0, 0.06695013040379227, 1.0, 1.0, 0.06695013040379227, 1.0, 1.0, 0.25063699170003556, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2626936435249766, 0.2626936435249764, 0.41338143826549134], 
reward next is 0.5866, 
noisyNet noise sample is [array([-1.0477635], dtype=float32), -0.9990655]. 
=============================================
[2019-03-26 11:16:09,639] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.1688373e-28 1.6254784e-14 1.7146832e-34 3.1990625e-21 1.0000000e+00], sum to 1.0000
[2019-03-26 11:16:09,652] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2810
[2019-03-26 11:16:09,661] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.03333333333333, 65.0, 1.0, 2.0, 0.3825194408280259, 1.0, 2.0, 0.3825194408280259, 1.0, 2.0, 0.6597916071704234, 6.9112, 6.9112, 170.5573041426782, 1604206.317975298, 1604206.317975298, 342824.4583171206], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6092400.0000, 
sim time next is 6093000.0000, 
raw observation next is [31.05, 65.0, 1.0, 2.0, 0.366094958305419, 1.0, 2.0, 0.366094958305419, 1.0, 2.0, 0.6310276168734376, 6.9112, 6.9112, 170.5573041426782, 1535276.146656827, 1535276.146656827, 334316.0512225088], 
processed observation next is [1.0, 0.5217391304347826, 0.6706161137440759, 0.65, 1.0, 1.0, 0.23625898591014335, 1.0, 1.0, 0.23625898591014335, 1.0, 1.0, 0.5500336791139482, 0.0, 0.0, 0.8375144448122397, 0.42646559629356307, 0.42646559629356307, 0.4989791809291176], 
reward next is 0.5010, 
noisyNet noise sample is [array([-0.9061882], dtype=float32), -0.3015926]. 
=============================================
[2019-03-26 11:16:09,687] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[71.35486 ]
 [70.59656 ]
 [69.578964]
 [69.48331 ]
 [69.07437 ]], R is [[71.16907501]
 [70.9457016 ]
 [70.65197754]
 [70.25980377]
 [69.8973465 ]].
[2019-03-26 11:16:09,785] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 11:16:09,788] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:16:09,789] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:16:09,789] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:16:09,791] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:16:09,790] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:16:09,794] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:16:09,792] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:16:09,795] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:16:09,800] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:16:09,798] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:16:09,816] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run33
[2019-03-26 11:16:09,817] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run33
[2019-03-26 11:16:09,861] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run33
[2019-03-26 11:16:09,881] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run33
[2019-03-26 11:16:09,881] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run33
[2019-03-26 11:16:19,015] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20964326], dtype=float32), 0.31817743]
[2019-03-26 11:16:19,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.86213185666667, 67.48959209, 1.0, 2.0, 0.2664333489999496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 436708.0439799879, 436708.0439799879, 162572.8508025005]
[2019-03-26 11:16:19,017] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:16:19,022] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.0113816e-26 1.0000000e+00 3.0346482e-31 2.8927476e-14 3.1247132e-09], sampled 0.7607318513600235
[2019-03-26 11:16:23,065] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.20964326], dtype=float32), 0.31817743]
[2019-03-26 11:16:23,067] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.5515679, 89.38290481333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 171.5212843490159, 437301.8225199922, 437301.8225199915, 218941.4822721897]
[2019-03-26 11:16:23,068] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:16:23,070] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.7333494e-25 4.3338078e-10 1.0945100e-27 4.6314276e-12 1.0000000e+00], sampled 0.5605838306829732
[2019-03-26 11:16:28,289] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.20964326], dtype=float32), 0.31817743]
[2019-03-26 11:16:28,291] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.08063868, 93.27073522666667, 1.0, 2.0, 0.3653341531702748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561075.7555225295, 561075.7555225302, 171536.7810388262]
[2019-03-26 11:16:28,292] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:16:28,295] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.5937362e-22 9.9955088e-01 1.8041996e-27 1.8265679e-06 4.4731738e-04], sampled 0.25396718750323743
[2019-03-26 11:16:31,817] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.20964326], dtype=float32), 0.31817743]
[2019-03-26 11:16:31,818] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.1, 48.0, 1.0, 2.0, 0.2854998437869897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461540.0882468025, 461540.0882468025, 164372.2947905182]
[2019-03-26 11:16:31,819] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:16:31,824] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.3151674e-27 1.0000000e+00 5.8956688e-33 7.2955402e-14 1.7090341e-12], sampled 0.023844758340229033
[2019-03-26 11:16:54,759] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.20964326], dtype=float32), 0.31817743]
[2019-03-26 11:16:54,764] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.2, 89.0, 1.0, 2.0, 0.1719479739870655, 1.0, 2.0, 0.1719479739870655, 1.0, 2.0, 0.2912596001424463, 6.9112, 6.9112, 169.0403247858759, 720819.0626247621, 720819.0626247621, 262040.5372097764]
[2019-03-26 11:16:54,765] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:16:54,768] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.5213332e-23 1.3063705e-08 3.7357172e-28 9.2817356e-07 9.9999905e-01], sampled 0.5516494410187961
[2019-03-26 11:16:57,648] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20964326], dtype=float32), 0.31817743]
[2019-03-26 11:16:57,650] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.85, 69.83333333333334, 1.0, 2.0, 0.1896629664647558, 1.0, 2.0, 0.1896629664647558, 1.0, 2.0, 0.3288124689259871, 6.9112, 6.9112, 178.6582176852504, 795093.431748263, 795093.431748263, 269036.8121781676]
[2019-03-26 11:16:57,653] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:16:57,656] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.1061377e-22 1.6851112e-06 4.3786553e-27 1.6306135e-12 9.9999833e-01], sampled 0.5539693348266892
[2019-03-26 11:17:06,775] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.20964326], dtype=float32), 0.31817743]
[2019-03-26 11:17:06,776] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.5, 65.0, 1.0, 2.0, 0.6095542188354509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 851818.6984207645, 851818.698420764, 202540.3727720164]
[2019-03-26 11:17:06,777] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:17:06,781] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8486533e-24 9.9999821e-01 1.8994975e-28 1.0442111e-13 1.7605144e-06], sampled 0.4273037524294895
[2019-03-26 11:17:39,058] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.20964326], dtype=float32), 0.31817743]
[2019-03-26 11:17:39,061] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.85371047333333, 89.54565536000001, 1.0, 2.0, 0.1899266136169042, 1.0, 2.0, 0.1899266136169042, 1.0, 2.0, 0.3262751744950682, 6.9112, 6.9112, 171.5212843490159, 796210.8082150792, 796210.8082150792, 267154.0222218491]
[2019-03-26 11:17:39,063] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:17:39,068] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3642461e-22 3.2544484e-11 4.5091894e-28 9.9771651e-06 9.9998999e-01], sampled 0.43303291646918696
[2019-03-26 11:18:05,275] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7593.0407 3137020200.6258 127.0000
[2019-03-26 11:18:05,494] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7631.0935 3110927380.9669 86.0000
[2019-03-26 11:18:05,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7787.3714 3268712319.2044 37.0000
[2019-03-26 11:18:05,727] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7837.0086 3213001408.9167 118.0000
[2019-03-26 11:18:05,760] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7117.1496 3448243365.0169 596.0000
[2019-03-26 11:18:06,777] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 800000, evaluation results [800000.0, 7117.149553775527, 3448243365.0169263, 596.0, 7837.008561610252, 3213001408.916714, 118.0, 7631.093538898424, 3110927380.9668593, 86.0, 7787.371426115177, 3268712319.2043796, 37.0, 7593.040690708127, 3137020200.625804, 127.0]
[2019-03-26 11:18:23,483] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6501498e-32 1.0000000e+00 1.8994921e-37 8.6898103e-23 1.6646621e-27], sum to 1.0000
[2019-03-26 11:18:23,495] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5747
[2019-03-26 11:18:23,504] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 82.0, 1.0, 2.0, 0.5247844716849677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733316.7185772388, 733316.7185772388, 187569.9571318318], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6382200.0000, 
sim time next is 6382800.0000, 
raw observation next is [27.5, 82.0, 1.0, 2.0, 0.5246813269307252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733172.5377181085, 733172.5377181091, 187552.9600008283], 
processed observation next is [0.0, 0.9130434782608695, 0.5023696682464456, 0.82, 1.0, 1.0, 0.4273268999165364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20365903825503015, 0.2036590382550303, 0.27992979104601234], 
reward next is 0.7201, 
noisyNet noise sample is [array([0.11061735], dtype=float32), 1.9544364]. 
=============================================
[2019-03-26 11:18:32,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4358284e-38 1.0000000e+00 0.0000000e+00 2.9891118e-34 1.7494136e-37], sum to 1.0000
[2019-03-26 11:18:32,471] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9898
[2019-03-26 11:18:32,476] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 78.33333333333334, 1.0, 2.0, 0.4979545386699867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695813.1654802216, 695813.1654802216, 183276.4264311119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6556800.0000, 
sim time next is 6557400.0000, 
raw observation next is [27.6, 79.0, 1.0, 2.0, 0.5000774113909455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698780.5219603968, 698780.5219603961, 183608.4527207948], 
processed observation next is [1.0, 0.9130434782608695, 0.5071090047393366, 0.79, 1.0, 1.0, 0.39768362818186204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19410570054455467, 0.19410570054455448, 0.2740424667474549], 
reward next is 0.7260, 
noisyNet noise sample is [array([0.11916857], dtype=float32), 0.55089]. 
=============================================
[2019-03-26 11:18:33,488] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2120183e-31 1.0000000e+00 8.5254209e-35 5.1262765e-24 7.3338775e-22], sum to 1.0000
[2019-03-26 11:18:33,495] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7384
[2019-03-26 11:18:33,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 89.5, 1.0, 2.0, 0.5175758127689657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723240.1443091163, 723240.1443091156, 186395.4929988685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6571800.0000, 
sim time next is 6572400.0000, 
raw observation next is [26.3, 89.66666666666667, 1.0, 2.0, 0.5165169162891372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721759.9812020903, 721759.9812020903, 186224.2413698407], 
processed observation next is [1.0, 0.043478260869565216, 0.4454976303317536, 0.8966666666666667, 1.0, 1.0, 0.4174902605893219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2004888836672473, 0.2004888836672473, 0.27794662891021], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.3658564], dtype=float32), 0.68952006]. 
=============================================
[2019-03-26 11:18:36,282] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.6639003e-24 9.9999976e-01 3.1690095e-23 2.4680720e-17 2.8896196e-07], sum to 1.0000
[2019-03-26 11:18:36,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2684
[2019-03-26 11:18:36,298] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2309572.971465984 W.
[2019-03-26 11:18:36,304] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.25, 61.5, 1.0, 2.0, 0.8258025239599235, 1.0, 2.0, 0.8258025239599235, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2309572.971465984, 2309572.971465984, 432621.4029418429], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6618600.0000, 
sim time next is 6619200.0000, 
raw observation next is [31.16666666666666, 62.0, 1.0, 2.0, 1.010721174721362, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.981930360508081, 6.9112, 168.9125360831033, 2310008.351782145, 2259829.924581877, 467483.847190032], 
processed observation next is [1.0, 0.6086956521739131, 0.6761453396524484, 0.62, 1.0, 1.0, 1.0129170779775447, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007073036050808135, 0.0, 0.8294378806616513, 0.6416689866061513, 0.627730534606077, 0.6977370853582567], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.56210494], dtype=float32), 0.045495335]. 
=============================================
[2019-03-26 11:18:37,988] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0354346e-32 1.0000000e+00 8.5878300e-35 6.1746435e-20 1.2330851e-21], sum to 1.0000
[2019-03-26 11:18:37,997] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9551
[2019-03-26 11:18:38,002] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 89.33333333333334, 1.0, 2.0, 0.3710377579456827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 564517.6232246225, 564517.6232246218, 171683.0425478053], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7240800.0000, 
sim time next is 7241400.0000, 
raw observation next is [22.9, 90.0, 1.0, 2.0, 0.3708435981233257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 564455.160426847, 564455.1604268476, 171684.554639717], 
processed observation next is [1.0, 0.8260869565217391, 0.2843601895734597, 0.9, 1.0, 1.0, 0.24198023870280203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15679310011856862, 0.15679310011856878, 0.2562456039398761], 
reward next is 0.7438, 
noisyNet noise sample is [array([1.0055737], dtype=float32), -0.15130782]. 
=============================================
[2019-03-26 11:18:43,325] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.7855694e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 11:18:43,336] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4666
[2019-03-26 11:18:43,341] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 67.66666666666667, 1.0, 2.0, 0.4013037451231862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 596171.6056717106, 596171.60567171, 174108.487279308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6726000.0000, 
sim time next is 6726600.0000, 
raw observation next is [26.85, 68.0, 1.0, 2.0, 0.3985852743677827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593272.3027718812, 593272.3027718806, 173874.6926830718], 
processed observation next is [1.0, 0.8695652173913043, 0.4715639810426541, 0.68, 1.0, 1.0, 0.2754039450214249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16479786188107812, 0.16479786188107795, 0.25951446669115197], 
reward next is 0.7405, 
noisyNet noise sample is [array([-0.3307979], dtype=float32), 1.8141149]. 
=============================================
[2019-03-26 11:18:44,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:18:44,176] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9233
[2019-03-26 11:18:44,180] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 85.33333333333334, 1.0, 2.0, 0.3150565004258759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502268.2413615047, 502268.2413615054, 167255.5722339166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6754800.0000, 
sim time next is 6755400.0000, 
raw observation next is [21.65, 85.5, 1.0, 2.0, 0.3190473610214441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 508937.811772104, 508937.8117721034, 167758.3191546652], 
processed observation next is [1.0, 0.17391304347826086, 0.22511848341232227, 0.855, 1.0, 1.0, 0.17957513376077602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14137161438114, 0.14137161438113985, 0.25038555097711224], 
reward next is 0.7496, 
noisyNet noise sample is [array([1.6116841], dtype=float32), 0.025682664]. 
=============================================
[2019-03-26 11:18:45,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.771792e-37 2.676854e-37], sum to 1.0000
[2019-03-26 11:18:45,764] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6853
[2019-03-26 11:18:45,767] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 76.5, 1.0, 2.0, 0.3844108377844545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603003.0325276207, 603003.0325276207, 175421.6376671486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6767400.0000, 
sim time next is 6768000.0000, 
raw observation next is [23.8, 76.0, 1.0, 2.0, 0.3695941449649225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 578925.4032060098, 578925.4032060092, 173304.9593922844], 
processed observation next is [1.0, 0.34782608695652173, 0.3270142180094788, 0.76, 1.0, 1.0, 0.24047487345171384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16081261200166938, 0.16081261200166921, 0.25866411849594684], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.48169014], dtype=float32), -0.41315335]. 
=============================================
[2019-03-26 11:18:45,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[66.824486]
 [66.90975 ]
 [66.97001 ]
 [66.99529 ]
 [66.86327 ]], R is [[67.05649567]
 [67.12410736]
 [67.19290161]
 [67.26359558]
 [67.3359375 ]].
[2019-03-26 11:18:51,717] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:18:51,722] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6175
[2019-03-26 11:18:51,725] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.65, 33.5, 1.0, 2.0, 0.2608519516318573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427038.6891723659, 427038.6891723666, 161990.3035745148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6874200.0000, 
sim time next is 6874800.0000, 
raw observation next is [29.73333333333333, 32.33333333333334, 1.0, 2.0, 0.2546949908180917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 418049.3633110436, 418049.3633110442, 161377.184670443], 
processed observation next is [0.0, 0.5652173913043478, 0.6082148499210109, 0.3233333333333334, 1.0, 1.0, 0.10204215761215868, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11612482314195656, 0.11612482314195671, 0.24086146965737762], 
reward next is 0.7591, 
noisyNet noise sample is [array([1.488286], dtype=float32), -0.2296156]. 
=============================================
[2019-03-26 11:18:52,066] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.451809e-34 0.000000e+00], sum to 1.0000
[2019-03-26 11:18:52,074] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6054
[2019-03-26 11:18:52,078] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.46666666666667, 68.16666666666666, 1.0, 2.0, 0.3826662405196189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 576728.5518387468, 576728.5518387475, 172593.8361468513], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6900600.0000, 
sim time next is 6901200.0000, 
raw observation next is [26.4, 69.0, 1.0, 2.0, 0.3857923114257717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580118.9827022328, 580118.9827022334, 172857.2448070596], 
processed observation next is [0.0, 0.9130434782608695, 0.45023696682464454, 0.69, 1.0, 1.0, 0.25999073665755623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16114416186173133, 0.1611441618617315, 0.2579958877717307], 
reward next is 0.7420, 
noisyNet noise sample is [array([-0.14143112], dtype=float32), 1.2349285]. 
=============================================
[2019-03-26 11:18:52,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:18:52,393] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5280
[2019-03-26 11:18:52,398] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 51.5, 1.0, 2.0, 0.3473424583430342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535688.6631004182, 535688.6631004182, 169481.8156276213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6888600.0000, 
sim time next is 6889200.0000, 
raw observation next is [28.66666666666666, 52.33333333333334, 1.0, 2.0, 0.3546870426126819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546388.899086117, 546388.8990861165, 170345.2886972882], 
processed observation next is [0.0, 0.7391304347826086, 0.5576619273301735, 0.5233333333333334, 1.0, 1.0, 0.2225145091719059, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15177469419058806, 0.1517746941905879, 0.25424669954819135], 
reward next is 0.7458, 
noisyNet noise sample is [array([-1.063289], dtype=float32), -0.25483277]. 
=============================================
[2019-03-26 11:18:58,802] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3768603e-29 1.2249041e-36], sum to 1.0000
[2019-03-26 11:18:58,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8720
[2019-03-26 11:18:58,812] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 76.33333333333334, 1.0, 2.0, 0.4428623873691883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634940.0951931007, 634940.0951931007, 177204.2987184869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6995400.0000, 
sim time next is 6996000.0000, 
raw observation next is [26.5, 76.66666666666667, 1.0, 2.0, 0.4449989567746867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 637915.7614895086, 637915.7614895086, 177500.9528324651], 
processed observation next is [0.0, 1.0, 0.4549763033175356, 0.7666666666666667, 1.0, 1.0, 0.3313240443068514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1771988226359746, 0.1771988226359746, 0.264926795272336], 
reward next is 0.7351, 
noisyNet noise sample is [array([-0.52598006], dtype=float32), 1.0671338]. 
=============================================
[2019-03-26 11:18:58,825] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.566315]
 [72.59782 ]
 [72.557495]
 [72.61121 ]
 [72.65862 ]], R is [[72.52978516]
 [72.54000854]
 [72.54947662]
 [72.55921936]
 [72.56928253]].
[2019-03-26 11:19:00,738] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 11:19:00,740] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:19:00,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:19:00,742] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:19:00,743] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:19:00,743] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:19:00,744] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:19:00,745] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:19:00,744] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:19:00,747] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:19:00,750] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:19:00,771] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run34
[2019-03-26 11:19:00,793] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run34
[2019-03-26 11:19:00,794] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run34
[2019-03-26 11:19:00,835] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run34
[2019-03-26 11:19:00,855] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run34
[2019-03-26 11:19:08,543] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:19:08,545] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.88266884333333, 79.35055538, 1.0, 2.0, 0.2646839125213737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 428851.1264579431, 428851.1264579431, 162217.8008948792]
[2019-03-26 11:19:08,546] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:19:08,549] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.26237359809103133
[2019-03-26 11:19:30,368] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:19:30,369] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.32844412, 79.022441, 1.0, 2.0, 0.5449666461208099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 828434.7762621127, 828434.7762621127, 199096.0234249546]
[2019-03-26 11:19:30,371] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:19:30,374] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.568622e-35 1.000000e+00 0.000000e+00 2.800241e-29 5.027242e-31], sampled 0.5132599508860984
[2019-03-26 11:19:37,363] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:19:37,364] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.93333333333333, 88.66666666666666, 1.0, 2.0, 0.6764111457569538, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.967807008820607, 6.9112, 168.9125717980147, 1842120.151162486, 1801961.282307423, 380877.4232387817]
[2019-03-26 11:19:37,365] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:19:37,368] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6254119e-27 1.0000000e+00 5.2431219e-31 1.4909782e-24 1.3350376e-15], sampled 0.8484378934791019
[2019-03-26 11:19:37,370] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1842120.151162486 W.
[2019-03-26 11:20:20,732] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:20:20,733] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.5, 52.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.222663926501381, 6.9112, 168.9107140176748, 2504841.774834062, 2283881.479126455, 475410.5829351241]
[2019-03-26 11:20:20,734] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:20:20,736] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2269025e-26 1.0000000e+00 8.9549784e-33 9.4507415e-16 3.5524640e-18], sampled 0.43562484692761017
[2019-03-26 11:20:20,737] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2504841.774834062 W.
[2019-03-26 11:20:21,914] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:20:21,916] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.83333333333334, 48.66666666666667, 1.0, 2.0, 0.8835059964638295, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005986744257642, 6.9112, 168.912315971287, 2131949.716865145, 2064704.993182212, 429521.4463250756]
[2019-03-26 11:20:21,918] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:20:21,921] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4853117e-27 1.0000000e+00 1.6396181e-33 5.3691788e-16 4.1682142e-20], sampled 0.7797621330621535
[2019-03-26 11:20:21,922] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2131949.716865145 W.
[2019-03-26 11:20:28,065] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:20:28,068] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.849797885, 89.952002685, 1.0, 2.0, 0.5440626636663014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 760265.0762490887, 760265.076249088, 190787.122843581]
[2019-03-26 11:20:28,068] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:20:28,071] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4325701e-38 1.0000000e+00 0.0000000e+00 9.6452588e-33 8.1544198e-38], sampled 0.6824478929265919
[2019-03-26 11:20:29,811] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:20:29,811] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.0, 81.0, 1.0, 2.0, 0.5364243363444026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749587.625095453, 749587.6250954537, 189499.2974181165]
[2019-03-26 11:20:29,813] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:20:29,815] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.92150270e-35 1.00000000e+00 0.00000000e+00 1.06694155e-23
 1.06567974e-32], sampled 0.8854461991827969
[2019-03-26 11:20:39,892] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:20:39,894] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.712713615, 67.95109514333333, 1.0, 2.0, 0.2957434532645272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480388.9717275206, 480388.9717275206, 165638.6484416486]
[2019-03-26 11:20:39,896] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:20:39,899] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 2.68998e-35 0.00000e+00], sampled 0.05144467410098341
[2019-03-26 11:20:40,148] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:20:40,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.02503191, 53.88183853, 1.0, 2.0, 0.4096135650415689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634342.0136009615, 634342.0136009615, 178180.0604116928]
[2019-03-26 11:20:40,150] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:20:40,153] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.126459e-37 0.000000e+00], sampled 0.04575570581916033
[2019-03-26 11:20:46,518] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.15916146], dtype=float32), 0.30047804]
[2019-03-26 11:20:46,521] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.50510842, 90.62117427499999, 1.0, 2.0, 0.5536044506163609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773603.4828839964, 773603.4828839964, 192418.8641840251]
[2019-03-26 11:20:46,521] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:20:46,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.04273969e-38 1.00000000e+00 0.00000000e+00 5.18348985e-35
 1.04557805e-32], sampled 0.9444365813962134
[2019-03-26 11:20:57,024] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6106 2779767509.2774 931.0000
[2019-03-26 11:20:57,140] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8248.4853 2928180320.3784 1354.0000
[2019-03-26 11:20:57,143] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.9715 3164013320.7650 1814.0000
[2019-03-26 11:20:57,192] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.3172 2842616576.1284 1132.0000
[2019-03-26 11:20:57,259] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.2564 3007850226.0276 1761.0000
[2019-03-26 11:20:58,278] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 825000, evaluation results [825000.0, 7885.971501371444, 3164013320.765015, 1814.0, 8248.485304597327, 2928180320.3784337, 1354.0, 8660.61060147915, 2779767509.2773747, 931.0, 7996.256393149556, 3007850226.0275693, 1761.0, 8494.31715451884, 2842616576.1284227, 1132.0]
[2019-03-26 11:20:58,831] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7494076e-14 7.5416747e-03 1.1074866e-18 2.5399387e-12 9.9245828e-01], sum to 1.0000
[2019-03-26 11:20:58,838] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8799
[2019-03-26 11:20:58,846] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.2, 50.33333333333334, 1.0, 2.0, 0.4782923931166018, 1.0, 2.0, 0.4782923931166018, 1.0, 2.0, 0.7975827824795172, 6.911199999999999, 6.9112, 170.5573041426782, 2006234.01060523, 2006234.01060523, 394583.3339638977], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7042200.0000, 
sim time next is 7042800.0000, 
raw observation next is [31.3, 49.66666666666667, 1.0, 2.0, 0.3743050309214748, 1.0, 2.0, 0.3743050309214748, 1.0, 2.0, 0.6229189667969407, 6.9112, 6.9112, 170.5573041426782, 1569731.5767959, 1569731.5767959, 335469.067810659], 
processed observation next is [1.0, 0.5217391304347826, 0.6824644549763034, 0.4966666666666667, 1.0, 1.0, 0.2461506396644275, 1.0, 1.0, 0.2461506396644275, 1.0, 1.0, 0.5401450814596838, 0.0, 0.0, 0.8375144448122397, 0.43603654910997225, 0.43603654910997225, 0.5007001012099388], 
reward next is 0.4993, 
noisyNet noise sample is [array([1.0928379], dtype=float32), 0.1622853]. 
=============================================
[2019-03-26 11:21:08,260] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.9087156e-37 1.2480791e-33], sum to 1.0000
[2019-03-26 11:21:08,270] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9029
[2019-03-26 11:21:08,275] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 88.66666666666667, 1.0, 2.0, 0.3514389348955866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 554537.0874032846, 554537.0874032853, 171297.7065912789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7279800.0000, 
sim time next is 7280400.0000, 
raw observation next is [21.8, 88.33333333333334, 1.0, 2.0, 0.3245502997702647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 512005.7534928402, 512005.7534928408, 167911.230375331], 
processed observation next is [1.0, 0.2608695652173913, 0.23222748815165886, 0.8833333333333334, 1.0, 1.0, 0.18620518044610207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14222382041467785, 0.142223820414678, 0.2506137766795985], 
reward next is 0.7494, 
noisyNet noise sample is [array([1.257721], dtype=float32), 0.4189015]. 
=============================================
[2019-03-26 11:21:13,344] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.20278666e-36 1.00000000e+00 0.00000000e+00 1.21725805e-32
 8.94237546e-27], sum to 1.0000
[2019-03-26 11:21:13,358] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9985
[2019-03-26 11:21:13,363] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 92.5, 1.0, 2.0, 0.4017426776871507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594625.5350755848, 594625.5350755848, 173898.9743302296], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7525800.0000, 
sim time next is 7526400.0000, 
raw observation next is [23.36666666666667, 92.33333333333333, 1.0, 2.0, 0.3984910285097117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590974.3716383805, 590974.3716383805, 173596.6020012604], 
processed observation next is [0.0, 0.08695652173913043, 0.30647709320695127, 0.9233333333333333, 1.0, 1.0, 0.27529039579483333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1641595476773279, 0.1641595476773279, 0.25909940597203046], 
reward next is 0.7409, 
noisyNet noise sample is [array([-3.7314746], dtype=float32), -0.114796765]. 
=============================================
[2019-03-26 11:21:15,175] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6958174e-33 1.0000000e+00 0.0000000e+00 8.9980834e-27 8.5201999e-17], sum to 1.0000
[2019-03-26 11:21:15,186] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8030
[2019-03-26 11:21:15,190] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.78333333333333, 76.33333333333333, 1.0, 2.0, 0.3702879380423064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563862.3652962283, 563862.3652962289, 171640.7311685271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7343400.0000, 
sim time next is 7344000.0000, 
raw observation next is [24.8, 76.0, 1.0, 2.0, 0.3686010774412307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561849.5578281358, 561849.5578281365, 171483.4946955817], 
processed observation next is [1.0, 0.0, 0.3744075829383887, 0.76, 1.0, 1.0, 0.23927840655569965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15606932161892662, 0.1560693216189268, 0.2559455144710175], 
reward next is 0.7441, 
noisyNet noise sample is [array([-0.5648782], dtype=float32), 1.0166423]. 
=============================================
[2019-03-26 11:21:15,203] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[89.53716]
 [89.5298 ]
 [89.51006]
 [89.49314]
 [89.47208]], R is [[87.16233063]
 [87.03453064]
 [86.90783691]
 [86.78231049]
 [86.65791321]].
[2019-03-26 11:21:16,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:16,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:17,005] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-03-26 11:21:17,960] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3895488e-23 9.9214071e-01 3.5036676e-28 3.8599857e-17 7.8592673e-03], sum to 1.0000
[2019-03-26 11:21:17,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7232
[2019-03-26 11:21:17,972] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.95, 92.0, 1.0, 2.0, 0.6429167467192056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1022892.708885162, 1022892.708885162, 223417.3396784005], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7392600.0000, 
sim time next is 7393200.0000, 
raw observation next is [20.93333333333333, 92.0, 1.0, 2.0, 0.6805497348109224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1083154.631615145, 1083154.631615145, 232199.5134803211], 
processed observation next is [1.0, 0.5652173913043478, 0.19115323854660338, 0.92, 1.0, 1.0, 0.6151201624227981, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30087628655976245, 0.30087628655976245, 0.34656643803033], 
reward next is 0.6534, 
noisyNet noise sample is [array([-0.3138746], dtype=float32), 1.7505983]. 
=============================================
[2019-03-26 11:21:18,034] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6587771e-30 1.0000000e+00 6.9554097e-34 2.0549739e-25 1.2403720e-09], sum to 1.0000
[2019-03-26 11:21:18,042] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9093
[2019-03-26 11:21:18,045] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.68333333333333, 90.83333333333334, 1.0, 2.0, 0.563201918617114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 903653.6607290354, 903653.6607290361, 207022.7656497492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7398600.0000, 
sim time next is 7399200.0000, 
raw observation next is [20.66666666666667, 90.66666666666667, 1.0, 2.0, 0.5809534376644467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 932794.6921029142, 932794.6921029142, 210679.6656828513], 
processed observation next is [1.0, 0.6521739130434783, 0.17851500789889443, 0.9066666666666667, 1.0, 1.0, 0.49512462369210447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25910963669525394, 0.25910963669525394, 0.3144472622132109], 
reward next is 0.6856, 
noisyNet noise sample is [array([-2.0032437], dtype=float32), -0.5735323]. 
=============================================
[2019-03-26 11:21:23,062] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.813647e-35], sum to 1.0000
[2019-03-26 11:21:23,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8025
[2019-03-26 11:21:23,076] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 81.66666666666667, 1.0, 2.0, 0.3972185431418468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588120.0993658563, 588120.099365857, 173303.1534490435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7477800.0000, 
sim time next is 7478400.0000, 
raw observation next is [25.0, 81.33333333333334, 1.0, 2.0, 0.3996700933779856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590680.1827695712, 590680.1827695712, 173505.6169786183], 
processed observation next is [0.0, 0.5652173913043478, 0.38388625592417064, 0.8133333333333335, 1.0, 1.0, 0.27671095587709105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16407782854710312, 0.16407782854710312, 0.25896360743077357], 
reward next is 0.7410, 
noisyNet noise sample is [array([0.32491785], dtype=float32), -2.1408617]. 
=============================================
[2019-03-26 11:21:23,767] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0074918e-33 1.8411717e-28], sum to 1.0000
[2019-03-26 11:21:23,775] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7855
[2019-03-26 11:21:23,779] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 78.33333333333333, 1.0, 2.0, 0.4134173336485709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604483.2407646085, 604483.2407646085, 174592.2971846314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7483800.0000, 
sim time next is 7484400.0000, 
raw observation next is [25.8, 78.0, 1.0, 2.0, 0.4147308461261768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606051.9174150872, 606051.9174150872, 174729.5287385545], 
processed observation next is [0.0, 0.6521739130434783, 0.42180094786729866, 0.78, 1.0, 1.0, 0.2948564411158756, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16834775483752423, 0.16834775483752423, 0.26079034140082763], 
reward next is 0.7392, 
noisyNet noise sample is [array([-0.74488044], dtype=float32), 1.6690395]. 
=============================================
[2019-03-26 11:21:25,709] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8345014e-38 1.0000000e+00 0.0000000e+00 1.7506456e-33 3.0893546e-26], sum to 1.0000
[2019-03-26 11:21:25,717] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8445
[2019-03-26 11:21:25,728] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 90.0, 1.0, 2.0, 0.374328916147929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 564747.1899638186, 564747.1899638181, 171555.5652551689], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7537200.0000, 
sim time next is 7537800.0000, 
raw observation next is [23.25, 90.0, 1.0, 2.0, 0.3762866042624627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566773.4268576526, 566773.4268576526, 171703.0952544681], 
processed observation next is [0.0, 0.21739130434782608, 0.30094786729857825, 0.9, 1.0, 1.0, 0.24853807742465384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15743706301601462, 0.15743706301601462, 0.2562732764992061], 
reward next is 0.7437, 
noisyNet noise sample is [array([0.0664924], dtype=float32), -0.6684663]. 
=============================================
[2019-03-26 11:21:37,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:37,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:37,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-03-26 11:21:40,170] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.498259e-33 1.000000e+00 2.372999e-38 8.282496e-28 7.693682e-19], sum to 1.0000
[2019-03-26 11:21:40,176] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1588
[2019-03-26 11:21:40,181] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 86.0, 1.0, 2.0, 0.3213215530535127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512655.718011903, 512655.718011903, 168040.2722682601], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 21600.0000, 
sim time next is 22200.0000, 
raw observation next is [21.66666666666667, 85.66666666666667, 1.0, 2.0, 0.3690137845416782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 588413.6889443405, 588413.6889443405, 174182.9217423461], 
processed observation next is [1.0, 0.2608695652173913, 0.22590837282780438, 0.8566666666666667, 1.0, 1.0, 0.23977564402611834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16344824692898346, 0.16344824692898346, 0.25997451006320316], 
reward next is 0.7400, 
noisyNet noise sample is [array([-1.6352793], dtype=float32), 2.0289855]. 
=============================================
[2019-03-26 11:21:44,871] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:44,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:44,898] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-03-26 11:21:48,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,133] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-03-26 11:21:48,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,154] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,178] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-03-26 11:21:48,229] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,230] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-03-26 11:21:48,352] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,352] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,367] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-03-26 11:21:48,429] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,430] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-03-26 11:21:48,536] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,537] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,547] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-03-26 11:21:48,777] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,777] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,782] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-03-26 11:21:48,805] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,806] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-03-26 11:21:48,960] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,960] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,962] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-03-26 11:21:48,987] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:48,987] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:48,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-03-26 11:21:49,011] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:49,012] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:49,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:49,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:49,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-03-26 11:21:49,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-03-26 11:21:49,069] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:21:49,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:49,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-03-26 11:21:50,009] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3519594e-35 1.0000000e+00 0.0000000e+00 5.1738994e-31 3.9200816e-21], sum to 1.0000
[2019-03-26 11:21:50,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2010
[2019-03-26 11:21:50,016] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.61666666666667, 93.0, 1.0, 2.0, 0.2231216326482891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 371605.0903524174, 371605.090352418, 157897.7036187377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 701400.0000, 
sim time next is 702000.0000, 
raw observation next is [17.6, 93.0, 1.0, 2.0, 0.2214840591204185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 368918.9041889188, 368918.9041889194, 157741.2396162729], 
processed observation next is [1.0, 0.13043478260869565, 0.0331753554502371, 0.93, 1.0, 1.0, 0.06202898689207046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10247747338581079, 0.10247747338581095, 0.23543468599443718], 
reward next is 0.7646, 
noisyNet noise sample is [array([1.4795113], dtype=float32), -1.2028297]. 
=============================================
[2019-03-26 11:21:50,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.014725]
 [77.07478 ]
 [77.281944]
 [77.567474]
 [77.48519 ]], R is [[77.09509277]
 [77.08847046]
 [77.08176422]
 [77.07529449]
 [77.06741333]].
[2019-03-26 11:21:50,054] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 11:21:50,054] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:21:50,054] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:21:50,054] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:50,055] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:50,055] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:21:50,055] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:21:50,055] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:50,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run35
[2019-03-26 11:21:50,056] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:50,055] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:21:50,059] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:21:50,083] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run35
[2019-03-26 11:21:50,107] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run35
[2019-03-26 11:21:50,129] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run35
[2019-03-26 11:21:50,197] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run35
[2019-03-26 11:22:15,641] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.05808636], dtype=float32), 0.30059788]
[2019-03-26 11:22:15,643] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.53333333333333, 67.66666666666667, 1.0, 2.0, 0.6817976637352592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 957248.2289622657, 957248.2289622657, 217554.8272944877]
[2019-03-26 11:22:15,645] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:22:15,647] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5990324e-37 1.0000000e+00 0.0000000e+00 5.5086085e-34 5.0617240e-24], sampled 0.33930109476230363
[2019-03-26 11:22:23,872] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.05808636], dtype=float32), 0.30059788]
[2019-03-26 11:22:23,873] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.55030653, 82.69696688, 1.0, 2.0, 0.6328729352919039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 884418.8974933061, 884418.8974933061, 207030.2879761513]
[2019-03-26 11:22:23,875] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:22:23,878] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1216918e-35 1.0000000e+00 0.0000000e+00 2.7088690e-25 6.5867791e-20], sampled 0.6327113961893395
[2019-03-26 11:22:41,585] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.05808636], dtype=float32), 0.30059788]
[2019-03-26 11:22:41,586] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.64991820333333, 74.83517815333335, 1.0, 2.0, 0.5420982720306206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757519.0857774548, 757519.0857774554, 190455.5935176703]
[2019-03-26 11:22:41,587] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:22:41,590] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5273166e-38 1.0000000e+00 0.0000000e+00 2.3031896e-27 1.2499643e-28], sampled 0.3767654012555395
[2019-03-26 11:22:42,326] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.05808636], dtype=float32), 0.30059788]
[2019-03-26 11:22:42,328] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.95459833, 53.8130788, 1.0, 2.0, 0.4773377616291465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698868.0784103635, 698868.0784103629, 184138.8869723308]
[2019-03-26 11:22:42,330] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:22:42,334] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.3723319e-30 1.0323374e-31], sampled 0.3456680902658902
[2019-03-26 11:22:55,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.05808636], dtype=float32), 0.30059788]
[2019-03-26 11:22:55,598] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.16666666666667, 88.16666666666667, 1.0, 2.0, 0.6123978910827148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855794.1759007145, 855794.1759007145, 203078.4648208303]
[2019-03-26 11:22:55,600] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:22:55,603] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.13297209e-36 1.00000000e+00 0.00000000e+00 1.09619124e-32
 1.10117642e-21], sampled 0.8241028248308124
[2019-03-26 11:23:45,723] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8356.9014 2998543855.4066 906.0000
[2019-03-26 11:23:46,475] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8689.3674 2840974517.0976 678.0000
[2019-03-26 11:23:46,557] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8151.0111 3156825097.1657 1159.0000
[2019-03-26 11:23:46,642] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8539.6379 2923682740.0844 703.0000
[2019-03-26 11:23:46,673] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8834.6703 2780486532.1826 530.0000
[2019-03-26 11:23:47,690] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 850000, evaluation results [850000.0, 8151.011060181979, 3156825097.165721, 1159.0, 8539.637949640439, 2923682740.084364, 703.0, 8834.670261686677, 2780486532.1826057, 530.0, 8356.901354045387, 2998543855.4066157, 906.0, 8689.367424283242, 2840974517.0976334, 678.0]
[2019-03-26 11:23:49,806] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2410376e-32 1.0000000e+00 8.8225276e-35 9.1298071e-29 1.4564619e-24], sum to 1.0000
[2019-03-26 11:23:49,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1508
[2019-03-26 11:23:49,824] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 49.33333333333334, 1.0, 2.0, 0.6256760036857524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1024025.362968306, 1024025.362968305, 221193.4819674156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 740400.0000, 
sim time next is 741000.0000, 
raw observation next is [25.93333333333333, 48.66666666666666, 1.0, 2.0, 0.6237115175825323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1021657.733317938, 1021657.733317938, 220771.0262853622], 
processed observation next is [1.0, 0.5652173913043478, 0.42812006319115314, 0.4866666666666666, 1.0, 1.0, 0.5466403826295569, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28379381481053834, 0.28379381481053834, 0.3295089944557645], 
reward next is 0.6705, 
noisyNet noise sample is [array([-0.16262692], dtype=float32), -0.862538]. 
=============================================
[2019-03-26 11:23:49,854] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[51.245495]
 [51.36574 ]
 [51.655266]
 [52.210373]
 [52.972336]], R is [[51.27767944]
 [51.43476486]
 [51.58932877]
 [51.74975204]
 [51.93077469]].
[2019-03-26 11:24:12,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1799189e-33 3.0816922e-26], sum to 1.0000
[2019-03-26 11:24:12,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2230
[2019-03-26 11:24:12,974] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 85.00000000000001, 1.0, 2.0, 0.2389475910213506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 394263.9203123889, 394263.9203123889, 159801.1288835176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 436200.0000, 
sim time next is 436800.0000, 
raw observation next is [19.6, 85.0, 1.0, 2.0, 0.2397853028395814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 395649.4862059378, 395649.4862059372, 159880.4258597444], 
processed observation next is [1.0, 0.043478260869565216, 0.127962085308057, 0.85, 1.0, 1.0, 0.08407867811997759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10990263505720495, 0.10990263505720478, 0.2386275012832006], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.93985945], dtype=float32), 0.3618812]. 
=============================================
[2019-03-26 11:24:15,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.9781160e-29 1.0000000e+00 1.0809870e-35 1.5642430e-32 1.4569843e-24], sum to 1.0000
[2019-03-26 11:24:15,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1312
[2019-03-26 11:24:15,428] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 57.66666666666666, 1.0, 2.0, 0.3073741975976325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 502920.4179524326, 502920.4179524321, 167114.4005510098], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 478200.0000, 
sim time next is 478800.0000, 
raw observation next is [24.5, 57.0, 1.0, 2.0, 0.3635119540660886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 594544.1725442765, 594544.1725442759, 174315.8896779703], 
processed observation next is [1.0, 0.5652173913043478, 0.3601895734597157, 0.57, 1.0, 1.0, 0.2331469326097453, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1651511590400768, 0.16515115904007663, 0.26017296966861236], 
reward next is 0.7398, 
noisyNet noise sample is [array([1.0329596], dtype=float32), 1.1259923]. 
=============================================
[2019-03-26 11:24:23,337] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6927871e-37 1.0000000e+00 0.0000000e+00 2.3978353e-35 3.1864819e-28], sum to 1.0000
[2019-03-26 11:24:23,347] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8073
[2019-03-26 11:24:23,352] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.96666666666667, 87.66666666666667, 1.0, 2.0, 0.2144727358452151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 357739.3476943186, 357739.3476943193, 156966.718099004], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 606000.0000, 
sim time next is 606600.0000, 
raw observation next is [17.9, 88.0, 1.0, 2.0, 0.2134813270544272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 356140.5977505077, 356140.5977505077, 156860.3708652732], 
processed observation next is [1.0, 0.0, 0.04739336492890995, 0.88, 1.0, 1.0, 0.052387141029430366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09892794381958547, 0.09892794381958547, 0.23411995651533313], 
reward next is 0.7659, 
noisyNet noise sample is [array([-0.00807036], dtype=float32), 0.49435648]. 
=============================================
[2019-03-26 11:24:30,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.8168936e-35 5.9949379e-17], sum to 1.0000
[2019-03-26 11:24:30,589] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6719
[2019-03-26 11:24:30,592] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.26666666666667, 54.5, 1.0, 2.0, 0.4705730174011961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766204.2374903259, 766204.2374903265, 190516.0469403185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 735000.0000, 
sim time next is 735600.0000, 
raw observation next is [25.33333333333334, 54.0, 1.0, 2.0, 0.3495018895137921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 569324.169269985, 569324.1692699844, 172337.6181527059], 
processed observation next is [1.0, 0.5217391304347826, 0.3996840442338076, 0.54, 1.0, 1.0, 0.2162673367636049, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15814560257499585, 0.15814560257499566, 0.25722032560105357], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.82993686], dtype=float32), 0.11065929]. 
=============================================
[2019-03-26 11:24:30,990] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3389526e-33 5.5002511e-19], sum to 1.0000
[2019-03-26 11:24:30,998] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1538
[2019-03-26 11:24:31,004] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.73333333333333, 56.0, 1.0, 2.0, 0.2310126852527967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381839.9661477196, 381839.9661477196, 159022.8178168577], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 754800.0000, 
sim time next is 755400.0000, 
raw observation next is [23.51666666666667, 57.5, 1.0, 2.0, 0.2341650968422406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 386864.8557787214, 386864.855778722, 159326.179546269], 
processed observation next is [1.0, 0.7391304347826086, 0.31358609794628767, 0.575, 1.0, 1.0, 0.07730734559306095, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10746245993853372, 0.10746245993853389, 0.23780026797950599], 
reward next is 0.7622, 
noisyNet noise sample is [array([-0.6615712], dtype=float32), -0.82405585]. 
=============================================
[2019-03-26 11:24:32,310] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.9857667e-36], sum to 1.0000
[2019-03-26 11:24:32,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4137
[2019-03-26 11:24:32,319] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 90.0, 1.0, 2.0, 0.2555112637412424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419070.017281094, 419070.017281094, 161458.2292110826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 779400.0000, 
sim time next is 780000.0000, 
raw observation next is [19.5, 90.33333333333334, 1.0, 2.0, 0.2563645306082605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 420244.7319452873, 420244.7319452867, 161542.4394418329], 
processed observation next is [0.0, 0.0, 0.12322274881516594, 0.9033333333333334, 1.0, 1.0, 0.10405365133525357, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11673464776257982, 0.11673464776257964, 0.24110811856989983], 
reward next is 0.7589, 
noisyNet noise sample is [array([0.34832937], dtype=float32), 0.37987408]. 
=============================================
[2019-03-26 11:24:32,333] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.09244 ]
 [74.93113 ]
 [74.514534]
 [74.02999 ]
 [74.03814 ]], R is [[74.95708466]
 [74.96652985]
 [74.97599792]
 [74.98547363]
 [74.99492645]].
[2019-03-26 11:24:32,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:24:32,810] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4225
[2019-03-26 11:24:32,814] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 91.0, 1.0, 2.0, 0.2593282128459597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 424640.0631925294, 424640.0631925301, 161836.7362211405], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 781200.0000, 
sim time next is 781800.0000, 
raw observation next is [19.48333333333333, 91.16666666666667, 1.0, 2.0, 0.2598342173364092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425380.873992416, 425380.873992416, 161886.837825513], 
processed observation next is [0.0, 0.043478260869565216, 0.12243285939968399, 0.9116666666666667, 1.0, 1.0, 0.10823399679085446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11816135388678223, 0.11816135388678223, 0.24162214600822837], 
reward next is 0.7584, 
noisyNet noise sample is [array([-0.24270587], dtype=float32), -0.23690084]. 
=============================================
[2019-03-26 11:24:33,278] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:24:33,286] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2138
[2019-03-26 11:24:33,291] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 92.0, 1.0, 2.0, 0.2597878229125039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425209.6171913282, 425209.6171913282, 161880.5881957645], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 788400.0000, 
sim time next is 789000.0000, 
raw observation next is [19.4, 92.16666666666667, 1.0, 2.0, 0.2602324545518706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 425857.082010168, 425857.0820101674, 161924.4744834583], 
processed observation next is [0.0, 0.13043478260869565, 0.11848341232227487, 0.9216666666666667, 1.0, 1.0, 0.10871380066490434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11829363389171334, 0.11829363389171317, 0.2416783201245646], 
reward next is 0.7583, 
noisyNet noise sample is [array([-0.7589232], dtype=float32), -0.15216672]. 
=============================================
[2019-03-26 11:24:33,309] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.82832]
 [73.61574]
 [73.53118]
 [73.7483 ]
 [74.18099]], R is [[74.05267334]
 [74.07053375]
 [74.08830261]
 [74.1059494 ]
 [74.1233902 ]].
[2019-03-26 11:24:42,089] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 11:24:42,091] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:24:42,092] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:24:42,093] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:24:42,094] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:24:42,095] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:24:42,094] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:24:42,097] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:24:42,096] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:24:42,100] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:24:42,102] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:24:42,120] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run36
[2019-03-26 11:24:42,141] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run36
[2019-03-26 11:24:42,173] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run36
[2019-03-26 11:24:42,176] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run36
[2019-03-26 11:24:42,197] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run36
[2019-03-26 11:24:59,986] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.08995193], dtype=float32), 0.38342586]
[2019-03-26 11:24:59,988] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.0, 66.0, 1.0, 2.0, 0.3859151477994501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 621090.1270088706, 621090.12700887, 176932.134357608]
[2019-03-26 11:24:59,991] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:24:59,993] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.5645693e-38 1.4087850e-30], sampled 0.009007468253046325
[2019-03-26 11:25:07,220] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.08995193], dtype=float32), 0.38342586]
[2019-03-26 11:25:07,221] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.915927735, 100.0, 1.0, 2.0, 0.3799980399104825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578114.5591771051, 578114.5591771051, 172874.1089070822]
[2019-03-26 11:25:07,222] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:25:07,224] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3282186e-33 2.2718455e-27], sampled 0.8466109859392058
[2019-03-26 11:25:09,972] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.08995193], dtype=float32), 0.38342586]
[2019-03-26 11:25:09,973] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.71666666666667, 89.66666666666667, 1.0, 2.0, 0.8951863576699861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1278924.57930032, 1278924.57930032, 272699.238558078]
[2019-03-26 11:25:09,975] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:25:09,979] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7878716e-33 1.0000000e+00 1.2198401e-36 7.6075408e-31 2.4449774e-14], sampled 0.7622388416441758
[2019-03-26 11:26:23,864] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.08995193], dtype=float32), 0.38342586]
[2019-03-26 11:26:23,865] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.33333333333334, 95.33333333333334, 1.0, 2.0, 0.4369592347870485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640473.5025719213, 640473.5025719213, 178112.2125608344]
[2019-03-26 11:26:23,866] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:26:23,868] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7286066e-38 5.1254446e-28], sampled 0.7342235778594977
[2019-03-26 11:26:27,313] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.08995193], dtype=float32), 0.38342586]
[2019-03-26 11:26:27,316] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.925284375, 80.05565027, 1.0, 2.0, 0.6683933166444858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 934079.2786781593, 934079.2786781593, 214167.7030625007]
[2019-03-26 11:26:27,318] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:26:27,322] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3026352e-37 1.0000000e+00 0.0000000e+00 8.8601670e-35 9.0583616e-25], sampled 0.6913201763303638
[2019-03-26 11:26:38,301] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8572.5113 2926682254.1747 627.0000
[2019-03-26 11:26:38,330] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8188.7919 3157657543.4940 1062.0000
[2019-03-26 11:26:38,331] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8415.4907 2998962172.4661 784.0000
[2019-03-26 11:26:38,486] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8861.7456 2782008718.0964 471.0000
[2019-03-26 11:26:38,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8731.0792 2841795967.1209 578.0000
[2019-03-26 11:26:39,544] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 875000, evaluation results [875000.0, 8188.791864789087, 3157657543.4940133, 1062.0, 8572.511333862965, 2926682254.174655, 627.0, 8861.745598478046, 2782008718.096427, 471.0, 8415.490656182124, 2998962172.4660997, 784.0, 8731.079178339682, 2841795967.1208863, 578.0]
[2019-03-26 11:26:45,626] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.6857002e-38 1.0000000e+00 0.0000000e+00 3.9426861e-34 2.2750815e-26], sum to 1.0000
[2019-03-26 11:26:45,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2787
[2019-03-26 11:26:45,640] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 96.16666666666666, 1.0, 2.0, 0.3827724543523893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 574503.0928234248, 574503.0928234248, 172321.7144258202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1043400.0000, 
sim time next is 1044000.0000, 
raw observation next is [22.6, 96.0, 1.0, 2.0, 0.3844018335886054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576693.127541197, 576693.1275411964, 172509.1392122945], 
processed observation next is [1.0, 0.08695652173913043, 0.27014218009478685, 0.96, 1.0, 1.0, 0.2583154621549463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16019253542811027, 0.16019253542811013, 0.2574763271825291], 
reward next is 0.7425, 
noisyNet noise sample is [array([1.1515341], dtype=float32), -0.7097585]. 
=============================================
[2019-03-26 11:26:45,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.96156]
 [75.08258]
 [75.12637]
 [75.1418 ]
 [75.08689]], R is [[74.9668045 ]
 [74.95994568]
 [74.95333862]
 [74.94680786]
 [74.94034576]].
[2019-03-26 11:26:46,554] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:26:46,560] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7978
[2019-03-26 11:26:46,570] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 95.0, 1.0, 2.0, 0.3003555778695772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478186.0030671549, 478186.0030671549, 165490.4903227347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1062000.0000, 
sim time next is 1062600.0000, 
raw observation next is [20.65, 95.0, 1.0, 2.0, 0.3155317453343924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501790.6406633075, 501790.6406633075, 167205.703821218], 
processed observation next is [1.0, 0.30434782608695654, 0.1777251184834123, 0.95, 1.0, 1.0, 0.1753394522101113, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13938628907314096, 0.13938628907314096, 0.24956075197196714], 
reward next is 0.7504, 
noisyNet noise sample is [array([0.82090807], dtype=float32), 0.49018118]. 
=============================================
[2019-03-26 11:26:49,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.1400276e-33 1.2294959e-22], sum to 1.0000
[2019-03-26 11:26:49,102] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9058
[2019-03-26 11:26:49,109] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 70.5, 1.0, 2.0, 0.3363306403399798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 523551.3131005259, 523551.3131005265, 168646.9554806755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1102200.0000, 
sim time next is 1102800.0000, 
raw observation next is [24.73333333333333, 71.0, 1.0, 2.0, 0.3367065924894096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524827.6123729647, 524827.6123729647, 168766.565330683], 
processed observation next is [1.0, 0.782608695652174, 0.3712480252764612, 0.71, 1.0, 1.0, 0.2008513162523007, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1457854478813791, 0.1457854478813791, 0.25189039601594476], 
reward next is 0.7481, 
noisyNet noise sample is [array([-0.16782637], dtype=float32), -0.49134958]. 
=============================================
[2019-03-26 11:26:55,710] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2805714e-35 1.0000000e+00 0.0000000e+00 1.1090848e-29 3.5587349e-23], sum to 1.0000
[2019-03-26 11:26:55,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8848
[2019-03-26 11:26:55,728] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 88.33333333333334, 1.0, 2.0, 0.3474276772837944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537843.0262861188, 537843.0262861188, 169715.7440230381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1210800.0000, 
sim time next is 1211400.0000, 
raw observation next is [22.5, 88.5, 1.0, 2.0, 0.3466011648988677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536737.8478110825, 536737.8478110818, 169630.4587974352], 
processed observation next is [1.0, 0.0, 0.2654028436018958, 0.885, 1.0, 1.0, 0.2127724878299611, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1490938466141896, 0.1490938466141894, 0.25317978924990325], 
reward next is 0.7468, 
noisyNet noise sample is [array([-1.5619874], dtype=float32), 0.765063]. 
=============================================
[2019-03-26 11:26:56,184] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.12161314e-32], sum to 1.0000
[2019-03-26 11:26:56,193] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3811
[2019-03-26 11:26:56,198] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 85.0, 1.0, 2.0, 0.3581687164955957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549893.545817771, 549893.5458177704, 170584.802836403], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1543200.0000, 
sim time next is 1543800.0000, 
raw observation next is [23.18333333333334, 85.5, 1.0, 2.0, 0.3573930548972085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548950.372092193, 548950.3720921937, 170512.9621013683], 
processed observation next is [0.0, 0.8695652173913043, 0.29778830963665126, 0.855, 1.0, 1.0, 0.2257747649363958, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15248621447005362, 0.1524862144700538, 0.25449695836025116], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.28461376], dtype=float32), -1.9507937]. 
=============================================
[2019-03-26 11:26:57,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1716985e-36 1.0000000e+00 0.0000000e+00 1.8770550e-29 2.3857704e-15], sum to 1.0000
[2019-03-26 11:26:57,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5504
[2019-03-26 11:26:57,898] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 80.0, 1.0, 2.0, 0.4722703245685806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659912.3674265278, 659912.3674265278, 179365.937710439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1274400.0000, 
sim time next is 1275000.0000, 
raw observation next is [26.75, 80.66666666666667, 1.0, 2.0, 0.477435728975629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 667132.3522935935, 667132.3522935928, 180136.3715375472], 
processed observation next is [1.0, 0.782608695652174, 0.4668246445497631, 0.8066666666666668, 1.0, 1.0, 0.3704044927417217, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18531454230377595, 0.18531454230377575, 0.26886025602618985], 
reward next is 0.7311, 
noisyNet noise sample is [array([0.33488852], dtype=float32), 0.114259385]. 
=============================================
[2019-03-26 11:26:57,929] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[82.23087]
 [82.71737]
 [83.37813]
 [85.76226]
 [86.055  ]], R is [[81.56866455]
 [81.48526764]
 [81.40375519]
 [81.324646  ]
 [80.51139832]].
[2019-03-26 11:27:00,661] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2564941e-33 1.0000000e+00 1.4413812e-35 1.1178899e-22 2.3670588e-12], sum to 1.0000
[2019-03-26 11:27:00,673] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6322
[2019-03-26 11:27:00,681] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4597026772879609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651966.8511016708, 651966.8511016708, 178762.9624516959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1299000.0000, 
sim time next is 1299600.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.45971656316523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651988.9023952944, 651988.902395295, 178765.3062099393], 
processed observation next is [1.0, 0.043478260869565216, 0.3507109004739337, 0.94, 1.0, 1.0, 0.3490561001990723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18110802844313734, 0.1811080284431375, 0.266813889865581], 
reward next is 0.7332, 
noisyNet noise sample is [array([0.02679825], dtype=float32), -1.7781409]. 
=============================================
[2019-03-26 11:27:01,445] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.543933e-32 1.000000e+00 0.000000e+00 6.688216e-33 7.857508e-16], sum to 1.0000
[2019-03-26 11:27:01,453] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1380
[2019-03-26 11:27:01,464] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.51666666666667, 91.33333333333334, 1.0, 2.0, 0.4829092636687409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688431.9385824759, 688431.9385824759, 182708.3712056043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1314600.0000, 
sim time next is 1315200.0000, 
raw observation next is [24.43333333333334, 91.66666666666667, 1.0, 2.0, 0.4445638809425742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634802.0317618322, 634802.0317618322, 177121.6507836706], 
processed observation next is [1.0, 0.21739130434782608, 0.3570300157977887, 0.9166666666666667, 1.0, 1.0, 0.33079985655731836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17633389771162006, 0.17633389771162006, 0.26436067281144865], 
reward next is 0.7356, 
noisyNet noise sample is [array([0.47719622], dtype=float32), -0.33298078]. 
=============================================
[2019-03-26 11:27:08,689] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5874255e-38 1.0000000e+00 4.6391995e-38 7.4255163e-35 3.9462151e-15], sum to 1.0000
[2019-03-26 11:27:08,695] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9566
[2019-03-26 11:27:08,700] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 88.16666666666667, 1.0, 2.0, 0.8792083519838492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1281363.709296258, 1281363.709296258, 271862.067511845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1763400.0000, 
sim time next is 1764000.0000, 
raw observation next is [24.3, 88.0, 1.0, 2.0, 0.8490209401339973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1241531.474424428, 1241531.474424428, 264079.1401714162], 
processed observation next is [1.0, 0.43478260869565216, 0.3507109004739337, 0.88, 1.0, 1.0, 0.8180975182337317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3448698540067856, 0.3448698540067856, 0.39414797040509886], 
reward next is 0.6059, 
noisyNet noise sample is [array([-1.2705163], dtype=float32), -0.60193187]. 
=============================================
[2019-03-26 11:27:08,718] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.577644]
 [70.640854]
 [71.09587 ]
 [70.92942 ]
 [70.363075]], R is [[70.66064453]
 [70.54827881]
 [70.44377136]
 [70.37892914]
 [70.30709076]].
[2019-03-26 11:27:09,412] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2617168e-37], sum to 1.0000
[2019-03-26 11:27:09,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2262
[2019-03-26 11:27:09,427] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 89.33333333333333, 1.0, 2.0, 0.3907755687023597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584580.0612230237, 584580.0612230243, 173168.24205628], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1453200.0000, 
sim time next is 1453800.0000, 
raw observation next is [23.25, 90.66666666666667, 1.0, 2.0, 0.3877274178202514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581494.2746672592, 581494.2746672592, 172934.328901482], 
processed observation next is [0.0, 0.8260869565217391, 0.30094786729857825, 0.9066666666666667, 1.0, 1.0, 0.2623221901448812, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.161526187407572, 0.161526187407572, 0.25811093865892837], 
reward next is 0.7419, 
noisyNet noise sample is [array([-0.15514846], dtype=float32), 0.6850753]. 
=============================================
[2019-03-26 11:27:15,492] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1196125e-32 1.0000000e+00 2.3420751e-35 1.0766858e-21 3.3495489e-12], sum to 1.0000
[2019-03-26 11:27:15,502] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4883
[2019-03-26 11:27:15,506] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.0, 1.0, 2.0, 0.3260876342298332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 511458.9051673762, 511458.9051673768, 167803.87279087], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1560000.0000, 
sim time next is 1560600.0000, 
raw observation next is [21.7, 91.0, 1.0, 2.0, 0.3260588268832261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511414.5032452514, 511414.5032452508, 167800.4732867103], 
processed observation next is [1.0, 0.043478260869565216, 0.2274881516587678, 0.91, 1.0, 1.0, 0.1880226829918387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14205958423479206, 0.1420595842347919, 0.25044846759210493], 
reward next is 0.7496, 
noisyNet noise sample is [array([1.2360154], dtype=float32), 0.09899157]. 
=============================================
[2019-03-26 11:27:27,578] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.09341081e-24 9.99989271e-01 1.70563760e-32 1.71141125e-18
 1.07040605e-05], sum to 1.0000
[2019-03-26 11:27:27,587] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0405
[2019-03-26 11:27:27,591] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 85.33333333333333, 1.0, 2.0, 0.6590610642077355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1039346.497714092, 1039346.497714092, 226309.5087335165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1779000.0000, 
sim time next is 1779600.0000, 
raw observation next is [21.93333333333333, 86.66666666666667, 1.0, 2.0, 0.6317802659924266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 997703.614510872, 997703.614510872, 220280.7408840962], 
processed observation next is [1.0, 0.6086956521739131, 0.23854660347551332, 0.8666666666666667, 1.0, 1.0, 0.5563617662559357, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27713989291968666, 0.27713989291968666, 0.3287772252001436], 
reward next is 0.6712, 
noisyNet noise sample is [array([0.05722573], dtype=float32), -0.66037863]. 
=============================================
[2019-03-26 11:27:29,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.3277020e-33 1.0000000e+00 5.0250787e-37 3.2615064e-24 2.1947479e-09], sum to 1.0000
[2019-03-26 11:27:29,354] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8611
[2019-03-26 11:27:29,357] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 95.0, 1.0, 2.0, 0.3646920546257617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561563.3696320932, 561563.3696320932, 171617.7061094161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1825200.0000, 
sim time next is 1825800.0000, 
raw observation next is [21.9, 95.16666666666667, 1.0, 2.0, 0.3628187414379867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558332.0404360032, 558332.0404360032, 171333.4046020497], 
processed observation next is [1.0, 0.13043478260869565, 0.23696682464454974, 0.9516666666666667, 1.0, 1.0, 0.23231173667227314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15509223345444534, 0.15509223345444534, 0.25572149940604433], 
reward next is 0.7443, 
noisyNet noise sample is [array([0.72884965], dtype=float32), 0.8134197]. 
=============================================
[2019-03-26 11:27:33,255] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 11:27:33,259] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:27:33,260] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:27:33,261] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:27:33,262] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:27:33,262] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:27:33,263] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:27:33,264] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:27:33,263] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:27:33,260] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:27:33,266] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:27:33,278] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run37
[2019-03-26 11:27:33,299] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run37
[2019-03-26 11:27:33,318] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run37
[2019-03-26 11:27:33,319] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run37
[2019-03-26 11:27:33,341] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run37
[2019-03-26 11:27:39,584] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:27:39,586] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.5, 77.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.3037730162142898, 6.911199999999999, 6.9112, 170.5573041426782, 798730.080348057, 798730.0803480576, 272303.656307769]
[2019-03-26 11:27:39,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:27:39,589] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8505325e-24 5.4913999e-06 4.9716029e-31 3.3297417e-22 9.9999452e-01], sampled 0.15239141855113836
[2019-03-26 11:27:44,294] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:27:44,296] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.76666666666667, 58.66666666666667, 1.0, 2.0, 0.2582991951487468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 424473.0759650796, 424473.075965079, 161738.4565234062]
[2019-03-26 11:27:44,298] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:27:44,305] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.3396657e-28 1.0000000e+00 7.9182430e-35 2.1644762e-23 3.1366852e-13], sampled 0.7293205090219066
[2019-03-26 11:27:46,264] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:27:46,265] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.43402426833333, 73.68697181166667, 1.0, 2.0, 0.3057054674209356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486439.1154025988, 486439.1154025988, 166080.038042697]
[2019-03-26 11:27:46,266] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:27:46,269] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.9861100e-30 1.0000000e+00 1.3036848e-36 1.0867349e-26 7.1819537e-14], sampled 0.07912480475655903
[2019-03-26 11:28:03,799] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:28:03,800] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.1, 91.0, 1.0, 2.0, 0.4368810511432824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635064.0412796895, 635064.0412796895, 177445.3820241003]
[2019-03-26 11:28:03,801] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:28:03,802] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1060878e-27 9.9999952e-01 5.2342409e-33 7.3690337e-24 4.2241160e-07], sampled 0.5878322196371654
[2019-03-26 11:28:04,224] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:28:04,227] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.56666666666667, 94.0, 1.0, 2.0, 0.4698459700214774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658284.6062091653, 658284.6062091653, 179234.0391854889]
[2019-03-26 11:28:04,229] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:28:04,232] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.2347468e-25 9.9958402e-01 8.6388067e-31 9.5051024e-19 4.1598003e-04], sampled 0.5816941289751232
[2019-03-26 11:28:06,427] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:28:06,429] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.71891016833333, 71.056853255, 1.0, 2.0, 0.2974556892917353, 1.0, 2.0, 0.2974556892917353, 1.0, 2.0, 0.5034800230233353, 6.9112, 6.9112, 171.5212843490159, 1247255.152250363, 1247255.152250363, 302294.9100400651]
[2019-03-26 11:28:06,431] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:28:06,434] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3862850e-26 1.3087378e-09 6.8384103e-35 8.8558758e-20 1.0000000e+00], sampled 0.4128523639445928
[2019-03-26 11:28:15,444] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:28:15,446] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.66746608333333, 87.49665469333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2003895457339395, 6.911199999999999, 6.9112, 184.5923449428631, 524970.311932387, 524970.3119323876, 237612.2737765047]
[2019-03-26 11:28:15,447] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:28:15,452] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.3274334e-23 1.6896741e-01 1.7497728e-29 2.7761048e-22 8.3103263e-01], sampled 0.4186144763299543
[2019-03-26 11:28:52,147] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:28:52,149] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.5, 92.66666666666667, 1.0, 2.0, 0.3645552408971894, 1.0, 2.0, 0.3645552408971894, 1.0, 2.0, 0.6331118931779844, 6.9112, 6.9112, 170.5573041426782, 1528814.496997037, 1528814.496997037, 334136.772496756]
[2019-03-26 11:28:52,151] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:28:52,155] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1458186e-31 5.3361413e-16 0.0000000e+00 3.6046999e-29 1.0000000e+00], sampled 0.7146410984301144
[2019-03-26 11:28:56,465] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:28:56,466] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.45, 60.33333333333333, 1.0, 2.0, 0.1825801656196502, 1.0, 2.0, 0.1825801656196502, 1.0, 2.0, 0.3153066833729847, 6.911200000000001, 6.9112, 170.5573041426782, 765403.6186406384, 765403.6186406378, 265157.1457029749]
[2019-03-26 11:28:56,466] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:28:56,471] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8436558e-23 4.4123479e-03 6.2790722e-30 4.1217142e-25 9.9558771e-01], sampled 0.8981077950326644
[2019-03-26 11:29:29,420] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7947.0966 3256465680.3713 80.0000
[2019-03-26 11:29:29,431] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7538.1107 3167122958.7199 103.0000
[2019-03-26 11:29:29,476] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7207.4479 3454642577.9548 568.0000
[2019-03-26 11:29:29,485] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7831.4842 3308005778.5117 16.0000
[2019-03-26 11:29:29,600] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.12544422], dtype=float32), 0.37576494]
[2019-03-26 11:29:29,600] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.54597701833333, 83.85678724166667, 1.0, 2.0, 0.1905023585523173, 1.0, 2.0, 0.1905023585523173, 1.0, 2.0, 0.3266169540995078, 6.911199999999999, 6.9112, 184.5923449428631, 798604.4058967935, 798604.4058967942, 271132.2094169077]
[2019-03-26 11:29:29,600] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:29:29,601] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8586310e-27 1.0588736e-14 2.0353600e-33 2.7970925e-21 1.0000000e+00], sampled 0.3946628273450894
[2019-03-26 11:29:29,626] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7669.7530 3136980733.9894 63.0000
[2019-03-26 11:29:30,644] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 900000, evaluation results [900000.0, 7207.447927979632, 3454642577.954753, 568.0, 7947.096634758, 3256465680.3713493, 80.0, 7669.752963070328, 3136980733.9893765, 63.0, 7831.484210053536, 3308005778.511682, 16.0, 7538.110669078882, 3167122958.719862, 103.0]
[2019-03-26 11:29:32,584] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0270578e-30 1.0000000e+00 9.2361909e-37 1.2266785e-25 1.1448038e-18], sum to 1.0000
[2019-03-26 11:29:32,594] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5635
[2019-03-26 11:29:32,598] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 92.0, 1.0, 2.0, 0.4313916223342477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632436.7785811314, 632436.778581132, 177323.0443746771], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1920600.0000, 
sim time next is 1921200.0000, 
raw observation next is [23.86666666666667, 91.33333333333334, 1.0, 2.0, 0.4351395411082026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 637203.9112639973, 637203.9112639967, 177774.0840417994], 
processed observation next is [1.0, 0.21739130434782608, 0.33017377567140627, 0.9133333333333334, 1.0, 1.0, 0.31944523025084653, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17700108646222149, 0.17700108646222132, 0.26533445379373044], 
reward next is 0.7347, 
noisyNet noise sample is [array([-1.2165602], dtype=float32), -0.26152426]. 
=============================================
[2019-03-26 11:29:32,729] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1799322e-36 1.0000000e+00 0.0000000e+00 3.1304049e-31 5.0420978e-12], sum to 1.0000
[2019-03-26 11:29:32,740] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5396
[2019-03-26 11:29:32,743] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.86666666666667, 95.0, 1.0, 2.0, 0.4686180503722527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672891.2471478183, 672891.2471478183, 181135.9230217072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1912200.0000, 
sim time next is 1912800.0000, 
raw observation next is [23.83333333333333, 95.0, 1.0, 2.0, 0.4435608342594137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 637819.7388658068, 637819.7388658068, 177542.4678258472], 
processed observation next is [1.0, 0.13043478260869565, 0.32859399684044216, 0.95, 1.0, 1.0, 0.32959136657760685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17717214968494635, 0.17717214968494635, 0.2649887579490257], 
reward next is 0.7350, 
noisyNet noise sample is [array([0.01529766], dtype=float32), -0.8415392]. 
=============================================
[2019-03-26 11:29:34,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.3232201e-34 4.0531212e-36], sum to 1.0000
[2019-03-26 11:29:34,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0771
[2019-03-26 11:29:34,975] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 90.0, 1.0, 2.0, 0.4197096847897979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614915.0034182948, 614915.0034182948, 175617.2591068219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1965600.0000, 
sim time next is 1966200.0000, 
raw observation next is [23.88333333333333, 90.5, 1.0, 2.0, 0.4203800822346568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617110.0769192826, 617110.0769192833, 175860.9795401179], 
processed observation next is [1.0, 0.782608695652174, 0.3309636650868877, 0.905, 1.0, 1.0, 0.3016627496803094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17141946581091183, 0.17141946581091203, 0.2624790739404745], 
reward next is 0.7375, 
noisyNet noise sample is [array([2.2557716], dtype=float32), -0.08300426]. 
=============================================
[2019-03-26 11:29:39,671] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.397177e-32], sum to 1.0000
[2019-03-26 11:29:39,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2008
[2019-03-26 11:29:39,684] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.08333333333333, 90.66666666666666, 1.0, 2.0, 0.5076028214960224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709299.624672002, 709299.624672002, 184796.0986747059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2029800.0000, 
sim time next is 2030400.0000, 
raw observation next is [26.2, 90.0, 1.0, 2.0, 0.5083006152379825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710275.0138739177, 710275.0138739183, 184907.0869315164], 
processed observation next is [0.0, 0.5217391304347826, 0.44075829383886256, 0.9, 1.0, 1.0, 0.40759110269636445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19729861496497716, 0.1972986149649773, 0.2759807267634573], 
reward next is 0.7240, 
noisyNet noise sample is [array([0.980183], dtype=float32), -0.3506516]. 
=============================================
[2019-03-26 11:29:40,600] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:29:40,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9282
[2019-03-26 11:29:40,613] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 84.66666666666667, 1.0, 2.0, 0.5084988919999548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710552.1690202173, 710552.1690202173, 184938.6034889804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2044200.0000, 
sim time next is 2044800.0000, 
raw observation next is [26.9, 85.0, 1.0, 2.0, 0.5087270310955853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710871.0664116039, 710871.0664116046, 184974.9145333862], 
processed observation next is [0.0, 0.6956521739130435, 0.4739336492890995, 0.85, 1.0, 1.0, 0.40810485674166896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19746418511433442, 0.1974641851143346, 0.2760819619901287], 
reward next is 0.7239, 
noisyNet noise sample is [array([-0.10892263], dtype=float32), -0.56146806]. 
=============================================
[2019-03-26 11:29:42,987] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:29:42,999] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3664
[2019-03-26 11:29:43,008] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.83333333333333, 1.0, 2.0, 0.5619807788988245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785312.8399107563, 785312.8399107563, 193877.3503473001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2119800.0000, 
sim time next is 2120400.0000, 
raw observation next is [30.0, 76.0, 1.0, 2.0, 0.5622836267046358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785736.1964479298, 785736.1964479298, 193930.428104229], 
processed observation next is [0.0, 0.5652173913043478, 0.6208530805687204, 0.76, 1.0, 1.0, 0.472630875547754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2182600545688694, 0.2182600545688694, 0.28944840015556567], 
reward next is 0.7106, 
noisyNet noise sample is [array([-0.4302207], dtype=float32), 1.519727]. 
=============================================
[2019-03-26 11:29:44,020] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:29:44,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7406
[2019-03-26 11:29:44,034] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.1, 76.66666666666667, 1.0, 2.0, 0.5696814698117408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796077.8302688397, 796077.8302688404, 195234.1858322718], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2125200.0000, 
sim time next is 2125800.0000, 
raw observation next is [30.15, 76.5, 1.0, 2.0, 0.5709930509376296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797911.3335308209, 797911.3335308209, 195466.9819729782], 
processed observation next is [0.0, 0.6086956521739131, 0.6279620853080569, 0.765, 1.0, 1.0, 0.4831241577561803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2216420370918947, 0.2216420370918947, 0.2917417641387734], 
reward next is 0.7083, 
noisyNet noise sample is [array([-0.5698285], dtype=float32), -0.3143002]. 
=============================================
[2019-03-26 11:29:47,307] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5301210e-25 1.0000000e+00 3.9655744e-31 3.2259082e-31 2.0021138e-09], sum to 1.0000
[2019-03-26 11:29:47,315] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9152
[2019-03-26 11:29:47,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1998121.644423944 W.
[2019-03-26 11:29:47,331] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.6, 76.0, 1.0, 2.0, 0.4763601818270994, 1.0, 2.0, 0.4763601818270994, 1.0, 2.0, 0.8272801011142698, 6.911200000000001, 6.9112, 170.5573041426782, 1998121.644423944, 1998121.644423944, 399183.1257506979], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2196000.0000, 
sim time next is 2196600.0000, 
raw observation next is [29.7, 75.5, 1.0, 2.0, 0.7544113102232289, 1.0, 2.0, 0.7544113102232289, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2109725.753699552, 2109725.753699552, 398288.8633212032], 
processed observation next is [1.0, 0.43478260869565216, 0.6066350710900474, 0.755, 1.0, 1.0, 0.7041100123171433, 1.0, 1.0, 0.7041100123171433, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5860349315832089, 0.5860349315832089, 0.5944609900316465], 
reward next is 0.4055, 
noisyNet noise sample is [array([0.1813645], dtype=float32), -0.25865987]. 
=============================================
[2019-03-26 11:29:51,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8618729e-28 1.4411735e-07 2.3187252e-32 6.3489364e-34 9.9999988e-01], sum to 1.0000
[2019-03-26 11:29:51,287] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4894
[2019-03-26 11:29:51,291] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.9, 83.33333333333333, 1.0, 2.0, 0.4471698983537586, 1.0, 2.0, 0.4471698983537586, 1.0, 2.0, 0.7720697339995389, 6.911200000000001, 6.9112, 170.5573041426782, 1875574.058587154, 1875574.058587154, 379817.7230704126], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2542200.0000, 
sim time next is 2542800.0000, 
raw observation next is [28.0, 82.66666666666667, 1.0, 2.0, 0.4784475570983374, 1.0, 2.0, 0.4784475570983374, 1.0, 2.0, 0.826221033611039, 6.9112, 6.9112, 170.5573041426782, 2006885.467406774, 2006885.467406774, 399756.9431585203], 
processed observation next is [1.0, 0.43478260869565216, 0.5260663507109005, 0.8266666666666667, 1.0, 1.0, 0.3716235627690812, 1.0, 1.0, 0.3716235627690812, 1.0, 1.0, 0.7880744312329744, 0.0, 0.0, 0.8375144448122397, 0.5574681853907706, 0.5574681853907706, 0.5966521539679408], 
reward next is 0.4033, 
noisyNet noise sample is [array([0.10652515], dtype=float32), 0.24439107]. 
=============================================
[2019-03-26 11:29:55,422] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.6766543e-19 4.4281282e-05 1.6029501e-29 1.2843919e-12 9.9995577e-01], sum to 1.0000
[2019-03-26 11:29:55,432] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4317
[2019-03-26 11:29:55,438] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.16666666666667, 65.33333333333334, 1.0, 2.0, 0.1776117174767418, 1.0, 2.0, 0.1776117174767418, 1.0, 2.0, 0.3084528161645755, 6.9112, 6.9112, 170.5573041426782, 744567.9043958422, 744567.9043958422, 263996.1404350233], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2308800.0000, 
sim time next is 2309400.0000, 
raw observation next is [32.05, 66.0, 1.0, 2.0, 0.1761603342886101, 1.0, 2.0, 0.1761603342886101, 1.0, 2.0, 0.3059322435465456, 6.9112, 6.9112, 170.5573041426782, 738481.4514491544, 738481.4514491544, 263641.0981025893], 
processed observation next is [1.0, 0.7391304347826086, 0.7180094786729857, 0.66, 1.0, 1.0, 0.007422089504349516, 1.0, 1.0, 0.007422089504349516, 1.0, 1.0, 0.15357590676407998, 0.0, 0.0, 0.8375144448122397, 0.205133736513654, 0.205133736513654, 0.39349417627252137], 
reward next is 0.6065, 
noisyNet noise sample is [array([-1.3248637], dtype=float32), -0.4631535]. 
=============================================
[2019-03-26 11:29:58,654] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6030865e-33], sum to 1.0000
[2019-03-26 11:29:58,663] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9904
[2019-03-26 11:29:58,673] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3957938915681671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590582.9688820627, 590582.9688820634, 173671.1171624665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2670000.0000, 
sim time next is 2670600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3956961589567615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590437.1791572652, 590437.1791572652, 173657.7319343442], 
processed observation next is [0.0, 0.9130434782608695, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27192308308043556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16401032754368478, 0.16401032754368478, 0.25919064467812564], 
reward next is 0.7408, 
noisyNet noise sample is [array([1.006203], dtype=float32), -0.5344165]. 
=============================================
[2019-03-26 11:30:01,743] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:30:01,752] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1722
[2019-03-26 11:30:01,759] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.38333333333333, 79.66666666666667, 1.0, 2.0, 0.5723932036697988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 799868.6575057631, 799868.6575057624, 195715.363803345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2415000.0000, 
sim time next is 2415600.0000, 
raw observation next is [29.3, 80.0, 1.0, 2.0, 0.5726442381513778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800219.5880377964, 800219.5880377964, 195759.9957346452], 
processed observation next is [1.0, 1.0, 0.5876777251184835, 0.8, 1.0, 1.0, 0.485113539941419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2222832188993879, 0.2222832188993879, 0.29217909811141074], 
reward next is 0.7078, 
noisyNet noise sample is [array([-0.72033536], dtype=float32), 1.125098]. 
=============================================
[2019-03-26 11:30:14,460] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:30:14,463] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0098
[2019-03-26 11:30:14,473] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 79.83333333333334, 1.0, 2.0, 0.555384375877018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776091.6482457706, 776091.6482457699, 192728.2716671492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3352200.0000, 
sim time next is 3352800.0000, 
raw observation next is [28.66666666666667, 80.66666666666667, 1.0, 2.0, 0.553952466700862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774089.9756430094, 774089.9756430094, 192480.7351894168], 
processed observation next is [0.0, 0.8260869565217391, 0.5576619273301741, 0.8066666666666668, 1.0, 1.0, 0.46259333337453246, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2150249932341693, 0.2150249932341693, 0.28728467938718927], 
reward next is 0.7127, 
noisyNet noise sample is [array([-0.7303257], dtype=float32), -1.0686649]. 
=============================================
[2019-03-26 11:30:16,535] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:30:16,543] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4944
[2019-03-26 11:30:16,549] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3977482839684885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 593474.40523567, 593474.40523567, 173936.5160771536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2664600.0000, 
sim time next is 2665200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3971945146211823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592670.2684337221, 592670.2684337215, 173863.0354437851], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27372833086889437, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16463063012047835, 0.16463063012047818, 0.2594970678265449], 
reward next is 0.7405, 
noisyNet noise sample is [array([-0.6350622], dtype=float32), 0.5255284]. 
=============================================
[2019-03-26 11:30:17,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7479245e-23 9.9992251e-01 4.1917777e-27 1.0555253e-21 7.7472330e-05], sum to 1.0000
[2019-03-26 11:30:17,139] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3339
[2019-03-26 11:30:17,153] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5893841972077076, 1.0, 1.0, 0.5893841972077076, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1647869.360069792, 1647869.360069792, 330372.4475976713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3398400.0000, 
sim time next is 3399000.0000, 
raw observation next is [29.16666666666667, 82.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 9.03242875469138, 6.9112, 168.901083799489, 2959477.840988366, 1454710.931241505, 309195.9528145814], 
processed observation next is [1.0, 0.34782608695652173, 0.581358609794629, 0.825, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.21212287546913808, 0.0, 0.8293816447062268, 0.8220771780523238, 0.40408636978930695, 0.4614864967381812], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6525208], dtype=float32), 0.71868676]. 
=============================================
[2019-03-26 11:30:17,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[56.210026]
 [57.109734]
 [58.443356]
 [59.518044]
 [59.609573]], R is [[51.07437134]
 [51.07053757]
 [50.55983353]
 [50.42894363]
 [50.52617264]].
[2019-03-26 11:30:22,810] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:30:22,819] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6970
[2019-03-26 11:30:22,822] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.6256001721435213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 963476.623504942, 963476.6235049415, 216502.4008818207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2797200.0000, 
sim time next is 2797800.0000, 
raw observation next is [22.0, 93.00000000000001, 1.0, 2.0, 0.5581851461648872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861713.3100151246, 861713.3100151246, 203000.1012405371], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.9300000000000002, 1.0, 1.0, 0.4676929471866111, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23936480833753462, 0.23936480833753462, 0.3029852257321449], 
reward next is 0.6970, 
noisyNet noise sample is [array([-0.59603614], dtype=float32), 0.9857213]. 
=============================================
[2019-03-26 11:30:23,033] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:30:23,044] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1928
[2019-03-26 11:30:23,049] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 87.16666666666667, 1.0, 2.0, 0.5352413794238481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840160.105262556, 840160.105262556, 200026.4735460519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2801400.0000, 
sim time next is 2802000.0000, 
raw observation next is [22.33333333333334, 86.33333333333334, 1.0, 2.0, 0.5305773171708572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 831678.084157785, 831678.0841577856, 199045.1700357139], 
processed observation next is [1.0, 0.43478260869565216, 0.2575039494470777, 0.8633333333333334, 1.0, 1.0, 0.4344305026154906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2310216900438292, 0.23102169004382933, 0.29708234333688643], 
reward next is 0.7029, 
noisyNet noise sample is [array([2.171363], dtype=float32), -0.17415062]. 
=============================================
[2019-03-26 11:30:23,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[69.865204]
 [70.25608 ]
 [69.82889 ]
 [69.87858 ]
 [69.905174]], R is [[69.8061676 ]
 [69.80956268]
 [69.83048248]
 [69.83511353]
 [69.83908081]].
[2019-03-26 11:30:23,075] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:30:23,087] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3278
[2019-03-26 11:30:23,095] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3394014927768481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522844.8595298342, 522844.8595298342, 168426.1075300047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2788800.0000, 
sim time next is 2789400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3401083283345756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 523933.2991944313, 523933.2991944307, 168513.0917043421], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 1.0, 1.0, 0.20494979317418746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14553702755400869, 0.14553702755400852, 0.2515120771706599], 
reward next is 0.7485, 
noisyNet noise sample is [array([-0.18891153], dtype=float32), 1.217289]. 
=============================================
[2019-03-26 11:30:24,750] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 11:30:24,751] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:30:24,752] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:30:24,752] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:30:24,754] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:30:24,754] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:30:24,755] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:30:24,756] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:30:24,755] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:30:24,757] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:30:24,758] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:30:24,781] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run38
[2019-03-26 11:30:24,802] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run38
[2019-03-26 11:30:24,803] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run38
[2019-03-26 11:30:24,803] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run38
[2019-03-26 11:30:24,859] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run38
[2019-03-26 11:31:39,285] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.16735716], dtype=float32), 0.3910659]
[2019-03-26 11:31:39,287] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.92337411333333, 84.53740143, 1.0, 2.0, 0.522715608341942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730424.7624074557, 730424.7624074551, 187230.8366571423]
[2019-03-26 11:31:39,289] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:31:39,292] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.919794e-33], sampled 0.10793484087380079
[2019-03-26 11:31:46,073] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16735716], dtype=float32), 0.3910659]
[2019-03-26 11:31:46,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.83333333333334, 50.33333333333334, 1.0, 2.0, 0.8352747996142865, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005983714494212, 6.9112, 168.9123461961102, 2064443.009187004, 1997200.422882126, 416747.3494308904]
[2019-03-26 11:31:46,079] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:31:46,082] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9654401e-30 1.0000000e+00 1.2411053e-32 3.2995517e-30 1.2702322e-13], sampled 0.006789630792149559
[2019-03-26 11:31:46,086] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2064443.009187004 W.
[2019-03-26 11:32:01,696] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16735716], dtype=float32), 0.3910659]
[2019-03-26 11:32:01,696] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.73258322666667, 72.02570938666666, 1.0, 2.0, 0.5177615171068364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723499.7286405975, 723499.7286405981, 186424.8212100072]
[2019-03-26 11:32:01,699] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:32:01,700] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3906176e-34], sampled 0.4665879417880122
[2019-03-26 11:32:21,064] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8117.3396 3004361595.0192 1471.0000
[2019-03-26 11:32:21,408] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8564.2142 2841329199.4750 974.0000
[2019-03-26 11:32:21,429] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8333.0150 2926218077.2480 1160.0000
[2019-03-26 11:32:21,472] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8712.6489 2779335375.0727 807.0000
[2019-03-26 11:32:21,537] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7983.6968 3159707348.8305 1600.0000
[2019-03-26 11:32:22,553] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 925000, evaluation results [925000.0, 7983.696789114612, 3159707348.830462, 1600.0, 8333.014984624257, 2926218077.2480392, 1160.0, 8712.648943002026, 2779335375.0726933, 807.0, 8117.3395912018495, 3004361595.0192366, 1471.0, 8564.214195167902, 2841329199.475018, 974.0]
[2019-03-26 11:32:24,844] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.3496693e-33 2.8877392e-31], sum to 1.0000
[2019-03-26 11:32:24,856] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9665
[2019-03-26 11:32:24,860] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3481086805393849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 536252.434237533, 536252.4342375337, 169509.2681583941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2857200.0000, 
sim time next is 2857800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3479205549480015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535962.7626764473, 535962.7626764467, 169485.5977016161], 
processed observation next is [1.0, 0.043478260869565216, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21436211439518252, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14887854518790203, 0.14887854518790186, 0.2529635786591285], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.54594904], dtype=float32), 0.95761657]. 
=============================================
[2019-03-26 11:32:30,742] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:32:30,749] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5982
[2019-03-26 11:32:30,755] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3107188938489137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491345.1982242824, 491345.1982242829, 166382.8351046422], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2957400.0000, 
sim time next is 2958000.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3083897449299289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 487664.5646585511, 487664.5646585504, 166112.7953484463], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 0.94, 1.0, 1.0, 0.16673463244569744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13546237907181977, 0.13546237907181954, 0.2479295452961885], 
reward next is 0.7521, 
noisyNet noise sample is [array([0.06872502], dtype=float32), -0.5888011]. 
=============================================
[2019-03-26 11:32:30,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.50271 ]
 [73.435486]
 [73.30639 ]
 [73.37696 ]
 [73.25538 ]], R is [[73.60726929]
 [73.6228714 ]
 [73.63821411]
 [73.64942932]
 [73.66432953]].
[2019-03-26 11:32:33,215] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.315293e-33], sum to 1.0000
[2019-03-26 11:32:33,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4017
[2019-03-26 11:32:33,232] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3015956000108688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480274.6810608006, 480274.6810608006, 165641.5433232328], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3027600.0000, 
sim time next is 3028200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3019530957307914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480844.2237597334, 480844.2237597327, 165682.3570734033], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15897963341059201, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13356783993325927, 0.13356783993325907, 0.24728710010955718], 
reward next is 0.7527, 
noisyNet noise sample is [array([0.47260922], dtype=float32), 0.8035918]. 
=============================================
[2019-03-26 11:32:35,102] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6420576e-35 1.4952191e-27], sum to 1.0000
[2019-03-26 11:32:35,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0993
[2019-03-26 11:32:35,117] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 74.83333333333334, 1.0, 2.0, 0.6414056247883516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 917541.7052253456, 917541.7052253456, 211475.3498743518], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3730200.0000, 
sim time next is 3730800.0000, 
raw observation next is [26.66666666666667, 75.66666666666667, 1.0, 2.0, 0.6117656248956994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876283.5734749376, 876283.5734749376, 205718.1153941139], 
processed observation next is [1.0, 0.17391304347826086, 0.4628751974723541, 0.7566666666666667, 1.0, 1.0, 0.532247740838192, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2434121037430382, 0.2434121037430382, 0.3070419632747969], 
reward next is 0.6930, 
noisyNet noise sample is [array([-0.18482432], dtype=float32), -2.148616]. 
=============================================
[2019-03-26 11:32:37,378] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4634994e-26 4.9849111e-21 8.7672468e-33 3.1009791e-22 1.0000000e+00], sum to 1.0000
[2019-03-26 11:32:37,387] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3647
[2019-03-26 11:32:37,392] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.16666666666667, 60.0, 1.0, 2.0, 0.6625502497621062, 1.0, 2.0, 0.6518651643953157, 1.0, 2.0, 1.03, 7.005094779463843, 6.9112, 170.5573041426782, 2735132.671500773, 2667872.02157539, 509761.2878213858], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3768600.0000, 
sim time next is 3769200.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.6131831978265395, 1.0, 2.0, 0.6131831978265395, 1.0, 2.0, 1.03, 6.950429741328028, 6.9112, 170.5573041426782, 2572661.585035996, 2544559.725558879, 492808.8337729169], 
processed observation next is [1.0, 0.6521739130434783, 0.8104265402843602, 0.6, 1.0, 1.0, 0.5339556600319753, 1.0, 1.0, 0.5339556600319753, 1.0, 1.0, 1.0365853658536586, 0.0039229741328028036, 0.0, 0.8375144448122397, 0.7146282180655544, 0.7068221459885775, 0.7355355727953984], 
reward next is 0.0683, 
noisyNet noise sample is [array([-2.2298765], dtype=float32), -0.27824247]. 
=============================================
[2019-03-26 11:32:46,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.248795e-37 7.876745e-31], sum to 1.0000
[2019-03-26 11:32:46,295] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6926
[2019-03-26 11:32:46,303] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4566762021398829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646602.4204941794, 646602.4204941794, 178181.9071402132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3216600.0000, 
sim time next is 3217200.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4566583325064311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646576.9607043526, 646576.9607043526, 178179.2799175668], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3453714849475074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17960471130676461, 0.17960471130676461, 0.2659392237575624], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.7455138], dtype=float32), 0.14270578]. 
=============================================
[2019-03-26 11:32:47,322] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:32:47,333] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3734
[2019-03-26 11:32:47,338] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.83333333333334, 1.0, 2.0, 0.5266959904319644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735988.7379402305, 735988.7379402305, 187884.2469297457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3271800.0000, 
sim time next is 3272400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5223146229001158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729864.2466818137, 729864.2466818132, 187165.8974737502], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4244754492772479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20274006852272602, 0.20274006852272589, 0.2793520857817167], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.8076728], dtype=float32), -0.23780276]. 
=============================================
[2019-03-26 11:32:47,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 9.318292e-30], sum to 1.0000
[2019-03-26 11:32:47,590] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0307
[2019-03-26 11:32:47,595] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.597699948860203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835246.4905922757, 835246.4905922757, 200318.4407519266], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3261600.0000, 
sim time next is 3262200.0000, 
raw observation next is [30.66666666666667, 75.66666666666667, 1.0, 2.0, 0.6046156823311563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844914.6186266005, 844914.6186266005, 201608.1970021895], 
processed observation next is [0.0, 0.782608695652174, 0.6524486571879939, 0.7566666666666667, 1.0, 1.0, 0.5236333522062123, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2346985051740557, 0.2346985051740557, 0.30090775671968584], 
reward next is 0.6991, 
noisyNet noise sample is [array([0.3874736], dtype=float32), 1.5819262]. 
=============================================
[2019-03-26 11:32:55,301] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.5636105e-37 4.6997653e-29], sum to 1.0000
[2019-03-26 11:32:55,311] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1448
[2019-03-26 11:32:55,319] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 88.16666666666667, 1.0, 2.0, 0.8604299167453348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1202602.283125204, 1202602.283125204, 259287.6254139024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3395400.0000, 
sim time next is 3396000.0000, 
raw observation next is [28.33333333333334, 87.33333333333334, 1.0, 2.0, 0.8712014140019531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217665.979690903, 1217665.979690903, 262141.6740370762], 
processed observation next is [1.0, 0.30434782608695654, 0.5418641390205374, 0.8733333333333334, 1.0, 1.0, 0.8448209807252446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33824054991413977, 0.33824054991413977, 0.3912562299060839], 
reward next is 0.6087, 
noisyNet noise sample is [array([-2.130448], dtype=float32), 0.47912842]. 
=============================================
[2019-03-26 11:32:55,337] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[60.69184 ]
 [60.603535]
 [60.700127]
 [60.956184]
 [61.28431 ]], R is [[60.37889862]
 [60.38811111]
 [60.3973465 ]
 [60.41419983]
 [60.4408989 ]].
[2019-03-26 11:32:58,979] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.931752e-34], sum to 1.0000
[2019-03-26 11:32:58,984] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0093
[2019-03-26 11:32:58,990] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5113403676923621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714524.0474111352, 714524.0474111352, 185391.8735271151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3448800.0000, 
sim time next is 3449400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5115172011573851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714771.2296816761, 714771.2296816761, 185420.167520429], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41146650741853624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1985475638004656, 0.1985475638004656, 0.27674651868720745], 
reward next is 0.7233, 
noisyNet noise sample is [array([0.5846749], dtype=float32), -1.5554148]. 
=============================================
[2019-03-26 11:33:00,706] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4599956e-37 1.0000000e+00 0.0000000e+00 9.1487146e-34 3.2039736e-20], sum to 1.0000
[2019-03-26 11:33:00,713] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5154
[2019-03-26 11:33:00,718] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.6895956485255197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 963722.9692951301, 963722.9692951301, 218612.0964300838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3474600.0000, 
sim time next is 3475200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6215309164514117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868562.3220550605, 868562.3220550605, 204816.1581560592], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5440131523510984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24126731168196125, 0.24126731168196125, 0.3056957584418794], 
reward next is 0.6943, 
noisyNet noise sample is [array([-0.25112945], dtype=float32), -0.8215145]. 
=============================================
[2019-03-26 11:33:12,135] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5086296e-37], sum to 1.0000
[2019-03-26 11:33:12,145] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2202
[2019-03-26 11:33:12,149] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 58.0, 1.0, 2.0, 0.6138748972006653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857859.0486237034, 857859.0486237034, 203359.9688358732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3853800.0000, 
sim time next is 3854400.0000, 
raw observation next is [35.0, 57.33333333333333, 1.0, 2.0, 0.6093395870336394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851518.6419786054, 851518.6419786054, 202499.3785488599], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5733333333333333, 1.0, 1.0, 0.5293248036549872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23653295610516817, 0.23653295610516817, 0.3022378784311342], 
reward next is 0.6978, 
noisyNet noise sample is [array([-1.5430764], dtype=float32), -2.2115176]. 
=============================================
[2019-03-26 11:33:15,816] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7929550e-20 9.9997866e-01 3.4065668e-24 3.7363080e-22 2.1301739e-05], sum to 1.0000
[2019-03-26 11:33:15,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7208
[2019-03-26 11:33:15,833] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.33333333333334, 64.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 9.916856656155984, 6.9112, 168.895469675574, 4417742.249381026, 2285646.720356109, 467701.7346949253], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3763200.0000, 
sim time next is 3763800.0000, 
raw observation next is [34.5, 63.5, 1.0, 2.0, 1.04, 1.0, 1.0, 0.8534443519317834, 1.0, 2.0, 1.03, 7.055338374342462, 6.9112, 170.5573041426782, 3582143.026379145, 3478890.845663439, 653296.4482718967], 
processed observation next is [1.0, 0.5652173913043478, 0.8341232227488152, 0.635, 1.0, 1.0, 1.0481927710843375, 1.0, 0.5, 0.8234269300382931, 1.0, 1.0, 1.0365853658536586, 0.01441383743424618, 0.0, 0.8375144448122397, 0.9950397295497626, 0.9663585682398441, 0.9750693257789503], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.866223], dtype=float32), -2.5890524]. 
=============================================
[2019-03-26 11:33:16,642] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 11:33:16,645] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:33:16,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:33:16,647] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:33:16,649] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:33:16,650] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:33:16,650] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:33:16,651] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:33:16,653] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:33:16,650] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:33:16,655] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:33:16,677] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run39
[2019-03-26 11:33:16,677] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run39
[2019-03-26 11:33:16,678] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run39
[2019-03-26 11:33:16,737] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run39
[2019-03-26 11:33:16,762] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run39
[2019-03-26 11:33:31,463] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20868237], dtype=float32), 0.37133554]
[2019-03-26 11:33:31,464] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.36666666666667, 78.33333333333334, 1.0, 2.0, 0.302138093373767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 484026.4024606326, 484026.4024606332, 165941.9087456423]
[2019-03-26 11:33:31,465] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:33:31,470] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.46945940755310156
[2019-03-26 11:33:43,695] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.20868237], dtype=float32), 0.37133554]
[2019-03-26 11:33:43,697] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.50936673666667, 88.42090359333334, 1.0, 2.0, 0.3251135414594545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514420.9508270748, 514420.9508270748, 168123.5321494131]
[2019-03-26 11:33:43,699] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:33:43,701] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07356296731446932
[2019-03-26 11:33:55,376] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.20868237], dtype=float32), 0.37133554]
[2019-03-26 11:33:55,377] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.21052488333334, 92.35105885666667, 1.0, 2.0, 0.5572843728899248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778747.668870426, 778747.668870426, 193058.1962444227]
[2019-03-26 11:33:55,378] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:33:55,381] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06639879099457335
[2019-03-26 11:34:12,773] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.20868237], dtype=float32), 0.37133554]
[2019-03-26 11:34:12,774] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.7, 43.5, 1.0, 2.0, 0.6857749804423674, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005974395279321, 6.9112, 168.912316047834, 1855223.145902997, 1787987.182946613, 381589.2310257309]
[2019-03-26 11:34:12,775] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:34:12,777] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0925476e-23 1.0000000e+00 5.0702942e-28 1.6928061e-20 1.0185371e-12], sampled 0.22402599519320976
[2019-03-26 11:34:12,778] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1855223.145902997 W.
[2019-03-26 11:34:45,954] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20868237], dtype=float32), 0.37133554]
[2019-03-26 11:34:45,956] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.29783857, 96.27823767999999, 1.0, 2.0, 0.7235884940964579, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976756363026, 6.9112, 168.9123160362026, 1908137.962224026, 1840900.32424458, 389838.4526236108]
[2019-03-26 11:34:45,957] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:34:45,960] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.9317994e-33 1.0000000e+00 9.4200825e-37 2.6614881e-38 1.6985530e-24], sampled 0.3911547511416431
[2019-03-26 11:34:45,961] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1908137.962224026 W.
[2019-03-26 11:34:47,104] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.20868237], dtype=float32), 0.37133554]
[2019-03-26 11:34:47,105] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.76098505833333, 60.530971635, 1.0, 2.0, 0.5933635835726637, 0.0, 2.0, 0.0, 1.0, 1.0, 1.012448127152387, 6.911200000000001, 6.9112, 168.9124356731488, 1659016.511468141, 1659016.51146814, 359479.8342600444]
[2019-03-26 11:34:47,106] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:34:47,108] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1667105e-27 1.0000000e+00 8.4021054e-31 6.6437397e-26 2.5555236e-19], sampled 0.36537114282256056
[2019-03-26 11:34:47,109] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1659016.511468141 W.
[2019-03-26 11:35:08,913] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20868237], dtype=float32), 0.37133554]
[2019-03-26 11:35:08,914] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.5, 85.0, 1.0, 2.0, 0.5533983537455419, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9442838858008701, 6.911199999999999, 6.9112, 168.9129563608125, 1547194.170970564, 1547194.170970564, 335178.704908529]
[2019-03-26 11:35:08,915] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:35:08,918] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7108576e-32 1.0000000e+00 8.9860997e-37 8.9246264e-33 4.9552321e-24], sampled 0.1674293421894827
[2019-03-26 11:35:13,030] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8656.9221 2779828976.7855 942.0000
[2019-03-26 11:35:13,415] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.4118 3164835405.9443 1840.0000
[2019-03-26 11:35:13,425] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7994.7946 3007921497.4313 1768.0000
[2019-03-26 11:35:13,449] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.0173 2927779323.9933 1347.0000
[2019-03-26 11:35:13,542] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.8022 2842660799.5234 1137.0000
[2019-03-26 11:35:14,558] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 950000, evaluation results [950000.0, 7876.411815709983, 3164835405.944335, 1840.0, 8249.017300979218, 2927779323.9933367, 1347.0, 8656.922117562091, 2779828976.785468, 942.0, 7994.79456392671, 3007921497.43128, 1768.0, 8493.802190946844, 2842660799.5233626, 1137.0]
[2019-03-26 11:35:15,706] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:35:15,714] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9327
[2019-03-26 11:35:15,718] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6284551505297877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 878242.6375546553, 878242.6375546553, 206167.8005014142], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3956400.0000, 
sim time next is 3957000.0000, 
raw observation next is [32.0, 74.33333333333333, 1.0, 2.0, 0.672243369778756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 939462.105483694, 939462.1054836946, 214975.247298513], 
processed observation next is [0.0, 0.8260869565217391, 0.7156398104265403, 0.7433333333333333, 1.0, 1.0, 0.6051124937093445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26096169596769275, 0.2609616959676929, 0.3208585780574821], 
reward next is 0.6791, 
noisyNet noise sample is [array([-0.52571845], dtype=float32), -0.6969573]. 
=============================================
[2019-03-26 11:35:15,735] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.06314 ]
 [70.95562 ]
 [70.90661 ]
 [70.82636 ]
 [70.734764]], R is [[70.79751587]
 [70.7818222 ]
 [70.76812744]
 [70.75632477]
 [70.7460556 ]].
[2019-03-26 11:35:18,137] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:35:18,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3375
[2019-03-26 11:35:18,150] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5358341797314213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748762.6625055039, 748762.6625055044, 189401.0652399772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3822600.0000, 
sim time next is 3823200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5357262742093543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748611.8246158515, 748611.8246158515, 189383.0216842303], 
processed observation next is [0.0, 0.2608695652173913, 0.4786729857819906, 0.89, 1.0, 1.0, 0.440634065312475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20794772905995876, 0.20794772905995876, 0.2826612263943736], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.13096718], dtype=float32), 0.30459306]. 
=============================================
[2019-03-26 11:35:20,773] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:35:20,778] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1533
[2019-03-26 11:35:20,784] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 55.16666666666667, 1.0, 2.0, 0.5886267292396848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 822562.3514021109, 822562.3514021116, 198646.955114458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3858600.0000, 
sim time next is 3859200.0000, 
raw observation next is [35.0, 55.0, 1.0, 2.0, 0.5874360757035686, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820897.8581129654, 820897.8581129654, 198429.3402831675], 
processed observation next is [0.0, 0.6956521739130435, 0.8578199052132701, 0.55, 1.0, 1.0, 0.502935030968155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22802718280915704, 0.22802718280915704, 0.2961631944524888], 
reward next is 0.7038, 
noisyNet noise sample is [array([-0.10545263], dtype=float32), 0.01781653]. 
=============================================
[2019-03-26 11:35:21,134] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:35:21,142] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4813
[2019-03-26 11:35:21,152] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 59.33333333333333, 1.0, 2.0, 0.6260128455368718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874828.201770172, 874828.201770172, 205692.8945068055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3852600.0000, 
sim time next is 3853200.0000, 
raw observation next is [35.0, 58.66666666666667, 1.0, 2.0, 0.6194142991590283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865603.2364676001, 865603.2364676001, 204419.2682650468], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5866666666666667, 1.0, 1.0, 0.5414630110349739, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24044534346322224, 0.24044534346322224, 0.3051033854702191], 
reward next is 0.6949, 
noisyNet noise sample is [array([-0.11662849], dtype=float32), 0.33649087]. 
=============================================
[2019-03-26 11:35:24,781] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:35:24,792] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7499
[2019-03-26 11:35:24,797] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5942153171896634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830375.0427994833, 830375.0427994839, 199674.0139918176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3938400.0000, 
sim time next is 3939000.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.6102890567413877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852846.0065062258, 852846.0065062258, 202678.21304662], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5304687430619128, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2369016684739516, 0.2369016684739516, 0.30250479559197013], 
reward next is 0.6975, 
noisyNet noise sample is [array([1.8495338], dtype=float32), 0.6805456]. 
=============================================
[2019-03-26 11:35:24,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.04485]
 [76.86462]
 [76.74212]
 [76.61348]
 [76.50147]], R is [[76.9950943 ]
 [76.92712402]
 [76.85974121]
 [76.79290771]
 [76.72647095]].
[2019-03-26 11:35:36,132] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:35:36,140] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2175
[2019-03-26 11:35:36,145] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.582306080816264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 813726.3289594805, 813726.3289594812, 197495.79698268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4146000.0000, 
sim time next is 4146600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5812969940711078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812315.6714570721, 812315.6714570721, 197313.3088154392], 
processed observation next is [1.0, 1.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4955385470736238, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22564324207140893, 0.22564324207140893, 0.2944974758439391], 
reward next is 0.7055, 
noisyNet noise sample is [array([0.69490707], dtype=float32), -0.5379008]. 
=============================================
[2019-03-26 11:35:40,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.4607940e-24 1.0000000e+00 8.4238923e-32 5.1452290e-21 1.4375897e-16], sum to 1.0000
[2019-03-26 11:35:40,210] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8388
[2019-03-26 11:35:40,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2180085.911317334 W.
[2019-03-26 11:35:40,221] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [36.0, 53.0, 1.0, 2.0, 0.7795456376518376, 1.0, 2.0, 0.7795456376518376, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2180085.911317334, 2180085.911317334, 410047.9340336172], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4192800.0000, 
sim time next is 4193400.0000, 
raw observation next is [36.0, 53.0, 1.0, 2.0, 0.9262165176686732, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005989366352042, 6.9112, 168.9123931523258, 2191732.941269308, 2124486.326662462, 441434.1421611226], 
processed observation next is [1.0, 0.5217391304347826, 0.9052132701421801, 0.53, 1.0, 1.0, 0.911104238155028, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00947893663520416, 0.0, 0.8294371788060997, 0.608814705908141, 0.5901350907395728, 0.6588569285986904], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.25113034], dtype=float32), -0.4863387]. 
=============================================
[2019-03-26 11:35:44,976] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2655982e-19 1.0000000e+00 1.2331534e-17 8.4193641e-24 1.7880548e-09], sum to 1.0000
[2019-03-26 11:35:44,980] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0939
[2019-03-26 11:35:44,987] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2864670.58087296 W.
[2019-03-26 11:35:44,995] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.5, 73.0, 1.0, 2.0, 1.024053935427149, 1.0, 2.0, 1.024053935427149, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2864670.58087296, 2864670.58087296, 543707.5445144436], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4267800.0000, 
sim time next is 4268400.0000, 
raw observation next is [33.66666666666666, 72.33333333333333, 1.0, 2.0, 0.8434883152401798, 1.0, 2.0, 0.7423341971343523, 1.0, 1.0, 1.03, 7.005109048804892, 6.9112, 170.5573041426782, 3115201.098594157, 3047930.226959342, 570482.7848231135], 
processed observation next is [1.0, 0.391304347826087, 0.7946287519747232, 0.7233333333333333, 1.0, 1.0, 0.8114317051086503, 1.0, 1.0, 0.6895592736558461, 1.0, 0.5, 1.0365853658536586, 0.00939090488048917, 0.0, 0.8375144448122397, 0.865333638498377, 0.8466472852664839, 0.8514668430195723], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5416843], dtype=float32), -1.127833]. 
=============================================
[2019-03-26 11:35:48,571] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0325043e-26 1.0000000e+00 4.0955235e-27 2.7860119e-25 2.8545423e-16], sum to 1.0000
[2019-03-26 11:35:48,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5431
[2019-03-26 11:35:48,584] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.9722146920117997, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104095, 1358941.177359415, 1358941.177359416, 290579.7720966761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4337400.0000, 
sim time next is 4338000.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.9721585919537984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1358862.711696296, 1358862.711696296, 290563.1431270884], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.89, 1.0, 1.0, 0.9664561348840944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3774618643600822, 0.3774618643600822, 0.43367633302550507], 
reward next is 0.5663, 
noisyNet noise sample is [array([-0.7623759], dtype=float32), 0.32993788]. 
=============================================
[2019-03-26 11:35:48,750] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[41.934143]
 [41.567104]
 [40.12789 ]
 [40.486347]
 [40.93731 ]], R is [[42.26325607]
 [42.4069252 ]
 [42.53104019]
 [42.60985184]
 [42.18375397]].
[2019-03-26 11:35:49,098] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:35:49,107] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2950
[2019-03-26 11:35:49,115] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.83333333333334, 1.0, 2.0, 0.498187785142159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696139.1975420427, 696139.1975420427, 183312.3870787389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4525800.0000, 
sim time next is 4526400.0000, 
raw observation next is [28.0, 75.66666666666667, 1.0, 2.0, 0.5004519591862049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699304.0666094151, 699304.0666094158, 183666.8470487834], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.7566666666666667, 1.0, 1.0, 0.398134890585789, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19425112961372643, 0.19425112961372662, 0.27412962246087075], 
reward next is 0.7259, 
noisyNet noise sample is [array([1.0509412], dtype=float32), 1.3967197]. 
=============================================
[2019-03-26 11:35:50,511] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5441913e-25 1.0000000e+00 1.7703674e-28 8.1496535e-25 1.8158199e-14], sum to 1.0000
[2019-03-26 11:35:50,517] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3512
[2019-03-26 11:35:50,524] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3111628.088919937 W.
[2019-03-26 11:35:50,529] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.33333333333334, 56.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.193276658075691, 6.9112, 170.5573041426782, 3111628.088919937, 2909565.10459548, 552169.6409850611], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4376400.0000, 
sim time next is 4377000.0000, 
raw observation next is [35.66666666666666, 56.83333333333333, 1.0, 2.0, 0.8754864806507591, 1.0, 2.0, 0.7583332798396423, 1.0, 1.0, 1.03, 7.005111573183497, 6.9112, 170.5573041426782, 3182426.700038602, 3115154.020088707, 582488.6483871844], 
processed observation next is [1.0, 0.6521739130434783, 0.889415481832543, 0.5683333333333332, 1.0, 1.0, 0.8499837116274206, 1.0, 1.0, 0.7088352769152316, 1.0, 0.5, 1.0365853658536586, 0.009391157318349741, 0.0, 0.8375144448122397, 0.8840074166773895, 0.865320561135752, 0.8693860423689319], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.14114761], dtype=float32), -0.54714453]. 
=============================================
[2019-03-26 11:35:50,546] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[40.3509  ]
 [40.983616]
 [40.986702]
 [41.231842]
 [41.312885]], R is [[39.33423233]
 [38.94089127]
 [38.7633667 ]
 [38.59248734]
 [38.20656204]].
[2019-03-26 11:35:51,793] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6438177e-16 9.9999976e-01 2.5284468e-21 4.1486651e-11 1.9082287e-07], sum to 1.0000
[2019-03-26 11:35:51,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5093
[2019-03-26 11:35:51,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2434977.725392828 W.
[2019-03-26 11:35:51,816] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.8705981493679982, 1.0, 1.0, 0.8705981493679982, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2434977.725392828, 2434977.725392828, 455700.8032755683], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4715400.0000, 
sim time next is 4716000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8497254511365688, 1.0, 2.0, 0.8497254511365688, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2376543.320193161, 2376543.320193161, 444802.0959379202], 
processed observation next is [1.0, 0.6086956521739131, 0.6682464454976303, 0.66, 1.0, 1.0, 0.8189463266705649, 1.0, 1.0, 0.8189463266705649, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.660150922275878, 0.660150922275878, 0.6638837252804779], 
reward next is 0.3361, 
noisyNet noise sample is [array([-0.2414184], dtype=float32), -0.40872264]. 
=============================================
[2019-03-26 11:35:51,840] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[38.698227]
 [39.91078 ]
 [41.684055]
 [42.9501  ]
 [44.62447 ]], R is [[39.3731575 ]
 [38.97942734]
 [38.58963394]
 [38.51651382]
 [38.13134766]].
[2019-03-26 11:36:01,581] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:36:01,592] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6899
[2019-03-26 11:36:01,598] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5224396684751482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730039.0410538552, 730039.0410538552, 187186.3100326881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4566000.0000, 
sim time next is 4566600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5222476749156969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729770.6637668267, 729770.6637668267, 187154.9695321051], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42439478905505645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20271407326856297, 0.20271407326856297, 0.2793357754210524], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.42038012], dtype=float32), 0.647905]. 
=============================================
[2019-03-26 11:36:03,647] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2836699e-27 3.3395300e-29 4.2054883e-29 1.1816639e-30 1.0000000e+00], sum to 1.0000
[2019-03-26 11:36:03,658] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3364
[2019-03-26 11:36:03,662] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.66666666666667, 76.5, 1.0, 2.0, 0.7141193858518537, 1.0, 2.0, 0.6776497324401894, 1.0, 2.0, 1.03, 7.005098845482821, 6.9112, 170.5573041426782, 2843444.14546915, 2776180.582888979, 525826.9639051358], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4611000.0000, 
sim time next is 4611600.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.7060254931359531, 1.0, 2.0, 0.673602786082239, 1.0, 2.0, 1.03, 7.00509820726531, 6.9112, 170.5573041426782, 2826443.792986277, 2759180.687587269, 523240.2622661399], 
processed observation next is [1.0, 0.391304347826087, 0.7156398104265403, 0.75, 1.0, 1.0, 0.6458138471517507, 1.0, 1.0, 0.6067503446773964, 1.0, 1.0, 1.0365853658536586, 0.009389820726531006, 0.0, 0.8375144448122397, 0.7851232758295213, 0.7664390798853525, 0.7809556153225968], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.44661996], dtype=float32), 1.4362103]. 
=============================================
[2019-03-26 11:36:04,649] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2910909e-20 9.9623591e-01 5.7485709e-25 4.1779068e-24 3.7640617e-03], sum to 1.0000
[2019-03-26 11:36:04,659] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3575
[2019-03-26 11:36:04,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2551229.031407085 W.
[2019-03-26 11:36:04,669] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 73.0, 1.0, 2.0, 0.6080800477307609, 1.0, 2.0, 0.6080800477307609, 1.0, 2.0, 1.03, 6.940466496088755, 6.9112, 170.5573041426782, 2551229.031407085, 2530264.249773764, 490928.105459802], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4617000.0000, 
sim time next is 4617600.0000, 
raw observation next is [32.0, 72.33333333333333, 1.0, 2.0, 0.6253253639406553, 1.0, 2.0, 0.6253253639406553, 1.0, 2.0, 1.03, 6.974136447868126, 6.9112, 170.5573041426782, 2623658.550556045, 2578574.612857089, 497345.0489187848], 
processed observation next is [1.0, 0.43478260869565216, 0.7156398104265403, 0.7233333333333333, 1.0, 1.0, 0.5485847758321148, 1.0, 1.0, 0.5485847758321148, 1.0, 1.0, 1.0365853658536586, 0.006293644786812625, 0.0, 0.8375144448122397, 0.7287940418211236, 0.7162707257936358, 0.7423060431623654], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02027072], dtype=float32), 0.47654158]. 
=============================================
[2019-03-26 11:36:06,248] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9578262e-34 1.0000000e+00 0.0000000e+00 1.3953729e-35 1.7507185e-26], sum to 1.0000
[2019-03-26 11:36:06,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6080
[2019-03-26 11:36:06,265] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7864823679441934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1099194.069068189, 1099194.069068189, 240619.8717489485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4681200.0000, 
sim time next is 4681800.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7798236279340122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1089882.984695318, 1089882.984695318, 239018.6264797775], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.734727262571099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3027452735264772, 0.3027452735264772, 0.3567442186265336], 
reward next is 0.6433, 
noisyNet noise sample is [array([-0.0165164], dtype=float32), 1.4564494]. 
=============================================
[2019-03-26 11:36:08,391] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4116736e-31 1.0000000e+00 6.4311192e-38 8.5252064e-32 8.8819758e-24], sum to 1.0000
[2019-03-26 11:36:08,399] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6365
[2019-03-26 11:36:08,406] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.83333333333333, 1.0, 2.0, 0.7809152526721664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1091409.425548674, 1091409.425548674, 239280.5506307053], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4679400.0000, 
sim time next is 4680000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7650871540744784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1069276.89129477, 1069276.89129477, 235521.5928027905], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.7169724747885281, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2970213586929917, 0.2970213586929917, 0.3515247653772992], 
reward next is 0.6485, 
noisyNet noise sample is [array([0.1453045], dtype=float32), -0.7871362]. 
=============================================
[2019-03-26 11:36:08,428] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[57.13424 ]
 [56.93481 ]
 [56.84128 ]
 [56.91819 ]
 [56.786118]], R is [[57.3354454 ]
 [57.40495682]
 [57.46718979]
 [57.52352142]
 [57.58030701]].
[2019-03-26 11:36:08,802] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 11:36:08,805] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:36:08,805] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:36:08,807] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:36:08,808] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:36:08,809] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:36:08,809] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:36:08,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:36:08,810] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:36:08,810] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:36:08,813] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:36:08,827] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run40
[2019-03-26 11:36:08,828] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run40
[2019-03-26 11:36:08,852] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run40
[2019-03-26 11:36:08,874] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run40
[2019-03-26 11:36:08,920] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run40
[2019-03-26 11:36:16,178] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12141059], dtype=float32), 0.46805423]
[2019-03-26 11:36:16,179] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.65, 83.5, 1.0, 2.0, 0.231066934285603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381818.5767483634, 381818.5767483634, 159035.0682385594]
[2019-03-26 11:36:16,181] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:36:16,184] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2234439698750158
[2019-03-26 11:36:19,434] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12141059], dtype=float32), 0.46805423]
[2019-03-26 11:36:19,435] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.7, 54.0, 1.0, 2.0, 0.6472894781032879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1062697.434414753, 1062697.434414753, 226150.7762930955]
[2019-03-26 11:36:19,435] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:36:19,437] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0388683e-37 1.0000000e+00 0.0000000e+00 2.4182648e-35 5.6155351e-29], sampled 0.7616099892908728
[2019-03-26 11:36:24,427] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.12141059], dtype=float32), 0.46805423]
[2019-03-26 11:36:24,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.56666666666667, 70.66666666666666, 1.0, 2.0, 0.2877399080799389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 468482.130972337, 468482.130972337, 164787.1905906526]
[2019-03-26 11:36:24,430] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:36:24,434] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33338448558337463
[2019-03-26 11:36:38,476] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.12141059], dtype=float32), 0.46805423]
[2019-03-26 11:36:38,477] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.7, 95.0, 1.0, 2.0, 0.4972551414677329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 694835.5482419465, 694835.5482419459, 183166.1301746223]
[2019-03-26 11:36:38,479] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:36:38,481] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4290332e-38], sampled 0.5776514321962825
[2019-03-26 11:36:52,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.12141059], dtype=float32), 0.46805423]
[2019-03-26 11:36:52,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.8527411, 100.0, 1.0, 2.0, 0.2523884178778384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 415810.7417701322, 415810.7417701322, 161126.9889417113]
[2019-03-26 11:36:52,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:36:52,669] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8197100660154681
[2019-03-26 11:37:09,902] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12141059], dtype=float32), 0.46805423]
[2019-03-26 11:37:09,902] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 0.5832576327811173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 815056.5571648519, 815056.5571648526, 197668.1565032419]
[2019-03-26 11:37:09,904] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:37:09,910] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0350077e-37], sampled 0.11386467843103176
[2019-03-26 11:37:29,275] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12141059], dtype=float32), 0.46805423]
[2019-03-26 11:37:29,276] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.35, 61.5, 1.0, 2.0, 0.7183532081549838, 1.0, 2.0, 0.6797666435917544, 1.0, 2.0, 1.03, 7.005099179333985, 6.9112, 170.5573041426782, 2852336.926730822, 2785073.124999481, 527190.5570327281]
[2019-03-26 11:37:29,278] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:37:29,282] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6210159e-23 9.9726267e-02 1.3232037e-31 1.5010977e-23 9.0027368e-01], sampled 0.8844332691327984
[2019-03-26 11:38:04,892] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8665.6096 2779715738.8950 920.0000
[2019-03-26 11:38:04,918] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7904.9399 3163981958.3083 1804.0000
[2019-03-26 11:38:05,071] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.4814 3007577071.3629 1735.0000
[2019-03-26 11:38:05,087] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.0457 2927986232.8518 1332.0000
[2019-03-26 11:38:05,119] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.2063 2842543599.2525 1132.0000
[2019-03-26 11:38:06,131] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 975000, evaluation results [975000.0, 7904.939883947922, 3163981958.3083334, 1804.0, 8259.04566428908, 2927986232.851835, 1332.0, 8665.609628310549, 2779715738.8950186, 920.0, 8002.481373534712, 3007577071.362932, 1735.0, 8498.206343246446, 2842543599.2524657, 1132.0]
[2019-03-26 11:38:08,436] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5555022e-22 9.4782877e-07 6.5604848e-30 2.3437899e-19 9.9999905e-01], sum to 1.0000
[2019-03-26 11:38:08,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3427
[2019-03-26 11:38:08,447] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5662035827107947, 1.0, 2.0, 0.5662035827107947, 1.0, 2.0, 0.9833083767825556, 6.9112, 6.9112, 170.5573041426782, 2375367.316592513, 2375367.316592513, 463945.6516272358], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4726800.0000, 
sim time next is 4727400.0000, 
raw observation next is [30.83333333333334, 70.0, 1.0, 2.0, 0.2561871253093548, 1.0, 2.0, 0.2561871253093548, 1.0, 2.0, 0.4449123142853757, 6.9112, 6.9112, 170.5573041426782, 1074129.641498495, 1074129.641498495, 287578.1815062871], 
processed observation next is [1.0, 0.7391304347826086, 0.6603475513428123, 0.7, 1.0, 1.0, 0.10383991001127081, 1.0, 1.0, 0.10383991001127081, 1.0, 1.0, 0.32306379790899475, 0.0, 0.0, 0.8375144448122397, 0.29836934486069305, 0.29836934486069305, 0.42922116642729413], 
reward next is 0.5708, 
noisyNet noise sample is [array([0.99123466], dtype=float32), 2.2101035]. 
=============================================
[2019-03-26 11:38:11,265] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1676076e-34 1.0000000e+00 1.2659622e-37 4.8921581e-33 3.3737832e-21], sum to 1.0000
[2019-03-26 11:38:11,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5877
[2019-03-26 11:38:11,276] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6414176210425633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 896364.8508104136, 896364.8508104136, 208705.0979931641], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4773600.0000, 
sim time next is 4774200.0000, 
raw observation next is [27.16666666666666, 79.83333333333334, 1.0, 2.0, 0.5919076776101134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827149.0195213107, 827149.0195213107, 199241.0098672827], 
processed observation next is [1.0, 0.2608695652173913, 0.4865718799368086, 0.7983333333333335, 1.0, 1.0, 0.5083225031447149, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22976361653369742, 0.22976361653369742, 0.29737464159295923], 
reward next is 0.7026, 
noisyNet noise sample is [array([1.8186971], dtype=float32), 1.4170904]. 
=============================================
[2019-03-26 11:38:11,889] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.2182456e-24 4.8850263e-10 7.8601245e-29 9.8020072e-26 1.0000000e+00], sum to 1.0000
[2019-03-26 11:38:11,898] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4508
[2019-03-26 11:38:11,906] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.4878635302258272, 1.0, 2.0, 0.4878635302258272, 1.0, 2.0, 0.8446983469324345, 6.911200000000001, 6.9112, 170.5573041426782, 2046419.222063641, 2046419.222063641, 406405.8379824949], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4792200.0000, 
sim time next is 4792800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5401606887457056, 1.0, 2.0, 0.5401606887457056, 1.0, 2.0, 0.9357972854122008, 6.9112, 6.9112, 170.5573041426782, 2266011.731067561, 2266011.731067561, 443580.3273370512], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44597673342856087, 1.0, 1.0, 0.44597673342856087, 1.0, 1.0, 0.9217040066002449, 0.0, 0.0, 0.8375144448122397, 0.6294477030743225, 0.6294477030743225, 0.6620601900553004], 
reward next is 0.3379, 
noisyNet noise sample is [array([-1.0725863], dtype=float32), 1.0002443]. 
=============================================
[2019-03-26 11:38:14,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5777850e-34 1.0000000e+00 0.0000000e+00 1.0194489e-34 3.4734812e-26], sum to 1.0000
[2019-03-26 11:38:14,133] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1702
[2019-03-26 11:38:14,140] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 87.33333333333333, 1.0, 2.0, 0.6417696448468935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 896857.0028543914, 896857.0028543914, 208777.9244044003], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4862400.0000, 
sim time next is 4863000.0000, 
raw observation next is [27.0, 88.16666666666667, 1.0, 2.0, 0.6440775226642074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900083.5709431359, 900083.5709431359, 209237.3864177582], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.8816666666666667, 1.0, 1.0, 0.5711777381496475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2500232141508711, 0.2500232141508711, 0.312294606593669], 
reward next is 0.6877, 
noisyNet noise sample is [array([1.7464346], dtype=float32), -0.95446056]. 
=============================================
[2019-03-26 11:38:14,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.881065]
 [66.94042 ]
 [66.986206]
 [67.08559 ]
 [67.16242 ]], R is [[66.9253006 ]
 [66.94444275]
 [66.96198273]
 [66.98184204]
 [66.99581146]].
[2019-03-26 11:38:15,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1306254e-33 1.0000000e+00 1.2034597e-36 2.3645978e-36 3.0168131e-25], sum to 1.0000
[2019-03-26 11:38:15,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1024
[2019-03-26 11:38:15,761] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 85.66666666666667, 1.0, 2.0, 0.6381011400471721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 891728.2070959712, 891728.2070959718, 208050.6939751662], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [27.0, 86.5, 1.0, 2.0, 0.6465221921074618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903501.3935972198, 903501.3935972198, 209724.8204450519], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.865, 1.0, 1.0, 0.5741231230210383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25097260933256105, 0.25097260933256105, 0.31302212006724167], 
reward next is 0.6870, 
noisyNet noise sample is [array([-2.1885872], dtype=float32), -0.33563662]. 
=============================================
[2019-03-26 11:38:15,768] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0629373e-28 1.0000000e+00 1.2619833e-31 2.2243690e-29 2.4798968e-14], sum to 1.0000
[2019-03-26 11:38:15,778] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0033
[2019-03-26 11:38:15,786] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.06666666666667, 84.66666666666667, 1.0, 2.0, 0.9078035022282398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1268854.773628794, 1268854.773628793, 272094.4613652963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5556000.0000, 
sim time next is 5556600.0000, 
raw observation next is [28.25, 84.0, 1.0, 2.0, 0.9526288836298518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1331547.379096038, 1331547.379096038, 284826.1462349914], 
processed observation next is [1.0, 0.30434782608695654, 0.537914691943128, 0.84, 1.0, 1.0, 0.9429263658190985, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.36987427197112166, 0.36987427197112166, 0.42511365109700205], 
reward next is 0.5749, 
noisyNet noise sample is [array([0.7704668], dtype=float32), -0.552873]. 
=============================================
[2019-03-26 11:38:19,353] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4454601e-36 1.0000000e+00 0.0000000e+00 1.7071384e-34 7.6361366e-27], sum to 1.0000
[2019-03-26 11:38:19,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5132
[2019-03-26 11:38:19,371] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 67.33333333333334, 1.0, 2.0, 0.4710857872773535, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129565104277, 658256.6779750518, 658256.6779750518, 179191.7411521341], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4900800.0000, 
sim time next is 4901400.0000, 
raw observation next is [29.5, 68.0, 1.0, 2.0, 0.4640639758638895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648441.9795204068, 648441.9795204068, 178159.3127153945], 
processed observation next is [1.0, 0.7391304347826086, 0.5971563981042655, 0.68, 1.0, 1.0, 0.3542939468239632, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1801227720890019, 0.1801227720890019, 0.2659094219632754], 
reward next is 0.7341, 
noisyNet noise sample is [array([0.11325282], dtype=float32), 1.277273]. 
=============================================
[2019-03-26 11:38:34,459] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7896504e-29 1.0000000e+00 1.7788189e-32 1.5933117e-26 5.8724596e-18], sum to 1.0000
[2019-03-26 11:38:34,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1146
[2019-03-26 11:38:34,479] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1786011.749504969 W.
[2019-03-26 11:38:34,484] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.379225645317952, 6.9112, 168.9108766384556, 1786011.749504969, 1453982.336937246, 311348.6666490386], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5191800.0000, 
sim time next is 5192400.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.4059217522686103, 1.0, 1.0, 0.4059217522686103, 1.0, 1.0, 0.6907182537257329, 6.9112, 6.9112, 170.5573041426782, 1702428.660038364, 1702428.660038364, 354139.5305545704], 
processed observation next is [1.0, 0.08695652173913043, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.2842430750224221, 1.0, 0.5, 0.2842430750224221, 1.0, 0.5, 0.6228271386899181, 0.0, 0.0, 0.8375144448122397, 0.4728968500106566, 0.4728968500106566, 0.5285664635142842], 
reward next is 0.4714, 
noisyNet noise sample is [array([-1.6987491], dtype=float32), 0.395843]. 
=============================================
[2019-03-26 11:38:34,555] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.80793627e-30 1.00000000e+00 4.74924513e-33 3.60840632e-26
 1.19578055e-14], sum to 1.0000
[2019-03-26 11:38:34,566] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5025
[2019-03-26 11:38:34,573] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.98333333333333, 94.16666666666667, 1.0, 2.0, 1.031517588784715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1441889.809772644, 1441889.809772644, 308675.2991374247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5883000.0000, 
sim time next is 5883600.0000, 
raw observation next is [25.96666666666667, 94.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.94598345494575, 6.9112, 168.9124660732522, 1478448.336140446, 1453771.827696021, 311349.5976433922], 
processed observation next is [1.0, 0.08695652173913043, 0.42969984202211703, 0.9433333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0034783454945750414, 0.0, 0.8294375368812317, 0.41068009337234607, 0.40382550769333914, 0.46470089200506304], 
reward next is 0.3614, 
noisyNet noise sample is [array([-0.9635663], dtype=float32), -0.20748225]. 
=============================================
[2019-03-26 11:38:34,902] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.3933078e-26 9.9966669e-01 6.3428504e-27 1.6425721e-16 3.3337667e-04], sum to 1.0000
[2019-03-26 11:38:34,912] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4351
[2019-03-26 11:38:34,917] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.516160055188965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721261.1485467441, 721261.1485467441, 186166.3600653783], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5187600.0000, 
sim time next is 5188200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5157352250759678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720667.3067852287, 720667.3067852287, 186097.8166697838], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4165484639469491, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20018536299589684, 0.20018536299589684, 0.2777579353280355], 
reward next is 0.7222, 
noisyNet noise sample is [array([-0.5055702], dtype=float32), 0.4992809]. 
=============================================
[2019-03-26 11:38:36,160] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.6834822e-18 9.9850994e-01 3.5620284e-20 9.6693833e-20 1.4900017e-03], sum to 1.0000
[2019-03-26 11:38:36,170] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8588
[2019-03-26 11:38:36,179] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2155462.632424626 W.
[2019-03-26 11:38:36,184] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.7707498025895431, 1.0, 2.0, 0.7707498025895431, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2155462.632424626, 2155462.632424626, 405879.2325447841], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5216400.0000, 
sim time next is 5217000.0000, 
raw observation next is [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.7185751019262686, 1.0, 2.0, 0.7185751019262686, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2009415.11617728, 2009415.11617728, 382175.9599052345], 
processed observation next is [1.0, 0.391304347826087, 0.6287519747235385, 0.6933333333333335, 1.0, 1.0, 0.6609338577424922, 1.0, 1.0, 0.6609338577424922, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5581708656048, 0.5581708656048, 0.5704118804555739], 
reward next is 0.4296, 
noisyNet noise sample is [array([-1.0082947], dtype=float32), 0.5230079]. 
=============================================
[2019-03-26 11:38:36,196] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[42.352932]
 [41.962833]
 [44.00017 ]
 [44.731052]
 [45.605984]], R is [[43.69924164]
 [43.65645981]
 [43.21989441]
 [42.78769684]
 [42.66143417]].
[2019-03-26 11:38:50,904] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5332209e-15 8.5381240e-01 1.6814625e-15 1.7214566e-21 1.4618756e-01], sum to 1.0000
[2019-03-26 11:38:50,908] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8411
[2019-03-26 11:38:50,916] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2737324.126209395 W.
[2019-03-26 11:38:50,923] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.76666666666667, 70.66666666666667, 1.0, 2.0, 0.6635936870087912, 1.0, 2.0, 0.6523868830186581, 1.0, 2.0, 1.03, 7.005094861727688, 6.9112, 170.5573041426782, 2737324.126209395, 2670063.417355074, 510077.3388952067], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5476800.0000, 
sim time next is 5477400.0000, 
raw observation next is [33.0, 70.0, 1.0, 2.0, 0.9902129598828954, 1.0, 2.0, 0.9902129598828954, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2769899.47379648, 2769899.47379648, 523109.6073366306], 
processed observation next is [1.0, 0.391304347826087, 0.7630331753554502, 0.7, 1.0, 1.0, 0.9882083854010788, 1.0, 1.0, 0.9882083854010788, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7694165204990222, 0.7694165204990222, 0.7807606079651203], 
reward next is 0.2192, 
noisyNet noise sample is [array([0.72047174], dtype=float32), 1.8454959]. 
=============================================
[2019-03-26 11:38:51,112] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6332406e-18 1.3007764e-03 3.0170423e-24 2.1575196e-13 9.9869919e-01], sum to 1.0000
[2019-03-26 11:38:51,120] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2162
[2019-03-26 11:38:51,127] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.8, 57.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.309894762402, 6.9112, 170.5573041426782, 3195263.685470528, 2909662.409295947, 551531.6885886904], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5487000.0000, 
sim time next is 5487600.0000, 
raw observation next is [35.90000000000001, 56.0, 1.0, 2.0, 0.7663472124986372, 1.0, 2.0, 0.7037636457635811, 1.0, 1.0, 1.03, 7.005102964155254, 6.9112, 170.5573041426782, 2953148.650509957, 2885882.137557185, 543104.5570195195], 
processed observation next is [1.0, 0.5217391304347826, 0.9004739336492897, 0.56, 1.0, 1.0, 0.7184906174682376, 1.0, 1.0, 0.6430887298356398, 1.0, 0.5, 1.0365853658536586, 0.009390296415525422, 0.0, 0.8375144448122397, 0.8203190695860992, 0.801633927099218, 0.810603816447044], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8369318], dtype=float32), 0.62804514]. 
=============================================
[2019-03-26 11:38:53,473] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:38:53,479] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5848
[2019-03-26 11:38:53,483] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 82.0, 1.0, 2.0, 0.5700071427972186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796533.0991668317, 796533.0991668324, 195291.3434902461], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5520000.0000, 
sim time next is 5520600.0000, 
raw observation next is [28.85, 82.5, 1.0, 2.0, 0.567361603932105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792834.8196694682, 792834.8196694682, 194822.9990674461], 
processed observation next is [1.0, 0.9130434782608695, 0.5663507109004741, 0.825, 1.0, 1.0, 0.47874892040012645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22023189435263005, 0.22023189435263005, 0.29078059562305386], 
reward next is 0.7092, 
noisyNet noise sample is [array([2.2974062], dtype=float32), 0.9026795]. 
=============================================
[2019-03-26 11:38:57,181] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.7202942e-30 1.0000000e+00 6.4202849e-38 3.7078568e-25 3.0181782e-13], sum to 1.0000
[2019-03-26 11:38:57,190] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6569
[2019-03-26 11:38:57,197] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.2, 60.0, 1.0, 2.0, 0.4871478623918892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680707.6964702278, 680707.6964702278, 181610.6422539631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5592600.0000, 
sim time next is 5593200.0000, 
raw observation next is [31.93333333333333, 61.66666666666666, 1.0, 2.0, 0.5006357723713235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699561.0016282806, 699561.0016282812, 183698.9226332711], 
processed observation next is [1.0, 0.7391304347826086, 0.7124802527646128, 0.6166666666666666, 1.0, 1.0, 0.3983563522546066, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19432250045230018, 0.19432250045230035, 0.27417749646756884], 
reward next is 0.7258, 
noisyNet noise sample is [array([0.25585133], dtype=float32), -0.010328352]. 
=============================================
[2019-03-26 11:38:57,781] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6607404e-36 1.0000000e+00 0.0000000e+00 2.1513574e-28 9.1508712e-28], sum to 1.0000
[2019-03-26 11:38:57,789] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4206
[2019-03-26 11:38:57,794] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.63333333333333, 83.66666666666667, 1.0, 2.0, 0.5594036030144646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781710.165004735, 781710.165004735, 193427.1474426003], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5600400.0000, 
sim time next is 5601000.0000, 
raw observation next is [28.36666666666667, 85.83333333333334, 1.0, 2.0, 0.5624936776486676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786029.8306384452, 786029.8306384459, 193967.0559697094], 
processed observation next is [1.0, 0.8260869565217391, 0.543443917851501, 0.8583333333333334, 1.0, 1.0, 0.47288394897429836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21834161962179033, 0.21834161962179052, 0.2895030686115066], 
reward next is 0.7105, 
noisyNet noise sample is [array([0.63025945], dtype=float32), -0.59567475]. 
=============================================
[2019-03-26 11:38:57,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[63.534748]
 [63.18004 ]
 [63.141052]
 [63.202522]
 [62.984833]], R is [[63.93955231]
 [64.01145935]
 [64.08357239]
 [64.15575409]
 [64.22796631]].
[2019-03-26 11:38:59,703] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 11:38:59,705] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:38:59,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:38:59,706] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:38:59,708] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:38:59,708] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:38:59,709] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:38:59,710] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:38:59,711] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:38:59,709] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:38:59,714] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:39:00,467] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run41
[2019-03-26 11:39:00,470] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run41
[2019-03-26 11:39:00,651] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run41
[2019-03-26 11:39:00,673] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run41
[2019-03-26 11:39:00,712] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run41
[2019-03-26 11:39:09,144] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.16132447], dtype=float32), 0.4885863]
[2019-03-26 11:39:09,145] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.76666666666667, 75.33333333333334, 1.0, 2.0, 0.2715928582785849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 442804.533344373, 442804.533344373, 163060.2952342359]
[2019-03-26 11:39:09,146] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:39:09,149] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.129307e-38], sampled 0.4412375941356902
[2019-03-26 11:39:34,854] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16132447], dtype=float32), 0.4885863]
[2019-03-26 11:39:34,858] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.06666666666667, 89.0, 1.0, 2.0, 0.4159153270391271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611761.8114943021, 611761.8114943021, 175386.0423279681]
[2019-03-26 11:39:34,860] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:39:34,862] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.013077985689791904
[2019-03-26 11:39:39,686] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16132447], dtype=float32), 0.4885863]
[2019-03-26 11:39:39,686] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.0, 94.0, 1.0, 2.0, 0.3956961589567615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590437.1791572652, 590437.1791572652, 173657.7319343442]
[2019-03-26 11:39:39,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:39:39,691] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9257205977443782
[2019-03-26 11:39:48,782] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16132447], dtype=float32), 0.4885863]
[2019-03-26 11:39:48,784] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.15, 65.5, 1.0, 2.0, 0.5795769271471589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809911.1002299297, 809911.1002299297, 197002.7307660911]
[2019-03-26 11:39:48,786] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:39:48,790] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.495976e-37], sampled 0.5297623783331408
[2019-03-26 11:39:57,877] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16132447], dtype=float32), 0.4885863]
[2019-03-26 11:39:57,879] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.49940603166667, 93.29416126166666, 1.0, 2.0, 0.635843975647228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 888572.5644887767, 888572.5644887767, 207615.3571841085]
[2019-03-26 11:39:57,880] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:39:57,883] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4637018e-37], sampled 0.7230937212758568
[2019-03-26 11:40:03,194] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.16132447], dtype=float32), 0.4885863]
[2019-03-26 11:40:03,196] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.61398100666667, 60.04928623333333, 1.0, 2.0, 0.6099938267142364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 852433.2729234035, 852433.2729234041, 202623.6522330692]
[2019-03-26 11:40:03,197] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:40:03,201] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0221569e-31 8.7665775e-33], sampled 0.765297820291707
[2019-03-26 11:40:40,562] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.16132447], dtype=float32), 0.4885863]
[2019-03-26 11:40:40,565] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.77639962, 74.0685018, 1.0, 2.0, 0.3433158044969594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 541363.2481005049, 541363.2481005054, 170217.4776675605]
[2019-03-26 11:40:40,567] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:40:40,569] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8981867064411053
[2019-03-26 11:40:42,256] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16132447], dtype=float32), 0.4885863]
[2019-03-26 11:40:42,257] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.63333333333333, 52.0, 1.0, 2.0, 0.4622469387438832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649180.906625034, 649180.906625034, 178316.582657033]
[2019-03-26 11:40:42,257] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:40:42,260] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8038275394949316
[2019-03-26 11:40:56,274] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7953.6483 3162142345.3309 1700.0000
[2019-03-26 11:40:56,693] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8050.5866 3006641048.2123 1636.0000
[2019-03-26 11:40:57,016] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8532.2759 2841189920.8976 1063.0000
[2019-03-26 11:40:57,032] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8307.3732 2927260196.5675 1235.0000
[2019-03-26 11:40:57,199] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8691.6428 2779362923.7187 871.0000
[2019-03-26 11:40:58,216] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1000000, evaluation results [1000000.0, 7953.648339228629, 3162142345.3309016, 1700.0, 8307.373168742459, 2927260196.5674524, 1235.0, 8691.64276162295, 2779362923.718658, 871.0, 8050.5865962531725, 3006641048.212312, 1636.0, 8532.275917965682, 2841189920.8975906, 1063.0]
[2019-03-26 11:40:58,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6139174e-33], sum to 1.0000
[2019-03-26 11:40:58,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6128
[2019-03-26 11:40:58,321] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 91.66666666666666, 1.0, 2.0, 0.522220160378864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729732.2027074888, 729732.2027074888, 187150.3899612127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5618400.0000, 
sim time next is 5619000.0000, 
raw observation next is [26.05, 91.83333333333333, 1.0, 2.0, 0.5207622642880092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727694.291853428, 727694.291853428, 186912.7021412179], 
processed observation next is [0.0, 0.0, 0.43364928909952616, 0.9183333333333333, 1.0, 1.0, 0.4226051376963966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2021373032926189, 0.2021373032926189, 0.2789741823003252], 
reward next is 0.7210, 
noisyNet noise sample is [array([1.4259343], dtype=float32), -1.2942187]. 
=============================================
[2019-03-26 11:40:58,340] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.77028]
 [73.47752]
 [73.14774]
 [72.82471]
 [72.80294]], R is [[73.96800995]
 [73.94900513]
 [73.93016815]
 [73.91139221]
 [73.8924408 ]].
[2019-03-26 11:40:59,036] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.439555e-34], sum to 1.0000
[2019-03-26 11:40:59,045] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5307
[2019-03-26 11:40:59,052] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 86.0, 1.0, 2.0, 0.5047848553754679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705360.6277993605, 705360.6277993611, 184349.2608425079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5637600.0000, 
sim time next is 5638200.0000, 
raw observation next is [26.75, 85.16666666666667, 1.0, 2.0, 0.5062960044907171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707472.9337201746, 707472.9337201746, 184588.4830706276], 
processed observation next is [0.0, 0.2608695652173913, 0.4668246445497631, 0.8516666666666667, 1.0, 1.0, 0.4051759090249603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19652025936671516, 0.19652025936671516, 0.27550519861287703], 
reward next is 0.7245, 
noisyNet noise sample is [array([-1.9433341], dtype=float32), -0.6015214]. 
=============================================
[2019-03-26 11:41:04,970] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:41:04,979] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7264
[2019-03-26 11:41:04,985] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.8, 53.0, 1.0, 2.0, 0.5123013271457002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715867.3014275094, 715867.3014275087, 185545.9111822582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5752800.0000, 
sim time next is 5753400.0000, 
raw observation next is [33.0, 53.0, 1.0, 2.0, 0.5296397848215211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740103.7385129881, 740103.7385129888, 188369.5678431455], 
processed observation next is [0.0, 0.6086956521739131, 0.7630331753554502, 0.53, 1.0, 1.0, 0.4333009455680977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20558437180916336, 0.20558437180916356, 0.2811486087211127], 
reward next is 0.7189, 
noisyNet noise sample is [array([-0.48475826], dtype=float32), 0.96035206]. 
=============================================
[2019-03-26 11:41:07,208] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2794965e-28 9.9997520e-01 2.6229714e-30 2.2406069e-20 2.4746234e-05], sum to 1.0000
[2019-03-26 11:41:07,219] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6255
[2019-03-26 11:41:07,227] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.88333333333333, 89.0, 1.0, 2.0, 0.5368667373309692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750206.0448565988, 750206.0448565994, 189573.5949418262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5793000.0000, 
sim time next is 5793600.0000, 
raw observation next is [26.86666666666667, 89.0, 1.0, 2.0, 0.5368137847416733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750132.0239185579, 750132.0239185579, 189564.6790577831], 
processed observation next is [1.0, 0.043478260869565216, 0.4723538704581361, 0.89, 1.0, 1.0, 0.4419443189658714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20837000664404387, 0.20837000664404387, 0.2829323568026613], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.8739872], dtype=float32), -1.5648465]. 
=============================================
[2019-03-26 11:41:08,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7365354e-20 1.3623552e-02 1.3219762e-26 1.1836645e-20 9.8637646e-01], sum to 1.0000
[2019-03-26 11:41:08,352] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9484
[2019-03-26 11:41:08,356] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 93.66666666666667, 1.0, 2.0, 0.2865895329757049, 1.0, 2.0, 0.2865895329757049, 1.0, 2.0, 0.4899251012983422, 6.911199999999999, 6.9112, 170.5573041426782, 1201670.896044889, 1201670.896044889, 298277.5051195255], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5805600.0000, 
sim time next is 5806200.0000, 
raw observation next is [25.95, 93.83333333333334, 1.0, 2.0, 0.282489207570603, 1.0, 2.0, 0.282489207570603, 1.0, 2.0, 0.4826694551711917, 6.911199999999999, 6.9112, 170.5573041426782, 1184468.720394544, 1184468.720394545, 296663.7385584133], 
processed observation next is [1.0, 0.17391304347826086, 0.42890995260663506, 0.9383333333333335, 1.0, 1.0, 0.13552916574771443, 1.0, 1.0, 0.13552916574771443, 1.0, 1.0, 0.369109091672185, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3290190889984845, 0.3290190889984847, 0.4427816993409154], 
reward next is 0.5572, 
noisyNet noise sample is [array([0.298167], dtype=float32), -0.3438322]. 
=============================================
[2019-03-26 11:41:09,060] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.1893164e-26 1.9020667e-14 2.2388194e-33 1.8891043e-26 1.0000000e+00], sum to 1.0000
[2019-03-26 11:41:09,063] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5346
[2019-03-26 11:41:09,070] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.55, 72.0, 1.0, 2.0, 0.5431539199639919, 1.0, 2.0, 0.5431539199639919, 1.0, 2.0, 0.9432787352313101, 6.9112, 6.9112, 170.5573041426782, 2278579.991642016, 2278579.991642016, 446270.6154664475], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5823000.0000, 
sim time next is 5823600.0000, 
raw observation next is [30.7, 71.33333333333333, 1.0, 2.0, 0.5725245906491258, 1.0, 2.0, 0.5725245906491258, 1.0, 2.0, 0.9942858771821682, 6.911199999999999, 6.9112, 170.5573041426782, 2401911.029891308, 2401911.029891308, 468920.3908519527], 
processed observation next is [1.0, 0.391304347826087, 0.6540284360189573, 0.7133333333333333, 1.0, 1.0, 0.48496938632424785, 1.0, 1.0, 0.48496938632424785, 1.0, 1.0, 0.9930315575392293, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6671975083031411, 0.6671975083031411, 0.6998811803760489], 
reward next is 0.3001, 
noisyNet noise sample is [array([1.8916996], dtype=float32), -0.23349206]. 
=============================================
[2019-03-26 11:41:10,716] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9332521e-28 1.0000000e+00 3.0826492e-38 7.9393669e-17 2.6253591e-11], sum to 1.0000
[2019-03-26 11:41:10,725] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2877
[2019-03-26 11:41:10,730] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.6, 72.16666666666667, 1.0, 2.0, 0.5496980070224968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768142.6686108892, 768142.6686108886, 191750.4191087584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5854200.0000, 
sim time next is 5854800.0000, 
raw observation next is [30.4, 73.33333333333334, 1.0, 2.0, 0.5562784091461541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777341.4234896065, 777341.4234896065, 192884.3202380471], 
processed observation next is [1.0, 0.782608695652174, 0.6398104265402843, 0.7333333333333334, 1.0, 1.0, 0.4653956736700652, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21592817319155735, 0.21592817319155735, 0.2878870451314136], 
reward next is 0.7121, 
noisyNet noise sample is [array([-0.59240997], dtype=float32), -1.7043432]. 
=============================================
[2019-03-26 11:41:10,770] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2196768e-30 7.7460162e-18 0.0000000e+00 2.4544837e-27 1.0000000e+00], sum to 1.0000
[2019-03-26 11:41:10,775] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7619
[2019-03-26 11:41:10,779] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.34999999999999, 62.83333333333333, 1.0, 2.0, 0.596861028486606, 1.0, 2.0, 0.596861028486606, 1.0, 2.0, 1.03, 6.918563401953119, 6.9112, 170.5573041426782, 2504111.930942536, 2498837.226599476, 486842.4980042857], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5845800.0000, 
sim time next is 5846400.0000, 
raw observation next is [32.3, 63.0, 1.0, 2.0, 0.5934323290819297, 1.0, 2.0, 0.5934323290819297, 1.0, 2.0, 1.03, 6.911869654720414, 6.9112, 170.5573041426782, 2489712.597625889, 2489232.896709844, 485608.6254480918], 
processed observation next is [1.0, 0.6956521739130435, 0.7298578199052131, 0.63, 1.0, 1.0, 0.5101594326288309, 1.0, 1.0, 0.5101594326288309, 1.0, 1.0, 1.0365853658536586, 6.696547204141368e-05, 0.0, 0.8375144448122397, 0.691586832673858, 0.6914535824194011, 0.7247889932061072], 
reward next is 0.2719, 
noisyNet noise sample is [array([0.5865324], dtype=float32), -1.0113028]. 
=============================================
[2019-03-26 11:41:11,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0856462e-24 1.0000000e+00 5.8182174e-30 1.9056927e-27 1.3670109e-10], sum to 1.0000
[2019-03-26 11:41:11,572] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1420
[2019-03-26 11:41:11,578] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 95.0, 1.0, 2.0, 0.9543048200995522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1333891.409172334, 1333891.409172334, 285311.4776112011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5886600.0000, 
sim time next is 5887200.0000, 
raw observation next is [25.83333333333334, 95.0, 1.0, 2.0, 0.9649197289373127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1348737.954654966, 1348737.954654966, 288418.6792144837], 
processed observation next is [1.0, 0.13043478260869565, 0.42338072669826254, 0.95, 1.0, 1.0, 0.9577346131774852, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37464943184860167, 0.37464943184860167, 0.4304756406186324], 
reward next is 0.5695, 
noisyNet noise sample is [array([-0.97887415], dtype=float32), -0.32391372]. 
=============================================
[2019-03-26 11:41:17,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.430483e-36 0.000000e+00], sum to 1.0000
[2019-03-26 11:41:17,092] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4922
[2019-03-26 11:41:17,096] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 85.0, 1.0, 2.0, 0.5572740638658205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 778733.257787407, 778733.2577874063, 193056.3040487598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5949600.0000, 
sim time next is 5950200.0000, 
raw observation next is [28.16666666666667, 85.5, 1.0, 2.0, 0.5575061115300578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779057.6394755057, 779057.6394755057, 193096.6139462574], 
processed observation next is [1.0, 0.8695652173913043, 0.5339652448657191, 0.855, 1.0, 1.0, 0.46687483316874434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21640489985430714, 0.21640489985430714, 0.2882039014123245], 
reward next is 0.7118, 
noisyNet noise sample is [array([-1.1878744], dtype=float32), 1.7983788]. 
=============================================
[2019-03-26 11:41:18,355] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2237150e-33 1.0000000e+00 0.0000000e+00 3.0582654e-29 5.2117348e-31], sum to 1.0000
[2019-03-26 11:41:18,365] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7967
[2019-03-26 11:41:18,368] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 84.5, 1.0, 2.0, 0.5560742024513614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777055.9614196413, 777055.9614196407, 192848.2197328485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5949000.0000, 
sim time next is 5949600.0000, 
raw observation next is [28.23333333333333, 85.0, 1.0, 2.0, 0.5572740638658205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 778733.257787407, 778733.2577874063, 193056.3040487598], 
processed observation next is [1.0, 0.8695652173913043, 0.537124802527646, 0.85, 1.0, 1.0, 0.4665952576696632, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2163147938298353, 0.2163147938298351, 0.2881437373862087], 
reward next is 0.7119, 
noisyNet noise sample is [array([-1.6821969], dtype=float32), 0.16032295]. 
=============================================
[2019-03-26 11:41:24,487] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.778774e-32 5.759465e-12 0.000000e+00 5.951379e-37 1.000000e+00], sum to 1.0000
[2019-03-26 11:41:24,496] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5530
[2019-03-26 11:41:24,506] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.6, 67.5, 1.0, 2.0, 0.4468229917203047, 1.0, 2.0, 0.4468229917203047, 1.0, 2.0, 0.7717789843158572, 6.911200000000001, 6.9112, 170.5573041426782, 1874117.748292412, 1874117.748292412, 379653.5826728347], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6085800.0000, 
sim time next is 6086400.0000, 
raw observation next is [30.73333333333333, 66.66666666666666, 1.0, 2.0, 0.4580597392549088, 1.0, 2.0, 0.4580597392549088, 1.0, 2.0, 0.7910710548542966, 6.9112, 6.9112, 170.5573041426782, 1921290.499283267, 1921290.499283267, 386631.4820859337], 
processed observation next is [1.0, 0.43478260869565216, 0.6556082148499209, 0.6666666666666665, 1.0, 1.0, 0.3470599268131431, 1.0, 1.0, 0.3470599268131431, 1.0, 1.0, 0.7452086034808494, 0.0, 0.0, 0.8375144448122397, 0.533691805356463, 0.533691805356463, 0.577061913561095], 
reward next is 0.4229, 
noisyNet noise sample is [array([-1.1022729], dtype=float32), -0.37290594]. 
=============================================
[2019-03-26 11:41:26,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9143609e-37 1.0000000e+00 0.0000000e+00 9.3897621e-23 6.1245742e-30], sum to 1.0000
[2019-03-26 11:41:26,231] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3213
[2019-03-26 11:41:26,235] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.38333333333333, 79.0, 1.0, 2.0, 0.5254278565227833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734216.0742925534, 734216.074292554, 187676.5075501182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6115800.0000, 
sim time next is 6116400.0000, 
raw observation next is [28.2, 80.0, 1.0, 2.0, 0.5251802323519601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733869.9326706474, 733869.9326706467, 187635.8053387474], 
processed observation next is [1.0, 0.8260869565217391, 0.5355450236966824, 0.8, 1.0, 1.0, 0.42792799078549404, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20385275907517983, 0.20385275907517963, 0.2800534408041006], 
reward next is 0.7199, 
noisyNet noise sample is [array([-0.24760236], dtype=float32), 0.56811404]. 
=============================================
[2019-03-26 11:41:26,342] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.3509052e-35 1.0000000e+00 0.0000000e+00 2.3963064e-29 3.4782335e-23], sum to 1.0000
[2019-03-26 11:41:26,348] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8693
[2019-03-26 11:41:26,357] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 71.5, 1.0, 2.0, 0.4871464070672719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680705.6622458267, 680705.6622458261, 181609.9749428794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6111000.0000, 
sim time next is 6111600.0000, 
raw observation next is [29.63333333333333, 72.33333333333333, 1.0, 2.0, 0.500363813360927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 699180.8559419446, 699180.8559419439, 183655.5954069788], 
processed observation next is [1.0, 0.7391304347826086, 0.6034755134281199, 0.7233333333333333, 1.0, 1.0, 0.3980286907962975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19421690442831796, 0.19421690442831777, 0.27411282896564], 
reward next is 0.7259, 
noisyNet noise sample is [array([-1.2871693], dtype=float32), 0.34569293]. 
=============================================
[2019-03-26 11:41:27,500] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.5112193e-27 9.9998558e-01 5.5113094e-32 6.1191912e-28 1.4368150e-05], sum to 1.0000
[2019-03-26 11:41:27,509] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1786
[2019-03-26 11:41:27,516] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2687926.665419532 W.
[2019-03-26 11:41:27,523] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.68333333333333, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.641743262424079, 6.9112, 169.6938334416075, 2687926.665419532, 1454544.080842924, 310321.3318525727], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6142200.0000, 
sim time next is 6142800.0000, 
raw observation next is [26.66666666666667, 92.0, 1.0, 2.0, 0.4292659083253524, 1.0, 1.0, 0.4292659083253524, 1.0, 1.0, 0.7405334651451544, 6.9112, 6.9112, 170.5573041426782, 1800415.887093644, 1800415.887093644, 368927.6044905849], 
processed observation next is [1.0, 0.08695652173913043, 0.4628751974723541, 0.92, 1.0, 1.0, 0.3123685642474125, 1.0, 0.5, 0.3123685642474125, 1.0, 0.5, 0.6835773965184808, 0.0, 0.0, 0.8375144448122397, 0.5001155241926789, 0.5001155241926789, 0.5506382156575894], 
reward next is 0.4494, 
noisyNet noise sample is [array([-1.0463737], dtype=float32), 0.027352503]. 
=============================================
[2019-03-26 11:41:29,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6995209e-29 1.8803847e-17 1.4931952e-35 8.1451364e-35 1.0000000e+00], sum to 1.0000
[2019-03-26 11:41:29,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7452
[2019-03-26 11:41:29,590] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.21666666666667, 84.5, 1.0, 2.0, 0.4879712842791546, 1.0, 2.0, 0.4879712842791546, 1.0, 2.0, 0.8474447462232324, 6.9112, 6.9112, 170.5573041426782, 2046871.644957464, 2046871.644957464, 406927.0995142937], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6166200.0000, 
sim time next is 6166800.0000, 
raw observation next is [28.3, 84.0, 1.0, 2.0, 0.489470009894996, 1.0, 2.0, 0.489470009894996, 1.0, 2.0, 0.850047536981814, 6.911200000000001, 6.9112, 170.5573041426782, 2053164.307682266, 2053164.307682265, 407940.2183741624], 
processed observation next is [1.0, 0.391304347826087, 0.5402843601895735, 0.84, 1.0, 1.0, 0.38490362637951325, 1.0, 1.0, 0.38490362637951325, 1.0, 1.0, 0.8171311426607487, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5703234188006294, 0.5703234188006292, 0.6088659975733767], 
reward next is 0.3911, 
noisyNet noise sample is [array([1.0283924], dtype=float32), -1.0696235]. 
=============================================
[2019-03-26 11:41:30,590] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0622448e-26 1.0000000e+00 2.3518775e-36 5.8218763e-24 3.7061497e-08], sum to 1.0000
[2019-03-26 11:41:30,600] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6283
[2019-03-26 11:41:30,607] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2242386.037769048 W.
[2019-03-26 11:41:30,612] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.5, 73.5, 1.0, 2.0, 0.5345339605012442, 1.0, 1.0, 0.5345339605012442, 1.0, 2.0, 0.923620425098307, 6.911199999999999, 6.9112, 170.5573041426782, 2242386.037769048, 2242386.037769048, 438936.6393494078], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6192600.0000, 
sim time next is 6193200.0000, 
raw observation next is [29.40000000000001, 74.0, 1.0, 2.0, 0.5409864081596404, 1.0, 2.0, 0.5409864081596404, 1.0, 2.0, 0.9358059600556505, 6.911199999999999, 6.9112, 170.5573041426782, 2269478.826728089, 2269478.82672809, 443915.5865178831], 
processed observation next is [1.0, 0.6956521739130435, 0.5924170616113749, 0.74, 1.0, 1.0, 0.4469715760959522, 1.0, 1.0, 0.4469715760959522, 1.0, 1.0, 0.9217145854337202, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6304107852022469, 0.6304107852022472, 0.6625605768923628], 
reward next is 0.3374, 
noisyNet noise sample is [array([-1.2091455], dtype=float32), -0.5728125]. 
=============================================
[2019-03-26 11:41:30,690] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6262658e-24 1.0000000e+00 3.9958281e-36 6.6877174e-22 1.8249640e-12], sum to 1.0000
[2019-03-26 11:41:30,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4713
[2019-03-26 11:41:30,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2234747.403872366 W.
[2019-03-26 11:41:30,711] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.8, 68.0, 1.0, 2.0, 0.7990720649793577, 1.0, 2.0, 0.7990720649793577, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2234747.403872366, 2234747.403872366, 419418.7325434951], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6184800.0000, 
sim time next is 6185400.0000, 
raw observation next is [30.7, 68.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.257896459912468, 6.9112, 168.9112434992101, 2537635.773263561, 2291679.866589087, 475733.7955218683], 
processed observation next is [1.0, 0.6086956521739131, 0.6540284360189573, 0.685, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.03466964599124678, 0.0, 0.829431533482988, 0.7048988259065447, 0.6365777407191908, 0.7100504410774153], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7536052], dtype=float32), -0.06018118]. 
=============================================
[2019-03-26 11:41:41,779] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0011048e-34 2.0409745e-17 2.0092286e-36 1.8672851e-29 1.0000000e+00], sum to 1.0000
[2019-03-26 11:41:41,787] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7344
[2019-03-26 11:41:41,795] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.08333333333334, 83.0, 1.0, 2.0, 0.1717078674511751, 1.0, 2.0, 0.1717078674511751, 1.0, 2.0, 0.2909007653135208, 6.911199999999999, 6.9112, 170.5573041426782, 719810.0093652306, 719810.0093652311, 262276.3534186528], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6397800.0000, 
sim time next is 6398400.0000, 
raw observation next is [27.06666666666667, 83.0, 1.0, 2.0, 0.1711093189247795, 1.0, 2.0, 0.1711093189247795, 1.0, 2.0, 0.2898179670556094, 6.911200000000001, 6.9112, 170.5573041426782, 717300.0178920586, 717300.0178920579, 262134.7222346361], 
processed observation next is [1.0, 0.043478260869565216, 0.48183254344391807, 0.83, 1.0, 1.0, 0.0013365288250355434, 1.0, 1.0, 0.0013365288250355434, 1.0, 1.0, 0.13392435006781636, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19925000497001627, 0.19925000497001608, 0.39124585408154644], 
reward next is 0.6088, 
noisyNet noise sample is [array([-0.3232215], dtype=float32), -0.85991335]. 
=============================================
[2019-03-26 11:41:48,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.0458302e-32 1.0000000e+00 0.0000000e+00 1.8469072e-33 4.3823890e-12], sum to 1.0000
[2019-03-26 11:41:48,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4051
[2019-03-26 11:41:48,378] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.21666666666667, 91.83333333333333, 1.0, 2.0, 0.7365155890166717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1029326.252026371, 1029326.252026371, 228923.5990744948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6493800.0000, 
sim time next is 6494400.0000, 
raw observation next is [26.2, 92.0, 1.0, 2.0, 0.725539458453854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1013979.104788194, 1013979.104788194, 226453.5873591808], 
processed observation next is [1.0, 0.17391304347826086, 0.44075829383886256, 0.92, 1.0, 1.0, 0.6693246487395832, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.281660862441165, 0.281660862441165, 0.3379904288942997], 
reward next is 0.6620, 
noisyNet noise sample is [array([2.139614], dtype=float32), 1.7981529]. 
=============================================
[2019-03-26 11:41:52,260] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 11:41:52,262] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:41:52,263] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:41:52,265] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:41:52,266] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:41:52,267] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:41:52,269] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:41:52,268] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:41:52,271] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:41:52,273] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:41:52,275] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:41:52,286] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run42
[2019-03-26 11:41:52,308] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run42
[2019-03-26 11:41:52,310] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run42
[2019-03-26 11:41:52,361] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run42
[2019-03-26 11:41:52,390] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run42
[2019-03-26 11:42:01,454] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.03760212], dtype=float32), 0.4973793]
[2019-03-26 11:42:01,457] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.71666666666667, 57.16666666666666, 1.0, 2.0, 0.3115956537643351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508002.9362410499, 508002.9362410499, 167565.3074400358]
[2019-03-26 11:42:01,458] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:42:01,459] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7463171650066177
[2019-03-26 11:42:15,400] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.03760212], dtype=float32), 0.4973793]
[2019-03-26 11:42:15,403] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.7, 91.00000000000001, 1.0, 2.0, 0.3264188989787366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511970.4120501378, 511970.4120501378, 167843.0780645584]
[2019-03-26 11:42:15,405] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:42:15,407] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.6300975e-37 4.1909329e-28], sampled 0.7604413587081187
[2019-03-26 11:42:18,723] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.03760212], dtype=float32), 0.4973793]
[2019-03-26 11:42:18,725] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.16666666666667, 85.66666666666666, 1.0, 2.0, 0.6861049299627314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1081520.073867193, 1081520.073867193, 232610.5293168236]
[2019-03-26 11:42:18,725] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:42:18,727] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0685078e-34 1.0000000e+00 8.8979190e-38 2.0959859e-31 9.3249229e-19], sampled 0.39360829374466444
[2019-03-26 11:42:45,736] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.03760212], dtype=float32), 0.4973793]
[2019-03-26 11:42:45,738] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.5, 67.0, 1.0, 2.0, 0.7413294088745471, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.001679466507825, 6.9112, 168.9123454736951, 1932964.843820283, 1868775.828419092, 394028.3983437061]
[2019-03-26 11:42:45,742] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:42:45,745] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1171481e-37 1.0000000e+00 0.0000000e+00 2.0785079e-32 2.5346511e-33], sampled 0.08974530049411111
[2019-03-26 11:42:45,745] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1932964.843820283 W.
[2019-03-26 11:42:55,081] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.03760212], dtype=float32), 0.4973793]
[2019-03-26 11:42:55,081] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.40000000000001, 51.33333333333334, 1.0, 2.0, 0.5862504455698473, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564911545, 819240.3899620934, 819240.3899620927, 198214.3148816345]
[2019-03-26 11:42:55,083] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:42:55,086] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0755645e-37 1.0000000e+00 0.0000000e+00 5.0994573e-32 7.7113308e-29], sampled 0.10487305007407632
[2019-03-26 11:43:14,939] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.03760212], dtype=float32), 0.4973793]
[2019-03-26 11:43:14,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 86.0, 1.0, 2.0, 0.5272880682244482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736816.3761386658, 736816.3761386652, 187981.5497783043]
[2019-03-26 11:43:14,943] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:43:14,945] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.2297766e-35], sampled 0.22673719753414856
[2019-03-26 11:43:19,857] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.03760212], dtype=float32), 0.4973793]
[2019-03-26 11:43:19,860] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.73384244833333, 97.99291740333334, 1.0, 2.0, 0.5254190321561847, 1.0, 2.0, 0.5254190321561847, 1.0, 2.0, 0.9124791001261697, 6.9112, 6.9112, 184.5923449428631, 2203941.270447088, 2203941.270447088, 437318.1798883274]
[2019-03-26 11:43:19,861] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:43:19,862] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8316025e-25 9.3256068e-01 3.0555231e-29 1.8260063e-28 6.7439377e-02], sampled 0.8031557222193448
[2019-03-26 11:43:19,863] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2203941.270447088 W.
[2019-03-26 11:43:29,824] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.03760212], dtype=float32), 0.4973793]
[2019-03-26 11:43:29,825] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.65734949666667, 85.79016844, 1.0, 2.0, 0.3776373701662855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 582940.8289291788, 582940.8289291793, 173505.4586522341]
[2019-03-26 11:43:29,825] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:43:29,828] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 3.353665e-35 8.475624e-25], sampled 0.3020963651648211
[2019-03-26 11:43:31,599] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.03760212], dtype=float32), 0.4973793]
[2019-03-26 11:43:31,600] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.02271274, 59.92283776, 1.0, 2.0, 0.8095236918360464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1216689.544527818, 1216689.544527818, 257845.5384096717]
[2019-03-26 11:43:31,601] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:43:31,605] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.355029e-30], sampled 0.39858824370247403
[2019-03-26 11:43:47,362] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7898.6486 3163979791.9851 1779.0000
[2019-03-26 11:43:47,578] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8267.8116 2927644592.8552 1319.0000
[2019-03-26 11:43:47,804] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8010.0089 3007573058.8082 1734.0000
[2019-03-26 11:43:47,882] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8502.9502 2842477690.1003 1117.0000
[2019-03-26 11:43:47,901] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8674.7368 2779593312.1244 906.0000
[2019-03-26 11:43:48,918] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1025000, evaluation results [1025000.0, 7898.64863965992, 3163979791.985093, 1779.0, 8267.811563125691, 2927644592.855185, 1319.0, 8674.736841824853, 2779593312.1243834, 906.0, 8010.008869806465, 3007573058.8081865, 1734.0, 8502.950231793171, 2842477690.1002913, 1117.0]
[2019-03-26 11:43:49,046] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4771106e-38 3.4683670e-28], sum to 1.0000
[2019-03-26 11:43:49,052] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5011
[2019-03-26 11:43:49,065] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333334, 87.33333333333333, 1.0, 2.0, 0.5196885430402731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726193.3998804901, 726193.3998804907, 186738.5075661945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6567000.0000, 
sim time next is 6567600.0000, 
raw observation next is [26.76666666666667, 87.66666666666667, 1.0, 2.0, 0.5191735408975247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725473.5093355966, 725473.5093355973, 186654.8340875067], 
processed observation next is [1.0, 0.0, 0.46761453396524505, 0.8766666666666667, 1.0, 1.0, 0.42069101312954776, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20152041925988795, 0.20152041925988814, 0.27858930460821896], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.27587295], dtype=float32), 1.2050816]. 
=============================================
[2019-03-26 11:43:49,835] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1483893e-35 1.0000000e+00 0.0000000e+00 9.7075052e-38 2.5181563e-22], sum to 1.0000
[2019-03-26 11:43:49,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6457
[2019-03-26 11:43:49,849] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 87.50000000000001, 1.0, 2.0, 0.658464865065084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 920198.2623011803, 920198.262301181, 212134.1025921805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6592200.0000, 
sim time next is 6592800.0000, 
raw observation next is [26.73333333333334, 87.0, 1.0, 2.0, 0.6559225515875734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 916643.8706559733, 916643.8706559733, 211617.5633366281], 
processed observation next is [1.0, 0.30434782608695654, 0.4660347551342816, 0.87, 1.0, 1.0, 0.5854488573344258, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.254623297404437, 0.254623297404437, 0.3158471094576539], 
reward next is 0.6842, 
noisyNet noise sample is [array([0.80547005], dtype=float32), 0.1590411]. 
=============================================
[2019-03-26 11:43:51,859] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2849359e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.9066767e-29], sum to 1.0000
[2019-03-26 11:43:51,866] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1269
[2019-03-26 11:43:51,870] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 90.0, 1.0, 2.0, 0.6755937162824901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 944146.3058865666, 944146.3058865673, 215663.5895416749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6588000.0000, 
sim time next is 6588600.0000, 
raw observation next is [26.18333333333334, 89.66666666666667, 1.0, 2.0, 0.6752231728762547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 943628.2391805528, 943628.2391805528, 215586.3919657916], 
processed observation next is [1.0, 0.2608695652173913, 0.4399684044233811, 0.8966666666666667, 1.0, 1.0, 0.6087026179231984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26211895532793134, 0.26211895532793134, 0.3217707342773009], 
reward next is 0.6782, 
noisyNet noise sample is [array([-1.3955787], dtype=float32), -0.29323804]. 
=============================================
[2019-03-26 11:44:06,849] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.191636e-38], sum to 1.0000
[2019-03-26 11:44:06,857] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3107
[2019-03-26 11:44:06,864] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.35, 80.0, 1.0, 2.0, 0.4167240385449648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 612002.4918850659, 612002.4918850666, 175381.8049249348], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6910200.0000, 
sim time next is 6910800.0000, 
raw observation next is [25.3, 80.33333333333334, 1.0, 2.0, 0.4170237444848601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612460.5297119211, 612460.5297119211, 175425.7835641209], 
processed observation next is [0.0, 1.0, 0.39810426540284366, 0.8033333333333335, 1.0, 1.0, 0.2976189692588676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17012792491997808, 0.17012792491997808, 0.2618295277076432], 
reward next is 0.7382, 
noisyNet noise sample is [array([1.6917763], dtype=float32), 0.6803833]. 
=============================================
[2019-03-26 11:44:28,128] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.22802655e-33], sum to 1.0000
[2019-03-26 11:44:28,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4253
[2019-03-26 11:44:28,140] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 90.33333333333334, 1.0, 2.0, 0.3292589459520169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519536.0258597115, 519536.0258597108, 168494.1916162801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7273200.0000, 
sim time next is 7273800.0000, 
raw observation next is [21.5, 90.5, 1.0, 2.0, 0.335819451357793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 530053.8533534574, 530053.8533534574, 169321.492985033], 
processed observation next is [1.0, 0.17391304347826086, 0.21800947867298584, 0.905, 1.0, 1.0, 0.19978247151541326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1472371814870715, 0.1472371814870715, 0.2527186462463179], 
reward next is 0.7473, 
noisyNet noise sample is [array([-0.6646924], dtype=float32), -1.0421113]. 
=============================================
[2019-03-26 11:44:29,531] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:44:29,532] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:44:29,608] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-03-26 11:44:31,455] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2449888e-30 1.9470460e-11 1.8485264e-35 2.9780045e-26 1.0000000e+00], sum to 1.0000
[2019-03-26 11:44:31,458] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4054
[2019-03-26 11:44:31,463] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.56666666666667, 62.33333333333334, 1.0, 2.0, 0.2807690734759385, 1.0, 2.0, 0.2807690734759385, 1.0, 2.0, 0.4824316257209441, 6.9112, 6.9112, 170.5573041426782, 1233109.562233801, 1233109.562233801, 301588.3366160778], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7315800.0000, 
sim time next is 7316400.0000, 
raw observation next is [27.53333333333333, 62.66666666666667, 1.0, 2.0, 0.2959159616997418, 1.0, 2.0, 0.2959159616997418, 1.0, 2.0, 0.5081365802677443, 6.9112, 6.9112, 170.5573041426782, 1298462.52315881, 1298462.52315881, 307582.4315723252], 
processed observation next is [1.0, 0.6956521739130435, 0.5039494470774091, 0.6266666666666667, 1.0, 1.0, 0.15170597795149618, 1.0, 1.0, 0.15170597795149618, 1.0, 1.0, 0.4001665613021272, 0.0, 0.0, 0.8375144448122397, 0.3606840342107806, 0.3606840342107806, 0.45907825607809727], 
reward next is 0.5409, 
noisyNet noise sample is [array([-1.4375268], dtype=float32), 0.51076084]. 
=============================================
[2019-03-26 11:44:32,502] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1628635e-32 1.0000000e+00 3.4279678e-38 1.2296069e-32 6.5379755e-09], sum to 1.0000
[2019-03-26 11:44:32,513] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7269
[2019-03-26 11:44:32,521] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 74.0, 1.0, 2.0, 0.3807836591491494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575343.7028946707, 575343.7028946707, 172514.4012383298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7333200.0000, 
sim time next is 7333800.0000, 
raw observation next is [25.35, 74.33333333333334, 1.0, 2.0, 0.3809585040310223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575809.4812119086, 575809.4812119086, 172561.7600756117], 
processed observation next is [1.0, 0.9130434782608695, 0.4004739336492892, 0.7433333333333334, 1.0, 1.0, 0.2541668723265329, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15994707811441905, 0.15994707811441905, 0.2575548657844951], 
reward next is 0.7424, 
noisyNet noise sample is [array([1.7221524], dtype=float32), -1.6972665]. 
=============================================
[2019-03-26 11:44:36,428] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3784934e-36 1.0000000e+00 0.0000000e+00 2.6145052e-30 1.1164210e-12], sum to 1.0000
[2019-03-26 11:44:36,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7730
[2019-03-26 11:44:36,449] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 91.5, 1.0, 2.0, 0.4855258533818431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775839.4433837334, 775839.4433837341, 192245.6599153558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7396200.0000, 
sim time next is 7396800.0000, 
raw observation next is [20.76666666666667, 91.33333333333334, 1.0, 2.0, 0.5568043743383155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 890735.134253962, 890735.1342539614, 205559.217615906], 
processed observation next is [1.0, 0.6086956521739131, 0.18325434439178534, 0.9133333333333334, 1.0, 1.0, 0.4660293666726693, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2474264261816561, 0.24742642618165595, 0.3068048024118], 
reward next is 0.6932, 
noisyNet noise sample is [array([0.02310388], dtype=float32), -0.5715625]. 
=============================================
[2019-03-26 11:44:39,262] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1048300: loss 0.8717
[2019-03-26 11:44:39,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1048301: learning rate 0.0005
[2019-03-26 11:44:43,024] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 11:44:43,027] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:44:43,028] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:44:43,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:44:43,030] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:44:43,031] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:44:43,034] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:44:43,035] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:44:43,035] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:44:43,037] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:44:43,039] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:44:43,063] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run43
[2019-03-26 11:44:43,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run43
[2019-03-26 11:44:43,118] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run43
[2019-03-26 11:44:43,119] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run43
[2019-03-26 11:44:43,161] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run43
[2019-03-26 11:45:00,078] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04404616], dtype=float32), 0.5063385]
[2019-03-26 11:45:00,079] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.3, 85.0, 1.0, 2.0, 0.295801482157286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 475829.1731492983, 475829.1731492977, 165367.2568752935]
[2019-03-26 11:45:00,080] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:45:00,082] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.6483956e-36 9.5905894e-26], sampled 0.08438775686202327
[2019-03-26 11:45:15,872] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04404616], dtype=float32), 0.5063385]
[2019-03-26 11:45:15,874] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.6, 76.0, 1.0, 2.0, 0.4763601818270994, 1.0, 2.0, 0.4763601818270994, 1.0, 2.0, 0.8272801011142698, 6.911200000000001, 6.9112, 170.5573041426782, 1998121.644423944, 1998121.644423944, 399183.1257506756]
[2019-03-26 11:45:15,875] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:45:15,878] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.0069800e-30 6.8505784e-07 2.9373843e-34 3.2670926e-33 9.9999928e-01], sampled 0.6241652693951196
[2019-03-26 11:45:20,723] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04404616], dtype=float32), 0.5063385]
[2019-03-26 11:45:20,724] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.3, 59.0, 1.0, 2.0, 0.6649859001113516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 929315.3326248827, 929315.3326248821, 213467.1435766809]
[2019-03-26 11:45:20,725] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:45:20,727] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4610699e-31 1.0000000e+00 2.2125652e-35 4.9850478e-27 3.1846505e-11], sampled 0.4529929767308908
[2019-03-26 11:45:45,573] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04404616], dtype=float32), 0.5063385]
[2019-03-26 11:45:45,573] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.85, 49.0, 1.0, 2.0, 0.5222617880430906, 1.0, 2.0, 0.5222617880430906, 1.0, 2.0, 0.9069960112183085, 6.9112, 6.9112, 169.0403247858759, 2190875.708178626, 2190875.708178626, 430581.9033824207]
[2019-03-26 11:45:45,574] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:45:45,577] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2947513e-28 2.3169241e-07 1.2185335e-35 1.7652508e-18 9.9999976e-01], sampled 0.8452653108627937
[2019-03-26 11:46:05,631] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04404616], dtype=float32), 0.5063385]
[2019-03-26 11:46:05,633] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.63333333333334, 90.66666666666667, 1.0, 2.0, 0.5782248946830332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 808021.0273824142, 808021.0273824149, 196758.5791830641]
[2019-03-26 11:46:05,633] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:46:05,636] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7034650e-36 1.0000000e+00 0.0000000e+00 7.3150085e-32 1.4542338e-19], sampled 0.5944812012461499
[2019-03-26 11:46:10,948] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04404616], dtype=float32), 0.5063385]
[2019-03-26 11:46:10,949] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.05282654833334, 82.66096491833333, 1.0, 2.0, 0.5738476955325702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801901.9479296941, 801901.9479296941, 195975.0676508331]
[2019-03-26 11:46:10,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:46:10,953] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.1627503e-38 1.0000000e+00 0.0000000e+00 2.7545212e-32 1.1127763e-21], sampled 0.64666112448792
[2019-03-26 11:46:25,288] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04404616], dtype=float32), 0.5063385]
[2019-03-26 11:46:25,289] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.13604362333334, 80.80713597, 1.0, 2.0, 0.3144957963716513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503219.8553954706, 503219.8553954712, 167340.3249820906]
[2019-03-26 11:46:25,291] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:46:25,293] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2002833e-29], sampled 0.9456544105587973
[2019-03-26 11:46:37,976] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8479.6297 2934911089.9297 833.0000
[2019-03-26 11:46:38,278] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8174.7227 3162058138.5136 1134.0000
[2019-03-26 11:46:38,311] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8727.9066 2845549698.2506 590.0000
[2019-03-26 11:46:38,563] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8840.7903 2786557633.5213 513.0000
[2019-03-26 11:46:38,638] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8351.7535 3005384920.4655 942.0000
[2019-03-26 11:46:39,654] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1050000, evaluation results [1050000.0, 8174.722730093238, 3162058138.513609, 1134.0, 8479.629675774797, 2934911089.9297247, 833.0, 8840.790317567378, 2786557633.52135, 513.0, 8351.753473861841, 3005384920.4655004, 942.0, 8727.906602688621, 2845549698.2505555, 590.0]
[2019-03-26 11:46:46,938] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:46:46,939] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:46:47,027] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-03-26 11:46:49,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8754255e-34 1.1565309e-13 3.5990244e-37 9.6938211e-29 1.0000000e+00], sum to 1.0000
[2019-03-26 11:46:49,054] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1120
[2019-03-26 11:46:49,057] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.45, 88.33333333333334, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 1.0, 1.0, 0.2668228914281722, 6.9112, 6.9112, 170.5573041426782, 670319.4509170505, 670319.4509170505, 255423.4821335067], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7686600.0000, 
sim time next is 7687200.0000, 
raw observation next is [25.4, 88.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2674795044163425, 6.911200000000001, 6.9112, 170.5573041426782, 669865.1468591506, 669865.1468591499, 255393.6923410975], 
processed observation next is [1.0, 1.0, 0.4028436018957346, 0.8866666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.10668232245895426, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.1860736519053196, 0.1860736519053194, 0.3811846154344739], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3506562], dtype=float32), -1.6230301]. 
=============================================
[2019-03-26 11:46:51,386] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1056175: loss 0.7202
[2019-03-26 11:46:51,394] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1056179: learning rate 0.0005
[2019-03-26 11:46:52,251] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6377072e-33 1.0000000e+00 0.0000000e+00 1.3551624e-29 3.2913336e-28], sum to 1.0000
[2019-03-26 11:46:52,258] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4972
[2019-03-26 11:46:52,264] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 85.66666666666667, 1.0, 2.0, 0.5195943645146639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726061.7533430048, 726061.7533430048, 186723.2850285625], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7762800.0000, 
sim time next is 7763400.0000, 
raw observation next is [27.05, 86.0, 1.0, 2.0, 0.5204531958061286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727262.2629279441, 727262.2629279447, 186862.8848569513], 
processed observation next is [1.0, 0.8695652173913043, 0.4810426540284361, 0.86, 1.0, 1.0, 0.4222327660314802, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20201729525776224, 0.2020172952577624, 0.27889982814470343], 
reward next is 0.7211, 
noisyNet noise sample is [array([-1.9830799], dtype=float32), 1.0372422]. 
=============================================
[2019-03-26 11:46:53,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:46:53,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:46:53,583] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-03-26 11:46:53,816] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2008832e-26 9.2639828e-01 7.2992303e-31 2.3107021e-28 7.3601663e-02], sum to 1.0000
[2019-03-26 11:46:53,823] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0977
[2019-03-26 11:46:53,827] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 86.33333333333334, 1.0, 2.0, 0.2436277264407792, 1.0, 2.0, 0.2436277264407792, 1.0, 2.0, 0.4113048565324481, 6.9112, 6.9112, 170.5573041426782, 1021446.088003819, 1021446.088003819, 282345.7817120624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7785600.0000, 
sim time next is 7786200.0000, 
raw observation next is [26.15, 86.66666666666666, 1.0, 2.0, 0.7005522796417735, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 979042.1266611277, 979042.1266611277, 220961.7371192963], 
processed observation next is [1.0, 0.08695652173913043, 0.43838862559241704, 0.8666666666666666, 1.0, 1.0, 0.6392196140262331, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2719561462947577, 0.2719561462947577, 0.32979363749148705], 
reward next is 0.6702, 
noisyNet noise sample is [array([0.09544079], dtype=float32), 0.032890003]. 
=============================================
[2019-03-26 11:46:55,264] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1058202: loss 0.0628
[2019-03-26 11:46:55,266] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1058202: learning rate 0.0005
[2019-03-26 11:47:01,816] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:01,817] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:01,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-03-26 11:47:02,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:02,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:02,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-03-26 11:47:02,669] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1061704: loss 1.5518
[2019-03-26 11:47:02,671] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1061704: learning rate 0.0005
[2019-03-26 11:47:03,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,117] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,148] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,149] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-03-26 11:47:03,211] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-03-26 11:47:03,297] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,299] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,360] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-03-26 11:47:03,446] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1062200: loss 1.6790
[2019-03-26 11:47:03,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1062201: learning rate 0.0005
[2019-03-26 11:47:03,454] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,455] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,496] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-03-26 11:47:03,531] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,531] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,555] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,556] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,569] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-03-26 11:47:03,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-03-26 11:47:03,624] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,624] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,640] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-03-26 11:47:03,666] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,666] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,670] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,670] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,683] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-03-26 11:47:03,710] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,711] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,716] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-03-26 11:47:03,756] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-03-26 11:47:03,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 11:47:03,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:03,902] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-03-26 11:47:05,710] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1063164: loss 0.0401
[2019-03-26 11:47:05,712] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1063164: learning rate 0.0005
[2019-03-26 11:47:07,946] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7247167e-36 1.0000000e+00 0.0000000e+00 2.0673809e-33 7.7326911e-28], sum to 1.0000
[2019-03-26 11:47:07,955] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6563
[2019-03-26 11:47:07,959] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 84.33333333333334, 1.0, 2.0, 0.358997254768441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552483.769309904, 552483.7693099045, 170839.8730776006], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 74400.0000, 
sim time next is 75000.0000, 
raw observation next is [23.18333333333334, 84.66666666666667, 1.0, 2.0, 0.3561662911640412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548715.2445120797, 548715.2445120803, 170540.4556415872], 
processed observation next is [1.0, 0.8695652173913043, 0.29778830963665126, 0.8466666666666667, 1.0, 1.0, 0.2242967363422183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15242090125335547, 0.15242090125335564, 0.2545379934949063], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.00141867], dtype=float32), -0.11934601]. 
=============================================
[2019-03-26 11:47:07,972] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[66.0267  ]
 [65.51227 ]
 [65.69052 ]
 [65.688416]
 [65.90322 ]], R is [[66.54819489]
 [66.62773132]
 [66.70619965]
 [66.78363037]
 [66.86010742]].
[2019-03-26 11:47:08,257] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1064339: loss 1.6905
[2019-03-26 11:47:08,265] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1064340: learning rate 0.0005
[2019-03-26 11:47:09,555] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1064952: loss 0.2749
[2019-03-26 11:47:09,557] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1064953: learning rate 0.0005
[2019-03-26 11:47:12,590] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1066357: loss 1.4072
[2019-03-26 11:47:12,594] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1066359: learning rate 0.0005
[2019-03-26 11:47:12,812] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1066457: loss 0.0537
[2019-03-26 11:47:12,814] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1066458: learning rate 0.0005
[2019-03-26 11:47:13,014] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1066551: loss 0.0373
[2019-03-26 11:47:13,020] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1066552: learning rate 0.0005
[2019-03-26 11:47:13,425] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0383766e-31 1.0000000e+00 0.0000000e+00 2.0595581e-24 2.5411999e-11], sum to 1.0000
[2019-03-26 11:47:13,435] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5103
[2019-03-26 11:47:13,443] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.45, 96.0, 1.0, 2.0, 0.3851173782438946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580498.9012723201, 580498.9012723208, 172932.9732346281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 153000.0000, 
sim time next is 153600.0000, 
raw observation next is [22.43333333333333, 96.0, 1.0, 2.0, 0.3820765921250238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576234.0410331846, 576234.0410331846, 172561.6602691386], 
processed observation next is [1.0, 0.782608695652174, 0.2622432859399683, 0.96, 1.0, 1.0, 0.25551396641569135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16006501139810683, 0.16006501139810683, 0.25755471681960984], 
reward next is 0.7424, 
noisyNet noise sample is [array([-0.88694847], dtype=float32), 0.63848627]. 
=============================================
[2019-03-26 11:47:13,556] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1066804: loss 0.0107
[2019-03-26 11:47:13,557] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1066804: learning rate 0.0005
[2019-03-26 11:47:13,593] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1066821: loss 0.8217
[2019-03-26 11:47:13,596] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1066822: learning rate 0.0005
[2019-03-26 11:47:13,637] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1066839: loss 0.1818
[2019-03-26 11:47:13,641] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1066839: learning rate 0.0005
[2019-03-26 11:47:13,805] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1066917: loss 3.2981
[2019-03-26 11:47:13,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1066917: learning rate 0.0005
[2019-03-26 11:47:13,915] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1066966: loss 8.0093
[2019-03-26 11:47:13,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1066967: learning rate 0.0005
[2019-03-26 11:47:14,026] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1067017: loss 1.9020
[2019-03-26 11:47:14,029] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1067019: learning rate 0.0005
[2019-03-26 11:47:14,071] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1067039: loss 1.2673
[2019-03-26 11:47:14,072] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1067039: learning rate 0.0005
[2019-03-26 11:47:14,319] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1067165: loss 0.0783
[2019-03-26 11:47:14,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1067166: learning rate 0.0005
[2019-03-26 11:47:14,503] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1067248: loss 0.0859
[2019-03-26 11:47:14,507] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1067250: learning rate 0.0005
[2019-03-26 11:47:16,973] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.7113377e-25], sum to 1.0000
[2019-03-26 11:47:16,979] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5442
[2019-03-26 11:47:16,984] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.13333333333333, 86.0, 1.0, 2.0, 0.3189720881451341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502668.9189887386, 502668.9189887393, 167189.863854938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 224400.0000, 
sim time next is 225000.0000, 
raw observation next is [22.1, 86.0, 1.0, 2.0, 0.3176992795855395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501055.3737746762, 501055.3737746769, 167077.0104738689], 
processed observation next is [0.0, 0.6086956521739131, 0.24644549763033188, 0.86, 1.0, 1.0, 0.17795093925968614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13918204827074337, 0.13918204827074357, 0.24936867234905805], 
reward next is 0.7506, 
noisyNet noise sample is [array([-0.05916244], dtype=float32), 0.686276]. 
=============================================
[2019-03-26 11:47:17,000] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.21048 ]
 [77.182335]
 [77.18611 ]
 [77.1102  ]
 [77.11526 ]], R is [[77.22132111]
 [77.19957733]
 [77.17788696]
 [77.15624237]
 [77.134758  ]].
[2019-03-26 11:47:17,500] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5904104e-38 1.0000000e+00 0.0000000e+00 2.4699795e-36 5.7716951e-19], sum to 1.0000
[2019-03-26 11:47:17,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1811
[2019-03-26 11:47:17,520] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333334, 87.0, 1.0, 2.0, 0.3060199014625193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486341.1507683891, 486341.1507683891, 166063.3599309269], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 233400.0000, 
sim time next is 234000.0000, 
raw observation next is [21.6, 87.0, 1.0, 2.0, 0.3057133130207178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 486206.4938149715, 486206.4938149721, 166059.3220237353], 
processed observation next is [0.0, 0.7391304347826086, 0.22274881516587688, 0.87, 1.0, 1.0, 0.1635100156876118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13505735939304764, 0.1350573593930478, 0.24784973436378402], 
reward next is 0.7522, 
noisyNet noise sample is [array([0.9492069], dtype=float32), 1.0409291]. 
=============================================
[2019-03-26 11:47:17,544] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.72359 ]
 [83.70745 ]
 [83.6748  ]
 [83.64639 ]
 [83.612465]], R is [[83.73204041]
 [83.64686584]
 [83.56247711]
 [83.47878265]
 [83.39577484]].
[2019-03-26 11:47:18,672] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1069167: loss 0.0920
[2019-03-26 11:47:18,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1069169: learning rate 0.0005
[2019-03-26 11:47:18,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.1539721e-37 7.2971863e-28], sum to 1.0000
[2019-03-26 11:47:18,876] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6255
[2019-03-26 11:47:18,881] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.7, 86.0, 1.0, 2.0, 0.2691632611297922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 436573.3455994083, 436573.3455994083, 162706.0330891417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 342000.0000, 
sim time next is 342600.0000, 
raw observation next is [20.68333333333333, 86.0, 1.0, 2.0, 0.2684175566462462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 435523.5626446234, 435523.5626446228, 162636.5003922702], 
processed observation next is [0.0, 1.0, 0.17930489731437588, 0.86, 1.0, 1.0, 0.1185753694533087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12097876740128427, 0.12097876740128412, 0.2427410453615973], 
reward next is 0.7573, 
noisyNet noise sample is [array([1.7678419], dtype=float32), 0.8557152]. 
=============================================
[2019-03-26 11:47:22,339] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1583616e-35 1.0000000e+00 0.0000000e+00 3.6374417e-34 4.4623617e-22], sum to 1.0000
[2019-03-26 11:47:22,354] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4173
[2019-03-26 11:47:22,359] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 77.5, 1.0, 2.0, 0.3108384143471128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490909.443054163, 490909.443054163, 166337.0988665487], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 315000.0000, 
sim time next is 315600.0000, 
raw observation next is [23.13333333333333, 77.66666666666667, 1.0, 2.0, 0.3094962625492548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 489258.1009426742, 489258.1009426742, 166226.0873306207], 
processed observation next is [0.0, 0.6521739130434783, 0.29541864139020524, 0.7766666666666667, 1.0, 1.0, 0.1680677862039214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13590502803963173, 0.13590502803963173, 0.2480986378068966], 
reward next is 0.7519, 
noisyNet noise sample is [array([-1.5189885], dtype=float32), 2.4923372]. 
=============================================
[2019-03-26 11:47:22,581] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1071143: loss 1.1342
[2019-03-26 11:47:22,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1071144: learning rate 0.0005
[2019-03-26 11:47:25,146] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1072300: loss 0.0965
[2019-03-26 11:47:25,154] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1072305: learning rate 0.0005
[2019-03-26 11:47:26,576] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1072939: loss 0.0365
[2019-03-26 11:47:26,581] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1072940: learning rate 0.0005
[2019-03-26 11:47:29,729] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1074381: loss 0.0379
[2019-03-26 11:47:29,734] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1074383: learning rate 0.0005
[2019-03-26 11:47:29,959] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1074487: loss 0.0283
[2019-03-26 11:47:29,961] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1074488: learning rate 0.0005
[2019-03-26 11:47:30,135] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1074566: loss 0.0192
[2019-03-26 11:47:30,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1074566: learning rate 0.0005
[2019-03-26 11:47:30,530] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1074742: loss 0.0262
[2019-03-26 11:47:30,531] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1074742: learning rate 0.0005
[2019-03-26 11:47:30,807] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1074865: loss 0.0294
[2019-03-26 11:47:30,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1074865: learning rate 0.0005
[2019-03-26 11:47:30,892] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1074904: loss 0.0087
[2019-03-26 11:47:30,896] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1074904: loss 0.0237
[2019-03-26 11:47:30,899] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1074904: learning rate 0.0005
[2019-03-26 11:47:30,901] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1074904: learning rate 0.0005
[2019-03-26 11:47:31,029] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1074967: loss 0.0139
[2019-03-26 11:47:31,032] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1074967: learning rate 0.0005
[2019-03-26 11:47:31,093] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 11:47:31,095] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:47:31,096] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:47:31,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:31,101] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:47:31,102] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:47:31,099] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:47:31,103] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:31,102] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:31,105] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:31,106] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:47:31,121] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run44
[2019-03-26 11:47:31,148] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run44
[2019-03-26 11:47:31,170] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run44
[2019-03-26 11:47:31,171] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run44
[2019-03-26 11:47:31,210] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run44
[2019-03-26 11:47:41,278] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1025003], dtype=float32), 0.48487958]
[2019-03-26 11:47:41,281] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.3, 53.0, 1.0, 2.0, 0.285831794188718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 460377.4101319971, 460377.4101319977, 164298.8800529283]
[2019-03-26 11:47:41,282] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:47:41,286] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.3171486e-35 8.1721155e-21], sampled 0.7124006568695576
[2019-03-26 11:47:45,110] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1025003], dtype=float32), 0.48487958]
[2019-03-26 11:47:45,112] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.15, 69.16666666666667, 1.0, 2.0, 0.3421141714631791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531770.5225193169, 531770.5225193169, 169282.3912233983]
[2019-03-26 11:47:45,113] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:47:45,115] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0568086e-33], sampled 0.592817701220385
[2019-03-26 11:47:55,906] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1025003], dtype=float32), 0.48487958]
[2019-03-26 11:47:55,907] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.23333333333333, 94.83333333333334, 1.0, 2.0, 0.4066870694485428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599598.29959398, 599598.29959398, 174287.8038466267]
[2019-03-26 11:47:55,908] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:47:55,911] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.139242e-31], sampled 0.8731396992843892
[2019-03-26 11:49:00,463] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1025003], dtype=float32), 0.48487958]
[2019-03-26 11:49:00,466] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.11666666666667, 75.0, 1.0, 2.0, 0.5250605276446781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733702.6034015524, 733702.6034015524, 187616.4269428337]
[2019-03-26 11:49:00,469] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:49:00,473] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.0379534e-35 2.1183944e-33], sampled 0.7377122606986226
[2019-03-26 11:49:01,616] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1025003], dtype=float32), 0.48487958]
[2019-03-26 11:49:01,619] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.68963372, 74.16133432, 1.0, 2.0, 0.7519302359250737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1050879.837295417, 1050879.837295417, 232459.8527840438]
[2019-03-26 11:49:01,621] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:49:01,626] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5114933e-36 1.0000000e+00 0.0000000e+00 5.8091840e-25 3.6647897e-15], sampled 0.6155726200775824
[2019-03-26 11:49:12,967] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1025003], dtype=float32), 0.48487958]
[2019-03-26 11:49:12,968] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.9, 56.0, 1.0, 2.0, 0.3939389622078006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589670.6571792588, 589670.6571792588, 173642.7863888592]
[2019-03-26 11:49:12,970] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:49:12,973] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.19446754e-29], sampled 0.6411379805361072
[2019-03-26 11:49:24,077] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1025003], dtype=float32), 0.48487958]
[2019-03-26 11:49:24,079] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.73217886333333, 92.20917051, 1.0, 2.0, 0.3783531121349569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575476.0153164268, 575476.0153164268, 172636.6992717219]
[2019-03-26 11:49:24,080] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:49:24,083] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6384893e-37 9.2464588e-20], sampled 0.9894934863388526
[2019-03-26 11:49:26,678] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8234.8288 3161926518.2253 979.0000
[2019-03-26 11:49:26,842] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8427.2036 3002733993.5654 771.0000
[2019-03-26 11:49:27,039] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8858.4764 2787708705.4538 455.0000
[2019-03-26 11:49:27,150] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8768.3825 2845647277.8426 490.0000
[2019-03-26 11:49:27,264] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8517.8345 2936248752.2424 752.0000
[2019-03-26 11:49:28,279] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1075000, evaluation results [1075000.0, 8234.8287663806, 3161926518.2253094, 979.0, 8517.834489373477, 2936248752.2423654, 752.0, 8858.476436909576, 2787708705.4537773, 455.0, 8427.20360555946, 3002733993.5653973, 771.0, 8768.38253292784, 2845647277.842612, 490.0]
[2019-03-26 11:49:28,418] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1075072: loss 0.0050
[2019-03-26 11:49:28,419] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1075072: loss 0.0021
[2019-03-26 11:49:28,420] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1075072: learning rate 0.0005
[2019-03-26 11:49:28,423] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1075072: learning rate 0.0005
[2019-03-26 11:49:28,538] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1075127: loss 0.0018
[2019-03-26 11:49:28,539] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1075127: learning rate 0.0005
[2019-03-26 11:49:28,753] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1075226: loss 1.5511
[2019-03-26 11:49:28,756] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1075226: learning rate 0.0005
[2019-03-26 11:49:28,986] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0593831e-29 1.0000000e+00 1.8277809e-32 4.0027971e-30 3.0784980e-10], sum to 1.0000
[2019-03-26 11:49:28,997] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3230
[2019-03-26 11:49:29,001] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 53.0, 1.0, 2.0, 0.600411052620166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 982013.5256071035, 982013.5256071041, 215640.7408157554], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 484800.0000, 
sim time next is 485400.0000, 
raw observation next is [25.15, 53.0, 1.0, 2.0, 0.5912178189896552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 967586.2510941398, 967586.2510941398, 213693.0376814211], 
processed observation next is [1.0, 0.6086956521739131, 0.3909952606635071, 0.53, 1.0, 1.0, 0.5074913481803074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26877395863726106, 0.26877395863726106, 0.31894483236033], 
reward next is 0.6811, 
noisyNet noise sample is [array([0.92230046], dtype=float32), 1.240739]. 
=============================================
[2019-03-26 11:49:31,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0806228e-35], sum to 1.0000
[2019-03-26 11:49:31,705] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7508
[2019-03-26 11:49:31,712] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.31666666666667, 84.5, 1.0, 2.0, 0.2362766085890909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 391222.0854305044, 391222.0854305038, 159458.5156744316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 511800.0000, 
sim time next is 512400.0000, 
raw observation next is [19.23333333333333, 85.0, 1.0, 2.0, 0.2357627921104252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 390452.6277067414, 390452.6277067414, 159402.8310996984], 
processed observation next is [1.0, 0.9565217391304348, 0.11058451816745649, 0.85, 1.0, 1.0, 0.07923227965111468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10845906325187261, 0.10845906325187261, 0.23791467328313196], 
reward next is 0.7621, 
noisyNet noise sample is [array([1.0253197], dtype=float32), 0.0963641]. 
=============================================
[2019-03-26 11:49:32,991] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1077202: loss 0.1526
[2019-03-26 11:49:32,993] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1077204: learning rate 0.0005
[2019-03-26 11:49:33,464] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0749166e-27], sum to 1.0000
[2019-03-26 11:49:33,473] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0947
[2019-03-26 11:49:33,482] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 56.0, 1.0, 2.0, 0.3915364578328762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642331.2885569048, 642331.2885569048, 178239.6055975177], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 565200.0000, 
sim time next is 565800.0000, 
raw observation next is [24.31666666666667, 56.5, 1.0, 2.0, 0.4380545207939711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718684.7162382543, 718684.716238255, 185290.6793592843], 
processed observation next is [1.0, 0.5652173913043478, 0.3515007898894157, 0.565, 1.0, 1.0, 0.3229572539686399, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1996346433995151, 0.19963464339951528, 0.27655325277505116], 
reward next is 0.7234, 
noisyNet noise sample is [array([-1.1995238], dtype=float32), 0.9195842]. 
=============================================
[2019-03-26 11:49:36,132] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5822887e-32], sum to 1.0000
[2019-03-26 11:49:36,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5642
[2019-03-26 11:49:36,145] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.98333333333333, 97.83333333333334, 1.0, 2.0, 0.3643513155949491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 553924.9319534462, 553924.9319534457, 170759.8298645259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1032600.0000, 
sim time next is 1033200.0000, 
raw observation next is [22.0, 98.0, 1.0, 2.0, 0.3695972679602647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561246.9547247476, 561246.9547247476, 171367.356081686], 
processed observation next is [1.0, 1.0, 0.2417061611374408, 0.98, 1.0, 1.0, 0.24047863609670447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15590193186798543, 0.15590193186798543, 0.2557721732562478], 
reward next is 0.7442, 
noisyNet noise sample is [array([0.34019512], dtype=float32), 0.35592237]. 
=============================================
[2019-03-26 11:49:37,485] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1079151: loss 0.0208
[2019-03-26 11:49:37,489] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1079151: learning rate 0.0005
[2019-03-26 11:49:38,425] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.803573e-28], sum to 1.0000
[2019-03-26 11:49:38,434] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2662
[2019-03-26 11:49:38,441] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 60.66666666666666, 1.0, 2.0, 0.5897084698377402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 966447.4489664057, 966447.4489664051, 213397.3104552402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 643200.0000, 
sim time next is 643800.0000, 
raw observation next is [23.83333333333333, 59.83333333333334, 1.0, 2.0, 0.5921751943221502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970182.2452247476, 970182.2452247476, 213914.8060947911], 
processed observation next is [1.0, 0.43478260869565216, 0.32859399684044216, 0.5983333333333334, 1.0, 1.0, 0.5086448124363254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26949506811798546, 0.26949506811798546, 0.31927582999222553], 
reward next is 0.6807, 
noisyNet noise sample is [array([0.8400697], dtype=float32), 0.05086862]. 
=============================================
[2019-03-26 11:49:38,772] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7449616e-37 1.0000000e+00 0.0000000e+00 1.5097365e-35 4.2667698e-19], sum to 1.0000
[2019-03-26 11:49:38,782] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7587
[2019-03-26 11:49:38,787] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.16666666666667, 1.0, 2.0, 0.5052768654037862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831122.8556594879, 831122.8556594879, 196791.6460520749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 659400.0000, 
sim time next is 660000.0000, 
raw observation next is [24.7, 53.33333333333334, 1.0, 2.0, 0.5585032590207535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 918411.6264856434, 918411.626485644, 206990.280047354], 
processed observation next is [1.0, 0.6521739130434783, 0.3696682464454976, 0.5333333333333334, 1.0, 1.0, 0.4680762156876548, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2551143406904565, 0.2551143406904567, 0.30894071648858806], 
reward next is 0.6911, 
noisyNet noise sample is [array([0.98167044], dtype=float32), 0.020156557]. 
=============================================
[2019-03-26 11:49:38,802] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[72.169624]
 [71.73842 ]
 [71.25302 ]
 [71.258446]
 [71.37368 ]], R is [[71.87221527]
 [71.85977936]
 [71.83238983]
 [71.78394318]
 [71.73669434]].
[2019-03-26 11:49:38,967] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2516421e-32 1.0000000e+00 0.0000000e+00 7.4148489e-34 3.5826953e-15], sum to 1.0000
[2019-03-26 11:49:38,975] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2228
[2019-03-26 11:49:38,980] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 75.33333333333334, 1.0, 2.0, 0.6293684352386377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 988845.7697850259, 988845.7697850252, 219286.1492592628], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1080600.0000, 
sim time next is 1081200.0000, 
raw observation next is [23.93333333333334, 74.66666666666667, 1.0, 2.0, 0.5078247384887102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 796602.1151457683, 796602.1151457677, 194948.9142936372], 
processed observation next is [1.0, 0.5217391304347826, 0.3333333333333337, 0.7466666666666667, 1.0, 1.0, 0.4070177572153135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22127836531826897, 0.2212783653182688, 0.29096852879647345], 
reward next is 0.7090, 
noisyNet noise sample is [array([0.15487432], dtype=float32), 0.60404533]. 
=============================================
[2019-03-26 11:49:39,929] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1080289: loss 1.0397
[2019-03-26 11:49:39,932] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1080290: learning rate 0.0005
[2019-03-26 11:49:41,177] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1080865: loss 0.4873
[2019-03-26 11:49:41,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1080868: learning rate 0.0005
[2019-03-26 11:49:44,238] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1082297: loss 0.2206
[2019-03-26 11:49:44,239] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1082297: learning rate 0.0005
[2019-03-26 11:49:44,766] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1082552: loss 0.0361
[2019-03-26 11:49:44,772] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1082553: learning rate 0.0005
[2019-03-26 11:49:44,806] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1082569: loss 0.1840
[2019-03-26 11:49:44,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1082570: learning rate 0.0005
[2019-03-26 11:49:45,205] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1082750: loss 0.0754
[2019-03-26 11:49:45,206] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1082751: learning rate 0.0005
[2019-03-26 11:49:45,353] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1082825: loss 0.0286
[2019-03-26 11:49:45,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1082825: learning rate 0.0005
[2019-03-26 11:49:45,433] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1082858: loss 0.0390
[2019-03-26 11:49:45,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1082858: learning rate 0.0005
[2019-03-26 11:49:45,531] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1082905: loss 0.0367
[2019-03-26 11:49:45,532] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1082905: learning rate 0.0005
[2019-03-26 11:49:45,815] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1083034: loss 0.0427
[2019-03-26 11:49:45,818] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1083036: learning rate 0.0005
[2019-03-26 11:49:45,901] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1083074: loss 0.0232
[2019-03-26 11:49:45,904] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1083074: learning rate 0.0005
[2019-03-26 11:49:45,996] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1083115: loss 0.0457
[2019-03-26 11:49:46,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1083117: learning rate 0.0005
[2019-03-26 11:49:46,094] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1083160: loss 0.0227
[2019-03-26 11:49:46,098] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1083162: learning rate 0.0005
[2019-03-26 11:49:46,108] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1083165: loss 0.0322
[2019-03-26 11:49:46,109] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1083167: learning rate 0.0005
[2019-03-26 11:49:50,331] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1085124: loss 0.0818
[2019-03-26 11:49:50,336] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1085124: learning rate 0.0005
[2019-03-26 11:49:52,814] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:49:52,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2041
[2019-03-26 11:49:52,827] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 89.0, 1.0, 2.0, 0.2893319553539936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463545.5184226623, 463545.5184226629, 164502.6423600968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 874800.0000, 
sim time next is 875400.0000, 
raw observation next is [20.98333333333333, 88.83333333333334, 1.0, 2.0, 0.2885767423864444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462619.7691883009, 462619.7691883009, 164441.5745848281], 
processed observation next is [0.0, 0.13043478260869565, 0.1935229067930489, 0.8883333333333334, 1.0, 1.0, 0.14286354504390888, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1285054914411947, 0.1285054914411947, 0.24543518594750463], 
reward next is 0.7546, 
noisyNet noise sample is [array([0.38648883], dtype=float32), -0.3415703]. 
=============================================
[2019-03-26 11:49:55,033] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1087224: loss 0.4973
[2019-03-26 11:49:55,038] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1087224: learning rate 0.0005
[2019-03-26 11:49:56,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0181276e-37 1.0000000e+00 0.0000000e+00 4.4275691e-29 7.2113658e-15], sum to 1.0000
[2019-03-26 11:49:56,378] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3151
[2019-03-26 11:49:56,381] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.66666666666666, 1.0, 2.0, 0.340495761810958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526394.0971710646, 526394.0971710652, 168768.8884986565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 952800.0000, 
sim time next is 953400.0000, 
raw observation next is [21.8, 94.83333333333333, 1.0, 2.0, 0.341666743957812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527903.3872326761, 527903.3872326761, 168880.8325417688], 
processed observation next is [1.0, 0.0, 0.23222748815165886, 0.9483333333333333, 1.0, 1.0, 0.20682740235880961, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1466398297868545, 0.1466398297868545, 0.2520609440921922], 
reward next is 0.7479, 
noisyNet noise sample is [array([-0.9784491], dtype=float32), -1.0047477]. 
=============================================
[2019-03-26 11:49:57,273] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1088255: loss 0.0007
[2019-03-26 11:49:57,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1088255: learning rate 0.0005
[2019-03-26 11:49:58,562] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1088857: loss 0.0027
[2019-03-26 11:49:58,565] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1088858: learning rate 0.0005
[2019-03-26 11:50:01,742] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1090338: loss 0.0008
[2019-03-26 11:50:01,747] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1090338: learning rate 0.0005
[2019-03-26 11:50:02,134] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1090519: loss 0.0116
[2019-03-26 11:50:02,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1090519: learning rate 0.0005
[2019-03-26 11:50:02,270] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1090579: loss 0.0021
[2019-03-26 11:50:02,272] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1090579: learning rate 0.0005
[2019-03-26 11:50:02,708] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1090781: loss 0.0223
[2019-03-26 11:50:02,710] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1090781: learning rate 0.0005
[2019-03-26 11:50:02,772] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1090810: loss 0.0050
[2019-03-26 11:50:02,774] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1090810: learning rate 0.0005
[2019-03-26 11:50:02,849] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1090845: loss 0.0029
[2019-03-26 11:50:02,850] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1090845: learning rate 0.0005
[2019-03-26 11:50:02,958] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1090894: loss 0.0008
[2019-03-26 11:50:02,960] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1090895: learning rate 0.0005
[2019-03-26 11:50:03,220] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1091018: loss 0.0165
[2019-03-26 11:50:03,222] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1091018: learning rate 0.0005
[2019-03-26 11:50:03,437] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1091114: loss 0.0028
[2019-03-26 11:50:03,439] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1091114: learning rate 0.0005
[2019-03-26 11:50:03,451] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1091120: loss 0.0010
[2019-03-26 11:50:03,456] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1091123: learning rate 0.0005
[2019-03-26 11:50:03,468] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1091127: loss 0.0097
[2019-03-26 11:50:03,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1091128: learning rate 0.0005
[2019-03-26 11:50:03,733] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1091252: loss 0.4324
[2019-03-26 11:50:03,740] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1091252: learning rate 0.0005
[2019-03-26 11:50:03,919] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2650685e-38], sum to 1.0000
[2019-03-26 11:50:03,928] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0018
[2019-03-26 11:50:03,932] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.05, 51.0, 1.0, 2.0, 0.3449039851276267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 530137.3790334774, 530137.3790334767, 168974.470872795], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1511400.0000, 
sim time next is 1512000.0000, 
raw observation next is [29.1, 51.0, 1.0, 2.0, 0.3480171406727051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534257.5264887547, 534257.5264887554, 169288.3780549595], 
processed observation next is [0.0, 0.5217391304347826, 0.5781990521327015, 0.51, 1.0, 1.0, 0.21447848273819894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1484048684690985, 0.1484048684690987, 0.2526692209775515], 
reward next is 0.7473, 
noisyNet noise sample is [array([1.3991781], dtype=float32), -0.40542975]. 
=============================================
[2019-03-26 11:50:03,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[73.21661]
 [73.16779]
 [73.10806]
 [73.05734]
 [73.01712]], R is [[73.35353851]
 [73.36780548]
 [73.38219452]
 [73.39640808]
 [73.4105072 ]].
[2019-03-26 11:50:04,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9962979e-30 1.0000000e+00 2.3630671e-34 1.8259151e-30 2.2448572e-14], sum to 1.0000
[2019-03-26 11:50:04,965] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9538
[2019-03-26 11:50:04,971] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 74.0, 1.0, 2.0, 0.4802119890069992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752163.5312592188, 752163.5312592195, 190034.2245998105], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1081800.0000, 
sim time next is 1082400.0000, 
raw observation next is [24.26666666666667, 73.33333333333334, 1.0, 2.0, 0.4124938654107276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645115.6366862279, 645115.6366862279, 179232.8502894043], 
processed observation next is [1.0, 0.5217391304347826, 0.34913112164297017, 0.7333333333333334, 1.0, 1.0, 0.29216128362738264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17919878796839664, 0.17919878796839664, 0.2675117168498572], 
reward next is 0.7325, 
noisyNet noise sample is [array([-0.01796141], dtype=float32), -0.5010766]. 
=============================================
[2019-03-26 11:50:07,820] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1093153: loss -41.8306
[2019-03-26 11:50:07,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1093153: learning rate 0.0005
[2019-03-26 11:50:09,067] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.2301499e-35 1.0204758e-12 9.1260181e-35 1.0485943e-31 1.0000000e+00], sum to 1.0000
[2019-03-26 11:50:09,075] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1570
[2019-03-26 11:50:09,079] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.6, 59.33333333333334, 1.0, 2.0, 0.3348037498574502, 1.0, 1.0, 0.3348037498574502, 1.0, 1.0, 0.5864847626198708, 6.911199999999999, 6.9112, 170.5573041426782, 1513835.442437977, 1513835.442437978, 329779.8153817833], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [27.6, 59.0, 1.0, 2.0, 0.3408374997997445, 1.0, 2.0, 0.3408374997997445, 1.0, 2.0, 0.5903736788687948, 6.9112, 6.9112, 170.5573041426782, 1515110.946642249, 1515110.946642249, 329873.598934782], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.59, 1.0, 1.0, 0.20582831301174034, 1.0, 1.0, 0.20582831301174034, 1.0, 1.0, 0.5004557059375546, 0.0, 0.0, 0.8375144448122397, 0.42086415184506915, 0.42086415184506915, 0.49234865512654036], 
reward next is 0.5077, 
noisyNet noise sample is [array([0.852529], dtype=float32), 0.07936572]. 
=============================================
[2019-03-26 11:50:10,316] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2654916e-28 8.7224329e-01 0.0000000e+00 6.2012450e-23 1.2775674e-01], sum to 1.0000
[2019-03-26 11:50:10,321] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7564
[2019-03-26 11:50:10,325] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 58.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1962876349601329, 6.911200000000001, 6.9112, 170.5573041426782, 508413.1355630038, 508413.1355630031, 231833.955504419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1185600.0000, 
sim time next is 1186200.0000, 
raw observation next is [27.15, 59.5, 1.0, 2.0, 0.3258408572148598, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500986.962171548, 500986.962171548, 166677.7605528618], 
processed observation next is [1.0, 0.7391304347826086, 0.485781990521327, 0.595, 1.0, 1.0, 0.187760068933566, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13916304504765223, 0.13916304504765223, 0.24877277694456984], 
reward next is 0.7512, 
noisyNet noise sample is [array([-0.6271318], dtype=float32), -0.6599244]. 
=============================================
[2019-03-26 11:50:11,930] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1095147: loss 0.0024
[2019-03-26 11:50:11,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1095147: learning rate 0.0005
[2019-03-26 11:50:12,599] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1823315e-22], sum to 1.0000
[2019-03-26 11:50:12,607] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7746
[2019-03-26 11:50:12,614] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 94.0, 1.0, 2.0, 0.3523946565407417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 547468.391229226, 547468.3912292254, 170556.3222392198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1225800.0000, 
sim time next is 1226400.0000, 
raw observation next is [21.66666666666667, 94.33333333333333, 1.0, 2.0, 0.3556166477422023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552366.4180396085, 552366.4180396092, 170960.6782596908], 
processed observation next is [1.0, 0.17391304347826086, 0.22590837282780438, 0.9433333333333332, 1.0, 1.0, 0.22363451535205098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15343511612211347, 0.15343511612211366, 0.2551651914323743], 
reward next is 0.7448, 
noisyNet noise sample is [array([-0.31215882], dtype=float32), -0.9863627]. 
=============================================
[2019-03-26 11:50:14,497] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1096319: loss 0.0961
[2019-03-26 11:50:14,501] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1096320: learning rate 0.0005
[2019-03-26 11:50:15,179] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3475432e-27 1.0000000e+00 1.9708110e-35 1.4971529e-21 1.9054776e-09], sum to 1.0000
[2019-03-26 11:50:15,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4689
[2019-03-26 11:50:15,196] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 90.0, 1.0, 2.0, 0.6095514503621252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954581.580616492, 954581.5806164913, 214705.5566387161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1344000.0000, 
sim time next is 1344600.0000, 
raw observation next is [21.75, 89.5, 1.0, 2.0, 0.6325786747451726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 994203.1120139954, 994203.1120139954, 220021.8074538035], 
processed observation next is [1.0, 0.5652173913043478, 0.2298578199052133, 0.895, 1.0, 1.0, 0.5573237045122561, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2761675311149987, 0.2761675311149987, 0.3283907573937366], 
reward next is 0.6716, 
noisyNet noise sample is [array([1.0650544], dtype=float32), 0.045465264]. 
=============================================
[2019-03-26 11:50:15,866] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1096933: loss 0.6628
[2019-03-26 11:50:15,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1096933: learning rate 0.0005
[2019-03-26 11:50:17,109] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1870523e-25 2.0625919e-10 4.4046921e-29 6.1247708e-27 1.0000000e+00], sum to 1.0000
[2019-03-26 11:50:17,115] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7032
[2019-03-26 11:50:17,121] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.35, 94.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2594718583722945, 6.9112, 6.9112, 170.5573041426782, 652839.5051760608, 652839.5051760608, 252844.2849937814], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1294200.0000, 
sim time next is 1294800.0000, 
raw observation next is [24.33333333333334, 94.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2589819756740179, 6.911200000000001, 6.9112, 170.5573041426782, 651784.1408082135, 651784.1408082128, 252687.0064188422], 
processed observation next is [1.0, 1.0, 0.35229067930489766, 0.94, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0963194825292901, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.18105115022450374, 0.18105115022450355, 0.3771447856997645], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8792809], dtype=float32), -1.5364282]. 
=============================================
[2019-03-26 11:50:18,934] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1098314: loss 0.1509
[2019-03-26 11:50:18,935] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1098314: learning rate 0.0005
[2019-03-26 11:50:19,350] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1098508: loss 0.0379
[2019-03-26 11:50:19,353] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1098509: learning rate 0.0005
[2019-03-26 11:50:19,564] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1098599: loss 0.0352
[2019-03-26 11:50:19,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1098600: learning rate 0.0005
[2019-03-26 11:50:19,844] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9212206e-32 9.9999833e-01 0.0000000e+00 4.0734000e-23 1.7126880e-06], sum to 1.0000
[2019-03-26 11:50:19,848] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0952
[2019-03-26 11:50:19,857] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 88.0, 1.0, 2.0, 0.6708684931584351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1065116.051333728, 1065116.051333728, 229682.4849612846], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1346400.0000, 
sim time next is 1347000.0000, 
raw observation next is [21.46666666666667, 88.16666666666667, 1.0, 2.0, 0.5704487401589582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 906774.0291150539, 906774.0291150539, 207844.0228276209], 
processed observation next is [1.0, 0.6086956521739131, 0.21642969984202226, 0.8816666666666667, 1.0, 1.0, 0.482468361637299, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25188167475418166, 0.25188167475418166, 0.3102149594442103], 
reward next is 0.6898, 
noisyNet noise sample is [array([1.1383262], dtype=float32), 0.33245692]. 
=============================================
[2019-03-26 11:50:19,887] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.37071]
 [75.54843]
 [75.80486]
 [76.18309]
 [76.2188 ]], R is [[75.59308624]
 [75.49434662]
 [75.39343262]
 [75.29653931]
 [75.21517944]].
[2019-03-26 11:50:19,927] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1098764: loss 2.5481
[2019-03-26 11:50:19,931] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1098764: learning rate 0.0005
[2019-03-26 11:50:20,023] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1098807: loss 0.0361
[2019-03-26 11:50:20,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1098807: learning rate 0.0005
[2019-03-26 11:50:20,035] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1098811: loss 0.1048
[2019-03-26 11:50:20,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1098815: learning rate 0.0005
[2019-03-26 11:50:20,289] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1098925: loss 0.1000
[2019-03-26 11:50:20,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1098925: learning rate 0.0005
[2019-03-26 11:50:20,407] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1098979: loss 0.4029
[2019-03-26 11:50:20,411] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1098979: learning rate 0.0005
[2019-03-26 11:50:20,551] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1099040: loss 1.2933
[2019-03-26 11:50:20,555] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1099042: learning rate 0.0005
[2019-03-26 11:50:20,699] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1099108: loss 0.2625
[2019-03-26 11:50:20,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1099109: learning rate 0.0005
[2019-03-26 11:50:20,788] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1099144: loss 0.0559
[2019-03-26 11:50:20,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1099145: learning rate 0.0005
[2019-03-26 11:50:20,944] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1099216: loss 0.0212
[2019-03-26 11:50:20,945] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1099216: learning rate 0.0005
[2019-03-26 11:50:21,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.6199566e-32 9.8688138e-01 8.3964495e-37 4.9538941e-32 1.3118654e-02], sum to 1.0000
[2019-03-26 11:50:21,049] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5570
[2019-03-26 11:50:21,057] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 88.66666666666667, 1.0, 2.0, 0.6527409758815224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912195.7474786133, 912195.7474786133, 210974.1382841424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2184000.0000, 
sim time next is 2184600.0000, 
raw observation next is [26.8, 87.83333333333333, 1.0, 2.0, 0.6433061124819769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 899005.0866422273, 899005.0866422273, 209083.0829255941], 
processed observation next is [1.0, 0.2608695652173913, 0.4691943127962086, 0.8783333333333333, 1.0, 1.0, 0.5702483282915384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24972363517839646, 0.24972363517839646, 0.31206430287402104], 
reward next is 0.6879, 
noisyNet noise sample is [array([1.8943212], dtype=float32), 0.4557411]. 
=============================================
[2019-03-26 11:50:22,672] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 11:50:22,674] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:50:22,675] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:50:22,675] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:50:22,676] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:50:22,678] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:50:22,679] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:50:22,680] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:50:22,681] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:50:22,686] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:50:22,688] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:50:22,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run45
[2019-03-26 11:50:22,726] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run45
[2019-03-26 11:50:22,748] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run45
[2019-03-26 11:50:22,749] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run45
[2019-03-26 11:50:22,791] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run45
[2019-03-26 11:50:29,440] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13948673], dtype=float32), 0.51737463]
[2019-03-26 11:50:29,441] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.61809128333333, 74.20081634666667, 1.0, 2.0, 0.2592261216433476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 432859.0563081789, 432859.0563081796, 160983.3443459477]
[2019-03-26 11:50:29,442] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:50:29,446] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2475298e-30], sampled 0.14985198740162076
[2019-03-26 11:50:43,878] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13948673], dtype=float32), 0.51737463]
[2019-03-26 11:50:43,879] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [17.43333333333333, 92.16666666666667, 1.0, 2.0, 0.2242909200312412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 374157.7234742764, 374157.7234742758, 157811.5008398094]
[2019-03-26 11:50:43,881] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:50:43,884] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8234442e-35], sampled 0.4826274680303694
[2019-03-26 11:50:50,648] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13948673], dtype=float32), 0.51737463]
[2019-03-26 11:50:50,650] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.2, 93.66666666666667, 1.0, 2.0, 0.3612490419165925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 553767.9510043203, 553767.9510043203, 170886.4295385744]
[2019-03-26 11:50:50,652] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:50:50,656] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.862001e-28], sampled 0.07096015703963898
[2019-03-26 11:50:51,137] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13948673], dtype=float32), 0.51737463]
[2019-03-26 11:50:51,138] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.13333333333333, 87.0, 1.0, 2.0, 0.5455974064124931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834626.7173105812, 834626.7173105812, 199787.1141120967]
[2019-03-26 11:50:51,141] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:50:51,143] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1942606e-35 1.0000000e+00 0.0000000e+00 2.4720773e-32 5.2995564e-12], sampled 0.002600121432380309
[2019-03-26 11:51:04,142] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13948673], dtype=float32), 0.51737463]
[2019-03-26 11:51:04,145] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.61835757, 99.65526069, 1.0, 2.0, 0.2938331891016036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471945.9611973386, 471945.9611973386, 165093.5418208837]
[2019-03-26 11:51:04,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:51:04,148] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3340355e-38 1.0000000e+00 0.0000000e+00 2.9948184e-34 8.3681355e-16], sampled 0.5943169855420631
[2019-03-26 11:51:10,996] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13948673], dtype=float32), 0.51737463]
[2019-03-26 11:51:10,997] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.08799076, 64.35790757, 1.0, 2.0, 0.5228258142093563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730578.8132434135, 730578.8132434142, 187248.3456481208]
[2019-03-26 11:51:10,998] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:51:11,001] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3775165e-37 1.0000000e+00 0.0000000e+00 3.5262405e-35 1.4236028e-16], sampled 0.9296620086848022
[2019-03-26 11:51:37,025] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13948673], dtype=float32), 0.51737463]
[2019-03-26 11:51:37,027] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 59.0, 1.0, 2.0, 0.5230332995882274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730868.8459204002, 730868.8459204008, 187284.1077407033]
[2019-03-26 11:51:37,027] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:51:37,031] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.3655588e-37 2.6268248e-20], sampled 0.9029095471972607
[2019-03-26 11:52:01,349] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13948673], dtype=float32), 0.51737463]
[2019-03-26 11:52:01,350] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.76666666666667, 70.66666666666667, 1.0, 2.0, 0.9608612065285895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1343061.476534453, 1343061.476534453, 287229.7585356228]
[2019-03-26 11:52:01,351] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:52:01,354] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.4671365e-33 9.9994040e-01 0.0000000e+00 5.0701802e-30 5.9660550e-05], sampled 0.6169750488188963
[2019-03-26 11:52:07,060] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13948673], dtype=float32), 0.51737463]
[2019-03-26 11:52:07,061] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.83333333333334, 89.16666666666667, 1.0, 2.0, 0.5238323674095936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746282.3318734601, 746282.3318734607, 189240.4292634965]
[2019-03-26 11:52:07,062] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:52:07,065] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.3999864e-38 1.0000000e+00 0.0000000e+00 1.7262116e-37 3.3328437e-17], sampled 0.42473835387748216
[2019-03-26 11:52:18,850] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8217.1732 3170161277.0253 993.0000
[2019-03-26 11:52:18,908] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8530.1983 2945797556.7010 699.0000
[2019-03-26 11:52:18,972] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8391.9968 3017266729.4396 809.0000
[2019-03-26 11:52:19,031] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8774.6769 2854692813.9144 446.0000
[2019-03-26 11:52:19,035] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8848.2743 2799607088.0511 439.0000
[2019-03-26 11:52:20,050] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1100000, evaluation results [1100000.0, 8217.173169322457, 3170161277.0252905, 993.0, 8530.198251706988, 2945797556.700974, 699.0, 8848.274308847682, 2799607088.051101, 439.0, 8391.996785642037, 3017266729.4396462, 809.0, 8774.676888601074, 2854692813.91443, 446.0]
[2019-03-26 11:52:20,607] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0271374e-27], sum to 1.0000
[2019-03-26 11:52:20,615] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2100
[2019-03-26 11:52:20,622] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333333, 82.33333333333334, 1.0, 2.0, 0.4220432983190036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 613200.330282592, 613200.3302825913, 175305.1974712079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1428000.0000, 
sim time next is 1428600.0000, 
raw observation next is [25.66666666666667, 80.66666666666667, 1.0, 2.0, 0.4230837763388082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612587.0450986268, 612587.0450986268, 175183.2670321294], 
processed observation next is [0.0, 0.5217391304347826, 0.4154818325434442, 0.8066666666666668, 1.0, 1.0, 0.30492021245639545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17016306808295187, 0.17016306808295187, 0.2614675627345215], 
reward next is 0.7385, 
noisyNet noise sample is [array([-0.01626872], dtype=float32), 0.99927425]. 
=============================================
[2019-03-26 11:52:22,643] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1101200: loss 3.7071
[2019-03-26 11:52:22,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1101200: learning rate 0.0005
[2019-03-26 11:52:22,994] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3792667e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.2413840e-23], sum to 1.0000
[2019-03-26 11:52:23,003] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0007
[2019-03-26 11:52:23,009] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 95.33333333333334, 1.0, 2.0, 0.366633294487637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558481.7451203557, 558481.7451203557, 171183.0095961729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1462800.0000, 
sim time next is 1463400.0000, 
raw observation next is [22.15, 95.5, 1.0, 2.0, 0.3652517941314636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556892.7480991762, 556892.7480991769, 171062.7908768603], 
processed observation next is [0.0, 0.9565217391304348, 0.24881516587677724, 0.955, 1.0, 1.0, 0.23524312545959467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15469243002754895, 0.15469243002754915, 0.25531759832367207], 
reward next is 0.7447, 
noisyNet noise sample is [array([-0.64755434], dtype=float32), -0.13307267]. 
=============================================
[2019-03-26 11:52:26,891] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1103176: loss -60.4700
[2019-03-26 11:52:26,895] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1103177: learning rate 0.0005
[2019-03-26 11:52:27,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8371558e-31], sum to 1.0000
[2019-03-26 11:52:27,468] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6576
[2019-03-26 11:52:27,474] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.55, 51.16666666666666, 1.0, 2.0, 0.357107558748814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 541535.631387054, 541535.6313870547, 169667.6832419138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1516200.0000, 
sim time next is 1516800.0000, 
raw observation next is [29.6, 51.33333333333334, 1.0, 2.0, 0.364549626844463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 551523.5021288316, 551523.5021288322, 170466.3297556811], 
processed observation next is [0.0, 0.5652173913043478, 0.6018957345971565, 0.5133333333333334, 1.0, 1.0, 0.23439714077646145, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15320097281356432, 0.15320097281356448, 0.25442735784430015], 
reward next is 0.7456, 
noisyNet noise sample is [array([-0.0479321], dtype=float32), -0.90325284]. 
=============================================
[2019-03-26 11:52:29,301] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1104300: loss 0.0014
[2019-03-26 11:52:29,305] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1104300: learning rate 0.0005
[2019-03-26 11:52:30,545] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1104871: loss 0.0012
[2019-03-26 11:52:30,547] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1104872: learning rate 0.0005
[2019-03-26 11:52:33,640] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1106311: loss 0.0862
[2019-03-26 11:52:33,641] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1106311: learning rate 0.0005
[2019-03-26 11:52:33,995] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1106475: loss 0.0009
[2019-03-26 11:52:33,998] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1106475: learning rate 0.0005
[2019-03-26 11:52:34,173] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1106557: loss 0.0537
[2019-03-26 11:52:34,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1106557: learning rate 0.0005
[2019-03-26 11:52:34,478] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1106708: loss 0.0200
[2019-03-26 11:52:34,480] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1106709: learning rate 0.0005
[2019-03-26 11:52:34,645] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1106782: loss 0.0051
[2019-03-26 11:52:34,648] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1106782: learning rate 0.0005
[2019-03-26 11:52:34,791] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1106849: loss 0.0040
[2019-03-26 11:52:34,794] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1106849: learning rate 0.0005
[2019-03-26 11:52:34,866] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1106882: loss 0.0172
[2019-03-26 11:52:34,869] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1106882: learning rate 0.0005
[2019-03-26 11:52:34,950] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1106920: loss 0.0237
[2019-03-26 11:52:34,954] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1106920: learning rate 0.0005
[2019-03-26 11:52:35,196] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1107038: loss 0.0204
[2019-03-26 11:52:35,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1107038: learning rate 0.0005
[2019-03-26 11:52:35,207] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1107043: loss 0.0095
[2019-03-26 11:52:35,208] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1107043: learning rate 0.0005
[2019-03-26 11:52:35,421] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1107143: loss 0.0252
[2019-03-26 11:52:35,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1107143: learning rate 0.0005
[2019-03-26 11:52:35,845] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1107343: loss -123.0122
[2019-03-26 11:52:35,847] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1107343: learning rate 0.0005
[2019-03-26 11:52:40,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0200160e-31 1.9117643e-13 5.1018205e-36 1.2786669e-32 1.0000000e+00], sum to 1.0000
[2019-03-26 11:52:40,109] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8422
[2019-03-26 11:52:40,118] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.86666666666667, 89.0, 1.0, 2.0, 0.3196945407609688, 1.0, 2.0, 0.3196945407609688, 1.0, 2.0, 0.5339546382820486, 6.911200000000001, 6.9112, 170.5573041426782, 1340567.096446614, 1340567.096446614, 310539.9749064104], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1759200.0000, 
sim time next is 1759800.0000, 
raw observation next is [24.78333333333333, 89.0, 1.0, 2.0, 0.3220019653456292, 1.0, 2.0, 0.3220019653456292, 1.0, 2.0, 0.537256913307326, 6.9112, 6.9112, 170.5573041426782, 1350248.855779862, 1350248.855779862, 311460.9834862447], 
processed observation next is [1.0, 0.34782608695652173, 0.37361769352290675, 0.89, 1.0, 1.0, 0.18313489800678215, 1.0, 1.0, 0.18313489800678215, 1.0, 1.0, 0.4356791625699097, 0.0, 0.0, 0.8375144448122397, 0.3750691266055172, 0.3750691266055172, 0.4648671395317085], 
reward next is 0.5351, 
noisyNet noise sample is [array([-1.0301133], dtype=float32), -0.015574652]. 
=============================================
[2019-03-26 11:52:40,564] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1109497: loss 43.3064
[2019-03-26 11:52:40,568] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1109498: learning rate 0.0005
[2019-03-26 11:52:44,277] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1111227: loss 2.8842
[2019-03-26 11:52:44,280] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1111227: learning rate 0.0005
[2019-03-26 11:52:46,599] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1112311: loss -84.8708
[2019-03-26 11:52:46,603] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1112313: learning rate 0.0005
[2019-03-26 11:52:47,824] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1112875: loss 0.3261
[2019-03-26 11:52:47,827] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1112877: learning rate 0.0005
[2019-03-26 11:52:51,066] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1114373: loss -98.1820
[2019-03-26 11:52:51,068] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1114373: learning rate 0.0005
[2019-03-26 11:52:51,376] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1114521: loss -153.5462
[2019-03-26 11:52:51,381] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1114522: learning rate 0.0005
[2019-03-26 11:52:51,459] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1114556: loss 0.4367
[2019-03-26 11:52:51,465] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1114561: learning rate 0.0005
[2019-03-26 11:52:51,737] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1114686: loss 47.5726
[2019-03-26 11:52:51,740] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1114686: learning rate 0.0005
[2019-03-26 11:52:51,928] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1114772: loss -127.8963
[2019-03-26 11:52:51,929] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1114772: learning rate 0.0005
[2019-03-26 11:52:51,972] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1114792: loss 15.4236
[2019-03-26 11:52:51,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1114793: learning rate 0.0005
[2019-03-26 11:52:52,115] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1114859: loss -37.3450
[2019-03-26 11:52:52,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1114860: learning rate 0.0005
[2019-03-26 11:52:52,178] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1114885: loss 33.5656
[2019-03-26 11:52:52,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1114885: learning rate 0.0005
[2019-03-26 11:52:52,433] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1115006: loss 15.5493
[2019-03-26 11:52:52,437] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1115008: learning rate 0.0005
[2019-03-26 11:52:52,476] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1115028: loss 6.8924
[2019-03-26 11:52:52,478] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1115028: learning rate 0.0005
[2019-03-26 11:52:52,703] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1115130: loss -50.6537
[2019-03-26 11:52:52,705] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1115131: learning rate 0.0005
[2019-03-26 11:52:53,190] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1115357: loss 1.2204
[2019-03-26 11:52:53,193] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1115359: learning rate 0.0005
[2019-03-26 11:52:57,597] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1117444: loss 0.0117
[2019-03-26 11:52:57,600] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1117444: learning rate 0.0005
[2019-03-26 11:53:01,680] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1119466: loss 0.2283
[2019-03-26 11:53:01,680] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1119466: learning rate 0.0005
[2019-03-26 11:53:03,479] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1120355: loss 1.8242
[2019-03-26 11:53:03,482] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1120356: learning rate 0.0005
[2019-03-26 11:53:04,814] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1120968: loss 1.7538
[2019-03-26 11:53:04,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1120968: learning rate 0.0005
[2019-03-26 11:53:05,955] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3920985e-19 9.9999976e-01 1.3811644e-27 1.1559973e-20 2.9324971e-07], sum to 1.0000
[2019-03-26 11:53:05,963] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2848
[2019-03-26 11:53:05,972] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2268764.665344616 W.
[2019-03-26 11:53:05,975] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.83333333333334, 64.66666666666666, 1.0, 2.0, 0.5408163245140576, 1.0, 2.0, 0.5408163245140576, 1.0, 2.0, 0.9392191049894024, 6.9112, 6.9112, 170.5573041426782, 2268764.665344616, 2268764.665344616, 444518.8062391228], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2295600.0000, 
sim time next is 2296200.0000, 
raw observation next is [31.81666666666667, 64.83333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.036318685305105, 6.9112, 168.9122821230306, 2374677.395994588, 2285914.24877589, 475897.9876009003], 
processed observation next is [1.0, 0.5652173913043478, 0.7069510268562403, 0.6483333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.012511868530510473, 0.0, 0.8294366336014308, 0.6596326099984966, 0.634976180215525, 0.7102955038819407], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46031132], dtype=float32), -0.46972325]. 
=============================================
[2019-03-26 11:53:06,499] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6111611e-38 3.9875455e-28], sum to 1.0000
[2019-03-26 11:53:06,508] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1978
[2019-03-26 11:53:06,513] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5061442769512821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707260.8465983135, 707260.8465983128, 184564.6073105778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2642400.0000, 
sim time next is 2643000.0000, 
raw observation next is [27.0, 83.16666666666667, 1.0, 2.0, 0.5042421916762652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704602.0857174951, 704602.0857174951, 184263.4967873398], 
processed observation next is [0.0, 0.6086956521739131, 0.4786729857819906, 0.8316666666666667, 1.0, 1.0, 0.4027014357545363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19572280158819308, 0.19572280158819308, 0.27502014445871614], 
reward next is 0.7250, 
noisyNet noise sample is [array([0.26473868], dtype=float32), -0.03783286]. 
=============================================
[2019-03-26 11:53:06,531] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[58.69689 ]
 [58.625637]
 [58.626385]
 [58.619274]
 [58.6137  ]], R is [[58.83215332]
 [58.9683609 ]
 [59.10321808]
 [59.23674774]
 [59.36898422]].
[2019-03-26 11:53:07,984] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1122383: loss 2.1279
[2019-03-26 11:53:07,987] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1122385: learning rate 0.0005
[2019-03-26 11:53:08,150] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1122460: loss 2.2008
[2019-03-26 11:53:08,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1122460: learning rate 0.0005
[2019-03-26 11:53:08,310] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1122529: loss 1.6838
[2019-03-26 11:53:08,314] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1122532: learning rate 0.0005
[2019-03-26 11:53:08,617] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1122669: loss 1.7621
[2019-03-26 11:53:08,618] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1122670: learning rate 0.0005
[2019-03-26 11:53:08,663] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1122691: loss 1.3988
[2019-03-26 11:53:08,668] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1122693: learning rate 0.0005
[2019-03-26 11:53:08,910] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1122797: loss 1.2054
[2019-03-26 11:53:08,913] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1122798: learning rate 0.0005
[2019-03-26 11:53:08,984] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1122829: loss 1.4010
[2019-03-26 11:53:08,986] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1122829: learning rate 0.0005
[2019-03-26 11:53:09,130] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1122893: loss 1.6111
[2019-03-26 11:53:09,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1122894: learning rate 0.0005
[2019-03-26 11:53:09,196] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1122921: loss 1.4070
[2019-03-26 11:53:09,200] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1122924: learning rate 0.0005
[2019-03-26 11:53:09,313] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1122984: loss 1.4707
[2019-03-26 11:53:09,315] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1122984: learning rate 0.0005
[2019-03-26 11:53:09,567] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1123095: loss 1.7679
[2019-03-26 11:53:09,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1123096: learning rate 0.0005
[2019-03-26 11:53:10,891] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1123572: loss -3.0318
[2019-03-26 11:53:10,894] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1123575: learning rate 0.0005
[2019-03-26 11:53:11,273] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1127828e-32 1.0000000e+00 0.0000000e+00 1.5223435e-28 1.4812971e-14], sum to 1.0000
[2019-03-26 11:53:11,282] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9915
[2019-03-26 11:53:11,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2278722.237093071 W.
[2019-03-26 11:53:11,296] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333334, 64.66666666666666, 1.0, 2.0, 0.8147816950158707, 1.0, 1.0, 0.8147816950158707, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2278722.237093071, 2278722.237093071, 427132.7739086409], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2295600.0000, 
sim time next is 2296200.0000, 
raw observation next is [31.81666666666667, 64.83333333333334, 1.0, 2.0, 0.8281854223041066, 1.0, 2.0, 0.8281854223041066, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2316243.546594975, 2316243.546594975, 433827.0042020272], 
processed observation next is [1.0, 0.5652173913043478, 0.7069510268562403, 0.6483333333333334, 1.0, 1.0, 0.7929944847037429, 1.0, 1.0, 0.7929944847037429, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6434009851652709, 0.6434009851652709, 0.6475029913463093], 
reward next is 0.3525, 
noisyNet noise sample is [array([1.8737388], dtype=float32), 0.049642272]. 
=============================================
[2019-03-26 11:53:12,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2161986e-34 1.0000000e+00 0.0000000e+00 2.8114526e-33 5.9307660e-26], sum to 1.0000
[2019-03-26 11:53:12,040] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6325
[2019-03-26 11:53:12,046] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1782456.082649455 W.
[2019-03-26 11:53:12,051] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.88333333333333, 64.16666666666667, 1.0, 2.0, 0.6374759156132409, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.991259369556857, 6.9112, 168.9123976377345, 1782456.082649455, 1725659.398375555, 371979.5001781397], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2293800.0000, 
sim time next is 2294400.0000, 
raw observation next is [31.86666666666667, 64.33333333333334, 1.0, 2.0, 0.4730241002263427, 1.0, 1.0, 0.4730241002263427, 1.0, 2.0, 0.8214864306327995, 6.9112, 6.9112, 170.5573041426782, 1984115.289982181, 1984115.289982181, 396992.649277174], 
processed observation next is [1.0, 0.5652173913043478, 0.7093206951026858, 0.6433333333333334, 1.0, 1.0, 0.36508927738113583, 1.0, 0.5, 0.36508927738113583, 1.0, 1.0, 0.7823005251619505, 0.0, 0.0, 0.8375144448122397, 0.5511431361061614, 0.5511431361061614, 0.5925263422047373], 
reward next is 0.4075, 
noisyNet noise sample is [array([1.8113943], dtype=float32), 0.189518]. 
=============================================
[2019-03-26 11:53:12,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:53:12,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8435
[2019-03-26 11:53:12,576] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 73.66666666666667, 1.0, 2.0, 0.5704033314268188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797086.944448815, 797086.944448815, 195362.3231645313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2316000.0000, 
sim time next is 2316600.0000, 
raw observation next is [30.55, 74.5, 1.0, 2.0, 0.5714326050483157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798525.8016781245, 798525.8016781245, 195545.2215697849], 
processed observation next is [1.0, 0.8260869565217391, 0.6469194312796209, 0.745, 1.0, 1.0, 0.4836537410220671, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2218127226883679, 0.2218127226883679, 0.29185853965639535], 
reward next is 0.7081, 
noisyNet noise sample is [array([1.2681117], dtype=float32), -0.9253365]. 
=============================================
[2019-03-26 11:53:14,064] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 11:53:14,065] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:53:14,065] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:53:14,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:53:14,068] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:53:14,068] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:53:14,070] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:53:14,070] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:53:14,069] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:53:14,075] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:53:14,075] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:53:14,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run46
[2019-03-26 11:53:14,117] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run46
[2019-03-26 11:53:14,118] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run46
[2019-03-26 11:53:14,170] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run46
[2019-03-26 11:53:14,197] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run46
[2019-03-26 11:53:15,389] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.10845353], dtype=float32), 0.4893037]
[2019-03-26 11:53:15,391] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.4, 64.5, 1.0, 2.0, 0.479648042394997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692896.4615053739, 692896.4615053739, 183350.6115317455]
[2019-03-26 11:53:15,393] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:53:15,395] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1882519667090351
[2019-03-26 11:53:19,150] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.10845353], dtype=float32), 0.4893037]
[2019-03-26 11:53:19,150] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.7, 80.0, 1.0, 2.0, 0.260631451450417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 427656.0568585759, 427656.0568585759, 161977.1024096991]
[2019-03-26 11:53:19,153] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:53:19,156] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5190322647960525
[2019-03-26 11:53:39,978] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.10845353], dtype=float32), 0.4893037]
[2019-03-26 11:53:39,980] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.2, 86.0, 1.0, 2.0, 0.5446046458970905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778325.8766628538, 778325.8766628543, 193089.2129921469]
[2019-03-26 11:53:39,981] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:53:39,984] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.751011e-35], sampled 0.29017986328946865
[2019-03-26 11:53:41,484] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.10845353], dtype=float32), 0.4893037]
[2019-03-26 11:53:41,486] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.96666666666667, 93.16666666666666, 1.0, 2.0, 0.3460372273495639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535188.9095105969, 535188.9095105969, 169485.4041763998]
[2019-03-26 11:53:41,487] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:53:41,494] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.807438e-29], sampled 0.46903263069387324
[2019-03-26 11:53:54,836] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.10845353], dtype=float32), 0.4893037]
[2019-03-26 11:53:54,838] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.5, 82.0, 1.0, 2.0, 0.6713048987034905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 938150.0101007543, 938150.0101007543, 214772.1346332941]
[2019-03-26 11:53:54,841] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:53:54,844] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.3210847e-38 1.0000000e+00 0.0000000e+00 1.2946297e-37 2.8164091e-20], sampled 0.2427479785800516
[2019-03-26 11:54:42,914] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.10845353], dtype=float32), 0.4893037]
[2019-03-26 11:54:42,916] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.312789585, 58.08250462, 1.0, 2.0, 0.7540752044772123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1053879.08193148, 1053879.081931481, 232950.1970955284]
[2019-03-26 11:54:42,918] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:54:42,921] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.6628313e-36 1.0000000e+00 0.0000000e+00 2.1382856e-33 3.1640535e-18], sampled 0.058123833901561284
[2019-03-26 11:54:54,473] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.10845353], dtype=float32), 0.4893037]
[2019-03-26 11:54:54,473] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.0, 43.33333333333334, 1.0, 2.0, 0.5553110839993439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775989.1930920882, 775989.1930920882, 192716.4159434028]
[2019-03-26 11:54:54,475] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:54:54,478] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7719924e-36], sampled 0.27920712852515106
[2019-03-26 11:55:00,277] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.10845353], dtype=float32), 0.4893037]
[2019-03-26 11:55:00,279] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.7, 64.0, 1.0, 2.0, 0.8185401984084328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1144022.486055392, 1144022.486055392, 248516.447811628]
[2019-03-26 11:55:00,283] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:55:00,285] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6338283e-35 9.6822292e-34], sampled 0.123066880813819
[2019-03-26 11:55:09,489] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8325.8768 2928172748.2278 1205.0000
[2019-03-26 11:55:09,558] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8573.9202 2841077446.7794 957.0000
[2019-03-26 11:55:09,575] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8098.7573 3005884072.3658 1532.0000
[2019-03-26 11:55:09,590] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8719.5919 2779160703.5596 803.0000
[2019-03-26 11:55:09,737] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7988.6602 3161916854.3076 1615.0000
[2019-03-26 11:55:10,754] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1125000, evaluation results [1125000.0, 7988.660206380373, 3161916854.30756, 1615.0, 8325.876835015888, 2928172748.227806, 1205.0, 8719.591939534163, 2779160703.559643, 803.0, 8098.757298552503, 3005884072.3657722, 1532.0, 8573.920165723304, 2841077446.779435, 957.0]
[2019-03-26 11:55:11,123] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1125175: loss 3.1112
[2019-03-26 11:55:11,127] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1125175: learning rate 0.0005
[2019-03-26 11:55:11,685] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.4468645e-26 2.2856572e-04 3.2354800e-34 1.2192249e-31 9.9977142e-01], sum to 1.0000
[2019-03-26 11:55:11,693] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2295
[2019-03-26 11:55:11,701] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.6, 65.33333333333334, 1.0, 2.0, 0.4911123977567756, 1.0, 2.0, 0.4911123977567756, 1.0, 2.0, 0.8528998215517596, 6.911200000000001, 6.9112, 170.5573041426782, 2060060.204026958, 2060060.204026958, 409054.0754240794], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2370000.0000, 
sim time next is 2370600.0000, 
raw observation next is [31.75, 65.0, 1.0, 2.0, 0.495510378330345, 1.0, 2.0, 0.495510378330345, 1.0, 2.0, 0.8605376593736489, 6.9112, 6.9112, 170.5573041426782, 2078526.230157337, 2078526.230157337, 412054.9308044252], 
processed observation next is [1.0, 0.43478260869565216, 0.7037914691943128, 0.65, 1.0, 1.0, 0.39218117871125907, 1.0, 1.0, 0.39218117871125907, 1.0, 1.0, 0.8299239748459133, 0.0, 0.0, 0.8375144448122397, 0.5773683972659269, 0.5773683972659269, 0.6150073594095898], 
reward next is 0.3850, 
noisyNet noise sample is [array([0.49844486], dtype=float32), -0.63507485]. 
=============================================
[2019-03-26 11:55:14,385] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:55:14,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7746
[2019-03-26 11:55:14,401] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.73333333333333, 74.66666666666667, 1.0, 2.0, 0.5842371234322484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 816425.8441769247, 816425.8441769254, 197846.3589911139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2406000.0000, 
sim time next is 2406600.0000, 
raw observation next is [30.65, 75.0, 1.0, 2.0, 0.5860945643731833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 819022.4738654997, 819022.473865499, 198184.0537058449], 
processed observation next is [1.0, 0.8695652173913043, 0.6516587677725119, 0.75, 1.0, 1.0, 0.5013187522568473, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2275062427404166, 0.2275062427404164, 0.2957970950833506], 
reward next is 0.7042, 
noisyNet noise sample is [array([-0.8821018], dtype=float32), 1.2657781]. 
=============================================
[2019-03-26 11:55:15,288] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1127104: loss 0.1421
[2019-03-26 11:55:15,293] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1127104: learning rate 0.0005
[2019-03-26 11:55:18,157] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.6188666e-24 1.6942339e-03 1.0192634e-28 3.4685259e-24 9.9830580e-01], sum to 1.0000
[2019-03-26 11:55:18,164] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5613
[2019-03-26 11:55:18,169] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.11666666666667, 86.83333333333333, 1.0, 2.0, 0.6768852117062192, 1.0, 1.0, 0.6768852117062192, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1892731.009278604, 1892731.009278604, 364383.5496083743], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2451000.0000, 
sim time next is 2451600.0000, 
raw observation next is [27.0, 87.0, 1.0, 2.0, 0.4471573094377753, 1.0, 2.0, 0.4471573094377753, 1.0, 1.0, 0.7676712123611036, 6.9112, 6.9112, 170.5573041426782, 1875521.210419419, 1875521.210419419, 379106.8245497345], 
processed observation next is [1.0, 0.391304347826087, 0.4786729857819906, 0.87, 1.0, 1.0, 0.3339244692021389, 1.0, 1.0, 0.3339244692021389, 1.0, 0.5, 0.7166722101964678, 0.0, 0.0, 0.8375144448122397, 0.5209781140053942, 0.5209781140053942, 0.5658310814175141], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.480592], dtype=float32), 0.5192998]. 
=============================================
[2019-03-26 11:55:18,398] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1128488: loss 13.4025
[2019-03-26 11:55:18,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1128490: learning rate 0.0005
[2019-03-26 11:55:19,468] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1128989: loss -2.0398
[2019-03-26 11:55:19,473] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1128992: learning rate 0.0005
[2019-03-26 11:55:22,631] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1130451: loss -248.9293
[2019-03-26 11:55:22,634] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1130451: learning rate 0.0005
[2019-03-26 11:55:23,042] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1130640: loss -201.1805
[2019-03-26 11:55:23,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1130640: learning rate 0.0005
[2019-03-26 11:55:23,132] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1130676: loss -78.3352
[2019-03-26 11:55:23,133] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1130677: learning rate 0.0005
[2019-03-26 11:55:23,206] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1130713: loss -252.4581
[2019-03-26 11:55:23,208] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1130713: learning rate 0.0005
[2019-03-26 11:55:23,468] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1130831: loss -90.2631
[2019-03-26 11:55:23,470] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1130831: learning rate 0.0005
[2019-03-26 11:55:23,751] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1130962: loss -52.4347
[2019-03-26 11:55:23,752] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1130962: learning rate 0.0005
[2019-03-26 11:55:23,776] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1130972: loss -190.4423
[2019-03-26 11:55:23,777] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1130973: learning rate 0.0005
[2019-03-26 11:55:23,793] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1130980: loss -192.2395
[2019-03-26 11:55:23,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1130980: learning rate 0.0005
[2019-03-26 11:55:23,829] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1130995: loss -15.6614
[2019-03-26 11:55:23,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1130996: learning rate 0.0005
[2019-03-26 11:55:23,834] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1130996: loss -168.3747
[2019-03-26 11:55:23,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1130996: learning rate 0.0005
[2019-03-26 11:55:24,264] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1131190: loss 0.2268
[2019-03-26 11:55:24,266] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1131190: learning rate 0.0005
[2019-03-26 11:55:24,370] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1131252: loss -236.9568
[2019-03-26 11:55:24,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1131252: learning rate 0.0005
[2019-03-26 11:55:26,417] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8840218e-36], sum to 1.0000
[2019-03-26 11:55:26,428] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2214
[2019-03-26 11:55:26,433] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.95, 89.16666666666667, 1.0, 2.0, 0.7309011927871052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1114585.836322638, 1114585.836322639, 239648.4685344286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2893800.0000, 
sim time next is 2894400.0000, 
raw observation next is [23.0, 89.0, 1.0, 2.0, 0.7212752840298917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1098952.566744534, 1098952.566744534, 237177.9138975594], 
processed observation next is [1.0, 0.5217391304347826, 0.28909952606635075, 0.89, 1.0, 1.0, 0.6641870891926406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3052646018734817, 0.3052646018734817, 0.35399688641426774], 
reward next is 0.6460, 
noisyNet noise sample is [array([-1.4551102], dtype=float32), -1.6074486]. 
=============================================
[2019-03-26 11:55:28,184] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1133023: loss 7.3367
[2019-03-26 11:55:28,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1133024: learning rate 0.0005
[2019-03-26 11:55:32,134] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1134862: loss 5.1704
[2019-03-26 11:55:32,136] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1134862: learning rate 0.0005
[2019-03-26 11:55:35,397] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1136377: loss 0.0426
[2019-03-26 11:55:35,398] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1136377: learning rate 0.0005
[2019-03-26 11:55:36,432] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1136854: loss 0.0114
[2019-03-26 11:55:36,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1136854: learning rate 0.0005
[2019-03-26 11:55:39,936] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1138401: loss 0.1093
[2019-03-26 11:55:39,938] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1138401: learning rate 0.0005
[2019-03-26 11:55:40,318] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1138579: loss 0.6528
[2019-03-26 11:55:40,325] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1138579: learning rate 0.0005
[2019-03-26 11:55:40,350] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1138589: loss 0.4595
[2019-03-26 11:55:40,353] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1138590: learning rate 0.0005
[2019-03-26 11:55:40,490] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1138656: loss 0.4477
[2019-03-26 11:55:40,492] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1138657: learning rate 0.0005
[2019-03-26 11:55:40,974] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1138876: loss 0.1019
[2019-03-26 11:55:40,981] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1138878: learning rate 0.0005
[2019-03-26 11:55:41,067] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1138923: loss 0.1648
[2019-03-26 11:55:41,072] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1138923: learning rate 0.0005
[2019-03-26 11:55:41,114] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1138940: loss 0.1193
[2019-03-26 11:55:41,119] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1138943: learning rate 0.0005
[2019-03-26 11:55:41,157] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1138963: loss 0.1023
[2019-03-26 11:55:41,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1138963: learning rate 0.0005
[2019-03-26 11:55:41,202] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1138986: loss 0.0889
[2019-03-26 11:55:41,210] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1138986: learning rate 0.0005
[2019-03-26 11:55:41,221] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1138992: loss 0.0433
[2019-03-26 11:55:41,223] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1138993: learning rate 0.0005
[2019-03-26 11:55:41,671] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1139196: loss 6.2027
[2019-03-26 11:55:41,675] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1139198: learning rate 0.0005
[2019-03-26 11:55:41,902] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1139308: loss 0.0596
[2019-03-26 11:55:41,905] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1139308: learning rate 0.0005
[2019-03-26 11:55:46,649] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1141507: loss 55.3616
[2019-03-26 11:55:46,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1141507: learning rate 0.0005
[2019-03-26 11:55:47,921] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3145507e-38 1.0000000e+00 0.0000000e+00 9.8243748e-37 3.3863021e-29], sum to 1.0000
[2019-03-26 11:55:47,932] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4631
[2019-03-26 11:55:47,939] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 96.0, 1.0, 2.0, 0.7583974758994021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1123210.192958733, 1123210.192958733, 242446.031017916], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3072000.0000, 
sim time next is 3072600.0000, 
raw observation next is [23.0, 97.0, 1.0, 2.0, 0.8339151105066627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1229316.07873723, 1229316.078737229, 261326.0559229381], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.97, 1.0, 1.0, 0.7998977235020032, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34147668853811947, 0.3414766885381192, 0.39003888943722104], 
reward next is 0.6100, 
noisyNet noise sample is [array([1.0804588], dtype=float32), 0.1229797]. 
=============================================
[2019-03-26 11:55:49,789] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1142965: loss 9.5432
[2019-03-26 11:55:49,793] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1142968: learning rate 0.0005
[2019-03-26 11:55:52,463] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1144296: loss 8.4325
[2019-03-26 11:55:52,464] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1144296: learning rate 0.0005
[2019-03-26 11:55:53,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.6687032e-30 1.0000000e+00 7.7476333e-35 2.5314095e-29 6.3499935e-22], sum to 1.0000
[2019-03-26 11:55:53,277] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1144782: loss 9.1538
[2019-03-26 11:55:53,279] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0341
[2019-03-26 11:55:53,281] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1144783: learning rate 0.0005
[2019-03-26 11:55:53,289] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 96.0, 1.0, 2.0, 0.8303671613011229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1197061.096569232, 1197061.096569233, 256670.0873613879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3080400.0000, 
sim time next is 3081000.0000, 
raw observation next is [23.83333333333333, 95.0, 1.0, 2.0, 0.8422351893929869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1212111.288152422, 1212111.288152422, 259527.3534516696], 
processed observation next is [1.0, 0.6521739130434783, 0.32859399684044216, 0.95, 1.0, 1.0, 0.8099219149313095, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33669758004233946, 0.33669758004233946, 0.38735425888308894], 
reward next is 0.6126, 
noisyNet noise sample is [array([0.98874915], dtype=float32), 1.5748734]. 
=============================================
[2019-03-26 11:55:53,309] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[54.01791 ]
 [54.242718]
 [54.358612]
 [54.326763]
 [54.036644]], R is [[53.89281464]
 [53.97079468]
 [54.05434418]
 [54.14456177]
 [54.2347374 ]].
[2019-03-26 11:55:53,574] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6611917e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 11:55:53,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3328
[2019-03-26 11:55:53,587] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5571405342094552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778546.5954261825, 778546.5954261832, 193033.5263727024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3524400.0000, 
sim time next is 3525000.0000, 
raw observation next is [29.83333333333333, 75.66666666666667, 1.0, 2.0, 0.5581109252399513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779903.114912132, 779903.114912132, 193201.9338440468], 
processed observation next is [1.0, 0.8260869565217391, 0.6129541864139019, 0.7566666666666667, 1.0, 1.0, 0.46760352438548347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2166397541422589, 0.2166397541422589, 0.2883610952896221], 
reward next is 0.7116, 
noisyNet noise sample is [array([0.63022393], dtype=float32), -0.50101155]. 
=============================================
[2019-03-26 11:55:53,607] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.81928 ]
 [67.4556  ]
 [67.325485]
 [66.55231 ]
 [66.16408 ]], R is [[67.55450439]
 [67.59085083]
 [67.6269455 ]
 [67.6625824 ]
 [67.69745636]].
[2019-03-26 11:55:53,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.1267730e-32 1.0000000e+00 0.0000000e+00 3.9732969e-31 2.3680718e-22], sum to 1.0000
[2019-03-26 11:55:53,819] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1558
[2019-03-26 11:55:53,825] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4960027251284814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719722.7308739586, 719722.730873958, 186343.3714263823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3085800.0000, 
sim time next is 3086400.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4234433790790483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614557.5426876508, 614557.5426876508, 175415.638473793], 
processed observation next is [1.0, 0.7391304347826086, 0.28909952606635075, 1.0, 1.0, 1.0, 0.30535346876993763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17071042852434742, 0.17071042852434742, 0.26181438578178057], 
reward next is 0.7382, 
noisyNet noise sample is [array([-0.49764106], dtype=float32), 0.50498015]. 
=============================================
[2019-03-26 11:55:55,146] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8664072e-36 1.0000000e+00 0.0000000e+00 2.5824127e-35 6.8669091e-26], sum to 1.0000
[2019-03-26 11:55:55,155] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9533
[2019-03-26 11:55:55,161] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 87.33333333333334, 1.0, 2.0, 0.8712014140019531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217665.979690903, 1217665.979690903, 262141.6740370762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3396000.0000, 
sim time next is 3396600.0000, 
raw observation next is [28.5, 86.5, 1.0, 2.0, 0.8891916667255768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1242825.419927272, 1242825.419927273, 266984.9592434841], 
processed observation next is [1.0, 0.30434782608695654, 0.5497630331753555, 0.865, 1.0, 1.0, 0.866495984006719, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34522928331313113, 0.34522928331313135, 0.3984850137962449], 
reward next is 0.6015, 
noisyNet noise sample is [array([2.3685846], dtype=float32), -1.4567825]. 
=============================================
[2019-03-26 11:55:56,634] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1146308: loss 8.4540
[2019-03-26 11:55:56,637] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1146308: learning rate 0.0005
[2019-03-26 11:55:57,163] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1146544: loss 9.1797
[2019-03-26 11:55:57,166] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1146546: learning rate 0.0005
[2019-03-26 11:55:57,219] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1146566: loss 8.9182
[2019-03-26 11:55:57,222] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1146566: learning rate 0.0005
[2019-03-26 11:55:57,393] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1146646: loss 10.3538
[2019-03-26 11:55:57,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1146646: learning rate 0.0005
[2019-03-26 11:55:57,686] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1146774: loss 10.2793
[2019-03-26 11:55:57,690] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1146776: learning rate 0.0005
[2019-03-26 11:55:57,892] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1146869: loss 9.5883
[2019-03-26 11:55:57,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1146870: learning rate 0.0005
[2019-03-26 11:55:57,897] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1146870: loss 10.1905
[2019-03-26 11:55:57,901] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1146872: learning rate 0.0005
[2019-03-26 11:55:57,955] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1146897: loss 11.1276
[2019-03-26 11:55:57,963] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1146897: learning rate 0.0005
[2019-03-26 11:55:57,981] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1146907: loss 11.1225
[2019-03-26 11:55:57,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1146907: learning rate 0.0005
[2019-03-26 11:55:58,048] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1146940: loss 10.1798
[2019-03-26 11:55:58,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1146940: learning rate 0.0005
[2019-03-26 11:55:58,865] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1147303: loss 5.8211
[2019-03-26 11:55:58,868] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1147305: learning rate 0.0005
[2019-03-26 11:55:58,893] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1147318: loss 10.3561
[2019-03-26 11:55:58,896] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1147318: learning rate 0.0005
[2019-03-26 11:56:00,787] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:56:00,798] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4170
[2019-03-26 11:56:00,806] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.4784003462120729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668480.6591032878, 668480.6591032878, 180281.2126089916], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3226200.0000, 
sim time next is 3226800.0000, 
raw observation next is [27.33333333333334, 77.33333333333334, 1.0, 2.0, 0.4801970682299169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670992.0562059153, 670992.0562059153, 180551.6786721637], 
processed observation next is [0.0, 0.34782608695652173, 0.4944707740916275, 0.7733333333333334, 1.0, 1.0, 0.373731407505924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1863866822794209, 0.1863866822794209, 0.2694801174211398], 
reward next is 0.7305, 
noisyNet noise sample is [array([-1.0041424], dtype=float32), 1.2932949]. 
=============================================
[2019-03-26 11:56:02,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:56:02,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8359
[2019-03-26 11:56:02,038] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 69.5, 1.0, 2.0, 0.5342622896260973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746565.3666367391, 746565.3666367391, 189139.0601715926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3237000.0000, 
sim time next is 3237600.0000, 
raw observation next is [30.66666666666667, 69.0, 1.0, 2.0, 0.5398229483269797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754338.4574006329, 754338.4574006336, 190071.577557585], 
processed observation next is [0.0, 0.4782608695652174, 0.6524486571879939, 0.69, 1.0, 1.0, 0.44556981726142136, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2095384603890647, 0.2095384603890649, 0.2836889217277388], 
reward next is 0.7163, 
noisyNet noise sample is [array([0.11583212], dtype=float32), 0.18766274]. 
=============================================
[2019-03-26 11:56:04,230] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1149714: loss 0.0239
[2019-03-26 11:56:04,233] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1149714: learning rate 0.0005
[2019-03-26 11:56:04,846] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 11:56:04,847] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:56:04,848] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:56:04,848] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:56:04,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:56:04,851] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:56:04,853] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:56:04,853] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:56:04,852] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:56:04,854] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:56:04,855] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:56:04,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run47
[2019-03-26 11:56:04,901] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run47
[2019-03-26 11:56:04,902] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run47
[2019-03-26 11:56:04,944] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run47
[2019-03-26 11:56:04,964] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run47
[2019-03-26 11:56:06,497] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:56:06,498] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.99095972, 75.25590267, 1.0, 2.0, 0.7249798916473273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1013196.708031669, 1013196.70803167, 226326.9444070113]
[2019-03-26 11:56:06,501] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:56:06,504] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7884078266227129
[2019-03-26 11:56:15,213] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:56:15,215] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.3, 87.00000000000001, 1.0, 2.0, 0.2950526358225667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472818.2617780674, 472818.2617780674, 165148.3231919254]
[2019-03-26 11:56:15,216] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:56:15,219] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6944767280855586
[2019-03-26 11:56:31,846] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:56:31,847] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.86666666666667, 84.33333333333334, 1.0, 2.0, 0.4410619605345426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644095.8986724725, 644095.8986724725, 178416.6533432927]
[2019-03-26 11:56:31,850] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:56:31,853] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07300911511781327
[2019-03-26 11:56:32,200] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:56:32,201] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.36666666666667, 82.66666666666666, 1.0, 2.0, 0.4472333264755042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 646415.005604352, 646415.0056043526, 178491.64970964]
[2019-03-26 11:56:32,203] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:56:32,205] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4875403959490817
[2019-03-26 11:56:49,088] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:56:49,089] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.45, 72.16666666666667, 1.0, 2.0, 0.5037910212756919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703971.4345210909, 703971.4345210909, 184191.6278012586]
[2019-03-26 11:56:49,090] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:56:49,092] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7346550433441024
[2019-03-26 11:56:53,329] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:56:53,330] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.45, 54.5, 1.0, 2.0, 0.5791407587602232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809301.3585677816, 809301.3585677816, 196923.9587070953]
[2019-03-26 11:56:53,332] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:56:53,334] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.009579965276479752
[2019-03-26 11:56:55,553] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:56:55,555] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.5165752683673743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721841.5477557711, 721841.5477557718, 186233.947470061]
[2019-03-26 11:56:55,556] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:56:55,559] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6964419421622414
[2019-03-26 11:57:04,118] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:57:04,118] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 6.958834095829585, 6.9112, 170.5573041426782, 2943491.748816529, 2909369.508342663, 553391.2022034315]
[2019-03-26 11:57:04,121] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 11:57:04,123] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.5353985e-23 9.9043429e-01 5.9677021e-32 6.1532770e-20 9.5656672e-03], sampled 0.9783073793069618
[2019-03-26 11:57:04,125] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2943491.748816529 W.
[2019-03-26 11:57:19,727] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:57:19,731] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.01666666666667, 62.5, 1.0, 2.0, 0.6243352235624284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872482.8284499068, 872482.8284499068, 205367.2623593592]
[2019-03-26 11:57:19,732] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:57:19,735] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7343195937798122
[2019-03-26 11:57:25,439] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:57:25,439] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.76666666666667, 48.83333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.310901352730586, 6.9112, 168.9107123586619, 1737507.265978892, 1453949.137053028, 311355.4572924018]
[2019-03-26 11:57:25,440] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 11:57:25,444] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.3206496e-31 1.0000000e+00 5.3016958e-36 1.7782196e-27 3.8239220e-18], sampled 0.9873222077353553
[2019-03-26 11:57:25,447] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1737507.265978892 W.
[2019-03-26 11:57:28,783] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:57:28,785] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.21424671666666, 73.92738865, 1.0, 2.0, 0.542348208394916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757868.4672796696, 757868.4672796696, 190496.3701209263]
[2019-03-26 11:57:28,785] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:57:28,789] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5193519009733175
[2019-03-26 11:57:57,074] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08157249], dtype=float32), 0.50136685]
[2019-03-26 11:57:57,075] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.52349737, 97.91563253166667, 1.0, 2.0, 0.4783306666339957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668383.2634717526, 668383.2634717526, 180271.0396041582]
[2019-03-26 11:57:57,075] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:57:57,078] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4864501641290776
[2019-03-26 11:58:00,711] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8250.4198 2927825097.6009 1351.0000
[2019-03-26 11:58:00,791] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.0480 3164200091.5458 1831.0000
[2019-03-26 11:58:00,880] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.1679 2842612360.1240 1135.0000
[2019-03-26 11:58:01,037] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.8791 3007761158.7254 1761.0000
[2019-03-26 11:58:01,080] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7164 2779626001.1460 932.0000
[2019-03-26 11:58:02,096] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1150000, evaluation results [1150000.0, 7879.048011824792, 3164200091.545849, 1831.0, 8250.419759521676, 2927825097.6008644, 1351.0, 8660.71640937274, 2779626001.1460466, 932.0, 8000.87912862758, 3007761158.725372, 1761.0, 8495.167884441827, 2842612360.1239734, 1135.0]
[2019-03-26 11:58:04,851] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1151283: loss -10.6030
[2019-03-26 11:58:04,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1151283: learning rate 0.0005
[2019-03-26 11:58:05,642] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:58:05,652] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9082
[2019-03-26 11:58:05,657] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.5975090512203506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834979.618799215, 834979.618799215, 200283.1510616608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3336600.0000, 
sim time next is 3337200.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.595696175992508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832445.2520829113, 832445.2520829113, 199947.2292352715], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5128869590271181, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23123479224525315, 0.23123479224525315, 0.2984287003511515], 
reward next is 0.7016, 
noisyNet noise sample is [array([0.5892129], dtype=float32), -0.63926125]. 
=============================================
[2019-03-26 11:58:07,129] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1152335: loss 7.1545
[2019-03-26 11:58:07,133] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1152338: learning rate 0.0005
[2019-03-26 11:58:08,176] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1152798: loss 7.3310
[2019-03-26 11:58:08,178] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1152798: learning rate 0.0005
[2019-03-26 11:58:11,249] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1154228: loss 6.8850
[2019-03-26 11:58:11,252] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1154230: learning rate 0.0005
[2019-03-26 11:58:11,822] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1154495: loss 8.5418
[2019-03-26 11:58:11,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1154495: learning rate 0.0005
[2019-03-26 11:58:11,867] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1154514: loss 7.5569
[2019-03-26 11:58:11,869] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1154514: learning rate 0.0005
[2019-03-26 11:58:12,038] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1154592: loss 10.1305
[2019-03-26 11:58:12,040] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1154593: learning rate 0.0005
[2019-03-26 11:58:12,328] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.860955e-26], sum to 1.0000
[2019-03-26 11:58:12,334] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3615
[2019-03-26 11:58:12,340] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 85.66666666666667, 1.0, 2.0, 0.8807175721614448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1230974.290583841, 1230974.290583841, 264685.8386431032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3464400.0000, 
sim time next is 3465000.0000, 
raw observation next is [26.5, 84.0, 1.0, 2.0, 0.7802758105508892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090515.280487867, 1090515.280487866, 239123.9517923286], 
processed observation next is [1.0, 0.08695652173913043, 0.4549763033175356, 0.84, 1.0, 1.0, 0.7352720609046858, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30292091124662973, 0.3029209112466294, 0.35690142058556507], 
reward next is 0.6431, 
noisyNet noise sample is [array([1.1521455], dtype=float32), 0.06737209]. 
=============================================
[2019-03-26 11:58:12,357] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0147996e-36 1.0000000e+00 0.0000000e+00 2.3156246e-38 3.9516200e-24], sum to 1.0000
[2019-03-26 11:58:12,364] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[67.42132 ]
 [68.17237 ]
 [70.787346]
 [70.578285]
 [70.543205]], R is [[67.27600098]
 [67.20819092]
 [67.11471558]
 [67.16914368]
 [67.22286224]].
[2019-03-26 11:58:12,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8984
[2019-03-26 11:58:12,373] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 86.5, 1.0, 2.0, 0.5054346614254485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706268.9355070761, 706268.9355070768, 184451.9452195093], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3461400.0000, 
sim time next is 3462000.0000, 
raw observation next is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5035865515859838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703685.6245507251, 703685.6245507258, 184159.9167561295], 
processed observation next is [1.0, 0.043478260869565216, 0.44707740916271754, 0.8733333333333333, 1.0, 1.0, 0.40191150793492014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19546822904186809, 0.19546822904186828, 0.2748655473972082], 
reward next is 0.7251, 
noisyNet noise sample is [array([0.9968892], dtype=float32), -0.069360115]. 
=============================================
[2019-03-26 11:58:12,391] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[69.75774]
 [69.77083]
 [69.75527]
 [69.78687]
 [69.71846]], R is [[69.78639221]
 [69.81323242]
 [69.83946228]
 [69.86529541]
 [69.89076233]].
[2019-03-26 11:58:12,535] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1154818: loss 10.0876
[2019-03-26 11:58:12,536] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1154819: learning rate 0.0005
[2019-03-26 11:58:12,538] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1154820: loss 8.6891
[2019-03-26 11:58:12,541] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1154821: learning rate 0.0005
[2019-03-26 11:58:12,574] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1154839: loss 9.7600
[2019-03-26 11:58:12,575] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1154839: learning rate 0.0005
[2019-03-26 11:58:12,578] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1154841: loss 9.1028
[2019-03-26 11:58:12,580] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1154842: learning rate 0.0005
[2019-03-26 11:58:12,598] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1154849: loss 8.9571
[2019-03-26 11:58:12,599] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1154850: learning rate 0.0005
[2019-03-26 11:58:12,621] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1154855: loss 8.9759
[2019-03-26 11:58:12,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1154855: learning rate 0.0005
[2019-03-26 11:58:13,560] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1155292: loss 8.9264
[2019-03-26 11:58:13,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1155294: learning rate 0.0005
[2019-03-26 11:58:14,063] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1155516: loss -188.3154
[2019-03-26 11:58:14,066] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1155517: learning rate 0.0005
[2019-03-26 11:58:17,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:58:17,259] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5242
[2019-03-26 11:58:17,264] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5183974007267487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724388.5905567933, 724388.590556794, 186528.7661862407], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3537000.0000, 
sim time next is 3537600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5177938402131588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723544.9110700776, 723544.911070077, 186431.0019518709], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.419028723148384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.200984697519466, 0.20098469751946582, 0.27825522679383713], 
reward next is 0.7217, 
noisyNet noise sample is [array([1.520024], dtype=float32), -2.1684868]. 
=============================================
[2019-03-26 11:58:19,287] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1157926: loss 2.6091
[2019-03-26 11:58:19,289] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1157926: learning rate 0.0005
[2019-03-26 11:58:21,936] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1159160: loss 0.0278
[2019-03-26 11:58:21,938] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1159161: learning rate 0.0005
[2019-03-26 11:58:23,638] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:58:23,647] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0587
[2019-03-26 11:58:23,653] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 72.0, 1.0, 2.0, 0.8874003259451771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1240320.195969404, 1240320.195969404, 266492.5916803419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3655800.0000, 
sim time next is 3656400.0000, 
raw observation next is [28.66666666666666, 71.33333333333333, 1.0, 2.0, 0.7719786979650235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1078913.32569186, 1078913.32569186, 237146.3100996096], 
processed observation next is [1.0, 0.30434782608695654, 0.5576619273301735, 0.7133333333333333, 1.0, 1.0, 0.7252755397168957, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29969814602551664, 0.29969814602551664, 0.3539497165665815], 
reward next is 0.6461, 
noisyNet noise sample is [array([0.4301619], dtype=float32), -0.99103945]. 
=============================================
[2019-03-26 11:58:24,630] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1160379: loss -18.4236
[2019-03-26 11:58:24,632] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1160379: learning rate 0.0005
[2019-03-26 11:58:25,343] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1160706: loss 1.9621
[2019-03-26 11:58:25,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1160707: learning rate 0.0005
[2019-03-26 11:58:27,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2325922e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0235193e-22], sum to 1.0000
[2019-03-26 11:58:27,763] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6725
[2019-03-26 11:58:27,770] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.9481919089031217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1325341.678273669, 1325341.678273669, 283541.3806938237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4158000.0000, 
sim time next is 4158600.0000, 
raw observation next is [28.83333333333334, 84.83333333333333, 1.0, 2.0, 0.9794442522346938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1369053.020016599, 1369053.020016599, 292728.666948068], 
processed observation next is [1.0, 0.13043478260869565, 0.5655608214849924, 0.8483333333333333, 1.0, 1.0, 0.9752340388369805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38029250556016636, 0.38029250556016636, 0.4369084581314448], 
reward next is 0.5631, 
noisyNet noise sample is [array([-1.0515661], dtype=float32), -1.0190781]. 
=============================================
[2019-03-26 11:58:28,866] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1162335: loss -30.8931
[2019-03-26 11:58:28,869] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1162335: learning rate 0.0005
[2019-03-26 11:58:29,417] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1162593: loss -88.0452
[2019-03-26 11:58:29,418] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1162594: learning rate 0.0005
[2019-03-26 11:58:29,459] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1162613: loss -103.6935
[2019-03-26 11:58:29,461] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1162613: learning rate 0.0005
[2019-03-26 11:58:29,494] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1162629: loss -106.6810
[2019-03-26 11:58:29,498] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1162632: learning rate 0.0005
[2019-03-26 11:58:29,857] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1162799: loss -265.6694
[2019-03-26 11:58:29,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1162799: learning rate 0.0005
[2019-03-26 11:58:29,884] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1162809: loss -208.1292
[2019-03-26 11:58:29,888] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1162810: learning rate 0.0005
[2019-03-26 11:58:30,060] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1162889: loss -63.2623
[2019-03-26 11:58:30,063] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1162889: learning rate 0.0005
[2019-03-26 11:58:30,086] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1162901: loss -27.1344
[2019-03-26 11:58:30,088] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1162902: learning rate 0.0005
[2019-03-26 11:58:30,100] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1162908: loss -245.2588
[2019-03-26 11:58:30,101] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1162908: learning rate 0.0005
[2019-03-26 11:58:30,140] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1162927: loss -301.0650
[2019-03-26 11:58:30,142] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1162927: learning rate 0.0005
[2019-03-26 11:58:30,948] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1163303: loss -3.1715
[2019-03-26 11:58:30,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1163303: learning rate 0.0005
[2019-03-26 11:58:31,057] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1163350: loss 0.0385
[2019-03-26 11:58:31,061] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1163351: learning rate 0.0005
[2019-03-26 11:58:35,702] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:58:35,712] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4889
[2019-03-26 11:58:35,719] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 58.0, 1.0, 2.0, 0.6138748972006653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857859.0486237034, 857859.0486237034, 203359.9688358732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3853800.0000, 
sim time next is 3854400.0000, 
raw observation next is [35.0, 57.33333333333333, 1.0, 2.0, 0.6093395870336394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851518.6419786054, 851518.6419786054, 202499.3785488599], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5733333333333333, 1.0, 1.0, 0.5293248036549872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23653295610516817, 0.23653295610516817, 0.3022378784311342], 
reward next is 0.6978, 
noisyNet noise sample is [array([-0.84370655], dtype=float32), -1.3818564]. 
=============================================
[2019-03-26 11:58:36,245] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1165746: loss 0.4426
[2019-03-26 11:58:36,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1165747: learning rate 0.0005
[2019-03-26 11:58:39,697] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1167272: loss 2.3568
[2019-03-26 11:58:39,699] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1167272: learning rate 0.0005
[2019-03-26 11:58:40,420] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:58:40,431] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1523
[2019-03-26 11:58:40,442] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5873009691091451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820708.9838378612, 820708.9838378606, 198403.9046008279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3915600.0000, 
sim time next is 3916200.0000, 
raw observation next is [29.83333333333333, 79.83333333333334, 1.0, 2.0, 0.5879990792192651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821684.9176211624, 821684.9176211624, 198531.4449304526], 
processed observation next is [0.0, 0.30434782608695654, 0.6129541864139019, 0.7983333333333335, 1.0, 1.0, 0.5036133484569458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22824581045032288, 0.22824581045032288, 0.2963155894484367], 
reward next is 0.7037, 
noisyNet noise sample is [array([0.13019268], dtype=float32), -0.16016865]. 
=============================================
[2019-03-26 11:58:41,986] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1168325: loss 0.0952
[2019-03-26 11:58:41,988] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1168327: learning rate 0.0005
[2019-03-26 11:58:42,429] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:58:42,438] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9612
[2019-03-26 11:58:42,443] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 76.33333333333333, 1.0, 2.0, 0.6164178685432905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 861414.1632499291, 861414.1632499298, 203844.9507847506], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3969600.0000, 
sim time next is 3970200.0000, 
raw observation next is [31.16666666666667, 77.66666666666667, 1.0, 2.0, 0.6188350292305955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864793.4035982647, 864793.4035982647, 204307.8921634679], 
processed observation next is [0.0, 0.9565217391304348, 0.6761453396524489, 0.7766666666666667, 1.0, 1.0, 0.5407650954585488, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24022038988840685, 0.24022038988840685, 0.3049371524827879], 
reward next is 0.6951, 
noisyNet noise sample is [array([-1.6416932], dtype=float32), 0.91655505]. 
=============================================
[2019-03-26 11:58:42,916] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1168752: loss 0.2286
[2019-03-26 11:58:42,917] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1168752: learning rate 0.0005
[2019-03-26 11:58:45,267] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6990472e-19 5.4012354e-20 6.2560835e-28 6.0791275e-11 1.0000000e+00], sum to 1.0000
[2019-03-26 11:58:45,277] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4210
[2019-03-26 11:58:45,282] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 0.7803271279181478, 1.0, 2.0, 0.7107536034733366, 1.0, 2.0, 1.03, 7.005104066730324, 6.9112, 170.5573041426782, 2982515.085609387, 2915247.782837243, 547900.4845902127], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4024800.0000, 
sim time next is 4025400.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.8368649098988851, 1.0, 2.0, 0.7390224944637052, 1.0, 2.0, 1.03, 7.005108526309402, 6.9112, 170.5573041426782, 3101286.302610633, 3034015.805260591, 568042.9178739426], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.6, 1.0, 1.0, 0.8034516986733555, 1.0, 1.0, 0.6855692704381989, 1.0, 1.0, 1.0365853658536586, 0.009390852630940217, 0.0, 0.8375144448122397, 0.8614684173918425, 0.842782168127942, 0.8478252505581233], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7195387], dtype=float32), 0.94196254]. 
=============================================
[2019-03-26 11:58:45,982] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1170372: loss 0.2073
[2019-03-26 11:58:45,983] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1170373: learning rate 0.0005
[2019-03-26 11:58:46,365] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1170545: loss 0.0303
[2019-03-26 11:58:46,368] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1170546: learning rate 0.0005
[2019-03-26 11:58:46,373] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1170548: loss 0.0528
[2019-03-26 11:58:46,375] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1170548: learning rate 0.0005
[2019-03-26 11:58:46,423] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1170567: loss 0.0180
[2019-03-26 11:58:46,425] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1170567: learning rate 0.0005
[2019-03-26 11:58:46,797] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1170738: loss 0.0077
[2019-03-26 11:58:46,801] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1170738: learning rate 0.0005
[2019-03-26 11:58:46,821] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1170753: loss 0.0182
[2019-03-26 11:58:46,827] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1170753: learning rate 0.0005
[2019-03-26 11:58:46,966] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1170817: loss 0.0038
[2019-03-26 11:58:46,970] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1170819: learning rate 0.0005
[2019-03-26 11:58:46,987] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1170826: loss 0.0127
[2019-03-26 11:58:46,990] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1170827: learning rate 0.0005
[2019-03-26 11:58:47,066] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1170864: loss 0.0438
[2019-03-26 11:58:47,068] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1170864: learning rate 0.0005
[2019-03-26 11:58:47,108] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1170883: loss 0.0237
[2019-03-26 11:58:47,111] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1170884: learning rate 0.0005
[2019-03-26 11:58:48,157] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1171357: loss 0.0130
[2019-03-26 11:58:48,159] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1171357: learning rate 0.0005
[2019-03-26 11:58:48,232] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1171388: loss 1.0074
[2019-03-26 11:58:48,239] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1171390: learning rate 0.0005
[2019-03-26 11:58:52,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 11:58:52,028] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2547
[2019-03-26 11:58:52,034] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.16666666666666, 69.16666666666667, 1.0, 2.0, 0.6245771916449745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872821.1078463277, 872821.1078463277, 205415.340563011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4218600.0000, 
sim time next is 4219200.0000, 
raw observation next is [33.0, 71.0, 1.0, 2.0, 0.6320546486463455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 883274.8935925959, 883274.8935925965, 206871.0734202056], 
processed observation next is [1.0, 0.8695652173913043, 0.7630331753554502, 0.71, 1.0, 1.0, 0.5566923477666813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24535413710905443, 0.2453541371090546, 0.3087627961495606], 
reward next is 0.6912, 
noisyNet noise sample is [array([-0.71714926], dtype=float32), 0.69200796]. 
=============================================
[2019-03-26 11:58:54,061] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1173908: loss 33.5995
[2019-03-26 11:58:54,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1173908: learning rate 0.0005
[2019-03-26 11:58:56,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7444216e-24 1.0000000e+00 2.1457940e-34 2.4878665e-17 3.6752318e-16], sum to 1.0000
[2019-03-26 11:58:56,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3067
[2019-03-26 11:58:56,129] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2503899.158905641 W.
[2019-03-26 11:58:56,133] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.8952155467932796, 1.0, 2.0, 0.8952155467932796, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2503899.158905641, 2503899.158905641, 468888.2993088249], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4640400.0000, 
sim time next is 4641000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.3961104806888393, 1.0, 2.0, 0.3961104806888393, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1107212.989440301, 1107212.989440301, 271158.8829412246], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.2724222658901678, 1.0, 1.0, 0.2724222658901678, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.30755916373341696, 0.30755916373341696, 0.4047147506585442], 
reward next is 0.5953, 
noisyNet noise sample is [array([1.0829648], dtype=float32), -1.1440493]. 
=============================================
[2019-03-26 11:58:56,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[47.66823 ]
 [46.979866]
 [46.861423]
 [46.961018]
 [45.94031 ]], R is [[53.89697647]
 [53.65817261]
 [53.40686417]
 [53.15403366]
 [52.93193054]].
[2019-03-26 11:58:56,495] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 11:58:56,498] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 11:58:56,498] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 11:58:56,500] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 11:58:56,499] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:58:56,502] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 11:58:56,503] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:58:56,501] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 11:58:56,502] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:58:56,504] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:58:56,506] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 11:58:56,533] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run48
[2019-03-26 11:58:56,559] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run48
[2019-03-26 11:58:56,583] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run48
[2019-03-26 11:58:56,583] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run48
[2019-03-26 11:58:56,583] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run48
[2019-03-26 11:59:00,556] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.23400265], dtype=float32), 0.59385055]
[2019-03-26 11:59:00,558] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.15491865333333, 94.59186249666668, 1.0, 2.0, 0.3167092342229063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501304.627874682, 501304.627874682, 167132.7791760181]
[2019-03-26 11:59:00,562] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:59:00,564] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22903025853542391
[2019-03-26 11:59:24,733] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.23400265], dtype=float32), 0.59385055]
[2019-03-26 11:59:24,738] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.4, 84.0, 1.0, 2.0, 0.5186983281338335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796305.0887224146, 796305.0887224146, 195174.0653527489]
[2019-03-26 11:59:24,740] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:59:24,745] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9287178039586944
[2019-03-26 11:59:25,474] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.23400265], dtype=float32), 0.59385055]
[2019-03-26 11:59:25,476] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.00570365, 81.506886255, 1.0, 2.0, 0.4902570774564224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685053.7056170929, 685053.7056170935, 182083.9823055031]
[2019-03-26 11:59:25,478] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 11:59:25,481] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0874377583558198
[2019-03-26 11:59:49,864] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.23400265], dtype=float32), 0.59385055]
[2019-03-26 11:59:49,867] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.54172839666667, 76.66368606166667, 1.0, 2.0, 0.6722546665850865, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005973551126365, 6.9112, 168.912316049949, 1836303.964919299, 1769068.600830962, 378748.2808070014]
[2019-03-26 11:59:49,868] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 11:59:49,872] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.5566187e-28 1.0000000e+00 2.7848409e-34 8.2140753e-32 2.4278365e-16], sampled 0.6860919270451761
[2019-03-26 11:59:49,875] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1836303.964919299 W.
[2019-03-26 11:59:50,136] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.23400265], dtype=float32), 0.59385055]
[2019-03-26 11:59:50,138] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.5, 55.0, 1.0, 2.0, 0.55250186282532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104276, 772062.1734779304, 772062.1734779304, 192232.916643146]
[2019-03-26 11:59:50,140] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 11:59:50,142] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4560629e-30 0.0000000e+00], sampled 0.9866136113277297
[2019-03-26 12:00:02,178] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23400265], dtype=float32), 0.59385055]
[2019-03-26 12:00:02,180] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.66666666666667, 76.33333333333333, 1.0, 2.0, 0.5949407814006721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 831389.2259173662, 831389.2259173656, 199807.3334906885]
[2019-03-26 12:00:02,182] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:00:02,185] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.26518866182323064
[2019-03-26 12:00:08,239] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.23400265], dtype=float32), 0.59385055]
[2019-03-26 12:00:08,242] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.34250433666667, 79.80943385, 1.0, 2.0, 0.4259377832955493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 625517.7645267033, 625517.7645267026, 176676.7312258125]
[2019-03-26 12:00:08,243] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:00:08,249] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8251460751962969
[2019-03-26 12:00:39,613] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.23400265], dtype=float32), 0.59385055]
[2019-03-26 12:00:39,615] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.6, 95.0, 1.0, 2.0, 0.7468281725083359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1043745.81040739, 1043745.810407391, 231276.3044426494]
[2019-03-26 12:00:39,615] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:00:39,620] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.401604e-35], sampled 0.1766139412605222
[2019-03-26 12:00:52,157] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.3478 2927836962.7235 1354.0000
[2019-03-26 12:00:52,252] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.3141 3164289840.2214 1832.0000
[2019-03-26 12:00:52,569] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.3625 3007798351.3435 1753.0000
[2019-03-26 12:00:52,585] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.6866 2779708565.4254 936.0000
[2019-03-26 12:00:52,627] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.0136 2842679403.1848 1147.0000
[2019-03-26 12:00:53,644] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1175000, evaluation results [1175000.0, 7885.3140701775765, 3164289840.2214212, 1832.0, 8249.34781208291, 2927836962.723477, 1354.0, 8659.686580276979, 2779708565.425445, 936.0, 8000.362459246696, 3007798351.343542, 1753.0, 8493.013598930822, 2842679403.184848, 1147.0]
[2019-03-26 12:00:54,098] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1175218: loss 0.0964
[2019-03-26 12:00:54,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1175219: learning rate 0.0005
[2019-03-26 12:00:55,211] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.3311084e-37 1.0000000e+00 0.0000000e+00 1.2711110e-37 4.4077510e-29], sum to 1.0000
[2019-03-26 12:00:55,218] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3689
[2019-03-26 12:00:55,225] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.5983498642354573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 836155.0622102371, 836155.0622102377, 200439.0969074108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4236600.0000, 
sim time next is 4237200.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5980822105192695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835780.8863707257, 835780.8863707257, 200389.3920534862], 
processed observation next is [1.0, 0.043478260869565216, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5157616994208065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23216135732520157, 0.23216135732520157, 0.29908864485594955], 
reward next is 0.7009, 
noisyNet noise sample is [array([2.045749], dtype=float32), 1.8198069]. 
=============================================
[2019-03-26 12:00:56,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4083837e-24 1.0000000e+00 2.2595508e-31 4.7326416e-26 3.0766406e-19], sum to 1.0000
[2019-03-26 12:00:56,382] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0213
[2019-03-26 12:00:56,390] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1660813.542173238 W.
[2019-03-26 12:00:56,398] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.5940102856076351, 1.0, 1.0, 0.5940102856076351, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1660813.542173238, 1660813.542173238, 332063.8507623412], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4336200.0000, 
sim time next is 4336800.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.507796566895707, 1.0, 2.0, 0.507796566895707, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1419606.037946698, 1419606.037946698, 302706.6652958892], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.89, 1.0, 1.0, 0.4069838155369963, 1.0, 1.0, 0.4069838155369963, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.39433501054074943, 0.39433501054074943, 0.4518009929789391], 
reward next is 0.5482, 
noisyNet noise sample is [array([0.46005014], dtype=float32), -0.1247561]. 
=============================================
[2019-03-26 12:00:56,400] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1176291: loss 1.5919
[2019-03-26 12:00:56,402] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1176292: learning rate 0.0005
[2019-03-26 12:00:57,479] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1176783: loss 9.3654
[2019-03-26 12:00:57,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1176783: learning rate 0.0005
[2019-03-26 12:01:00,876] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1178365: loss 1.0923
[2019-03-26 12:01:00,878] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1178367: learning rate 0.0005
[2019-03-26 12:01:01,137] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1178484: loss 2.5516
[2019-03-26 12:01:01,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1178484: learning rate 0.0005
[2019-03-26 12:01:01,371] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1178595: loss 2.1821
[2019-03-26 12:01:01,377] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1178596: learning rate 0.0005
[2019-03-26 12:01:01,380] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1178600: loss -92.0983
[2019-03-26 12:01:01,384] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1178600: learning rate 0.0005
[2019-03-26 12:01:01,573] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1178689: loss 4.6098
[2019-03-26 12:01:01,575] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1178689: learning rate 0.0005
[2019-03-26 12:01:01,722] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1178756: loss -102.1781
[2019-03-26 12:01:01,724] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1178756: learning rate 0.0005
[2019-03-26 12:01:01,937] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1178853: loss -34.0813
[2019-03-26 12:01:01,940] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1178854: learning rate 0.0005
[2019-03-26 12:01:02,082] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1178916: loss -22.3425
[2019-03-26 12:01:02,084] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1178917: learning rate 0.0005
[2019-03-26 12:01:02,095] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1178923: loss 2.6374
[2019-03-26 12:01:02,096] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1178923: learning rate 0.0005
[2019-03-26 12:01:02,147] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1178944: loss -23.7421
[2019-03-26 12:01:02,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1178945: learning rate 0.0005
[2019-03-26 12:01:02,832] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1179265: loss 0.0818
[2019-03-26 12:01:02,833] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1179265: learning rate 0.0005
[2019-03-26 12:01:02,995] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1179340: loss 1.1686
[2019-03-26 12:01:02,998] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1179342: learning rate 0.0005
[2019-03-26 12:01:05,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:01:05,370] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1298
[2019-03-26 12:01:05,380] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.581932232265512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 813203.7054230262, 813203.7054230255, 197428.1532226166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4423200.0000, 
sim time next is 4423800.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5809484113354244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811828.3690380023, 811828.3690380023, 197250.3394906312], 
processed observation next is [0.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49511856787400527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.225507880288334, 0.225507880288334, 0.2944034917770615], 
reward next is 0.7056, 
noisyNet noise sample is [array([-0.87917435], dtype=float32), -0.7971897]. 
=============================================
[2019-03-26 12:01:07,346] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:01:07,354] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0111
[2019-03-26 12:01:07,360] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.16666666666667, 74.33333333333333, 1.0, 2.0, 0.645239582179994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 901708.2118517492, 901708.2118517499, 209476.9051486873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4446600.0000, 
sim time next is 4447200.0000, 
raw observation next is [32.33333333333334, 73.66666666666667, 1.0, 2.0, 0.638097030752201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 891722.4620622793, 891722.4620622793, 208058.7000524227], 
processed observation next is [0.0, 0.4782608695652174, 0.7314375987361774, 0.7366666666666667, 1.0, 1.0, 0.5639723262074711, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2477006839061887, 0.2477006839061887, 0.3105353732125712], 
reward next is 0.6895, 
noisyNet noise sample is [array([0.5692189], dtype=float32), -1.4442394]. 
=============================================
[2019-03-26 12:01:07,419] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5254014e-21 1.0000000e+00 3.5064257e-29 1.4777283e-24 1.6449842e-14], sum to 1.0000
[2019-03-26 12:01:07,429] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6555
[2019-03-26 12:01:07,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2630744.711893832 W.
[2019-03-26 12:01:07,447] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.3844363483507, 6.9112, 168.9100078810047, 2630744.711893832, 2295020.419509954, 475612.6082496032], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4881600.0000, 
sim time next is 4882200.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.836658143355772, 1.0, 1.0, 0.836658143355772, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2339961.977404635, 2339961.977404636, 438109.411310863], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.8032025823563519, 1.0, 0.5, 0.8032025823563519, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6499894381679542, 0.6499894381679544, 0.6538946437475567], 
reward next is 0.3461, 
noisyNet noise sample is [array([0.24228415], dtype=float32), 1.7222005]. 
=============================================
[2019-03-26 12:01:08,364] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1181823: loss 0.3040
[2019-03-26 12:01:08,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1181826: learning rate 0.0005
[2019-03-26 12:01:08,939] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:01:08,950] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1474
[2019-03-26 12:01:08,955] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.4892636336620337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683665.0844578402, 683665.0844578402, 181930.6123048576], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4755000.0000, 
sim time next is 4755600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4879558388627915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681837.0708440433, 681837.070844044, 181730.1153975545], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 1.0, 1.0, 0.383079323931074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18939918634556757, 0.18939918634556777, 0.27123897820530524], 
reward next is 0.7288, 
noisyNet noise sample is [array([-0.25836217], dtype=float32), -0.716614]. 
=============================================
[2019-03-26 12:01:11,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:01:11,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5737
[2019-03-26 12:01:11,756] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5108780687742124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713877.8346131871, 713877.8346131865, 185317.563013489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4522800.0000, 
sim time next is 4523400.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.5084769099822527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710521.4421212187, 710521.4421212187, 184934.4049114291], 
processed observation next is [0.0, 0.34782608695652173, 0.5023696682464456, 0.79, 1.0, 1.0, 0.4078035060027141, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19736706725589406, 0.19736706725589406, 0.2760214998678046], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.577426], dtype=float32), -0.8352306]. 
=============================================
[2019-03-26 12:01:11,788] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1183420: loss 64.3230
[2019-03-26 12:01:11,796] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1183420: learning rate 0.0005
[2019-03-26 12:01:13,586] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1184203: loss 0.0881
[2019-03-26 12:01:13,588] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1184203: learning rate 0.0005
[2019-03-26 12:01:14,717] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1184735: loss 0.3945
[2019-03-26 12:01:14,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1184735: learning rate 0.0005
[2019-03-26 12:01:16,401] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6001854e-22 1.9092694e-08 9.7504681e-26 1.9038209e-24 1.0000000e+00], sum to 1.0000
[2019-03-26 12:01:16,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4190
[2019-03-26 12:01:16,412] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 87.33333333333334, 1.0, 2.0, 0.6083062079246087, 1.0, 1.0, 0.6083062079246087, 1.0, 1.0, 1.03, 6.940908041209208, 6.9112, 170.5573041426782, 2552178.866442685, 2530897.788078676, 491009.4283904542], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4587600.0000, 
sim time next is 4588200.0000, 
raw observation next is [28.0, 86.5, 1.0, 2.0, 0.4644731532949091, 1.0, 2.0, 0.4644731532949091, 1.0, 2.0, 0.8066362636542616, 6.9112, 6.9112, 170.5573041426782, 1948215.447494898, 1948215.447494898, 391447.7631190495], 
processed observation next is [1.0, 0.08695652173913043, 0.5260663507109005, 0.865, 1.0, 1.0, 0.35478693168061337, 1.0, 1.0, 0.35478693168061337, 1.0, 1.0, 0.7641905654320261, 0.0, 0.0, 0.8375144448122397, 0.5411709576374717, 0.5411709576374717, 0.5842503927149992], 
reward next is 0.4157, 
noisyNet noise sample is [array([0.9745948], dtype=float32), -0.18653628]. 
=============================================
[2019-03-26 12:01:18,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7413756e-18 9.7853231e-01 5.2530383e-29 3.4900855e-18 2.1467671e-02], sum to 1.0000
[2019-03-26 12:01:18,044] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7738
[2019-03-26 12:01:18,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2598141.345768861 W.
[2019-03-26 12:01:18,057] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.66666666666666, 69.0, 1.0, 2.0, 0.6192498851967013, 1.0, 2.0, 0.6192498851967013, 1.0, 2.0, 1.03, 6.962274391502443, 6.9112, 170.5573041426782, 2598141.345768861, 2561554.681562247, 495064.1031977878], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4639200.0000, 
sim time next is 4639800.0000, 
raw observation next is [31.33333333333334, 69.5, 1.0, 2.0, 0.6173685299438122, 1.0, 2.0, 0.6173685299438122, 1.0, 2.0, 1.03, 6.958601192264477, 6.9112, 170.5573041426782, 2590239.699288444, 2556284.297112177, 494362.0866308089], 
processed observation next is [1.0, 0.6956521739130435, 0.6840442338072673, 0.695, 1.0, 1.0, 0.5389982288479664, 1.0, 1.0, 0.5389982288479664, 1.0, 1.0, 1.0365853658536586, 0.004740119226447704, 0.0, 0.8375144448122397, 0.7195110275801234, 0.7100789714200492, 0.7378538606429983], 
reward next is 0.0251, 
noisyNet noise sample is [array([0.51357543], dtype=float32), 1.7273284]. 
=============================================
[2019-03-26 12:01:18,080] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1186282: loss 0.4042
[2019-03-26 12:01:18,086] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1186282: learning rate 0.0005
[2019-03-26 12:01:18,420] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1186440: loss 0.1819
[2019-03-26 12:01:18,422] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1186441: learning rate 0.0005
[2019-03-26 12:01:18,626] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1186538: loss 0.0508
[2019-03-26 12:01:18,628] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1186539: learning rate 0.0005
[2019-03-26 12:01:18,650] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1186547: loss 0.2248
[2019-03-26 12:01:18,651] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1186547: learning rate 0.0005
[2019-03-26 12:01:18,864] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1186643: loss 0.1656
[2019-03-26 12:01:18,866] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1186644: learning rate 0.0005
[2019-03-26 12:01:18,947] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1186684: loss 0.4676
[2019-03-26 12:01:18,949] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1186684: learning rate 0.0005
[2019-03-26 12:01:19,270] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1186830: loss 1.0306
[2019-03-26 12:01:19,275] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1186831: learning rate 0.0005
[2019-03-26 12:01:19,347] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1186867: loss 0.5200
[2019-03-26 12:01:19,349] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1186867: learning rate 0.0005
[2019-03-26 12:01:19,366] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1186877: loss 0.6179
[2019-03-26 12:01:19,367] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1186877: learning rate 0.0005
[2019-03-26 12:01:19,397] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1186893: loss 0.9279
[2019-03-26 12:01:19,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1186894: learning rate 0.0005
[2019-03-26 12:01:19,683] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:01:19,695] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2509
[2019-03-26 12:01:19,701] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5163970000001702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721592.3580844634, 721592.3580844641, 186205.1566826971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4740600.0000, 
sim time next is 4741200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5156445368819844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720540.5398331, 720540.5398331007, 186083.7356781453], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4164392010626318, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2001501499536389, 0.20015014995363908, 0.27773691892260494], 
reward next is 0.7223, 
noisyNet noise sample is [array([1.3861986], dtype=float32), -1.0611542]. 
=============================================
[2019-03-26 12:01:20,341] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1187339: loss 0.6793
[2019-03-26 12:01:20,342] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1187339: learning rate 0.0005
[2019-03-26 12:01:20,834] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1187570: loss 40.7353
[2019-03-26 12:01:20,843] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1187574: learning rate 0.0005
[2019-03-26 12:01:25,847] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1189895: loss 2.3674
[2019-03-26 12:01:25,850] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1189896: learning rate 0.0005
[2019-03-26 12:01:28,911] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1191297: loss 0.0030
[2019-03-26 12:01:28,914] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1191299: learning rate 0.0005
[2019-03-26 12:01:30,950] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1192258: loss 61.1926
[2019-03-26 12:01:30,954] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1192259: learning rate 0.0005
[2019-03-26 12:01:32,059] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1192769: loss 22.1694
[2019-03-26 12:01:32,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1192770: learning rate 0.0005
[2019-03-26 12:01:32,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0963621e-22 1.0000000e+00 3.3301206e-34 1.0653229e-23 3.3868774e-10], sum to 1.0000
[2019-03-26 12:01:32,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9453
[2019-03-26 12:01:32,860] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3176948.941047517 W.
[2019-03-26 12:01:32,865] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.08333333333334, 53.0, 1.0, 2.0, 0.8728792313088031, 1.0, 2.0, 0.757029655168664, 1.0, 2.0, 1.03, 7.0051113674839, 6.9112, 170.5573041426782, 3176948.941047517, 3109676.40844861, 581495.5279600952], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5325000.0000, 
sim time next is 5325600.0000, 
raw observation next is [36.06666666666667, 53.0, 1.0, 2.0, 0.8532389612941436, 1.0, 2.0, 0.7472095201613345, 1.0, 2.0, 1.03, 7.005109818017793, 6.9112, 170.5573041426782, 3135686.047368099, 3068414.624714793, 574099.4264456219], 
processed observation next is [1.0, 0.6521739130434783, 0.9083728278041076, 0.53, 1.0, 1.0, 0.8231794714387272, 1.0, 1.0, 0.6954331568208849, 1.0, 1.0, 1.0365853658536586, 0.009390981801779308, 0.0, 0.8375144448122397, 0.8710239020466941, 0.8523373957541092, 0.8568648155904804], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9914493], dtype=float32), -0.47454157]. 
=============================================
[2019-03-26 12:01:35,077] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1194260: loss 61.2639
[2019-03-26 12:01:35,079] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1194261: learning rate 0.0005
[2019-03-26 12:01:35,297] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1194400: loss 13.9675
[2019-03-26 12:01:35,299] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1194400: learning rate 0.0005
[2019-03-26 12:01:35,506] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1194540: loss 58.7790
[2019-03-26 12:01:35,507] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1194540: learning rate 0.0005
[2019-03-26 12:01:35,666] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1194634: loss 62.3036
[2019-03-26 12:01:35,672] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1194635: learning rate 0.0005
[2019-03-26 12:01:35,678] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1194640: loss 21.1922
[2019-03-26 12:01:35,686] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1194643: learning rate 0.0005
[2019-03-26 12:01:35,795] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1194692: loss 20.0497
[2019-03-26 12:01:35,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1194692: learning rate 0.0005
[2019-03-26 12:01:36,027] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1194798: loss 60.2785
[2019-03-26 12:01:36,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1194798: learning rate 0.0005
[2019-03-26 12:01:36,153] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1194851: loss 9.0759
[2019-03-26 12:01:36,157] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1194853: learning rate 0.0005
[2019-03-26 12:01:36,187] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1194869: loss 9.9514
[2019-03-26 12:01:36,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1194870: learning rate 0.0005
[2019-03-26 12:01:36,290] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1194907: loss 20.5541
[2019-03-26 12:01:36,293] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1194909: learning rate 0.0005
[2019-03-26 12:01:37,214] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1195320: loss 25.1482
[2019-03-26 12:01:37,218] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1195320: learning rate 0.0005
[2019-03-26 12:01:37,657] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1195516: loss 0.1529
[2019-03-26 12:01:37,660] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1195517: learning rate 0.0005
[2019-03-26 12:01:39,903] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:01:39,913] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0879
[2019-03-26 12:01:39,919] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 78.0, 1.0, 2.0, 0.533352413587641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745293.4815419449, 745293.4815419449, 188986.9330234895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5041200.0000, 
sim time next is 5041800.0000, 
raw observation next is [29.0, 75.0, 1.0, 2.0, 0.5296682617982389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740143.5453028558, 740143.5453028558, 188375.121748588], 
processed observation next is [0.0, 0.34782608695652173, 0.5734597156398105, 0.75, 1.0, 1.0, 0.43333525517860105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20559542925079327, 0.20559542925079327, 0.2811568981322209], 
reward next is 0.7188, 
noisyNet noise sample is [array([-1.152198], dtype=float32), -0.15597534]. 
=============================================
[2019-03-26 12:01:42,610] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1197757: loss 4.3790
[2019-03-26 12:01:42,613] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1197757: learning rate 0.0005
[2019-03-26 12:01:44,278] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:01:44,292] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5239
[2019-03-26 12:01:44,296] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4811471539177737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672445.3420356801, 672445.3420356801, 180710.5309765235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5116200.0000, 
sim time next is 5116800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4802510403429787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671192.9425618037, 671192.9425618037, 180575.3256395515], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3737964341481671, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18644248404494548, 0.18644248404494548, 0.26951541140231566], 
reward next is 0.7305, 
noisyNet noise sample is [array([-0.5724724], dtype=float32), 0.13382103]. 
=============================================
[2019-03-26 12:01:46,233] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1199388: loss 2.5448
[2019-03-26 12:01:46,235] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1199388: learning rate 0.0005
[2019-03-26 12:01:46,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:01:46,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6304
[2019-03-26 12:01:46,964] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4810110475793439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 672255.3262501468, 672255.3262501474, 180690.0064989834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5118600.0000, 
sim time next is 5119200.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4804699605607315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671499.0081530006, 671499.0081530006, 180608.3478709486], 
processed observation next is [0.0, 0.2608695652173913, 0.4312796208530806, 0.84, 1.0, 1.0, 0.37406019344666447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1865275022647224, 0.1865275022647224, 0.2695646983148487], 
reward next is 0.7304, 
noisyNet noise sample is [array([-0.84428877], dtype=float32), -0.09997141]. 
=============================================
[2019-03-26 12:01:47,609] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 12:01:47,610] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:01:47,611] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:01:47,612] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:01:47,613] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:01:47,613] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:01:47,614] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:01:47,616] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:01:47,616] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:01:47,616] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:01:47,618] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:01:47,646] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run49
[2019-03-26 12:01:47,668] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run49
[2019-03-26 12:01:47,669] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run49
[2019-03-26 12:01:47,711] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run49
[2019-03-26 12:01:47,731] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run49
[2019-03-26 12:01:56,063] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:01:56,067] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.66666666666667, 71.33333333333333, 1.0, 2.0, 0.2292654182518555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380958.4659331106, 380958.4659331106, 158636.7980638742]
[2019-03-26 12:01:56,068] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:01:56,074] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5673870914382744
[2019-03-26 12:01:58,222] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:01:58,224] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.05, 57.0, 1.0, 2.0, 0.2372704020837894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 390053.9359032724, 390053.9359032724, 159683.1448555862]
[2019-03-26 12:01:58,225] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:01:58,229] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.023667887620972716
[2019-03-26 12:02:09,196] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:02:09,197] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.15418370333333, 91.10515419333333, 1.0, 2.0, 0.5324128175916664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743980.0535170387, 743980.0535170387, 188831.851339133]
[2019-03-26 12:02:09,199] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:02:09,200] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.866204667898026
[2019-03-26 12:02:10,602] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:02:10,604] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.7, 51.66666666666667, 1.0, 2.0, 0.3714434896909298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 559244.8657543177, 559244.8657543184, 171039.0920033883]
[2019-03-26 12:02:10,605] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:02:10,608] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7771137582130342
[2019-03-26 12:02:16,484] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:02:16,485] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.94086054333333, 99.53873220333332, 1.0, 2.0, 0.4470020338862266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 645551.8616789894, 645551.8616789888, 178390.985095159]
[2019-03-26 12:02:16,488] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:02:16,492] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6041202e-36], sampled 0.5762936271789284
[2019-03-26 12:02:37,819] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:02:37,821] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.85, 62.5, 1.0, 2.0, 0.7415875448416128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1036418.084141251, 1036418.084141252, 230078.9703861855]
[2019-03-26 12:02:37,823] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:02:37,827] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7657284e-34 0.0000000e+00], sampled 0.7203128649416415
[2019-03-26 12:02:42,942] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:02:42,942] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.46245588, 67.76433945, 1.0, 2.0, 0.5324474155046056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744028.4166875852, 744028.4166875852, 188834.0535823367]
[2019-03-26 12:02:42,944] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:02:42,947] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.029393e-38 0.000000e+00], sampled 0.7263664435388671
[2019-03-26 12:03:06,596] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:03:06,597] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.78649835, 82.716377715, 1.0, 2.0, 0.737964860761965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1031352.682805418, 1031352.682805418, 229266.4106071141]
[2019-03-26 12:03:06,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:03:06,600] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2186559e-33], sampled 0.2970199761375014
[2019-03-26 12:03:11,042] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:03:11,043] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.19101347333334, 65.61807956333334, 1.0, 2.0, 0.5009300262500082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699972.311347998, 699972.3113479987, 183740.6330173048]
[2019-03-26 12:03:11,044] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:03:11,048] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.31608306922424745
[2019-03-26 12:03:19,623] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:03:19,625] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.45212514666667, 70.42159134333333, 1.0, 2.0, 0.5492157218560821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767468.4845220939, 767468.4845220945, 191666.6437836138]
[2019-03-26 12:03:19,627] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:03:19,629] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.25916870088272326
[2019-03-26 12:03:26,525] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.21618189], dtype=float32), 0.5466914]
[2019-03-26 12:03:26,526] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.1, 90.0, 1.0, 2.0, 0.4849727729608908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679466.8920740408, 679466.8920740403, 181506.5623697153]
[2019-03-26 12:03:26,528] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:03:26,531] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3449965e-37], sampled 0.8941093072445522
[2019-03-26 12:03:43,692] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.4856 2928331863.4074 1349.0000
[2019-03-26 12:03:43,712] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8007.5206 3007387139.3006 1740.0000
[2019-03-26 12:03:43,737] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.5567 2779720226.6712 926.0000
[2019-03-26 12:03:43,777] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.9433 2842256092.5147 1130.0000
[2019-03-26 12:03:43,838] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7894.9279 3164192691.9442 1825.0000
[2019-03-26 12:03:44,852] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1200000, evaluation results [1200000.0, 7894.927942807379, 3164192691.9441624, 1825.0, 8255.485569523466, 2928331863.4074354, 1349.0, 8663.55667786353, 2779720226.671228, 926.0, 8007.520578110966, 3007387139.300574, 1740.0, 8496.943317199044, 2842256092.514698, 1130.0]
[2019-03-26 12:03:45,348] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1200236: loss 1.2220
[2019-03-26 12:03:45,353] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1200237: learning rate 0.0005
[2019-03-26 12:03:45,860] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:03:45,868] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0656
[2019-03-26 12:03:45,873] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 82.33333333333334, 1.0, 2.0, 0.5056631574720717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706588.3305639104, 706588.3305639104, 184487.849993399], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5182800.0000, 
sim time next is 5183400.0000, 
raw observation next is [27.0, 83.16666666666666, 1.0, 2.0, 0.5090934949786691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711383.3171582368, 711383.3171582373, 185032.8877888426], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.8316666666666666, 1.0, 1.0, 0.4085463794923723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1976064769883991, 0.19760647698839925, 0.27616848923707854], 
reward next is 0.7238, 
noisyNet noise sample is [array([0.831692], dtype=float32), -0.11229838]. 
=============================================
[2019-03-26 12:03:46,503] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1200763: loss 0.1764
[2019-03-26 12:03:46,506] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1200764: learning rate 0.0005
[2019-03-26 12:03:47,637] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5136174e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.4751815e-31], sum to 1.0000
[2019-03-26 12:03:47,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9849
[2019-03-26 12:03:47,653] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.8712444868087609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217726.216471143, 1217726.216471143, 262147.7242956045], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5197200.0000, 
sim time next is 5197800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.8397750199891579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1173717.47252946, 1173717.47252946, 253905.863698364], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.806957855408624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3260326312581833, 0.3260326312581833, 0.3789639756692], 
reward next is 0.6210, 
noisyNet noise sample is [array([0.18928146], dtype=float32), -1.1128101]. 
=============================================
[2019-03-26 12:03:49,678] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1202227: loss 3.0791
[2019-03-26 12:03:49,679] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1202228: learning rate 0.0005
[2019-03-26 12:03:50,074] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1202410: loss 0.1942
[2019-03-26 12:03:50,078] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1202411: learning rate 0.0005
[2019-03-26 12:03:50,273] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1202505: loss 0.4361
[2019-03-26 12:03:50,275] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1202505: learning rate 0.0005
[2019-03-26 12:03:50,448] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1202584: loss 0.7900
[2019-03-26 12:03:50,449] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1202584: learning rate 0.0005
[2019-03-26 12:03:50,599] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1202653: loss 0.4202
[2019-03-26 12:03:50,603] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1202654: learning rate 0.0005
[2019-03-26 12:03:50,749] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1202716: loss 0.8500
[2019-03-26 12:03:50,752] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1202716: learning rate 0.0005
[2019-03-26 12:03:51,335] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1202861: loss 0.2308
[2019-03-26 12:03:51,341] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1202863: learning rate 0.0005
[2019-03-26 12:03:51,454] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1202915: loss 0.0601
[2019-03-26 12:03:51,462] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1202917: learning rate 0.0005
[2019-03-26 12:03:51,492] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1202932: loss 0.0898
[2019-03-26 12:03:51,494] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1202932: learning rate 0.0005
[2019-03-26 12:03:51,564] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1202961: loss 0.0937
[2019-03-26 12:03:51,568] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1202962: learning rate 0.0005
[2019-03-26 12:03:52,372] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1203337: loss 0.0212
[2019-03-26 12:03:52,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1203337: learning rate 0.0005
[2019-03-26 12:03:52,918] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1203590: loss 1.5183
[2019-03-26 12:03:52,920] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1203591: learning rate 0.0005
[2019-03-26 12:03:57,852] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1205894: loss 1.4779
[2019-03-26 12:03:57,853] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1205894: learning rate 0.0005
[2019-03-26 12:03:58,496] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.2070576e-38 1.0000000e+00 0.0000000e+00 1.0305759e-36 2.3952470e-36], sum to 1.0000
[2019-03-26 12:03:58,507] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9025
[2019-03-26 12:03:58,512] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 90.0, 1.0, 2.0, 0.8687227565670639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104229, 1214199.614104304, 1214199.614104304, 261484.4982820235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5376600.0000, 
sim time next is 5377200.0000, 
raw observation next is [29.1, 88.66666666666666, 1.0, 2.0, 0.8738278258082947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1221338.989919095, 1221338.989919095, 262845.3602452702], 
processed observation next is [1.0, 0.21739130434782608, 0.5781990521327015, 0.8866666666666666, 1.0, 1.0, 0.8479853322991502, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.339260830533082, 0.339260830533082, 0.3923065078287615], 
reward next is 0.6077, 
noisyNet noise sample is [array([-0.7170586], dtype=float32), 0.0072470796]. 
=============================================
[2019-03-26 12:03:58,876] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5371810e-31 1.0000000e+00 1.1788931e-33 2.5966512e-32 6.5158809e-27], sum to 1.0000
[2019-03-26 12:03:58,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1765
[2019-03-26 12:03:58,889] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 83.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.950632730875828, 6.9112, 168.9125534133584, 1481748.955499687, 1453774.085806435, 311354.3730987394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5466600.0000, 
sim time next is 5467200.0000, 
raw observation next is [29.5, 82.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.151427281635259, 6.9112, 168.9114775842434, 1624295.654549244, 1453871.645141601, 311354.6066453936], 
processed observation next is [1.0, 0.2608695652173913, 0.5971563981042655, 0.8266666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.024022728163525908, 0.0, 0.8294326829476663, 0.45119323737479, 0.4038532347615559, 0.46470836812745314], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.39822185], dtype=float32), -0.4943601]. 
=============================================
[2019-03-26 12:04:00,748] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1207236: loss 4.3778
[2019-03-26 12:04:00,750] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1207236: learning rate 0.0005
[2019-03-26 12:04:00,889] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:04:00,900] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3975
[2019-03-26 12:04:00,904] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 85.16666666666667, 1.0, 2.0, 0.5308337023828295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741772.6678518901, 741772.6678518901, 188568.29408851], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6207000.0000, 
sim time next is 6207600.0000, 
raw observation next is [27.43333333333334, 85.33333333333334, 1.0, 2.0, 0.5302720112969685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740987.5021478784, 740987.5021478777, 188475.2167745492], 
processed observation next is [1.0, 0.8695652173913043, 0.49921011058451853, 0.8533333333333334, 1.0, 1.0, 0.4340626642132151, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.205829861707744, 0.2058298617077438, 0.281306293693357], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.20648937], dtype=float32), 0.5135059]. 
=============================================
[2019-03-26 12:04:02,859] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1208225: loss 1.3885
[2019-03-26 12:04:02,861] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1208226: learning rate 0.0005
[2019-03-26 12:04:04,033] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1208768: loss 0.9867
[2019-03-26 12:04:04,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1208769: learning rate 0.0005
[2019-03-26 12:04:06,861] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1210101: loss 1.3813
[2019-03-26 12:04:06,865] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1210102: learning rate 0.0005
[2019-03-26 12:04:07,313] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1210309: loss 1.6291
[2019-03-26 12:04:07,315] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1210309: learning rate 0.0005
[2019-03-26 12:04:07,802] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1210535: loss 1.5841
[2019-03-26 12:04:07,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1210537: learning rate 0.0005
[2019-03-26 12:04:07,981] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1210615: loss 1.3881
[2019-03-26 12:04:07,983] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1210615: learning rate 0.0005
[2019-03-26 12:04:08,096] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1210671: loss 2.7157
[2019-03-26 12:04:08,098] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1210673: learning rate 0.0005
[2019-03-26 12:04:08,110] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1210679: loss 1.4876
[2019-03-26 12:04:08,115] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1210679: learning rate 0.0005
[2019-03-26 12:04:08,589] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1210896: loss 1.2853
[2019-03-26 12:04:08,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1210896: learning rate 0.0005
[2019-03-26 12:04:08,617] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1210909: loss 1.8025
[2019-03-26 12:04:08,618] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1210910: learning rate 0.0005
[2019-03-26 12:04:08,738] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1210962: loss 0.9805
[2019-03-26 12:04:08,741] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1210962: learning rate 0.0005
[2019-03-26 12:04:08,799] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1210991: loss 1.8508
[2019-03-26 12:04:08,801] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1210992: learning rate 0.0005
[2019-03-26 12:04:09,609] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1211369: loss 1.2354
[2019-03-26 12:04:09,611] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1211371: learning rate 0.0005
[2019-03-26 12:04:09,725] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1211420: loss 0.2442
[2019-03-26 12:04:09,730] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1211421: learning rate 0.0005
[2019-03-26 12:04:12,345] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.8089076e-38 1.0000000e+00 0.0000000e+00 6.4581189e-36 2.1473634e-29], sum to 1.0000
[2019-03-26 12:04:12,353] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2788
[2019-03-26 12:04:12,359] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 90.5, 1.0, 2.0, 0.5268983477403281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736271.603802308, 736271.603802308, 187917.6247835923], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5614200.0000, 
sim time next is 5614800.0000, 
raw observation next is [26.4, 90.66666666666667, 1.0, 2.0, 0.5261981594406021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735292.8434909399, 735292.8434909392, 187802.383680857], 
processed observation next is [1.0, 1.0, 0.45023696682464454, 0.9066666666666667, 1.0, 1.0, 0.42915440896458085, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20424801208081664, 0.20424801208081644, 0.2803020651953089], 
reward next is 0.7197, 
noisyNet noise sample is [array([-1.4774625], dtype=float32), -0.042378005]. 
=============================================
[2019-03-26 12:04:13,014] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2172747e-35], sum to 1.0000
[2019-03-26 12:04:13,021] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0428
[2019-03-26 12:04:13,027] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 92.33333333333333, 1.0, 2.0, 0.5112668492724083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714421.2815292643, 714421.2815292637, 185379.8402548226], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5625600.0000, 
sim time next is 5626200.0000, 
raw observation next is [25.7, 92.16666666666667, 1.0, 2.0, 0.5096127999432747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712109.2127325597, 712109.2127325597, 185115.6167547054], 
processed observation next is [0.0, 0.08695652173913043, 0.4170616113744076, 0.9216666666666667, 1.0, 1.0, 0.4091720481244273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19780811464793324, 0.19780811464793324, 0.27629196530553046], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.03530381], dtype=float32), -0.5791793]. 
=============================================
[2019-03-26 12:04:14,991] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1213761: loss 183.3746
[2019-03-26 12:04:14,994] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1213762: learning rate 0.0005
[2019-03-26 12:04:18,476] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1215373: loss 0.7096
[2019-03-26 12:04:18,478] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1215374: learning rate 0.0005
[2019-03-26 12:04:20,223] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1216197: loss 5.2291
[2019-03-26 12:04:20,225] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1216197: learning rate 0.0005
[2019-03-26 12:04:20,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7321632e-35], sum to 1.0000
[2019-03-26 12:04:20,911] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4194
[2019-03-26 12:04:20,921] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 82.66666666666667, 1.0, 2.0, 0.5468786600017321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764201.5231817621, 764201.5231817621, 191266.5657343108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5775600.0000, 
sim time next is 5776200.0000, 
raw observation next is [28.05, 83.0, 1.0, 2.0, 0.546436695655818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763583.7056478023, 763583.7056478023, 191191.241292817], 
processed observation next is [0.0, 0.8695652173913043, 0.528436018957346, 0.83, 1.0, 1.0, 0.4535381875371301, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2121065849021673, 0.2121065849021673, 0.28536006163107014], 
reward next is 0.7146, 
noisyNet noise sample is [array([1.2848278], dtype=float32), -1.0793166]. 
=============================================
[2019-03-26 12:04:21,443] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1216771: loss 2.2253
[2019-03-26 12:04:21,445] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1216771: learning rate 0.0005
[2019-03-26 12:04:24,197] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1218040: loss 2.5904
[2019-03-26 12:04:24,213] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1218045: learning rate 0.0005
[2019-03-26 12:04:24,911] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1218383: loss 4.7827
[2019-03-26 12:04:24,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1218384: learning rate 0.0005
[2019-03-26 12:04:25,153] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1218497: loss 5.0967
[2019-03-26 12:04:25,157] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1218497: learning rate 0.0005
[2019-03-26 12:04:25,402] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1218606: loss 2.4787
[2019-03-26 12:04:25,404] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1218608: learning rate 0.0005
[2019-03-26 12:04:25,413] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1218613: loss 3.6498
[2019-03-26 12:04:25,415] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1218613: learning rate 0.0005
[2019-03-26 12:04:25,490] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1218651: loss 4.4418
[2019-03-26 12:04:25,493] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1218651: learning rate 0.0005
[2019-03-26 12:04:25,982] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1218873: loss 1.1627
[2019-03-26 12:04:25,984] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1218874: learning rate 0.0005
[2019-03-26 12:04:26,183] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1218972: loss 4.0840
[2019-03-26 12:04:26,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1218972: learning rate 0.0005
[2019-03-26 12:04:26,229] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1218997: loss 1.6364
[2019-03-26 12:04:26,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1218997: learning rate 0.0005
[2019-03-26 12:04:26,254] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1219012: loss 0.8117
[2019-03-26 12:04:26,256] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1219013: learning rate 0.0005
[2019-03-26 12:04:26,732] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1865116e-32 1.0000000e+00 9.5456783e-37 9.8966155e-33 1.7013087e-17], sum to 1.0000
[2019-03-26 12:04:26,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1252
[2019-03-26 12:04:26,748] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 86.0, 1.0, 2.0, 0.5581219703799527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779918.55503559, 779918.55503559, 193203.6357014503], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5950800.0000, 
sim time next is 5951400.0000, 
raw observation next is [28.01666666666667, 86.5, 1.0, 2.0, 0.5603987752780217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783101.330594362, 783101.3305943614, 193600.1619550526], 
processed observation next is [1.0, 0.9130434782608695, 0.5268562401263824, 0.865, 1.0, 1.0, 0.470359970214484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21752814738732276, 0.21752814738732262, 0.28895546560455615], 
reward next is 0.7110, 
noisyNet noise sample is [array([-0.33201635], dtype=float32), -1.8599516]. 
=============================================
[2019-03-26 12:04:26,848] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1219383: loss 0.7602
[2019-03-26 12:04:26,849] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1219383: learning rate 0.0005
[2019-03-26 12:04:27,097] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1219550: loss 0.1358
[2019-03-26 12:04:27,098] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1219551: learning rate 0.0005
[2019-03-26 12:04:32,190] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1221908: loss 0.1592
[2019-03-26 12:04:32,193] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1221909: learning rate 0.0005
[2019-03-26 12:04:34,207] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.6761395e-26 7.7026032e-13 0.0000000e+00 2.7461780e-26 1.0000000e+00], sum to 1.0000
[2019-03-26 12:04:34,217] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0024
[2019-03-26 12:04:34,225] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.3, 69.0, 1.0, 2.0, 0.9394663536775776, 1.0, 2.0, 0.9394663536775776, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2627797.892909335, 2627797.892909335, 493486.9089390529], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6003000.0000, 
sim time next is 6003600.0000, 
raw observation next is [32.53333333333333, 68.33333333333333, 1.0, 2.0, 0.6093610429643973, 1.0, 2.0, 0.6093610429643973, 1.0, 1.0, 1.03, 6.942967458733204, 6.9112, 170.5573041426782, 2556609.0069443, 2533852.684036104, 491398.6305454973], 
processed observation next is [1.0, 0.4782608695652174, 0.7409162717219588, 0.6833333333333332, 1.0, 1.0, 0.5293506541739726, 1.0, 1.0, 0.5293506541739726, 1.0, 0.5, 1.0365853658536586, 0.0031767458733203747, 0.0, 0.8375144448122397, 0.7101691685956389, 0.7038479677878067, 0.7334307918589512], 
reward next is 0.1077, 
noisyNet noise sample is [array([-0.10575034], dtype=float32), 1.7626562]. 
=============================================
[2019-03-26 12:04:35,448] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1223384: loss 7.0814
[2019-03-26 12:04:35,452] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1223386: learning rate 0.0005
[2019-03-26 12:04:37,159] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1224151: loss 1.7493
[2019-03-26 12:04:37,161] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1224151: learning rate 0.0005
[2019-03-26 12:04:38,549] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1224774: loss 0.1261
[2019-03-26 12:04:38,552] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1224775: learning rate 0.0005
[2019-03-26 12:04:39,052] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 12:04:39,054] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:04:39,055] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:04:39,055] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:04:39,057] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:04:39,058] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:04:39,058] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:04:39,056] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:04:39,061] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:04:39,059] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:04:39,063] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:04:39,086] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run50
[2019-03-26 12:04:39,087] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run50
[2019-03-26 12:04:39,130] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run50
[2019-03-26 12:04:39,131] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run50
[2019-03-26 12:04:39,150] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run50
[2019-03-26 12:05:06,621] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.22650938], dtype=float32), 0.526348]
[2019-03-26 12:05:06,622] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.88749465, 96.162632505, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2209694118373594, 6.9112, 6.9112, 184.5923449428631, 571266.859608465, 571266.859608465, 244755.6696122698]
[2019-03-26 12:05:06,624] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:05:06,627] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2383016e-31 1.3123274e-09 9.5122045e-36 7.1745253e-35 1.0000000e+00], sampled 0.25744077469260307
[2019-03-26 12:05:09,123] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.22650938], dtype=float32), 0.526348]
[2019-03-26 12:05:09,124] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.23864985, 100.0, 1.0, 2.0, 0.1808769937606726, 1.0, 2.0, 0.1808769937606726, 1.0, 2.0, 0.3089593837880936, 6.9112, 6.9112, 184.5923449428631, 758240.7630997258, 758240.7630997258, 268635.9691820092]
[2019-03-26 12:05:09,126] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:05:09,130] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3690282e-32 6.4161654e-10 9.9899038e-37 5.0913997e-36 1.0000000e+00], sampled 0.527969561136394
[2019-03-26 12:05:09,215] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.22650938], dtype=float32), 0.526348]
[2019-03-26 12:05:09,217] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.03333333333333, 93.0, 1.0, 2.0, 0.3929371329103826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 588677.7308919433, 588677.7308919426, 173567.1918340437]
[2019-03-26 12:05:09,218] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:05:09,221] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.1464758e-34 1.0000000e+00 0.0000000e+00 1.3711624e-37 6.4725948e-14], sampled 0.5648532839212864
[2019-03-26 12:05:21,655] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.22650938], dtype=float32), 0.526348]
[2019-03-26 12:05:21,657] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.4981764430497135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767387.3877705503, 767387.3877705509, 191842.9506205099]
[2019-03-26 12:05:21,659] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:05:21,664] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.4771360e-34 1.0000000e+00 0.0000000e+00 1.1830624e-37 2.4146023e-15], sampled 0.1019988141192476
[2019-03-26 12:05:35,283] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.22650938], dtype=float32), 0.526348]
[2019-03-26 12:05:35,285] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.2378359, 69.20693557666667, 1.0, 2.0, 0.5676501889381842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 793238.2408287537, 793238.2408287544, 194874.249911772]
[2019-03-26 12:05:35,289] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:05:35,293] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.3966893e-30 1.0000000e+00 6.0437622e-37 7.5301049e-31 4.7933040e-09], sampled 0.7129299129392096
[2019-03-26 12:06:02,273] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.22650938], dtype=float32), 0.526348]
[2019-03-26 12:06:02,275] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.76666666666667, 77.0, 1.0, 2.0, 0.1783949646781263, 1.0, 2.0, 0.1783949646781263, 1.0, 2.0, 0.3054427192649314, 6.9112, 6.9112, 170.5573041426782, 747852.5077324318, 747852.5077324318, 264005.1561628345]
[2019-03-26 12:06:02,275] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:06:02,278] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.0799505e-26 5.3505552e-01 1.6658538e-31 1.3866807e-29 4.6494454e-01], sampled 0.29387565449195974
[2019-03-26 12:06:07,218] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.22650938], dtype=float32), 0.526348]
[2019-03-26 12:06:07,219] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.83333333333334, 70.16666666666667, 1.0, 2.0, 0.5916308368256952, 1.0, 2.0, 0.5916308368256952, 1.0, 2.0, 1.027467108258706, 6.9112, 6.9112, 170.5573041426782, 2482147.029342627, 2482147.029342627, 484292.5095659381]
[2019-03-26 12:06:07,220] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:06:07,222] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3301092e-34 3.8078601e-21 0.0000000e+00 2.5089278e-33 1.0000000e+00], sampled 0.10117561600720792
[2019-03-26 12:06:31,888] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.22650938], dtype=float32), 0.526348]
[2019-03-26 12:06:31,889] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.84092904, 81.386366855, 1.0, 2.0, 0.5230568752890936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745289.3261120552, 745289.3261120545, 189124.8235193951]
[2019-03-26 12:06:31,890] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:06:31,892] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.8694859e-30 9.9999964e-01 3.9465498e-35 8.6175682e-33 4.0286088e-07], sampled 0.5572331292012825
[2019-03-26 12:06:34,814] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7826.3332 3077277657.5754 104.0000
[2019-03-26 12:06:34,951] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7300.3543 3398098466.5033 590.0000
[2019-03-26 12:06:34,995] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7758.1430 3099221547.7581 118.0000
[2019-03-26 12:06:35,101] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7872.1595 3245031500.7912 48.0000
[2019-03-26 12:06:35,103] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7879.8411 3203510931.1376 190.0000
[2019-03-26 12:06:36,119] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1225000, evaluation results [1225000.0, 7300.354296514835, 3398098466.5033455, 590.0, 7879.841105443162, 3203510931.1376257, 190.0, 7826.33321012592, 3077277657.57543, 104.0, 7872.159488303221, 3245031500.7911577, 48.0, 7758.143020352899, 3099221547.7581015, 118.0]
[2019-03-26 12:06:38,324] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1226017: loss 1.4512
[2019-03-26 12:06:38,327] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1226017: learning rate 0.0005
[2019-03-26 12:06:38,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0972658e-33 7.2918435e-13 0.0000000e+00 5.7756592e-34 1.0000000e+00], sum to 1.0000
[2019-03-26 12:06:38,918] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3670
[2019-03-26 12:06:38,922] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.65, 85.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2439672462712397, 6.9112, 6.9112, 170.5573041426782, 620288.7435673563, 620288.7435673563, 248083.5158714059], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6918600.0000, 
sim time next is 6919200.0000, 
raw observation next is [24.6, 86.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2437107481618188, 6.9112, 6.9112, 170.5573041426782, 619667.0308285576, 619667.0308285576, 247993.3346141244], 
processed observation next is [0.0, 0.08695652173913043, 0.36492890995260674, 0.86, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07769603434368148, 0.0, 0.0, 0.8375144448122397, 0.17212973078571042, 0.17212973078571042, 0.3701393053942155], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4927162], dtype=float32), 0.8130462]. 
=============================================
[2019-03-26 12:06:39,082] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1226368: loss 1.1128
[2019-03-26 12:06:39,083] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1226368: learning rate 0.0005
[2019-03-26 12:06:39,404] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1226515: loss 0.9814
[2019-03-26 12:06:39,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1226516: learning rate 0.0005
[2019-03-26 12:06:39,629] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1226630: loss 0.2120
[2019-03-26 12:06:39,630] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1226630: learning rate 0.0005
[2019-03-26 12:06:39,707] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1226660: loss 2.3652
[2019-03-26 12:06:39,712] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1226661: learning rate 0.0005
[2019-03-26 12:06:39,789] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1226700: loss -0.0192
[2019-03-26 12:06:39,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1226700: learning rate 0.0005
[2019-03-26 12:06:40,102] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1226846: loss 0.1644
[2019-03-26 12:06:40,107] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1226847: learning rate 0.0005
[2019-03-26 12:06:40,330] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1226946: loss 0.3458
[2019-03-26 12:06:40,332] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1226947: loss 0.0410
[2019-03-26 12:06:40,332] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1226947: learning rate 0.0005
[2019-03-26 12:06:40,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1226947: learning rate 0.0005
[2019-03-26 12:06:40,468] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1227006: loss 0.6230
[2019-03-26 12:06:40,470] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1227006: learning rate 0.0005
[2019-03-26 12:06:40,543] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8933144e-30 9.9999964e-01 1.7426817e-34 2.0112805e-31 3.4988744e-07], sum to 1.0000
[2019-03-26 12:06:40,553] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8362
[2019-03-26 12:06:40,559] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 63.33333333333333, 1.0, 2.0, 0.4431919400985395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635224.6156589491, 635224.6156589491, 177227.8468569522], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6946800.0000, 
sim time next is 6947400.0000, 
raw observation next is [28.95, 62.66666666666667, 1.0, 2.0, 0.4456971908442084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 638240.6781133377, 638240.6781133377, 177516.0811146153], 
processed observation next is [0.0, 0.391304347826087, 0.5710900473933649, 0.6266666666666667, 1.0, 1.0, 0.3321652901737451, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17728907725370494, 0.17728907725370494, 0.2649493747979333], 
reward next is 0.7351, 
noisyNet noise sample is [array([-0.03532445], dtype=float32), -0.35639095]. 
=============================================
[2019-03-26 12:06:41,192] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1227339: loss 0.0229
[2019-03-26 12:06:41,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1227341: learning rate 0.0005
[2019-03-26 12:06:41,767] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1227610: loss 10.9678
[2019-03-26 12:06:41,770] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1227611: learning rate 0.0005
[2019-03-26 12:06:43,817] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2011130e-32 9.9999988e-01 0.0000000e+00 1.3020998e-35 7.7538154e-08], sum to 1.0000
[2019-03-26 12:06:43,827] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0950
[2019-03-26 12:06:43,831] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 90.5, 1.0, 2.0, 0.5208180888733931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727772.3258259722, 727772.325825973, 186922.3721718111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6226200.0000, 
sim time next is 6226800.0000, 
raw observation next is [26.43333333333333, 90.66666666666667, 1.0, 2.0, 0.521970848881068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729383.7039258432, 729383.7039258427, 187110.2387879301], 
processed observation next is [0.0, 0.043478260869565216, 0.4518167456556081, 0.9066666666666667, 1.0, 1.0, 0.42406126371213015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20260658442384535, 0.2026065844238452, 0.2792690131163136], 
reward next is 0.7207, 
noisyNet noise sample is [array([1.5331513], dtype=float32), 1.0928166]. 
=============================================
[2019-03-26 12:06:44,096] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9272251e-34 1.8440719e-16 2.7409283e-38 4.9548715e-32 1.0000000e+00], sum to 1.0000
[2019-03-26 12:06:44,106] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4515
[2019-03-26 12:06:44,111] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.58333333333333, 91.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2788307337554528, 6.911199999999999, 6.9112, 170.5573041426782, 692872.4512378315, 692872.4512378321, 258884.6688675374], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6655800.0000, 
sim time next is 6656400.0000, 
raw observation next is [25.5, 92.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2784576839526287, 6.911200000000001, 6.9112, 170.5573041426782, 692102.1155465323, 692102.1155465317, 258767.6489631411], 
processed observation next is [1.0, 0.043478260869565216, 0.40758293838862564, 0.92, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.12007034628369355, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19225058765181455, 0.19225058765181438, 0.3862203715867778], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4724467], dtype=float32), -0.8073624]. 
=============================================
[2019-03-26 12:06:46,836] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1229964: loss 0.8564
[2019-03-26 12:06:46,839] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1229965: learning rate 0.0005
[2019-03-26 12:06:49,867] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1231372: loss 0.1403
[2019-03-26 12:06:49,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1231372: learning rate 0.0005
[2019-03-26 12:06:51,553] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1232152: loss 5.8156
[2019-03-26 12:06:51,555] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1232153: learning rate 0.0005
[2019-03-26 12:06:53,001] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1232825: loss 7.5948
[2019-03-26 12:06:53,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1232825: learning rate 0.0005
[2019-03-26 12:06:55,628] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1234046: loss 4.6908
[2019-03-26 12:06:55,630] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1234047: learning rate 0.0005
[2019-03-26 12:06:56,240] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1234336: loss 2.4274
[2019-03-26 12:06:56,242] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1234336: learning rate 0.0005
[2019-03-26 12:06:56,505] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1234452: loss 2.5532
[2019-03-26 12:06:56,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1234453: learning rate 0.0005
[2019-03-26 12:06:56,784] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1234579: loss 1.3119
[2019-03-26 12:06:56,788] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1234580: learning rate 0.0005
[2019-03-26 12:06:56,817] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1234593: loss -81.9976
[2019-03-26 12:06:56,820] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1234596: learning rate 0.0005
[2019-03-26 12:06:57,120] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1234734: loss 1.5681
[2019-03-26 12:06:57,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1234734: learning rate 0.0005
[2019-03-26 12:06:57,448] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1234884: loss 6.5335
[2019-03-26 12:06:57,450] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1234884: learning rate 0.0005
[2019-03-26 12:06:57,620] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1234962: loss 9.6950
[2019-03-26 12:06:57,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1234962: learning rate 0.0005
[2019-03-26 12:06:57,734] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1235014: loss 8.9039
[2019-03-26 12:06:57,736] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1235014: learning rate 0.0005
[2019-03-26 12:06:57,794] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1235039: loss 58.3529
[2019-03-26 12:06:57,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1235041: learning rate 0.0005
[2019-03-26 12:06:58,533] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1235384: loss 41.8890
[2019-03-26 12:06:58,536] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1235384: learning rate 0.0005
[2019-03-26 12:06:58,601] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8093737e-35 1.0000000e+00 0.0000000e+00 2.7977648e-36 1.8708505e-21], sum to 1.0000
[2019-03-26 12:06:58,609] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7880
[2019-03-26 12:06:58,618] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 83.0, 1.0, 2.0, 0.5221058914024111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729572.4723983603, 729572.472398361, 187132.1495908636], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6472800.0000, 
sim time next is 6473400.0000, 
raw observation next is [27.45, 83.66666666666667, 1.0, 2.0, 0.5240114768023031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732236.1884234525, 732236.1884234525, 187443.7484477302], 
processed observation next is [1.0, 0.9565217391304348, 0.5, 0.8366666666666667, 1.0, 1.0, 0.4265198515690398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2033989412287368, 0.2033989412287368, 0.2797667887279555], 
reward next is 0.7202, 
noisyNet noise sample is [array([0.784532], dtype=float32), -0.92362344]. 
=============================================
[2019-03-26 12:06:59,050] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1235621: loss 0.0858
[2019-03-26 12:06:59,051] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1235621: learning rate 0.0005
[2019-03-26 12:06:59,337] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2534659e-30 1.0000000e+00 3.8695263e-34 7.7872533e-34 1.5505567e-11], sum to 1.0000
[2019-03-26 12:06:59,350] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3379
[2019-03-26 12:06:59,356] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 89.0, 1.0, 2.0, 0.7170657950208603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1002131.132092961, 1002131.132092961, 224570.4285035005], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6589800.0000, 
sim time next is 6590400.0000, 
raw observation next is [26.43333333333334, 88.66666666666667, 1.0, 2.0, 0.6891841234206213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 963147.5943829968, 963147.5943829975, 218526.509921939], 
processed observation next is [1.0, 0.2608695652173913, 0.4518167456556086, 0.8866666666666667, 1.0, 1.0, 0.6255230402658087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26754099843972134, 0.2675409984397215, 0.32615897003274474], 
reward next is 0.6738, 
noisyNet noise sample is [array([-0.05787879], dtype=float32), -0.40558586]. 
=============================================
[2019-03-26 12:07:04,169] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1237942: loss 3.1932
[2019-03-26 12:07:04,172] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1237943: learning rate 0.0005
[2019-03-26 12:07:05,310] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0523980e-37 2.3053975e-33 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 12:07:05,321] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9033
[2019-03-26 12:07:05,327] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.76666666666667, 70.66666666666667, 1.0, 2.0, 0.5307025271461663, 1.0, 2.0, 0.5307025271461663, 1.0, 2.0, 0.9149252660889565, 6.911200000000001, 6.9112, 170.5573041426782, 2226298.743834639, 2226298.743834639, 435726.0121474782], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6604800.0000, 
sim time next is 6605400.0000, 
raw observation next is [29.93333333333333, 69.83333333333333, 1.0, 2.0, 0.5285319698209487, 1.0, 2.0, 0.5285319698209487, 1.0, 2.0, 0.9113177620546682, 6.9112, 6.9112, 170.5573041426782, 2217185.172382691, 2217185.172382691, 434173.6978982059], 
processed observation next is [1.0, 0.43478260869565216, 0.6176935229067929, 0.6983333333333333, 1.0, 1.0, 0.4319662286999382, 1.0, 1.0, 0.4319662286999382, 1.0, 1.0, 0.8918509293349612, 0.0, 0.0, 0.8375144448122397, 0.615884770106303, 0.615884770106303, 0.648020444624188], 
reward next is 0.3520, 
noisyNet noise sample is [array([-0.18234155], dtype=float32), -0.72105885]. 
=============================================
[2019-03-26 12:07:06,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8067014e-36 2.0402461e-29 0.0000000e+00 8.1855858e-38 1.0000000e+00], sum to 1.0000
[2019-03-26 12:07:06,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8870
[2019-03-26 12:07:06,037] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.25, 68.5, 1.0, 2.0, 0.5219835261760745, 1.0, 2.0, 0.5219835261760745, 1.0, 2.0, 0.9008422752347023, 6.911200000000001, 6.9112, 170.5573041426782, 2189687.116513548, 2189687.116513547, 429605.7907672192], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6606600.0000, 
sim time next is 6607200.0000, 
raw observation next is [30.4, 68.0, 1.0, 2.0, 0.5273454312313843, 1.0, 2.0, 0.5273454312313843, 1.0, 2.0, 0.9107260718775769, 6.9112, 6.9112, 170.5573041426782, 2212203.188839014, 2212203.188839014, 433593.7563193419], 
processed observation next is [1.0, 0.4782608695652174, 0.6398104265402843, 0.68, 1.0, 1.0, 0.43053666413419794, 1.0, 1.0, 0.43053666413419794, 1.0, 1.0, 0.8911293559482646, 0.0, 0.0, 0.8375144448122397, 0.6145008857886151, 0.6145008857886151, 0.6471548601781222], 
reward next is 0.3528, 
noisyNet noise sample is [array([0.8930851], dtype=float32), -0.72336876]. 
=============================================
[2019-03-26 12:07:07,087] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1239303: loss 0.3499
[2019-03-26 12:07:07,090] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1239303: learning rate 0.0005
[2019-03-26 12:07:08,945] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1240165: loss 0.0506
[2019-03-26 12:07:08,946] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1240165: learning rate 0.0005
[2019-03-26 12:07:10,359] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1240830: loss 0.0708
[2019-03-26 12:07:10,364] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1240830: learning rate 0.0005
[2019-03-26 12:07:11,083] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8057965e-20 9.9496293e-01 1.7595955e-29 2.8890026e-20 5.0371387e-03], sum to 1.0000
[2019-03-26 12:07:11,094] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4543
[2019-03-26 12:07:11,103] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1981797.859270111 W.
[2019-03-26 12:07:11,108] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.13333333333334, 61.00000000000001, 1.0, 2.0, 0.4724721224157369, 1.0, 2.0, 0.4724721224157369, 1.0, 2.0, 0.7984073475882582, 6.9112, 6.9112, 170.5573041426782, 1981797.859270111, 1981797.859270111, 392808.7429944304], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6704400.0000, 
sim time next is 6705000.0000, 
raw observation next is [30.1, 61.5, 1.0, 2.0, 0.4834559432463699, 1.0, 2.0, 0.4834559432463699, 1.0, 2.0, 0.8177527143125989, 6.9112, 6.9112, 170.5573041426782, 2027913.416018545, 2027913.416018545, 400022.3440146448], 
processed observation next is [1.0, 0.6086956521739131, 0.6255924170616115, 0.615, 1.0, 1.0, 0.3776577629474337, 1.0, 1.0, 0.3776577629474337, 1.0, 1.0, 0.7777472125763402, 0.0, 0.0, 0.8375144448122397, 0.5633092822273736, 0.5633092822273736, 0.5970482746487236], 
reward next is 0.4030, 
noisyNet noise sample is [array([0.16248448], dtype=float32), -0.86778176]. 
=============================================
[2019-03-26 12:07:11,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[55.339558]
 [55.525257]
 [55.600464]
 [55.35762 ]
 [55.443394]], R is [[54.96860123]
 [54.83263397]
 [54.69845581]
 [54.58777618]
 [54.47231674]].
[2019-03-26 12:07:12,962] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1242036: loss 0.4488
[2019-03-26 12:07:12,965] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1242038: learning rate 0.0005
[2019-03-26 12:07:13,535] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1242302: loss 0.7142
[2019-03-26 12:07:13,539] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1242303: learning rate 0.0005
[2019-03-26 12:07:13,910] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1242475: loss 0.7813
[2019-03-26 12:07:13,914] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1242476: learning rate 0.0005
[2019-03-26 12:07:14,052] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1242540: loss 0.5625
[2019-03-26 12:07:14,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1242540: learning rate 0.0005
[2019-03-26 12:07:14,222] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1242622: loss 0.3520
[2019-03-26 12:07:14,224] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1242623: learning rate 0.0005
[2019-03-26 12:07:14,507] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1242762: loss 0.2935
[2019-03-26 12:07:14,510] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1242762: learning rate 0.0005
[2019-03-26 12:07:14,801] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1242894: loss 0.2925
[2019-03-26 12:07:14,807] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1242894: learning rate 0.0005
[2019-03-26 12:07:15,046] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1243005: loss 0.2522
[2019-03-26 12:07:15,049] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1243005: learning rate 0.0005
[2019-03-26 12:07:15,176] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1243066: loss 0.1758
[2019-03-26 12:07:15,179] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1243068: learning rate 0.0005
[2019-03-26 12:07:15,188] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1243070: loss 0.2770
[2019-03-26 12:07:15,190] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1243071: learning rate 0.0005
[2019-03-26 12:07:16,010] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1243403: loss 0.6852
[2019-03-26 12:07:16,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1243404: learning rate 0.0005
[2019-03-26 12:07:16,664] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.88618757e-24 9.99999881e-01 1.12818226e-33 6.23605084e-26
 1.11126965e-07], sum to 1.0000
[2019-03-26 12:07:16,672] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4776
[2019-03-26 12:07:16,679] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2135545.31953334 W.
[2019-03-26 12:07:16,685] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.1, 58.0, 1.0, 2.0, 0.5090899113840595, 1.0, 2.0, 0.5090899113840595, 1.0, 1.0, 0.8538933725171659, 6.9112, 6.9112, 170.5573041426782, 2135545.31953334, 2135545.31953334, 415766.1988314128], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7054800.0000, 
sim time next is 7055400.0000, 
raw observation next is [29.9, 59.5, 1.0, 2.0, 0.9123158671233714, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.95669940951797, 6.9112, 168.9126849947314, 2172275.444767532, 2139996.649572641, 438980.5518384888], 
processed observation next is [1.0, 0.6521739130434783, 0.6161137440758293, 0.595, 1.0, 1.0, 0.8943564664137005, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.004549940951797016, 0.0, 0.8294386118859172, 0.6034098457687589, 0.5944435137701781, 0.6551948534902817], 
reward next is 0.1173, 
noisyNet noise sample is [array([1.364367], dtype=float32), 0.90012544]. 
=============================================
[2019-03-26 12:07:16,735] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1243741: loss -170.2761
[2019-03-26 12:07:16,737] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1243742: learning rate 0.0005
[2019-03-26 12:07:20,834] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1245823: loss -297.1393
[2019-03-26 12:07:20,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1245824: learning rate 0.0005
[2019-03-26 12:07:24,304] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1247385: loss 0.0064
[2019-03-26 12:07:24,305] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1247385: learning rate 0.0005
[2019-03-26 12:07:25,911] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1248123: loss -33.3442
[2019-03-26 12:07:25,919] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1248123: learning rate 0.0005
[2019-03-26 12:07:27,473] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1248821: loss -372.9402
[2019-03-26 12:07:27,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1248822: learning rate 0.0005
[2019-03-26 12:07:30,049] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7803707e-35], sum to 1.0000
[2019-03-26 12:07:30,056] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3410
[2019-03-26 12:07:30,060] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 83.33333333333334, 1.0, 2.0, 0.5721651635853019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 816484.9814542426, 816484.9814542432, 197846.1502322542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7018800.0000, 
sim time next is 7019400.0000, 
raw observation next is [25.75, 82.16666666666666, 1.0, 2.0, 0.5667075972798076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809342.2881778523, 809342.2881778523, 196938.5464743145], 
processed observation next is [1.0, 0.21739130434782608, 0.41943127962085314, 0.8216666666666665, 1.0, 1.0, 0.4779609605780814, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22481730227162564, 0.22481730227162564, 0.29393812906614103], 
reward next is 0.7061, 
noisyNet noise sample is [array([0.3986662], dtype=float32), 0.37115386]. 
=============================================
[2019-03-26 12:07:30,077] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 12:07:30,080] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:07:30,080] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:07:30,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:07:30,082] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:07:30,082] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:07:30,083] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:07:30,081] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:07:30,089] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:07:30,090] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:07:30,091] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:07:30,108] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run51
[2019-03-26 12:07:30,131] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run51
[2019-03-26 12:07:30,153] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run51
[2019-03-26 12:07:30,173] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run51
[2019-03-26 12:07:30,174] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run51
[2019-03-26 12:07:56,591] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.30020183], dtype=float32), 0.52960134]
[2019-03-26 12:07:56,592] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.02785104, 92.50964955, 1.0, 2.0, 0.482513047247708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674229.2618920613, 674229.2618920613, 180901.3819144178]
[2019-03-26 12:07:56,593] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:07:56,597] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6769669611987911
[2019-03-26 12:08:18,555] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.30020183], dtype=float32), 0.52960134]
[2019-03-26 12:08:18,557] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.0, 61.0, 1.0, 2.0, 0.5254351752067659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 734226.3047218391, 734226.3047218386, 187676.1958793461]
[2019-03-26 12:08:18,557] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:08:18,564] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5950931298541412
[2019-03-26 12:08:20,371] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.30020183], dtype=float32), 0.52960134]
[2019-03-26 12:08:20,373] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.57036522666667, 91.15536106, 1.0, 2.0, 0.7706791529520609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1077096.167364322, 1077096.167364323, 236846.0993403135]
[2019-03-26 12:08:20,374] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:08:20,377] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2634702e-36], sampled 0.616168806370691
[2019-03-26 12:08:45,439] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.30020183], dtype=float32), 0.52960134]
[2019-03-26 12:08:45,441] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.56666666666666, 81.33333333333334, 1.0, 2.0, 0.5882750943207727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 822070.7774318729, 822070.7774318735, 198581.8185699073]
[2019-03-26 12:08:45,443] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:08:45,447] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.2781825e-30], sampled 0.12690244505181625
[2019-03-26 12:08:48,375] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.30020183], dtype=float32), 0.52960134]
[2019-03-26 12:08:48,376] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.65000000000001, 61.33333333333334, 1.0, 2.0, 1.004171548478356, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00599358407514, 6.9112, 168.9123159224519, 2300840.884791078, 2233591.308744047, 464589.7063816259]
[2019-03-26 12:08:48,380] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:08:48,385] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.09764166e-30 1.00000000e+00 8.75609417e-38 3.99969558e-24
 1.06224906e-19], sampled 0.1910419824409466
[2019-03-26 12:08:48,387] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2300840.884791078 W.
[2019-03-26 12:08:51,995] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.30020183], dtype=float32), 0.52960134]
[2019-03-26 12:08:51,997] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.4, 88.5, 1.0, 2.0, 0.5492782176054444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767555.847002081, 767555.847002081, 191676.9936162308]
[2019-03-26 12:08:51,998] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:08:52,001] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08596836404199881
[2019-03-26 12:09:07,436] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.30020183], dtype=float32), 0.52960134]
[2019-03-26 12:09:07,438] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.00678196, 96.91588770333333, 1.0, 2.0, 0.5149789924880682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719610.2203655146, 719610.2203655146, 185975.4464649556]
[2019-03-26 12:09:07,439] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:09:07,442] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5333430958585686
[2019-03-26 12:09:19,324] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.30020183], dtype=float32), 0.52960134]
[2019-03-26 12:09:19,326] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.15, 63.5, 1.0, 2.0, 0.3866301677153365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584237.1608334717, 584237.1608334717, 173310.7626909201]
[2019-03-26 12:09:19,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:09:19,330] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8814209165573204
[2019-03-26 12:09:25,812] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.9699 2842554311.7820 1131.0000
[2019-03-26 12:09:26,000] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8669.4875 2779531542.1701 915.0000
[2019-03-26 12:09:26,090] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.7741 3007708655.1678 1748.0000
[2019-03-26 12:09:26,189] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.6363 2927981067.4231 1333.0000
[2019-03-26 12:09:26,289] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7892.5887 3163741539.9879 1800.0000
[2019-03-26 12:09:27,305] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1250000, evaluation results [1250000.0, 7892.588724023961, 3163741539.9878573, 1800.0, 8260.636292026973, 2927981067.423145, 1333.0, 8669.487465159455, 2779531542.1700883, 915.0, 8001.774053024587, 3007708655.167793, 1748.0, 8498.969919322695, 2842554311.782033, 1131.0]
[2019-03-26 12:09:27,529] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1250103: loss -59.7203
[2019-03-26 12:09:27,534] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1250104: learning rate 0.0005
[2019-03-26 12:09:27,931] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1250287: loss -123.6303
[2019-03-26 12:09:27,934] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1250288: learning rate 0.0005
[2019-03-26 12:09:28,403] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1250504: loss 23.5040
[2019-03-26 12:09:28,405] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1250504: learning rate 0.0005
[2019-03-26 12:09:28,544] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1250569: loss 2.9487
[2019-03-26 12:09:28,547] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1250569: learning rate 0.0005
[2019-03-26 12:09:28,659] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1250622: loss 0.4855
[2019-03-26 12:09:28,662] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1250623: learning rate 0.0005
[2019-03-26 12:09:28,934] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1250747: loss -194.8720
[2019-03-26 12:09:28,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1250747: learning rate 0.0005
[2019-03-26 12:09:29,199] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1250867: loss -187.4717
[2019-03-26 12:09:29,201] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1250871: learning rate 0.0005
[2019-03-26 12:09:29,444] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1250986: loss -61.0375
[2019-03-26 12:09:29,447] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1250986: learning rate 0.0005
[2019-03-26 12:09:29,467] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1250994: loss 27.3995
[2019-03-26 12:09:29,473] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1250995: learning rate 0.0005
[2019-03-26 12:09:29,505] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1251017: loss -101.8034
[2019-03-26 12:09:29,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1251017: learning rate 0.0005
[2019-03-26 12:09:30,337] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1251408: loss -163.4203
[2019-03-26 12:09:30,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1251409: learning rate 0.0005
[2019-03-26 12:09:31,092] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1251751: loss 0.0235
[2019-03-26 12:09:31,094] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1251753: learning rate 0.0005
[2019-03-26 12:09:36,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:09:36,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:09:36,929] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-03-26 12:09:38,064] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1255039: loss 89.7670
[2019-03-26 12:09:38,066] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1255040: learning rate 0.0005
[2019-03-26 12:09:39,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1188368e-35 1.0000000e+00 0.0000000e+00 1.3572807e-33 4.7135903e-14], sum to 1.0000
[2019-03-26 12:09:39,258] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9494
[2019-03-26 12:09:39,265] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 86.0, 1.0, 2.0, 0.8779474416936573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1302279.63198307, 1302279.63198307, 274652.6092441225], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7228800.0000, 
sim time next is 7229400.0000, 
raw observation next is [24.23333333333333, 85.16666666666667, 1.0, 2.0, 0.851632616314867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1266186.331566902, 1266186.331566902, 267617.892755491], 
processed observation next is [1.0, 0.6956521739130435, 0.3475513428120062, 0.8516666666666667, 1.0, 1.0, 0.8212441160420084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35171842543525056, 0.35171842543525056, 0.39942969067983736], 
reward next is 0.6006, 
noisyNet noise sample is [array([-0.09353525], dtype=float32), 0.18935134]. 
=============================================
[2019-03-26 12:09:40,110] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1256000: loss 0.2568
[2019-03-26 12:09:40,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1256000: learning rate 0.0005
[2019-03-26 12:09:41,012] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:09:41,019] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0688
[2019-03-26 12:09:41,023] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666667, 92.0, 1.0, 2.0, 0.3352619262176988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542173.378754787, 542173.3787547864, 170283.4793762723], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7373400.0000, 
sim time next is 7374000.0000, 
raw observation next is [20.23333333333333, 92.0, 1.0, 2.0, 0.4580419886240947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739880.830377802, 739880.830377802, 188125.9636183687], 
processed observation next is [1.0, 0.34782608695652173, 0.15797788309636643, 0.92, 1.0, 1.0, 0.34703854051095745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20552245288272278, 0.20552245288272278, 0.28078502032592345], 
reward next is 0.7192, 
noisyNet noise sample is [array([-1.3106968], dtype=float32), -0.8612887]. 
=============================================
[2019-03-26 12:09:41,031] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.959435]
 [74.09811 ]
 [73.94591 ]
 [73.933914]
 [73.91618 ]], R is [[72.96703339]
 [72.9832077 ]
 [73.0042572 ]
 [73.02256012]
 [73.04016876]].
[2019-03-26 12:09:41,200] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:09:41,204] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5394
[2019-03-26 12:09:41,206] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 89.66666666666667, 1.0, 2.0, 0.3310579444283711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522246.5324323068, 522246.5324323062, 168702.9998408542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7270800.0000, 
sim time next is 7271400.0000, 
raw observation next is [21.61666666666667, 89.83333333333333, 1.0, 2.0, 0.3282927001426869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517834.9143904585, 517834.9143904591, 168359.0085167426], 
processed observation next is [1.0, 0.13043478260869565, 0.22353870458135885, 0.8983333333333333, 1.0, 1.0, 0.1907140965574541, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14384303177512736, 0.14384303177512753, 0.2512821022637949], 
reward next is 0.7487, 
noisyNet noise sample is [array([0.01115635], dtype=float32), 0.05244299]. 
=============================================
[2019-03-26 12:09:41,208] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1256583: loss 0.1241
[2019-03-26 12:09:41,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1256584: learning rate 0.0005
[2019-03-26 12:09:43,970] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1257938: loss 0.4731
[2019-03-26 12:09:43,972] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1257938: learning rate 0.0005
[2019-03-26 12:09:44,547] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1258215: loss 0.6404
[2019-03-26 12:09:44,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1258216: learning rate 0.0005
[2019-03-26 12:09:44,999] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1258434: loss 0.1250
[2019-03-26 12:09:45,002] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1258435: learning rate 0.0005
[2019-03-26 12:09:45,109] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1258497: loss 0.1793
[2019-03-26 12:09:45,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1258498: learning rate 0.0005
[2019-03-26 12:09:45,121] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1258506: loss 0.0762
[2019-03-26 12:09:45,125] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1258506: learning rate 0.0005
[2019-03-26 12:09:45,290] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:09:45,297] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6081
[2019-03-26 12:09:45,298] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:09:45,305] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 91.33333333333334, 1.0, 2.0, 0.3249963478896877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 518082.6856964402, 518082.6856964395, 168450.9846246839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7370400.0000, 
sim time next is 7371000.0000, 
raw observation next is [20.7, 91.5, 1.0, 2.0, 0.3245143444982701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519267.1339732052, 519267.1339732046, 168551.2841020618], 
processed observation next is [1.0, 0.30434782608695654, 0.18009478672985785, 0.915, 1.0, 1.0, 0.18616186084128927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14424087054811255, 0.14424087054811238, 0.251569080749346], 
reward next is 0.7484, 
noisyNet noise sample is [array([0.668191], dtype=float32), 0.07286836]. 
=============================================
[2019-03-26 12:09:45,308] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5892
[2019-03-26 12:09:45,314] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.01666666666667, 84.0, 1.0, 2.0, 0.437610632479189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660008.927405107, 660008.9274051064, 180420.9252212617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7359000.0000, 
sim time next is 7359600.0000, 
raw observation next is [23.93333333333334, 85.0, 1.0, 2.0, 0.4228847626078269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 636638.8618278255, 636638.8618278255, 178114.0312853055], 
processed observation next is [1.0, 0.17391304347826086, 0.3333333333333337, 0.85, 1.0, 1.0, 0.3046804368768999, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1768441282855071, 0.1768441282855071, 0.26584183773926195], 
reward next is 0.7342, 
noisyNet noise sample is [array([0.21924336], dtype=float32), -0.7168483]. 
=============================================
[2019-03-26 12:09:45,317] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.81985 ]
 [70.806496]
 [70.768745]
 [70.67849 ]
 [70.52128 ]], R is [[70.896698  ]
 [70.93630981]
 [70.97445679]
 [71.01364899]
 [71.05241394]].
[2019-03-26 12:09:45,442] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1258687: loss 0.1538
[2019-03-26 12:09:45,449] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1258688: learning rate 0.0005
[2019-03-26 12:09:45,699] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1258840: loss 0.0888
[2019-03-26 12:09:45,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1258842: learning rate 0.0005
[2019-03-26 12:09:45,845] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1258929: loss 0.0097
[2019-03-26 12:09:45,847] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1258929: learning rate 0.0005
[2019-03-26 12:09:45,851] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1258931: loss 0.0146
[2019-03-26 12:09:45,853] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1258932: learning rate 0.0005
[2019-03-26 12:09:45,888] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1258957: loss 0.0083
[2019-03-26 12:09:45,890] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1258957: learning rate 0.0005
[2019-03-26 12:09:46,826] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1259406: loss 0.1286
[2019-03-26 12:09:46,828] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1259406: learning rate 0.0005
[2019-03-26 12:09:47,108] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1259540: loss -46.5416
[2019-03-26 12:09:47,112] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1259541: learning rate 0.0005
[2019-03-26 12:09:51,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:09:51,315] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5523
[2019-03-26 12:09:51,321] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.58333333333334, 95.0, 1.0, 2.0, 0.3355813437204231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 521353.1412142153, 521353.141214216, 168444.106806597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7458600.0000, 
sim time next is 7459200.0000, 
raw observation next is [21.6, 95.0, 1.0, 2.0, 0.3359611040407641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 521705.286226209, 521705.2862262096, 168465.0910193421], 
processed observation next is [0.0, 0.34782608695652173, 0.22274881516587688, 0.95, 1.0, 1.0, 0.19995313739851098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14491813506283585, 0.144918135062836, 0.251440434357227], 
reward next is 0.7486, 
noisyNet noise sample is [array([0.13832779], dtype=float32), 0.38882923]. 
=============================================
[2019-03-26 12:09:53,195] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:09:53,201] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5032
[2019-03-26 12:09:53,204] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.48333333333333, 75.5, 1.0, 2.0, 0.4253355507484468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614370.1557356397, 614370.1557356397, 175311.0352889171], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7492200.0000, 
sim time next is 7492800.0000, 
raw observation next is [26.36666666666667, 76.0, 1.0, 2.0, 0.4240355883393812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 613336.4816620288, 613336.4816620282, 175236.5273522121], 
processed observation next is [0.0, 0.7391304347826086, 0.4486571879936811, 0.76, 1.0, 1.0, 0.30606697390286897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17037124490611913, 0.17037124490611896, 0.2615470557495703], 
reward next is 0.7385, 
noisyNet noise sample is [array([0.08136508], dtype=float32), -0.30472398]. 
=============================================
[2019-03-26 12:09:56,046] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:09:56,047] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:09:56,054] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:09:56,066] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0312
[2019-03-26 12:09:56,069] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 90.0, 1.0, 2.0, 0.3903633188034258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 581048.563585924, 581048.5635859234, 172756.612261902], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7543200.0000, 
sim time next is 7543800.0000, 
raw observation next is [23.65, 90.0, 1.0, 2.0, 0.3927705050899581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 583607.899235491, 583607.8992354916, 172956.4466228047], 
processed observation next is [0.0, 0.30434782608695654, 0.31990521327014215, 0.9, 1.0, 1.0, 0.268398198903564, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16211330534319193, 0.16211330534319213, 0.2581439501832906], 
reward next is 0.7419, 
noisyNet noise sample is [array([-0.92599213], dtype=float32), 0.0010755428]. 
=============================================
[2019-03-26 12:09:56,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-03-26 12:09:56,247] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1263931: loss -240.4466
[2019-03-26 12:09:56,251] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1263932: learning rate 0.0005
[2019-03-26 12:09:56,272] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:09:56,283] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5959
[2019-03-26 12:09:56,292] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 88.0, 1.0, 2.0, 0.4077110617057987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 598537.6821633442, 598537.6821633449, 174110.5122894122], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7546800.0000, 
sim time next is 7547400.0000, 
raw observation next is [24.5, 87.0, 1.0, 2.0, 0.4119874013504434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 602609.851093716, 602609.851093716, 174422.9458410385], 
processed observation next is [0.0, 0.34782608695652173, 0.3601895734597157, 0.87, 1.0, 1.0, 0.29155108596438967, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16739162530381, 0.16739162530381, 0.26033275498662467], 
reward next is 0.7397, 
noisyNet noise sample is [array([-1.9605896], dtype=float32), -1.004703]. 
=============================================
[2019-03-26 12:09:57,162] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1264466: loss -78.3042
[2019-03-26 12:09:57,163] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1264466: learning rate 0.0005
[2019-03-26 12:09:59,745] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1265830: loss 0.3421
[2019-03-26 12:09:59,746] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1265831: learning rate 0.0005
[2019-03-26 12:09:59,867] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2917668e-22 2.6287697e-02 4.5241675e-35 1.4479784e-24 9.7371227e-01], sum to 1.0000
[2019-03-26 12:09:59,873] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3259
[2019-03-26 12:09:59,877] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.16666666666667, 64.66666666666667, 1.0, 2.0, 0.5487371909967288, 1.0, 2.0, 0.5487371909967288, 1.0, 2.0, 0.9499489862730652, 6.911200000000001, 6.9112, 170.5573041426782, 2302023.892603778, 2302023.892603777, 449879.590359756], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7729800.0000, 
sim time next is 7730400.0000, 
raw observation next is [31.23333333333334, 64.33333333333334, 1.0, 2.0, 0.5371877938583889, 1.0, 2.0, 0.5371877938583889, 1.0, 2.0, 0.9300506990347892, 6.9112, 6.9112, 170.5573041426782, 2253528.985698762, 2253528.985698762, 441254.7187166954], 
processed observation next is [1.0, 0.4782608695652174, 0.6793048973143764, 0.6433333333333334, 1.0, 1.0, 0.44239493235950467, 1.0, 1.0, 0.44239493235950467, 1.0, 1.0, 0.9146959744326698, 0.0, 0.0, 0.8375144448122397, 0.6259802738052116, 0.6259802738052116, 0.6585891324129782], 
reward next is 0.3414, 
noisyNet noise sample is [array([0.45097616], dtype=float32), -1.0225543]. 
=============================================
[2019-03-26 12:10:00,067] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1266034: loss 1.5809
[2019-03-26 12:10:00,070] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1266035: learning rate 0.0005
[2019-03-26 12:10:00,670] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1266341: loss 2.7987
[2019-03-26 12:10:00,672] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1266343: learning rate 0.0005
[2019-03-26 12:10:00,773] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1266393: loss 2.7342
[2019-03-26 12:10:00,776] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1266394: learning rate 0.0005
[2019-03-26 12:10:00,812] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1266410: loss 5.5221
[2019-03-26 12:10:00,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1266410: learning rate 0.0005
[2019-03-26 12:10:00,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3594822e-24 1.9256094e-02 6.9884197e-32 3.9564517e-21 9.8074394e-01], sum to 1.0000
[2019-03-26 12:10:00,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1260
[2019-03-26 12:10:00,844] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.7, 61.66666666666667, 1.0, 2.0, 0.5263492517720018, 1.0, 2.0, 0.5263492517720018, 1.0, 2.0, 0.8987734647558604, 6.9112, 6.9112, 170.5573041426782, 2208019.929255093, 2208019.929255093, 430887.2013114919], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7748400.0000, 
sim time next is 7749000.0000, 
raw observation next is [30.6, 62.0, 1.0, 2.0, 0.5143961468329438, 1.0, 2.0, 0.5143961468329438, 1.0, 2.0, 0.8776225051847227, 6.9112, 6.9112, 170.5573041426782, 2157826.491609579, 2157826.491609579, 422284.3290103635], 
processed observation next is [1.0, 0.6956521739130435, 0.6492890995260664, 0.62, 1.0, 1.0, 0.41493511666619726, 1.0, 1.0, 0.41493511666619726, 1.0, 1.0, 0.8507591526642959, 0.0, 0.0, 0.8375144448122397, 0.5993962476693275, 0.5993962476693275, 0.6302751179259156], 
reward next is 0.3697, 
noisyNet noise sample is [array([-0.91899663], dtype=float32), -0.42572376]. 
=============================================
[2019-03-26 12:10:00,859] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[60.708645]
 [59.823044]
 [59.71228 ]
 [59.243195]
 [58.710926]], R is [[60.88543701]
 [60.63346863]
 [60.34464264]
 [60.06642914]
 [59.7849884 ]].
[2019-03-26 12:10:01,060] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1266519: loss 4.7118
[2019-03-26 12:10:01,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1266520: learning rate 0.0005
[2019-03-26 12:10:01,314] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1266637: loss 0.8249
[2019-03-26 12:10:01,317] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1266637: learning rate 0.0005
[2019-03-26 12:10:01,635] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1266791: loss -147.0557
[2019-03-26 12:10:01,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1266791: learning rate 0.0005
[2019-03-26 12:10:01,663] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1266804: loss 42.2164
[2019-03-26 12:10:01,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1266804: learning rate 0.0005
[2019-03-26 12:10:01,775] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1266870: loss -45.9191
[2019-03-26 12:10:01,778] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1266873: learning rate 0.0005
[2019-03-26 12:10:02,487] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1267292: loss 7.6699
[2019-03-26 12:10:02,490] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1267293: learning rate 0.0005
[2019-03-26 12:10:02,946] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9205962e-33 1.0000000e+00 1.8865228e-37 1.0080531e-33 4.7350677e-16], sum to 1.0000
[2019-03-26 12:10:02,955] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1167
[2019-03-26 12:10:02,959] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 89.33333333333334, 1.0, 2.0, 0.5249685783710689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733574.0720935244, 733574.0720935238, 187600.6626257591], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7769400.0000, 
sim time next is 7770000.0000, 
raw observation next is [26.6, 89.66666666666667, 1.0, 2.0, 0.5250500867856346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733688.0086441493, 733688.0086441486, 187614.0235659747], 
processed observation next is [1.0, 0.9565217391304348, 0.4597156398104266, 0.8966666666666667, 1.0, 1.0, 0.4277711888983549, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2038022246233748, 0.2038022246233746, 0.2800209306954846], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.8530781], dtype=float32), 1.3708566]. 
=============================================
[2019-03-26 12:10:02,968] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.30135]
 [70.3358 ]
 [70.3711 ]
 [70.58735]
 [70.74811]], R is [[70.30998993]
 [70.32688904]
 [70.34347534]
 [70.36000824]
 [70.3765564 ]].
[2019-03-26 12:10:03,067] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.5521494e-20], sum to 1.0000
[2019-03-26 12:10:03,078] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9024
[2019-03-26 12:10:03,085] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.53333333333333, 88.0, 1.0, 2.0, 0.4831288606916362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675090.0290286097, 675090.0290286097, 180994.4590735345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7685400.0000, 
sim time next is 7686000.0000, 
raw observation next is [25.5, 88.0, 1.0, 2.0, 0.4802845801489442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 671114.3775500917, 671114.3775500911, 180564.3190882282], 
processed observation next is [1.0, 1.0, 0.40758293838862564, 0.88, 1.0, 1.0, 0.3738368435529449, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18642066043058103, 0.18642066043058086, 0.2694989837137734], 
reward next is 0.7305, 
noisyNet noise sample is [array([0.11588801], dtype=float32), -1.2993308]. 
=============================================
[2019-03-26 12:10:03,099] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.26226 ]
 [70.24467 ]
 [70.234314]
 [70.2341  ]
 [70.22664 ]], R is [[70.39345551]
 [70.41938019]
 [70.44454956]
 [70.4694519 ]
 [70.49409485]].
[2019-03-26 12:10:04,067] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:04,068] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:04,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-03-26 12:10:04,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6265573e-37 1.0000000e+00 0.0000000e+00 2.2276960e-38 1.2536581e-24], sum to 1.0000
[2019-03-26 12:10:04,447] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9588
[2019-03-26 12:10:04,450] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 96.0, 1.0, 2.0, 0.3819986570044325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 574951.4300596697, 574951.4300596697, 172411.9948171903], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 150000.0000, 
sim time next is 150600.0000, 
raw observation next is [22.5, 96.0, 1.0, 2.0, 0.3810339826397562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 573499.0209116923, 573499.0209116916, 172282.9621455444], 
processed observation next is [1.0, 0.7391304347826086, 0.2654028436018958, 0.96, 1.0, 1.0, 0.2542578104093448, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1593052835865812, 0.159305283586581, 0.2571387494709618], 
reward next is 0.7429, 
noisyNet noise sample is [array([-0.7208871], dtype=float32), 0.21562369]. 
=============================================
[2019-03-26 12:10:06,159] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6408890e-32 1.0000000e+00 2.0353714e-37 9.2413240e-32 1.6547945e-16], sum to 1.0000
[2019-03-26 12:10:06,165] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2643
[2019-03-26 12:10:06,167] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 88.0, 1.0, 2.0, 0.5101332702930573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712836.7378048967, 712836.737804896, 185198.9423960287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7777800.0000, 
sim time next is 7778400.0000, 
raw observation next is [26.4, 87.66666666666667, 1.0, 2.0, 0.5083499806877253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710344.0178609152, 710344.0178609157, 184914.6543833675], 
processed observation next is [1.0, 0.0, 0.45023696682464454, 0.8766666666666667, 1.0, 1.0, 0.40765057914183767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1973177827391431, 0.19731778273914327, 0.2759920214677127], 
reward next is 0.7240, 
noisyNet noise sample is [array([1.2885844], dtype=float32), 0.7163299]. 
=============================================
[2019-03-26 12:10:07,317] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5128964e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5467335e-31], sum to 1.0000
[2019-03-26 12:10:07,325] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2748
[2019-03-26 12:10:07,331] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 83.5, 1.0, 2.0, 0.7140558042943365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 997922.5606028958, 997922.5606028958, 223907.2392876085], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7803000.0000, 
sim time next is 7803600.0000, 
raw observation next is [27.43333333333333, 83.0, 1.0, 2.0, 0.7642823916688315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1068151.599059242, 1068151.599059242, 235331.2969281832], 
processed observation next is [1.0, 0.30434782608695654, 0.49921011058451803, 0.83, 1.0, 1.0, 0.7160028815287126, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29670877751645613, 0.29670877751645613, 0.35124074168385555], 
reward next is 0.6488, 
noisyNet noise sample is [array([-0.15894714], dtype=float32), -1.6201005]. 
=============================================
[2019-03-26 12:10:09,896] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3737801e-37 1.0000000e+00 0.0000000e+00 7.0623549e-34 2.1312095e-25], sum to 1.0000
[2019-03-26 12:10:09,901] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0181
[2019-03-26 12:10:09,904] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 84.33333333333334, 1.0, 2.0, 0.5362200177786424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749302.0141029314, 749302.0141029321, 189466.2023328312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7845600.0000, 
sim time next is 7846200.0000, 
raw observation next is [27.75, 85.5, 1.0, 2.0, 0.5388911077864368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753035.8593542437, 753035.8593542431, 189914.3213791161], 
processed observation next is [1.0, 0.8260869565217391, 0.514218009478673, 0.855, 1.0, 1.0, 0.444447117814984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20917662759840103, 0.2091766275984009, 0.2834542110136061], 
reward next is 0.7165, 
noisyNet noise sample is [array([-0.7832688], dtype=float32), 0.10141438]. 
=============================================
[2019-03-26 12:10:10,067] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0744502e-34 2.3130749e-14 0.0000000e+00 7.9394920e-31 1.0000000e+00], sum to 1.0000
[2019-03-26 12:10:10,076] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0717
[2019-03-26 12:10:10,083] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.08333333333333, 68.83333333333333, 1.0, 2.0, 0.5159298452619827, 1.0, 2.0, 0.5159298452619827, 1.0, 2.0, 0.8890940048727668, 6.9112, 6.9112, 170.5573041426782, 2164266.661308516, 2164266.661308516, 425042.7144975822], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7836600.0000, 
sim time next is 7837200.0000, 
raw observation next is [29.9, 70.0, 1.0, 2.0, 0.4855830641011024, 1.0, 2.0, 0.4855830641011024, 1.0, 2.0, 0.8368164478296767, 6.911200000000001, 6.9112, 170.5573041426782, 2036844.359467261, 2036844.359467261, 404185.046972896], 
processed observation next is [1.0, 0.7391304347826086, 0.6161137440758293, 0.7, 1.0, 1.0, 0.38022055915795466, 1.0, 1.0, 0.38022055915795466, 1.0, 1.0, 0.8009956680849716, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5657900998520169, 0.5657900998520169, 0.6032612641386507], 
reward next is 0.3967, 
noisyNet noise sample is [array([-0.96391094], dtype=float32), -0.36969432]. 
=============================================
[2019-03-26 12:10:11,325] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:11,325] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:11,400] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-03-26 12:10:11,507] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4641977e-34 1.0000000e+00 7.2320018e-37 8.5461020e-34 4.4894919e-15], sum to 1.0000
[2019-03-26 12:10:11,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2444
[2019-03-26 12:10:11,521] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.83333333333333, 1.0, 2.0, 0.5804288207173234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 953424.9344160293, 953424.9344160286, 211471.7032750262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 661800.0000, 
sim time next is 662400.0000, 
raw observation next is [24.7, 54.0, 1.0, 2.0, 0.5500446819532137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903196.7534990396, 903196.7534990396, 205302.8673187337], 
processed observation next is [1.0, 0.6956521739130435, 0.3696682464454976, 0.54, 1.0, 1.0, 0.45788515897977544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25088798708306653, 0.25088798708306653, 0.3064221900279608], 
reward next is 0.6936, 
noisyNet noise sample is [array([0.16275795], dtype=float32), -0.6651816]. 
=============================================
[2019-03-26 12:10:12,649] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:12,649] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:12,705] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2454253e-22 1.0933598e-01 3.6767021e-31 1.3026760e-27 8.9066404e-01], sum to 1.0000
[2019-03-26 12:10:12,718] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3953
[2019-03-26 12:10:12,724] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.46666666666667, 72.66666666666666, 1.0, 2.0, 0.4232437622881275, 1.0, 2.0, 0.4232437622881275, 1.0, 1.0, 0.7282555711737067, 6.9112, 6.9112, 170.5573041426782, 1775137.037430194, 1775137.037430194, 365113.8605830126], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7901400.0000, 
sim time next is 7902000.0000, 
raw observation next is [29.6, 72.0, 1.0, 2.0, 0.4271227830070348, 1.0, 2.0, 0.4271227830070348, 1.0, 2.0, 0.7354995450023859, 6.9112, 6.9112, 170.5573041426782, 1791419.732895219, 1791419.732895219, 367462.5660803008], 
processed observation next is [1.0, 0.4782608695652174, 0.6018957345971565, 0.72, 1.0, 1.0, 0.3097864855506444, 1.0, 1.0, 0.3097864855506444, 1.0, 1.0, 0.6774384695151047, 0.0, 0.0, 0.8375144448122397, 0.4976165924708942, 0.4976165924708942, 0.548451591164628], 
reward next is 0.4515, 
noisyNet noise sample is [array([-0.9884009], dtype=float32), -0.7918468]. 
=============================================
[2019-03-26 12:10:12,728] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-03-26 12:10:12,775] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[61.832344]
 [61.854584]
 [62.531776]
 [62.76777 ]
 [64.09862 ]], R is [[62.06838608]
 [61.44770432]
 [61.3110733 ]
 [60.69796371]
 [60.12883759]].
[2019-03-26 12:10:14,696] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:14,697] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:14,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-03-26 12:10:15,322] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1558557e-36 1.0000000e+00 0.0000000e+00 6.8836821e-37 8.7272116e-24], sum to 1.0000
[2019-03-26 12:10:15,324] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4540
[2019-03-26 12:10:15,328] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.63333333333333, 90.0, 1.0, 2.0, 0.528340222514659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738287.1356138209, 738287.1356138209, 188155.5502397603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7944000.0000, 
sim time next is 7944600.0000, 
raw observation next is [26.56666666666667, 90.5, 1.0, 2.0, 0.527304807204318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736839.7747948361, 736839.7747948361, 187984.8601255989], 
processed observation next is [1.0, 0.9565217391304348, 0.45813586097946307, 0.905, 1.0, 1.0, 0.43048771952327475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2046777152207878, 0.2046777152207878, 0.28057441809790884], 
reward next is 0.7194, 
noisyNet noise sample is [array([-1.0046067], dtype=float32), 0.0018451075]. 
=============================================
[2019-03-26 12:10:15,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:15,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:15,516] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-03-26 12:10:15,592] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:10:15,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5770
[2019-03-26 12:10:15,601] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 82.50000000000001, 1.0, 2.0, 0.2573163386722432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 422445.1565407084, 422445.1565407078, 161640.7203193426], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 767400.0000, 
sim time next is 768000.0000, 
raw observation next is [20.26666666666667, 83.0, 1.0, 2.0, 0.2571927438817255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 422256.5592356881, 422256.5592356881, 161628.2295664809], 
processed observation next is [1.0, 0.9130434782608695, 0.15955766192733034, 0.83, 1.0, 1.0, 0.10505149865268135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11729348867658003, 0.11729348867658003, 0.24123616353206107], 
reward next is 0.7588, 
noisyNet noise sample is [array([1.0781363], dtype=float32), -0.882704]. 
=============================================
[2019-03-26 12:10:15,609] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.52236 ]
 [78.349495]
 [78.0471  ]
 [77.83458 ]
 [77.65271 ]], R is [[78.72777557]
 [78.69924164]
 [78.67107391]
 [78.64334106]
 [78.6159668 ]].
[2019-03-26 12:10:15,889] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:15,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:15,919] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:15,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:15,946] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-03-26 12:10:15,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-03-26 12:10:16,011] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:16,012] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,072] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:16,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-03-26 12:10:16,156] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-03-26 12:10:16,174] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:16,174] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-03-26 12:10:16,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:16,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,305] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-03-26 12:10:16,328] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:16,328] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,346] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-03-26 12:10:16,437] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:16,437] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,443] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-03-26 12:10:16,650] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:10:16,650] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,655] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-03-26 12:10:16,884] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3773615e-26 3.4196151e-03 1.8018618e-32 5.1746620e-34 9.9658042e-01], sum to 1.0000
[2019-03-26 12:10:16,884] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4920
[2019-03-26 12:10:16,890] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 64.33333333333334, 1.0, 2.0, 0.3075450785735866, 1.0, 2.0, 0.3075450785735866, 1.0, 2.0, 0.5300544249798183, 6.9112, 6.9112, 170.5573041426782, 1356893.742618211, 1356893.742618211, 313356.6188896297], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 44400.0000, 
sim time next is 45000.0000, 
raw observation next is [27.1, 64.0, 1.0, 2.0, 0.2984190092570368, 1.0, 2.0, 0.2984190092570368, 1.0, 2.0, 0.514073620595221, 6.9112, 6.9112, 170.5573041426782, 1315648.199977014, 1315648.199977014, 309342.751483112], 
processed observation next is [1.0, 0.5217391304347826, 0.4834123222748816, 0.64, 1.0, 1.0, 0.15472169790004434, 1.0, 1.0, 0.15472169790004434, 1.0, 1.0, 0.4074068543844158, 0.0, 0.0, 0.8375144448122397, 0.3654578333269483, 0.3654578333269483, 0.4617055992285254], 
reward next is 0.5383, 
noisyNet noise sample is [array([-1.0991356], dtype=float32), -0.5471608]. 
=============================================
[2019-03-26 12:10:16,893] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 12:10:16,893] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:10:16,895] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:10:16,898] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,898] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:10:16,899] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,900] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,899] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:10:16,904] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run52
[2019-03-26 12:10:16,905] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,904] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:10:16,925] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:10:16,927] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run52
[2019-03-26 12:10:16,924] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run52
[2019-03-26 12:10:16,974] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run52
[2019-03-26 12:10:16,999] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run52
[2019-03-26 12:10:33,768] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2573368], dtype=float32), 0.49818236]
[2019-03-26 12:10:33,771] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.48345222166667, 94.22528611, 1.0, 2.0, 0.281607351518618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457874.2332055973, 457874.2332055979, 164085.7165338627]
[2019-03-26 12:10:33,772] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:10:33,775] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6965969705456235
[2019-03-26 12:10:34,486] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.2573368], dtype=float32), 0.49818236]
[2019-03-26 12:10:34,489] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.58333333333334, 80.66666666666667, 1.0, 2.0, 0.311493984397065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493855.1373513195, 493855.1373513188, 166592.9363236253]
[2019-03-26 12:10:34,489] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:10:34,492] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.47989795470869967
[2019-03-26 12:10:40,961] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2573368], dtype=float32), 0.49818236]
[2019-03-26 12:10:40,963] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.93231438666667, 91.80905463333335, 1.0, 2.0, 0.3456225410950366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538193.1879255606, 538193.1879255606, 169826.4188828394]
[2019-03-26 12:10:40,963] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:10:40,966] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3932358975963426
[2019-03-26 12:10:42,549] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2573368], dtype=float32), 0.49818236]
[2019-03-26 12:10:42,551] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.17433024, 97.606646315, 1.0, 2.0, 0.4789452335263711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 671519.5720174764, 671519.572017477, 180656.6744364651]
[2019-03-26 12:10:42,553] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:10:42,557] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6490449548001158
[2019-03-26 12:10:45,565] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.2573368], dtype=float32), 0.49818236]
[2019-03-26 12:10:45,566] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.73333333333333, 87.66666666666667, 1.0, 2.0, 0.4902860519465426, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685094.2057614902, 685094.2057614907, 182089.9130827588]
[2019-03-26 12:10:45,567] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:10:45,570] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.0418673e-34 1.0000000e+00 0.0000000e+00 7.5971081e-29 2.2090918e-19], sampled 0.8902512474301553
[2019-03-26 12:11:23,615] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2573368], dtype=float32), 0.49818236]
[2019-03-26 12:11:23,616] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.65, 70.5, 1.0, 2.0, 0.5930399193096856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 828731.8642975513, 828731.8642975513, 199456.5679258005]
[2019-03-26 12:11:23,616] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:11:23,620] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7352279613996288
[2019-03-26 12:11:43,776] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.2573368], dtype=float32), 0.49818236]
[2019-03-26 12:11:43,779] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.42593090666666, 97.43368954666667, 1.0, 2.0, 0.5458340176455324, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9479331785822989, 6.911199999999999, 6.9112, 168.912591397905, 1526030.558075976, 1526030.558075976, 334131.0594457596]
[2019-03-26 12:11:43,781] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:11:43,783] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.7242546e-28 9.9999499e-01 1.6830204e-31 1.4283343e-30 4.9857085e-06], sampled 0.7298753873351557
[2019-03-26 12:11:49,450] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.2573368], dtype=float32), 0.49818236]
[2019-03-26 12:11:49,454] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.34037254, 64.46983409166667, 1.0, 2.0, 0.5817176089888092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812903.6718072412, 812903.6718072412, 197389.0916976766]
[2019-03-26 12:11:49,455] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:11:49,458] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.862586e-34], sampled 0.4029010812487679
[2019-03-26 12:12:03,337] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2573368], dtype=float32), 0.49818236]
[2019-03-26 12:12:03,339] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.9, 64.0, 1.0, 2.0, 0.5531309415351133, 0.0, 2.0, 0.0, 1.0, 2.0, 0.960605522248896, 6.911199999999999, 6.9112, 168.9129264077988, 1546445.994001769, 1546445.994001769, 338475.4384719728]
[2019-03-26 12:12:03,341] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:12:03,344] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.7888048e-26 9.9999845e-01 8.5661095e-31 2.8318325e-27 1.5069011e-06], sampled 0.7439331761619427
[2019-03-26 12:12:12,360] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8151.0499 3004086109.9558 1399.0000
[2019-03-26 12:12:13,163] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8363.6935 2928463377.6179 1131.0000
[2019-03-26 12:12:13,195] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8624.2355 2839499016.3582 855.0000
[2019-03-26 12:12:13,433] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8015.7451 3160529750.7700 1555.0000
[2019-03-26 12:12:13,449] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8744.3598 2779244363.8107 752.0000
[2019-03-26 12:12:14,462] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1275000, evaluation results [1275000.0, 8015.74509969266, 3160529750.770024, 1555.0, 8363.693515601415, 2928463377.6178603, 1131.0, 8744.359809861518, 2779244363.810724, 752.0, 8151.049896300412, 3004086109.9557767, 1399.0, 8624.235498367989, 2839499016.3582234, 855.0]
[2019-03-26 12:12:14,479] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[62.193737]
 [61.82083 ]
 [61.276863]
 [60.754272]
 [60.30074 ]], R is [[62.31332016]
 [62.22249222]
 [62.13222122]
 [62.022995  ]
 [61.90473175]].
[2019-03-26 12:12:23,416] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.7247241e-37 1.0000000e+00 0.0000000e+00 2.4686848e-32 1.3735010e-24], sum to 1.0000
[2019-03-26 12:12:23,426] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6681
[2019-03-26 12:12:23,431] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 96.0, 1.0, 2.0, 0.3837643577259084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579520.9411866262, 579520.9411866268, 172876.7395139048], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 156000.0000, 
sim time next is 156600.0000, 
raw observation next is [22.4, 96.0, 1.0, 2.0, 0.3825525042234031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577691.8573886231, 577691.8573886231, 172713.5673948243], 
processed observation next is [1.0, 0.8260869565217391, 0.2606635071090047, 0.96, 1.0, 1.0, 0.2560873544860278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16046996038572864, 0.16046996038572864, 0.25778144387287205], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.5815765], dtype=float32), -0.39742026]. 
=============================================
[2019-03-26 12:12:26,068] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0092623e-31], sum to 1.0000
[2019-03-26 12:12:26,077] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6054
[2019-03-26 12:12:26,084] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 89.0, 1.0, 2.0, 0.2972226638213607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474524.5789758748, 474524.5789758748, 165249.8959981345], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 243600.0000, 
sim time next is 244200.0000, 
raw observation next is [21.13333333333334, 89.0, 1.0, 2.0, 0.2970557821000253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474587.9209543765, 474587.9209543765, 165258.5003072337], 
processed observation next is [0.0, 0.8260869565217391, 0.20063191153238583, 0.89, 1.0, 1.0, 0.15307925554219917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13182997804288238, 0.13182997804288238, 0.2466544780704981], 
reward next is 0.7533, 
noisyNet noise sample is [array([0.16522288], dtype=float32), -0.083017506]. 
=============================================
[2019-03-26 12:12:36,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1697611e-37], sum to 1.0000
[2019-03-26 12:12:36,455] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1135
[2019-03-26 12:12:36,461] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.98333333333333, 74.5, 1.0, 2.0, 0.478043141022105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778574.8402033462, 778574.8402033462, 191805.6140737044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 382200.0000, 
sim time next is 382800.0000, 
raw observation next is [22.06666666666667, 74.0, 1.0, 2.0, 0.4619755834979364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752241.4872019907, 752241.4872019907, 189065.6488971295], 
processed observation next is [1.0, 0.43478260869565216, 0.2448657187993683, 0.74, 1.0, 1.0, 0.3517778114432969, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20895596866721963, 0.20895596866721963, 0.28218753566735744], 
reward next is 0.7178, 
noisyNet noise sample is [array([-0.44786033], dtype=float32), 0.6987988]. 
=============================================
[2019-03-26 12:12:38,726] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2261617e-34], sum to 1.0000
[2019-03-26 12:12:38,732] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6098
[2019-03-26 12:12:38,737] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.06666666666667, 83.33333333333334, 1.0, 2.0, 0.2502838529823139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 411744.1720489719, 411744.1720489725, 160932.0449632001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 423600.0000, 
sim time next is 424200.0000, 
raw observation next is [20.03333333333333, 83.66666666666666, 1.0, 2.0, 0.2483630317335039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 408540.0226770592, 408540.0226770592, 160744.0411312169], 
processed observation next is [1.0, 0.9130434782608695, 0.14849921011058448, 0.8366666666666666, 1.0, 1.0, 0.09441329124518542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11348333963251646, 0.11348333963251646, 0.2399164793003237], 
reward next is 0.7601, 
noisyNet noise sample is [array([0.00335959], dtype=float32), 0.77142835]. 
=============================================
[2019-03-26 12:12:40,320] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:12:40,328] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9409
[2019-03-26 12:12:40,332] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.66666666666667, 83.33333333333333, 1.0, 2.0, 0.230959230019219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381657.6106084033, 381657.6106084033, 159024.0760276225], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 445200.0000, 
sim time next is 445800.0000, 
raw observation next is [19.68333333333333, 83.16666666666667, 1.0, 2.0, 0.2306033050243999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381086.0892401392, 381086.0892401392, 158990.3042123528], 
processed observation next is [1.0, 0.13043478260869565, 0.13191153238546593, 0.8316666666666667, 1.0, 1.0, 0.0730160301498794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10585724701114978, 0.10585724701114978, 0.23729896151097435], 
reward next is 0.7627, 
noisyNet noise sample is [array([-0.05405029], dtype=float32), -1.3841099]. 
=============================================
[2019-03-26 12:12:40,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:12:40,888] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2171
[2019-03-26 12:12:40,893] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.05, 73.5, 1.0, 2.0, 0.3166512713863182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497365.1924567967, 497365.1924567967, 166751.2736341544], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [24.0, 74.0, 1.0, 2.0, 0.3174580353314876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 498329.8766802831, 498329.8766802831, 166815.7604252182], 
processed observation next is [0.0, 0.7391304347826086, 0.3364928909952607, 0.74, 1.0, 1.0, 0.17766028353191274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1384249657445231, 0.1384249657445231, 0.24897874690331076], 
reward next is 0.7510, 
noisyNet noise sample is [array([-0.43255764], dtype=float32), 1.6552712]. 
=============================================
[2019-03-26 12:12:40,963] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:12:40,972] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1198
[2019-03-26 12:12:40,977] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 81.0, 1.0, 2.0, 0.2383487008260658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392901.3570151539, 392901.3570151546, 159759.5334614988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 457200.0000, 
sim time next is 457800.0000, 
raw observation next is [20.23333333333333, 80.83333333333333, 1.0, 2.0, 0.2636482956071631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 434520.2358408161, 434520.2358408161, 162264.0863842669], 
processed observation next is [1.0, 0.30434782608695654, 0.15797788309636643, 0.8083333333333332, 1.0, 1.0, 0.11282927181585911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12070006551133781, 0.12070006551133781, 0.24218520355860731], 
reward next is 0.7578, 
noisyNet noise sample is [array([0.88410777], dtype=float32), 0.1439674]. 
=============================================
[2019-03-26 12:12:43,701] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1209510e-35 1.0000000e+00 0.0000000e+00 1.8874957e-36 3.1620956e-20], sum to 1.0000
[2019-03-26 12:12:43,716] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9327
[2019-03-26 12:12:43,723] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 56.0, 1.0, 2.0, 0.338113927322048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 555031.0528448228, 555031.0528448233, 170949.8978471664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 649800.0000, 
sim time next is 650400.0000, 
raw observation next is [24.4, 55.66666666666667, 1.0, 2.0, 0.3825066546266599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628027.2524169016, 628027.2524169016, 176960.4552578797], 
processed observation next is [1.0, 0.5217391304347826, 0.3554502369668246, 0.5566666666666668, 1.0, 1.0, 0.25603211400802395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17445201456025045, 0.17445201456025045, 0.2641200824744473], 
reward next is 0.7359, 
noisyNet noise sample is [array([1.0867057], dtype=float32), -0.76642483]. 
=============================================
[2019-03-26 12:12:46,436] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:12:46,446] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4498
[2019-03-26 12:12:46,454] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 75.0, 1.0, 2.0, 0.3653496452052277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600724.2853475936, 600724.2853475936, 174600.5831611978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 550200.0000, 
sim time next is 550800.0000, 
raw observation next is [21.2, 75.0, 1.0, 2.0, 0.4174778319579757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686437.3715234355, 686437.3715234349, 182086.0896672615], 
processed observation next is [1.0, 0.391304347826087, 0.20379146919431282, 0.75, 1.0, 1.0, 0.29816606259997075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19067704764539875, 0.19067704764539858, 0.2717702830854649], 
reward next is 0.7282, 
noisyNet noise sample is [array([-0.6187991], dtype=float32), -0.6492609]. 
=============================================
[2019-03-26 12:12:56,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.157194e-35], sum to 1.0000
[2019-03-26 12:12:56,943] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5621
[2019-03-26 12:12:56,947] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.53333333333333, 52.5, 1.0, 2.0, 0.4037994560166214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658645.1152586079, 658645.1152586079, 179944.0940086785], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 737400.0000, 
sim time next is 738000.0000, 
raw observation next is [25.6, 52.0, 1.0, 2.0, 0.4115975966727267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671624.6867285226, 671624.6867285226, 181114.0266756926], 
processed observation next is [1.0, 0.5652173913043478, 0.4123222748815167, 0.52, 1.0, 1.0, 0.29108144177436956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18656241298014517, 0.18656241298014517, 0.27031944279954123], 
reward next is 0.7297, 
noisyNet noise sample is [array([-0.24357532], dtype=float32), -0.37880298]. 
=============================================
[2019-03-26 12:12:56,958] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.457375]
 [83.62059 ]
 [83.8327  ]
 [83.428665]
 [82.52798 ]], R is [[83.35821533]
 [83.25606537]
 [83.16004944]
 [83.07685089]
 [82.98886871]].
[2019-03-26 12:12:59,934] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:12:59,944] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6807
[2019-03-26 12:12:59,949] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 82.66666666666667, 1.0, 2.0, 0.2808857387208815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 452917.6797387828, 452917.6797387835, 163795.2069979346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 804000.0000, 
sim time next is 804600.0000, 
raw observation next is [21.65, 82.0, 1.0, 2.0, 0.2824387353538764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 454906.8589584904, 454906.8589584911, 163928.4994987001], 
processed observation next is [0.0, 0.30434782608695654, 0.22511848341232227, 0.82, 1.0, 1.0, 0.13546835584804387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12636301637735844, 0.12636301637735864, 0.24466940223686584], 
reward next is 0.7553, 
noisyNet noise sample is [array([1.878465], dtype=float32), 1.2619425]. 
=============================================
[2019-03-26 12:13:05,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:13:05,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6095
[2019-03-26 12:13:05,700] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.48333333333333, 83.5, 1.0, 2.0, 0.2840777943892459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457149.539446097, 457149.539446097, 164079.0088635158], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 886200.0000, 
sim time next is 886800.0000, 
raw observation next is [21.56666666666667, 83.0, 1.0, 2.0, 0.2836067054072956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456236.4502784533, 456236.4502784533, 164016.7009061235], 
processed observation next is [0.0, 0.2608695652173913, 0.22116903633491333, 0.83, 1.0, 1.0, 0.13687554868348867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12673234729957036, 0.12673234729957036, 0.24480104612854253], 
reward next is 0.7552, 
noisyNet noise sample is [array([-1.6517525], dtype=float32), -0.6030062]. 
=============================================
[2019-03-26 12:13:08,631] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 12:13:08,632] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:13:08,634] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:13:08,634] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:13:08,634] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:13:08,635] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:13:08,636] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:13:08,637] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:13:08,635] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:13:08,642] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:13:08,642] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:13:08,673] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run53
[2019-03-26 12:13:08,673] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run53
[2019-03-26 12:13:08,718] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run53
[2019-03-26 12:13:08,737] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run53
[2019-03-26 12:13:08,759] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run53
[2019-03-26 12:13:16,025] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17102577], dtype=float32), 0.4759746]
[2019-03-26 12:13:16,027] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.2, 89.0, 1.0, 2.0, 0.2472565054531773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 407669.6456850696, 407669.6456850702, 160611.9367584206]
[2019-03-26 12:13:16,028] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:13:16,032] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1891895e-36], sampled 0.190584407732994
[2019-03-26 12:13:42,404] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.17102577], dtype=float32), 0.4759746]
[2019-03-26 12:13:42,408] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.01314643, 90.39578734, 1.0, 2.0, 0.5156503064120977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720548.6046723557, 720548.6046723557, 186083.9537886107]
[2019-03-26 12:13:42,409] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:13:42,414] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.81795806e-33 9.99999523e-01 2.06053690e-34 1.55276945e-30
 5.26783765e-07], sampled 0.5296030752001614
[2019-03-26 12:13:48,821] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.17102577], dtype=float32), 0.4759746]
[2019-03-26 12:13:48,822] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.58219532166667, 99.36162347166668, 1.0, 2.0, 0.3818824357594956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 583083.5259935156, 583083.5259935156, 173370.1294029953]
[2019-03-26 12:13:48,822] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:13:48,826] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8477435e-18], sampled 0.5965350101585668
[2019-03-26 12:13:49,544] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.17102577], dtype=float32), 0.4759746]
[2019-03-26 12:13:49,545] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.53870615, 93.35366375000001, 1.0, 2.0, 0.4091416381573234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 603689.7596281931, 603689.7596281925, 174682.7866315656]
[2019-03-26 12:13:49,546] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:13:49,549] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8917975e-30], sampled 0.8163874514287623
[2019-03-26 12:14:00,970] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.17102577], dtype=float32), 0.4759746]
[2019-03-26 12:14:00,972] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.78304684666666, 71.31390719999999, 1.0, 2.0, 0.5118751012301231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715271.5115922301, 715271.5115922295, 185476.9023135258]
[2019-03-26 12:14:00,974] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:14:00,976] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4042227e-36 4.6939075e-21], sampled 0.9360454488349745
[2019-03-26 12:14:03,737] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.17102577], dtype=float32), 0.4759746]
[2019-03-26 12:14:03,738] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.0, 59.0, 1.0, 2.0, 0.6451908940859504, 1.0, 2.0, 0.6431854865572377, 1.0, 2.0, 1.03, 7.00509341090698, 6.9112, 170.5573041426782, 2698674.654115309, 2631414.984542877, 504574.1554551778]
[2019-03-26 12:14:03,740] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:14:03,742] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.5207803e-23 0.0000000e+00 4.7671734e-35 1.0000000e+00], sampled 0.2770072523985829
[2019-03-26 12:14:36,360] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.17102577], dtype=float32), 0.4759746]
[2019-03-26 12:14:36,362] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.63333333333333, 95.66666666666667, 1.0, 2.0, 0.6432529430951973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898930.7522360573, 898930.7522360573, 209072.247016933]
[2019-03-26 12:14:36,363] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:14:36,366] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.1882719e-37 1.0000000e+00 0.0000000e+00 2.9041558e-38 3.5356322e-18], sampled 0.5784808390418368
[2019-03-26 12:15:00,725] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.17102577], dtype=float32), 0.4759746]
[2019-03-26 12:15:00,727] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.780434115, 87.56696270166665, 1.0, 2.0, 0.2054951152191613, 1.0, 2.0, 0.2054951152191613, 1.0, 2.0, 0.3564018300660126, 6.9112, 6.9112, 171.5212843490159, 861503.1842467266, 861503.1842467266, 271555.5425807926]
[2019-03-26 12:15:00,729] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:15:00,731] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.043983e-31 9.999800e-01 0.000000e+00 4.051246e-29 2.005028e-05], sampled 0.9214405618094843
[2019-03-26 12:15:04,707] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8856.9654 2861678176.6065 203.0000
[2019-03-26 12:15:04,996] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8684.6211 2953787367.4508 365.0000
[2019-03-26 12:15:05,107] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8918.6432 2811608700.9195 241.0000
[2019-03-26 12:15:05,243] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8666.3196 3012978081.8275 177.0000
[2019-03-26 12:15:05,355] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8304.7061 3176357755.4572 683.0000
[2019-03-26 12:15:06,373] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1300000, evaluation results [1300000.0, 8304.706116153246, 3176357755.45718, 683.0, 8684.62110371207, 2953787367.4507875, 365.0, 8918.64315304899, 2811608700.9195004, 241.0, 8666.319624535097, 3012978081.827461, 177.0, 8856.965356896348, 2861678176.6064973, 203.0]
[2019-03-26 12:15:07,834] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7617513e-38 1.9910536e-29], sum to 1.0000
[2019-03-26 12:15:07,841] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9682
[2019-03-26 12:15:07,848] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666667, 78.0, 1.0, 2.0, 0.318137602354141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501965.7990923684, 501965.7990923684, 167150.1890269072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1110000.0000, 
sim time next is 1110600.0000, 
raw observation next is [23.05, 78.5, 1.0, 2.0, 0.3171040195629773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500848.2221666477, 500848.2221666477, 167076.9413814815], 
processed observation next is [1.0, 0.8695652173913043, 0.2914691943127963, 0.785, 1.0, 1.0, 0.17723375850961118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13912450615740216, 0.13912450615740216, 0.2493685692260918], 
reward next is 0.7506, 
noisyNet noise sample is [array([1.2983052], dtype=float32), 1.3734808]. 
=============================================
[2019-03-26 12:15:09,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.67550101e-32 1.00000000e+00 1.03078035e-36 2.73614953e-34
 1.99968793e-19], sum to 1.0000
[2019-03-26 12:15:09,652] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1040
[2019-03-26 12:15:09,658] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 94.66666666666666, 1.0, 2.0, 0.5663689881175034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 871975.5004917851, 871975.5004917858, 204341.0745419479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 992400.0000, 
sim time next is 993000.0000, 
raw observation next is [21.91666666666666, 94.83333333333333, 1.0, 2.0, 0.5968879034950557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 918821.9658997034, 918821.9658997034, 210431.7754291751], 
processed observation next is [1.0, 0.4782608695652174, 0.23775671406003138, 0.9483333333333333, 1.0, 1.0, 0.5143227752952478, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2552283238610287, 0.2552283238610287, 0.31407727675996283], 
reward next is 0.6859, 
noisyNet noise sample is [array([1.2123196], dtype=float32), 0.31282967]. 
=============================================
[2019-03-26 12:15:09,671] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[61.096912]
 [61.508118]
 [61.295975]
 [60.79447 ]
 [60.38401 ]], R is [[60.90209961]
 [60.98809433]
 [61.09471893]
 [61.19675446]
 [61.276371  ]].
[2019-03-26 12:15:13,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:15:13,732] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5340
[2019-03-26 12:15:13,736] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.75, 95.0, 1.0, 2.0, 0.3074585171128207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487850.8660610176, 487850.8660610176, 166159.7398412576], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1063800.0000, 
sim time next is 1064400.0000, 
raw observation next is [20.8, 95.0, 1.0, 2.0, 0.3073458732453746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 487114.345083642, 487114.3450836426, 166095.502004618], 
processed observation next is [1.0, 0.30434782608695654, 0.1848341232227489, 0.95, 1.0, 1.0, 0.1654769557173188, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13530954030101167, 0.13530954030101183, 0.24790373433525076], 
reward next is 0.7521, 
noisyNet noise sample is [array([-0.15572123], dtype=float32), -1.5119663]. 
=============================================
[2019-03-26 12:15:15,967] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6711326e-35 3.9578350e-13 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 12:15:15,978] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7469
[2019-03-26 12:15:15,983] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.16666666666666, 71.66666666666667, 1.0, 2.0, 0.4008802800778435, 1.0, 2.0, 0.4008802800778435, 1.0, 2.0, 0.6761930934848209, 6.911199999999999, 6.9112, 170.5573041426782, 1681268.228428366, 1681268.228428367, 350507.3982543465], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1251600.0000, 
sim time next is 1252200.0000, 
raw observation next is [28.18333333333333, 71.83333333333333, 1.0, 2.0, 0.410633894413016, 1.0, 2.0, 0.410633894413016, 1.0, 2.0, 0.6932504881454927, 6.911199999999999, 6.9112, 170.5573041426782, 1722207.181562132, 1722207.181562133, 355923.0544178103], 
processed observation next is [1.0, 0.4782608695652174, 0.5347551342812005, 0.7183333333333333, 1.0, 1.0, 0.28992035471447714, 1.0, 1.0, 0.28992035471447714, 1.0, 1.0, 0.6259152294457228, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.47839088376725886, 0.47839088376725913, 0.5312284394295675], 
reward next is 0.4688, 
noisyNet noise sample is [array([-0.22902808], dtype=float32), 0.7753035]. 
=============================================
[2019-03-26 12:15:25,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4149282e-23 9.9747294e-01 1.1077083e-31 4.2341215e-21 2.5270395e-03], sum to 1.0000
[2019-03-26 12:15:25,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2448
[2019-03-26 12:15:25,514] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1934833.258398185 W.
[2019-03-26 12:15:25,523] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.9, 75.5, 1.0, 2.0, 0.4612855899864752, 1.0, 2.0, 0.4612855899864752, 1.0, 1.0, 0.7826383923201331, 6.9112, 6.9112, 170.5573041426782, 1934833.258398185, 1934833.258398185, 386313.9636149751], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1269000.0000, 
sim time next is 1269600.0000, 
raw observation next is [27.86666666666667, 75.66666666666667, 1.0, 2.0, 0.6029565227770964, 0.0, 1.0, 0.0, 1.0, 2.0, 1.015529631433608, 6.911199999999999, 6.9112, 168.912956510431, 1685859.214451093, 1685859.214451094, 362490.2743582599], 
processed observation next is [1.0, 0.6956521739130435, 0.519747235387046, 0.7566666666666667, 1.0, 1.0, 0.5216343647916825, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.018938574919034, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4682942262364147, 0.468294226236415, 0.5410302602362088], 
reward next is 0.4590, 
noisyNet noise sample is [array([-0.3640035], dtype=float32), 1.2997679]. 
=============================================
[2019-03-26 12:15:27,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3519212e-30], sum to 1.0000
[2019-03-26 12:15:27,094] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0878
[2019-03-26 12:15:27,097] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.43333333333333, 93.83333333333334, 1.0, 2.0, 0.4652921092080324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656526.3596905082, 656526.3596905082, 179158.3157068784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1291800.0000, 
sim time next is 1292400.0000, 
raw observation next is [24.4, 94.0, 1.0, 2.0, 0.4646238933654218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655924.6479070106, 655924.6479070106, 179103.355951271], 
processed observation next is [1.0, 1.0, 0.3554502369668246, 0.94, 1.0, 1.0, 0.3549685462233998, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18220129108528074, 0.18220129108528074, 0.2673184417183149], 
reward next is 0.7327, 
noisyNet noise sample is [array([-1.2000831], dtype=float32), 0.70694536]. 
=============================================
[2019-03-26 12:15:36,024] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4850656e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8423189e-23], sum to 1.0000
[2019-03-26 12:15:36,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3176
[2019-03-26 12:15:36,051] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 85.0, 1.0, 2.0, 0.6123591816638826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 920756.4851794781, 920756.4851794788, 211193.4751485365], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1600200.0000, 
sim time next is 1600800.0000, 
raw observation next is [23.96666666666667, 85.0, 1.0, 2.0, 0.5557216071884554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835169.4369399251, 835169.4369399251, 200024.7237217609], 
processed observation next is [1.0, 0.5217391304347826, 0.33491311216429714, 0.85, 1.0, 1.0, 0.4647248279378981, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2319915102610903, 0.2319915102610903, 0.29854436376382226], 
reward next is 0.7015, 
noisyNet noise sample is [array([0.5124868], dtype=float32), -0.20984639]. 
=============================================
[2019-03-26 12:15:40,357] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:15:40,367] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1939
[2019-03-26 12:15:40,380] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 94.33333333333334, 1.0, 2.0, 0.4738998502137316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 662190.044487812, 662190.0444878126, 179607.5331538906], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1995600.0000, 
sim time next is 1996200.0000, 
raw observation next is [24.55, 94.5, 1.0, 2.0, 0.4735450091595796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661865.5358308558, 661865.5358308565, 179576.8497709683], 
processed observation next is [0.0, 0.08695652173913043, 0.3625592417061612, 0.945, 1.0, 1.0, 0.3657168785055176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1838515377307933, 0.1838515377307935, 0.268025148911893], 
reward next is 0.7320, 
noisyNet noise sample is [array([-0.28781322], dtype=float32), 0.5159646]. 
=============================================
[2019-03-26 12:15:45,335] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3547798e-29], sum to 1.0000
[2019-03-26 12:15:45,343] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4902
[2019-03-26 12:15:45,348] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333334, 87.66666666666666, 1.0, 2.0, 0.7740902642945382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1154497.710712508, 1154497.710712507, 247411.900702819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1608000.0000, 
sim time next is 1608600.0000, 
raw observation next is [23.76666666666667, 88.33333333333334, 1.0, 2.0, 0.7809257562176644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1163778.242535404, 1163778.242535404, 249046.0578140412], 
processed observation next is [1.0, 0.6086956521739131, 0.32543443917851517, 0.8833333333333334, 1.0, 1.0, 0.7360551279730896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3232717340376122, 0.3232717340376122, 0.3717105340508078], 
reward next is 0.6283, 
noisyNet noise sample is [array([-1.4957403], dtype=float32), -0.7345227]. 
=============================================
[2019-03-26 12:15:49,694] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.9925015e-22 1.0000000e+00 1.0366873e-29 1.5539921e-21 1.8854038e-11], sum to 1.0000
[2019-03-26 12:15:49,702] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9267
[2019-03-26 12:15:49,708] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1956674.670550039 W.
[2019-03-26 12:15:49,716] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.13333333333333, 79.5, 1.0, 2.0, 0.6997321092386097, 1.0, 1.0, 0.6997321092386097, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1956674.670550039, 1956674.670550039, 374006.3719117657], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1695000.0000, 
sim time next is 1695600.0000, 
raw observation next is [28.2, 79.0, 1.0, 2.0, 0.5793470506214042, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9867693376388863, 6.9112, 6.9112, 168.912956254771, 1619797.038930222, 1619797.038930222, 350370.7144959553], 
processed observation next is [1.0, 0.6521739130434783, 0.5355450236966824, 0.79, 1.0, 1.0, 0.49318921761614964, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.9838650459010807, 0.0, 0.0, 0.829439943896895, 0.44994362192506165, 0.44994362192506165, 0.5229413649193363], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0226618], dtype=float32), 0.17921053]. 
=============================================
[2019-03-26 12:15:50,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:15:50,271] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5398
[2019-03-26 12:15:50,275] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.95, 91.50000000000001, 1.0, 2.0, 0.50972278043252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712262.9458737795, 712262.9458737788, 185133.5208410282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1721400.0000, 
sim time next is 1722000.0000, 
raw observation next is [25.9, 92.0, 1.0, 2.0, 0.5097685494312868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712326.9227911418, 712326.9227911412, 185140.8630280854], 
processed observation next is [1.0, 0.9565217391304348, 0.42654028436018954, 0.92, 1.0, 1.0, 0.40935969810998407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19786858966420606, 0.1978685896642059, 0.2763296463105752], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.11324938], dtype=float32), -0.80456513]. 
=============================================
[2019-03-26 12:15:50,286] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[59.736706]
 [59.763233]
 [59.873577]
 [60.031464]
 [60.23149 ]], R is [[59.87355804]
 [59.99850464]
 [60.12220383]
 [60.24454117]
 [60.36548996]].
[2019-03-26 12:15:50,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:15:50,892] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7112
[2019-03-26 12:15:50,895] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 88.5, 1.0, 2.0, 0.5088501122598655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711043.111764811, 711043.1117648116, 184994.6356829656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1716600.0000, 
sim time next is 1717200.0000, 
raw observation next is [26.4, 89.0, 1.0, 2.0, 0.5099836094240121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712627.5384505275, 712627.5384505275, 185175.3723690306], 
processed observation next is [1.0, 0.9130434782608695, 0.45023696682464454, 0.89, 1.0, 1.0, 0.4096188065349543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1979520940140354, 0.1979520940140354, 0.2763811527895979], 
reward next is 0.7236, 
noisyNet noise sample is [array([-0.0954155], dtype=float32), 0.9442247]. 
=============================================
[2019-03-26 12:16:00,262] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 12:16:00,264] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:16:00,265] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:16:00,266] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:16:00,267] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:16:00,267] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:16:00,268] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:16:00,268] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:16:00,270] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:16:00,273] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:16:00,274] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:16:00,300] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run54
[2019-03-26 12:16:00,324] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run54
[2019-03-26 12:16:00,324] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run54
[2019-03-26 12:16:00,367] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run54
[2019-03-26 12:16:00,368] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run54
[2019-03-26 12:16:11,926] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1748968], dtype=float32), 0.43587723]
[2019-03-26 12:16:11,927] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.32887861, 80.22156722, 1.0, 2.0, 0.1883939345746549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 315029.8295161931, 315029.8295161931, 154339.0229389087]
[2019-03-26 12:16:11,928] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:16:11,930] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06079421482011893
[2019-03-26 12:16:17,507] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1748968], dtype=float32), 0.43587723]
[2019-03-26 12:16:17,508] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.01229625, 62.88619195, 1.0, 2.0, 0.5749353351538188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 873611.7528401094, 873611.7528401101, 204767.5753423836]
[2019-03-26 12:16:17,509] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:16:17,513] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.5663835e-37], sampled 0.626848112498701
[2019-03-26 12:16:21,331] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1748968], dtype=float32), 0.43587723]
[2019-03-26 12:16:21,332] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.1, 94.0, 1.0, 2.0, 0.3215548565950076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507499.2158023393, 507499.2158023393, 167571.6449226296]
[2019-03-26 12:16:21,334] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:16:21,336] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6226488717981914
[2019-03-26 12:16:22,716] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1748968], dtype=float32), 0.43587723]
[2019-03-26 12:16:22,717] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.31666666666667, 87.16666666666667, 1.0, 2.0, 0.3065010556360652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490350.1744154746, 490350.1744154753, 166393.8376033607]
[2019-03-26 12:16:22,719] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:16:22,723] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.385831207989088
[2019-03-26 12:16:36,570] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1748968], dtype=float32), 0.43587723]
[2019-03-26 12:16:36,571] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.16905829666667, 84.45911460666667, 1.0, 2.0, 0.6105694772129724, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.937593146194933, 6.9112, 168.9126701524821, 1707162.066317299, 1688437.891767753, 366849.3469396029]
[2019-03-26 12:16:36,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:16:36,575] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7270032e-32 1.0000000e+00 0.0000000e+00 1.3632686e-26 1.1476860e-17], sampled 0.08375951220460676
[2019-03-26 12:16:36,577] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1707162.066317299 W.
[2019-03-26 12:16:47,637] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1748968], dtype=float32), 0.43587723]
[2019-03-26 12:16:47,639] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.90000000000001, 54.66666666666667, 1.0, 2.0, 0.5800820298386071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956498347, 810617.2090508466, 810617.2090508466, 197094.7227493366]
[2019-03-26 12:16:47,641] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:16:47,645] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1753728e-35 1.0000000e+00 0.0000000e+00 7.0761295e-28 7.3391222e-19], sampled 0.5593873780903951
[2019-03-26 12:16:49,613] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.1748968], dtype=float32), 0.43587723]
[2019-03-26 12:16:49,616] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.83333333333334, 79.00000000000001, 1.0, 2.0, 0.5146523632729149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719153.6477883166, 719153.647788316, 185923.4973853219]
[2019-03-26 12:16:49,618] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:16:49,619] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6282761273643168
[2019-03-26 12:16:57,840] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1748968], dtype=float32), 0.43587723]
[2019-03-26 12:16:57,841] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.17400894666667, 76.39563856999999, 1.0, 2.0, 0.5605007676480324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783243.9072950992, 783243.9072950992, 193616.8213648821]
[2019-03-26 12:16:57,843] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:16:57,846] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.037147920925825306
[2019-03-26 12:17:11,920] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1748968], dtype=float32), 0.43587723]
[2019-03-26 12:17:11,922] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.7, 82.0, 1.0, 2.0, 0.6066735228640102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 847791.4766380768, 847791.4766380762, 201995.1078115364]
[2019-03-26 12:17:11,923] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:17:11,926] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7368456e-30], sampled 0.6280450559925657
[2019-03-26 12:17:56,224] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8681.0360 2779202611.0920 878.0000
[2019-03-26 12:17:56,510] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8276.4593 2927745033.3163 1286.0000
[2019-03-26 12:17:56,585] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8063.9245 3004728816.9339 1587.0000
[2019-03-26 12:17:56,749] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8542.2914 2840717642.6584 1038.0000
[2019-03-26 12:17:56,806] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7950.3141 3161155947.8061 1717.0000
[2019-03-26 12:17:57,822] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1325000, evaluation results [1325000.0, 7950.314109669218, 3161155947.806079, 1717.0, 8276.459280181572, 2927745033.316297, 1286.0, 8681.036028064187, 2779202611.0920053, 878.0, 8063.9244910674215, 3004728816.93387, 1587.0, 8542.291356327141, 2840717642.6583853, 1038.0]
[2019-03-26 12:18:15,016] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2601876e-36 1.0000000e+00 0.0000000e+00 1.5941844e-37 9.8828300e-21], sum to 1.0000
[2019-03-26 12:18:15,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4602
[2019-03-26 12:18:15,028] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 96.33333333333333, 1.0, 2.0, 0.5916131441029758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826737.2696399585, 826737.2696399585, 199186.5375727023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2175600.0000, 
sim time next is 2176200.0000, 
raw observation next is [24.65, 96.5, 1.0, 2.0, 0.5889505493944133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 823015.0414544895, 823015.0414544889, 198698.2665550767], 
processed observation next is [1.0, 0.17391304347826086, 0.3672985781990521, 0.965, 1.0, 1.0, 0.5047596980655581, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22861528929291375, 0.22861528929291358, 0.2965645769478757], 
reward next is 0.7034, 
noisyNet noise sample is [array([0.948404], dtype=float32), -0.12991604]. 
=============================================
[2019-03-26 12:18:20,778] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2867264e-31 1.0000000e+00 0.0000000e+00 4.0511408e-22 4.5016163e-15], sum to 1.0000
[2019-03-26 12:18:20,784] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2750
[2019-03-26 12:18:20,792] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.43333333333334, 70.66666666666667, 1.0, 2.0, 0.5799419195826917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810421.3417291143, 810421.3417291143, 197069.1712000144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2402400.0000, 
sim time next is 2403000.0000, 
raw observation next is [31.3, 71.5, 1.0, 2.0, 0.5793604633452023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809608.4945816617, 809608.4945816617, 196964.4400551743], 
processed observation next is [1.0, 0.8260869565217391, 0.6824644549763034, 0.715, 1.0, 1.0, 0.4932053775243401, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22489124849490602, 0.22489124849490602, 0.29397677620175267], 
reward next is 0.7060, 
noisyNet noise sample is [array([0.8634587], dtype=float32), -1.35636]. 
=============================================
[2019-03-26 12:18:20,804] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[64.41921 ]
 [64.5221  ]
 [64.859985]
 [64.98774 ]
 [65.208466]], R is [[64.69298553]
 [64.75192261]
 [64.81040192]
 [64.86923981]
 [64.92831421]].
[2019-03-26 12:18:25,116] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.3493229e-31 1.0000000e+00 1.2235672e-38 1.1717860e-34 2.1576343e-15], sum to 1.0000
[2019-03-26 12:18:25,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0455
[2019-03-26 12:18:25,131] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 80.0, 1.0, 2.0, 0.7301873107500682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1020477.840927375, 1020477.840927375, 227494.3686723046], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2353200.0000, 
sim time next is 2353800.0000, 
raw observation next is [27.76666666666667, 79.5, 1.0, 2.0, 0.7255714514771513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014023.838048129, 1014023.838048129, 226460.0582356986], 
processed observation next is [1.0, 0.21739130434782608, 0.515007898894155, 0.795, 1.0, 1.0, 0.6693631945507846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2816732883467025, 0.2816732883467025, 0.33800008691895317], 
reward next is 0.6620, 
noisyNet noise sample is [array([0.12540914], dtype=float32), -0.34873348]. 
=============================================
[2019-03-26 12:18:25,770] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0383682e-32], sum to 1.0000
[2019-03-26 12:18:25,778] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1792
[2019-03-26 12:18:25,783] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 93.16666666666667, 1.0, 2.0, 0.4920327516193084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687535.7214298253, 687535.7214298247, 182357.0076422961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3181800.0000, 
sim time next is 3182400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4913679868666576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686606.5206404879, 686606.5206404879, 182254.3936055178], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.94, 1.0, 1.0, 0.387190345622479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19072403351124664, 0.19072403351124664, 0.27202148299331014], 
reward next is 0.7280, 
noisyNet noise sample is [array([0.24972084], dtype=float32), -1.4792504]. 
=============================================
[2019-03-26 12:18:26,645] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:18:26,660] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7013
[2019-03-26 12:18:26,666] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 91.5, 1.0, 2.0, 0.402772868211731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596731.3529397737, 596731.3529397737, 174111.2858917953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [23.33333333333333, 92.33333333333334, 1.0, 2.0, 0.3994368544729122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 593040.6507495865, 593040.6507495872, 173807.9887622953], 
processed observation next is [1.0, 0.8695652173913043, 0.30489731437598716, 0.9233333333333335, 1.0, 1.0, 0.276429945148087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16473351409710738, 0.16473351409710757, 0.25941490860044075], 
reward next is 0.7406, 
noisyNet noise sample is [array([-2.2016459], dtype=float32), 1.3056633]. 
=============================================
[2019-03-26 12:18:35,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:18:35,627] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4373
[2019-03-26 12:18:35,635] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.46666666666667, 93.33333333333334, 1.0, 2.0, 0.7492603171921808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1047146.582536944, 1047146.582536944, 231837.6183765737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2533200.0000, 
sim time next is 2533800.0000, 
raw observation next is [26.48333333333333, 93.16666666666666, 1.0, 2.0, 0.7783651110863742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1087843.514641576, 1087843.514641576, 238669.8148637448], 
processed observation next is [1.0, 0.30434782608695654, 0.4541864139020536, 0.9316666666666665, 1.0, 1.0, 0.7329700133570773, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3021787540671044, 0.3021787540671044, 0.356223604274246], 
reward next is 0.6438, 
noisyNet noise sample is [array([-0.36356902], dtype=float32), -1.4473567]. 
=============================================
[2019-03-26 12:18:50,104] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4311532e-27 5.7071781e-10 0.0000000e+00 1.0353984e-15 1.0000000e+00], sum to 1.0000
[2019-03-26 12:18:50,113] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7109
[2019-03-26 12:18:50,119] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.83333333333334, 63.66666666666666, 1.0, 2.0, 0.2774771936194928, 1.0, 2.0, 0.2774771936194928, 1.0, 2.0, 0.4818861222069065, 6.9112, 6.9112, 170.5573041426782, 1163442.094767875, 1163442.094767875, 295442.367723436], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3604200.0000, 
sim time next is 3604800.0000, 
raw observation next is [32.66666666666667, 64.33333333333334, 1.0, 2.0, 0.1842671412768282, 1.0, 2.0, 0.1842671412768282, 1.0, 2.0, 0.320011086323042, 6.9112, 6.9112, 170.5573041426782, 772478.222029188, 772478.222029188, 265661.9366622151], 
processed observation next is [1.0, 0.7391304347826086, 0.7472353870458138, 0.6433333333333334, 1.0, 1.0, 0.017189326839552026, 1.0, 1.0, 0.017189326839552026, 1.0, 1.0, 0.1707452272232219, 0.0, 0.0, 0.8375144448122397, 0.21457728389699665, 0.21457728389699665, 0.3965103532271867], 
reward next is 0.6035, 
noisyNet noise sample is [array([-0.8639947], dtype=float32), 0.4792216]. 
=============================================
[2019-03-26 12:18:50,372] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:18:50,384] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7571
[2019-03-26 12:18:50,391] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3393744085700938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522788.8774418494, 522788.8774418494, 168421.1684589302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2784000.0000, 
sim time next is 2784600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3354699099821118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516784.415103858, 516784.4151038575, 167944.6289302371], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.19936133732784553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14355122641773832, 0.1435512264177382, 0.2506636252690106], 
reward next is 0.7493, 
noisyNet noise sample is [array([-0.02935116], dtype=float32), 0.39650545]. 
=============================================
[2019-03-26 12:18:51,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.7871794e-33], sum to 1.0000
[2019-03-26 12:18:51,781] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4184
[2019-03-26 12:18:51,787] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 83.0, 1.0, 2.0, 0.6701957018074916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1026873.463135121, 1026873.463135121, 225814.6117439364], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2806800.0000, 
sim time next is 2807400.0000, 
raw observation next is [23.83333333333333, 83.0, 1.0, 2.0, 0.699917861205047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1067141.025263553, 1067141.025263553, 232137.4131958989], 
processed observation next is [1.0, 0.4782608695652174, 0.32859399684044216, 0.83, 1.0, 1.0, 0.6384552544639119, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2964280625732092, 0.2964280625732092, 0.3464737510386551], 
reward next is 0.6535, 
noisyNet noise sample is [array([-0.70305604], dtype=float32), 0.51412237]. 
=============================================
[2019-03-26 12:18:51,809] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 12:18:51,810] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:18:51,811] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:18:51,812] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:18:51,812] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:18:51,813] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:18:51,814] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:18:51,816] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:18:51,817] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:18:51,817] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:18:51,819] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:18:51,847] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run55
[2019-03-26 12:18:51,874] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run55
[2019-03-26 12:18:51,896] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run55
[2019-03-26 12:18:51,896] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run55
[2019-03-26 12:18:51,915] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run55
[2019-03-26 12:19:13,140] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.20785698], dtype=float32), 0.43942618]
[2019-03-26 12:19:13,141] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.1, 83.0, 1.0, 2.0, 0.2321504960322814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 385731.6157148984, 385731.6157148978, 158905.0821587507]
[2019-03-26 12:19:13,142] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:19:13,144] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3782852619041338
[2019-03-26 12:20:06,943] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.20785698], dtype=float32), 0.43942618]
[2019-03-26 12:20:06,945] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.33082310166667, 80.54348097, 1.0, 2.0, 0.5420647542827128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757472.2319354231, 757472.2319354231, 190448.9366322173]
[2019-03-26 12:20:06,946] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:20:06,950] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7574505586125475
[2019-03-26 12:20:26,744] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.20785698], dtype=float32), 0.43942618]
[2019-03-26 12:20:26,746] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.33333333333333, 91.0, 1.0, 2.0, 0.7272886022556856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1016424.794027235, 1016424.794027235, 226844.8089140306]
[2019-03-26 12:20:26,747] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:20:26,750] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.051133e-33], sampled 0.4800148446740793
[2019-03-26 12:20:38,637] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.20785698], dtype=float32), 0.43942618]
[2019-03-26 12:20:38,638] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 78.0, 1.0, 2.0, 0.5711884970637353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 798184.5544990575, 798184.554499058, 195500.6848327048]
[2019-03-26 12:20:38,639] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:20:38,643] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1636116e-36], sampled 0.19308916331156167
[2019-03-26 12:20:47,763] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8068.8645 3157662190.1635 1421.0000
[2019-03-26 12:20:47,985] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8705.0326 2778931325.3505 827.0000
[2019-03-26 12:20:48,187] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8602.2724 2838609997.9127 883.0000
[2019-03-26 12:20:48,203] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8199.8102 2999661770.3002 1265.0000
[2019-03-26 12:20:48,312] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8344.3583 2926472297.6560 1150.0000
[2019-03-26 12:20:49,328] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1350000, evaluation results [1350000.0, 8068.864466546821, 3157662190.1634865, 1421.0, 8344.358270889163, 2926472297.6559777, 1150.0, 8705.03258963686, 2778931325.350469, 827.0, 8199.810167487443, 2999661770.300175, 1265.0, 8602.27239323876, 2838609997.9127426, 883.0]
[2019-03-26 12:20:53,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:20:53,760] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3640
[2019-03-26 12:20:53,764] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 92.0, 1.0, 2.0, 0.5896047368670424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 906798.5690595781, 906798.5690595781, 208860.9060423482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2887200.0000, 
sim time next is 2887800.0000, 
raw observation next is [22.36666666666667, 91.66666666666667, 1.0, 2.0, 0.6015080193165437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 924546.5356235106, 924546.5356235106, 211234.4279068697], 
processed observation next is [1.0, 0.43478260869565216, 0.2590837282780413, 0.9166666666666667, 1.0, 1.0, 0.5198891798994503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25681848211764186, 0.25681848211764186, 0.31527526553264135], 
reward next is 0.6847, 
noisyNet noise sample is [array([1.4834087], dtype=float32), 0.13595007]. 
=============================================
[2019-03-26 12:21:01,427] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:21:01,438] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0491
[2019-03-26 12:21:01,442] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3038629939412646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483883.6642718208, 483883.6642718214, 165900.8632374746], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3022200.0000, 
sim time next is 3022800.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3030254175235287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482550.5332816411, 482550.5332816406, 165804.8657590378], 
processed observation next is [1.0, 1.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16027158737774538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13404181480045585, 0.13404181480045574, 0.2474699488940863], 
reward next is 0.7525, 
noisyNet noise sample is [array([0.29405144], dtype=float32), -1.0165461]. 
=============================================
[2019-03-26 12:21:11,148] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:21:11,156] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4635
[2019-03-26 12:21:11,164] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5745439460520508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802875.2648342384, 802875.2648342384, 196099.9543997516], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3326400.0000, 
sim time next is 3327000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5880381520989124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821739.5401817085, 821739.5401817085, 198538.2724028351], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.5036604242155571, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22826098338380793, 0.22826098338380793, 0.29632577970572405], 
reward next is 0.7037, 
noisyNet noise sample is [array([-0.79954195], dtype=float32), -1.2901928]. 
=============================================
[2019-03-26 12:21:11,184] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.95496 ]
 [75.854996]
 [75.8626  ]
 [75.852516]
 [75.8503  ]], R is [[75.82693481]
 [75.77597809]
 [75.72599792]
 [75.67714691]
 [75.62948608]].
[2019-03-26 12:21:13,229] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:21:13,238] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7430
[2019-03-26 12:21:13,243] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 76.33333333333334, 1.0, 2.0, 0.556248390816755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777299.4606370268, 777299.4606370262, 192878.5460170452], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3525600.0000, 
sim time next is 3526200.0000, 
raw observation next is [29.5, 77.0, 1.0, 2.0, 0.5552167309940984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775857.2964768948, 775857.2964768953, 192699.7982324186], 
processed observation next is [1.0, 0.8260869565217391, 0.5971563981042655, 0.77, 1.0, 1.0, 0.4641165433663836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21551591568802633, 0.21551591568802647, 0.2876116391528636], 
reward next is 0.7124, 
noisyNet noise sample is [array([0.7302604], dtype=float32), 0.86421585]. 
=============================================
[2019-03-26 12:21:23,317] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7556490e-24 9.9999988e-01 3.3321202e-31 5.1340545e-28 1.1500341e-07], sum to 1.0000
[2019-03-26 12:21:23,324] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8492
[2019-03-26 12:21:23,332] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2388105.110482139 W.
[2019-03-26 12:21:23,337] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.5, 79.5, 1.0, 2.0, 0.8538553859742815, 1.0, 2.0, 0.8538553859742815, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2388105.110482139, 2388105.110482139, 446944.0777074306], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3400200.0000, 
sim time next is 3400800.0000, 
raw observation next is [29.66666666666667, 78.0, 1.0, 2.0, 0.7982643711673577, 1.0, 2.0, 0.7982643711673577, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2232486.526716406, 2232486.526716406, 419030.3251746919], 
processed observation next is [1.0, 0.34782608695652173, 0.6050552922590839, 0.78, 1.0, 1.0, 0.7569450255028406, 1.0, 1.0, 0.7569450255028406, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6201351463101128, 0.6201351463101128, 0.6254183957831223], 
reward next is 0.3746, 
noisyNet noise sample is [array([-0.03286785], dtype=float32), 0.25448036]. 
=============================================
[2019-03-26 12:21:30,720] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:21:30,729] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1374
[2019-03-26 12:21:30,734] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 77.0, 1.0, 2.0, 0.5552167309940984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775857.2964768948, 775857.2964768953, 192699.7982324186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3526200.0000, 
sim time next is 3526800.0000, 
raw observation next is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.5547762969872104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775241.6114318797, 775241.611431879, 192623.499144762], 
processed observation next is [1.0, 0.8260869565217391, 0.5892575039494474, 0.7766666666666667, 1.0, 1.0, 0.4635858999845909, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21534489206441101, 0.21534489206441082, 0.2874977599175552], 
reward next is 0.7125, 
noisyNet noise sample is [array([0.38877207], dtype=float32), 1.5375904]. 
=============================================
[2019-03-26 12:21:39,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3138923e-22 5.6626779e-01 9.1381530e-31 1.2483862e-13 4.3373224e-01], sum to 1.0000
[2019-03-26 12:21:39,574] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5045
[2019-03-26 12:21:39,582] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2469777.271954267 W.
[2019-03-26 12:21:39,586] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 0.5886853608118946, 1.0, 2.0, 0.5886853608118946, 1.0, 2.0, 1.022351790506537, 6.9112, 6.9112, 170.5573041426782, 2469777.271954267, 2469777.271954267, 481889.026733109], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3680400.0000, 
sim time next is 3681000.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.677739567908175, 6.9112, 168.9090804710887, 2830177.515806561, 2286380.442854245, 474477.2936240162], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.59, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.07665395679081746, 0.0, 0.8294209120246736, 0.786160421057378, 0.6351056785706236, 0.708175065110472], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2897822], dtype=float32), -0.6902299]. 
=============================================
[2019-03-26 12:21:39,607] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[49.37629 ]
 [48.784615]
 [46.967045]
 [46.00187 ]
 [45.517982]], R is [[47.34238815]
 [47.14972687]
 [46.96708298]
 [46.49741364]
 [46.03244019]].
[2019-03-26 12:21:43,640] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 12:21:43,642] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:21:43,643] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:21:43,644] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:21:43,644] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:21:43,645] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:21:43,646] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:21:43,648] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:21:43,649] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:21:43,650] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:21:43,651] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:21:43,670] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run56
[2019-03-26 12:21:43,697] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run56
[2019-03-26 12:21:43,698] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run56
[2019-03-26 12:21:43,744] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run56
[2019-03-26 12:21:43,764] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run56
[2019-03-26 12:21:53,749] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:21:53,751] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.42609741, 83.46103071333333, 1.0, 2.0, 0.1856482594401366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 310543.4431100144, 310543.4431100144, 142558.580830945]
[2019-03-26 12:21:53,751] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:21:53,754] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2895531405250883
[2019-03-26 12:21:59,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:21:59,177] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.46666666666667, 82.83333333333334, 1.0, 2.0, 0.3102009561492267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 508141.3803222649, 508141.3803222643, 167461.6075997728]
[2019-03-26 12:21:59,180] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:21:59,183] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3780523872030337
[2019-03-26 12:22:02,150] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:22:02,152] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.08853709, 74.43887773, 1.0, 2.0, 0.2598865221496576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 427812.1566154228, 427812.1566154228, 161890.0818004336]
[2019-03-26 12:22:02,153] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:22:02,155] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5390248944961604
[2019-03-26 12:22:28,307] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:22:28,308] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.90718317, 89.37720343000001, 1.0, 2.0, 0.2475529385545095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 412044.3170425948, 412044.3170425948, 160234.4416673297]
[2019-03-26 12:22:28,309] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:22:28,313] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9510085475573502
[2019-03-26 12:22:42,663] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:22:42,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.54982068833333, 82.82152526166666, 1.0, 2.0, 0.650312379030251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 908800.3676257946, 908800.367625794, 210495.6237612372]
[2019-03-26 12:22:42,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:22:42,672] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9984899123533131
[2019-03-26 12:23:02,473] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:23:02,475] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.81828349333333, 61.53476246, 1.0, 2.0, 0.5586490711363722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780655.3952779752, 780655.3952779752, 193293.9009453053]
[2019-03-26 12:23:02,478] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:23:02,481] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1941984e-37 0.0000000e+00], sampled 0.4121875863823754
[2019-03-26 12:23:05,487] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:23:05,491] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.23333333333333, 69.0, 1.0, 2.0, 0.6116701208638289, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.947815370579656, 6.9112, 168.9126407390577, 1710241.96355396, 1684265.808021714, 366751.9152342188]
[2019-03-26 12:23:05,494] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:23:05,496] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8718824e-34 1.0000000e+00 0.0000000e+00 2.9711680e-22 1.6545933e-30], sampled 0.929129272494785
[2019-03-26 12:23:05,499] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1710241.96355396 W.
[2019-03-26 12:23:21,119] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:23:21,121] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.07086584, 91.51080740166667, 1.0, 2.0, 0.9375865611228441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1310508.832159945, 1310508.832159945, 280484.5087916638]
[2019-03-26 12:23:21,122] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:23:21,124] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.893847e-38 0.000000e+00], sampled 0.9659207642065359
[2019-03-26 12:23:30,829] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:23:30,831] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.7, 74.0, 1.0, 2.0, 0.6199242854769469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 866316.2097447643, 866316.2097447636, 204516.7273347748]
[2019-03-26 12:23:30,832] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:23:30,836] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32827013787839454
[2019-03-26 12:23:31,668] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.21366821], dtype=float32), 0.4306626]
[2019-03-26 12:23:31,669] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.30721589333334, 60.07207446333334, 1.0, 2.0, 0.6125656860055204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 938382.383242108, 938382.383242108, 213192.860693185]
[2019-03-26 12:23:31,671] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:23:31,673] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.644277858383402
[2019-03-26 12:23:39,439] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.0580 3008053528.2721 1773.0000
[2019-03-26 12:23:39,459] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.6132 3163763739.0272 1840.0000
[2019-03-26 12:23:39,587] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.4354 2927633366.4933 1353.0000
[2019-03-26 12:23:39,712] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.2915 2842798624.0361 1144.0000
[2019-03-26 12:23:39,817] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.2056 2779553025.0034 938.0000
[2019-03-26 12:23:40,834] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1375000, evaluation results [1375000.0, 7876.613211555776, 3163763739.0271726, 1840.0, 8249.435413292298, 2927633366.4933043, 1353.0, 8657.20562587209, 2779553025.0034156, 938.0, 7996.057959396432, 3008053528.2720637, 1773.0, 8494.291528072526, 2842798624.0360622, 1144.0]
[2019-03-26 12:23:45,103] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:23:45,111] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1640
[2019-03-26 12:23:45,117] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 87.33333333333334, 1.0, 2.0, 0.538615001787055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752649.8975572637, 752649.8975572631, 189867.5037448714], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3824400.0000, 
sim time next is 3825000.0000, 
raw observation next is [27.5, 86.5, 1.0, 2.0, 0.540033231347673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754632.4073264824, 754632.4073264824, 190106.2629778988], 
processed observation next is [0.0, 0.2608695652173913, 0.5023696682464456, 0.865, 1.0, 1.0, 0.4458231702984012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2096201131462451, 0.2096201131462451, 0.2837406910117892], 
reward next is 0.7163, 
noisyNet noise sample is [array([-0.9978537], dtype=float32), -0.93833727]. 
=============================================
[2019-03-26 12:23:45,134] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[68.54597]
 [68.55907]
 [68.55923]
 [68.44404]
 [68.44186]], R is [[68.55596161]
 [68.58701324]
 [68.61818695]
 [68.6493454 ]
 [68.68016052]].
[2019-03-26 12:23:48,640] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8191947e-24 1.0000000e+00 4.5981208e-27 2.1024307e-25 3.2181576e-13], sum to 1.0000
[2019-03-26 12:23:48,653] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1163
[2019-03-26 12:23:48,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2174888.985173347 W.
[2019-03-26 12:23:48,668] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.9141830156982687, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.002402726720491, 6.9112, 168.9124140456636, 2174888.985173347, 2110186.839674146, 438148.276953474], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4698000.0000, 
sim time next is 4698600.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.071753232037835, 6.9112, 168.9120297994354, 2401042.71619691, 2287141.352476554, 475895.2467857531], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.75, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.016055323203783535, 0.0, 0.8294353945770642, 0.6669563100546972, 0.6353170423545983, 0.7102914131130643], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5286166], dtype=float32), 0.53403527]. 
=============================================
[2019-03-26 12:23:52,017] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:23:52,028] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8182
[2019-03-26 12:23:52,036] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5944829802785813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 830749.229837461, 830749.2298374603, 199723.4140715139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3941400.0000, 
sim time next is 3942000.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5946767612717427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 831020.1314330462, 831020.1314330468, 199759.1916639667], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5116587485201719, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23083892539806838, 0.23083892539806855, 0.29814804725965177], 
reward next is 0.7019, 
noisyNet noise sample is [array([0.01052098], dtype=float32), -1.2597252]. 
=============================================
[2019-03-26 12:23:52,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.877914]
 [80.83784 ]
 [80.754845]
 [80.65455 ]
 [80.550064]], R is [[80.91139984]
 [80.80419159]
 [80.6980896 ]
 [80.59299469]
 [80.4881134 ]].
[2019-03-26 12:24:00,910] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:24:00,920] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3594
[2019-03-26 12:24:00,925] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.00000000000001, 1.0, 2.0, 0.6144071240269208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858603.1092581487, 858603.1092581487, 203461.6278172622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4396200.0000, 
sim time next is 4396800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6172120494033209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862524.4432381731, 862524.4432381738, 203997.1329587804], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5388096980762902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23959012312171477, 0.23959012312171496, 0.3044733327742991], 
reward next is 0.6955, 
noisyNet noise sample is [array([-1.1427863], dtype=float32), 0.86600935]. 
=============================================
[2019-03-26 12:24:03,143] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.3141780e-34 1.0000000e+00 2.3797433e-38 2.0795596e-35 1.5952604e-23], sum to 1.0000
[2019-03-26 12:24:03,152] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8637
[2019-03-26 12:24:03,157] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 79.83333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.04801514267572, 6.9112, 168.912066365651, 1550882.258341664, 1453821.398811901, 311359.0642958368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4175400.0000, 
sim time next is 4176000.0000, 
raw observation next is [32.0, 79.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.721612155932043, 6.9112, 168.9084636780111, 2029067.733308447, 1454148.750049689, 311359.7393262128], 
processed observation next is [1.0, 0.34782608695652173, 0.7156398104265403, 0.79, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.08104121559320428, 0.0, 0.8294178832882934, 0.5636299259190131, 0.40393020834713583, 0.4647160288450937], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4350273], dtype=float32), 1.0704771]. 
=============================================
[2019-03-26 12:24:03,169] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[53.125347]
 [54.32515 ]
 [54.041824]
 [55.251522]
 [56.71687 ]], R is [[51.16294479]
 [50.6513176 ]
 [50.67802429]
 [50.17124557]
 [50.17988968]].
[2019-03-26 12:24:07,326] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.591381e-29 0.000000e+00], sum to 1.0000
[2019-03-26 12:24:07,334] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5912
[2019-03-26 12:24:07,341] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.66666666666667, 57.33333333333334, 1.0, 2.0, 0.5838024596998019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815818.2022191727, 815818.2022191727, 197768.6241460458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4213200.0000, 
sim time next is 4213800.0000, 
raw observation next is [34.5, 58.0, 1.0, 2.0, 0.5827109087956605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814292.2606402056, 814292.260640205, 197570.7189601218], 
processed observation next is [1.0, 0.782608695652174, 0.8341232227488152, 0.58, 1.0, 1.0, 0.4972420587899523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22619229462227933, 0.22619229462227916, 0.29488167008973404], 
reward next is 0.7051, 
noisyNet noise sample is [array([0.36465204], dtype=float32), -0.8672356]. 
=============================================
[2019-03-26 12:24:13,572] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:24:13,583] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2935
[2019-03-26 12:24:13,588] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6196549133722872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865939.6209307091, 865939.6209307091, 204465.4127909181], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4312800.0000, 
sim time next is 4313400.0000, 
raw observation next is [31.0, 79.00000000000001, 1.0, 2.0, 0.6225927536582418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 870046.8009729541, 870046.8009729536, 205030.9167263089], 
processed observation next is [1.0, 0.9565217391304348, 0.6682464454976303, 0.7900000000000001, 1.0, 1.0, 0.5452924742870383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24167966693693171, 0.24167966693693155, 0.30601629362135657], 
reward next is 0.6940, 
noisyNet noise sample is [array([-0.18921514], dtype=float32), -0.59099805]. 
=============================================
[2019-03-26 12:24:16,241] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.3206912e-33 1.0000000e+00 0.0000000e+00 3.1799417e-25 8.5376929e-27], sum to 1.0000
[2019-03-26 12:24:16,250] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4269
[2019-03-26 12:24:16,255] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2687181.73704914 W.
[2019-03-26 12:24:16,261] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.0, 54.00000000000001, 1.0, 2.0, 0.6404492781971619, 1.0, 2.0, 0.6404492781971619, 1.0, 2.0, 1.03, 7.003666155306047, 6.9112, 170.5573041426782, 2687181.73704914, 2620944.468720702, 503112.6866906177], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4364400.0000, 
sim time next is 4365000.0000, 
raw observation next is [37.0, 54.0, 1.0, 2.0, 0.5905984932448676, 1.0, 2.0, 0.5905984932448676, 1.0, 2.0, 1.025674268860047, 6.911200000000001, 6.9112, 170.5573041426782, 2477811.607621133, 2477811.607621132, 483450.7975400405], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.54, 1.0, 1.0, 0.5067451725841777, 1.0, 1.0, 0.5067451725841777, 1.0, 1.0, 1.0313100839756673, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6882810021169814, 0.6882810021169811, 0.7215683545373739], 
reward next is 0.2784, 
noisyNet noise sample is [array([0.28818586], dtype=float32), -1.0475826]. 
=============================================
[2019-03-26 12:24:16,282] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[46.60275 ]
 [43.26468 ]
 [42.33891 ]
 [41.060852]
 [40.490685]], R is [[48.44958496]
 [47.96508789]
 [47.4854393 ]
 [47.01058578]
 [46.54048157]].
[2019-03-26 12:24:19,509] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:24:19,519] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1827
[2019-03-26 12:24:19,525] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 85.66666666666667, 1.0, 2.0, 0.5950050426232064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 831479.0617492531, 831479.0617492524, 199818.7971498834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4419600.0000, 
sim time next is 4420200.0000, 
raw observation next is [29.0, 84.83333333333333, 1.0, 2.0, 0.5906901144109165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825446.9001150929, 825446.9001150929, 199024.0094299826], 
processed observation next is [0.0, 0.13043478260869565, 0.5734597156398105, 0.8483333333333333, 1.0, 1.0, 0.5068555595312246, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2292908055875258, 0.2292908055875258, 0.2970507603432576], 
reward next is 0.7029, 
noisyNet noise sample is [array([-0.37750116], dtype=float32), -1.488217]. 
=============================================
[2019-03-26 12:24:21,813] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:24:21,823] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9751
[2019-03-26 12:24:21,833] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.83333333333334, 1.0, 2.0, 0.4903505526481806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685184.3639620292, 685184.3639620292, 182097.7699068118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4914600.0000, 
sim time next is 4915200.0000, 
raw observation next is [27.0, 80.66666666666667, 1.0, 2.0, 0.4957283208447902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692701.3618745023, 692701.3618745023, 182929.238097918], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.8066666666666668, 1.0, 1.0, 0.3924437600539641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1924170449651395, 0.1924170449651395, 0.27302871357898206], 
reward next is 0.7270, 
noisyNet noise sample is [array([1.0209553], dtype=float32), 0.9537369]. 
=============================================
[2019-03-26 12:24:23,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:24:23,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9002
[2019-03-26 12:24:23,100] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5527835446855871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772455.9368315182, 772455.9368315176, 192278.7896705648], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4482600.0000, 
sim time next is 4483200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5501475598523213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 191825.7660821197], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4580091082558088, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2135475268641311, 0.21354752686413092, 0.2863071135554025], 
reward next is 0.7137, 
noisyNet noise sample is [array([-1.3633227], dtype=float32), -0.69831103]. 
=============================================
[2019-03-26 12:24:23,580] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:24:23,586] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6595
[2019-03-26 12:24:23,590] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5183244166576846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724286.5706564924, 724286.5706564924, 186516.2963389615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4489800.0000, 
sim time next is 4490400.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5162255930472541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721352.7595897908, 721352.7595897908, 186176.7387330674], 
processed observation next is [0.0, 1.0, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.41713926873163143, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2003757665527197, 0.2003757665527197, 0.2778757294523394], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.13991159], dtype=float32), 1.0313739]. 
=============================================
[2019-03-26 12:24:28,463] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5342814e-23 4.9754636e-12 4.9296990e-26 2.0802526e-25 1.0000000e+00], sum to 1.0000
[2019-03-26 12:24:28,470] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2664
[2019-03-26 12:24:28,479] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 85.66666666666667, 1.0, 2.0, 0.4258152922637329, 1.0, 2.0, 0.4258152922637329, 1.0, 2.0, 0.7395003433931079, 6.9112, 6.9112, 170.5573041426782, 1785931.342300821, 1785931.342300821, 367638.7691273548], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4588800.0000, 
sim time next is 4589400.0000, 
raw observation next is [28.0, 84.83333333333333, 1.0, 2.0, 0.3979288058605365, 1.0, 2.0, 0.3979288058605365, 1.0, 2.0, 0.6899075193415749, 6.9112, 6.9112, 170.5573041426782, 1668880.281342294, 1668880.281342294, 351583.896331193], 
processed observation next is [1.0, 0.08695652173913043, 0.5260663507109005, 0.8483333333333333, 1.0, 1.0, 0.27461301910908015, 1.0, 1.0, 0.27461301910908015, 1.0, 1.0, 0.6218384382214328, 0.0, 0.0, 0.8375144448122397, 0.463577855928415, 0.463577855928415, 0.5247520840764074], 
reward next is 0.4752, 
noisyNet noise sample is [array([0.5656074], dtype=float32), -0.8329191]. 
=============================================
[2019-03-26 12:24:33,748] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.132808e-28], sum to 1.0000
[2019-03-26 12:24:33,755] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6455
[2019-03-26 12:24:33,762] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5069772357225876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708425.1698864554, 708425.169886456, 184696.6834716284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4669200.0000, 
sim time next is 4669800.0000, 
raw observation next is [27.0, 84.83333333333333, 1.0, 2.0, 0.5101189259999527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712816.6870251385, 712816.6870251378, 185196.9682904144], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.8483333333333333, 1.0, 1.0, 0.40978183855415984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1980046352847607, 0.1980046352847605, 0.2764133855080812], 
reward next is 0.7236, 
noisyNet noise sample is [array([-1.4403664], dtype=float32), 0.2533799]. 
=============================================
[2019-03-26 12:24:35,016] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 12:24:35,017] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:24:35,018] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:24:35,019] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:24:35,020] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:24:35,020] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:24:35,022] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:24:35,023] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:24:35,021] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:24:35,026] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:24:35,025] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:24:35,054] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run57
[2019-03-26 12:24:35,055] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run57
[2019-03-26 12:24:35,102] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run57
[2019-03-26 12:24:35,126] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run57
[2019-03-26 12:24:35,145] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run57
[2019-03-26 12:24:37,781] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:24:37,782] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.75, 79.0, 1.0, 2.0, 0.3526506101813654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546855.0561187116, 546855.056118711, 170480.7606230683]
[2019-03-26 12:24:37,784] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:24:37,788] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07356131717267766
[2019-03-26 12:24:42,176] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:24:42,177] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.7, 90.5, 1.0, 2.0, 0.3213465040497456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504999.9986074638, 504999.9986074632, 167333.8199578223]
[2019-03-26 12:24:42,179] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:24:42,183] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7725159847983156
[2019-03-26 12:24:49,583] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:24:49,586] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.1, 76.66666666666667, 1.0, 2.0, 0.2813540754065097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 455357.9960442733, 455357.9960442726, 163950.6832403006]
[2019-03-26 12:24:49,587] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:24:49,588] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9133049204292212
[2019-03-26 12:24:49,792] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:24:49,794] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.66780675333333, 77.08827770666667, 1.0, 2.0, 0.3546646204984896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542298.4410509589, 542298.4410509595, 169882.403511407]
[2019-03-26 12:24:49,796] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:24:49,799] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9651189327602515
[2019-03-26 12:24:51,517] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:24:51,519] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.66666666666667, 87.33333333333334, 1.0, 2.0, 0.49236697089481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 781061.3879490444, 781061.3879490449, 192992.9151911481]
[2019-03-26 12:24:51,520] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:24:51,523] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7531604124934119
[2019-03-26 12:25:35,386] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:25:35,388] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.00662144666667, 90.11444481666666, 1.0, 2.0, 0.7274491167557777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1016649.22902149, 1016649.229021489, 226879.6468535025]
[2019-03-26 12:25:35,391] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:25:35,396] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1344596e-38], sampled 0.0044063817483459555
[2019-03-26 12:25:38,799] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:25:38,800] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.56386405, 69.02929567000001, 1.0, 2.0, 0.6082876708877674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850048.0586334479, 850048.0586334479, 202300.8124299003]
[2019-03-26 12:25:38,802] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:25:38,805] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.46037689066578147
[2019-03-26 12:25:53,028] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:25:53,030] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.6, 77.0, 1.0, 2.0, 0.8313356181237102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161915.629275047, 1161915.629275047, 251749.934258082]
[2019-03-26 12:25:53,031] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:25:53,035] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1780765e-31], sampled 0.26635761237561406
[2019-03-26 12:26:06,366] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:26:06,368] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.19571733333333, 79.90646401666666, 1.0, 2.0, 0.6949502829852422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 971209.5940216926, 971209.5940216926, 219766.6283161367]
[2019-03-26 12:26:06,369] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:26:06,372] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4527247746788796
[2019-03-26 12:26:25,018] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17618164], dtype=float32), 0.4629042]
[2019-03-26 12:26:25,020] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.3, 62.0, 1.0, 2.0, 0.3845478764326002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 583162.8233306999, 583162.8233306999, 173272.2236452956]
[2019-03-26 12:26:25,023] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:26:25,025] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.658876612746215
[2019-03-26 12:26:30,725] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7929.1115 3162249181.1363 1764.0000
[2019-03-26 12:26:31,443] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8037.4734 3006406433.4466 1659.0000
[2019-03-26 12:26:31,494] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8523.2868 2841278610.5107 1068.0000
[2019-03-26 12:26:31,525] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8270.4126 2927572577.5544 1312.0000
[2019-03-26 12:26:31,551] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8678.5581 2779247498.3095 891.0000
[2019-03-26 12:26:32,567] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1400000, evaluation results [1400000.0, 7929.11150561618, 3162249181.1363244, 1764.0, 8270.412645788725, 2927572577.5543566, 1312.0, 8678.558138617873, 2779247498.3094563, 891.0, 8037.473433801907, 3006406433.446626, 1659.0, 8523.286776085895, 2841278610.5106993, 1068.0]
[2019-03-26 12:26:34,616] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3316040e-24 6.2627864e-01 7.5221172e-26 1.0926203e-28 3.7372133e-01], sum to 1.0000
[2019-03-26 12:26:34,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1861
[2019-03-26 12:26:34,636] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1698515.122769473 W.
[2019-03-26 12:26:34,643] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.23333333333333, 95.0, 1.0, 2.0, 0.6074793544462477, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.920771471725332, 6.9112, 168.9122488835891, 1698515.122769473, 1691724.819749616, 366660.4525854584], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5538000.0000, 
sim time next is 5538600.0000, 
raw observation next is [26.2, 95.0, 1.0, 2.0, 0.5086861770946968, 1.0, 1.0, 0.5086861770946968, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1422094.699434403, 1422094.699434403, 302973.4183357868], 
processed observation next is [1.0, 0.08695652173913043, 0.44075829383886256, 0.95, 1.0, 1.0, 0.4080556350538515, 1.0, 0.5, 0.4080556350538515, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3950263053984453, 0.3950263053984453, 0.45219913184445787], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.810164], dtype=float32), 0.3475613]. 
=============================================
[2019-03-26 12:26:35,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2210048e-24 9.9998689e-01 2.8233700e-31 1.2050098e-22 1.3089056e-05], sum to 1.0000
[2019-03-26 12:26:35,815] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6310
[2019-03-26 12:26:35,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2289874.323110449 W.
[2019-03-26 12:26:35,825] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 65.0, 1.0, 2.0, 0.8187655937303515, 1.0, 2.0, 0.8187655937303515, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2289874.323110449, 2289874.323110449, 429108.5404475905], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4886400.0000, 
sim time next is 4887000.0000, 
raw observation next is [31.5, 64.5, 1.0, 2.0, 0.5657708393579233, 1.0, 2.0, 0.5657708393579233, 1.0, 1.0, 0.9825568446890314, 6.911199999999999, 6.9112, 170.5573041426782, 2373550.124578681, 2373550.124578682, 463606.5285778407], 
processed observation next is [1.0, 0.5652173913043478, 0.6919431279620853, 0.645, 1.0, 1.0, 0.47683233657581114, 1.0, 1.0, 0.47683233657581114, 1.0, 0.5, 0.9787278593768676, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6593194790496336, 0.6593194790496338, 0.6919500426534936], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0623719], dtype=float32), 0.39462537]. 
=============================================
[2019-03-26 12:26:35,839] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[49.723682]
 [51.346237]
 [52.357056]
 [52.105328]
 [52.42455 ]], R is [[48.04413605]
 [47.92323303]
 [47.8373642 ]
 [47.36454773]
 [46.89090347]].
[2019-03-26 12:26:37,426] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:26:37,434] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9554
[2019-03-26 12:26:37,438] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.6504004919059506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 908923.556545309, 908923.5565453096, 210500.1410584522], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4770600.0000, 
sim time next is 4771200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6452299467925651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 901694.7408924326, 901694.7408924319, 209463.995573637], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5725662009548976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2504707613590091, 0.25047076135900886, 0.31263282921438357], 
reward next is 0.6874, 
noisyNet noise sample is [array([0.43778968], dtype=float32), -0.6386955]. 
=============================================
[2019-03-26 12:26:38,542] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0443840e-31 9.9993956e-01 8.3319662e-36 8.6789645e-27 6.0473179e-05], sum to 1.0000
[2019-03-26 12:26:38,551] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3893
[2019-03-26 12:26:38,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2470761.401888269 W.
[2019-03-26 12:26:38,565] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333333, 63.5, 1.0, 2.0, 0.5889197020548521, 1.0, 2.0, 0.5889197020548521, 1.0, 2.0, 1.022758763747721, 6.911200000000001, 6.9112, 170.5573041426782, 2470761.401888269, 2470761.401888268, 482079.4981407667], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4809000.0000, 
sim time next is 4809600.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.8835612635676653, 1.0, 2.0, 0.8835612635676653, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2471270.138158389, 2471270.138158389, 462600.6642029311], 
processed observation next is [1.0, 0.6956521739130435, 0.7156398104265403, 0.63, 1.0, 1.0, 0.8597123657441751, 1.0, 1.0, 0.8597123657441751, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6864639272662192, 0.6864639272662192, 0.6904487525416882], 
reward next is 0.3096, 
noisyNet noise sample is [array([-0.21971913], dtype=float32), 1.2068775]. 
=============================================
[2019-03-26 12:26:44,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1570354e-36 1.0000000e+00 2.1519797e-37 6.9610833e-36 2.1985299e-14], sum to 1.0000
[2019-03-26 12:26:44,967] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0168
[2019-03-26 12:26:44,971] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.63333333333333, 84.33333333333334, 1.0, 2.0, 0.6051297231927679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845633.2462582663, 845633.2462582663, 201705.7104423832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5358000.0000, 
sim time next is 5358600.0000, 
raw observation next is [29.6, 84.5, 1.0, 2.0, 0.6041303385577099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844236.1098871104, 844236.109887111, 201518.2010715595], 
processed observation next is [1.0, 0.0, 0.6018957345971565, 0.845, 1.0, 1.0, 0.5230486006719396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23451003052419733, 0.2345100305241975, 0.30077343443516347], 
reward next is 0.6992, 
noisyNet noise sample is [array([-1.6505665], dtype=float32), 0.4893726]. 
=============================================
[2019-03-26 12:26:45,501] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8770288e-24], sum to 1.0000
[2019-03-26 12:26:45,509] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6838
[2019-03-26 12:26:45,513] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5033067450059844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703294.5079751044, 703294.5079751044, 184115.6964239863], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4931400.0000, 
sim time next is 4932000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5016176043346112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 700933.41246658, 700933.4124665793, 183849.7965545334], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.3995392823308569, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19470372568516112, 0.19470372568516092, 0.2744026814246767], 
reward next is 0.7256, 
noisyNet noise sample is [array([1.0758351], dtype=float32), -0.09707243]. 
=============================================
[2019-03-26 12:26:45,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.60113]
 [71.62   ]
 [71.70946]
 [71.79205]
 [71.8516 ]], R is [[72.03943634]
 [72.04424286]
 [72.04864502]
 [72.05268097]
 [72.05632019]].
[2019-03-26 12:26:49,242] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.020712e-38], sum to 1.0000
[2019-03-26 12:26:49,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2459
[2019-03-26 12:26:49,261] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.5172741435004169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722818.4600814283, 722818.4600814289, 186347.3689148637], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4993200.0000, 
sim time next is 4993800.0000, 
raw observation next is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5188867101759513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725072.5661193123, 725072.5661193117, 186608.4977729133], 
processed observation next is [1.0, 0.8260869565217391, 0.5655608214849924, 0.7483333333333334, 1.0, 1.0, 0.4203454339469293, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20140904614425342, 0.20140904614425326, 0.27852014592972135], 
reward next is 0.7215, 
noisyNet noise sample is [array([0.04103799], dtype=float32), -0.83993614]. 
=============================================
[2019-03-26 12:26:53,127] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:26:53,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9936
[2019-03-26 12:26:53,145] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5364795520307343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749664.8095138671, 749664.8095138664, 189509.7604375454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5040000.0000, 
sim time next is 5040600.0000, 
raw observation next is [28.33333333333334, 81.0, 1.0, 2.0, 0.5360444949806962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749056.655939734, 749056.6559397334, 189436.6097146941], 
processed observation next is [0.0, 0.34782608695652173, 0.5418641390205374, 0.81, 1.0, 1.0, 0.44101746383216406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20807129331659277, 0.2080712933165926, 0.28274120852939416], 
reward next is 0.7173, 
noisyNet noise sample is [array([1.9390115], dtype=float32), -0.55363303]. 
=============================================
[2019-03-26 12:26:54,323] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:26:54,334] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9922
[2019-03-26 12:26:54,337] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5224838780806448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730100.8392509491, 730100.8392509497, 187193.9312395524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5061600.0000, 
sim time next is 5062200.0000, 
raw observation next is [31.16666666666667, 62.33333333333333, 1.0, 2.0, 0.5346691542317358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747134.109469819, 747134.109469819, 189205.5921577475], 
processed observation next is [0.0, 0.6086956521739131, 0.6761453396524489, 0.6233333333333333, 1.0, 1.0, 0.4393604267852238, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20753725263050526, 0.20753725263050526, 0.2823964062055933], 
reward next is 0.7176, 
noisyNet noise sample is [array([0.45687202], dtype=float32), -0.039549448]. 
=============================================
[2019-03-26 12:27:01,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.20738295e-20 2.13483162e-02 1.52484889e-28 1.46440069e-17
 9.78651702e-01], sum to 1.0000
[2019-03-26 12:27:01,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7005
[2019-03-26 12:27:01,295] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.1, 53.0, 1.0, 2.0, 0.7229114794376699, 1.0, 2.0, 0.6820457792330976, 1.0, 2.0, 1.03, 7.005099538774358, 6.9112, 170.5573041426782, 2861911.256066955, 2794647.196853854, 528664.0351093449], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5321400.0000, 
sim time next is 5322000.0000, 
raw observation next is [36.1, 53.0, 1.0, 2.0, 0.7767104737627327, 1.0, 2.0, 0.708945276395629, 1.0, 2.0, 1.03, 7.005103781485239, 6.9112, 170.5573041426782, 2974917.817537045, 2907650.719097557, 546652.9849103169], 
processed observation next is [1.0, 0.6086956521739131, 0.9099526066350712, 0.53, 1.0, 1.0, 0.730976474412931, 1.0, 1.0, 0.6493316583079867, 1.0, 1.0, 1.0365853658536586, 0.009390378148523925, 0.0, 0.8375144448122397, 0.8263660604269569, 0.8076807553048769, 0.8158999774780848], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7249879], dtype=float32), 0.2323056]. 
=============================================
[2019-03-26 12:27:01,307] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[42.710857]
 [42.134453]
 [40.912395]
 [40.94075 ]
 [42.200623]], R is [[41.71816254]
 [41.30097961]
 [40.88796997]
 [40.47909164]
 [40.07430267]].
[2019-03-26 12:27:02,809] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:27:02,818] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9613
[2019-03-26 12:27:02,824] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.7, 56.33333333333333, 1.0, 2.0, 0.5287573612843998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738870.2358272427, 738870.2358272427, 188227.1951197892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5506800.0000, 
sim time next is 5507400.0000, 
raw observation next is [33.45, 57.66666666666667, 1.0, 2.0, 0.5341408854567656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746395.6597459071, 746395.6597459076, 189120.6253001515], 
processed observation next is [1.0, 0.7391304347826086, 0.7843601895734599, 0.5766666666666667, 1.0, 1.0, 0.4387239583816453, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20733212770719642, 0.20733212770719656, 0.2822695900002261], 
reward next is 0.7177, 
noisyNet noise sample is [array([-0.9755551], dtype=float32), 0.42834193]. 
=============================================
[2019-03-26 12:27:03,925] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:27:03,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0176
[2019-03-26 12:27:03,934] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.81666666666667, 65.66666666666667, 1.0, 2.0, 0.5311470644255317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742210.7044822134, 742210.7044822134, 188620.4143638385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5680200.0000, 
sim time next is 5680800.0000, 
raw observation next is [30.6, 67.0, 1.0, 2.0, 0.5343108440677539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746633.2392995686, 746633.239299568, 189146.6838414601], 
processed observation next is [0.0, 0.782608695652174, 0.6492890995260664, 0.67, 1.0, 1.0, 0.4389287277924745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20739812202765795, 0.20739812202765778, 0.2823084833454628], 
reward next is 0.7177, 
noisyNet noise sample is [array([0.67111105], dtype=float32), 1.0705533]. 
=============================================
[2019-03-26 12:27:10,756] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8132344e-22 9.9796498e-01 1.3439777e-33 5.3023911e-18 2.0350998e-03], sum to 1.0000
[2019-03-26 12:27:10,764] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0855
[2019-03-26 12:27:10,771] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2994821.945834619 W.
[2019-03-26 12:27:10,776] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.7, 58.0, 1.0, 2.0, 0.7861857210474713, 1.0, 2.0, 0.7136829000379982, 1.0, 2.0, 1.03, 7.005104528804228, 6.9112, 170.5573041426782, 2994821.945834619, 2927554.312060141, 549933.6174613422], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5486400.0000, 
sim time next is 5487000.0000, 
raw observation next is [35.8, 57.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.309908616084964, 6.9112, 170.5573041426782, 3195273.620986896, 2909662.420855676, 551531.6116575437], 
processed observation next is [1.0, 0.5217391304347826, 0.895734597156398, 0.57, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.03987086160849636, 0.0, 0.8375144448122397, 0.8875760058296933, 0.8082395613487988, 0.8231815099366324], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.39166772], dtype=float32), -0.48776013]. 
=============================================
[2019-03-26 12:27:10,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[45.232582]
 [44.151875]
 [44.37161 ]
 [45.625854]
 [44.49187 ]], R is [[43.68633652]
 [43.24947357]
 [42.81697845]
 [42.3888092 ]
 [41.96492004]].
[2019-03-26 12:27:16,531] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:27:16,541] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5118
[2019-03-26 12:27:16,547] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 53.0, 1.0, 2.0, 0.5359444086305802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748916.7481963548, 748916.7481963541, 189420.552626323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5756400.0000, 
sim time next is 5757000.0000, 
raw observation next is [33.81666666666667, 53.83333333333334, 1.0, 2.0, 0.5579740119941156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779711.722389029, 779711.722389029, 193177.2254882831], 
processed observation next is [0.0, 0.6521739130434783, 0.8017377567140602, 0.5383333333333334, 1.0, 1.0, 0.4674385686676091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21658658955250806, 0.21658658955250806, 0.28832421714669115], 
reward next is 0.7117, 
noisyNet noise sample is [array([0.6302925], dtype=float32), 0.2398129]. 
=============================================
[2019-03-26 12:27:16,559] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.11597 ]
 [74.01049 ]
 [74.019485]
 [74.00788 ]
 [73.983116]], R is [[73.97915649]
 [73.95664978]
 [73.93569183]
 [73.91604614]
 [73.89785004]].
[2019-03-26 12:27:25,439] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.76626212e-16 9.99884963e-01 4.71876034e-25 1.11778216e-04
 3.27215503e-06], sum to 1.0000
[2019-03-26 12:27:25,448] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5904
[2019-03-26 12:27:25,458] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2418922.935168427 W.
[2019-03-26 12:27:25,461] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.53333333333333, 52.0, 1.0, 2.0, 0.8648634951359877, 1.0, 2.0, 0.8648634951359877, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2418922.935168427, 2418922.935168427, 452678.0594423977], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5586000.0000, 
sim time next is 5586600.0000, 
raw observation next is [33.46666666666666, 52.5, 1.0, 2.0, 0.5778496119149105, 1.0, 2.0, 0.5778496119149105, 1.0, 1.0, 0.9961952676744207, 6.911200000000001, 6.9112, 170.5573041426782, 2424272.748952238, 2424272.748952237, 471595.8309069578], 
processed observation next is [1.0, 0.6521739130434783, 0.7851500789889413, 0.525, 1.0, 1.0, 0.49138507459627767, 1.0, 1.0, 0.49138507459627767, 1.0, 0.5, 0.9953600825297811, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6734090969311772, 0.6734090969311769, 0.7038743744879967], 
reward next is 0.2961, 
noisyNet noise sample is [array([-1.6788777], dtype=float32), -1.5622934]. 
=============================================
[2019-03-26 12:27:27,157] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-26 12:27:27,159] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:27:27,161] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:27:27,161] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:27:27,162] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:27:27,164] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:27:27,165] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:27:27,167] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:27:27,168] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:27:27,166] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:27:27,171] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:27:27,193] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run58
[2019-03-26 12:27:27,217] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run58
[2019-03-26 12:27:27,242] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run58
[2019-03-26 12:27:27,262] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run58
[2019-03-26 12:27:27,263] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run58
[2019-03-26 12:27:33,103] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.14446667], dtype=float32), 0.46631476]
[2019-03-26 12:27:33,106] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.52477131, 80.6911074, 1.0, 2.0, 0.267837439507755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 438709.523191856, 438709.5231918566, 162715.124599222]
[2019-03-26 12:27:33,107] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:27:33,110] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23616881813099344
[2019-03-26 12:27:55,309] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.14446667], dtype=float32), 0.46631476]
[2019-03-26 12:27:55,311] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.83962977666667, 81.50136602333333, 1.0, 2.0, 0.8313474086546171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161932.117318587, 1161932.117318587, 251755.4938229596]
[2019-03-26 12:27:55,313] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:27:55,318] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9545197e-37 1.8501974e-36], sampled 0.7028314391801387
[2019-03-26 12:28:18,724] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.14446667], dtype=float32), 0.46631476]
[2019-03-26 12:28:18,725] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.43899971666667, 85.42095266333334, 1.0, 2.0, 0.9981246044466612, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005993260244413, 6.9112, 168.9123159258886, 2292377.076341299, 2225127.730028692, 462729.6652767183]
[2019-03-26 12:28:18,727] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:28:18,730] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.3450663e-18 9.8917472e-01 7.9961949e-25 1.2423424e-14 1.0825351e-02], sampled 0.2494427865947707
[2019-03-26 12:28:18,734] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2292377.076341299 W.
[2019-03-26 12:28:23,880] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.14446667], dtype=float32), 0.46631476]
[2019-03-26 12:28:23,882] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.0, 58.0, 1.0, 2.0, 0.6138748972006653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857859.0486237034, 857859.0486237034, 203359.9688358732]
[2019-03-26 12:28:23,884] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:28:23,888] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8621767777827758
[2019-03-26 12:28:23,930] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.14446667], dtype=float32), 0.46631476]
[2019-03-26 12:28:23,931] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.55940993, 77.58423242, 1.0, 2.0, 0.4930465978344014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691973.4599417766, 691973.459941776, 182901.1846966126]
[2019-03-26 12:28:23,933] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:28:23,937] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4579839518739005
[2019-03-26 12:28:29,004] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.14446667], dtype=float32), 0.46631476]
[2019-03-26 12:28:29,007] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.72033601, 76.63273229, 1.0, 2.0, 0.5569276061341126, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9491450071319993, 6.9112, 6.9112, 168.9125980280361, 1557068.514089907, 1557068.514089907, 337011.8667101213]
[2019-03-26 12:28:29,008] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:28:29,011] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.6614911e-32 1.0000000e+00 0.0000000e+00 3.5437745e-18 7.5779974e-31], sampled 0.5481423722528662
[2019-03-26 12:28:49,946] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.14446667], dtype=float32), 0.46631476]
[2019-03-26 12:28:49,951] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.36666666666667, 93.00000000000001, 1.0, 2.0, 0.6165569323062552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861608.5769588333, 861608.5769588333, 203870.5887991779]
[2019-03-26 12:28:49,951] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:28:49,956] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.39210238524708685
[2019-03-26 12:29:07,840] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.14446667], dtype=float32), 0.46631476]
[2019-03-26 12:29:07,843] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.35, 86.5, 1.0, 2.0, 0.5413104819095325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756417.8490335743, 756417.8490335743, 190321.2972628213]
[2019-03-26 12:29:07,844] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:29:07,850] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07288921196786702
[2019-03-26 12:29:23,025] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7984.3990 3159550536.5135 1622.0000
[2019-03-26 12:29:23,467] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8697.3476 2778904629.5802 850.0000
[2019-03-26 12:29:23,498] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8095.3462 3002047710.5815 1510.0000
[2019-03-26 12:29:23,504] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8300.7159 2927152813.9174 1236.0000
[2019-03-26 12:29:23,552] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8561.5480 2839702208.0627 976.0000
[2019-03-26 12:29:24,569] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1425000, evaluation results [1425000.0, 7984.398983793849, 3159550536.5134945, 1622.0, 8300.71588721291, 2927152813.9173717, 1236.0, 8697.3476041742, 2778904629.580178, 850.0, 8095.346247721418, 3002047710.581501, 1510.0, 8561.548047392063, 2839702208.0626645, 976.0]
[2019-03-26 12:29:25,940] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:29:25,950] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1179
[2019-03-26 12:29:25,955] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 85.16666666666667, 1.0, 2.0, 0.5062960044907171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707472.9337201746, 707472.9337201746, 184588.4830706276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5638200.0000, 
sim time next is 5638800.0000, 
raw observation next is [26.90000000000001, 84.33333333333334, 1.0, 2.0, 0.5078105802265798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709590.0336350915, 709590.0336350915, 184828.9231312728], 
processed observation next is [0.0, 0.2608695652173913, 0.4739336492891, 0.8433333333333334, 1.0, 1.0, 0.4070006990681684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1971083426764143, 0.1971083426764143, 0.275864064375034], 
reward next is 0.7241, 
noisyNet noise sample is [array([1.5398077], dtype=float32), 1.3859895]. 
=============================================
[2019-03-26 12:29:26,653] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1702800e-35 3.0160075e-25], sum to 1.0000
[2019-03-26 12:29:26,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1849
[2019-03-26 12:29:26,666] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 89.66666666666666, 1.0, 2.0, 0.53079302813172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741715.8109090619, 741715.8109090626, 188561.3232133546], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6483000.0000, 
sim time next is 6483600.0000, 
raw observation next is [26.7, 90.0, 1.0, 2.0, 0.5309965257489656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742000.2722897487, 742000.2722897487, 188595.0932419482], 
processed observation next is [1.0, 0.043478260869565216, 0.46445497630331756, 0.9, 1.0, 1.0, 0.4349355731915248, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2061111867471524, 0.2061111867471524, 0.2814852137939525], 
reward next is 0.7185, 
noisyNet noise sample is [array([2.2022934], dtype=float32), -0.12528987]. 
=============================================
[2019-03-26 12:29:30,556] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:29:30,569] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2244
[2019-03-26 12:29:30,576] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.85, 89.0, 1.0, 2.0, 0.5302566578761228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740966.0402176224, 740966.0402176218, 188472.5355455579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5873400.0000, 
sim time next is 5874000.0000, 
raw observation next is [26.8, 89.33333333333334, 1.0, 2.0, 0.5298431296027968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740387.9858188217, 740387.985818821, 188404.0809221939], 
processed observation next is [1.0, 1.0, 0.4691943127962086, 0.8933333333333334, 1.0, 1.0, 0.433545939280478, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20566332939411713, 0.20566332939411694, 0.2812001207793939], 
reward next is 0.7188, 
noisyNet noise sample is [array([-0.19616054], dtype=float32), -1.1826922]. 
=============================================
[2019-03-26 12:29:30,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.829796]
 [70.845345]
 [70.85224 ]
 [70.83977 ]
 [70.68824 ]], R is [[70.83359528]
 [70.84395599]
 [70.85406494]
 [70.86386108]
 [70.87321472]].
[2019-03-26 12:29:32,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7469575e-38], sum to 1.0000
[2019-03-26 12:29:32,708] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1656
[2019-03-26 12:29:32,713] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333333, 84.33333333333333, 1.0, 2.0, 0.5468479034284822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764158.5288643345, 764158.5288643345, 191261.2868749911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5778600.0000, 
sim time next is 5779200.0000, 
raw observation next is [27.76666666666667, 84.66666666666667, 1.0, 2.0, 0.5463694601758492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763489.7178750446, 763489.717875044, 191179.7054464957], 
processed observation next is [0.0, 0.9130434782608695, 0.515007898894155, 0.8466666666666667, 1.0, 1.0, 0.45345718093475806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21208047718751238, 0.21208047718751222, 0.2853428439499936], 
reward next is 0.7147, 
noisyNet noise sample is [array([-0.55405325], dtype=float32), 0.34304026]. 
=============================================
[2019-03-26 12:29:34,038] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.7693471e-16 6.1982595e-03 2.0862728e-28 5.5375898e-07 9.9380118e-01], sum to 1.0000
[2019-03-26 12:29:34,045] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4033
[2019-03-26 12:29:34,052] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.5614451103431051, 1.0, 2.0, 0.5614451103431051, 1.0, 2.0, 0.9709145156837155, 6.9112, 6.9112, 170.5573041426782, 2355385.50732503, 2355385.50732503, 459389.025642855], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6091200.0000, 
sim time next is 6091800.0000, 
raw observation next is [31.01666666666667, 65.0, 1.0, 2.0, 0.4655801178652571, 1.0, 2.0, 0.4655801178652571, 1.0, 2.0, 0.8043614751369506, 6.9112, 6.9112, 170.5573041426782, 1952862.801105285, 1952862.801105285, 391461.0688253737], 
processed observation next is [1.0, 0.5217391304347826, 0.6690363349131123, 0.65, 1.0, 1.0, 0.3561206239340447, 1.0, 1.0, 0.3561206239340447, 1.0, 1.0, 0.7614164330938421, 0.0, 0.0, 0.8375144448122397, 0.5424618891959125, 0.5424618891959125, 0.5842702519781697], 
reward next is 0.4157, 
noisyNet noise sample is [array([-0.17685574], dtype=float32), 1.6766161]. 
=============================================
[2019-03-26 12:29:39,358] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:29:39,369] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1027
[2019-03-26 12:29:39,374] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 87.66666666666667, 1.0, 2.0, 0.5391237589005263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753361.0767429426, 753361.0767429426, 189952.7347433777], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5870400.0000, 
sim time next is 5871000.0000, 
raw observation next is [27.08333333333333, 87.83333333333334, 1.0, 2.0, 0.5359402213410656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748910.894908808, 748910.8949088088, 189418.5853816767], 
processed observation next is [1.0, 0.9565217391304348, 0.4826224328593995, 0.8783333333333334, 1.0, 1.0, 0.44089183294104284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20803080414133557, 0.20803080414133576, 0.282714306539816], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.6269657], dtype=float32), -0.0076052]. 
=============================================
[2019-03-26 12:29:39,395] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[67.67714 ]
 [67.70465 ]
 [67.76164 ]
 [67.84064 ]
 [67.918724]], R is [[67.70264435]
 [67.74211121]
 [67.78041077]
 [67.81778717]
 [67.85462952]].
[2019-03-26 12:29:39,970] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.7166915e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1308957e-35], sum to 1.0000
[2019-03-26 12:29:39,980] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2527
[2019-03-26 12:29:39,985] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 93.5, 1.0, 2.0, 0.663683141225422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 927493.9369278362, 927493.9369278367, 213201.2962658487], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5895000.0000, 
sim time next is 5895600.0000, 
raw observation next is [26.4, 92.66666666666666, 1.0, 2.0, 0.6906817531133346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 965241.5107897021, 965241.5107897026, 218846.6931405332], 
processed observation next is [1.0, 0.21739130434782608, 0.45023696682464454, 0.9266666666666665, 1.0, 1.0, 0.6273274133895597, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26812264188602836, 0.2681226418860285, 0.32663685543363163], 
reward next is 0.6734, 
noisyNet noise sample is [array([-0.9670002], dtype=float32), 1.0034461]. 
=============================================
[2019-03-26 12:29:42,983] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4925771e-32 1.0000000e+00 0.0000000e+00 2.5385855e-36 1.1356381e-20], sum to 1.0000
[2019-03-26 12:29:42,992] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1764
[2019-03-26 12:29:42,998] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 91.33333333333334, 1.0, 2.0, 0.6156870231956513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 860392.4285413763, 860392.4285413756, 203698.7310376298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5982000.0000, 
sim time next is 5982600.0000, 
raw observation next is [26.65, 90.66666666666666, 1.0, 2.0, 0.6207729797828606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 867502.7060887204, 867502.7060887197, 204673.5221518826], 
processed observation next is [1.0, 0.21739130434782608, 0.462085308056872, 0.9066666666666666, 1.0, 1.0, 0.5430999756420007, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24097297391353342, 0.24097297391353323, 0.30548286888340687], 
reward next is 0.6945, 
noisyNet noise sample is [array([-0.43297708], dtype=float32), -0.47007197]. 
=============================================
[2019-03-26 12:29:43,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8829527e-30], sum to 1.0000
[2019-03-26 12:29:43,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0103
[2019-03-26 12:29:43,361] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333334, 89.5, 1.0, 2.0, 0.5570404966408042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778406.7519905453, 778406.7519905453, 193015.6653702966], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5955000.0000, 
sim time next is 5955600.0000, 
raw observation next is [27.46666666666667, 90.0, 1.0, 2.0, 0.5575782937230792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779158.5436857993, 779158.5436857993, 193109.0351841725], 
processed observation next is [1.0, 0.9565217391304348, 0.500789889415482, 0.9, 1.0, 1.0, 0.4669617996663605, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21643292880161094, 0.21643292880161094, 0.2882224405733918], 
reward next is 0.7118, 
noisyNet noise sample is [array([0.4477076], dtype=float32), -0.0743851]. 
=============================================
[2019-03-26 12:29:49,891] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8891153e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.5943022e-22], sum to 1.0000
[2019-03-26 12:29:49,902] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3071
[2019-03-26 12:29:49,909] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 86.0, 1.0, 2.0, 0.5304955854144774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741300.0274749437, 741300.0274749431, 188512.1514061113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6210000.0000, 
sim time next is 6210600.0000, 
raw observation next is [27.25, 86.16666666666667, 1.0, 2.0, 0.5296273506821226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740086.3573434277, 740086.3573434271, 188368.3708849643], 
processed observation next is [1.0, 0.9130434782608695, 0.490521327014218, 0.8616666666666667, 1.0, 1.0, 0.43328596467725616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2055795437065077, 0.20557954370650755, 0.2811468222163646], 
reward next is 0.7189, 
noisyNet noise sample is [array([0.01564919], dtype=float32), 0.16446541]. 
=============================================
[2019-03-26 12:29:57,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7111491e-25 9.9999976e-01 1.3232534e-31 2.4032591e-25 2.3311220e-07], sum to 1.0000
[2019-03-26 12:29:57,377] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1334
[2019-03-26 12:29:57,385] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.1, 70.0, 1.0, 2.0, 0.5352173682689337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747900.4405693329, 747900.4405693322, 189298.1262683131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6343200.0000, 
sim time next is 6343800.0000, 
raw observation next is [30.25, 69.33333333333334, 1.0, 2.0, 0.5376065980060923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751240.2758638071, 751240.2758638064, 189698.1634940658], 
processed observation next is [0.0, 0.43478260869565216, 0.6327014218009479, 0.6933333333333335, 1.0, 1.0, 0.44289951566999075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20867785440661307, 0.20867785440661288, 0.2831315873045758], 
reward next is 0.7169, 
noisyNet noise sample is [array([0.26542354], dtype=float32), -0.25944382]. 
=============================================
[2019-03-26 12:29:59,337] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.8732792e-32 1.4696098e-14 0.0000000e+00 4.6132317e-34 1.0000000e+00], sum to 1.0000
[2019-03-26 12:29:59,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0805
[2019-03-26 12:29:59,356] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.78333333333333, 88.16666666666667, 1.0, 2.0, 0.1738614312311815, 1.0, 2.0, 0.1738614312311815, 1.0, 2.0, 0.2964680639389273, 6.9112, 6.9112, 170.5573041426782, 728840.9493261697, 728840.9493261697, 262864.0575088335], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6217800.0000, 
sim time next is 6218400.0000, 
raw observation next is [26.76666666666667, 88.33333333333334, 1.0, 2.0, 0.1739811450376845, 1.0, 2.0, 0.1739811450376845, 1.0, 2.0, 0.2967021300638834, 6.9112, 6.9112, 170.5573041426782, 729342.969750874, 729342.969750874, 262893.6520728118], 
processed observation next is [1.0, 1.0, 0.46761453396524505, 0.8833333333333334, 1.0, 1.0, 0.00479656028636685, 1.0, 1.0, 0.00479656028636685, 1.0, 1.0, 0.1423196708096139, 0.0, 0.0, 0.8375144448122397, 0.20259526937524278, 0.20259526937524278, 0.39237858518330115], 
reward next is 0.6076, 
noisyNet noise sample is [array([-0.19263399], dtype=float32), -0.054672375]. 
=============================================
[2019-03-26 12:30:01,383] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6641966e-33 1.0000000e+00 0.0000000e+00 1.6702665e-34 1.1218492e-12], sum to 1.0000
[2019-03-26 12:30:01,393] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4155
[2019-03-26 12:30:01,397] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 77.33333333333334, 1.0, 2.0, 0.541993001003538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757371.9293559034, 757371.9293559034, 190437.4756021247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6254400.0000, 
sim time next is 6255000.0000, 
raw observation next is [29.25, 76.5, 1.0, 2.0, 0.5417929320526041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757092.2567262761, 757092.2567262755, 190403.6895709978], 
processed observation next is [0.0, 0.391304347826087, 0.5853080568720379, 0.765, 1.0, 1.0, 0.4479432916296435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21030340464618782, 0.21030340464618766, 0.2841846112999967], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.00463036], dtype=float32), 0.14919655]. 
=============================================
[2019-03-26 12:30:01,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.11473]
 [78.10844]
 [78.1091 ]
 [77.9935 ]
 [77.99672]], R is [[78.05760956]
 [77.99279785]
 [77.92855835]
 [77.86495209]
 [77.80200195]].
[2019-03-26 12:30:02,670] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2893151e-34 4.0510882e-25 0.0000000e+00 2.6120183e-26 1.0000000e+00], sum to 1.0000
[2019-03-26 12:30:02,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9853
[2019-03-26 12:30:02,688] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.9, 69.0, 1.0, 2.0, 0.5558870968635033, 1.0, 2.0, 0.5558870968635033, 1.0, 2.0, 0.9568529894920034, 6.911200000000001, 6.9112, 170.5573041426782, 2332046.664161191, 2332046.66416119, 454204.7008570703], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6437400.0000, 
sim time next is 6438000.0000, 
raw observation next is [29.9, 69.0, 1.0, 2.0, 0.4817689061771764, 1.0, 2.0, 0.4817689061771764, 1.0, 2.0, 0.8283562158496997, 6.911200000000001, 6.9112, 170.5573041426782, 2020830.264154046, 2020830.264154045, 401322.0198863897], 
processed observation next is [1.0, 0.5217391304347826, 0.6161137440758293, 0.69, 1.0, 1.0, 0.37562518816527274, 1.0, 1.0, 0.37562518816527274, 1.0, 1.0, 0.7906783120118287, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5613417400427906, 0.5613417400427902, 0.5989880893826712], 
reward next is 0.4010, 
noisyNet noise sample is [array([-0.71584886], dtype=float32), 0.5958325]. 
=============================================
[2019-03-26 12:30:02,725] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[58.936844]
 [58.7612  ]
 [58.529964]
 [58.410374]
 [58.30168 ]], R is [[59.51289368]
 [59.23984909]
 [58.95375443]
 [58.66315842]
 [58.3776474 ]].
[2019-03-26 12:30:04,931] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0329484e-20 1.5220758e-01 1.4714339e-31 2.7918030e-22 8.4779245e-01], sum to 1.0000
[2019-03-26 12:30:04,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4109
[2019-03-26 12:30:04,949] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.25, 52.5, 1.0, 2.0, 0.2960987596299327, 1.0, 2.0, 0.2960987596299327, 1.0, 2.0, 0.5202510510126361, 6.9112, 6.9112, 170.5573041426782, 1344941.838242366, 1344941.838242366, 312720.5437964163], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6780600.0000, 
sim time next is 6781200.0000, 
raw observation next is [28.33333333333334, 52.0, 1.0, 2.0, 0.3007323773345246, 1.0, 2.0, 0.3007323773345246, 1.0, 2.0, 0.5285444658243056, 6.9112, 6.9112, 170.5573041426782, 1366609.001360997, 1366609.001360997, 314800.1442080198], 
processed observation next is [1.0, 0.4782608695652174, 0.5418641390205374, 0.52, 1.0, 1.0, 0.15750888835484894, 1.0, 1.0, 0.15750888835484894, 1.0, 1.0, 0.4250542266150068, 0.0, 0.0, 0.8375144448122397, 0.37961361148916584, 0.37961361148916584, 0.4698509615045071], 
reward next is 0.5301, 
noisyNet noise sample is [array([0.93898827], dtype=float32), -0.040209655]. 
=============================================
[2019-03-26 12:30:05,881] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4273141e-34 1.0000000e+00 0.0000000e+00 2.7601442e-35 2.0087031e-14], sum to 1.0000
[2019-03-26 12:30:05,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1738
[2019-03-26 12:30:05,897] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.6, 69.0, 1.0, 2.0, 0.5174312777073655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723038.1079451895, 723038.1079451895, 186372.1032476231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6367200.0000, 
sim time next is 6367800.0000, 
raw observation next is [29.25, 70.5, 1.0, 2.0, 0.5158433814178596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720818.4913110913, 720818.4913110913, 186115.4075364385], 
processed observation next is [0.0, 0.6956521739130435, 0.5853080568720379, 0.705, 1.0, 1.0, 0.41667877279260185, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20022735869752536, 0.20022735869752536, 0.27778419035289326], 
reward next is 0.7222, 
noisyNet noise sample is [array([-0.99263036], dtype=float32), 0.5037519]. 
=============================================
[2019-03-26 12:30:06,980] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.8119921e-32 9.9999976e-01 1.6793822e-35 5.3747287e-32 2.7732278e-07], sum to 1.0000
[2019-03-26 12:30:06,987] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7356
[2019-03-26 12:30:06,990] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 82.0, 1.0, 2.0, 0.5276148445948841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737273.162009482, 737273.1620094826, 188035.4726312438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6380400.0000, 
sim time next is 6381000.0000, 
raw observation next is [27.6, 82.0, 1.0, 2.0, 0.5264462443910909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735639.6296786687, 735639.6296786681, 187843.0045480424], 
processed observation next is [0.0, 0.8695652173913043, 0.5071090047393366, 0.82, 1.0, 1.0, 0.4294533064952902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20434434157740797, 0.2043443415774078, 0.2803626933552872], 
reward next is 0.7196, 
noisyNet noise sample is [array([0.960848], dtype=float32), 0.6063455]. 
=============================================
[2019-03-26 12:30:07,006] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.5656  ]
 [73.62383 ]
 [73.67905 ]
 [73.611534]
 [73.657166]], R is [[73.50157166]
 [73.48590851]
 [73.47011566]
 [73.45424652]
 [73.43856049]].
[2019-03-26 12:30:08,032] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0034555e-28 4.1218307e-16 1.0859809e-33 3.1919355e-26 1.0000000e+00], sum to 1.0000
[2019-03-26 12:30:08,045] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5485
[2019-03-26 12:30:08,050] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.03333333333333, 83.0, 1.0, 2.0, 0.1704219369519489, 1.0, 2.0, 0.1704219369519489, 1.0, 2.0, 0.2885148358426727, 6.9112, 6.9112, 170.5573041426782, 714417.5138385054, 714417.5138385054, 261970.0879427396], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6399600.0000, 
sim time next is 6400200.0000, 
raw observation next is [27.01666666666667, 83.0, 1.0, 2.0, 0.1701103170617135, 1.0, 2.0, 0.1701103170617135, 1.0, 2.0, 0.2879185156006036, 6.911199999999999, 6.9112, 170.5573041426782, 713110.7530008149, 713110.7530008155, 261895.4415219651], 
processed observation next is [1.0, 0.043478260869565216, 0.4794628751974725, 0.83, 1.0, 1.0, 0.00013291212254637843, 1.0, 1.0, 0.00013291212254637843, 1.0, 1.0, 0.13160794585439464, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19808632027800416, 0.1980863202780043, 0.39088871868950015], 
reward next is 0.6091, 
noisyNet noise sample is [array([-2.196246], dtype=float32), 0.5310402]. 
=============================================
[2019-03-26 12:30:10,161] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5205873e-25 1.0000000e+00 1.4436404e-31 4.3827165e-30 8.6792729e-16], sum to 1.0000
[2019-03-26 12:30:10,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4255
[2019-03-26 12:30:10,177] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 84.0, 1.0, 2.0, 0.8269378378153063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1155765.729560847, 1155765.729560847, 250629.2598203579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6417000.0000, 
sim time next is 6417600.0000, 
raw observation next is [27.33333333333334, 83.66666666666666, 1.0, 2.0, 0.8193236408153665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1145118.044983677, 1145118.044983677, 248708.1982623088], 
processed observation next is [1.0, 0.2608695652173913, 0.4944707740916275, 0.8366666666666666, 1.0, 1.0, 0.7823176395365861, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3180883458287992, 0.3180883458287992, 0.3712062660631475], 
reward next is 0.6288, 
noisyNet noise sample is [array([-0.6822894], dtype=float32), -0.31509402]. 
=============================================
[2019-03-26 12:30:12,368] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.46094450e-26 1.01322035e-08 6.28801947e-37 2.40064819e-25
 1.00000000e+00], sum to 1.0000
[2019-03-26 12:30:12,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6391
[2019-03-26 12:30:12,380] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 68.0, 1.0, 2.0, 0.5312933423194451, 1.0, 2.0, 0.5312933423194451, 1.0, 2.0, 0.9129583712439708, 6.911199999999999, 6.9112, 170.5573041426782, 2228779.425275135, 2228779.425275136, 435574.0401547612], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6446400.0000, 
sim time next is 6447000.0000, 
raw observation next is [30.0, 68.0, 1.0, 2.0, 0.5298947631576683, 1.0, 2.0, 0.5298947631576683, 1.0, 2.0, 0.9104985360136693, 6.911200000000001, 6.9112, 170.5573041426782, 2222907.157907841, 2222907.15790784, 434547.3722418061], 
processed observation next is [1.0, 0.6086956521739131, 0.6208530805687204, 0.68, 1.0, 1.0, 0.4336081483827328, 1.0, 1.0, 0.4336081483827328, 1.0, 1.0, 0.8908518731874016, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6174742105299558, 0.6174742105299555, 0.6485781675250838], 
reward next is 0.3514, 
noisyNet noise sample is [array([-0.42601493], dtype=float32), -0.78700143]. 
=============================================
[2019-03-26 12:30:12,405] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[67.45863 ]
 [66.64993 ]
 [65.75264 ]
 [64.50323 ]
 [63.080982]], R is [[67.91061401]
 [67.58139801]
 [67.24860382]
 [66.92346191]
 [66.60466766]].
[2019-03-26 12:30:13,022] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6558291e-22 3.4657262e-02 8.0713953e-33 4.0452718e-27 9.6534270e-01], sum to 1.0000
[2019-03-26 12:30:13,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0441
[2019-03-26 12:30:13,042] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.33333333333333, 91.0, 1.0, 2.0, 0.2424705729153011, 1.0, 2.0, 0.2424705729153011, 1.0, 2.0, 0.4139574604866619, 6.9112, 6.9112, 170.5573041426782, 1016592.247089969, 1016592.247089969, 282323.0390477391], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6489600.0000, 
sim time next is 6490200.0000, 
raw observation next is [26.31666666666667, 91.0, 1.0, 2.0, 0.2331429362765576, 1.0, 2.0, 0.2331429362765576, 1.0, 2.0, 0.3977748667481839, 6.9112, 6.9112, 170.5573041426782, 977466.984177163, 977466.984177163, 279284.1687259754], 
processed observation next is [1.0, 0.08695652173913043, 0.4462875197472356, 0.91, 1.0, 1.0, 0.07607582683922602, 1.0, 1.0, 0.07607582683922602, 1.0, 1.0, 0.2655791057904682, 0.0, 0.0, 0.8375144448122397, 0.27151860671587863, 0.27151860671587863, 0.41684204287459015], 
reward next is 0.5832, 
noisyNet noise sample is [array([0.486663], dtype=float32), 0.49987498]. 
=============================================
[2019-03-26 12:30:14,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9703695e-33 6.8833024e-18 1.8462412e-37 6.0731721e-33 1.0000000e+00], sum to 1.0000
[2019-03-26 12:30:14,282] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5520
[2019-03-26 12:30:14,290] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.1, 87.33333333333334, 1.0, 2.0, 0.1777204040831608, 1.0, 1.0, 0.1777204040831608, 1.0, 1.0, 0.3030143446404701, 6.911200000000001, 6.9112, 170.5573041426782, 745023.6888770198, 745023.6888770192, 263784.941197302], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6477600.0000, 
sim time next is 6478200.0000, 
raw observation next is [27.05, 87.5, 1.0, 2.0, 0.1774506663756071, 1.0, 2.0, 0.1774506663756071, 1.0, 2.0, 0.3034278397341235, 6.9112, 6.9112, 170.5573041426782, 743892.5262946667, 743892.5262946667, 263758.4169681287], 
processed observation next is [1.0, 1.0, 0.4810426540284361, 0.875, 1.0, 1.0, 0.008976706476635055, 1.0, 1.0, 0.008976706476635055, 1.0, 1.0, 0.15052175577332136, 0.0, 0.0, 0.8375144448122397, 0.20663681285962965, 0.20663681285962965, 0.3936692790569085], 
reward next is 0.6063, 
noisyNet noise sample is [array([0.15271547], dtype=float32), -1.7542807]. 
=============================================
[2019-03-26 12:30:18,630] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 12:30:18,635] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:30:18,637] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:30:18,638] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:30:18,639] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:30:18,639] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:30:18,641] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:30:18,639] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:30:18,643] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:30:18,645] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:30:18,645] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:30:18,666] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run59
[2019-03-26 12:30:18,690] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run59
[2019-03-26 12:30:18,708] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run59
[2019-03-26 12:30:18,730] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run59
[2019-03-26 12:30:18,730] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run59
[2019-03-26 12:30:40,092] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1919917], dtype=float32), 0.4422978]
[2019-03-26 12:30:40,093] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.5, 66.33333333333334, 1.0, 2.0, 0.2508234671182152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412601.1378249778, 412601.1378249784, 160985.8440673603]
[2019-03-26 12:30:40,094] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:30:40,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18987350082781262
[2019-03-26 12:31:10,539] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1919917], dtype=float32), 0.4422978]
[2019-03-26 12:31:10,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.46666666666667, 49.33333333333333, 1.0, 2.0, 0.3991224369974197, 1.0, 1.0, 0.3991224369974197, 1.0, 1.0, 0.6871345738366746, 6.911200000000001, 6.9112, 169.0403247858759, 1673901.912537261, 1673901.91253726, 351269.5621085391]
[2019-03-26 12:31:10,544] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:31:10,547] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.4787687e-29 8.5539231e-13 4.2190051e-35 2.6794885e-23 1.0000000e+00], sampled 0.1255564961028408
[2019-03-26 12:31:11,692] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1919917], dtype=float32), 0.4422978]
[2019-03-26 12:31:11,693] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.96666666666667, 54.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.964176275580639, 6.9112, 168.9125260686342, 1491363.762907934, 1453780.665481792, 311354.5096509814]
[2019-03-26 12:31:11,695] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:31:11,700] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.4460514e-26 9.9999952e-01 8.4218316e-32 3.0455936e-24 4.4313975e-07], sampled 0.7091219041362583
[2019-03-26 12:31:11,814] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1919917], dtype=float32), 0.4422978]
[2019-03-26 12:31:11,817] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.06666666666667, 53.0, 1.0, 2.0, 0.8160928203143851, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005982533348984, 6.9112, 168.9123160005539, 2037596.180840902, 1970354.444498521, 411842.9101129859]
[2019-03-26 12:31:11,820] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:31:11,823] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.3874168e-34 1.0000000e+00 0.0000000e+00 2.3361153e-27 7.1286439e-26], sampled 0.42050754475400376
[2019-03-26 12:31:11,824] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2037596.180840902 W.
[2019-03-26 12:31:54,443] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1919917], dtype=float32), 0.4422978]
[2019-03-26 12:31:54,444] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.43333333333334, 86.0, 1.0, 2.0, 0.7313467106201924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1022098.947227915, 1022098.947227915, 227757.0139735678]
[2019-03-26 12:31:54,445] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:31:54,449] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.9860495e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4320686e-27], sampled 0.8631725029065125
[2019-03-26 12:31:58,223] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1919917], dtype=float32), 0.4422978]
[2019-03-26 12:31:58,225] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.46511674333333, 81.60705068, 1.0, 2.0, 0.3648689031791533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561606.6212546966, 561606.6212546972, 171615.0447985077]
[2019-03-26 12:31:58,226] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:31:58,230] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2136748e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7083617e-21], sampled 0.4820930447198426
[2019-03-26 12:32:00,715] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1919917], dtype=float32), 0.4422978]
[2019-03-26 12:32:00,716] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.58333333333334, 88.5, 1.0, 2.0, 0.5259905123784842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735002.5835967764, 735002.5835967764, 187767.9308064764]
[2019-03-26 12:32:00,718] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:32:00,720] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.019160014208113307
[2019-03-26 12:32:14,800] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8123.7436 3006294326.4716 1504.0000
[2019-03-26 12:32:14,941] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7998.8135 3163134837.6928 1604.0000
[2019-03-26 12:32:14,977] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8355.5295 2929932036.0799 1141.0000
[2019-03-26 12:32:15,094] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8752.9909 2780674554.5645 727.0000
[2019-03-26 12:32:15,113] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8613.1138 2840883441.9753 865.0000
[2019-03-26 12:32:16,133] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1450000, evaluation results [1450000.0, 7998.813540919348, 3163134837.69281, 1604.0, 8355.529489306547, 2929932036.0799484, 1141.0, 8752.990920882208, 2780674554.5645385, 727.0, 8123.74359854289, 3006294326.4715686, 1504.0, 8613.113778556437, 2840883441.975323, 865.0]
[2019-03-26 12:32:25,764] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:32:25,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6432
[2019-03-26 12:32:25,777] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 69.66666666666667, 1.0, 2.0, 0.3764670437434964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570601.8994316269, 570601.8994316269, 172149.2990639542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6730800.0000, 
sim time next is 6731400.0000, 
raw observation next is [25.83333333333334, 69.83333333333333, 1.0, 2.0, 0.3731243859180476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 567287.1956160339, 567287.1956160332, 171911.6274366681], 
processed observation next is [1.0, 0.9130434782608695, 0.42338072669826254, 0.6983333333333333, 1.0, 1.0, 0.2447281758048766, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1575797765600094, 0.1575797765600092, 0.25658451856219117], 
reward next is 0.7434, 
noisyNet noise sample is [array([-1.5311161], dtype=float32), 0.6391815]. 
=============================================
[2019-03-26 12:32:28,634] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:32:28,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5908
[2019-03-26 12:32:28,649] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 70.66666666666666, 1.0, 2.0, 0.7568114523384156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1181391.360127434, 1181391.360127434, 249189.4448756497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6770400.0000, 
sim time next is 6771000.0000, 
raw observation next is [24.96666666666667, 69.33333333333334, 1.0, 2.0, 0.7703379065677078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1201873.870929913, 1201873.870929913, 252678.3682327296], 
processed observation next is [1.0, 0.34782608695652173, 0.3823064770932071, 0.6933333333333335, 1.0, 1.0, 0.7232986826116962, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3338538530360869, 0.3338538530360869, 0.3771318928846711], 
reward next is 0.6229, 
noisyNet noise sample is [array([-0.9001986], dtype=float32), 1.1614743]. 
=============================================
[2019-03-26 12:32:28,668] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.24219 ]
 [64.97026 ]
 [66.25594 ]
 [68.10702 ]
 [68.355255]], R is [[63.56715012]
 [63.55955505]
 [63.55252457]
 [63.58114243]
 [63.67749023]].
[2019-03-26 12:32:37,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:32:37,608] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1009
[2019-03-26 12:32:37,611] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 92.16666666666667, 1.0, 2.0, 0.412606165552435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654922.4811184471, 654922.4811184466, 180124.86327734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7390200.0000, 
sim time next is 7390800.0000, 
raw observation next is [21.0, 92.0, 1.0, 2.0, 0.4834909667026012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768347.1244336583, 768347.1244336583, 191563.4716844815], 
processed observation next is [1.0, 0.5652173913043478, 0.19431279620853087, 0.92, 1.0, 1.0, 0.3776999598826521, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2134297567871273, 0.2134297567871273, 0.2859156293798232], 
reward next is 0.7141, 
noisyNet noise sample is [array([0.73787874], dtype=float32), -0.39685103]. 
=============================================
[2019-03-26 12:32:39,013] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:32:39,022] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2909
[2019-03-26 12:32:39,027] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 54.83333333333333, 1.0, 2.0, 0.4578875567716384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648424.7410526436, 648424.7410526443, 178372.5749946614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6954600.0000, 
sim time next is 6955200.0000, 
raw observation next is [31.0, 54.0, 1.0, 2.0, 0.456527340715469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 646895.5492950778, 646895.5492950785, 178224.758685741], 
processed observation next is [0.0, 0.5217391304347826, 0.6682464454976303, 0.54, 1.0, 1.0, 0.3452136635126133, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1796932081375216, 0.1796932081375218, 0.2660071025160313], 
reward next is 0.7340, 
noisyNet noise sample is [array([0.22698282], dtype=float32), 0.18259671]. 
=============================================
[2019-03-26 12:32:44,584] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3660582e-22 1.0000000e+00 1.3653937e-31 8.8033513e-23 1.4647468e-13], sum to 1.0000
[2019-03-26 12:32:44,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4762
[2019-03-26 12:32:44,603] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.78333333333333, 46.50000000000001, 1.0, 2.0, 0.3658420647825354, 1.0, 1.0, 0.3658420647825354, 1.0, 2.0, 0.615152266292371, 6.9112, 6.9112, 170.5573041426782, 1557271.999745884, 1557271.999745884, 334191.4690791823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7045800.0000, 
sim time next is 7046400.0000, 
raw observation next is [31.86666666666667, 46.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.794935716011347, 6.9112, 168.9080832902526, 2136683.469973228, 1509749.025081443, 320143.3863749042], 
processed observation next is [1.0, 0.5652173913043478, 0.7093206951026858, 0.46, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.08837357160113468, 0.0, 0.8294160154102569, 0.5935231861036744, 0.4193747291892897, 0.47782594981328985], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.42638746], dtype=float32), -0.071737856]. 
=============================================
[2019-03-26 12:32:47,387] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:32:47,399] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0678
[2019-03-26 12:32:47,405] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 91.33333333333334, 1.0, 2.0, 0.5568043743383154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 890735.134253962, 890735.1342539614, 205559.217615906], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7396800.0000, 
sim time next is 7397400.0000, 
raw observation next is [20.73333333333333, 91.16666666666667, 1.0, 2.0, 0.5776088857498417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 924981.165246668, 924981.165246668, 209820.0580557807], 
processed observation next is [1.0, 0.6086956521739131, 0.18167456556082143, 0.9116666666666667, 1.0, 1.0, 0.4910950430720984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2569392125685189, 0.2569392125685189, 0.3131642657548966], 
reward next is 0.6868, 
noisyNet noise sample is [array([-2.6230671], dtype=float32), -1.1080569]. 
=============================================
[2019-03-26 12:32:50,083] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:32:50,084] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:32:50,155] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run8
[2019-03-26 12:32:55,396] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:32:55,405] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4417
[2019-03-26 12:32:55,411] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 92.0, 1.0, 2.0, 0.3669486473806905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 560136.9997718777, 560136.999771877, 171360.1716835419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7245000.0000, 
sim time next is 7245600.0000, 
raw observation next is [22.53333333333333, 92.0, 1.0, 2.0, 0.3660153972335382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 558996.2606867763, 558996.2606867768, 171270.7980575553], 
processed observation next is [1.0, 0.8695652173913043, 0.26698262243285936, 0.92, 1.0, 1.0, 0.23616312919703397, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15527673907966008, 0.15527673907966022, 0.25562805680232137], 
reward next is 0.7444, 
noisyNet noise sample is [array([0.31973237], dtype=float32), 1.9024484]. 
=============================================
[2019-03-26 12:32:59,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.445459e-38 1.000000e+00 0.000000e+00 0.000000e+00 4.530102e-33], sum to 1.0000
[2019-03-26 12:32:59,269] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5732
[2019-03-26 12:32:59,275] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 61.33333333333333, 1.0, 2.0, 0.8867398394209484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1336500.396340828, 1336500.396340828, 280017.0378093412], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7314000.0000, 
sim time next is 7314600.0000, 
raw observation next is [27.63333333333334, 61.66666666666667, 1.0, 2.0, 0.8903003218614303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1340734.917294767, 1340734.917294767, 280916.8603076679], 
processed observation next is [1.0, 0.6521739130434783, 0.5086887835703005, 0.6166666666666667, 1.0, 1.0, 0.8678317130860606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37242636591521305, 0.37242636591521305, 0.4192788959815939], 
reward next is 0.5807, 
noisyNet noise sample is [array([0.34292337], dtype=float32), -0.15342279]. 
=============================================
[2019-03-26 12:33:00,311] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:33:00,325] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8414
[2019-03-26 12:33:00,330] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 91.66666666666667, 1.0, 2.0, 0.4926362209958013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786271.4478508017, 786271.4478508023, 193419.6816024164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7395600.0000, 
sim time next is 7396200.0000, 
raw observation next is [20.8, 91.5, 1.0, 2.0, 0.4855258533818431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775839.4433837334, 775839.4433837341, 192245.6599153558], 
processed observation next is [1.0, 0.6086956521739131, 0.1848341232227489, 0.915, 1.0, 1.0, 0.3801516305805339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2155109564954815, 0.2155109564954817, 0.2869338207691878], 
reward next is 0.7131, 
noisyNet noise sample is [array([-0.9156416], dtype=float32), 0.43754274]. 
=============================================
[2019-03-26 12:33:00,893] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:33:00,904] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7163
[2019-03-26 12:33:00,909] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 74.5, 1.0, 2.0, 0.4143424388512938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636987.9260370054, 636987.9260370047, 178372.5192024837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7353000.0000, 
sim time next is 7353600.0000, 
raw observation next is [24.66666666666666, 75.33333333333333, 1.0, 2.0, 0.4143900331792998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636312.9102827344, 636312.9102827339, 178298.6179107785], 
processed observation next is [1.0, 0.08695652173913043, 0.36808846761453373, 0.7533333333333333, 1.0, 1.0, 0.2944458231075901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17675358618964845, 0.1767535861896483, 0.26611734016534105], 
reward next is 0.7339, 
noisyNet noise sample is [array([-0.616718], dtype=float32), -0.7657895]. 
=============================================
[2019-03-26 12:33:02,327] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:33:02,336] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9543
[2019-03-26 12:33:02,341] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.63333333333333, 92.33333333333334, 1.0, 2.0, 0.5817624780692395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 931735.4583651241, 931735.4583651234, 210682.2938787546], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7377600.0000, 
sim time next is 7378200.0000, 
raw observation next is [20.7, 92.5, 1.0, 2.0, 0.5993035104420347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958112.2415123684, 958112.2415123684, 214230.8905918325], 
processed observation next is [1.0, 0.391304347826087, 0.18009478672985785, 0.925, 1.0, 1.0, 0.5172331451108851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26614228930899125, 0.26614228930899125, 0.31974759789825746], 
reward next is 0.6803, 
noisyNet noise sample is [array([-0.6906463], dtype=float32), -1.7249043]. 
=============================================
[2019-03-26 12:33:09,464] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 12:33:09,466] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:33:09,466] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:33:09,466] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:33:09,468] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:33:09,469] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:33:09,470] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:33:09,471] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:33:09,467] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:33:09,473] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:33:09,476] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:33:09,502] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run60
[2019-03-26 12:33:09,525] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run60
[2019-03-26 12:33:09,547] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run60
[2019-03-26 12:33:09,571] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run60
[2019-03-26 12:33:09,589] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run60
[2019-03-26 12:33:30,024] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24449381], dtype=float32), 0.43583924]
[2019-03-26 12:33:30,028] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.3, 63.0, 1.0, 2.0, 0.2475131356467106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 409919.7843004173, 409919.7843004167, 160526.3945569436]
[2019-03-26 12:33:30,029] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:33:30,034] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4999139978482299
[2019-03-26 12:34:13,240] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.24449381], dtype=float32), 0.43583924]
[2019-03-26 12:34:13,241] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.56386405, 69.02929567000001, 1.0, 2.0, 0.6082876708877674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850048.0586334479, 850048.0586334479, 202300.8124299003]
[2019-03-26 12:34:13,242] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:34:13,243] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13755941375243652
[2019-03-26 12:34:35,074] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.24449381], dtype=float32), 0.43583924]
[2019-03-26 12:34:35,075] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.64260755, 67.52887272, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.212076045745453, 6.9112, 168.9109621534419, 1667350.418019215, 1453901.116040908, 311348.4356018196]
[2019-03-26 12:34:35,076] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:34:35,078] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.5135395e-36 1.0000000e+00 2.7119940e-38 2.3730763e-34 2.4658618e-24], sampled 0.42346493376146277
[2019-03-26 12:34:35,081] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1667350.418019215 W.
[2019-03-26 12:34:36,606] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.24449381], dtype=float32), 0.43583924]
[2019-03-26 12:34:36,609] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.86666666666667, 86.5, 1.0, 2.0, 0.8402938919263644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1174443.0786203, 1174443.078620299, 254045.9011047765]
[2019-03-26 12:34:36,609] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:34:36,613] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4922319177896908
[2019-03-26 12:34:39,736] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.24449381], dtype=float32), 0.43583924]
[2019-03-26 12:34:39,737] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.63333333333333, 61.33333333333333, 1.0, 2.0, 0.5628623093352455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786545.1480468205, 786545.1480468205, 194031.4267601763]
[2019-03-26 12:34:39,737] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:34:39,739] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7338509999602479
[2019-03-26 12:34:54,445] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24449381], dtype=float32), 0.43583924]
[2019-03-26 12:34:54,446] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.36666666666667, 81.16666666666667, 1.0, 2.0, 0.5851178035477683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817656.9996951284, 817656.9996951284, 198005.6750797826]
[2019-03-26 12:34:54,448] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:34:54,449] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6114712295176178
[2019-03-26 12:34:59,180] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.24449381], dtype=float32), 0.43583924]
[2019-03-26 12:34:59,180] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.8, 82.0, 1.0, 2.0, 0.394913388322563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 585719.1098138968, 585719.1098138962, 173115.0791195052]
[2019-03-26 12:34:59,183] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:34:59,185] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.690978996867259
[2019-03-26 12:34:59,767] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.24449381], dtype=float32), 0.43583924]
[2019-03-26 12:34:59,769] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.13333333333334, 65.66666666666667, 1.0, 2.0, 0.4015764440044791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600183.8243556176, 600183.8243556176, 174580.5457020507]
[2019-03-26 12:34:59,770] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:34:59,773] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2870128083579535
[2019-03-26 12:35:04,870] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.1568 2779581835.6050 935.0000
[2019-03-26 12:35:05,248] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.7643 2927510135.9551 1343.0000
[2019-03-26 12:35:05,267] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7877.1568 3164489325.8825 1811.0000
[2019-03-26 12:35:05,339] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6875 3007826568.9530 1767.0000
[2019-03-26 12:35:05,411] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.4318 2842694272.5366 1133.0000
[2019-03-26 12:35:06,429] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1475000, evaluation results [1475000.0, 7877.156831836378, 3164489325.882539, 1811.0, 8251.764272169026, 2927510135.9551473, 1343.0, 8657.156844063844, 2779581835.605044, 935.0, 7997.687533267032, 3007826568.9530106, 1767.0, 8495.431787245472, 2842694272.5366197, 1133.0]
[2019-03-26 12:35:07,496] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:07,497] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:07,573] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run8
[2019-03-26 12:35:13,201] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:13,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4133
[2019-03-26 12:35:13,212] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 91.0, 1.0, 2.0, 0.5763747279123347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 805434.5934760862, 805434.5934760856, 196420.8555686802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7794000.0000, 
sim time next is 7794600.0000, 
raw observation next is [25.53333333333333, 90.50000000000001, 1.0, 2.0, 0.6111462772103123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 854044.4074519783, 854044.4074519777, 202832.4062318242], 
processed observation next is [1.0, 0.21739130434782608, 0.4091627172195892, 0.9050000000000001, 1.0, 1.0, 0.5315015388076052, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23723455762554954, 0.23723455762554937, 0.30273493467436446], 
reward next is 0.6973, 
noisyNet noise sample is [array([-0.344847], dtype=float32), 0.21851023]. 
=============================================
[2019-03-26 12:35:15,352] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:15,352] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:15,423] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run8
[2019-03-26 12:35:21,397] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:21,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:21,459] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run8
[2019-03-26 12:35:21,688] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:21,689] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:21,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run8
[2019-03-26 12:35:22,653] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5153068e-22 9.7194594e-01 2.0104005e-27 4.9423537e-23 2.8054029e-02], sum to 1.0000
[2019-03-26 12:35:22,662] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6897
[2019-03-26 12:35:22,668] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2341922.853452278 W.
[2019-03-26 12:35:22,671] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 64.16666666666666, 1.0, 2.0, 0.5582390681457062, 1.0, 2.0, 0.5582390681457062, 1.0, 2.0, 0.9640311631206443, 6.9112, 6.9112, 170.5573041426782, 2341922.853452278, 2341922.853452278, 456643.6987989473], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7833000.0000, 
sim time next is 7833600.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.8200257347511726, 1.0, 2.0, 0.8200257347511726, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2293401.843411985, 2293401.843411985, 429731.9813985396], 
processed observation next is [1.0, 0.6956521739130435, 0.6682464454976303, 0.63, 1.0, 1.0, 0.7831635358447863, 1.0, 1.0, 0.7831635358447863, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6370560676144402, 0.6370560676144402, 0.6413910170127457], 
reward next is 0.3586, 
noisyNet noise sample is [array([-1.5239393], dtype=float32), 0.2616201]. 
=============================================
[2019-03-26 12:35:24,563] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2664988e-29 1.0000000e+00 2.6354507e-32 2.0417347e-35 2.3376432e-09], sum to 1.0000
[2019-03-26 12:35:24,575] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5877
[2019-03-26 12:35:24,583] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 91.66666666666667, 1.0, 2.0, 0.7750309242955185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1083181.276651891, 1083181.276651891, 237872.7383336706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7872000.0000, 
sim time next is 7872600.0000, 
raw observation next is [26.08333333333334, 91.33333333333334, 1.0, 2.0, 0.7600381123005231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1062216.880056028, 1062216.880056028, 234336.6552992772], 
processed observation next is [1.0, 0.08695652173913043, 0.43522906793049004, 0.9133333333333334, 1.0, 1.0, 0.710889291928341, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2950602444600078, 0.2950602444600078, 0.3497562019392197], 
reward next is 0.6502, 
noisyNet noise sample is [array([-0.04608195], dtype=float32), -0.72157913]. 
=============================================
[2019-03-26 12:35:26,174] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4677002e-19 2.1715781e-05 8.9445739e-26 1.3720556e-19 9.9997830e-01], sum to 1.0000
[2019-03-26 12:35:26,184] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0713
[2019-03-26 12:35:26,191] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 72.0, 1.0, 2.0, 0.4980620553614582, 1.0, 2.0, 0.4980620553614582, 1.0, 2.0, 0.8595133792623505, 6.911199999999999, 6.9112, 170.5573041426782, 2089240.233028271, 2089240.233028272, 412826.7704530438], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7903200.0000, 
sim time next is 7903800.0000, 
raw observation next is [29.7, 72.0, 1.0, 2.0, 0.5164357753392422, 1.0, 2.0, 0.5164357753392422, 1.0, 2.0, 0.892108349894553, 6.911200000000001, 6.9112, 170.5573041426782, 2166391.12627361, 2166391.126273609, 425804.690151376], 
processed observation next is [1.0, 0.4782608695652174, 0.6066350710900474, 0.72, 1.0, 1.0, 0.41739250040872555, 1.0, 1.0, 0.41739250040872555, 1.0, 1.0, 0.8684248169445768, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6017753128537806, 0.6017753128537804, 0.6355293882856359], 
reward next is 0.3645, 
noisyNet noise sample is [array([-0.12819318], dtype=float32), -0.27146602]. 
=============================================
[2019-03-26 12:35:26,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:26,901] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:26,964] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run8
[2019-03-26 12:35:27,616] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:27,619] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0886
[2019-03-26 12:35:27,623] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.43333333333333, 91.66666666666666, 1.0, 2.0, 0.2594312418392346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424664.7965696771, 424664.7965696771, 161844.985704398], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 783600.0000, 
sim time next is 784200.0000, 
raw observation next is [19.41666666666667, 91.83333333333333, 1.0, 2.0, 0.2596210836681977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 424958.8554000512, 424958.8554000505, 161864.003403954], 
processed observation next is [0.0, 0.043478260869565216, 0.11927330173775699, 0.9183333333333333, 1.0, 1.0, 0.1079772092387924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11804412650001422, 0.11804412650001403, 0.2415880647820209], 
reward next is 0.7584, 
noisyNet noise sample is [array([1.8248645], dtype=float32), -0.80421066]. 
=============================================
[2019-03-26 12:35:27,753] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:27,758] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9203
[2019-03-26 12:35:27,764] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.36666666666667, 88.33333333333334, 1.0, 2.0, 0.3010246977849387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479543.8533998129, 479543.8533998135, 165591.9594199328], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 238800.0000, 
sim time next is 239400.0000, 
raw observation next is [21.35, 88.5, 1.0, 2.0, 0.3009590015651488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479394.5417234031, 479394.5417234038, 165580.6139584887], 
processed observation next is [0.0, 0.782608695652174, 0.2109004739336494, 0.885, 1.0, 1.0, 0.15778192959656484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13316515047872307, 0.13316515047872327, 0.24713524471416226], 
reward next is 0.7529, 
noisyNet noise sample is [array([-0.44142202], dtype=float32), 0.30573437]. 
=============================================
[2019-03-26 12:35:28,165] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:28,165] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:28,223] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run8
[2019-03-26 12:35:28,563] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:28,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:28,576] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:28,576] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:28,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run8
[2019-03-26 12:35:28,662] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run8
[2019-03-26 12:35:28,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:28,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0127
[2019-03-26 12:35:28,698] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 63.00000000000001, 1.0, 2.0, 0.288240026047877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462102.2383976206, 462102.2383976199, 164406.2240309341], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 825600.0000, 
sim time next is 826200.0000, 
raw observation next is [24.7, 63.0, 1.0, 2.0, 0.2869081936848457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460245.8817735857, 460245.8817735857, 164281.435198904], 
processed observation next is [0.0, 0.5652173913043478, 0.3696682464454976, 0.63, 1.0, 1.0, 0.14085324540342856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12784607827044048, 0.12784607827044048, 0.24519617193866267], 
reward next is 0.7548, 
noisyNet noise sample is [array([0.23381536], dtype=float32), -0.11310478]. 
=============================================
[2019-03-26 12:35:28,975] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:28,977] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:29,032] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run8
[2019-03-26 12:35:29,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:29,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:29,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:29,100] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:29,131] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run8
[2019-03-26 12:35:29,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run8
[2019-03-26 12:35:29,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:29,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:29,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run8
[2019-03-26 12:35:29,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:29,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:29,261] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run8
[2019-03-26 12:35:29,296] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:29,296] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:29,301] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-03-26 12:35:29,321] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:35:29,321] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:29,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run8
[2019-03-26 12:35:36,144] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:36,152] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9037
[2019-03-26 12:35:36,156] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 90.0, 1.0, 2.0, 0.3810260201994432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586932.2282260108, 586932.2282260101, 173833.5239642372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 100200.0000, 
sim time next is 100800.0000, 
raw observation next is [22.5, 90.0, 1.0, 2.0, 0.379467015531183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 584533.8555543392, 584533.8555543387, 173620.7069622091], 
processed observation next is [1.0, 0.17391304347826086, 0.2654028436018958, 0.9, 1.0, 1.0, 0.25236989823034095, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1623705154317609, 0.16237051543176073, 0.2591353835256852], 
reward next is 0.7409, 
noisyNet noise sample is [array([-0.5539474], dtype=float32), 0.6517554]. 
=============================================
[2019-03-26 12:35:40,668] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:40,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6940
[2019-03-26 12:35:40,684] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.91666666666666, 96.0, 1.0, 2.0, 0.2878720947516818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463490.9715213497, 463490.9715213497, 164511.4154489585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 183000.0000, 
sim time next is 183600.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.2872358174508378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462614.425810521, 462614.4258105204, 164451.5098731454], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.1412479728323347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12850400716958918, 0.128504007169589, 0.24545001473603792], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.5599818], dtype=float32), 2.3592305]. 
=============================================
[2019-03-26 12:35:42,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:42,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7799
[2019-03-26 12:35:42,783] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.75, 92.5, 1.0, 2.0, 0.3023365824055306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482675.1279515924, 482675.1279515917, 165830.0677799411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 210600.0000, 
sim time next is 211200.0000, 
raw observation next is [20.76666666666667, 92.33333333333333, 1.0, 2.0, 0.3016456660709038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481606.5066932109, 481606.5066932116, 165753.8694256359], 
processed observation next is [0.0, 0.43478260869565216, 0.18325434439178534, 0.9233333333333333, 1.0, 1.0, 0.15860923623000459, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1337795851925586, 0.13377958519255878, 0.24739383496363568], 
reward next is 0.7526, 
noisyNet noise sample is [array([-1.7684293], dtype=float32), 0.55562824]. 
=============================================
[2019-03-26 12:35:45,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:45,062] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5790
[2019-03-26 12:35:45,069] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 90.66666666666667, 1.0, 2.0, 0.2943811953996872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471934.3171387933, 471934.3171387933, 165087.7251199895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 250800.0000, 
sim time next is 251400.0000, 
raw observation next is [20.73333333333333, 90.83333333333334, 1.0, 2.0, 0.293988935910098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471426.8767904231, 471426.8767904231, 165053.0886931898], 
processed observation next is [0.0, 0.9130434782608695, 0.18167456556082143, 0.9083333333333334, 1.0, 1.0, 0.1493842601326482, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13095191021956198, 0.13095191021956198, 0.2463478935719251], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.67247546], dtype=float32), -1.7242951]. 
=============================================
[2019-03-26 12:35:45,291] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:45,296] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9977
[2019-03-26 12:35:45,301] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.45, 61.83333333333334, 1.0, 2.0, 0.5168262526456806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 847277.1018349476, 847277.1018349469, 198908.6540762966], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 474600.0000, 
sim time next is 475200.0000, 
raw observation next is [23.6, 61.0, 1.0, 2.0, 0.5221417570858481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855868.9371883216, 855868.9371883216, 199902.7141333732], 
processed observation next is [1.0, 0.5217391304347826, 0.3175355450236968, 0.61, 1.0, 1.0, 0.42426717721186513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23774137144120044, 0.23774137144120044, 0.29836225990055704], 
reward next is 0.7016, 
noisyNet noise sample is [array([-0.13681628], dtype=float32), 0.28253222]. 
=============================================
[2019-03-26 12:35:54,037] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:35:54,045] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9210
[2019-03-26 12:35:54,050] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.35, 81.0, 1.0, 2.0, 0.2483863533733219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 408670.3053262848, 408670.3053262854, 160744.8091226011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 419400.0000, 
sim time next is 420000.0000, 
raw observation next is [20.3, 81.33333333333334, 1.0, 2.0, 0.2477303762414592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 407624.7265960263, 407624.7265960256, 160680.0559784349], 
processed observation next is [1.0, 0.8695652173913043, 0.16113744075829392, 0.8133333333333335, 1.0, 1.0, 0.09365105571260142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1132290907211184, 0.11322909072111821, 0.23982097907229089], 
reward next is 0.7602, 
noisyNet noise sample is [array([-0.40686777], dtype=float32), -0.28494245]. 
=============================================
[2019-03-26 12:35:54,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.0858  ]
 [75.148056]
 [75.24506 ]
 [75.44607 ]
 [75.41064 ]], R is [[75.08410645]
 [75.09335327]
 [75.10240936]
 [75.11128235]
 [75.11985779]].
[2019-03-26 12:35:56,488] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 12:35:56,490] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:35:56,491] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:56,494] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:35:56,497] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:56,499] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:35:56,501] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:35:56,501] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:56,506] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:56,522] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:35:56,619] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:35:57,208] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run61
[2019-03-26 12:35:57,535] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run61
[2019-03-26 12:35:57,560] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run61
[2019-03-26 12:35:57,648] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run61
[2019-03-26 12:35:57,674] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run61
[2019-03-26 12:36:41,174] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24034472], dtype=float32), 0.3861343]
[2019-03-26 12:36:41,177] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.95, 77.83333333333333, 1.0, 2.0, 0.5230005542807501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730823.0730070027, 730823.0730070027, 187277.3895765281]
[2019-03-26 12:36:41,178] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:36:41,183] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2449367e-32], sampled 0.0708824420474351
[2019-03-26 12:36:59,608] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24034472], dtype=float32), 0.3861343]
[2019-03-26 12:36:59,610] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.16666666666667, 58.33333333333334, 1.0, 2.0, 0.5601726950555516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782785.2896467927, 782785.2896467927, 193560.9435179983]
[2019-03-26 12:36:59,612] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:36:59,614] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12204918516627217
[2019-03-26 12:37:08,188] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.24034472], dtype=float32), 0.3861343]
[2019-03-26 12:37:08,189] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.15000000000001, 53.5, 1.0, 2.0, 0.5957363296863563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104262, 832501.3861537739, 832501.3861537739, 199956.3044483581]
[2019-03-26 12:37:08,191] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:37:08,194] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5782473e-35 5.2379079e-37], sampled 0.056596743750895184
[2019-03-26 12:37:16,244] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.24034472], dtype=float32), 0.3861343]
[2019-03-26 12:37:16,248] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.61038678, 89.15667439, 1.0, 2.0, 0.7383667121791735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1031914.568507998, 1031914.568507999, 229346.4371470429]
[2019-03-26 12:37:16,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:37:16,251] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.540593e-29], sampled 0.6014739077920513
[2019-03-26 12:37:21,885] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.24034472], dtype=float32), 0.3861343]
[2019-03-26 12:37:21,886] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.27866834, 61.89137057000001, 1.0, 2.0, 0.5718862638228696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799159.9876037741, 799159.9876037741, 195624.2630176183]
[2019-03-26 12:37:21,887] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:37:21,890] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9806485008950093
[2019-03-26 12:37:26,502] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.24034472], dtype=float32), 0.3861343]
[2019-03-26 12:37:26,505] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.06533353833333, 85.21793725666667, 1.0, 2.0, 0.7528819372580887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1052210.571432703, 1052210.571432702, 232675.3029131253]
[2019-03-26 12:37:26,507] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:37:26,509] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11827707253876474
[2019-03-26 12:37:34,368] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.24034472], dtype=float32), 0.3861343]
[2019-03-26 12:37:34,371] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.8, 66.0, 1.0, 2.0, 0.7674160855248775, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.971263102317671, 6.9112, 168.9125490791335, 1969471.95343949, 1926861.224223559, 401119.841312627]
[2019-03-26 12:37:34,372] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:37:34,376] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5503328e-34 1.0000000e+00 0.0000000e+00 1.1430894e-32 2.0362143e-18], sampled 0.8258693853169479
[2019-03-26 12:37:34,378] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1969471.95343949 W.
[2019-03-26 12:37:39,365] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.24034472], dtype=float32), 0.3861343]
[2019-03-26 12:37:39,366] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.8, 46.66666666666667, 1.0, 2.0, 0.5536626246089003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773684.8044722677, 773684.8044722683, 192431.0725409464]
[2019-03-26 12:37:39,369] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:37:39,371] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04917637399086128
[2019-03-26 12:37:54,478] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8038.5133 3006759520.0645 1662.0000
[2019-03-26 12:37:54,602] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7925.3489 3162450636.6881 1775.0000
[2019-03-26 12:37:54,634] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8677.9457 2779415556.2789 895.0000
[2019-03-26 12:37:54,647] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8525.4480 2840940003.3886 1063.0000
[2019-03-26 12:37:54,648] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8270.9508 2927813257.7117 1308.0000
[2019-03-26 12:37:55,666] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1500000, evaluation results [1500000.0, 7925.348890255756, 3162450636.6881056, 1775.0, 8270.950833616178, 2927813257.711662, 1308.0, 8677.94569299147, 2779415556.2789187, 895.0, 8038.513299471942, 3006759520.0644684, 1662.0, 8525.447991159317, 2840940003.3885508, 1063.0]
[2019-03-26 12:37:57,339] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:37:57,350] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3299
[2019-03-26 12:37:57,353] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 53.66666666666667, 1.0, 2.0, 0.5948683764483776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 971085.5900813485, 971085.5900813492, 214405.7497413956], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 481800.0000, 
sim time next is 482400.0000, 
raw observation next is [25.4, 53.0, 1.0, 2.0, 0.6031069522156591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 984257.8149547238, 984257.8149547231, 216162.7083517028], 
processed observation next is [1.0, 0.6086956521739131, 0.4028436018957346, 0.53, 1.0, 1.0, 0.5218156050791074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27340494859853437, 0.2734049485985342, 0.3226309079876161], 
reward next is 0.6774, 
noisyNet noise sample is [array([0.4879048], dtype=float32), 0.35874036]. 
=============================================
[2019-03-26 12:37:58,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:37:58,310] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2127
[2019-03-26 12:37:58,314] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 79.5, 1.0, 2.0, 0.2408018876667755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 398325.5686412163, 398325.5686412163, 159918.3715501579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 505800.0000, 
sim time next is 506400.0000, 
raw observation next is [19.96666666666667, 80.0, 1.0, 2.0, 0.2409283209151263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 398606.4369118143, 398606.436911815, 159924.9910059306], 
processed observation next is [1.0, 0.8695652173913043, 0.14533965244865735, 0.8, 1.0, 1.0, 0.08545580833147746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11072401025328175, 0.11072401025328195, 0.2386940164267621], 
reward next is 0.7613, 
noisyNet noise sample is [array([0.7842558], dtype=float32), -0.85602957]. 
=============================================
[2019-03-26 12:38:00,922] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:38:00,932] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3748
[2019-03-26 12:38:00,941] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.1, 87.0, 1.0, 2.0, 0.21592877989939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 360056.391797975, 360056.3917979744, 157132.8464596541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 604800.0000, 
sim time next is 605400.0000, 
raw observation next is [18.03333333333333, 87.33333333333333, 1.0, 2.0, 0.2152333624153343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 358951.9704273189, 358951.9704273189, 157053.0264604676], 
processed observation next is [1.0, 0.0, 0.05371248025276459, 0.8733333333333333, 1.0, 1.0, 0.054498027006426863, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09970888067425525, 0.09970888067425525, 0.23440750217980239], 
reward next is 0.7656, 
noisyNet noise sample is [array([0.3673175], dtype=float32), 0.8647452]. 
=============================================
[2019-03-26 12:38:01,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.6086821e-33 1.0000000e+00 1.3035541e-38 2.3288079e-29 1.5333894e-19], sum to 1.0000
[2019-03-26 12:38:01,403] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9337
[2019-03-26 12:38:01,410] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.51666666666667, 63.66666666666666, 1.0, 2.0, 0.6213822625029956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1013779.672211657, 1013779.672211657, 220151.5694562062], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 579000.0000, 
sim time next is 579600.0000, 
raw observation next is [23.5, 64.0, 1.0, 2.0, 0.6167703106639623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1005722.353483141, 1005722.353483141, 219114.3159739852], 
processed observation next is [1.0, 0.7391304347826086, 0.31279620853080575, 0.64, 1.0, 1.0, 0.5382774827276654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27936732041198364, 0.27936732041198364, 0.32703629249848537], 
reward next is 0.6730, 
noisyNet noise sample is [array([0.14898252], dtype=float32), 0.36556548]. 
=============================================
[2019-03-26 12:38:05,357] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:38:05,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7164
[2019-03-26 12:38:05,370] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 96.0, 1.0, 2.0, 0.2735505203460503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 443018.5352876315, 443018.5352876315, 163131.2002154956], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1141200.0000, 
sim time next is 1141800.0000, 
raw observation next is [19.75, 95.16666666666667, 1.0, 2.0, 0.2720137407407139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440269.0698627054, 440269.0698627054, 162954.1796970875], 
processed observation next is [1.0, 0.21739130434782608, 0.13507109004739343, 0.9516666666666667, 1.0, 1.0, 0.12290812137435407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1222969638507515, 0.1222969638507515, 0.24321519357774254], 
reward next is 0.7568, 
noisyNet noise sample is [array([-0.84067416], dtype=float32), -0.5741027]. 
=============================================
[2019-03-26 12:38:14,515] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:38:14,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2065
[2019-03-26 12:38:14,529] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.76666666666667, 95.0, 1.0, 2.0, 0.4490403868515986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693652.4545397687, 693652.4545397687, 183952.0066599745], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 996000.0000, 
sim time next is 996600.0000, 
raw observation next is [21.73333333333333, 95.0, 1.0, 2.0, 0.4628382711410866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715629.8000872062, 715629.8000872068, 186219.443187303], 
processed observation next is [1.0, 0.5217391304347826, 0.22906793048973137, 0.95, 1.0, 1.0, 0.3528171941458875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1987860555797795, 0.19878605557977966, 0.2779394674437358], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.42134094], dtype=float32), -0.836593]. 
=============================================
[2019-03-26 12:38:16,991] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:38:17,001] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9392
[2019-03-26 12:38:17,007] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.63333333333334, 63.0, 1.0, 2.0, 0.2852090371693841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 458069.2428035328, 458069.2428035334, 164137.1022577812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 827400.0000, 
sim time next is 828000.0000, 
raw observation next is [24.6, 63.0, 1.0, 2.0, 0.2842858543230357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456857.1522414534, 456857.1522414534, 164056.4707419873], 
processed observation next is [0.0, 0.6086956521739131, 0.36492890995260674, 0.63, 1.0, 1.0, 0.13769380038919965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12690476451151483, 0.12690476451151483, 0.24486040409251833], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.82128835], dtype=float32), 0.14360358]. 
=============================================
[2019-03-26 12:38:17,041] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[79.619774]
 [79.594986]
 [79.55676 ]
 [79.5183  ]
 [79.47804 ]], R is [[79.69888306]
 [79.65691376]
 [79.61525726]
 [79.57391357]
 [79.53279114]].
[2019-03-26 12:38:19,246] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:38:19,253] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5210
[2019-03-26 12:38:19,257] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.4961679268021104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769069.1211663409, 769069.1211663409, 191988.1119634396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 981000.0000, 
sim time next is 981600.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.5464192945959767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 846897.5477797518, 846897.5477797525, 201097.2470592801], 
processed observation next is [1.0, 0.34782608695652173, 0.23696682464454974, 0.93, 1.0, 1.0, 0.4535172224047912, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23524931882770883, 0.23524931882770903, 0.3001451448645972], 
reward next is 0.6999, 
noisyNet noise sample is [array([-0.24780741], dtype=float32), 0.49029297]. 
=============================================
[2019-03-26 12:38:21,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:38:21,663] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0588
[2019-03-26 12:38:21,670] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333333, 88.16666666666667, 1.0, 2.0, 0.3725595977082771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564521.0010182146, 564521.0010182146, 171613.2947494101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1422600.0000, 
sim time next is 1423200.0000, 
raw observation next is [23.46666666666667, 88.33333333333334, 1.0, 2.0, 0.3831691465845746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 577820.2629453858, 577820.2629453852, 172701.2652246524], 
processed observation next is [0.0, 0.4782608695652174, 0.31121642969984215, 0.8833333333333334, 1.0, 1.0, 0.25683029708984895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1605056285959405, 0.16050562859594034, 0.2577630824248543], 
reward next is 0.7422, 
noisyNet noise sample is [array([1.590962], dtype=float32), -1.1217165]. 
=============================================
[2019-03-26 12:38:22,834] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.344247e-31], sum to 1.0000
[2019-03-26 12:38:22,845] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4072
[2019-03-26 12:38:22,852] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 96.0, 1.0, 2.0, 0.3844018335886054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576693.127541197, 576693.1275411964, 172509.1392122945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1044000.0000, 
sim time next is 1044600.0000, 
raw observation next is [22.45, 96.16666666666666, 1.0, 2.0, 0.4858714802355741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731001.4755006314, 731001.475500632, 187824.1420854495], 
processed observation next is [1.0, 0.08695652173913043, 0.26303317535545023, 0.9616666666666666, 1.0, 1.0, 0.3805680484765953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20305596541684204, 0.2030559654168422, 0.28033454042604405], 
reward next is 0.7197, 
noisyNet noise sample is [array([-0.12483212], dtype=float32), -0.51277757]. 
=============================================
[2019-03-26 12:38:32,096] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1217851e-23], sum to 1.0000
[2019-03-26 12:38:32,106] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4870
[2019-03-26 12:38:32,112] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.61666666666667, 81.33333333333334, 1.0, 2.0, 0.3555029715227145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548170.3150592564, 548170.3150592571, 170508.2645285386], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1201800.0000, 
sim time next is 1202400.0000, 
raw observation next is [23.5, 82.0, 1.0, 2.0, 0.354365058693034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546761.3378697922, 546761.3378697922, 170400.5367797331], 
processed observation next is [1.0, 0.9565217391304348, 0.31279620853080575, 0.82, 1.0, 1.0, 0.22212657673859515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15187814940827563, 0.15187814940827563, 0.254329159372736], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.02016433], dtype=float32), -1.3919237]. 
=============================================
[2019-03-26 12:38:32,900] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8491778e-34 4.7233334e-07 0.0000000e+00 5.5252441e-37 9.9999952e-01], sum to 1.0000
[2019-03-26 12:38:32,904] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9853
[2019-03-26 12:38:32,914] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.78333333333333, 67.0, 1.0, 2.0, 0.2627521387944772, 1.0, 2.0, 0.2627521387944772, 1.0, 2.0, 0.4606125214095786, 6.9112, 6.9112, 170.5573041426782, 1189224.717444076, 1189224.717444076, 298688.1917651193], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1097400.0000, 
sim time next is 1098000.0000, 
raw observation next is [25.8, 67.0, 1.0, 2.0, 0.2633105241797448, 1.0, 2.0, 0.2633105241797448, 1.0, 2.0, 0.4614220613226457, 6.911200000000001, 6.9112, 170.5573041426782, 1191082.904475722, 1191082.904475722, 298829.7645366043], 
processed observation next is [1.0, 0.7391304347826086, 0.42180094786729866, 0.67, 1.0, 1.0, 0.11242231828884917, 1.0, 1.0, 0.11242231828884917, 1.0, 1.0, 0.343197635759324, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3308563623543672, 0.3308563623543672, 0.44601457393523025], 
reward next is 0.5540, 
noisyNet noise sample is [array([0.5557077], dtype=float32), -0.30333278]. 
=============================================
[2019-03-26 12:38:32,937] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.26456 ]
 [74.17567 ]
 [74.160416]
 [74.01464 ]
 [73.7684  ]], R is [[74.0884552 ]
 [73.90177155]
 [73.71802521]
 [73.53866577]
 [73.36359406]].
[2019-03-26 12:38:34,529] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0346287e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3361253e-17], sum to 1.0000
[2019-03-26 12:38:34,540] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9090
[2019-03-26 12:38:34,550] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 89.5, 1.0, 2.0, 0.2928451233047513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 469547.5094624624, 469547.5094624631, 164921.6273135842], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1125000.0000, 
sim time next is 1125600.0000, 
raw observation next is [20.86666666666667, 89.66666666666667, 1.0, 2.0, 0.292297394789205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 468788.9812294227, 468788.9812294221, 164869.7039335522], 
processed observation next is [1.0, 0.0, 0.18799368088467638, 0.8966666666666667, 1.0, 1.0, 0.14734625878217472, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13021916145261742, 0.13021916145261725, 0.24607418497545105], 
reward next is 0.7539, 
noisyNet noise sample is [array([-1.621482], dtype=float32), -0.79782856]. 
=============================================
[2019-03-26 12:38:36,983] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 9.9658269e-01 4.7708027e-36 0.0000000e+00 3.4173245e-03], sum to 1.0000
[2019-03-26 12:38:36,988] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6146
[2019-03-26 12:38:36,995] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.46666666666667, 94.33333333333334, 1.0, 2.0, 0.4670434045005027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656258.9195113995, 656258.9195113995, 179065.532972666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2074800.0000, 
sim time next is 2075400.0000, 
raw observation next is [24.45, 94.5, 1.0, 2.0, 0.4668428552344496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655833.8261731579, 655833.8261731579, 179017.4397863802], 
processed observation next is [0.0, 0.0, 0.3578199052132702, 0.945, 1.0, 1.0, 0.35764199425837306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1821760628258772, 0.1821760628258772, 0.2671902086363883], 
reward next is 0.7328, 
noisyNet noise sample is [array([0.14916092], dtype=float32), -0.90068865]. 
=============================================
[2019-03-26 12:38:48,050] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:38:48,061] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9629
[2019-03-26 12:38:48,070] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.25, 98.0, 1.0, 2.0, 0.3048391428512721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485239.5302921806, 485239.53029218, 165995.854233575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1391400.0000, 
sim time next is 1392000.0000, 
raw observation next is [20.26666666666667, 98.0, 1.0, 2.0, 0.3050233257361261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485352.8525933396, 485352.8525933389, 166001.3273036943], 
processed observation next is [0.0, 0.08695652173913043, 0.15955766192733034, 0.98, 1.0, 1.0, 0.16267870570617604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13482023683148323, 0.13482023683148303, 0.24776317508014076], 
reward next is 0.7522, 
noisyNet noise sample is [array([-1.723875], dtype=float32), -1.1774815]. 
=============================================
[2019-03-26 12:38:48,084] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.0902  ]
 [73.31157 ]
 [73.66852 ]
 [73.62981 ]
 [73.664955]], R is [[72.99770355]
 [73.01997375]
 [73.04203033]
 [73.06389618]
 [73.08554077]].
[2019-03-26 12:38:49,350] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:38:49,360] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5854
[2019-03-26 12:38:49,366] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 98.0, 1.0, 2.0, 0.3042925356341632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484791.7632128386, 484791.7632128386, 165969.5747779477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1389600.0000, 
sim time next is 1390200.0000, 
raw observation next is [20.21666666666667, 98.0, 1.0, 2.0, 0.3043332204446107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 484784.5682728032, 484784.5682728025, 165968.0686210966], 
processed observation next is [0.0, 0.08695652173913043, 0.15718799368088482, 0.98, 1.0, 1.0, 0.16184725354772372, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13466238007577866, 0.13466238007577847, 0.24771353525536807], 
reward next is 0.7523, 
noisyNet noise sample is [array([1.4262817], dtype=float32), 0.067709856]. 
=============================================
[2019-03-26 12:38:49,756] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 12:38:49,757] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:38:49,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:38:49,758] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:38:49,759] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:38:49,760] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:38:49,761] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:38:49,761] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:38:49,761] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:38:49,762] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:38:49,763] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:38:49,801] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run62
[2019-03-26 12:38:49,802] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run62
[2019-03-26 12:38:49,802] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run62
[2019-03-26 12:38:49,827] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run62
[2019-03-26 12:38:49,854] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run62
[2019-03-26 12:39:55,651] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23279664], dtype=float32), 0.37705696]
[2019-03-26 12:39:55,653] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 84.0, 1.0, 2.0, 0.5501475598523213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 191825.7660821197]
[2019-03-26 12:39:55,655] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:39:55,657] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8740605e-33], sampled 0.8055946572916525
[2019-03-26 12:40:37,594] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.23279664], dtype=float32), 0.37705696]
[2019-03-26 12:40:37,595] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.75, 55.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 8.096271003593523, 6.9112, 168.9063436992508, 3125150.80522903, 2284453.562216898, 473286.2714587576]
[2019-03-26 12:40:37,596] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:40:37,599] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6717316e-30 1.5444846e-01 1.8860144e-35 2.4664603e-20 8.4555149e-01], sampled 0.008091413546344906
[2019-03-26 12:40:37,601] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 3125150.80522903 W.
[2019-03-26 12:40:40,376] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.23279664], dtype=float32), 0.37705696]
[2019-03-26 12:40:40,377] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.6374738, 86.93956755333335, 1.0, 2.0, 0.3817853079108803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 576336.4573623523, 576336.457362353, 172587.2168937824]
[2019-03-26 12:40:40,380] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:40:40,384] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.9584194e-34], sampled 0.694024858749765
[2019-03-26 12:40:43,384] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23279664], dtype=float32), 0.37705696]
[2019-03-26 12:40:43,385] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.8, 81.50000000000001, 1.0, 2.0, 0.7127881500550555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 996150.1304022573, 996150.1304022567, 223629.0965376089]
[2019-03-26 12:40:43,386] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:40:43,390] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.130691e-31], sampled 0.4868727963965739
[2019-03-26 12:40:45,430] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8082.5954 3157635448.0682 1392.0000
[2019-03-26 12:40:45,986] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8766.9928 2779335449.9027 689.0000
[2019-03-26 12:40:46,102] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8658.6047 2838882472.2286 770.0000
[2019-03-26 12:40:46,132] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8402.3457 2928203261.0012 1036.0000
[2019-03-26 12:40:46,141] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8245.9420 3000128488.2178 1168.0000
[2019-03-26 12:40:47,157] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1525000, evaluation results [1525000.0, 8082.59535107265, 3157635448.068233, 1392.0, 8402.345685810866, 2928203261.0011864, 1036.0, 8766.9927939235, 2779335449.9027348, 689.0, 8245.941987902324, 3000128488.217811, 1168.0, 8658.604673338154, 2838882472.228596, 770.0]
[2019-03-26 12:40:51,589] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:40:51,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0065
[2019-03-26 12:40:51,605] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 96.0, 1.0, 2.0, 0.3428002150982115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531282.5586665417, 531282.558666541, 169200.4993951761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1473600.0000, 
sim time next is 1474200.0000, 
raw observation next is [21.5, 96.0, 1.0, 2.0, 0.3419572955289975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 530467.4887371445, 530467.4887371439, 169148.6773291997], 
processed observation next is [0.0, 0.043478260869565216, 0.21800947867298584, 0.96, 1.0, 1.0, 0.20717746449276808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14735208020476237, 0.1473520802047622, 0.25246071243164137], 
reward next is 0.7475, 
noisyNet noise sample is [array([0.6577502], dtype=float32), 0.3799337]. 
=============================================
[2019-03-26 12:40:52,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:40:52,477] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3697
[2019-03-26 12:40:52,482] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 98.16666666666667, 1.0, 2.0, 0.3148335086808646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497406.8588087675, 497406.8588087681, 166822.593909154], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1483800.0000, 
sim time next is 1484400.0000, 
raw observation next is [20.46666666666667, 98.33333333333334, 1.0, 2.0, 0.3131027740343431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 495233.7151540142, 495233.7151540136, 166672.5821964107], 
processed observation next is [0.0, 0.17391304347826086, 0.16903633491311232, 0.9833333333333334, 1.0, 1.0, 0.1724129807642688, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13756492087611505, 0.13756492087611488, 0.2487650480543443], 
reward next is 0.7512, 
noisyNet noise sample is [array([-0.87230545], dtype=float32), 1.2015246]. 
=============================================
[2019-03-26 12:40:52,838] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:40:52,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6582
[2019-03-26 12:40:52,854] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 51.66666666666667, 1.0, 2.0, 0.3714434896909298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 559244.8657543177, 559244.8657543184, 171039.0920033883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1518000.0000, 
sim time next is 1518600.0000, 
raw observation next is [29.75, 51.83333333333334, 1.0, 2.0, 0.3751803408185347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563479.9992026711, 563479.9992026711, 171361.2091889103], 
processed observation next is [0.0, 0.5652173913043478, 0.6090047393364929, 0.5183333333333334, 1.0, 1.0, 0.247205229901849, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15652222200074198, 0.15652222200074198, 0.2557629987894183], 
reward next is 0.7442, 
noisyNet noise sample is [array([-1.0122855], dtype=float32), -1.3372165]. 
=============================================
[2019-03-26 12:40:53,070] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4665495e-31], sum to 1.0000
[2019-03-26 12:40:53,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6349
[2019-03-26 12:40:53,082] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 90.33333333333334, 1.0, 2.0, 0.5109346140577616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713956.8749801518, 713956.8749801518, 185327.0788929491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1719600.0000, 
sim time next is 1720200.0000, 
raw observation next is [26.06666666666667, 90.66666666666667, 1.0, 2.0, 0.5102235314160604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712962.9068680446, 712962.9068680452, 185213.433450836], 
processed observation next is [1.0, 0.9130434782608695, 0.4344391785150081, 0.9066666666666667, 1.0, 1.0, 0.4099078691759763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19804525190779018, 0.19804525190779035, 0.27643796037438206], 
reward next is 0.7236, 
noisyNet noise sample is [array([-0.5460256], dtype=float32), 0.36982533]. 
=============================================
[2019-03-26 12:41:04,224] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0164348e-37 1.2236229e-16 0.0000000e+00 2.7067259e-36 1.0000000e+00], sum to 1.0000
[2019-03-26 12:41:04,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2669
[2019-03-26 12:41:04,239] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.6, 82.66666666666667, 1.0, 2.0, 0.4112380182635015, 1.0, 2.0, 0.4112380182635015, 1.0, 2.0, 0.7051810367020033, 6.911199999999999, 6.9112, 170.5573041426782, 1724742.928706474, 1724742.928706475, 357889.4811346039], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1690800.0000, 
sim time next is 1691400.0000, 
raw observation next is [27.7, 82.33333333333334, 1.0, 2.0, 0.410003019035006, 1.0, 2.0, 0.410003019035006, 1.0, 2.0, 0.7034794922281531, 6.9112, 6.9112, 170.5573041426782, 1719559.154063116, 1719559.154063116, 357257.1734991198], 
processed observation next is [1.0, 0.5652173913043478, 0.5118483412322274, 0.8233333333333335, 1.0, 1.0, 0.28916026389759764, 1.0, 1.0, 0.28916026389759764, 1.0, 1.0, 0.6383896246684794, 0.0, 0.0, 0.8375144448122397, 0.4776553205730878, 0.4776553205730878, 0.5332196619389848], 
reward next is 0.4668, 
noisyNet noise sample is [array([-0.04924529], dtype=float32), 0.5025279]. 
=============================================
[2019-03-26 12:41:05,365] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.3912423e-32], sum to 1.0000
[2019-03-26 12:41:05,371] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4026
[2019-03-26 12:41:05,376] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 83.0, 1.0, 2.0, 0.4865390156703512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698938.4954897553, 698938.4954897553, 183945.1048319371], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1929600.0000, 
sim time next is 1930200.0000, 
raw observation next is [25.55, 82.83333333333334, 1.0, 2.0, 0.5465004769424989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 784310.8514507177, 784310.8514507182, 193833.0144228241], 
processed observation next is [1.0, 0.34782608695652173, 0.40995260663507116, 0.8283333333333335, 1.0, 1.0, 0.45361503246084206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21786412540297712, 0.21786412540297728, 0.28930300660123004], 
reward next is 0.7107, 
noisyNet noise sample is [array([-0.4364584], dtype=float32), -0.66577816]. 
=============================================
[2019-03-26 12:41:08,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2069700e-30 4.2080726e-03 8.6944130e-36 9.9598843e-33 9.9579191e-01], sum to 1.0000
[2019-03-26 12:41:08,883] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8416
[2019-03-26 12:41:08,889] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.71666666666667, 82.66666666666667, 1.0, 2.0, 0.2658298641246671, 1.0, 2.0, 0.2658298641246671, 1.0, 2.0, 0.4630315429310171, 6.911199999999999, 6.9112, 170.5573041426782, 1191475.338579863, 1191475.338579864, 298597.255327674], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1768200.0000, 
sim time next is 1768800.0000, 
raw observation next is [23.63333333333334, 83.33333333333334, 1.0, 2.0, 0.2597536985254771, 1.0, 2.0, 0.2597536985254771, 1.0, 2.0, 0.4524063302225979, 6.911199999999999, 6.9112, 170.5573041426782, 1164064.894644789, 1164064.894644789, 296283.4740057804], 
processed observation next is [1.0, 0.4782608695652174, 0.3191153238546607, 0.8333333333333335, 1.0, 1.0, 0.10813698617527359, 1.0, 1.0, 0.10813698617527359, 1.0, 1.0, 0.33220284173487546, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3233513596235525, 0.3233513596235525, 0.44221414030713496], 
reward next is 0.5578, 
noisyNet noise sample is [array([0.4295305], dtype=float32), -0.97000086]. 
=============================================
[2019-03-26 12:41:09,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.944258e-21], sum to 1.0000
[2019-03-26 12:41:09,406] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5647
[2019-03-26 12:41:09,412] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 94.83333333333334, 1.0, 2.0, 0.4649283893598679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654700.6073903277, 654700.607390327, 178935.8505071668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1991400.0000, 
sim time next is 1992000.0000, 
raw observation next is [24.43333333333334, 94.66666666666667, 1.0, 2.0, 0.4677261088709502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657310.2762225271, 657310.2762225271, 179178.3553379314], 
processed observation next is [0.0, 0.043478260869565216, 0.3570300157977887, 0.9466666666666668, 1.0, 1.0, 0.3587061552662051, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18258618783959088, 0.18258618783959088, 0.26743038110139017], 
reward next is 0.7326, 
noisyNet noise sample is [array([-1.5918602], dtype=float32), 0.6975665]. 
=============================================
[2019-03-26 12:41:09,423] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.57546 ]
 [75.73395 ]
 [75.61229 ]
 [75.77497 ]
 [75.944725]], R is [[75.43383789]
 [75.41242981]
 [75.39147186]
 [75.3706665 ]
 [75.3500061 ]].
[2019-03-26 12:41:13,020] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6295008e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.9201950e-25], sum to 1.0000
[2019-03-26 12:41:13,026] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5794
[2019-03-26 12:41:13,031] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 91.33333333333334, 1.0, 2.0, 0.4285184590579332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 637823.9111242497, 637823.9111242504, 178076.8735267251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1840800.0000, 
sim time next is 1841400.0000, 
raw observation next is [23.55, 91.0, 1.0, 2.0, 0.4267019877135209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 633488.4147150277, 633488.4147150284, 177618.3907838576], 
processed observation next is [1.0, 0.30434782608695654, 0.3151658767772513, 0.91, 1.0, 1.0, 0.3092795032693023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17596900408750768, 0.17596900408750787, 0.2651020757968024], 
reward next is 0.7349, 
noisyNet noise sample is [array([0.00727337], dtype=float32), 0.066312164]. 
=============================================
[2019-03-26 12:41:13,531] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1591674e-36 1.0000000e+00 0.0000000e+00 3.3879749e-33 7.5403090e-16], sum to 1.0000
[2019-03-26 12:41:13,541] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6334
[2019-03-26 12:41:13,545] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 91.0, 1.0, 2.0, 0.4171403672420007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613512.9859921176, 613512.9859921176, 175550.5973072631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1966800.0000, 
sim time next is 1967400.0000, 
raw observation next is [23.65, 91.5, 1.0, 2.0, 0.4119405009587084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607025.506881203, 607025.506881203, 174971.4143551735], 
processed observation next is [1.0, 0.782608695652174, 0.31990521327014215, 0.915, 1.0, 1.0, 0.29149457946832336, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16861819635588973, 0.16861819635588973, 0.2611513647092142], 
reward next is 0.7388, 
noisyNet noise sample is [array([0.49941418], dtype=float32), -1.4001949]. 
=============================================
[2019-03-26 12:41:18,620] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5401308e-32 9.9998271e-01 1.9529356e-35 8.1330096e-28 1.7268685e-05], sum to 1.0000
[2019-03-26 12:41:18,631] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6606
[2019-03-26 12:41:18,638] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.76666666666667, 82.16666666666667, 1.0, 2.0, 0.5841462171021359, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9807795407890635, 6.9112, 6.9112, 168.9129243052227, 1654282.647006707, 1654282.647006707, 351676.7280837927], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1957800.0000, 
sim time next is 1958400.0000, 
raw observation next is [25.6, 83.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.060504871625332, 6.9112, 168.9117659350127, 1588907.550905837, 1482986.282884309, 316010.7337070209], 
processed observation next is [1.0, 0.6956521739130435, 0.4123222748815167, 0.83, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.014930487162533179, 0.0, 0.8294340988819511, 0.4413632085849547, 0.4119406341345303, 0.4716578115030162], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.562615], dtype=float32), 0.23123144]. 
=============================================
[2019-03-26 12:41:20,500] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:41:20,507] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3367
[2019-03-26 12:41:20,510] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 94.66666666666667, 1.0, 2.0, 0.3977044069292393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595420.8676647046, 595420.8676647046, 174174.446602265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1972200.0000, 
sim time next is 1972800.0000, 
raw observation next is [22.7, 95.0, 1.0, 2.0, 0.396608801474673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594971.5360482881, 594971.5360482887, 174166.8095628442], 
processed observation next is [1.0, 0.8695652173913043, 0.27488151658767773, 0.95, 1.0, 1.0, 0.27302265237912404, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1652698711245245, 0.16526987112452463, 0.25995046203409583], 
reward next is 0.7400, 
noisyNet noise sample is [array([0.81421536], dtype=float32), -0.30785888]. 
=============================================
[2019-03-26 12:41:30,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3257430e-26 1.0000000e+00 6.1132316e-37 7.5720678e-25 2.9801666e-09], sum to 1.0000
[2019-03-26 12:41:30,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6276
[2019-03-26 12:41:30,121] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2239771.098269973 W.
[2019-03-26 12:41:30,131] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.91666666666667, 69.5, 1.0, 2.0, 0.5339111763053401, 1.0, 2.0, 0.5339111763053401, 1.0, 1.0, 0.9272271461182677, 6.9112, 6.9112, 170.5573041426782, 2239771.098269973, 2239771.098269973, 439388.3799982022], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2203800.0000, 
sim time next is 2204400.0000, 
raw observation next is [31.03333333333333, 69.0, 1.0, 2.0, 0.4435734302269605, 1.0, 2.0, 0.4435734302269605, 1.0, 2.0, 0.7703403563292694, 6.9112, 6.9112, 170.5573041426782, 1860476.220406455, 1860476.220406455, 378319.9838233016], 
processed observation next is [1.0, 0.5217391304347826, 0.669826224328594, 0.69, 1.0, 1.0, 0.32960654244212106, 1.0, 1.0, 0.32960654244212106, 1.0, 1.0, 0.7199272638161823, 0.0, 0.0, 0.8375144448122397, 0.5167989501129041, 0.5167989501129041, 0.5646566922735845], 
reward next is 0.4353, 
noisyNet noise sample is [array([-0.5476381], dtype=float32), -0.578268]. 
=============================================
[2019-03-26 12:41:31,212] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5717575e-37 1.0000000e+00 2.0564189e-38 8.8779042e-33 2.2015612e-20], sum to 1.0000
[2019-03-26 12:41:31,224] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0970
[2019-03-26 12:41:31,228] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333333, 93.83333333333334, 1.0, 2.0, 0.5110489068622259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714116.6362485503, 714116.6362485496, 185345.2931599961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2163000.0000, 
sim time next is 2163600.0000, 
raw observation next is [25.6, 94.0, 1.0, 2.0, 0.5105656148420609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713441.0790447135, 713441.0790447135, 185268.0269526198], 
processed observation next is [1.0, 0.043478260869565216, 0.4123222748815167, 0.94, 1.0, 1.0, 0.410320017882001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19817807751242042, 0.19817807751242042, 0.27651944321286537], 
reward next is 0.7235, 
noisyNet noise sample is [array([-0.53726834], dtype=float32), 0.8288729]. 
=============================================
[2019-03-26 12:41:41,593] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 12:41:41,595] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:41:41,596] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:41:41,597] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:41:41,599] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:41:41,599] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:41:41,601] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:41:41,602] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:41:41,604] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:41:41,604] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:41:41,605] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:41:41,634] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run63
[2019-03-26 12:41:41,660] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run63
[2019-03-26 12:41:41,684] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run63
[2019-03-26 12:41:41,706] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run63
[2019-03-26 12:41:41,706] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run63
[2019-03-26 12:41:51,102] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:41:51,105] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.16666666666666, 69.0, 1.0, 2.0, 0.2550291931835963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 418790.1225689863, 418790.1225689856, 161410.4931594187]
[2019-03-26 12:41:51,106] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:41:51,109] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4752856365860664
[2019-03-26 12:42:00,314] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:42:00,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.18333333333333, 46.83333333333334, 1.0, 2.0, 0.3080607909101908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 505491.3080749356, 505491.3080749356, 167216.3101167089]
[2019-03-26 12:42:00,316] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:42:00,320] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7821315523433195
[2019-03-26 12:42:04,915] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:42:04,916] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.65, 51.0, 1.0, 2.0, 0.2880700709675729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 464586.6198987179, 464586.6198987179, 164585.8479003081]
[2019-03-26 12:42:04,918] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:42:04,921] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5484870906976171
[2019-03-26 12:42:14,949] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:42:14,950] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.41666666666666, 89.16666666666666, 1.0, 2.0, 0.4289052336150918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 643590.8593529591, 643590.8593529584, 178746.3598850783]
[2019-03-26 12:42:14,952] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:42:14,958] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2542149568467912
[2019-03-26 12:42:22,368] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:42:22,371] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.8, 61.0, 1.0, 2.0, 0.569869285708768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796340.3842187244, 796340.3842187251, 195266.8199608327]
[2019-03-26 12:42:22,373] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:42:22,376] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2660086820506109
[2019-03-26 12:42:27,058] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:42:27,059] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.099323185, 99.922216055, 1.0, 2.0, 0.3952578406265184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 587145.5991431486, 587145.5991431492, 173273.9958172922]
[2019-03-26 12:42:27,060] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:42:27,063] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.25900348978213095
[2019-03-26 12:42:30,489] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:42:30,490] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5566791690368277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777901.6489690338, 777901.6489690333, 192952.5473174102]
[2019-03-26 12:42:30,491] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:42:30,495] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.512699880308504
[2019-03-26 12:42:34,264] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:42:34,266] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.6, 84.0, 1.0, 2.0, 0.7683391592018998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1073824.153776662, 1073824.153776662, 236287.4384678275]
[2019-03-26 12:42:34,268] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:42:34,272] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5386835e-32], sampled 0.39259740710444146
[2019-03-26 12:43:21,268] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:43:21,269] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.46051327833334, 86.47410508000002, 1.0, 2.0, 0.3669450293360909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 557286.5941619533, 557286.5941619539, 171028.7492202964]
[2019-03-26 12:43:21,271] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:43:21,274] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07700763884627115
[2019-03-26 12:43:21,779] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:43:21,781] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.2, 45.0, 1.0, 2.0, 0.5717521529086298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798972.5090914161, 798972.5090914161, 195600.0395841948]
[2019-03-26 12:43:21,782] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:43:21,785] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.47307239575965065
[2019-03-26 12:43:26,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:43:26,597] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.4, 93.5, 1.0, 2.0, 0.5059415729660321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 716214.6686024184, 716214.6686024178, 185715.4746040754]
[2019-03-26 12:43:26,598] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:43:26,600] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14413590215731964
[2019-03-26 12:43:32,818] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.23253876], dtype=float32), 0.36938593]
[2019-03-26 12:43:32,821] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.03333333333333, 76.66666666666667, 1.0, 2.0, 0.5490048720518695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767173.738740574, 767173.738740574, 191629.7028632362]
[2019-03-26 12:43:32,823] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:43:32,825] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.944660621506059
[2019-03-26 12:43:37,104] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8302.5910 2927018889.8417 1241.0000
[2019-03-26 12:43:37,602] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8698.6164 2779216767.2013 863.0000
[2019-03-26 12:43:37,640] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8071.1461 3004858808.1790 1571.0000
[2019-03-26 12:43:37,824] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8555.9589 2840165071.5054 996.0000
[2019-03-26 12:43:37,864] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7946.4083 3161392154.9538 1717.0000
[2019-03-26 12:43:38,881] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1550000, evaluation results [1550000.0, 7946.408266950107, 3161392154.9537745, 1717.0, 8302.591042234404, 2927018889.8416533, 1241.0, 8698.616441585707, 2779216767.2013063, 863.0, 8071.146149961859, 3004858808.1790047, 1571.0, 8555.958863972455, 2840165071.5053935, 996.0]
[2019-03-26 12:43:45,190] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:43:45,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7606
[2019-03-26 12:43:45,207] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4981254129902304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696052.0137000309, 696052.0137000309, 183302.9807375376], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2650200.0000, 
sim time next is 2650800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4977197739144745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695485.011262063, 695485.0112620625, 183239.6990366384], 
processed observation next is [0.0, 0.6956521739130435, 0.4312796208530806, 0.89, 1.0, 1.0, 0.3948431011017765, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19319028090612864, 0.19319028090612847, 0.2734920881143857], 
reward next is 0.7265, 
noisyNet noise sample is [array([0.85519755], dtype=float32), -0.40889016]. 
=============================================
[2019-03-26 12:43:47,188] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:43:47,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0144
[2019-03-26 12:43:47,200] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.4420703068049817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 634967.3739069798, 634967.3739069805, 177237.9213649489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2687400.0000, 
sim time next is 2688000.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.4412583521901511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 633801.0473858996, 633801.047385899, 177121.2809928687], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 1.0, 1.0, 0.32681729179536273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17605584649608322, 0.17605584649608305, 0.26436012088487865], 
reward next is 0.7356, 
noisyNet noise sample is [array([-0.56201136], dtype=float32), 0.014011004]. 
=============================================
[2019-03-26 12:43:47,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.6424  ]
 [73.64356 ]
 [73.550446]
 [72.6746  ]
 [72.520515]], R is [[73.66181183]
 [73.66065979]
 [73.6594696 ]
 [73.6584549 ]
 [73.65768433]].
[2019-03-26 12:43:47,604] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4954868e-35 3.0434355e-31], sum to 1.0000
[2019-03-26 12:43:47,612] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9259
[2019-03-26 12:43:47,618] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 83.0, 1.0, 2.0, 0.5099099938752594, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712524.6369833215, 712524.6369833208, 185166.13173778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2481600.0000, 
sim time next is 2482200.0000, 
raw observation next is [28.15, 83.5, 1.0, 2.0, 0.507630211870906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709337.9114357067, 709337.9114357072, 184803.1326986041], 
processed observation next is [1.0, 0.7391304347826086, 0.533175355450237, 0.835, 1.0, 1.0, 0.4067833877962722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19703830873214073, 0.1970383087321409, 0.27582557119194645], 
reward next is 0.7242, 
noisyNet noise sample is [array([-0.27541262], dtype=float32), -0.7264131]. 
=============================================
[2019-03-26 12:43:52,815] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:43:52,826] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9765
[2019-03-26 12:43:52,830] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 97.0, 1.0, 2.0, 0.31207332160021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495274.6924825286, 495274.6924825286, 166706.4719911926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2926800.0000, 
sim time next is 2927400.0000, 
raw observation next is [20.58333333333334, 96.50000000000001, 1.0, 2.0, 0.312656314054623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495910.2715004249, 495910.2715004249, 166748.6065074436], 
processed observation next is [1.0, 0.9130434782608695, 0.17456556082148533, 0.9650000000000002, 1.0, 1.0, 0.1718750771742446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13775285319456246, 0.13775285319456246, 0.24887851717528894], 
reward next is 0.7511, 
noisyNet noise sample is [array([0.66933477], dtype=float32), -0.6553355]. 
=============================================
[2019-03-26 12:43:53,198] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:43:53,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0776
[2019-03-26 12:43:53,213] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.68333333333334, 84.66666666666667, 1.0, 2.0, 0.5394514929958127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753819.2085097961, 753819.2085097961, 190008.0673911301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2577000.0000, 
sim time next is 2577600.0000, 
raw observation next is [27.6, 85.0, 1.0, 2.0, 0.5374626672178296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751039.0788716051, 751039.0788716044, 189673.8380465444], 
processed observation next is [1.0, 0.8695652173913043, 0.5071090047393366, 0.85, 1.0, 1.0, 0.44272610508172244, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20862196635322364, 0.20862196635322344, 0.2830952806664842], 
reward next is 0.7169, 
noisyNet noise sample is [array([-1.5139385], dtype=float32), -0.38266325]. 
=============================================
[2019-03-26 12:43:57,900] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:43:57,911] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4153
[2019-03-26 12:43:57,919] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3971945146211823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592670.2684337221, 592670.2684337215, 173863.0354437851], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2665200.0000, 
sim time next is 2665800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3969508427242628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592308.4251204082, 592308.4251204075, 173829.7584087286], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27343475027019615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16453011808900228, 0.1645301180890021, 0.2594474006100427], 
reward next is 0.7406, 
noisyNet noise sample is [array([1.293728], dtype=float32), -0.23649384]. 
=============================================
[2019-03-26 12:44:02,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:44:02,894] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7277
[2019-03-26 12:44:02,898] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3931210197645402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586595.8043169726, 586595.804316972, 173306.165648713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2742000.0000, 
sim time next is 2742600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3927254813660197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586005.8747645393, 586005.87476454, 173252.3685203411], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2683439534530358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16277940965681648, 0.16277940965681667, 0.2585856246572255], 
reward next is 0.7414, 
noisyNet noise sample is [array([-1.2257142], dtype=float32), 0.639033]. 
=============================================
[2019-03-26 12:44:06,161] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:44:06,170] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1071
[2019-03-26 12:44:06,176] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 95.0, 1.0, 2.0, 0.3245970395610622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511336.1056912722, 511336.1056912722, 167844.7569561113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2919000.0000, 
sim time next is 2919600.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3214245079321091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 507786.9718983622, 507786.9718983629, 167602.9986496499], 
processed observation next is [1.0, 0.8260869565217391, 0.19431279620853087, 0.94, 1.0, 1.0, 0.18243916618326395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14105193663843396, 0.14105193663843416, 0.2501537293278357], 
reward next is 0.7498, 
noisyNet noise sample is [array([1.4590254], dtype=float32), -0.66573685]. 
=============================================
[2019-03-26 12:44:09,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:44:09,291] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3812
[2019-03-26 12:44:09,296] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3505705274872588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 540041.7948673894, 540041.79486739, 169819.9699637635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2852400.0000, 
sim time next is 2853000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3502677172761182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539576.9196183477, 539576.9196183482, 169781.7801830027], 
processed observation next is [1.0, 0.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21719002081460026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14988247767176324, 0.1498824776717634, 0.25340564206418315], 
reward next is 0.7466, 
noisyNet noise sample is [array([0.48309818], dtype=float32), 0.95635927]. 
=============================================
[2019-03-26 12:44:09,310] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.189064]
 [73.54372 ]
 [74.791435]
 [77.494675]
 [77.41363 ]], R is [[72.95738983]
 [72.97434998]
 [72.99102783]
 [73.00740814]
 [73.02333832]].
[2019-03-26 12:44:11,179] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:44:11,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0482
[2019-03-26 12:44:11,196] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 90.66666666666667, 1.0, 2.0, 0.5593425410334616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847230.9749196429, 847230.9749196429, 201459.0260754201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2895600.0000, 
sim time next is 2896200.0000, 
raw observation next is [23.0, 91.5, 1.0, 2.0, 0.527734823461151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796695.6009873647, 796695.6009873652, 195321.1992612997], 
processed observation next is [1.0, 0.5217391304347826, 0.28909952606635075, 0.915, 1.0, 1.0, 0.43100581139897703, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2213043336076013, 0.22130433360760146, 0.29152417800193986], 
reward next is 0.7085, 
noisyNet noise sample is [array([-0.64619094], dtype=float32), -0.74813396]. 
=============================================
[2019-03-26 12:44:22,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:44:22,168] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6958
[2019-03-26 12:44:22,178] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 75.0, 1.0, 2.0, 0.6145677356389444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858827.6466668961, 858827.6466668961, 203491.7290027667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3969000.0000, 
sim time next is 3969600.0000, 
raw observation next is [31.33333333333333, 76.33333333333333, 1.0, 2.0, 0.6164178685432905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 861414.1632499291, 861414.1632499298, 203844.9507847506], 
processed observation next is [0.0, 0.9565217391304348, 0.6840442338072668, 0.7633333333333333, 1.0, 1.0, 0.5378528536666151, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2392817120138692, 0.23928171201386939, 0.3042461952011203], 
reward next is 0.6958, 
noisyNet noise sample is [array([0.89048696], dtype=float32), -0.25601488]. 
=============================================
[2019-03-26 12:44:28,533] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:44:28,541] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6615
[2019-03-26 12:44:28,549] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 93.33333333333334, 1.0, 2.0, 0.4839596938315129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676251.3459021929, 676251.3459021929, 181120.9643241533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3201600.0000, 
sim time next is 3202200.0000, 
raw observation next is [25.0, 93.0, 1.0, 2.0, 0.482662126705178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674437.641003603, 674437.641003603, 180924.049788991], 
processed observation next is [0.0, 0.043478260869565216, 0.38388625592417064, 0.93, 1.0, 1.0, 0.3767013574761181, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1873437891676675, 0.1873437891676675, 0.27003589520744925], 
reward next is 0.7300, 
noisyNet noise sample is [array([-1.4059458], dtype=float32), -0.9157734]. 
=============================================
[2019-03-26 12:44:33,049] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 12:44:33,054] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:44:33,054] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:44:33,054] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:44:33,055] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:44:33,055] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:44:33,058] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:44:33,057] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:44:33,061] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:44:33,062] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:44:33,059] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:44:33,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run64
[2019-03-26 12:44:33,117] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run64
[2019-03-26 12:44:33,119] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run64
[2019-03-26 12:44:33,167] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run64
[2019-03-26 12:44:33,168] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run64
[2019-03-26 12:44:48,791] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:44:48,793] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.7, 72.0, 1.0, 2.0, 0.4394282649871268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711415.4711745853, 711415.4711745853, 185192.051844127]
[2019-03-26 12:44:48,794] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:44:48,797] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.38192625494059806
[2019-03-26 12:44:59,087] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:44:59,088] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.55, 84.0, 1.0, 2.0, 0.7829286534486163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1196817.775381536, 1196817.775381536, 253267.13430812]
[2019-03-26 12:44:59,089] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:44:59,091] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9330245694100683
[2019-03-26 12:45:00,586] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:45:00,588] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.2, 93.16666666666666, 1.0, 2.0, 0.3685323613740975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565961.9757733321, 565961.9757733321, 171955.7524334078]
[2019-03-26 12:45:00,588] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:45:00,590] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2928386288348178
[2019-03-26 12:45:08,626] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:45:08,628] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.85, 76.5, 1.0, 2.0, 0.5557236987819503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 816887.9374751004, 816887.9374751004, 197858.7328494205]
[2019-03-26 12:45:08,630] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:45:08,633] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5138811106461707
[2019-03-26 12:45:22,976] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:45:22,977] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.93333333333333, 64.0, 1.0, 2.0, 0.5310063215389439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742013.9654443464, 742013.9654443464, 188596.5346909824]
[2019-03-26 12:45:22,979] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:45:22,981] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9147796054325343
[2019-03-26 12:45:38,731] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:45:38,732] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 85.66666666666667, 1.0, 2.0, 0.4953754810496339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692208.163827431, 692208.163827431, 182873.8577909603]
[2019-03-26 12:45:38,734] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 12:45:38,735] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.17866479331224538
[2019-03-26 12:45:38,809] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:45:38,810] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.40000000000001, 48.0, 1.0, 2.0, 0.6817241701573943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 952717.4950716476, 952717.4950716483, 216953.6952039981]
[2019-03-26 12:45:38,811] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:45:38,813] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0052830143382009975
[2019-03-26 12:45:55,391] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:45:55,393] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.55, 55.5, 1.0, 2.0, 0.9928285801292128, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005992976631033, 6.9112, 168.912315929138, 2284964.365452916, 2217715.220343345, 461130.9865916568]
[2019-03-26 12:45:55,394] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:45:55,398] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7997407e-28 1.0000000e+00 3.4581836e-34 1.0517811e-20 6.8315348e-14], sampled 0.20251565712276676
[2019-03-26 12:45:55,399] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2284964.365452916 W.
[2019-03-26 12:46:09,071] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:46:09,072] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.8, 91.0, 1.0, 2.0, 0.6353402587357898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887868.3412344487, 887868.3412344487, 207508.0074926001]
[2019-03-26 12:46:09,072] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:46:09,077] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.47864049656241103
[2019-03-26 12:46:10,065] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.27351886], dtype=float32), 0.36867285]
[2019-03-26 12:46:10,066] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.8, 85.0, 1.0, 2.0, 0.552665558431716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 772291.0037117143, 772291.003711715, 192258.3426855786]
[2019-03-26 12:46:10,067] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:46:10,070] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8412286891779294
[2019-03-26 12:46:28,095] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7873.8470 3164152625.7410 1857.0000
[2019-03-26 12:46:28,601] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.0744 2927734378.6263 1357.0000
[2019-03-26 12:46:29,003] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.3619 2779615641.7404 938.0000
[2019-03-26 12:46:29,065] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4521 3007907526.1473 1766.0000
[2019-03-26 12:46:29,093] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.9855 2842689436.8023 1145.0000
[2019-03-26 12:46:30,106] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1575000, evaluation results [1575000.0, 7873.847027243989, 3164152625.7410445, 1857.0, 8249.07435807046, 2927734378.626281, 1357.0, 8658.361909277191, 2779615641.7404113, 938.0, 7997.452106576753, 3007907526.147288, 1766.0, 8493.985476963662, 2842689436.8022785, 1145.0]
[2019-03-26 12:46:31,108] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:46:31,116] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4700
[2019-03-26 12:46:31,122] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333333, 77.33333333333333, 1.0, 2.0, 0.4831848330666783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675168.2657133715, 675168.2657133715, 181003.4236145625], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3310800.0000, 
sim time next is 3311400.0000, 
raw observation next is [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4854969919839591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678400.1444755488, 678400.1444755488, 181354.9769944343], 
processed observation next is [0.0, 0.30434782608695654, 0.5102685624012641, 0.7566666666666667, 1.0, 1.0, 0.38011685781199894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1884444845765413, 0.1884444845765413, 0.2706790701409467], 
reward next is 0.7293, 
noisyNet noise sample is [array([-0.05386751], dtype=float32), -1.8727192]. 
=============================================
[2019-03-26 12:46:32,609] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5245286e-28 1.0000000e+00 1.6883442e-34 1.4210238e-26 1.4318493e-15], sum to 1.0000
[2019-03-26 12:46:32,618] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5072
[2019-03-26 12:46:32,626] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2567527.477461425 W.
[2019-03-26 12:46:32,631] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.66666666666666, 64.16666666666667, 1.0, 2.0, 0.9179411409516492, 1.0, 2.0, 0.9179411409516492, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2567527.477461425, 2567527.477461425, 481377.2916829146], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3427800.0000, 
sim time next is 3428400.0000, 
raw observation next is [32.33333333333334, 65.33333333333334, 1.0, 2.0, 0.8963846593057205, 1.0, 2.0, 0.8963846593057205, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2507172.422797213, 2507172.422797213, 469526.0251748787], 
processed observation next is [1.0, 0.6956521739130435, 0.7314375987361774, 0.6533333333333334, 1.0, 1.0, 0.8751622401273741, 1.0, 1.0, 0.8751622401273741, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6964367841103369, 0.6964367841103369, 0.7007851122013116], 
reward next is 0.2992, 
noisyNet noise sample is [array([-0.36123562], dtype=float32), 0.343041]. 
=============================================
[2019-03-26 12:46:34,771] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:46:34,779] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0384
[2019-03-26 12:46:34,783] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 79.0, 1.0, 2.0, 0.5622576004434091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 785699.8138503868, 785699.8138503875, 193925.0032059908], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3351000.0000, 
sim time next is 3351600.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5573115122804657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778785.6072839798, 778785.6072839798, 193062.3429412053], 
processed observation next is [0.0, 0.8260869565217391, 0.5734597156398105, 0.79, 1.0, 1.0, 0.46664037624152493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21632933535666105, 0.21632933535666105, 0.28815275065851537], 
reward next is 0.7118, 
noisyNet noise sample is [array([0.8589249], dtype=float32), -0.23672208]. 
=============================================
[2019-03-26 12:46:39,700] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:46:39,711] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2851
[2019-03-26 12:46:39,717] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7947494250403578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1110754.217460622, 1110754.217460622, 242622.5759142873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3551400.0000, 
sim time next is 3552000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7789486931806512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1088659.547395305, 1088659.547395306, 238805.6360232325], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7336731243140375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30240542983202917, 0.30240542983202945, 0.3564263224227351], 
reward next is 0.6436, 
noisyNet noise sample is [array([-1.2282615], dtype=float32), 0.020947197]. 
=============================================
[2019-03-26 12:46:39,734] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[62.96337 ]
 [62.939445]
 [63.69737 ]
 [66.683395]
 [66.34374 ]], R is [[62.95540237]
 [62.96372604]
 [62.93132401]
 [62.87071609]
 [62.9715271 ]].
[2019-03-26 12:46:42,089] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2504733e-22 9.9995005e-01 3.8255584e-31 3.5695824e-16 4.9960199e-05], sum to 1.0000
[2019-03-26 12:46:42,100] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2885
[2019-03-26 12:46:42,106] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2518379.506058584 W.
[2019-03-26 12:46:42,111] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 59.66666666666667, 1.0, 2.0, 0.6002583152963384, 1.0, 2.0, 0.6002583152963384, 1.0, 1.0, 1.03, 6.925195899323693, 6.9112, 170.5573041426782, 2518379.506058584, 2508353.674056238, 488071.6290005818], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3513000.0000, 
sim time next is 3513600.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.5910351639170685, 1.0, 2.0, 0.5910351639170685, 1.0, 2.0, 1.026432621408462, 6.911199999999999, 6.9112, 170.5573041426782, 2479645.444395541, 2479645.444395542, 483804.6211781737], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.59, 1.0, 1.0, 0.5072712818277932, 1.0, 1.0, 0.5072712818277932, 1.0, 1.0, 1.032234904156661, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6887904012209837, 0.6887904012209839, 0.7220964495196622], 
reward next is 0.2779, 
noisyNet noise sample is [array([-0.5402541], dtype=float32), -1.0944785]. 
=============================================
[2019-03-26 12:46:43,397] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:46:43,407] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4297
[2019-03-26 12:46:43,411] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 63.0, 1.0, 2.0, 0.6134717693504033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 857295.4705314585, 857295.4705314592, 203283.5157147623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3841200.0000, 
sim time next is 3841800.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.6240732084916257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 872116.5228211972, 872116.5228211966, 205316.8012722254], 
processed observation next is [0.0, 0.4782608695652174, 0.8104265402843602, 0.63, 1.0, 1.0, 0.5470761548091876, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24225458967255478, 0.24225458967255462, 0.3064429869734707], 
reward next is 0.6936, 
noisyNet noise sample is [array([1.5623682], dtype=float32), -0.5720176]. 
=============================================
[2019-03-26 12:46:46,906] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:46:46,913] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9726
[2019-03-26 12:46:46,919] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5261972484939502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735291.5701220223, 735291.5701220216, 187802.2786802547], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3618600.0000, 
sim time next is 3619200.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5251488687914434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733826.0911034971, 733826.0911034965, 187630.0401750312], 
processed observation next is [1.0, 0.9130434782608695, 0.5576619273301741, 0.7566666666666667, 1.0, 1.0, 0.4278902033631848, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2038405808620825, 0.20384058086208237, 0.2800448360821361], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.32245454], dtype=float32), 0.015261663]. 
=============================================
[2019-03-26 12:46:58,123] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.8428290e-17 9.9992692e-01 6.9557984e-28 7.3066258e-05 6.7885114e-10], sum to 1.0000
[2019-03-26 12:46:58,133] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3070
[2019-03-26 12:46:58,143] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2539474.292565729 W.
[2019-03-26 12:46:58,148] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.16666666666667, 71.0, 1.0, 2.0, 0.9079217622619623, 1.0, 2.0, 0.9079217622619623, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2539474.292565729, 2539474.292565729, 475840.5974384139], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4101000.0000, 
sim time next is 4101600.0000, 
raw observation next is [32.33333333333334, 71.0, 1.0, 2.0, 0.9153882225952643, 1.0, 2.0, 0.9153882225952643, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2560379.520284847, 2560379.520284847, 479966.02234149], 
processed observation next is [1.0, 0.4782608695652174, 0.7314375987361774, 0.71, 1.0, 1.0, 0.8980580995123666, 1.0, 1.0, 0.8980580995123666, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7112165334124575, 0.7112165334124575, 0.716367197524612], 
reward next is 0.2836, 
noisyNet noise sample is [array([-0.07703123], dtype=float32), 0.33345765]. 
=============================================
[2019-03-26 12:47:00,209] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:47:00,222] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2033
[2019-03-26 12:47:00,227] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5205810441340722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727440.9746776012, 727440.9746776005, 186883.3720751748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3799200.0000, 
sim time next is 3799800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.519615069662219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726090.6958310549, 726090.6958310549, 186726.3315488285], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42122297549664944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20169185995307082, 0.20169185995307082, 0.27869601723705745], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.35092503], dtype=float32), -0.484656]. 
=============================================
[2019-03-26 12:47:01,946] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:47:01,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9454
[2019-03-26 12:47:01,957] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 55.0, 1.0, 2.0, 0.5874360757035686, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820897.8581129654, 820897.8581129654, 198429.3402831675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3859200.0000, 
sim time next is 3859800.0000, 
raw observation next is [35.0, 54.66666666666667, 1.0, 2.0, 0.603812365442395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843791.5849227291, 843791.5849227291, 201457.7535669962], 
processed observation next is [0.0, 0.6956521739130435, 0.8578199052132701, 0.5466666666666667, 1.0, 1.0, 0.5226655005330061, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23438655136742473, 0.23438655136742473, 0.3006832142790988], 
reward next is 0.6993, 
noisyNet noise sample is [array([0.12042536], dtype=float32), 0.8857989]. 
=============================================
[2019-03-26 12:47:08,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:47:08,420] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2847
[2019-03-26 12:47:08,424] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5973702957298898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834785.6409243108, 834785.6409243108, 200257.6478620151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3936000.0000, 
sim time next is 3936600.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5959703130598959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832828.4903472299, 832828.4903472299, 199998.3037417744], 
processed observation next is [0.0, 0.5652173913043478, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5132172446504769, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23134124731867498, 0.23134124731867498, 0.2985049309578722], 
reward next is 0.7015, 
noisyNet noise sample is [array([-1.7520669], dtype=float32), 0.39309227]. 
=============================================
[2019-03-26 12:47:09,345] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3686583e-19 9.9997878e-01 2.6127674e-28 6.6784958e-23 2.1202148e-05], sum to 1.0000
[2019-03-26 12:47:09,355] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9293
[2019-03-26 12:47:09,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1923309.877128724 W.
[2019-03-26 12:47:09,374] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.4585407525016811, 1.0, 1.0, 0.4585407525016811, 1.0, 1.0, 0.796333645351346, 6.9112, 6.9112, 170.5573041426782, 1923309.877128724, 1923309.877128724, 387660.8308080292], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4155600.0000, 
sim time next is 4156200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5323908681944557, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9245868737032366, 6.9112, 6.9112, 168.9129561860859, 1488420.147801124, 1488420.147801124, 326272.6941959737], 
processed observation next is [1.0, 0.08695652173913043, 0.5734597156398105, 0.84, 1.0, 1.0, 0.43661550384874176, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.9080327728088251, 0.0, 0.0, 0.8294399435596197, 0.4134500410558678, 0.4134500410558678, 0.4869741704417518], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.90779847], dtype=float32), 1.6237588]. 
=============================================
[2019-03-26 12:47:22,261] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8757651e-38 1.0000000e+00 0.0000000e+00 9.7145442e-38 1.2440173e-37], sum to 1.0000
[2019-03-26 12:47:22,270] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7178
[2019-03-26 12:47:22,276] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.8569236478978629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1197698.891135359, 1197698.891135359, 258369.2484678563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4172400.0000, 
sim time next is 4173000.0000, 
raw observation next is [30.33333333333333, 83.16666666666667, 1.0, 2.0, 0.957621953381459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1338530.896345231, 1338530.896345231, 286286.2908198257], 
processed observation next is [1.0, 0.30434782608695654, 0.6366508688783569, 0.8316666666666667, 1.0, 1.0, 0.9489421125077818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3718141378736753, 0.3718141378736753, 0.4272929713728742], 
reward next is 0.5727, 
noisyNet noise sample is [array([0.6527475], dtype=float32), 0.7783041]. 
=============================================
[2019-03-26 12:47:22,299] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[57.456863]
 [56.988358]
 [57.239143]
 [57.459644]
 [56.813976]], R is [[55.99991608]
 [56.05429459]
 [56.11202621]
 [56.17922211]
 [56.25617218]].
[2019-03-26 12:47:24,655] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 12:47:24,656] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:47:24,657] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:47:24,658] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:47:24,658] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:47:24,659] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:47:24,662] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:47:24,660] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:47:24,665] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:47:24,665] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:47:24,665] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:47:24,686] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run65
[2019-03-26 12:47:24,712] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run65
[2019-03-26 12:47:24,742] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run65
[2019-03-26 12:47:24,766] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run65
[2019-03-26 12:47:24,767] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run65
[2019-03-26 12:47:33,865] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1861975], dtype=float32), 0.3762096]
[2019-03-26 12:47:33,867] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.4, 53.5, 1.0, 2.0, 0.4151778774223552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666566.5182865998, 666566.5182865998, 181086.8405743054]
[2019-03-26 12:47:33,867] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:47:33,872] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.20129110405688944
[2019-03-26 12:47:35,960] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.1861975], dtype=float32), 0.3762096]
[2019-03-26 12:47:35,962] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.8, 74.0, 1.0, 2.0, 0.3719148415018976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586568.7366534609, 586568.7366534609, 174009.972652377]
[2019-03-26 12:47:35,963] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:47:35,966] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4994407313430038
[2019-03-26 12:48:11,857] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1861975], dtype=float32), 0.3762096]
[2019-03-26 12:48:11,860] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.44407091, 74.64734429, 1.0, 2.0, 0.6248387451782286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 935324.4527605993, 935324.4527605999, 213287.3912346182]
[2019-03-26 12:48:11,860] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:48:11,864] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6072091496508638
[2019-03-26 12:48:14,981] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.1861975], dtype=float32), 0.3762096]
[2019-03-26 12:48:14,982] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.15912500833333, 91.63807265000001, 1.0, 2.0, 0.6924820477588642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 967758.6047663495, 967758.6047663488, 219237.8964388348]
[2019-03-26 12:48:14,983] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:48:14,986] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8242641650497571
[2019-03-26 12:48:15,606] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.1861975], dtype=float32), 0.3762096]
[2019-03-26 12:48:15,607] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.95, 63.5, 1.0, 2.0, 0.9035256202754154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1262871.937940169, 1262871.937940169, 270908.5731113324]
[2019-03-26 12:48:15,609] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:48:15,611] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7960822e-36 1.5087233e-35], sampled 0.1703652821831344
[2019-03-26 12:48:31,790] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.1861975], dtype=float32), 0.3762096]
[2019-03-26 12:48:31,793] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.509568265, 78.67171376833333, 1.0, 2.0, 0.5422958761423604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757795.3129652061, 757795.3129652061, 190487.7120396122]
[2019-03-26 12:48:31,794] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:48:31,796] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.31022506291658014
[2019-03-26 12:49:19,581] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.0758 3164114467.8291 1876.0000
[2019-03-26 12:49:19,995] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.2118 2779655412.9160 933.0000
[2019-03-26 12:49:20,304] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.6849 2927920349.9957 1355.0000
[2019-03-26 12:49:20,370] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8491.8115 2842593226.5799 1158.0000
[2019-03-26 12:49:20,499] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8008.8031 3007428421.9489 1738.0000
[2019-03-26 12:49:21,516] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1600000, evaluation results [1600000.0, 7881.075758324215, 3164114467.829077, 1876.0, 8249.684887473943, 2927920349.9957104, 1355.0, 8660.21183644404, 2779655412.915969, 933.0, 8008.803148221621, 3007428421.948869, 1738.0, 8491.811454842093, 2842593226.5799274, 1158.0]
[2019-03-26 12:49:27,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.25248900e-36 1.00000000e+00 0.00000000e+00 1.06545455e-35
 2.17636881e-35], sum to 1.0000
[2019-03-26 12:49:27,799] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9206
[2019-03-26 12:49:27,806] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6186151948370953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864486.0701509388, 864486.0701509388, 204265.4046309429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4325400.0000, 
sim time next is 4326000.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6182377847733076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863958.4425649181, 863958.4425649188, 204193.0585556088], 
processed observation next is [1.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5400455238232621, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2399884562680328, 0.239988456268033, 0.30476575903822206], 
reward next is 0.6952, 
noisyNet noise sample is [array([-1.4393396], dtype=float32), -1.164754]. 
=============================================
[2019-03-26 12:49:27,815] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[53.157665]
 [53.304058]
 [53.428318]
 [53.58594 ]
 [53.49812 ]], R is [[53.20615768]
 [53.36922073]
 [53.53037262]
 [53.69010925]
 [53.848629  ]].
[2019-03-26 12:49:28,725] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:49:28,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8999
[2019-03-26 12:49:28,741] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 51.5, 1.0, 2.0, 0.5282437871637812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738152.332860504, 738152.332860504, 188140.407343605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4545000.0000, 
sim time next is 4545600.0000, 
raw observation next is [34.0, 52.0, 1.0, 2.0, 0.5316923754061694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 742972.973956, 742972.9739560005, 188711.4153926059], 
processed observation next is [0.0, 0.6086956521739131, 0.8104265402843602, 0.52, 1.0, 1.0, 0.4357739462724933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20638138165444442, 0.20638138165444458, 0.2816588289441879], 
reward next is 0.7183, 
noisyNet noise sample is [array([-0.9891907], dtype=float32), -1.2177498]. 
=============================================
[2019-03-26 12:49:30,777] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4823819e-21 1.0000000e+00 6.1301435e-28 1.0873403e-26 8.7538768e-11], sum to 1.0000
[2019-03-26 12:49:30,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9324
[2019-03-26 12:49:30,794] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2147386.478517374 W.
[2019-03-26 12:49:30,800] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.5119098834902922, 1.0, 1.0, 0.5119098834902922, 1.0, 2.0, 0.8790326980095048, 6.911200000000001, 6.9112, 170.5573041426782, 2147386.478517374, 2147386.478517374, 421619.611554528], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4871400.0000, 
sim time next is 4872000.0000, 
raw observation next is [29.33333333333334, 72.66666666666667, 1.0, 2.0, 0.7632575592029588, 1.0, 2.0, 0.7632575592029588, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2134489.106812867, 2134489.106812867, 402375.7997493582], 
processed observation next is [1.0, 0.391304347826087, 0.5892575039494474, 0.7266666666666667, 1.0, 1.0, 0.7147681436180227, 1.0, 1.0, 0.7147681436180227, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5929136407813519, 0.5929136407813519, 0.6005608951482958], 
reward next is 0.3994, 
noisyNet noise sample is [array([-0.09083481], dtype=float32), -0.21848713]. 
=============================================
[2019-03-26 12:49:30,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[37.05581 ]
 [39.785915]
 [40.951077]
 [41.255135]
 [45.34879 ]], R is [[36.93684769]
 [36.56747818]
 [36.20180511]
 [35.83978653]
 [35.92450714]].
[2019-03-26 12:49:34,432] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:49:34,443] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0488
[2019-03-26 12:49:34,450] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666666, 90.0, 1.0, 2.0, 0.4572043061861639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651777.7078058596, 651777.7078058602, 178824.3913709563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4654200.0000, 
sim time next is 4654800.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.4518132224550012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647699.9483840843, 647699.9483840849, 178493.5296850415], 
processed observation next is [1.0, 0.9130434782608695, 0.3364928909952607, 0.94, 1.0, 1.0, 0.33953400295783276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1799166523289123, 0.17991665232891246, 0.26640825326125595], 
reward next is 0.7336, 
noisyNet noise sample is [array([1.0948968], dtype=float32), -1.3734909]. 
=============================================
[2019-03-26 12:49:38,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:49:38,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1193
[2019-03-26 12:49:38,521] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5079039030453599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709720.4820082138, 709720.4820082145, 184843.2213397491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4501200.0000, 
sim time next is 4501800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5078265170962295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709612.3104869892, 709612.3104869892, 184830.9208811251], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40701990011593914, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19711453069083035, 0.19711453069083035, 0.27586704609123147], 
reward next is 0.7241, 
noisyNet noise sample is [array([0.35986966], dtype=float32), -0.669201]. 
=============================================
[2019-03-26 12:49:40,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6039145e-30], sum to 1.0000
[2019-03-26 12:49:40,823] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8144
[2019-03-26 12:49:40,828] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4905067062660677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685402.6333898423, 685402.6333898423, 182121.8009545039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4753200.0000, 
sim time next is 4753800.0000, 
raw observation next is [27.5, 76.5, 1.0, 2.0, 0.4897629725854023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684363.0528513591, 684363.0528513585, 182007.4195761309], 
processed observation next is [1.0, 0.0, 0.5023696682464456, 0.765, 1.0, 1.0, 0.38525659347638835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1901008480142664, 0.19010084801426627, 0.2716528650390013], 
reward next is 0.7283, 
noisyNet noise sample is [array([1.0309004], dtype=float32), 0.0107542435]. 
=============================================
[2019-03-26 12:49:42,236] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:49:42,243] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8524
[2019-03-26 12:49:42,247] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5224396684751482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730039.0410538552, 730039.0410538552, 187186.3100326881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4566000.0000, 
sim time next is 4566600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5222476749156969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729770.6637668267, 729770.6637668267, 187154.9695321051], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42439478905505645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20271407326856297, 0.20271407326856297, 0.2793357754210524], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.99891007], dtype=float32), -0.009942346]. 
=============================================
[2019-03-26 12:49:43,951] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.1363822e-29 1.0000000e+00 9.6461960e-37 1.7083435e-19 5.2786378e-18], sum to 1.0000
[2019-03-26 12:49:43,959] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2458
[2019-03-26 12:49:43,969] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1861566.917624502 W.
[2019-03-26 12:49:43,975] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.6657498716748744, 1.0, 2.0, 0.6657498716748744, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1861566.917624502, 1861566.917624502, 359806.9013813463], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4713000.0000, 
sim time next is 4713600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5310225667695562, 1.0, 2.0, 0.5310225667695562, 1.0, 1.0, 0.9190195996448822, 6.9112, 6.9112, 170.5573041426782, 2227642.507550884, 2227642.507550884, 436646.2464834482], 
processed observation next is [1.0, 0.5652173913043478, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4349669479151279, 1.0, 1.0, 0.4349669479151279, 1.0, 0.5, 0.9012434142010759, 0.0, 0.0, 0.8375144448122397, 0.6187895854308011, 0.6187895854308011, 0.6517108156469376], 
reward next is 0.3483, 
noisyNet noise sample is [array([-1.0735471], dtype=float32), 0.50300634]. 
=============================================
[2019-03-26 12:49:48,805] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9649435e-22 7.1744359e-01 4.3741917e-27 1.2993627e-19 2.8255641e-01], sum to 1.0000
[2019-03-26 12:49:48,815] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9404
[2019-03-26 12:49:48,818] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5762104964979409, 1.0, 2.0, 0.5762104964979409, 1.0, 1.0, 1.000687076693869, 6.9112, 6.9112, 170.5573041426782, 2417389.45898178, 2417389.45898178, 471846.367793697], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4888800.0000, 
sim time next is 4889400.0000, 
raw observation next is [31.91666666666667, 63.33333333333333, 1.0, 2.0, 0.5262759556305276, 1.0, 2.0, 0.5262759556305276, 1.0, 2.0, 0.91396729281218, 6.911200000000001, 6.9112, 170.5573041426782, 2207712.137179487, 2207712.137179486, 433791.7372349787], 
processed observation next is [1.0, 0.6086956521739131, 0.7116903633491314, 0.6333333333333333, 1.0, 1.0, 0.4292481393138887, 1.0, 1.0, 0.4292481393138887, 1.0, 1.0, 0.8950820644050976, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6132533714387464, 0.6132533714387461, 0.6474503540820578], 
reward next is 0.3525, 
noisyNet noise sample is [array([-0.72090703], dtype=float32), 1.687912]. 
=============================================
[2019-03-26 12:50:04,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4054035e-30 1.0000000e+00 0.0000000e+00 2.5762428e-28 2.5116213e-16], sum to 1.0000
[2019-03-26 12:50:04,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4572
[2019-03-26 12:50:04,514] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1902131.17897495 W.
[2019-03-26 12:50:04,519] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 66.66666666666666, 1.0, 2.0, 0.4534959683016505, 1.0, 2.0, 0.4534959683016505, 1.0, 2.0, 0.775669135558485, 6.9112, 6.9112, 170.5573041426782, 1902131.17897495, 1902131.17897495, 382537.1688220687], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4963800.0000, 
sim time next is 4964400.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.4519618042271488, 1.0, 2.0, 0.4519618042271488, 1.0, 2.0, 0.7717268649795137, 6.911199999999999, 6.9112, 170.5573041426782, 1895690.630127793, 1895690.630127793, 381372.7588842331], 
processed observation next is [1.0, 0.4782608695652174, 0.6208530805687204, 0.66, 1.0, 1.0, 0.3397130171411431, 1.0, 1.0, 0.3397130171411431, 1.0, 1.0, 0.7216181280237972, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5265807305910536, 0.5265807305910536, 0.5692130729615419], 
reward next is 0.4308, 
noisyNet noise sample is [array([-0.17194621], dtype=float32), -1.1791422]. 
=============================================
[2019-03-26 12:50:11,907] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:50:11,914] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6054
[2019-03-26 12:50:11,918] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 83.0, 1.0, 2.0, 0.6339479583494733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 885921.8330604339, 885921.8330604339, 207241.9374351025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5428800.0000, 
sim time next is 5429400.0000, 
raw observation next is [30.73333333333333, 82.83333333333334, 1.0, 2.0, 0.6342889859199617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 886398.606966944, 886398.606966944, 207308.6455057407], 
processed observation next is [1.0, 0.8695652173913043, 0.6556082148499209, 0.8283333333333335, 1.0, 1.0, 0.559384320385496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24622183526859553, 0.24622183526859553, 0.30941588881453835], 
reward next is 0.6906, 
noisyNet noise sample is [array([1.0651104], dtype=float32), 1.2741184]. 
=============================================
[2019-03-26 12:50:15,004] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:50:15,012] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7833
[2019-03-26 12:50:15,019] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5682425839325336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794066.3670615365, 794066.3670615365, 194979.8072214978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5140800.0000, 
sim time next is 5141400.0000, 
raw observation next is [32.0, 66.33333333333334, 1.0, 2.0, 0.5779105018786459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807581.5226610766, 807581.5226610772, 196702.8963192595], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.6633333333333334, 1.0, 1.0, 0.4914584359983685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22432820073918794, 0.2243282007391881, 0.2935864124168052], 
reward next is 0.7064, 
noisyNet noise sample is [array([-0.11774369], dtype=float32), -0.38521245]. 
=============================================
[2019-03-26 12:50:15,684] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 12:50:15,686] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:50:15,687] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:50:15,688] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:50:15,689] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:50:15,690] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:50:15,692] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:50:15,693] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:50:15,694] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:50:15,695] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:50:15,697] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:50:15,719] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run66
[2019-03-26 12:50:15,745] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run66
[2019-03-26 12:50:15,771] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run66
[2019-03-26 12:50:15,771] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run66
[2019-03-26 12:50:15,791] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run66
[2019-03-26 12:50:23,331] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.25544596], dtype=float32), 0.36993298]
[2019-03-26 12:50:23,332] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.44054475333333, 87.53919113666667, 1.0, 2.0, 0.2310500349999906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 383874.094153278, 383874.0941532773, 158808.368478434]
[2019-03-26 12:50:23,333] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:50:23,336] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5995628156305842
[2019-03-26 12:50:34,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25544596], dtype=float32), 0.36993298]
[2019-03-26 12:50:34,952] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.32925133, 77.58645411, 1.0, 2.0, 0.8395254671283541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1173368.490495221, 1173368.490495221, 253845.7080771655]
[2019-03-26 12:50:34,954] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:50:34,958] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6438976029023753
[2019-03-26 12:51:22,425] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.25544596], dtype=float32), 0.36993298]
[2019-03-26 12:51:22,427] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.0, 46.0, 1.0, 2.0, 0.5892289486890585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 823404.2347748608, 823404.2347748614, 198756.2826932218]
[2019-03-26 12:51:22,429] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:51:22,432] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.42819195330825777
[2019-03-26 12:51:24,804] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.25544596], dtype=float32), 0.36993298]
[2019-03-26 12:51:24,805] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.05642053333334, 97.53353580000001, 1.0, 2.0, 0.5189625977316419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725178.6447020551, 725178.6447020546, 186619.6825819526]
[2019-03-26 12:51:24,806] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:51:24,808] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4503662138530812
[2019-03-26 12:51:33,917] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.25544596], dtype=float32), 0.36993298]
[2019-03-26 12:51:33,918] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.559649215, 88.863614125, 1.0, 2.0, 0.8324329244542575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1163450.11891176, 1163450.118911761, 252031.8162361683]
[2019-03-26 12:51:33,919] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:51:33,920] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18836184929730082
[2019-03-26 12:51:42,656] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.25544596], dtype=float32), 0.36993298]
[2019-03-26 12:51:42,659] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.66666666666667, 79.33333333333334, 1.0, 2.0, 0.7707692077404498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1077222.091188974, 1077222.091188975, 236865.5009105385]
[2019-03-26 12:51:42,662] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:51:42,663] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18467333950025122
[2019-03-26 12:51:58,237] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25544596], dtype=float32), 0.36993298]
[2019-03-26 12:51:58,238] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.98409089333333, 60.241650685, 1.0, 2.0, 0.562476309606542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786005.5515205364, 786005.5515205364, 193964.1244606692]
[2019-03-26 12:51:58,239] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:51:58,241] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4970150175692478
[2019-03-26 12:52:11,346] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.0962 3164352807.5508 1832.0000
[2019-03-26 12:52:11,693] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8247.9695 2927873714.7879 1356.0000
[2019-03-26 12:52:11,699] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.1609 2779752782.7483 933.0000
[2019-03-26 12:52:11,706] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.25544596], dtype=float32), 0.36993298]
[2019-03-26 12:52:11,706] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.54474338, 67.91784472500001, 1.0, 2.0, 0.9229256730305496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1290004.169573407, 1290004.169573406, 276319.7769274162]
[2019-03-26 12:52:11,707] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:52:11,708] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5006582e-36 5.5697477e-38], sampled 0.1360849692896604
[2019-03-26 12:52:11,898] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.4854 2842860136.4015 1143.0000
[2019-03-26 12:52:12,030] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.3365 3007916534.6840 1765.0000
[2019-03-26 12:52:13,049] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1625000, evaluation results [1625000.0, 7875.096192262437, 3164352807.550764, 1832.0, 8247.969517937, 2927873714.7879477, 1356.0, 8660.160903757378, 2779752782.7482843, 933.0, 7996.33650718147, 3007916534.684046, 1765.0, 8493.48536921701, 2842860136.401477, 1143.0]
[2019-03-26 12:52:13,264] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:52:13,274] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1657
[2019-03-26 12:52:13,283] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 83.0, 1.0, 2.0, 0.5541845953824116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 774414.4691675132, 774414.4691675126, 192521.3302356667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5266800.0000, 
sim time next is 5267400.0000, 
raw observation next is [28.5, 83.16666666666667, 1.0, 2.0, 0.555245578425055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775897.6224667907, 775897.6224667907, 192704.7612388712], 
processed observation next is [1.0, 1.0, 0.5497630331753555, 0.8316666666666667, 1.0, 1.0, 0.4641512993072951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21552711735188632, 0.21552711735188632, 0.2876190466251809], 
reward next is 0.7124, 
noisyNet noise sample is [array([0.76675075], dtype=float32), 1.2416967]. 
=============================================
[2019-03-26 12:52:13,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:52:13,568] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3151
[2019-03-26 12:52:13,575] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5409040698056533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755849.733772564, 755849.7337725633, 190253.2434629186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5162400.0000, 
sim time next is 5163000.0000, 
raw observation next is [30.66666666666667, 67.33333333333333, 1.0, 2.0, 0.5427599173223027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 758443.9879589273, 758443.9879589267, 190566.7009740643], 
processed observation next is [0.0, 0.782608695652174, 0.6524486571879939, 0.6733333333333333, 1.0, 1.0, 0.4491083341232562, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21067888554414646, 0.21067888554414632, 0.2844279119015885], 
reward next is 0.7156, 
noisyNet noise sample is [array([-0.5488196], dtype=float32), 1.3974696]. 
=============================================
[2019-03-26 12:52:13,588] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.28988 ]
 [73.173454]
 [73.17799 ]
 [73.169174]
 [73.1624  ]], R is [[73.30233002]
 [73.28534698]
 [73.26859283]
 [73.25199127]
 [73.23530579]].
[2019-03-26 12:52:23,983] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:52:23,992] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9565
[2019-03-26 12:52:23,997] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 95.0, 1.0, 2.0, 0.8628775180420613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1206025.179706315, 1206025.179706315, 259929.5579927798], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5542800.0000, 
sim time next is 5543400.0000, 
raw observation next is [25.85, 95.0, 1.0, 2.0, 0.8429484127294752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1178155.247943872, 1178155.247943872, 254725.3105663987], 
processed observation next is [1.0, 0.13043478260869565, 0.4241706161137442, 0.95, 1.0, 1.0, 0.8107812201559942, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32726534665107554, 0.32726534665107554, 0.3801870306961175], 
reward next is 0.6198, 
noisyNet noise sample is [array([-1.6487178], dtype=float32), -1.2452809]. 
=============================================
[2019-03-26 12:52:24,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:52:24,356] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3463
[2019-03-26 12:52:24,365] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 87.33333333333333, 1.0, 2.0, 0.8827643314145461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1233836.697391026, 1233836.697391026, 265246.1361687498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5377800.0000, 
sim time next is 5378400.0000, 
raw observation next is [29.5, 86.0, 1.0, 2.0, 0.8926081449586144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1247603.445734231, 1247603.445734231, 267918.023827204], 
processed observation next is [1.0, 0.2608695652173913, 0.5971563981042655, 0.86, 1.0, 1.0, 0.8706122228417041, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34655651270395305, 0.34655651270395305, 0.3998776475032896], 
reward next is 0.6001, 
noisyNet noise sample is [array([1.497133], dtype=float32), -0.030604662]. 
=============================================
[2019-03-26 12:52:24,495] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9330681e-20 1.0000000e+00 9.8863396e-28 1.1114182e-22 1.0018995e-19], sum to 1.0000
[2019-03-26 12:52:24,503] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9958
[2019-03-26 12:52:24,512] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2484231.840323061 W.
[2019-03-26 12:52:24,516] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.65, 71.5, 1.0, 2.0, 0.8881909010337881, 1.0, 1.0, 0.8881909010337881, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2484231.840323061, 2484231.840323062, 465089.9313565078], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5563800.0000, 
sim time next is 5564400.0000, 
raw observation next is [30.86666666666667, 70.33333333333333, 1.0, 2.0, 0.8767946243991694, 1.0, 2.0, 0.8767946243991694, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2452325.665196851, 2452325.665196851, 458989.8840092348], 
processed observation next is [1.0, 0.391304347826087, 0.6619273301737759, 0.7033333333333333, 1.0, 1.0, 0.8515597884327343, 1.0, 1.0, 0.8515597884327343, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6812015736657919, 0.6812015736657919, 0.6850595283719922], 
reward next is 0.3149, 
noisyNet noise sample is [array([0.49120185], dtype=float32), -0.7928098]. 
=============================================
[2019-03-26 12:52:30,671] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2582226e-32 1.0000000e+00 0.0000000e+00 3.1113367e-36 1.5336799e-29], sum to 1.0000
[2019-03-26 12:52:30,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4781
[2019-03-26 12:52:30,685] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 92.0, 1.0, 2.0, 0.9810407587863431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129362690295, 1371286.035191058, 1371286.035191059, 293205.9122839144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5455200.0000, 
sim time next is 5455800.0000, 
raw observation next is [27.8, 92.0, 1.0, 2.0, 0.9599228893481812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565053767, 1341749.097575785, 1341749.097575786, 286956.0388693541], 
processed observation next is [1.0, 0.13043478260869565, 0.5165876777251186, 0.92, 1.0, 1.0, 0.9517143245158809, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451274838, 0.37270808265994027, 0.3727080826599406, 0.4282925953273942], 
reward next is 0.5717, 
noisyNet noise sample is [array([-1.0134385], dtype=float32), 1.7732594]. 
=============================================
[2019-03-26 12:52:35,562] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0119182e-33 1.0000000e+00 4.6456102e-38 6.3497817e-35 1.0038066e-30], sum to 1.0000
[2019-03-26 12:52:35,572] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9760
[2019-03-26 12:52:35,577] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 95.0, 1.0, 2.0, 0.9105049435848319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1272632.894257895, 1272632.894257895, 272843.5281923113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5540400.0000, 
sim time next is 5541000.0000, 
raw observation next is [26.05, 95.0, 1.0, 2.0, 0.9738793817596663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1361269.537394623, 1361269.537394624, 291067.9874402713], 
processed observation next is [1.0, 0.13043478260869565, 0.43364928909952616, 0.95, 1.0, 1.0, 0.9685293756140557, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37813042705406197, 0.3781304270540622, 0.4344298320004049], 
reward next is 0.5656, 
noisyNet noise sample is [array([-0.25758988], dtype=float32), 1.5730071]. 
=============================================
[2019-03-26 12:52:35,590] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[48.751484]
 [48.00829 ]
 [47.42127 ]
 [47.3062  ]
 [45.874176]], R is [[48.58993149]
 [48.69680405]
 [48.79941177]
 [48.88291931]
 [48.39408875]].
[2019-03-26 12:52:40,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:52:40,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0289
[2019-03-26 12:52:40,462] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 92.0, 1.0, 2.0, 0.508023626854121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709887.8341999968, 709887.8341999968, 184862.5178650199], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5626800.0000, 
sim time next is 5627400.0000, 
raw observation next is [25.68333333333333, 91.83333333333334, 1.0, 2.0, 0.506999960384305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708456.9348052735, 708456.9348052735, 184699.8230652409], 
processed observation next is [0.0, 0.13043478260869565, 0.41627172195892564, 0.9183333333333334, 1.0, 1.0, 0.40602404865578906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19679359300146487, 0.19679359300146487, 0.27567137770931477], 
reward next is 0.7243, 
noisyNet noise sample is [array([0.65208447], dtype=float32), -1.2082472]. 
=============================================
[2019-03-26 12:52:41,291] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:52:41,300] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0019
[2019-03-26 12:52:41,305] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.43333333333333, 72.83333333333334, 1.0, 2.0, 0.5322740261263339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743786.0419720585, 743786.0419720579, 188807.3144585471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5683800.0000, 
sim time next is 5684400.0000, 
raw observation next is [29.2, 74.0, 1.0, 2.0, 0.5315173219168408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742728.2732834636, 742728.2732834629, 188681.5681455161], 
processed observation next is [0.0, 0.8260869565217391, 0.5829383886255924, 0.74, 1.0, 1.0, 0.435563038454025, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20631340924540653, 0.20631340924540634, 0.28161428081420314], 
reward next is 0.7184, 
noisyNet noise sample is [array([0.917636], dtype=float32), 0.087248094]. 
=============================================
[2019-03-26 12:52:45,321] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5009575e-26 1.0000000e+00 7.0373283e-31 5.6856591e-27 8.0276860e-13], sum to 1.0000
[2019-03-26 12:52:45,330] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0382
[2019-03-26 12:52:45,335] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 93.0, 1.0, 2.0, 1.030843541314846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 169.2856565914384, 1440944.811024248, 1440944.811024248, 308531.7359730095], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6055800.0000, 
sim time next is 6056400.0000, 
raw observation next is [26.2, 93.0, 1.0, 2.0, 0.9961360363984695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1392399.855754101, 1392399.8557541, 297750.2141445098], 
processed observation next is [1.0, 0.08695652173913043, 0.44075829383886256, 0.93, 1.0, 1.0, 0.9953446221668307, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3867777377094725, 0.3867777377094722, 0.4444033046932982], 
reward next is 0.5556, 
noisyNet noise sample is [array([0.89160544], dtype=float32), -1.1139051]. 
=============================================
[2019-03-26 12:52:51,015] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5452218e-26 1.0000000e+00 8.9531225e-28 1.8174747e-27 1.5523499e-11], sum to 1.0000
[2019-03-26 12:52:51,023] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2500
[2019-03-26 12:52:51,026] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 92.0, 1.0, 2.0, 0.7604910440428951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1062850.206865031, 1062850.206865031, 234443.2284224976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5808000.0000, 
sim time next is 5808600.0000, 
raw observation next is [26.45, 91.0, 1.0, 2.0, 0.7641237128576236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1067929.720005651, 1067929.720005651, 235294.4849185148], 
processed observation next is [1.0, 0.21739130434782608, 0.45260663507109006, 0.91, 1.0, 1.0, 0.7158117022381008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29664714444601414, 0.29664714444601414, 0.35118579838584296], 
reward next is 0.6488, 
noisyNet noise sample is [array([-1.6616195], dtype=float32), 1.2027194]. 
=============================================
[2019-03-26 12:52:51,375] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.8962900e-26 1.0000000e+00 2.9579472e-30 2.2236535e-24 7.1563458e-13], sum to 1.0000
[2019-03-26 12:52:51,382] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4917
[2019-03-26 12:52:51,393] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.95, 93.83333333333334, 1.0, 2.0, 0.8472554202854963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1184178.336783012, 1184178.336783011, 255839.8913648883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5806200.0000, 
sim time next is 5806800.0000, 
raw observation next is [25.9, 94.0, 1.0, 2.0, 0.8235487305750202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1151026.392597042, 1151026.392597041, 249772.1937324391], 
processed observation next is [1.0, 0.21739130434782608, 0.42654028436018954, 0.94, 1.0, 1.0, 0.7874081091265304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31972955349917836, 0.3197295534991781, 0.37279431900364046], 
reward next is 0.6272, 
noisyNet noise sample is [array([1.9380891], dtype=float32), -0.06907376]. 
=============================================
[2019-03-26 12:52:58,078] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6714276e-27 8.6694102e-24], sum to 1.0000
[2019-03-26 12:52:58,082] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4428
[2019-03-26 12:52:58,087] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 81.33333333333334, 1.0, 2.0, 0.5666369186222883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791821.7622108033, 791821.7622108033, 194695.9060357983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5942400.0000, 
sim time next is 5943000.0000, 
raw observation next is [29.21666666666667, 81.66666666666667, 1.0, 2.0, 0.566209917345607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 791224.845874145, 791224.8458741456, 194620.4623591429], 
processed observation next is [1.0, 0.782608695652174, 0.5837282780410744, 0.8166666666666668, 1.0, 1.0, 0.47736134619952647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21978467940948473, 0.2197846794094849, 0.2904783020285715], 
reward next is 0.7095, 
noisyNet noise sample is [array([-0.6940859], dtype=float32), -0.7984284]. 
=============================================
[2019-03-26 12:52:58,126] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.46302]
 [65.1381 ]
 [64.85167]
 [64.67285]
 [64.33087]], R is [[65.52315521]
 [65.57733917]
 [65.63040161]
 [65.68244171]
 [65.7338562 ]].
[2019-03-26 12:53:04,066] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.5405012e-35 2.0851640e-16 3.0750988e-37 2.9391193e-37 1.0000000e+00], sum to 1.0000
[2019-03-26 12:53:04,077] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0217
[2019-03-26 12:53:04,083] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.86666666666667, 90.33333333333333, 1.0, 2.0, 0.1788782495616153, 1.0, 2.0, 0.1788782495616153, 1.0, 2.0, 0.3068209559685017, 6.9112, 6.9112, 170.5573041426782, 749879.2025848228, 749879.2025848228, 264147.2306675614], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6138600.0000, 
sim time next is 6139200.0000, 
raw observation next is [26.83333333333333, 90.66666666666667, 1.0, 2.0, 0.1790665083898507, 1.0, 2.0, 0.1790665083898507, 1.0, 2.0, 0.3072052088404165, 6.9112, 6.9112, 170.5573041426782, 750668.6824796748, 750668.6824796748, 264195.977760028], 
processed observation next is [1.0, 0.043478260869565216, 0.470774091627172, 0.9066666666666667, 1.0, 1.0, 0.010923504084157447, 1.0, 1.0, 0.010923504084157447, 1.0, 1.0, 0.1551283034639226, 0.0, 0.0, 0.8375144448122397, 0.20851907846657633, 0.20851907846657633, 0.3943223548657134], 
reward next is 0.6057, 
noisyNet noise sample is [array([0.43939614], dtype=float32), 0.7027502]. 
=============================================
[2019-03-26 12:53:07,478] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 12:53:07,479] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:53:07,481] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:53:07,481] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:53:07,482] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:53:07,482] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:53:07,483] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:53:07,484] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:53:07,486] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:53:07,483] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:53:07,487] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:53:07,513] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run67
[2019-03-26 12:53:07,539] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run67
[2019-03-26 12:53:07,564] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run67
[2019-03-26 12:53:07,587] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run67
[2019-03-26 12:53:07,588] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run67
[2019-03-26 12:53:18,655] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.22671378], dtype=float32), 0.35723242]
[2019-03-26 12:53:18,657] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.6, 74.0, 1.0, 2.0, 0.3092405043549293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 489911.4746846778, 489911.4746846778, 166295.9588699967]
[2019-03-26 12:53:18,658] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:53:18,662] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.528364937192777
[2019-03-26 12:53:30,889] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.22671378], dtype=float32), 0.35723242]
[2019-03-26 12:53:30,890] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.2, 80.33333333333334, 1.0, 2.0, 0.3343083576537585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 523337.0035129956, 523337.0035129949, 168705.0295428389]
[2019-03-26 12:53:30,891] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:53:30,894] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 2.870098e-26], sampled 0.21876704603282582
[2019-03-26 12:53:35,366] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.22671378], dtype=float32), 0.35723242]
[2019-03-26 12:53:35,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.9, 90.5, 1.0, 2.0, 0.645481610843356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 980669.2851604467, 980669.285160446, 219343.8134598392]
[2019-03-26 12:53:35,368] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 12:53:35,371] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.7207993e-33 1.0000000e+00 5.5406844e-34 3.3390570e-30 4.3772665e-09], sampled 0.5099709077604231
[2019-03-26 12:53:44,939] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.22671378], dtype=float32), 0.35723242]
[2019-03-26 12:53:44,940] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.1, 86.5, 1.0, 2.0, 0.4665415168050364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667546.6466097475, 667546.6466097475, 180523.2380605807]
[2019-03-26 12:53:44,941] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:53:44,945] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8579898e-38 1.0000000e+00 0.0000000e+00 2.7872733e-35 5.6240155e-17], sampled 0.0949850160209822
[2019-03-26 12:54:02,879] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.22671378], dtype=float32), 0.35723242]
[2019-03-26 12:54:02,881] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.41573502666667, 77.93851754166667, 1.0, 2.0, 0.5279516286394501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737743.9373597209, 737743.9373597209, 188091.3589740665]
[2019-03-26 12:54:02,882] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:54:02,884] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.2702453e-31 9.9110258e-01 3.7168403e-31 1.0166957e-27 8.8973707e-03], sampled 0.2953112498956886
[2019-03-26 12:55:02,653] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8039.9364 3189724421.4911 1250.0000
[2019-03-26 12:55:02,751] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8730.7183 2817462546.1486 525.0000
[2019-03-26 12:55:03,383] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8236.0155 3033873778.1613 1034.0000
[2019-03-26 12:55:03,407] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8623.8559 2872402004.4347 615.0000
[2019-03-26 12:55:03,472] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8373.0360 2969253804.9335 939.0000
[2019-03-26 12:55:04,492] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1650000, evaluation results [1650000.0, 8039.936420063796, 3189724421.491128, 1250.0, 8373.03597949226, 2969253804.933463, 939.0, 8730.71832109149, 2817462546.148627, 525.0, 8236.015547798888, 3033873778.1613426, 1034.0, 8623.855936335398, 2872402004.434704, 615.0]
[2019-03-26 12:55:13,665] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5624196e-30], sum to 1.0000
[2019-03-26 12:55:13,678] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1482
[2019-03-26 12:55:13,682] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666667, 90.33333333333334, 1.0, 2.0, 0.5244757669673655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732885.1958526161, 732885.1958526167, 187520.0216786226], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6237600.0000, 
sim time next is 6238200.0000, 
raw observation next is [26.58333333333334, 90.16666666666666, 1.0, 2.0, 0.5240187084028317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732246.2971042078, 732246.2971042078, 187445.1373803102], 
processed observation next is [0.0, 0.17391304347826086, 0.45892575039494504, 0.9016666666666666, 1.0, 1.0, 0.4265285643407611, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20340174919561327, 0.20340174919561327, 0.279768861761657], 
reward next is 0.7202, 
noisyNet noise sample is [array([-1.227475], dtype=float32), -0.0972952]. 
=============================================
[2019-03-26 12:55:15,875] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3516078e-26 4.3433974e-04 4.0201571e-35 5.3004040e-36 9.9956566e-01], sum to 1.0000
[2019-03-26 12:55:15,881] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9518
[2019-03-26 12:55:15,886] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.31666666666667, 91.0, 1.0, 2.0, 0.2331429341028548, 1.0, 2.0, 0.2331429341028548, 1.0, 2.0, 0.3977814335445417, 6.9112, 6.9112, 170.5573041426782, 977466.9750596184, 977466.9750596184, 279284.6366966511], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6490200.0000, 
sim time next is 6490800.0000, 
raw observation next is [26.3, 91.0, 1.0, 2.0, 0.2278126677665315, 1.0, 2.0, 0.2278126677665315, 1.0, 2.0, 0.3885169539206865, 6.9112, 6.9112, 170.5573041426782, 955109.5332588971, 955109.5332588971, 277601.1149986899], 
processed observation next is [1.0, 0.13043478260869565, 0.4454976303317536, 0.91, 1.0, 1.0, 0.06965381658618253, 1.0, 1.0, 0.06965381658618253, 1.0, 1.0, 0.25428896819595914, 0.0, 0.0, 0.8375144448122397, 0.265308203683027, 0.265308203683027, 0.41433002238610434], 
reward next is 0.5857, 
noisyNet noise sample is [array([1.6233774], dtype=float32), 0.94798094]. 
=============================================
[2019-03-26 12:55:17,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.1381190e-24 1.7588539e-04 1.0237084e-33 2.2925044e-26 9.9982411e-01], sum to 1.0000
[2019-03-26 12:55:17,846] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1386
[2019-03-26 12:55:17,850] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.53333333333333, 56.0, 1.0, 2.0, 0.42865164713977, 1.0, 2.0, 0.42865164713977, 1.0, 2.0, 0.7273111241368887, 6.9112, 6.9112, 170.5573041426782, 1797837.407503629, 1797837.407503629, 366679.7356503113], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6524400.0000, 
sim time next is 6525000.0000, 
raw observation next is [31.6, 56.0, 1.0, 2.0, 0.3926024453772735, 1.0, 2.0, 0.3926024453772735, 1.0, 2.0, 0.6657894409629979, 6.9112, 6.9112, 170.5573041426782, 1646524.808320418, 1646524.808320418, 346601.5055333335], 
processed observation next is [1.0, 0.5217391304347826, 0.6966824644549764, 0.56, 1.0, 1.0, 0.2681957173220163, 1.0, 1.0, 0.2681957173220163, 1.0, 1.0, 0.592426147515851, 0.0, 0.0, 0.8375144448122397, 0.4573680023112272, 0.4573680023112272, 0.5173156799004978], 
reward next is 0.4827, 
noisyNet noise sample is [array([-0.16814502], dtype=float32), -1.7749338]. 
=============================================
[2019-03-26 12:55:17,867] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[69.48635 ]
 [68.10677 ]
 [67.929375]
 [67.08896 ]
 [66.16138 ]], R is [[70.39645386]
 [70.14520264]
 [69.7949295 ]
 [69.49748993]
 [69.22216034]].
[2019-03-26 12:55:19,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7006905e-34 5.0953049e-21], sum to 1.0000
[2019-03-26 12:55:19,309] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0674
[2019-03-26 12:55:19,319] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 70.0, 1.0, 2.0, 0.486308992516264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679535.1411445088, 679535.1411445083, 181478.709931066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6548400.0000, 
sim time next is 6549000.0000, 
raw observation next is [28.5, 70.5, 1.0, 2.0, 0.4884577931310006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 682538.6937208483, 682538.6937208476, 181807.1998407198], 
processed observation next is [1.0, 0.8260869565217391, 0.5497630331753555, 0.705, 1.0, 1.0, 0.3836840881096392, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18959408158912452, 0.18959408158912433, 0.27135402961301464], 
reward next is 0.7286, 
noisyNet noise sample is [array([-0.7235643], dtype=float32), -0.042591464]. 
=============================================
[2019-03-26 12:55:19,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.9404  ]
 [66.982124]
 [67.16702 ]
 [67.16435 ]
 [67.647705]], R is [[66.93222809]
 [66.99204254]
 [67.05185699]
 [67.11130524]
 [67.16982269]].
[2019-03-26 12:55:19,547] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.212364e-26], sum to 1.0000
[2019-03-26 12:55:19,556] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5776
[2019-03-26 12:55:19,560] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 63.00000000000001, 1.0, 2.0, 0.5446081712998979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761027.6335776714, 761027.633577672, 190880.4809788294], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6351600.0000, 
sim time next is 6352200.0000, 
raw observation next is [31.5, 63.0, 1.0, 2.0, 0.5382711482541161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752169.2333426882, 752169.2333426889, 189809.7982757424], 
processed observation next is [0.0, 0.5217391304347826, 0.6919431279620853, 0.63, 1.0, 1.0, 0.443700178619417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20893589815074673, 0.20893589815074692, 0.28329820638170505], 
reward next is 0.7167, 
noisyNet noise sample is [array([-0.84147066], dtype=float32), 0.097138844]. 
=============================================
[2019-03-26 12:55:37,236] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:55:37,244] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4687
[2019-03-26 12:55:37,247] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.76666666666667, 80.33333333333334, 1.0, 2.0, 0.3590989701861249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 567779.8603219942, 567779.8603219942, 172412.9630785576], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6763200.0000, 
sim time next is 6763800.0000, 
raw observation next is [22.88333333333333, 79.66666666666667, 1.0, 2.0, 0.3714902160695102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587046.9243801547, 587046.924380154, 174058.8901870213], 
processed observation next is [1.0, 0.2608695652173913, 0.28357030015797774, 0.7966666666666667, 1.0, 1.0, 0.24275929646928943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16306859010559852, 0.16306859010559832, 0.25978938833883775], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.5557362], dtype=float32), 1.8593187]. 
=============================================
[2019-03-26 12:55:38,570] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:55:38,577] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5295
[2019-03-26 12:55:38,583] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 45.0, 1.0, 2.0, 0.31181910228717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491540.4331209147, 491540.4331209154, 166362.4966810736], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6884400.0000, 
sim time next is 6885000.0000, 
raw observation next is [29.3, 46.0, 1.0, 2.0, 0.3207212474971916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503496.4204848484, 503496.4204848491, 167206.9356454084], 
processed observation next is [0.0, 0.6956521739130435, 0.5876777251184835, 0.46, 1.0, 1.0, 0.18159186445444772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13986011680134677, 0.13986011680134697, 0.24956259051553492], 
reward next is 0.7504, 
noisyNet noise sample is [array([-1.1803817], dtype=float32), 2.422923]. 
=============================================
[2019-03-26 12:55:38,598] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.86698 ]
 [77.88561 ]
 [77.881805]
 [77.7658  ]
 [77.75375 ]], R is [[77.79842377]
 [77.7721405 ]
 [77.74687958]
 [77.72199249]
 [77.69805908]].
[2019-03-26 12:55:39,301] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4712952e-29 4.7036545e-05 6.6977128e-38 3.1841983e-25 9.9995291e-01], sum to 1.0000
[2019-03-26 12:55:39,310] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8062
[2019-03-26 12:55:39,314] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.85, 52.5, 1.0, 2.0, 0.6732870574670798, 1.0, 2.0, 0.6732870574670798, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1889200.091214583, 1889200.091214583, 363701.4238640756], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7039800.0000, 
sim time next is 7040400.0000, 
raw observation next is [30.93333333333334, 52.0, 1.0, 2.0, 0.4628204211399169, 1.0, 2.0, 0.4628204211399169, 1.0, 1.0, 0.7719258812650733, 6.911199999999999, 6.9112, 170.5573041426782, 1941276.843114582, 1941276.843114583, 384933.3952047706], 
processed observation next is [1.0, 0.4782608695652174, 0.6650868878357034, 0.52, 1.0, 1.0, 0.3527956881203818, 1.0, 1.0, 0.3527956881203818, 1.0, 0.5, 0.7218608308110651, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5392435675318283, 0.5392435675318287, 0.5745274555295083], 
reward next is 0.4255, 
noisyNet noise sample is [array([-0.0090799], dtype=float32), 0.75198376]. 
=============================================
[2019-03-26 12:55:40,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.9152053e-32 2.6664854e-10 3.3328911e-34 5.0327027e-34 1.0000000e+00], sum to 1.0000
[2019-03-26 12:55:40,960] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6238
[2019-03-26 12:55:40,965] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.56666666666667, 78.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2015585097993847, 6.911200000000001, 6.9112, 170.5573041426782, 524926.9350634364, 524926.9350634357, 233890.6771058062], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6741600.0000, 
sim time next is 6742200.0000, 
raw observation next is [23.43333333333333, 78.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2002653193293963, 6.9112, 6.9112, 170.5573041426782, 521882.6330472665, 521882.6330472665, 233437.3299841561], 
processed observation next is [1.0, 0.0, 0.30963665086887826, 0.785, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.024713804060239413, 0.0, 0.0, 0.8375144448122397, 0.14496739806868514, 0.14496739806868514, 0.34841392534948673], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46028447], dtype=float32), 1.101659]. 
=============================================
[2019-03-26 12:55:49,142] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:55:49,151] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5842
[2019-03-26 12:55:49,164] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 39.33333333333333, 1.0, 2.0, 0.2861820342438035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461700.470572328, 461700.4705723287, 164388.0575221332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6871200.0000, 
sim time next is 6871800.0000, 
raw observation next is [29.31666666666667, 38.16666666666667, 1.0, 2.0, 0.2806612041765062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 454205.2490424214, 454205.2490424214, 163873.6512266393], 
processed observation next is [0.0, 0.5217391304347826, 0.5884676145339655, 0.3816666666666667, 1.0, 1.0, 0.13332675201988697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12616812473400593, 0.12616812473400593, 0.24458753914423778], 
reward next is 0.7554, 
noisyNet noise sample is [array([2.0910714], dtype=float32), -0.39744857]. 
=============================================
[2019-03-26 12:55:49,801] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:55:49,810] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0123
[2019-03-26 12:55:49,813] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.31666666666667, 69.83333333333333, 1.0, 2.0, 0.3881709464628906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 582580.531923671, 582580.5319236703, 173045.1849274878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6901800.0000, 
sim time next is 6902400.0000, 
raw observation next is [26.23333333333333, 70.66666666666667, 1.0, 2.0, 0.3902538720148053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584690.2530060142, 584690.2530060142, 173205.4094153247], 
processed observation next is [0.0, 0.9130434782608695, 0.44233807266982617, 0.7066666666666667, 1.0, 1.0, 0.26536611086121126, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16241395916833726, 0.16241395916833726, 0.2585155364407832], 
reward next is 0.7415, 
noisyNet noise sample is [array([-1.5675844], dtype=float32), -2.2627313]. 
=============================================
[2019-03-26 12:55:54,721] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:55:54,727] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2305
[2019-03-26 12:55:54,733] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 87.0, 1.0, 2.0, 0.3709798392562047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561317.5032886798, 561317.5032886798, 171309.6733639968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7470000.0000, 
sim time next is 7470600.0000, 
raw observation next is [23.6, 86.66666666666667, 1.0, 2.0, 0.3723569981784813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562537.1515428284, 562537.1515428284, 171387.7869494212], 
processed observation next is [0.0, 0.4782608695652174, 0.3175355450236968, 0.8666666666666667, 1.0, 1.0, 0.24380361226323047, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1562603198730079, 0.1562603198730079, 0.2558026670886884], 
reward next is 0.7442, 
noisyNet noise sample is [array([3.1347818], dtype=float32), 0.29883623]. 
=============================================
[2019-03-26 12:55:57,936] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:55:57,945] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0904
[2019-03-26 12:55:57,950] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.68333333333333, 88.0, 1.0, 2.0, 0.4133061746971389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619006.4355631499, 619006.4355631499, 176363.4572957827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7361400.0000, 
sim time next is 7362000.0000, 
raw observation next is [23.6, 89.0, 1.0, 2.0, 0.417661102547572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 624482.1407145852, 624482.1407145846, 176857.0954445016], 
processed observation next is [1.0, 0.21739130434782608, 0.3175355450236968, 0.89, 1.0, 1.0, 0.29838687053924334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.173467261309607, 0.17346726130960685, 0.26396581409627107], 
reward next is 0.7360, 
noisyNet noise sample is [array([-0.36493847], dtype=float32), 0.60054564]. 
=============================================
[2019-03-26 12:55:57,963] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.11169 ]
 [71.1161  ]
 [71.16913 ]
 [71.26721 ]
 [71.430954]], R is [[71.08825684]
 [71.11414337]
 [71.13842773]
 [71.16085052]
 [71.18340302]].
[2019-03-26 12:55:58,660] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 12:55:58,662] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:55:58,663] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:55:58,664] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:55:58,665] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:55:58,665] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:55:58,666] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:55:58,666] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:55:58,667] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:55:58,667] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:55:58,668] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:55:58,701] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run68
[2019-03-26 12:55:58,729] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run68
[2019-03-26 12:55:58,729] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run68
[2019-03-26 12:55:58,730] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run68
[2019-03-26 12:55:58,804] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run68
[2019-03-26 12:56:30,189] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.26219976], dtype=float32), 0.34110937]
[2019-03-26 12:56:30,190] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.988994805, 89.3432206, 1.0, 2.0, 0.4883998830662613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 682457.7480238753, 682457.7480238753, 181799.2470532477]
[2019-03-26 12:56:30,192] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:56:30,194] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08472330046841414
[2019-03-26 12:56:41,228] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.26219976], dtype=float32), 0.34110937]
[2019-03-26 12:56:41,230] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.9, 77.66666666666667, 1.0, 2.0, 0.521244803528913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728368.8058545503, 728368.8058545503, 186990.7309233405]
[2019-03-26 12:56:41,231] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:56:41,232] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37272539952989947
[2019-03-26 12:57:15,853] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.26219976], dtype=float32), 0.34110937]
[2019-03-26 12:57:15,854] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.56666666666667, 58.66666666666667, 1.0, 2.0, 0.6140800605649076, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.952564614721437, 6.9112, 168.9123080438136, 1716985.657035403, 1687640.288350851, 367208.6802771215]
[2019-03-26 12:57:15,855] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:57:15,859] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0342812e-30 1.0000000e+00 2.8075636e-32 8.1729591e-28 3.2442595e-12], sampled 0.1675272412030978
[2019-03-26 12:57:15,859] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1716985.657035403 W.
[2019-03-26 12:57:47,599] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.26219976], dtype=float32), 0.34110937]
[2019-03-26 12:57:47,600] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.117443205, 92.77923079499999, 1.0, 2.0, 0.2695733428049807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 441343.3794271227, 441343.3794271227, 162894.1422052704]
[2019-03-26 12:57:47,602] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:57:47,604] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6064929791398134
[2019-03-26 12:57:54,536] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.5441 2779625947.8173 933.0000
[2019-03-26 12:57:54,712] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6126 3007879675.3696 1763.0000
[2019-03-26 12:57:54,800] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.9193 2927581036.3871 1348.0000
[2019-03-26 12:57:54,805] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.6453 3164055250.1131 1821.0000
[2019-03-26 12:57:54,991] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.7116 2842649778.0666 1133.0000
[2019-03-26 12:57:56,008] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1675000, evaluation results [1675000.0, 7881.645330471894, 3164055250.1130934, 1821.0, 8251.919290686094, 2927581036.3870964, 1348.0, 8659.544118220558, 2779625947.8172565, 933.0, 7997.612647550441, 3007879675.369553, 1763.0, 8495.711639958407, 2842649778.0666394, 1133.0]
[2019-03-26 12:57:57,385] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.93757365e-28 1.00000000e+00 7.96803909e-32 1.26167084e-23
 4.53272277e-21], sum to 1.0000
[2019-03-26 12:57:57,395] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1308
[2019-03-26 12:57:57,400] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1890482.953271299 W.
[2019-03-26 12:57:57,405] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.95, 45.5, 1.0, 2.0, 0.6585357646866491, 1.0, 1.0, 0.6585357646866491, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1890482.953271299, 1890482.953271299, 362890.6023720079], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7047000.0000, 
sim time next is 7047600.0000, 
raw observation next is [32.03333333333334, 45.0, 1.0, 2.0, 0.6364551996041917, 1.0, 2.0, 0.6364551996041917, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1823962.713714327, 1823962.713714326, 353503.8215135405], 
processed observation next is [1.0, 0.5652173913043478, 0.7172195892575042, 0.45, 1.0, 1.0, 0.5619942163905924, 1.0, 1.0, 0.5619942163905924, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5066563093650909, 0.5066563093650905, 0.5276176440500605], 
reward next is 0.4724, 
noisyNet noise sample is [array([0.47006083], dtype=float32), 0.88353455]. 
=============================================
[2019-03-26 12:58:01,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:01,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:01,316] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run9
[2019-03-26 12:58:02,109] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:58:02,116] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0756
[2019-03-26 12:58:02,122] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.46666666666667, 75.33333333333334, 1.0, 2.0, 0.4226127910491406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 612005.3843742938, 612005.3843742944, 175130.3512210485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7490400.0000, 
sim time next is 7491000.0000, 
raw observation next is [26.53333333333333, 75.16666666666666, 1.0, 2.0, 0.4242490707016482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613440.8641981283, 613440.8641981283, 175240.5397031254], 
processed observation next is [0.0, 0.6956521739130435, 0.45655608214849913, 0.7516666666666666, 1.0, 1.0, 0.3063241815682508, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17040024005503565, 0.17040024005503565, 0.26155304433302295], 
reward next is 0.7384, 
noisyNet noise sample is [array([1.2372214], dtype=float32), 0.13027307]. 
=============================================
[2019-03-26 12:58:02,130] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.6342  ]
 [74.62716 ]
 [74.62898 ]
 [74.629845]
 [74.63615 ]], R is [[74.62494659]
 [74.61730957]
 [74.60993195]
 [74.60279846]
 [74.59589386]].
[2019-03-26 12:58:13,063] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:58:13,071] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7958
[2019-03-26 12:58:13,077] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 95.0, 1.0, 2.0, 0.324788456360997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508829.2329260107, 508829.2329260114, 167586.9011929026], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7450800.0000, 
sim time next is 7451400.0000, 
raw observation next is [21.28333333333333, 95.0, 1.0, 2.0, 0.3253127911839967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509433.2232813542, 509433.2232813542, 167627.651623421], 
processed observation next is [0.0, 0.21739130434782608, 0.20774091627172192, 0.95, 1.0, 1.0, 0.18712384479999603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14150922868926505, 0.14150922868926505, 0.2501905248110761], 
reward next is 0.7498, 
noisyNet noise sample is [array([0.62410116], dtype=float32), 0.32842013]. 
=============================================
[2019-03-26 12:58:15,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:58:15,049] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6133
[2019-03-26 12:58:15,060] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 93.0, 1.0, 2.0, 0.5517688611380813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 877217.9051937595, 877217.9051937595, 204146.8271148559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7380000.0000, 
sim time next is 7380600.0000, 
raw observation next is [20.96666666666667, 93.0, 1.0, 2.0, 0.605203892119211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 960627.050068549, 960627.0500685496, 214940.985378375], 
processed observation next is [1.0, 0.43478260869565216, 0.1927330173775673, 0.93, 1.0, 1.0, 0.5243420386978446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2668408472412636, 0.2668408472412638, 0.32080744086324625], 
reward next is 0.6792, 
noisyNet noise sample is [array([0.39001498], dtype=float32), -1.0166326]. 
=============================================
[2019-03-26 12:58:16,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:58:16,959] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0159
[2019-03-26 12:58:16,967] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.45, 86.66666666666667, 1.0, 2.0, 0.3001582676229261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479565.9014857172, 479565.9014857177, 165611.9216484478], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7420200.0000, 
sim time next is 7420800.0000, 
raw observation next is [21.4, 87.33333333333334, 1.0, 2.0, 0.3024593106594867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482888.18072191, 482888.1807219094, 165845.5775294041], 
processed observation next is [1.0, 0.9130434782608695, 0.21327014218009477, 0.8733333333333334, 1.0, 1.0, 0.15958953091504424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1341356057560861, 0.13413560575608593, 0.2475307127304539], 
reward next is 0.7525, 
noisyNet noise sample is [array([-0.28542498], dtype=float32), -0.30627152]. 
=============================================
[2019-03-26 12:58:19,550] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:58:19,558] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2906
[2019-03-26 12:58:19,563] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 92.66666666666667, 1.0, 2.0, 0.3161169590869229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 498397.8951746832, 498397.8951746839, 166874.1749414471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7442400.0000, 
sim time next is 7443000.0000, 
raw observation next is [21.3, 93.0, 1.0, 2.0, 0.3175647437520359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 500191.5668919739, 500191.5668919745, 166997.5080082818], 
processed observation next is [0.0, 0.13043478260869565, 0.2085308056872039, 0.93, 1.0, 1.0, 0.17778884789401916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1389421019144372, 0.13894210191443734, 0.2492500119526594], 
reward next is 0.7507, 
noisyNet noise sample is [array([1.8028282], dtype=float32), -1.0912324]. 
=============================================
[2019-03-26 12:58:19,587] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.697395]
 [75.63191 ]
 [75.53208 ]
 [75.30097 ]
 [75.345505]], R is [[75.86948395]
 [75.86171722]
 [75.85409546]
 [75.84662628]
 [75.83931732]].
[2019-03-26 12:58:19,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:19,935] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:20,021] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run9
[2019-03-26 12:58:20,901] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:58:20,910] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7180
[2019-03-26 12:58:20,921] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 79.0, 1.0, 2.0, 0.4158893333616966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606903.4914427524, 606903.4914427524, 174784.5962774617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7496400.0000, 
sim time next is 7497000.0000, 
raw observation next is [25.55, 79.5, 1.0, 2.0, 0.4140619188921128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 605155.6039935349, 605155.6039935349, 174647.3863554103], 
processed observation next is [0.0, 0.782608695652174, 0.40995260663507116, 0.795, 1.0, 1.0, 0.29405050468929256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16809877888709304, 0.16809877888709304, 0.2606677408289706], 
reward next is 0.7393, 
noisyNet noise sample is [array([0.15480027], dtype=float32), -0.3644954]. 
=============================================
[2019-03-26 12:58:20,932] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.646545]
 [82.58795 ]
 [82.52545 ]
 [82.346695]
 [82.28889 ]], R is [[82.61179352]
 [82.52480316]
 [82.43846893]
 [82.3528595 ]
 [82.26811981]].
[2019-03-26 12:58:23,625] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.10746983e-29 1.00000000e+00 1.36503679e-35 1.11212334e-35
 2.42364712e-29], sum to 1.0000
[2019-03-26 12:58:23,637] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2646
[2019-03-26 12:58:23,645] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2327642.427895051 W.
[2019-03-26 12:58:23,652] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.9, 61.0, 1.0, 2.0, 0.5548382412676681, 1.0, 2.0, 0.5548382412676681, 1.0, 2.0, 0.9486768168174813, 6.9112, 6.9112, 170.5573041426782, 2327642.427895051, 2327642.427895051, 452094.4910767837], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7747200.0000, 
sim time next is 7747800.0000, 
raw observation next is [30.8, 61.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.189491614774166, 6.9112, 168.911346628328, 2502522.411185856, 2305094.643276976, 476459.0660075679], 
processed observation next is [1.0, 0.6956521739130435, 0.6587677725118484, 0.6133333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.027829161477416608, 0.0, 0.8294320398941698, 0.6951451142182933, 0.6403040675769378, 0.7111329343396536], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1243786], dtype=float32), 0.52299917]. 
=============================================
[2019-03-26 12:58:28,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:28,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:28,095] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run9
[2019-03-26 12:58:29,487] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2444404e-36], sum to 1.0000
[2019-03-26 12:58:29,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5565
[2019-03-26 12:58:29,505] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2077293.679690482 W.
[2019-03-26 12:58:29,512] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.73333333333333, 60.0, 1.0, 2.0, 0.8444563954463666, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.969979987067115, 6.9112, 168.9125483943121, 2077293.679690482, 2035593.2345681, 420318.2414653023], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7654800.0000, 
sim time next is 7655400.0000, 
raw observation next is [30.6, 61.0, 1.0, 2.0, 0.8733916574194555, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.968305876940789, 6.9112, 168.912568728755, 2117792.841409076, 2077280.059882052, 428026.4987546484], 
processed observation next is [1.0, 0.6086956521739131, 0.6492890995260664, 0.61, 1.0, 1.0, 0.8474598282162114, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005710587694078928, 0.0, 0.8294380409667462, 0.5882757892802989, 0.5770222388561256, 0.638845520529326], 
reward next is 0.0756, 
noisyNet noise sample is [array([1.6894939], dtype=float32), -1.3070474]. 
=============================================
[2019-03-26 12:58:32,956] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:32,956] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:33,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run9
[2019-03-26 12:58:33,982] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:33,983] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:34,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run9
[2019-03-26 12:58:38,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:38,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:38,804] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:58:38,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6195
[2019-03-26 12:58:38,817] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.03333333333333, 92.33333333333334, 1.0, 2.0, 0.8849227226535293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1236855.230653404, 1236855.230653404, 265822.6574180614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7870800.0000, 
sim time next is 7871400.0000, 
raw observation next is [26.05, 92.0, 1.0, 2.0, 0.8139185253152822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1137559.602306956, 1137559.602306956, 247354.6019092652], 
processed observation next is [1.0, 0.08695652173913043, 0.43364928909952616, 0.92, 1.0, 1.0, 0.775805452187087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3159887784185989, 0.3159887784185989, 0.3691859729989033], 
reward next is 0.6308, 
noisyNet noise sample is [array([-1.178442], dtype=float32), 0.81438637]. 
=============================================
[2019-03-26 12:58:38,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run9
[2019-03-26 12:58:41,764] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:41,765] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:41,837] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run9
[2019-03-26 12:58:42,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:42,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:42,103] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run9
[2019-03-26 12:58:42,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:42,738] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:42,779] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run9
[2019-03-26 12:58:43,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:43,427] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:43,479] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 12:58:43,481] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3297
[2019-03-26 12:58:43,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-03-26 12:58:43,485] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 89.0, 1.0, 2.0, 0.3090518464448617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488945.2291765588, 488945.2291765588, 166211.5719075229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 860400.0000, 
sim time next is 861000.0000, 
raw observation next is [21.56666666666667, 89.0, 1.0, 2.0, 0.3091973448710491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 489324.0105647654, 489324.0105647654, 166242.3931793907], 
processed observation next is [0.0, 1.0, 0.22116903633491333, 0.89, 1.0, 1.0, 0.16770764442295072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13592333626799039, 0.13592333626799039, 0.248122974894613], 
reward next is 0.7519, 
noisyNet noise sample is [array([-0.6997575], dtype=float32), 0.5360983]. 
=============================================
[2019-03-26 12:58:43,526] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.0501  ]
 [86.881096]
 [86.842255]
 [86.77991 ]
 [86.72194 ]], R is [[86.98010254]
 [86.86222839]
 [86.7456131 ]
 [86.63024902]
 [86.51612854]].
[2019-03-26 12:58:43,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:43,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:43,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:43,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:43,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run9
[2019-03-26 12:58:43,791] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run9
[2019-03-26 12:58:43,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:43,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:43,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run9
[2019-03-26 12:58:44,254] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:44,254] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:44,305] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:44,306] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:44,306] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run9
[2019-03-26 12:58:44,372] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run9
[2019-03-26 12:58:44,400] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 12:58:44,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:44,434] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run9
[2019-03-26 12:58:44,581] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 12:58:44,583] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 12:58:44,584] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:44,585] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 12:58:44,586] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:44,586] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 12:58:44,587] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:44,588] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 12:58:44,588] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 12:58:44,588] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:44,589] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 12:58:44,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run69
[2019-03-26 12:58:44,635] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run69
[2019-03-26 12:58:44,637] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run69
[2019-03-26 12:58:44,686] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run69
[2019-03-26 12:58:44,713] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/2/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run69
[2019-03-26 12:58:58,243] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.3451157], dtype=float32), 0.31373084]
[2019-03-26 12:58:58,244] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.67098818333334, 73.35827955333333, 1.0, 2.0, 0.2905278578257989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 468143.7930755459, 468143.7930755453, 164831.2493984311]
[2019-03-26 12:58:58,245] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:58:58,250] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5642207743926366
[2019-03-26 12:59:16,735] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.3451157], dtype=float32), 0.31373084]
[2019-03-26 12:59:16,737] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.57297178333334, 97.25491425, 1.0, 2.0, 0.4976953894481549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695450.9266563959, 695450.9266563959, 183235.1293082124]
[2019-03-26 12:59:16,738] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:59:16,740] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4835680752514284
[2019-03-26 12:59:24,934] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.3451157], dtype=float32), 0.31373084]
[2019-03-26 12:59:24,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.06530484166667, 88.40547547166668, 1.0, 2.0, 0.3939043012815091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594830.9074397794, 594830.9074397794, 174260.9771434677]
[2019-03-26 12:59:24,938] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 12:59:24,940] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7921996522959504
[2019-03-26 12:59:38,434] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.3451157], dtype=float32), 0.31373084]
[2019-03-26 12:59:38,435] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.21666666666667, 69.33333333333333, 1.0, 2.0, 0.5778877373488086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807549.6990443046, 807549.6990443052, 196698.2986179103]
[2019-03-26 12:59:38,437] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 12:59:38,439] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9652033125866561
[2019-03-26 12:59:53,586] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.3451157], dtype=float32), 0.31373084]
[2019-03-26 12:59:53,587] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.68863845166667, 94.78396854833333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.761462362214672, 6.9112, 169.2678784031959, 2769994.057528736, 1454596.122455011, 309947.4273828237]
[2019-03-26 12:59:53,588] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 12:59:53,590] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.37920055401597397
[2019-03-26 12:59:53,591] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2769994.057528736 W.
[2019-03-26 13:00:02,741] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.3451157], dtype=float32), 0.31373084]
[2019-03-26 13:00:02,743] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.49158309, 77.00550009, 1.0, 2.0, 0.6749905230515787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 943302.9653301984, 943302.965330199, 215544.0738189078]
[2019-03-26 13:00:02,744] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 13:00:02,747] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.031609089799759404
[2019-03-26 13:00:18,852] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.3451157], dtype=float32), 0.31373084]
[2019-03-26 13:00:18,853] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.36666666666667, 91.0, 1.0, 2.0, 0.8897702990469574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1243634.649434489, 1243634.64943449, 267138.3405819759]
[2019-03-26 13:00:18,856] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 13:00:18,859] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9793506061889481
[2019-03-26 13:00:26,610] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.3451157], dtype=float32), 0.31373084]
[2019-03-26 13:00:26,611] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.9, 75.66666666666667, 1.0, 2.0, 0.5176318771208388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723318.5131088246, 723318.5131088246, 186403.3682654855]
[2019-03-26 13:00:26,611] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 13:00:26,614] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7897330628094188
[2019-03-26 13:00:33,877] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.3451157], dtype=float32), 0.31373084]
[2019-03-26 13:00:33,881] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.95, 88.16666666666667, 1.0, 2.0, 0.5739715661886261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802075.1116900512, 802075.1116900512, 195996.4629180903]
[2019-03-26 13:00:33,882] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 13:00:33,884] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8779088883418416
[2019-03-26 13:00:39,722] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.5239 3164221502.4670 1781.0000
[2019-03-26 13:00:40,581] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.4400 2779342923.9390 934.0000
[2019-03-26 13:00:40,801] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9726 2927423362.6803 1341.0000
[2019-03-26 13:00:40,835] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5749 3007726976.5989 1766.0000
[2019-03-26 13:00:40,954] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1326 2842568365.7369 1131.0000
[2019-03-26 13:00:41,971] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1700000, evaluation results [1700000.0, 7881.523944000656, 3164221502.4670277, 1781.0, 8252.97256612743, 2927423362.6803246, 1341.0, 8659.440042916716, 2779342923.9390087, 934.0, 7997.574940102971, 3007726976.5988917, 1766.0, 8496.13258090819, 2842568365.7369456, 1131.0]
[2019-03-26 13:00:42,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:00:42,685] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9578
[2019-03-26 13:00:42,695] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 84.33333333333334, 1.0, 2.0, 0.358997254768441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552483.769309904, 552483.7693099045, 170839.8730776006], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 74400.0000, 
sim time next is 75000.0000, 
raw observation next is [23.18333333333334, 84.66666666666667, 1.0, 2.0, 0.3561662911640412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548715.2445120797, 548715.2445120803, 170540.4556415872], 
processed observation next is [1.0, 0.8695652173913043, 0.29778830963665126, 0.8466666666666667, 1.0, 1.0, 0.2242967363422183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15242090125335547, 0.15242090125335564, 0.2545379934949063], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.31992376], dtype=float32), 0.082159966]. 
=============================================
[2019-03-26 13:00:42,713] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.36245 ]
 [74.23277 ]
 [74.37041 ]
 [74.49555 ]
 [74.432884]], R is [[74.56371307]
 [74.56309509]
 [74.56221008]
 [74.56108093]
 [74.55978394]].
[2019-03-26 13:00:45,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:00:45,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9162
[2019-03-26 13:00:45,227] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.83333333333334, 1.0, 2.0, 0.4098947061082622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 633078.9493693406, 633078.9493693413, 178043.0394659328], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 958200.0000, 
sim time next is 958800.0000, 
raw observation next is [21.8, 94.66666666666667, 1.0, 2.0, 0.3648637457433531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563820.101702934, 563820.1017029347, 171859.8805726792], 
processed observation next is [1.0, 0.08695652173913043, 0.23222748815165886, 0.9466666666666668, 1.0, 1.0, 0.23477559728114833, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15661669491748167, 0.15661669491748187, 0.2565072844368346], 
reward next is 0.7435, 
noisyNet noise sample is [array([0.6599721], dtype=float32), 1.1005346]. 
=============================================
[2019-03-26 13:00:48,557] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:00:48,567] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3818
[2019-03-26 13:00:48,575] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.95, 91.0, 1.0, 2.0, 0.4854247404875331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734863.4458652891, 734863.4458652891, 188265.2080226837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 113400.0000, 
sim time next is 114000.0000, 
raw observation next is [22.96666666666667, 91.0, 1.0, 2.0, 0.4732006339763806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715966.8271768078, 715966.8271768078, 186217.6687613569], 
processed observation next is [1.0, 0.30434782608695654, 0.2875197472353872, 0.91, 1.0, 1.0, 0.36530196864624176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19887967421577996, 0.19887967421577996, 0.27793681904680134], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.1280751], dtype=float32), -1.3458701]. 
=============================================
[2019-03-26 13:00:48,585] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.95562]
 [74.21357]
 [74.39469]
 [75.08521]
 [74.94689]], R is [[73.95487213]
 [73.9343338 ]
 [73.91850281]
 [73.90250397]
 [73.90818787]].
[2019-03-26 13:00:53,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:00:53,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3290
[2019-03-26 13:00:53,083] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 84.33333333333334, 1.0, 2.0, 0.2337647099792791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 386033.3361524688, 386033.3361524688, 159298.7815445992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 441600.0000, 
sim time next is 442200.0000, 
raw observation next is [19.6, 84.16666666666666, 1.0, 2.0, 0.2323332236833912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 383754.6070810878, 383754.6070810872, 159161.3058196733], 
processed observation next is [1.0, 0.08695652173913043, 0.127962085308057, 0.8416666666666666, 1.0, 1.0, 0.07510026949806169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10659850196696882, 0.10659850196696867, 0.23755418779055715], 
reward next is 0.7624, 
noisyNet noise sample is [array([0.6293371], dtype=float32), -0.51371276]. 
=============================================
[2019-03-26 13:00:55,637] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:00:55,648] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0718
[2019-03-26 13:00:55,652] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.25, 87.5, 1.0, 2.0, 0.2567408921909044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418251.4206480645, 418251.4206480645, 161518.4713780084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 366600.0000, 
sim time next is 367200.0000, 
raw observation next is [20.3, 87.0, 1.0, 2.0, 0.2569084511794936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418598.7393006994, 418598.7393006994, 161538.0374686603], 
processed observation next is [1.0, 0.2608695652173913, 0.16113744075829392, 0.87, 1.0, 1.0, 0.1047089773246911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1162774275835276, 0.1162774275835276, 0.24110154846068702], 
reward next is 0.7589, 
noisyNet noise sample is [array([0.48021933], dtype=float32), -1.2437444]. 
=============================================
[2019-03-26 13:01:00,519] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:00,527] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0579
[2019-03-26 13:01:00,534] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.76666666666667, 73.0, 1.0, 2.0, 0.4527235919640077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730803.3037629497, 730803.3037629491, 187222.6351621543], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 393600.0000, 
sim time next is 394200.0000, 
raw observation next is [22.8, 73.0, 1.0, 2.0, 0.4791762917907562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773024.1119324766, 773024.1119324766, 191641.6318986931], 
processed observation next is [1.0, 0.5652173913043478, 0.2796208530805688, 0.73, 1.0, 1.0, 0.37250155637440513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21472891998124352, 0.21472891998124352, 0.28603228641595985], 
reward next is 0.7140, 
noisyNet noise sample is [array([-1.4243996], dtype=float32), -1.0159862]. 
=============================================
[2019-03-26 13:01:06,875] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:06,888] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6513
[2019-03-26 13:01:06,893] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 79.5, 1.0, 2.0, 0.2408018876667755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 398325.5686412163, 398325.5686412163, 159918.3715501579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 505800.0000, 
sim time next is 506400.0000, 
raw observation next is [19.96666666666667, 80.0, 1.0, 2.0, 0.2409283209151263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 398606.4369118143, 398606.436911815, 159924.9910059306], 
processed observation next is [1.0, 0.8695652173913043, 0.14533965244865735, 0.8, 1.0, 1.0, 0.08545580833147746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11072401025328175, 0.11072401025328195, 0.2386940164267621], 
reward next is 0.7613, 
noisyNet noise sample is [array([-1.16975], dtype=float32), -0.759887]. 
=============================================
[2019-03-26 13:01:07,105] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:07,114] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3105
[2019-03-26 13:01:07,121] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 84.0, 1.0, 2.0, 0.2326657747229007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 384388.3960436397, 384388.3960436397, 159187.0100435298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 442800.0000, 
sim time next is 443400.0000, 
raw observation next is [19.61666666666667, 83.83333333333333, 1.0, 2.0, 0.2367096035335818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 391107.8924866772, 391107.8924866778, 159561.6484643982], 
processed observation next is [1.0, 0.13043478260869565, 0.12875197472353894, 0.8383333333333333, 1.0, 1.0, 0.08037301630552024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10864108124629923, 0.10864108124629938, 0.23815171412596745], 
reward next is 0.7618, 
noisyNet noise sample is [array([0.04015502], dtype=float32), -0.72679985]. 
=============================================
[2019-03-26 13:01:07,625] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:07,632] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7877
[2019-03-26 13:01:07,635] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 53.66666666666667, 1.0, 2.0, 0.5948683764483776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 971085.5900813485, 971085.5900813492, 214405.7497413956], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 481800.0000, 
sim time next is 482400.0000, 
raw observation next is [25.4, 53.0, 1.0, 2.0, 0.6031069522156591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 984257.8149547238, 984257.8149547231, 216162.7083517028], 
processed observation next is [1.0, 0.6086956521739131, 0.4028436018957346, 0.53, 1.0, 1.0, 0.5218156050791074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27340494859853437, 0.2734049485985342, 0.3226309079876161], 
reward next is 0.6774, 
noisyNet noise sample is [array([0.74249786], dtype=float32), -0.8479201]. 
=============================================
[2019-03-26 13:01:09,819] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:09,829] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0115
[2019-03-26 13:01:09,836] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.36666666666667, 97.0, 1.0, 2.0, 0.3802825846655978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572803.730547473, 572803.730547473, 172234.9270182879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1039800.0000, 
sim time next is 1040400.0000, 
raw observation next is [22.4, 97.0, 1.0, 2.0, 0.3809588480346077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573181.2049272379, 573181.2049272379, 172248.3978779776], 
processed observation next is [1.0, 0.043478260869565216, 0.2606635071090047, 0.97, 1.0, 1.0, 0.254167286788684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1592170013686772, 0.1592170013686772, 0.2570871610119069], 
reward next is 0.7429, 
noisyNet noise sample is [array([-0.46236098], dtype=float32), 1.4762614]. 
=============================================
[2019-03-26 13:01:11,840] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:11,848] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5367
[2019-03-26 13:01:11,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:11,860] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 73.66666666666667, 1.0, 2.0, 0.2404900417532672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397648.4020922547, 397648.4020922547, 159900.2461510204], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 591000.0000, 
sim time next is 591600.0000, 
raw observation next is [20.73333333333333, 74.33333333333334, 1.0, 2.0, 0.2391987161748259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395672.5005185914, 395672.5005185914, 159765.9931771098], 
processed observation next is [1.0, 0.8695652173913043, 0.18167456556082143, 0.7433333333333334, 1.0, 1.0, 0.08337194719858543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10990902792183094, 0.10990902792183094, 0.23845670623449225], 
reward next is 0.7615, 
noisyNet noise sample is [array([0.09060648], dtype=float32), 0.59913003]. 
=============================================
[2019-03-26 13:01:11,866] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3405
[2019-03-26 13:01:11,871] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.61666666666667, 64.5, 1.0, 2.0, 0.4529028689701042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745795.1515899012, 745795.1515899012, 187695.6014266481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 555000.0000, 
sim time next is 555600.0000, 
raw observation next is [22.83333333333333, 63.00000000000001, 1.0, 2.0, 0.4305719741467672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709213.5452492372, 709213.5452492366, 184102.8378267748], 
processed observation next is [1.0, 0.43478260869565216, 0.2812006319115322, 0.6300000000000001, 1.0, 1.0, 0.31394213752622546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19700376256923255, 0.1970037625692324, 0.27478035496533554], 
reward next is 0.7252, 
noisyNet noise sample is [array([-0.00506751], dtype=float32), -1.0084716]. 
=============================================
[2019-03-26 13:01:17,971] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:17,980] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3453
[2019-03-26 13:01:17,984] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 74.0, 1.0, 2.0, 0.2443662082116722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403511.8456770882, 403511.8456770882, 160304.7886285869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 633600.0000, 
sim time next is 634200.0000, 
raw observation next is [21.16666666666667, 73.16666666666667, 1.0, 2.0, 0.3049239794157823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503168.4676308419, 503168.4676308419, 166812.8907504143], 
processed observation next is [1.0, 0.34782608695652173, 0.2022116903633494, 0.7316666666666667, 1.0, 1.0, 0.16255901134431602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13976901878634496, 0.13976901878634496, 0.24897446380658853], 
reward next is 0.7510, 
noisyNet noise sample is [array([2.7646644], dtype=float32), 0.25635]. 
=============================================
[2019-03-26 13:01:21,252] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:21,259] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7511
[2019-03-26 13:01:21,266] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.25, 90.5, 1.0, 2.0, 0.2269942232922713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 377042.6333263163, 377042.633326317, 158453.9648819507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 693000.0000, 
sim time next is 693600.0000, 
raw observation next is [18.2, 90.66666666666667, 1.0, 2.0, 0.2260716568205668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 375605.6039571176, 375605.603957117, 158354.8183598971], 
processed observation next is [1.0, 0.0, 0.06161137440758297, 0.9066666666666667, 1.0, 1.0, 0.06755621303682745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10433488998808822, 0.10433488998808806, 0.23635047516402552], 
reward next is 0.7636, 
noisyNet noise sample is [array([0.89695925], dtype=float32), 0.09360868]. 
=============================================
[2019-03-26 13:01:28,009] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:28,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8224
[2019-03-26 13:01:28,026] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 71.0, 1.0, 2.0, 0.2884861582285086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462428.4785197963, 462428.4785197969, 164428.0509224132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 811200.0000, 
sim time next is 811800.0000, 
raw observation next is [23.65, 70.0, 1.0, 2.0, 0.2890485833937786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463072.8546722764, 463072.8546722764, 164469.9306947154], 
processed observation next is [0.0, 0.391304347826087, 0.31990521327014215, 0.7, 1.0, 1.0, 0.1434320281852754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1286313485200768, 0.1286313485200768, 0.24547750849957525], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.28707686], dtype=float32), 0.099084996]. 
=============================================
[2019-03-26 13:01:30,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 13:01:30,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4942
[2019-03-26 13:01:30,914] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 87.5, 1.0, 2.0, 0.306765042468953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486322.4256024822, 486322.4256024816, 166040.3258112925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 858600.0000, 
sim time next is 859200.0000, 
raw observation next is [21.66666666666667, 88.0, 1.0, 2.0, 0.3075302115573703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487202.2530669138, 487202.2530669138, 166097.8522348183], 
processed observation next is [0.0, 0.9565217391304348, 0.22590837282780438, 0.88, 1.0, 1.0, 0.16569905006912086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13533395918525384, 0.13533395918525384, 0.24790724214151985], 
reward next is 0.7521, 
noisyNet noise sample is [array([-0.82243896], dtype=float32), -1.6809416]. 
=============================================
[2019-03-26 13:01:31,688] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
