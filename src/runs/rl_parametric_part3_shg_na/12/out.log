Using TensorFlow backend.
[2019-03-27 04:43:13,792] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=5e-06, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-03-27 04:43:13,793] A3C_AGENT_MAIN INFO:Start compiling...
2019-03-27 04:43:13.838399: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-03-27 04:43:32,751] A3C_AGENT_MAIN INFO:Start the learning...
[2019-03-27 04:43:32,752] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-03-27 04:43:32,761] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-03-27 04:43:32,766] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-03-27 04:43:32,771] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-03-27 04:43:32,776] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-03-27 04:43:32,779] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-03-27 04:43:32,779] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:32,779] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-03-27 04:43:32,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:32,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-03-27 04:43:33,780] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:33,783] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-03-27 04:43:33,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:33,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-03-27 04:43:34,084] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 04:43:34,085] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:43:34,085] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:43:34,085] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:43:34,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:34,086] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:34,087] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:34,086] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:43:34,087] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:43:34,087] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:34,088] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:34,091] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run1
[2019-03-27 04:43:34,092] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run1
[2019-03-27 04:43:34,109] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run1
[2019-03-27 04:43:34,110] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run1
[2019-03-27 04:43:34,110] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run1
[2019-03-27 04:43:34,784] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:34,785] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-03-27 04:43:34,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:34,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-03-27 04:43:35,785] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:35,789] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-03-27 04:43:35,887] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:35,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-03-27 04:43:36,788] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:36,796] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-03-27 04:43:36,889] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:36,894] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-03-27 04:43:37,794] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:37,799] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-03-27 04:43:37,867] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:37,868] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-03-27 04:43:38,802] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:38,804] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-03-27 04:43:38,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:38,918] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-03-27 04:43:39,805] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:39,808] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-03-27 04:43:39,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:39,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-03-27 04:43:40,810] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:40,814] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-03-27 04:43:40,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:40,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-03-27 04:43:41,815] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:41,822] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-03-27 04:43:41,887] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:41,888] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-03-27 04:43:42,818] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:42,822] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-03-27 04:43:42,887] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:42,888] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-03-27 04:43:43,825] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:43,832] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-03-27 04:43:43,948] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:43,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-03-27 04:43:44,829] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:44,832] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-03-27 04:43:44,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:44,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-03-27 04:43:45,833] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:45,839] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-03-27 04:43:45,899] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:45,899] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-03-27 04:43:46,838] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:46,840] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-03-27 04:43:46,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:46,901] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-03-27 04:43:47,842] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 04:43:47,848] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-03-27 04:43:47,910] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:43:47,911] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-03-27 04:44:08,807] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 04:44:08,809] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.51666666666667, 87.33333333333333, 1.0, 2.0, 0.2733599199585588, 1.0, 1.0, 0.2733599199585588, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 763976.8613523102, 763976.8613523107, 244939.761422886]
[2019-03-27 04:44:08,811] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:44:08,815] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.14903793 0.32164758 0.04733637 0.33131948 0.15065865], sampled 0.5956222382309405
[2019-03-27 04:44:12,172] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 04:44:12,174] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.2, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.540105895804591, 6.9112, 168.9094137268961, 2730891.37210265, 2284733.242273734, 474727.2988634427]
[2019-03-27 04:44:12,176] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:44:12,181] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.18583569 0.33138362 0.04950669 0.28263032 0.15064362], sampled 0.3397525660429366
[2019-03-27 04:44:12,183] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2730891.37210265 W.
[2019-03-27 04:44:14,536] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 04:44:14,538] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.22482773333333, 91.82947281, 1.0, 2.0, 0.7276130674387851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1037477.469151002, 1037477.469151002, 229707.5521750977]
[2019-03-27 04:44:14,539] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:44:14,543] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.19847311 0.2888375  0.06121043 0.28755465 0.16392435], sampled 0.7593318707683574
[2019-03-27 04:44:50,821] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 04:44:50,822] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.80713717666666, 77.17547555, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.994764997516357, 6.9112, 168.9123161432625, 888107.7086828399, 828824.0481110661, 254811.6209048298]
[2019-03-27 04:44:50,822] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:44:50,824] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.17611916 0.28709036 0.0664625  0.30563244 0.16469555], sampled 0.039263313182778425
[2019-03-27 04:44:50,826] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 888107.7086828399 W.
[2019-03-27 04:45:17,939] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 04:45:17,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.16666666666667, 88.66666666666667, 1.0, 2.0, 0.4230102015526667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 618320.1724005945, 618320.1724005952, 175903.3377275686]
[2019-03-27 04:45:17,942] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:45:17,943] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.18083183 0.2866025  0.05147256 0.34508404 0.13600907], sampled 0.42165865741944364
[2019-03-27 04:45:20,283] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 04:45:20,285] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.17322179666667, 85.23477686333334, 1.0, 1.0, 0.1923904777262808, 1.0, 1.0, 0.1923904777262808, 0.0, 1.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 585195.5762397284, 585195.5762397284, 240580.3896473674]
[2019-03-27 04:45:20,286] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:45:20,290] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.16709897 0.3068959  0.10144845 0.23186582 0.19269092], sampled 0.5650637974484957
[2019-03-27 04:45:30,336] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 4027.4860 3257111136.5439 922.0000
[2019-03-27 04:45:30,446] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 3862.8950 3320833877.9456 1184.0000
[2019-03-27 04:45:30,459] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 04:45:30,460] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.88534048166666, 67.55213923, 1.0, 2.0, 0.2712836630412224, 1.0, 1.0, 0.2712836630412224, 1.0, 1.0, 0.4711300078243119, 6.9112, 6.9112, 184.5923449428631, 1137413.416047559, 1137413.416047559, 297160.0116108444]
[2019-03-27 04:45:30,460] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:45:30,463] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.18670452 0.2649545  0.05237566 0.3309444  0.16502094], sampled 0.18787494746992284
[2019-03-27 04:45:30,779] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 4028.7176 3135285754.9326 609.0000
[2019-03-27 04:45:30,851] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3959.9677 3177673280.9457 775.0000
[2019-03-27 04:45:31,034] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3784.9027 3472849740.4331 1281.0000
[2019-03-27 04:45:32,050] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3784.902657082256, 3472849740.4331408, 1281.0, 4027.4860290023344, 3257111136.5439434, 922.0, 4028.7176038259818, 3135285754.932574, 609.0, 3862.894979976856, 3320833877.945599, 1184.0, 3959.9677190888106, 3177673280.9456797, 775.0]
[2019-03-27 04:45:34,757] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.18815795 0.2833867  0.14375655 0.24183093 0.14286791], sum to 1.0000
[2019-03-27 04:45:34,764] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3137
[2019-03-27 04:45:34,882] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 85.0, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 474976.3909619027, 474976.3909619021, 228155.1364199565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 10800.0000, 
sim time next is 11400.0000, 
raw observation next is [21.03333333333333, 85.00000000000001, 1.0, 2.0, 0.3452468245777338, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 556931.7869135276, 556931.7869135276, 171486.7393772454], 
processed observation next is [1.0, 0.13043478260869565, 0.19589257503949445, 0.8500000000000001, 1.0, 1.0, 0.21114075250329375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15470327414264656, 0.15470327414264656, 0.25595035727947074], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.12020718], dtype=float32), 0.8383579]. 
=============================================
[2019-03-27 04:45:50,963] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7936: loss 3.3710
[2019-03-27 04:45:50,968] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7936: loss 4.1630
[2019-03-27 04:45:51,024] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.18066359 0.42994663 0.04764461 0.24566483 0.09608035], sum to 1.0000
[2019-03-27 04:45:51,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7936: learning rate 0.0000
[2019-03-27 04:45:51,025] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6351
[2019-03-27 04:45:51,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7938: learning rate 0.0000
[2019-03-27 04:45:51,177] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7957: loss 16.1216
[2019-03-27 04:45:51,178] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.13333333333333, 77.33333333333334, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 483878.8310243412, 483878.8310243412, 230603.4287350775], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 298200.0000, 
sim time next is 298800.0000, 
raw observation next is [23.2, 77.0, 1.0, 2.0, 0.17, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2763969745266652, 6.9112, 6.9112, 168.912956510431, 483668.0285035223, 483668.0285035223, 188817.4623936583], 
processed observation next is [0.0, 0.4782608695652174, 0.29857819905213273, 0.77, 1.0, 1.0, 0.0, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.11755728600812826, 0.0, 0.0, 0.8294399451523027, 0.1343522301398673, 0.1343522301398673, 0.28181710805023624], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5968167], dtype=float32), -1.9047865]. 
=============================================
[2019-03-27 04:45:51,183] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7957: learning rate 0.0000
[2019-03-27 04:45:51,188] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7958: loss 13.8062
[2019-03-27 04:45:51,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7959: learning rate 0.0000
[2019-03-27 04:45:51,199] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7964: loss 6.2499
[2019-03-27 04:45:51,200] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7964: loss 2.8258
[2019-03-27 04:45:51,201] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7964: learning rate 0.0000
[2019-03-27 04:45:51,203] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7964: learning rate 0.0000
[2019-03-27 04:45:51,203] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7965: loss -0.9303
[2019-03-27 04:45:51,207] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7965: learning rate 0.0000
[2019-03-27 04:45:51,222] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7972: loss 5.3101
[2019-03-27 04:45:51,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7973: learning rate 0.0000
[2019-03-27 04:45:51,241] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7982: loss -1.5220
[2019-03-27 04:45:51,246] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7983: loss 7.6629
[2019-03-27 04:45:51,247] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7983: learning rate 0.0000
[2019-03-27 04:45:51,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7984: learning rate 0.0000
[2019-03-27 04:45:51,264] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7992: loss 1.8785
[2019-03-27 04:45:51,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7992: learning rate 0.0000
[2019-03-27 04:45:51,302] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8013: loss 11.9797
[2019-03-27 04:45:51,303] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8013: learning rate 0.0000
[2019-03-27 04:45:51,342] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 8030: loss 4.9908
[2019-03-27 04:45:51,344] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8030: loss -1.7349
[2019-03-27 04:45:51,345] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8030: learning rate 0.0000
[2019-03-27 04:45:51,349] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 8031: learning rate 0.0000
[2019-03-27 04:45:51,369] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8042: loss -1.2356
[2019-03-27 04:45:51,371] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8043: learning rate 0.0000
[2019-03-27 04:45:51,447] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 8078: loss 10.0594
[2019-03-27 04:45:51,448] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 8078: learning rate 0.0000
[2019-03-27 04:45:58,363] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.12177103 0.58878356 0.01712139 0.21734667 0.0549774 ], sum to 1.0000
[2019-03-27 04:45:58,372] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2704
[2019-03-27 04:45:58,484] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 84.66666666666667, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 411512.9938195024, 411512.9938195024, 215485.666639067], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 426000.0000, 
sim time next is 426600.0000, 
raw observation next is [19.85, 85.0, 1.0, 2.0, 0.2492708398053106, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 409610.029001041, 409610.0290010416, 160837.7935520826], 
processed observation next is [1.0, 0.9565217391304348, 0.1398104265402845, 0.85, 1.0, 1.0, 0.09550703591001275, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11378056361140028, 0.11378056361140045, 0.24005640828669048], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6709546], dtype=float32), 0.37520304]. 
=============================================
[2019-03-27 04:46:05,221] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.07061341 0.81349325 0.00542176 0.08881076 0.02166087], sum to 1.0000
[2019-03-27 04:46:05,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9006
[2019-03-27 04:46:05,341] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.8, 81.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4278286458847652, 6.911199999999999, 6.9112, 168.912956510431, 383785.0787003994, 383785.0787004, 143416.3176318143], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 543000.0000, 
sim time next is 543600.0000, 
raw observation next is [20.0, 80.0, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 384856.5325257319, 384856.5325257313, 210227.9814064855], 
processed observation next is [1.0, 0.30434782608695654, 0.1469194312796209, 0.8, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.10690459236825886, 0.1069045923682587, 0.313773106576844], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0050784], dtype=float32), 0.04449473]. 
=============================================
[2019-03-27 04:46:08,384] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15908: loss 7.6997
[2019-03-27 04:46:08,387] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15908: learning rate 0.0000
[2019-03-27 04:46:08,388] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15908: loss 11.2238
[2019-03-27 04:46:08,393] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15909: learning rate 0.0000
[2019-03-27 04:46:08,479] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15951: loss 8.1241
[2019-03-27 04:46:08,483] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15951: learning rate 0.0000
[2019-03-27 04:46:08,498] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15958: loss 7.5671
[2019-03-27 04:46:08,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15959: learning rate 0.0000
[2019-03-27 04:46:08,507] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15961: loss 10.0805
[2019-03-27 04:46:08,512] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15961: learning rate 0.0000
[2019-03-27 04:46:08,515] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15963: loss 7.3059
[2019-03-27 04:46:08,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15964: learning rate 0.0000
[2019-03-27 04:46:08,520] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15965: loss 11.5101
[2019-03-27 04:46:08,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15966: learning rate 0.0000
[2019-03-27 04:46:08,529] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15967: loss 7.0286
[2019-03-27 04:46:08,535] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15970: learning rate 0.0000
[2019-03-27 04:46:08,551] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15978: loss 7.7316
[2019-03-27 04:46:08,553] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15979: learning rate 0.0000
[2019-03-27 04:46:08,575] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15992: loss 2.4838
[2019-03-27 04:46:08,578] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15993: learning rate 0.0000
[2019-03-27 04:46:08,584] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15995: loss 11.2686
[2019-03-27 04:46:08,585] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15995: learning rate 0.0000
[2019-03-27 04:46:08,641] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 16020: loss 8.6212
[2019-03-27 04:46:08,644] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 16020: learning rate 0.0000
[2019-03-27 04:46:08,668] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16032: loss 5.2975
[2019-03-27 04:46:08,669] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16032: learning rate 0.0000
[2019-03-27 04:46:08,732] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16058: loss 7.4942
[2019-03-27 04:46:08,734] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16058: learning rate 0.0000
[2019-03-27 04:46:08,799] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 16090: loss 7.2064
[2019-03-27 04:46:08,799] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16090: loss 8.0153
[2019-03-27 04:46:08,801] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 16090: learning rate 0.0000
[2019-03-27 04:46:08,802] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16090: learning rate 0.0000
[2019-03-27 04:46:09,831] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.02138571 0.8591389  0.00149733 0.10838848 0.00958966], sum to 1.0000
[2019-03-27 04:46:09,839] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7408
[2019-03-27 04:46:09,947] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.9, 92.0, 1.0, 2.0, 0.1996635674244439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 333873.5449098317, 333873.544909831, 155225.058259189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 622800.0000, 
sim time next is 623400.0000, 
raw observation next is [17.13333333333333, 91.0, 1.0, 2.0, 0.2286987588463722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 382335.3404762928, 382335.3404762922, 157779.9561717853], 
processed observation next is [1.0, 0.21739130434782608, 0.011058451816745531, 0.91, 1.0, 1.0, 0.07072139620044841, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10620426124341467, 0.10620426124341449, 0.23549247189818703], 
reward next is 0.7645, 
noisyNet noise sample is [array([-0.51920396], dtype=float32), 0.06121358]. 
=============================================
[2019-03-27 04:46:11,169] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01870822 0.9185612  0.00182643 0.05651516 0.00438901], sum to 1.0000
[2019-03-27 04:46:11,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6753
[2019-03-27 04:46:11,184] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 61.5, 1.0, 2.0, 0.5755322113276665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 943539.055656543, 943539.055656543, 210440.7834478318], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 642600.0000, 
sim time next is 643200.0000, 
raw observation next is [23.66666666666667, 60.66666666666666, 1.0, 2.0, 0.5897084698377402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 966447.4489664057, 966447.4489664051, 213397.3104552402], 
processed observation next is [1.0, 0.43478260869565216, 0.3206951026856243, 0.6066666666666666, 1.0, 1.0, 0.505672855226193, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2684576247128905, 0.2684576247128903, 0.318503448440657], 
reward next is 0.6815, 
noisyNet noise sample is [array([-0.8865006], dtype=float32), -0.582781]. 
=============================================
[2019-03-27 04:46:11,257] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02263802 0.8778064  0.00132123 0.08680371 0.01143066], sum to 1.0000
[2019-03-27 04:46:11,267] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6685
[2019-03-27 04:46:11,281] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666667, 63.16666666666667, 1.0, 2.0, 0.5723031764163317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 938955.3949793652, 938955.3949793659, 209783.9554726041], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 641400.0000, 
sim time next is 642000.0000, 
raw observation next is [23.33333333333334, 62.33333333333334, 1.0, 2.0, 0.5348901396939983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 877201.9130404252, 877201.9130404245, 202335.4272167778], 
processed observation next is [1.0, 0.43478260869565216, 0.3048973143759877, 0.6233333333333334, 1.0, 1.0, 0.4396266743301185, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24366719806678477, 0.24366719806678458, 0.30199317495041467], 
reward next is 0.6980, 
noisyNet noise sample is [array([0.21048227], dtype=float32), -0.8130204]. 
=============================================
[2019-03-27 04:46:11,295] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[8.621316]
 [8.602365]
 [8.559564]
 [8.417009]
 [8.614717]], R is [[ 9.49705219]
 [10.08897114]
 [10.70029163]
 [11.30476189]
 [11.19171429]].
[2019-03-27 04:46:13,456] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2231375e-02 9.6162820e-01 1.9225456e-04 2.4044830e-02 1.9033176e-03], sum to 1.0000
[2019-03-27 04:46:13,461] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4754
[2019-03-27 04:46:13,468] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 81.33333333333334, 1.0, 2.0, 0.2408809441466113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 398089.8076944284, 398089.807694429, 159951.0079024768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [19.8, 82.0, 1.0, 2.0, 0.240195975440092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 397035.174257764, 397035.1742577647, 159880.80331467], 
processed observation next is [1.0, 0.9130434782608695, 0.13744075829383895, 0.82, 1.0, 1.0, 0.084573464385653, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11028754840493445, 0.11028754840493464, 0.23862806464876118], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.72101414], dtype=float32), 0.4500561]. 
=============================================
[2019-03-27 04:46:14,388] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1469206e-02 9.3120247e-01 3.1141812e-04 5.5430103e-02 1.5868196e-03], sum to 1.0000
[2019-03-27 04:46:14,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8139
[2019-03-27 04:46:14,405] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.9, 92.0, 1.0, 2.0, 0.2224132996837782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 369961.1697761447, 369961.1697761447, 157944.6875884107], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 696600.0000, 
sim time next is 697200.0000, 
raw observation next is [17.83333333333333, 92.33333333333334, 1.0, 2.0, 0.22213393700398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 369577.2077384248, 369577.2077384248, 157902.5166785965], 
processed observation next is [1.0, 0.043478260869565216, 0.044233807266982464, 0.9233333333333335, 1.0, 1.0, 0.06281197229395177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10266033548289578, 0.10266033548289578, 0.23567539802775597], 
reward next is 0.7643, 
noisyNet noise sample is [array([-0.6864666], dtype=float32), -1.0276773]. 
=============================================
[2019-03-27 04:46:17,761] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5963686e-03 9.7637546e-01 3.5107067e-05 2.1707809e-02 2.8531541e-04], sum to 1.0000
[2019-03-27 04:46:17,769] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7195
[2019-03-27 04:46:17,777] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 54.5, 1.0, 2.0, 0.2284614883460498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377813.0948803165, 377813.0948803165, 158775.7680412834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 754200.0000, 
sim time next is 754800.0000, 
raw observation next is [23.73333333333333, 56.0, 1.0, 2.0, 0.2310126852527967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381839.9661477196, 381839.9661477196, 159022.8178168577], 
processed observation next is [1.0, 0.7391304347826086, 0.3238546603475513, 0.56, 1.0, 1.0, 0.0735092593407189, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10606665726325544, 0.10606665726325544, 0.2373474892788921], 
reward next is 0.7627, 
noisyNet noise sample is [array([0.83397543], dtype=float32), 2.0153847]. 
=============================================
[2019-03-27 04:46:26,017] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23874: loss 4.5947
[2019-03-27 04:46:26,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23874: learning rate 0.0000
[2019-03-27 04:46:26,054] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23887: loss 4.6105
[2019-03-27 04:46:26,058] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23888: learning rate 0.0000
[2019-03-27 04:46:26,104] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23913: loss 5.0268
[2019-03-27 04:46:26,109] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23913: learning rate 0.0000
[2019-03-27 04:46:26,109] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23914: loss 5.0093
[2019-03-27 04:46:26,122] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23917: learning rate 0.0000
[2019-03-27 04:46:26,182] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23945: loss 4.9449
[2019-03-27 04:46:26,186] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23948: loss 5.0245
[2019-03-27 04:46:26,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23947: learning rate 0.0000
[2019-03-27 04:46:26,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23948: learning rate 0.0000
[2019-03-27 04:46:26,247] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23973: loss 4.8511
[2019-03-27 04:46:26,249] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23974: learning rate 0.0000
[2019-03-27 04:46:26,270] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23984: loss 5.2163
[2019-03-27 04:46:26,273] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23985: learning rate 0.0000
[2019-03-27 04:46:26,280] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23989: loss 4.8941
[2019-03-27 04:46:26,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23991: learning rate 0.0000
[2019-03-27 04:46:26,300] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23997: loss 4.7855
[2019-03-27 04:46:26,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23998: learning rate 0.0000
[2019-03-27 04:46:26,351] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24018: loss 5.2524
[2019-03-27 04:46:26,353] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24019: learning rate 0.0000
[2019-03-27 04:46:26,364] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 24021: loss 4.9134
[2019-03-27 04:46:26,368] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 24023: learning rate 0.0000
[2019-03-27 04:46:26,395] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24034: loss 5.1547
[2019-03-27 04:46:26,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24035: learning rate 0.0000
[2019-03-27 04:46:26,478] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24077: loss 5.3003
[2019-03-27 04:46:26,479] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24077: learning rate 0.0000
[2019-03-27 04:46:26,513] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 24093: loss 4.8037
[2019-03-27 04:46:26,514] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 24093: learning rate 0.0000
[2019-03-27 04:46:26,591] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 24127: loss 4.8864
[2019-03-27 04:46:26,592] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 24127: learning rate 0.0000
[2019-03-27 04:46:27,290] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.00044776e-03 9.89902854e-01 1.46734155e-05 6.78738160e-03
 2.94653728e-04], sum to 1.0000
[2019-03-27 04:46:27,301] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0649
[2019-03-27 04:46:27,304] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 68.5, 1.0, 2.0, 0.3146142390193817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495163.9308618236, 495163.9308618236, 166612.2068282399], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 917400.0000, 
sim time next is 918000.0000, 
raw observation next is [24.7, 69.0, 1.0, 2.0, 0.3156583322287182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496459.4292085069, 496459.4292085069, 166700.2157904219], 
processed observation next is [0.0, 0.6521739130434783, 0.3696682464454976, 0.69, 1.0, 1.0, 0.17549196654062432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.137905397002363, 0.137905397002363, 0.2488062922245103], 
reward next is 0.7512, 
noisyNet noise sample is [array([2.0747871], dtype=float32), 1.0029039]. 
=============================================
[2019-03-27 04:46:27,314] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[17.504948]
 [17.439907]
 [17.505947]
 [17.421474]
 [17.356678]], R is [[18.08654594]
 [18.65700722]
 [19.22195053]
 [19.78148842]
 [20.33562088]].
[2019-03-27 04:46:28,258] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2233345e-03 9.9334210e-01 1.2250598e-06 5.3640651e-03 6.9286354e-05], sum to 1.0000
[2019-03-27 04:46:28,268] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7169
[2019-03-27 04:46:28,373] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333334, 82.83333333333334, 1.0, 2.0, 0.3303007847433389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 514730.1321471648, 514730.1321471654, 167969.7924274406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 933000.0000, 
sim time next is 933600.0000, 
raw observation next is [22.96666666666667, 83.66666666666667, 1.0, 2.0, 0.3323981950555168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 517377.7231322267, 517377.7231322261, 168159.1274346377], 
processed observation next is [0.0, 0.8260869565217391, 0.2875197472353872, 0.8366666666666667, 1.0, 1.0, 0.19566047597050218, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1437160342033963, 0.14371603420339613, 0.250983772290504], 
reward next is 0.7490, 
noisyNet noise sample is [array([1.423805], dtype=float32), -1.422183]. 
=============================================
[2019-03-27 04:46:28,603] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 04:46:28,606] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:46:28,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:46:28,606] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:46:28,608] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:46:28,608] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:46:28,609] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:46:28,610] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:46:28,611] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:46:28,611] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:46:28,614] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:46:28,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run2
[2019-03-27 04:46:28,645] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run2
[2019-03-27 04:46:28,664] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run2
[2019-03-27 04:46:28,665] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run2
[2019-03-27 04:46:28,711] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run2
[2019-03-27 04:46:40,818] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.018150063]
[2019-03-27 04:46:40,819] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.28144735166667, 86.70880751333333, 1.0, 2.0, 0.2619782183791633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 432139.7877679596, 432139.7877679589, 162079.5157486193]
[2019-03-27 04:46:40,821] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:46:40,824] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.2064960e-04 9.9479705e-01 1.6250435e-06 4.6386807e-03 4.1964297e-05], sampled 0.2472760511595995
[2019-03-27 04:46:59,059] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.018150063]
[2019-03-27 04:46:59,061] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.83333333333333, 90.33333333333334, 1.0, 2.0, 0.4637962557258971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656729.5635047563, 656729.5635047563, 179234.1623439344]
[2019-03-27 04:46:59,062] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:46:59,065] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.4342919e-04 9.9563038e-01 1.3425488e-06 3.8894131e-03 3.5524878e-05], sampled 0.25798377966009645
[2019-03-27 04:47:12,586] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.018150063]
[2019-03-27 04:47:12,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 88.0, 1.0, 2.0, 0.4964058985078508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 779975.7503229374, 779975.7503229368, 193047.9795588907]
[2019-03-27 04:47:12,588] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:47:12,590] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.7457261e-04 9.9602050e-01 1.0175274e-06 3.5743522e-03 2.9597313e-05], sampled 0.6644006608644625
[2019-03-27 04:47:21,303] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.018150063]
[2019-03-27 04:47:21,305] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.03333333333333, 73.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.168351044699773, 6.9112, 168.9118002779124, 1636310.418787268, 1453879.865992213, 311352.1187036757]
[2019-03-27 04:47:21,305] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:47:21,311] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5107528e-04 9.9733138e-01 2.7681000e-07 2.5080808e-03 9.1512447e-06], sampled 0.10373250851060634
[2019-03-27 04:47:31,337] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.018150063]
[2019-03-27 04:47:31,339] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.4, 70.0, 1.0, 2.0, 0.8860667578923775, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.992899601008183, 6.9112, 168.9124037884646, 2135534.000938893, 2077573.68176052, 430713.954138997]
[2019-03-27 04:47:31,341] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:47:31,343] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1138301e-04 9.9699748e-01 2.9783538e-07 2.7750605e-03 1.5827532e-05], sampled 0.2123484224128902
[2019-03-27 04:47:31,344] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2135534.000938893 W.
[2019-03-27 04:47:42,131] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.018150063]
[2019-03-27 04:47:42,133] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.791259925, 76.20824203000001, 1.0, 2.0, 0.5747074898352832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803103.8894664122, 803103.8894664122, 196127.8163957293]
[2019-03-27 04:47:42,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:47:42,138] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6935504e-04 9.9764192e-01 2.3871354e-07 2.1794345e-03 9.0716630e-06], sampled 0.31967452311351874
[2019-03-27 04:47:49,693] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.018150063]
[2019-03-27 04:47:49,695] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.693867755, 59.89018651, 1.0, 2.0, 0.4359599537498029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 624018.0204331019, 624018.0204331013, 176089.4055869539]
[2019-03-27 04:47:49,696] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:47:49,699] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2681296e-04 9.9702829e-01 3.5064627e-07 2.7327915e-03 1.1782415e-05], sampled 0.8565618868838599
[2019-03-27 04:48:22,015] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8445.1806 2845333896.7830 1130.0000
[2019-03-27 04:48:22,787] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7842.8641 3165899177.9381 1776.0000
[2019-03-27 04:48:22,977] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8614.4237 2781769405.4825 930.0000
[2019-03-27 04:48:22,998] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7956.2750 3009428450.3815 1759.0000
[2019-03-27 04:48:23,028] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8213.4619 2929546309.3029 1335.0000
[2019-03-27 04:48:24,043] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 25000, evaluation results [25000.0, 7842.86408932454, 3165899177.938097, 1776.0, 8213.461882211275, 2929546309.3028717, 1335.0, 8614.423697134114, 2781769405.4824986, 930.0, 7956.275033442869, 3009428450.381522, 1759.0, 8445.180609012361, 2845333896.7830334, 1130.0]
[2019-03-27 04:48:24,352] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5475127e-04 9.9738449e-01 1.5974784e-06 2.2867443e-03 7.2379000e-05], sum to 1.0000
[2019-03-27 04:48:24,368] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0557
[2019-03-27 04:48:24,484] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.0, 1.0, 2.0, 0.3380691997389883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523681.2813667222, 523681.2813667222, 168583.7487958096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 946800.0000, 
sim time next is 947400.0000, 
raw observation next is [21.8, 94.00000000000001, 1.0, 2.0, 0.3376778345863402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523118.7175607817, 523118.7175607817, 168540.317590579], 
processed observation next is [0.0, 1.0, 0.23222748815165886, 0.9400000000000002, 1.0, 1.0, 0.20202148745342194, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1453107548779949, 0.1453107548779949, 0.2515527128217597], 
reward next is 0.7484, 
noisyNet noise sample is [array([-0.6039736], dtype=float32), 0.6042906]. 
=============================================
[2019-03-27 04:48:31,791] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9295600e-06 9.9995089e-01 8.6574792e-10 4.5936427e-05 2.3024849e-07], sum to 1.0000
[2019-03-27 04:48:31,797] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9145
[2019-03-27 04:48:31,806] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 82.33333333333334, 1.0, 2.0, 0.5205601970345691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827988.8692706674, 827988.8692706674, 198243.4293223979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1074000.0000, 
sim time next is 1074600.0000, 
raw observation next is [22.3, 81.5, 1.0, 2.0, 0.54169015138309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 861854.4169165186, 861854.416916518, 202247.3206357979], 
processed observation next is [1.0, 0.43478260869565216, 0.25592417061611383, 0.815, 1.0, 1.0, 0.4478194594976987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23940400469903295, 0.23940400469903278, 0.30186167259074315], 
reward next is 0.6981, 
noisyNet noise sample is [array([-0.97971404], dtype=float32), -0.2570785]. 
=============================================
[2019-03-27 04:48:32,633] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.1421667e-05 9.9982339e-01 2.1464450e-09 1.3454632e-04 6.2627930e-07], sum to 1.0000
[2019-03-27 04:48:32,645] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5273
[2019-03-27 04:48:32,649] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 71.5, 1.0, 2.0, 0.4523465566367458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703886.8113209782, 703886.8113209788, 184994.2462644377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1084200.0000, 
sim time next is 1084800.0000, 
raw observation next is [24.93333333333334, 71.0, 1.0, 2.0, 0.597826819227571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928211.6839156456, 928211.6839156456, 211449.8592102245], 
processed observation next is [1.0, 0.5652173913043478, 0.3807266982622437, 0.71, 1.0, 1.0, 0.5154539990693626, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25783657886545713, 0.25783657886545713, 0.31559680479137986], 
reward next is 0.6844, 
noisyNet noise sample is [array([0.24329534], dtype=float32), 0.31964165]. 
=============================================
[2019-03-27 04:48:36,564] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.69327711e-06 9.99944687e-01 1.80162673e-11 5.25885516e-05
 1.30363125e-08], sum to 1.0000
[2019-03-27 04:48:36,569] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8962
[2019-03-27 04:48:36,681] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 70.33333333333334, 1.0, 2.0, 0.800817054196954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1234726.663193558, 1234726.663193559, 259279.9750308123], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1161600.0000, 
sim time next is 1162200.0000, 
raw observation next is [25.5, 69.66666666666666, 1.0, 2.0, 0.8224270581344343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1264903.845157319, 1264903.845157319, 264872.7545283222], 
processed observation next is [1.0, 0.43478260869565216, 0.40758293838862564, 0.6966666666666665, 1.0, 1.0, 0.7860566965475112, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3513621792103664, 0.3513621792103664, 0.395332469445257], 
reward next is 0.6047, 
noisyNet noise sample is [array([-0.8419682], dtype=float32), 0.65428746]. 
=============================================
[2019-03-27 04:48:36,795] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3138350e-06 9.9994898e-01 4.0428366e-11 4.4717835e-05 8.4972420e-09], sum to 1.0000
[2019-03-27 04:48:36,802] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1385
[2019-03-27 04:48:36,806] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 69.0, 1.0, 2.0, 0.7513651351288666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1152749.972506847, 1152749.972506846, 245571.9352812504], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1162800.0000, 
sim time next is 1163400.0000, 
raw observation next is [25.8, 68.5, 1.0, 2.0, 0.7205659639855024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1104329.640723859, 1104329.640723859, 237746.4048036471], 
processed observation next is [1.0, 0.4782608695652174, 0.42180094786729866, 0.685, 1.0, 1.0, 0.6633324867295209, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3067582335344053, 0.3067582335344053, 0.3548453803039509], 
reward next is 0.6452, 
noisyNet noise sample is [array([-1.9569597], dtype=float32), 0.8220447]. 
=============================================
[2019-03-27 04:48:38,047] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4244449e-07 9.9999833e-01 4.9539314e-12 1.4347459e-06 1.8387754e-09], sum to 1.0000
[2019-03-27 04:48:38,058] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1221
[2019-03-27 04:48:38,065] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 57.16666666666666, 1.0, 2.0, 0.8749218469490505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1349022.648056325, 1349022.648056325, 280355.8618688554], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1183800.0000, 
sim time next is 1184400.0000, 
raw observation next is [27.6, 57.0, 1.0, 2.0, 0.8641109400539346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1333435.593337787, 1333435.593337787, 277297.3716745664], 
processed observation next is [1.0, 0.7391304347826086, 0.5071090047393366, 0.57, 1.0, 1.0, 0.8362782410288369, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3703987759271631, 0.3703987759271631, 0.4138766741411439], 
reward next is 0.5861, 
noisyNet noise sample is [array([-0.75441927], dtype=float32), -1.7638745]. 
=============================================
[2019-03-27 04:48:38,669] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31850: loss 0.9238
[2019-03-27 04:48:38,675] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31852: learning rate 0.0000
[2019-03-27 04:48:38,769] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31901: loss 0.7834
[2019-03-27 04:48:38,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31901: learning rate 0.0000
[2019-03-27 04:48:38,787] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31909: loss 0.8607
[2019-03-27 04:48:38,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31909: learning rate 0.0000
[2019-03-27 04:48:38,863] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 31946: loss 0.7628
[2019-03-27 04:48:38,867] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 31946: learning rate 0.0000
[2019-03-27 04:48:38,870] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31948: loss 0.8410
[2019-03-27 04:48:38,873] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31948: learning rate 0.0000
[2019-03-27 04:48:38,878] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31952: loss 0.8515
[2019-03-27 04:48:38,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31953: learning rate 0.0000
[2019-03-27 04:48:38,909] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31966: loss 0.8654
[2019-03-27 04:48:38,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31966: learning rate 0.0000
[2019-03-27 04:48:38,921] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31971: loss 0.8110
[2019-03-27 04:48:38,924] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31972: learning rate 0.0000
[2019-03-27 04:48:38,934] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31978: loss 0.7001
[2019-03-27 04:48:38,936] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31978: loss 0.8341
[2019-03-27 04:48:38,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31978: learning rate 0.0000
[2019-03-27 04:48:38,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31978: learning rate 0.0000
[2019-03-27 04:48:38,967] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31991: loss 0.8086
[2019-03-27 04:48:38,970] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31991: learning rate 0.0000
[2019-03-27 04:48:39,003] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32008: loss 0.8225
[2019-03-27 04:48:39,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32009: learning rate 0.0000
[2019-03-27 04:48:39,012] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32012: loss 0.7028
[2019-03-27 04:48:39,013] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32012: learning rate 0.0000
[2019-03-27 04:48:39,285] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32114: loss 0.7500
[2019-03-27 04:48:39,289] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8028026e-06 9.9977440e-01 5.0621216e-11 2.2279246e-04 2.0153127e-08], sum to 1.0000
[2019-03-27 04:48:39,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32114: learning rate 0.0000
[2019-03-27 04:48:39,297] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5220
[2019-03-27 04:48:39,406] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 85.33333333333334, 1.0, 2.0, 0.3554413071659422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547751.2515474142, 547751.2515474149, 170464.3484644801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1204800.0000, 
sim time next is 1205400.0000, 
raw observation next is [23.0, 86.16666666666666, 1.0, 2.0, 0.3560965178943104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548579.436239281, 548579.436239281, 170528.3612299317], 
processed observation next is [1.0, 0.9565217391304348, 0.28909952606635075, 0.8616666666666666, 1.0, 1.0, 0.22421267216181975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1523831767331336, 0.1523831767331336, 0.2545199421342264], 
reward next is 0.7455, 
noisyNet noise sample is [array([-1.1576722], dtype=float32), -0.8156042]. 
=============================================
[2019-03-27 04:48:39,413] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 32132: loss 0.7065
[2019-03-27 04:48:39,415] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 32132: learning rate 0.0000
[2019-03-27 04:48:39,462] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 32156: loss 0.7631
[2019-03-27 04:48:39,466] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 32157: learning rate 0.0000
[2019-03-27 04:48:42,898] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.80321410e-08 9.99993205e-01 1.12043614e-13 6.76277887e-06
 9.05914233e-11], sum to 1.0000
[2019-03-27 04:48:42,911] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4311
[2019-03-27 04:48:42,917] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.65, 76.66666666666667, 1.0, 2.0, 0.4399265721338178, 1.0, 1.0, 0.4399265721338178, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1229758.518390524, 1229758.518390524, 282644.6195841048], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1271400.0000, 
sim time next is 1272000.0000, 
raw observation next is [27.5, 77.33333333333334, 1.0, 2.0, 0.4753063719108577, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129560957153, 664156.0206692404, 664156.0206692404, 179818.2843729566], 
processed observation next is [1.0, 0.7391304347826086, 0.5023696682464456, 0.7733333333333334, 1.0, 1.0, 0.36783900230223815, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399431158587, 0.18448778351923345, 0.18448778351923345, 0.26838549906411435], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3109809], dtype=float32), 0.43279275]. 
=============================================
[2019-03-27 04:48:42,932] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[42.573547]
 [42.736248]
 [42.89397 ]
 [42.92503 ]
 [42.61771 ]], R is [[41.99651337]
 [41.57654953]
 [41.16078568]
 [41.23663712]
 [41.29908371]].
[2019-03-27 04:48:51,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4469312e-07 9.9999881e-01 5.6328878e-13 1.0723725e-06 3.4577072e-10], sum to 1.0000
[2019-03-27 04:48:51,653] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5278
[2019-03-27 04:48:51,758] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4029470154690746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594825.9957638703, 594825.9957638709, 173869.0295096684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1425600.0000, 
sim time next is 1426200.0000, 
raw observation next is [24.33333333333333, 87.33333333333334, 1.0, 2.0, 0.4081313560275022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599762.2408565386, 599762.2408565386, 174243.1794307405], 
processed observation next is [0.0, 0.5217391304347826, 0.35229067930489716, 0.8733333333333334, 1.0, 1.0, 0.2869052482259063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16660062246014962, 0.16660062246014962, 0.260064446911553], 
reward next is 0.7399, 
noisyNet noise sample is [array([0.6259249], dtype=float32), 0.102121085]. 
=============================================
[2019-03-27 04:48:55,782] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39794: loss 1.5661
[2019-03-27 04:48:55,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39794: learning rate 0.0000
[2019-03-27 04:48:55,954] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39871: loss 1.3768
[2019-03-27 04:48:55,957] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39873: learning rate 0.0000
[2019-03-27 04:48:56,032] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39907: loss 1.5112
[2019-03-27 04:48:56,036] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39907: learning rate 0.0000
[2019-03-27 04:48:56,051] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39914: loss 1.4007
[2019-03-27 04:48:56,054] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39915: learning rate 0.0000
[2019-03-27 04:48:56,055] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39916: loss 1.4710
[2019-03-27 04:48:56,059] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39917: learning rate 0.0000
[2019-03-27 04:48:56,079] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39926: loss 1.5982
[2019-03-27 04:48:56,082] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39926: learning rate 0.0000
[2019-03-27 04:48:56,091] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39929: loss 1.5288
[2019-03-27 04:48:56,095] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39930: learning rate 0.0000
[2019-03-27 04:48:56,122] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39946: loss 1.4966
[2019-03-27 04:48:56,123] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39946: learning rate 0.0000
[2019-03-27 04:48:56,177] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39970: loss 1.5228
[2019-03-27 04:48:56,181] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39971: loss 1.3373
[2019-03-27 04:48:56,181] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39971: learning rate 0.0000
[2019-03-27 04:48:56,184] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39971: learning rate 0.0000
[2019-03-27 04:48:56,212] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39986: loss 1.4399
[2019-03-27 04:48:56,213] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39987: learning rate 0.0000
[2019-03-27 04:48:56,357] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 40052: loss 1.3748
[2019-03-27 04:48:56,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 40053: learning rate 0.0000
[2019-03-27 04:48:56,396] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40068: loss 1.4950
[2019-03-27 04:48:56,398] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40068: learning rate 0.0000
[2019-03-27 04:48:56,511] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40120: loss 1.3975
[2019-03-27 04:48:56,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40123: learning rate 0.0000
[2019-03-27 04:48:56,702] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 40207: loss 1.3736
[2019-03-27 04:48:56,703] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 40208: learning rate 0.0000
[2019-03-27 04:48:56,749] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 40228: loss 1.3039
[2019-03-27 04:48:56,751] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 40229: learning rate 0.0000
[2019-03-27 04:48:57,018] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.0736663e-09 9.9999964e-01 1.3384518e-14 3.2889420e-07 1.8538376e-11], sum to 1.0000
[2019-03-27 04:48:57,027] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0468
[2019-03-27 04:48:57,031] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 51.00000000000001, 1.0, 2.0, 0.3512963936820009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537565.9046053564, 537565.9046053557, 169504.0934773705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1513200.0000, 
sim time next is 1513800.0000, 
raw observation next is [29.3, 51.0, 1.0, 2.0, 0.3526428651476576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538711.9606284414, 538711.9606284414, 169568.5944270689], 
processed observation next is [0.0, 0.5217391304347826, 0.5876777251184835, 0.51, 1.0, 1.0, 0.22005164475621394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14964221128567817, 0.14964221128567817, 0.25308745436875957], 
reward next is 0.7469, 
noisyNet noise sample is [array([0.09703494], dtype=float32), -0.08207129]. 
=============================================
[2019-03-27 04:49:03,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9677619e-12 1.0000000e+00 7.9575209e-19 8.1054263e-10 5.2243370e-15], sum to 1.0000
[2019-03-27 04:49:03,578] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1774
[2019-03-27 04:49:03,687] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 95.5, 1.0, 2.0, 0.4137812188467885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608971.9607899502, 608971.9607899502, 175132.4409361442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1629000.0000, 
sim time next is 1629600.0000, 
raw observation next is [23.2, 95.66666666666667, 1.0, 2.0, 0.4158999286635896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611613.6734360954, 611613.6734360954, 175368.4103524756], 
processed observation next is [1.0, 0.8695652173913043, 0.29857819905213273, 0.9566666666666667, 1.0, 1.0, 0.2962649742934814, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16989268706558205, 0.16989268706558205, 0.26174389604847104], 
reward next is 0.7383, 
noisyNet noise sample is [array([1.3082982], dtype=float32), 1.4069281]. 
=============================================
[2019-03-27 04:49:06,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1965853e-10 1.0000000e+00 3.4742508e-18 1.5338477e-09 9.7559967e-14], sum to 1.0000
[2019-03-27 04:49:06,168] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3941
[2019-03-27 04:49:06,175] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.05, 96.0, 1.0, 2.0, 0.8629951862666918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1223988.652060425, 1223988.652060425, 262545.9603633804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1672200.0000, 
sim time next is 1672800.0000, 
raw observation next is [24.16666666666666, 95.33333333333334, 1.0, 2.0, 0.8906065988511283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1261283.914661627, 1261283.914661628, 269797.2266853297], 
processed observation next is [1.0, 0.34782608695652173, 0.34439178515007873, 0.9533333333333335, 1.0, 1.0, 0.8682007215073835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35035664296156305, 0.3503566429615633, 0.4026824278885518], 
reward next is 0.5973, 
noisyNet noise sample is [array([-0.25183594], dtype=float32), -0.80358356]. 
=============================================
[2019-03-27 04:49:11,789] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.9110851e-11 1.0000000e+00 6.7582538e-19 1.6457424e-08 1.5159037e-15], sum to 1.0000
[2019-03-27 04:49:11,799] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4258
[2019-03-27 04:49:11,806] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.13333333333334, 86.0, 1.0, 2.0, 0.8258029945387197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1225044.500419657, 1225044.500419656, 260161.9662070169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1765200.0000, 
sim time next is 1765800.0000, 
raw observation next is [24.05, 85.0, 1.0, 2.0, 0.8158929262739981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1219510.515088458, 1219510.515088459, 258696.1748876208], 
processed observation next is [1.0, 0.43478260869565216, 0.3388625592417062, 0.85, 1.0, 1.0, 0.7781842485228893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33875292085790504, 0.33875292085790526, 0.3861136938621206], 
reward next is 0.6139, 
noisyNet noise sample is [array([-0.68449146], dtype=float32), 0.56195396]. 
=============================================
[2019-03-27 04:49:13,466] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47802: loss 0.0149
[2019-03-27 04:49:13,469] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47802: learning rate 0.0000
[2019-03-27 04:49:13,618] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47872: loss 0.0011
[2019-03-27 04:49:13,622] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47872: learning rate 0.0000
[2019-03-27 04:49:13,674] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47893: loss 0.0108
[2019-03-27 04:49:13,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47894: learning rate 0.0000
[2019-03-27 04:49:13,692] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 47899: loss 0.0261
[2019-03-27 04:49:13,694] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 47899: learning rate 0.0000
[2019-03-27 04:49:13,736] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47921: loss 0.0192
[2019-03-27 04:49:13,738] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47921: learning rate 0.0000
[2019-03-27 04:49:13,764] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47934: loss 0.0162
[2019-03-27 04:49:13,769] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47934: learning rate 0.0000
[2019-03-27 04:49:13,814] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47960: loss 0.0034
[2019-03-27 04:49:13,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47961: learning rate 0.0000
[2019-03-27 04:49:13,839] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47969: loss 0.0094
[2019-03-27 04:49:13,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47970: learning rate 0.0000
[2019-03-27 04:49:13,842] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47970: loss 0.0123
[2019-03-27 04:49:13,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47971: learning rate 0.0000
[2019-03-27 04:49:13,852] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47973: loss 0.0030
[2019-03-27 04:49:13,853] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47973: learning rate 0.0000
[2019-03-27 04:49:13,926] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48000: loss 0.0368
[2019-03-27 04:49:13,929] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48000: learning rate 0.0000
[2019-03-27 04:49:14,039] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 48050: loss 0.0397
[2019-03-27 04:49:14,044] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 48053: learning rate 0.0000
[2019-03-27 04:49:14,076] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 48068: loss 0.0053
[2019-03-27 04:49:14,078] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 48068: learning rate 0.0000
[2019-03-27 04:49:14,191] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48121: loss 0.0004
[2019-03-27 04:49:14,194] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48122: learning rate 0.0000
[2019-03-27 04:49:14,318] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 48178: loss 0.0408
[2019-03-27 04:49:14,322] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 48180: learning rate 0.0000
[2019-03-27 04:49:14,406] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 48220: loss 0.0277
[2019-03-27 04:49:14,411] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 48221: learning rate 0.0000
[2019-03-27 04:49:17,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1991159e-15 1.0000000e+00 1.4317997e-23 3.8527124e-12 1.7474287e-19], sum to 1.0000
[2019-03-27 04:49:17,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7206
[2019-03-27 04:49:17,830] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.06666666666667, 83.66666666666667, 1.0, 2.0, 0.3883117877622428, 1.0, 2.0, 0.3883117877622428, 1.0, 1.0, 0.6617812461339883, 6.9112, 6.9112, 170.5573041426782, 1628516.662889319, 1628516.662889319, 344794.0144262397], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1866000.0000, 
sim time next is 1866600.0000, 
raw observation next is [27.05, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.566555968187858, 6.9112, 168.9092991121784, 1918995.396001928, 1454073.383528432, 311348.8217838103], 
processed observation next is [1.0, 0.6086956521739131, 0.4810426540284361, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0655355968187858, 0.0, 0.8294219856525182, 0.5330542766672022, 0.40390927320234227, 0.464699734005687], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.55853534], dtype=float32), -0.5231854]. 
=============================================
[2019-03-27 04:49:18,319] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 04:49:18,323] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:49:18,325] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:49:18,325] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:49:18,326] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:49:18,328] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:49:18,329] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:49:18,330] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:49:18,332] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:49:18,332] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:49:18,333] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:49:18,348] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run3
[2019-03-27 04:49:18,351] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run3
[2019-03-27 04:49:18,391] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run3
[2019-03-27 04:49:18,391] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run3
[2019-03-27 04:49:18,411] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run3
[2019-03-27 04:49:19,952] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033533484]
[2019-03-27 04:49:19,955] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.23333333333333, 67.0, 1.0, 2.0, 0.9721089334689055, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564484743, 1434747.928896608, 1434747.928896609, 301983.3517653185]
[2019-03-27 04:49:19,956] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:49:19,959] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.03080710e-11 1.00000000e+00 1.18307383e-18 2.99003933e-09
 1.07771695e-14], sampled 0.6806905456639497
[2019-03-27 04:49:27,573] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033533484]
[2019-03-27 04:49:27,574] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.83333333333333, 66.5, 1.0, 2.0, 0.2553916522124396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417912.0570407765, 417912.0570407765, 161435.166928166]
[2019-03-27 04:49:27,575] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:49:27,579] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0629697e-10 1.0000000e+00 9.3122372e-18 9.1938928e-09 6.4896046e-14], sampled 0.9296094460972685
[2019-03-27 04:49:30,274] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033533484]
[2019-03-27 04:49:30,275] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.11369694, 56.47606043333333, 1.0, 2.0, 0.2501755903002061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 415229.529781752, 415229.529781752, 160690.202859732]
[2019-03-27 04:49:30,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:49:30,280] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0146355e-09 1.0000000e+00 1.4153775e-16 3.2958273e-08 5.6075961e-13], sampled 0.9530002965171707
[2019-03-27 04:50:42,672] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033533484]
[2019-03-27 04:50:42,672] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.02595811, 51.04033274, 1.0, 2.0, 0.5643527850955803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788628.7156663778, 788628.7156663778, 194293.0448680572]
[2019-03-27 04:50:42,675] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:50:42,678] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.2444855e-10 1.0000000e+00 1.9132413e-17 1.3158420e-08 1.1465090e-13], sampled 0.5210933601268284
[2019-03-27 04:50:49,591] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033533484]
[2019-03-27 04:50:49,591] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.4575, 68.40193024, 1.0, 2.0, 0.5671602422680794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792553.3305829421, 792553.3305829421, 194787.9232045138]
[2019-03-27 04:50:49,592] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:50:49,594] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0692631e-10 1.0000000e+00 9.5262718e-18 9.2683701e-09 6.6680406e-14], sampled 0.0817207908952533
[2019-03-27 04:50:51,771] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033533484]
[2019-03-27 04:50:51,771] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.42245931, 83.75996932666666, 1.0, 2.0, 0.809983616618513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1132057.11139725, 1132057.111397249, 246383.2690113441]
[2019-03-27 04:50:51,772] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:50:51,774] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.4748644e-11 1.0000000e+00 1.4163861e-18 3.6300234e-09 1.3430657e-14], sampled 0.7889578048862919
[2019-03-27 04:51:00,759] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033533484]
[2019-03-27 04:51:00,761] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.73333333333333, 86.0, 1.0, 2.0, 0.4736779639699703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661879.9016697747, 661879.9016697753, 179574.5086698887]
[2019-03-27 04:51:00,762] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:51:00,765] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2085823e-10 1.0000000e+00 3.3175191e-18 5.5445604e-09 2.9904481e-14], sampled 0.18274547292473464
[2019-03-27 04:51:11,383] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 04:51:11,463] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1701 3164000012.1395 1778.0000
[2019-03-27 04:51:11,923] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5735 3007679959.6581 1766.0000
[2019-03-27 04:51:11,988] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:51:11,989] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7625 2842566295.3938 1131.0000
[2019-03-27 04:51:13,007] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 50000, evaluation results [50000.0, 7884.170123390672, 3164000012.13951, 1778.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7997.573485503451, 3007679959.6581187, 1766.0, 8496.762508909627, 2842566295.393817, 1131.0]
[2019-03-27 04:51:22,134] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2259307e-09 9.9999619e-01 3.2061812e-16 3.8527646e-06 1.2126087e-11], sum to 1.0000
[2019-03-27 04:51:22,142] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9078
[2019-03-27 04:51:22,265] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 91.33333333333334, 1.0, 2.0, 0.5071619266429156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708683.3340249904, 708683.3340249899, 184726.0234999185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2029200.0000, 
sim time next is 2029800.0000, 
raw observation next is [26.08333333333333, 90.66666666666666, 1.0, 2.0, 0.5076028214960224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709299.624672002, 709299.624672002, 184796.0986747059], 
processed observation next is [0.0, 0.4782608695652174, 0.43522906793048954, 0.9066666666666666, 1.0, 1.0, 0.4067503873446053, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19702767352000056, 0.19702767352000056, 0.2758150726488148], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.21494094], dtype=float32), -0.5208052]. 
=============================================
[2019-03-27 04:51:24,085] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2150031e-10 1.0000000e+00 2.8031960e-18 3.0181613e-09 1.2828095e-14], sum to 1.0000
[2019-03-27 04:51:24,096] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7058
[2019-03-27 04:51:24,102] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 90.33333333333334, 1.0, 2.0, 0.4763995910352775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665684.0783574794, 665684.0783574794, 179980.8065091273], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2061600.0000, 
sim time next is 2062200.0000, 
raw observation next is [25.15, 90.66666666666667, 1.0, 2.0, 0.4761736845432868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665368.315175805, 665368.3151758057, 179947.0001062051], 
processed observation next is [0.0, 0.8695652173913043, 0.3909952606635071, 0.9066666666666667, 1.0, 1.0, 0.3688839572810684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18482453199327917, 0.18482453199327936, 0.2685776120988136], 
reward next is 0.7314, 
noisyNet noise sample is [array([-0.11927301], dtype=float32), 0.19709969]. 
=============================================
[2019-03-27 04:51:25,335] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55787: loss 0.4868
[2019-03-27 04:51:25,337] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55787: learning rate 0.0000
[2019-03-27 04:51:25,498] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55867: loss 0.5524
[2019-03-27 04:51:25,500] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55868: learning rate 0.0000
[2019-03-27 04:51:25,522] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55878: loss 0.5161
[2019-03-27 04:51:25,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55879: learning rate 0.0000
[2019-03-27 04:51:25,568] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55899: loss 0.6474
[2019-03-27 04:51:25,570] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55899: learning rate 0.0000
[2019-03-27 04:51:25,631] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55926: loss 0.4692
[2019-03-27 04:51:25,633] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55926: loss 0.5462
[2019-03-27 04:51:25,635] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55927: loss 0.5580
[2019-03-27 04:51:25,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55927: learning rate 0.0000
[2019-03-27 04:51:25,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55927: learning rate 0.0000
[2019-03-27 04:51:25,639] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55927: learning rate 0.0000
[2019-03-27 04:51:25,686] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55954: loss 0.5015
[2019-03-27 04:51:25,690] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55954: learning rate 0.0000
[2019-03-27 04:51:25,752] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55982: loss 0.5059
[2019-03-27 04:51:25,754] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55982: learning rate 0.0000
[2019-03-27 04:51:25,763] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55987: loss 0.3981
[2019-03-27 04:51:25,766] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55988: learning rate 0.0000
[2019-03-27 04:51:25,798] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56004: loss 0.5083
[2019-03-27 04:51:25,800] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56005: learning rate 0.0000
[2019-03-27 04:51:25,859] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56035: loss 0.4611
[2019-03-27 04:51:25,862] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56035: learning rate 0.0000
[2019-03-27 04:51:25,890] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 56047: loss 0.4939
[2019-03-27 04:51:25,895] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 56048: learning rate 0.0000
[2019-03-27 04:51:26,117] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56153: loss 0.4980
[2019-03-27 04:51:26,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56155: learning rate 0.0000
[2019-03-27 04:51:26,224] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 56203: loss 0.4242
[2019-03-27 04:51:26,227] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 56204: learning rate 0.0000
[2019-03-27 04:51:26,249] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 56213: loss 0.4580
[2019-03-27 04:51:26,253] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 56213: learning rate 0.0000
[2019-03-27 04:51:26,842] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0674833e-12 1.0000000e+00 3.6046099e-18 5.0082116e-10 3.1489243e-14], sum to 1.0000
[2019-03-27 04:51:26,850] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7406
[2019-03-27 04:51:26,967] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 74.66666666666667, 1.0, 2.0, 0.5532418400802809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773096.5887330913, 773096.5887330906, 192359.046273165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2115600.0000, 
sim time next is 2116200.0000, 
raw observation next is [30.0, 74.83333333333333, 1.0, 2.0, 0.5548215783842794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775304.9105216585, 775304.9105216579, 192631.8159201287], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.7483333333333333, 1.0, 1.0, 0.4636404558846739, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21536247514490514, 0.21536247514490497, 0.28751017301511744], 
reward next is 0.7125, 
noisyNet noise sample is [array([-0.10906427], dtype=float32), 1.4767928]. 
=============================================
[2019-03-27 04:51:28,259] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1326939e-10 1.0000000e+00 1.3762685e-19 3.7605354e-09 5.5762758e-15], sum to 1.0000
[2019-03-27 04:51:28,268] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5914
[2019-03-27 04:51:28,272] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 73.5, 1.0, 2.0, 0.5695801170196924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795936.1459319879, 795936.1459319879, 195216.2238433336], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2133000.0000, 
sim time next is 2133600.0000, 
raw observation next is [30.7, 73.33333333333333, 1.0, 2.0, 0.570174314702858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796766.7943068421, 796766.7943068421, 195321.6356351433], 
processed observation next is [0.0, 0.6956521739130435, 0.6540284360189573, 0.7333333333333333, 1.0, 1.0, 0.4821377285576602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22132410952967838, 0.22132410952967838, 0.291524829306184], 
reward next is 0.7085, 
noisyNet noise sample is [array([0.61947066], dtype=float32), -0.0482572]. 
=============================================
[2019-03-27 04:51:29,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8634658e-12 1.0000000e+00 1.0950182e-22 1.0837878e-10 4.0419027e-18], sum to 1.0000
[2019-03-27 04:51:29,781] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5820
[2019-03-27 04:51:29,785] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 93.5, 1.0, 2.0, 0.512143694793553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715646.9587259385, 715646.958725939, 185520.5706594243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2161800.0000, 
sim time next is 2162400.0000, 
raw observation next is [25.66666666666667, 93.66666666666667, 1.0, 2.0, 0.511557470526853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714827.5192284478, 714827.5192284472, 185426.6728663435], 
processed observation next is [1.0, 0.0, 0.4154818325434442, 0.9366666666666668, 1.0, 1.0, 0.41151502473114815, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19856319978567993, 0.1985631997856798, 0.27675622815872164], 
reward next is 0.7232, 
noisyNet noise sample is [array([0.23464254], dtype=float32), 1.7893003]. 
=============================================
[2019-03-27 04:51:33,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9936247e-12 1.0000000e+00 1.0292833e-21 2.4358763e-11 3.8707113e-17], sum to 1.0000
[2019-03-27 04:51:33,921] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3740
[2019-03-27 04:51:34,042] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 86.0, 1.0, 2.0, 0.913705205082607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1277108.660853028, 1277108.660853029, 273733.0476552404], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2254200.0000, 
sim time next is 2254800.0000, 
raw observation next is [26.4, 86.0, 1.0, 2.0, 0.8730304101673947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1220223.810903348, 1220223.810903349, 262624.1888260762], 
processed observation next is [1.0, 0.08695652173913043, 0.45023696682464454, 0.86, 1.0, 1.0, 0.8470245905631261, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33895105858426333, 0.3389510585842636, 0.3919764012329496], 
reward next is 0.6080, 
noisyNet noise sample is [array([0.53634346], dtype=float32), -1.2901847]. 
=============================================
[2019-03-27 04:51:39,233] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3560774e-12 1.0000000e+00 5.5958203e-21 1.9126864e-10 2.1892464e-16], sum to 1.0000
[2019-03-27 04:51:39,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1251
[2019-03-27 04:51:39,253] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.05, 81.0, 1.0, 2.0, 0.5323853369376412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 743941.6393067535, 743941.6393067541, 188825.6811382481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2337000.0000, 
sim time next is 2337600.0000, 
raw observation next is [28.0, 81.0, 1.0, 2.0, 0.5300839181191508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740724.5742801601, 740724.5742801601, 188443.8136541813], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.81, 1.0, 1.0, 0.4338360459266877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20575682618893337, 0.20575682618893337, 0.2812594233644497], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.0206307], dtype=float32), 0.5799707]. 
=============================================
[2019-03-27 04:51:40,808] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.12865866e-13 1.00000000e+00 9.54987126e-21 3.44643925e-10
 2.54051849e-17], sum to 1.0000
[2019-03-27 04:51:40,820] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7558
[2019-03-27 04:51:40,827] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2138426.511711185 W.
[2019-03-27 04:51:40,831] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.3, 66.0, 1.0, 2.0, 0.5097760701938502, 1.0, 2.0, 0.5097760701938502, 1.0, 1.0, 0.8853124484041668, 6.9112, 6.9112, 170.5573041426782, 2138426.511711185, 2138426.511711185, 421971.7597567013], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2368800.0000, 
sim time next is 2369400.0000, 
raw observation next is [31.45, 65.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.103127262108151, 6.9112, 168.9119479576894, 2424901.698972608, 2288742.706595338, 475912.4674810026], 
processed observation next is [1.0, 0.43478260869565216, 0.6895734597156398, 0.6566666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.01919272621081509, 0.0, 0.8294349926966239, 0.6735838052701689, 0.6357618629431494, 0.7103171156432875], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.18214536], dtype=float32), -0.35993832]. 
=============================================
[2019-03-27 04:51:42,589] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63848: loss 1.1366
[2019-03-27 04:51:42,593] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63849: learning rate 0.0000
[2019-03-27 04:51:42,604] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 63855: loss 1.7118
[2019-03-27 04:51:42,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 63855: learning rate 0.0000
[2019-03-27 04:51:42,622] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63861: loss 0.2781
[2019-03-27 04:51:42,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63861: learning rate 0.0000
[2019-03-27 04:51:42,635] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63865: loss 0.3409
[2019-03-27 04:51:42,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63867: learning rate 0.0000
[2019-03-27 04:51:42,688] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63888: loss 3.2423
[2019-03-27 04:51:42,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63888: learning rate 0.0000
[2019-03-27 04:51:42,733] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63911: loss 0.4460
[2019-03-27 04:51:42,737] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63911: learning rate 0.0000
[2019-03-27 04:51:42,805] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63942: loss 0.5762
[2019-03-27 04:51:42,808] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63942: learning rate 0.0000
[2019-03-27 04:51:42,909] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63990: loss 1.2564
[2019-03-27 04:51:42,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63990: learning rate 0.0000
[2019-03-27 04:51:42,948] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64008: loss 0.1467
[2019-03-27 04:51:42,954] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64011: learning rate 0.0000
[2019-03-27 04:51:42,954] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64011: loss 0.1301
[2019-03-27 04:51:42,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64011: learning rate 0.0000
[2019-03-27 04:51:43,032] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64045: loss 0.9347
[2019-03-27 04:51:43,033] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64045: learning rate 0.0000
[2019-03-27 04:51:43,084] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 64072: loss 0.8371
[2019-03-27 04:51:43,089] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 64072: learning rate 0.0000
[2019-03-27 04:51:43,154] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 64101: loss 0.3505
[2019-03-27 04:51:43,158] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 64102: learning rate 0.0000
[2019-03-27 04:51:43,193] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 64116: loss 0.9785
[2019-03-27 04:51:43,198] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 64117: learning rate 0.0000
[2019-03-27 04:51:43,304] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64170: loss 0.7336
[2019-03-27 04:51:43,306] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64171: learning rate 0.0000
[2019-03-27 04:51:43,344] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64185: loss 0.3210
[2019-03-27 04:51:43,348] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64186: learning rate 0.0000
[2019-03-27 04:51:48,354] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4495700e-12 1.0000000e+00 1.4876065e-20 2.4181296e-10 7.2873760e-18], sum to 1.0000
[2019-03-27 04:51:48,365] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2974
[2019-03-27 04:51:48,370] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 93.0, 1.0, 2.0, 0.5557684444988097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776628.5402283682, 776628.5402283682, 192794.9312219789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2493000.0000, 
sim time next is 2493600.0000, 
raw observation next is [26.93333333333333, 93.0, 1.0, 2.0, 0.5542783131522021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774545.4776053568, 774545.4776053568, 192537.2200161644], 
processed observation next is [1.0, 0.8695652173913043, 0.4755134281200631, 0.93, 1.0, 1.0, 0.46298591946048445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21515152155704356, 0.21515152155704356, 0.28736898509875286], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.76243246], dtype=float32), 0.29493314]. 
=============================================
[2019-03-27 04:51:49,931] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.10206028e-11 1.00000000e+00 7.52435021e-19 2.25473893e-11
 1.11736995e-14], sum to 1.0000
[2019-03-27 04:51:49,941] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1852
[2019-03-27 04:51:49,948] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.21666666666667, 96.83333333333334, 1.0, 2.0, 0.6362318134670158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889114.7828770584, 889114.7828770584, 207684.0529895017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2523000.0000, 
sim time next is 2523600.0000, 
raw observation next is [26.2, 97.0, 1.0, 2.0, 0.6396759181259455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893929.8408518926, 893929.8408518926, 208364.1872846041], 
processed observation next is [1.0, 0.21739130434782608, 0.44075829383886256, 0.97, 1.0, 1.0, 0.5658746001517415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24831384468108128, 0.24831384468108128, 0.31099132430537924], 
reward next is 0.6890, 
noisyNet noise sample is [array([1.392326], dtype=float32), 2.4759662]. 
=============================================
[2019-03-27 04:51:57,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6494651e-11 1.0000000e+00 1.5439134e-19 3.3637526e-10 1.6107683e-15], sum to 1.0000
[2019-03-27 04:51:57,968] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5785
[2019-03-27 04:51:57,972] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 89.0, 1.0, 2.0, 0.4262090879940847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622792.4104582334, 622792.4104582329, 176329.4834988362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2659800.0000, 
sim time next is 2660400.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4196727544789652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617079.4322294805, 617079.4322294812, 175885.7840774528], 
processed observation next is [0.0, 0.8260869565217391, 0.3364928909952607, 0.89, 1.0, 1.0, 0.3008105475650183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1714109533970779, 0.1714109533970781, 0.2625160956379892], 
reward next is 0.7375, 
noisyNet noise sample is [array([-0.40629193], dtype=float32), -0.07994288]. 
=============================================
[2019-03-27 04:52:00,106] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71791: loss 0.4830
[2019-03-27 04:52:00,108] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71791: learning rate 0.0000
[2019-03-27 04:52:00,191] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71827: loss 0.6256
[2019-03-27 04:52:00,194] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71828: learning rate 0.0000
[2019-03-27 04:52:00,237] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71850: loss 0.5166
[2019-03-27 04:52:00,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71850: learning rate 0.0000
[2019-03-27 04:52:00,248] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71855: loss 0.6376
[2019-03-27 04:52:00,249] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71855: learning rate 0.0000
[2019-03-27 04:52:00,308] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71879: loss 0.6352
[2019-03-27 04:52:00,309] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71879: loss 0.5533
[2019-03-27 04:52:00,310] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71879: learning rate 0.0000
[2019-03-27 04:52:00,311] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71879: learning rate 0.0000
[2019-03-27 04:52:00,515] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71973: loss 0.4888
[2019-03-27 04:52:00,518] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71973: learning rate 0.0000
[2019-03-27 04:52:00,544] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71985: loss 0.5602
[2019-03-27 04:52:00,547] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71985: learning rate 0.0000
[2019-03-27 04:52:00,581] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 72002: loss 0.4338
[2019-03-27 04:52:00,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 72002: learning rate 0.0000
[2019-03-27 04:52:00,638] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 72029: loss 0.5995
[2019-03-27 04:52:00,644] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 72031: learning rate 0.0000
[2019-03-27 04:52:00,647] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72033: loss 0.5640
[2019-03-27 04:52:00,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72033: learning rate 0.0000
[2019-03-27 04:52:00,663] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72039: loss 0.5376
[2019-03-27 04:52:00,665] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72040: learning rate 0.0000
[2019-03-27 04:52:00,791] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 72099: loss 0.7168
[2019-03-27 04:52:00,792] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 72099: learning rate 0.0000
[2019-03-27 04:52:00,858] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72126: loss 0.6384
[2019-03-27 04:52:00,861] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72127: learning rate 0.0000
[2019-03-27 04:52:00,949] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 72168: loss 0.6386
[2019-03-27 04:52:00,954] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 72169: learning rate 0.0000
[2019-03-27 04:52:01,097] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 72230: loss 0.5495
[2019-03-27 04:52:01,103] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 72230: learning rate 0.0000
[2019-03-27 04:52:03,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5268465e-11 1.0000000e+00 1.0707484e-18 2.0071896e-11 1.4802062e-15], sum to 1.0000
[2019-03-27 04:52:03,153] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2514
[2019-03-27 04:52:03,157] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3932790645978489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586831.5939398808, 586831.5939398803, 173327.68430533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2737200.0000, 
sim time next is 2737800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3935176726354349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 587187.849900421, 587187.8499004217, 173360.2207104361], 
processed observation next is [0.0, 0.6956521739130435, 0.28909952606635075, 0.94, 1.0, 1.0, 0.26929840076558426, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16310773608345028, 0.16310773608345047, 0.2587465980752777], 
reward next is 0.7413, 
noisyNet noise sample is [array([1.1493565], dtype=float32), -0.28702077]. 
=============================================
[2019-03-27 04:52:03,259] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2450278e-11 1.0000000e+00 3.6670157e-21 2.4680613e-11 4.4794649e-16], sum to 1.0000
[2019-03-27 04:52:03,270] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0423
[2019-03-27 04:52:03,396] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 97.0, 1.0, 2.0, 0.3879939931931745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 581264.5881379509, 581264.5881379503, 172894.327902857], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2748600.0000, 
sim time next is 2749200.0000, 
raw observation next is [22.33333333333334, 98.0, 1.0, 2.0, 0.385555475527079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578518.6681064686, 578518.6681064686, 172675.5154239802], 
processed observation next is [0.0, 0.8260869565217391, 0.2575039494470777, 0.98, 1.0, 1.0, 0.25970539220129996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16069963002957463, 0.16069963002957463, 0.2577246498865376], 
reward next is 0.7423, 
noisyNet noise sample is [array([0.6505127], dtype=float32), 0.9357415]. 
=============================================
[2019-03-27 04:52:05,609] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.8157345e-11 1.0000000e+00 8.4421067e-23 1.6663370e-11 1.9913631e-16], sum to 1.0000
[2019-03-27 04:52:05,622] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9315
[2019-03-27 04:52:05,630] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3484596300850965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536791.4933470496, 536791.4933470496, 169553.3087885754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2775600.0000, 
sim time next is 2776200.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.3493758065039241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538680.3763129507, 538680.37631295, 169722.2388710988], 
processed observation next is [1.0, 0.13043478260869565, 0.23380726698262277, 0.95, 1.0, 1.0, 0.21611542952280008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14963343786470854, 0.14963343786470834, 0.2533167744344758], 
reward next is 0.7467, 
noisyNet noise sample is [array([0.35519227], dtype=float32), 0.15767957]. 
=============================================
[2019-03-27 04:52:07,276] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 04:52:07,277] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:52:07,278] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:52:07,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:52:07,279] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:52:07,279] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:52:07,280] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:52:07,281] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:52:07,281] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:52:07,282] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:52:07,282] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:52:07,299] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run4
[2019-03-27 04:52:07,330] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run4
[2019-03-27 04:52:07,330] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run4
[2019-03-27 04:52:07,331] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run4
[2019-03-27 04:52:07,381] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run4
[2019-03-27 04:52:22,322] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036933232]
[2019-03-27 04:52:22,324] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.12734893166667, 92.58235625833333, 1.0, 2.0, 0.3122207346714616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494264.259142336, 494264.259142336, 166609.2872435643]
[2019-03-27 04:52:22,325] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:52:22,326] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2928529e-11 1.0000000e+00 2.1080345e-19 1.0236935e-10 1.9597357e-15], sampled 0.35849371709132727
[2019-03-27 04:52:26,461] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036933232]
[2019-03-27 04:52:26,463] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.9, 41.0, 1.0, 2.0, 0.3184922318508753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 525636.508197284, 525636.5081972833, 168443.4953522156]
[2019-03-27 04:52:26,463] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:52:26,467] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0559911e-11 1.0000000e+00 1.1730473e-19 8.4884003e-11 1.3216048e-15], sampled 0.9654383308333462
[2019-03-27 04:52:28,932] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036933232]
[2019-03-27 04:52:28,933] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.47206444333333, 97.46722134000001, 1.0, 2.0, 0.4062946784704189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 591817.0572441474, 591817.0572441468, 173333.5132766709]
[2019-03-27 04:52:28,935] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:52:28,937] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4061479e-11 1.0000000e+00 5.7801556e-19 1.7222362e-10 4.4394564e-15], sampled 0.17405377498533536
[2019-03-27 04:53:50,114] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036933232]
[2019-03-27 04:53:50,115] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.93333333333333, 89.66666666666667, 1.0, 2.0, 0.7731886028394834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1080605.142696126, 1080605.142696126, 237438.9165761317]
[2019-03-27 04:53:50,117] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:53:50,119] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1380915e-13 1.0000000e+00 3.7212248e-22 3.3085939e-12 1.2376257e-17], sampled 0.5574985752199948
[2019-03-27 04:54:01,271] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:54:01,408] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:54:01,431] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6842 2927263131.0635 1338.0000
[2019-03-27 04:54:01,595] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2975 3007593468.2009 1766.0000
[2019-03-27 04:54:01,710] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:54:02,725] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 75000, evaluation results [75000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8253.684238692544, 2927263131.0635424, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.297545845454, 3007593468.2008953, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:54:06,426] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2096950e-13 1.0000000e+00 6.1246753e-20 1.7565409e-12 2.8772711e-16], sum to 1.0000
[2019-03-27 04:54:06,437] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1112
[2019-03-27 04:54:06,443] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.5490838375494109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845737.4036167788, 845737.4036167788, 201057.6340968163], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2881800.0000, 
sim time next is 2882400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.5848522530852681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 900782.2706751936, 900782.2706751942, 208039.4108585454], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4998219916689977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.250217297409776, 0.25021729740977616, 0.31050658337096326], 
reward next is 0.6895, 
noisyNet noise sample is [array([0.4190491], dtype=float32), -0.67518616]. 
=============================================
[2019-03-27 04:54:12,729] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 79769: loss 0.1180
[2019-03-27 04:54:12,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 79769: learning rate 0.0000
[2019-03-27 04:54:12,874] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79795: loss 0.0315
[2019-03-27 04:54:12,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79795: learning rate 0.0000
[2019-03-27 04:54:13,093] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 79867: loss 0.0292
[2019-03-27 04:54:13,097] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 79869: learning rate 0.0000
[2019-03-27 04:54:13,212] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79889: loss 0.0422
[2019-03-27 04:54:13,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79889: learning rate 0.0000
[2019-03-27 04:54:13,320] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79907: loss 0.0784
[2019-03-27 04:54:13,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79907: learning rate 0.0000
[2019-03-27 04:54:13,450] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79933: loss 0.0014
[2019-03-27 04:54:13,455] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79933: learning rate 0.0000
[2019-03-27 04:54:13,594] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79972: loss 0.0363
[2019-03-27 04:54:13,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79973: learning rate 0.0000
[2019-03-27 04:54:13,708] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79990: loss 0.0280
[2019-03-27 04:54:13,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79990: learning rate 0.0000
[2019-03-27 04:54:13,815] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 80000: loss 0.0170
[2019-03-27 04:54:13,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 80002: learning rate 0.0000
[2019-03-27 04:54:13,959] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80042: loss 0.0391
[2019-03-27 04:54:13,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80044: learning rate 0.0000
[2019-03-27 04:54:14,061] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80054: loss 0.0409
[2019-03-27 04:54:14,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80054: learning rate 0.0000
[2019-03-27 04:54:14,066] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 80054: loss 0.0040
[2019-03-27 04:54:14,149] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 80056: learning rate 0.0000
[2019-03-27 04:54:14,267] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 80079: loss 0.0261
[2019-03-27 04:54:14,270] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 80079: learning rate 0.0000
[2019-03-27 04:54:14,374] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80093: loss 0.0114
[2019-03-27 04:54:14,376] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80093: learning rate 0.0000
[2019-03-27 04:54:14,533] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 80143: loss 0.0158
[2019-03-27 04:54:14,536] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 80143: learning rate 0.0000
[2019-03-27 04:54:14,658] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 80162: loss 0.0174
[2019-03-27 04:54:14,661] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 80162: learning rate 0.0000
[2019-03-27 04:54:22,169] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.0671802e-12 1.0000000e+00 2.9796383e-20 4.2749006e-11 6.6161519e-17], sum to 1.0000
[2019-03-27 04:54:22,175] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4567
[2019-03-27 04:54:22,180] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 96.0, 1.0, 2.0, 0.3409954632436853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 526919.4917858489, 526919.4917858489, 168803.4877679479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3130800.0000, 
sim time next is 3131400.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.342347309670555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 528301.0408940149, 528301.0408940143, 168892.7728461347], 
processed observation next is [1.0, 0.21739130434782608, 0.23380726698262277, 0.95, 1.0, 1.0, 0.20764736104886142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14675028913722635, 0.14675028913722618, 0.2520787654419921], 
reward next is 0.7479, 
noisyNet noise sample is [array([1.6390002], dtype=float32), 2.0349948]. 
=============================================
[2019-03-27 04:54:23,204] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8024015e-13 1.0000000e+00 1.6699335e-21 6.8868821e-12 4.7699411e-19], sum to 1.0000
[2019-03-27 04:54:23,209] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9771
[2019-03-27 04:54:23,213] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 82.33333333333334, 1.0, 2.0, 0.5879840693374689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831913.991455402, 831913.991455402, 199849.2379311827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3156000.0000, 
sim time next is 3156600.0000, 
raw observation next is [26.0, 83.16666666666666, 1.0, 2.0, 0.5879904127852683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 827587.1979108624, 827587.197910863, 199290.3774250898], 
processed observation next is [1.0, 0.5217391304347826, 0.4312796208530806, 0.8316666666666666, 1.0, 1.0, 0.5036029069702027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22988533275301734, 0.22988533275301748, 0.2974483245150594], 
reward next is 0.7026, 
noisyNet noise sample is [array([0.8174068], dtype=float32), 0.38895464]. 
=============================================
[2019-03-27 04:54:26,379] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.23075747e-12 1.00000000e+00 3.66019951e-19 1.33254466e-11
 1.23594796e-14], sum to 1.0000
[2019-03-27 04:54:26,388] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5743
[2019-03-27 04:54:26,394] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4559155698054902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645525.3728558918, 645525.3728558918, 178071.0172771911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3218400.0000, 
sim time next is 3219000.0000, 
raw observation next is [25.16666666666667, 88.16666666666667, 1.0, 2.0, 0.4575084408629382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646776.5353416, 646776.5353416, 178174.6390386999], 
processed observation next is [0.0, 0.2608695652173913, 0.39178515007898923, 0.8816666666666667, 1.0, 1.0, 0.346395711883058, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.179660148706, 0.179660148706, 0.26593229707268645], 
reward next is 0.7341, 
noisyNet noise sample is [array([0.00120365], dtype=float32), 1.3350468]. 
=============================================
[2019-03-27 04:54:26,413] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[63.63717 ]
 [63.618763]
 [63.609676]
 [63.592007]
 [63.573597]], R is [[63.76322174]
 [63.85981369]
 [63.95534897]
 [64.04985809]
 [64.14341736]].
[2019-03-27 04:54:29,909] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.2478983e-14 1.0000000e+00 9.1648339e-23 4.1608995e-12 5.4689310e-19], sum to 1.0000
[2019-03-27 04:54:29,911] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8219
[2019-03-27 04:54:29,916] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5199735503351353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726591.7946593971, 726591.7946593978, 186784.5782920096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3273600.0000, 
sim time next is 3274200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5187093207022149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724824.6042412353, 724824.604241236, 186579.3330116044], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4201317116894155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2013401678447876, 0.20134016784478778, 0.2784766164352305], 
reward next is 0.7215, 
noisyNet noise sample is [array([0.3082238], dtype=float32), 1.0877492]. 
=============================================
[2019-03-27 04:54:30,723] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87771: loss 0.1735
[2019-03-27 04:54:30,724] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87772: loss 0.0772
[2019-03-27 04:54:30,725] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87772: learning rate 0.0000
[2019-03-27 04:54:30,727] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87772: learning rate 0.0000
[2019-03-27 04:54:30,762] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87787: loss 0.3095
[2019-03-27 04:54:30,767] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87790: learning rate 0.0000
[2019-03-27 04:54:30,773] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87793: loss 0.2419
[2019-03-27 04:54:30,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87793: learning rate 0.0000
[2019-03-27 04:54:31,113] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87947: loss 0.1616
[2019-03-27 04:54:31,116] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87948: learning rate 0.0000
[2019-03-27 04:54:31,151] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87964: loss 0.1019
[2019-03-27 04:54:31,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87964: learning rate 0.0000
[2019-03-27 04:54:31,166] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87968: loss 0.1047
[2019-03-27 04:54:31,168] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87969: learning rate 0.0000
[2019-03-27 04:54:31,237] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88000: loss 0.1955
[2019-03-27 04:54:31,238] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88000: learning rate 0.0000
[2019-03-27 04:54:31,245] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 88002: loss 0.1038
[2019-03-27 04:54:31,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 88005: learning rate 0.0000
[2019-03-27 04:54:31,360] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 88055: loss 0.1881
[2019-03-27 04:54:31,366] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 88056: learning rate 0.0000
[2019-03-27 04:54:31,366] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88056: loss 0.1880
[2019-03-27 04:54:31,372] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88057: learning rate 0.0000
[2019-03-27 04:54:31,402] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88075: loss 0.2515
[2019-03-27 04:54:31,403] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88075: learning rate 0.0000
[2019-03-27 04:54:31,418] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88082: loss 0.1059
[2019-03-27 04:54:31,423] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88082: learning rate 0.0000
[2019-03-27 04:54:31,517] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 88128: loss 0.1735
[2019-03-27 04:54:31,519] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 88128: learning rate 0.0000
[2019-03-27 04:54:31,641] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 88181: loss 0.2231
[2019-03-27 04:54:31,646] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 88182: learning rate 0.0000
[2019-03-27 04:54:31,668] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 88194: loss 0.1757
[2019-03-27 04:54:31,671] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 88194: learning rate 0.0000
[2019-03-27 04:54:42,894] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9442724e-13 1.0000000e+00 4.2155099e-22 1.0790741e-11 1.0889185e-17], sum to 1.0000
[2019-03-27 04:54:42,902] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2608
[2019-03-27 04:54:42,910] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2379645.876338051 W.
[2019-03-27 04:54:42,914] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.030118558449625, 6.9112, 168.912032629461, 2379645.876338051, 2295281.419552667, 476384.1586002428], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3495600.0000, 
sim time next is 3496200.0000, 
raw observation next is [31.16666666666667, 66.16666666666667, 1.0, 2.0, 1.008508891165214, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.992940529283848, 6.9112, 168.9123738516414, 2306911.827170257, 2248922.482437114, 466428.2241338421], 
processed observation next is [1.0, 0.4782608695652174, 0.6761453396524489, 0.6616666666666667, 1.0, 1.0, 1.0102516761026674, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008174052928384778, 0.0, 0.8294370840309034, 0.640808840880627, 0.6247006895658651, 0.6961615285579733], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.18765332], dtype=float32), -0.7101414]. 
=============================================
[2019-03-27 04:54:44,624] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7118130e-12 1.0000000e+00 1.7741199e-20 7.4862417e-12 1.2513275e-15], sum to 1.0000
[2019-03-27 04:54:44,633] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7520
[2019-03-27 04:54:44,637] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.5547762969872104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775241.6114318797, 775241.611431879, 192623.499144762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3526800.0000, 
sim time next is 3527400.0000, 
raw observation next is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 0.5545144262252285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774875.5410675411, 774875.5410675411, 192578.0990301134], 
processed observation next is [1.0, 0.8260869565217391, 0.581358609794629, 0.7833333333333334, 1.0, 1.0, 0.46327039304244394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21524320585209475, 0.21524320585209475, 0.2874299985524081], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.56697696], dtype=float32), 0.4036942]. 
=============================================
[2019-03-27 04:54:48,475] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95797: loss -125.5547
[2019-03-27 04:54:48,477] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95798: learning rate 0.0000
[2019-03-27 04:54:48,510] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95814: loss -187.4767
[2019-03-27 04:54:48,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95814: learning rate 0.0000
[2019-03-27 04:54:48,512] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95814: loss -160.8751
[2019-03-27 04:54:48,515] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95814: learning rate 0.0000
[2019-03-27 04:54:48,549] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95832: loss -166.4331
[2019-03-27 04:54:48,550] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95832: learning rate 0.0000
[2019-03-27 04:54:48,767] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95926: loss -242.3345
[2019-03-27 04:54:48,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95926: learning rate 0.0000
[2019-03-27 04:54:48,837] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95960: loss -140.9111
[2019-03-27 04:54:48,838] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95960: learning rate 0.0000
[2019-03-27 04:54:48,920] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95995: loss -57.5822
[2019-03-27 04:54:48,922] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95995: learning rate 0.0000
[2019-03-27 04:54:48,945] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96006: loss -99.6961
[2019-03-27 04:54:48,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96006: learning rate 0.0000
[2019-03-27 04:54:48,985] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 96025: loss 53.7075
[2019-03-27 04:54:48,988] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 96026: learning rate 0.0000
[2019-03-27 04:54:49,043] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96048: loss -218.6266
[2019-03-27 04:54:49,047] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96050: learning rate 0.0000
[2019-03-27 04:54:49,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5765992e-11 1.0000000e+00 3.5169022e-21 6.0049843e-10 2.0342532e-15], sum to 1.0000
[2019-03-27 04:54:49,074] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8869
[2019-03-27 04:54:49,082] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 96068: loss -162.4542
[2019-03-27 04:54:49,083] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 96068: learning rate 0.0000
[2019-03-27 04:54:49,085] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2894880.464029552 W.
[2019-03-27 04:54:49,090] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.7386076216372951, 1.0, 2.0, 0.6898938503329102, 1.0, 1.0, 1.03, 7.005100776528515, 6.9112, 170.5573041426782, 2894880.464029552, 2827615.5181628, 533800.3465886905], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3600600.0000, 
sim time next is 3601200.0000, 
raw observation next is [33.0, 63.00000000000001, 1.0, 2.0, 1.032227976457691, 1.0, 2.0, 1.032227976457691, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2887562.92928191, 2887562.92928191, 548766.0147882012], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.6300000000000001, 1.0, 1.0, 1.0388288872984228, 1.0, 1.0, 1.0388288872984228, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.8021008136894194, 0.8021008136894194, 0.8190537534152257], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.94314176], dtype=float32), -0.36910588]. 
=============================================
[2019-03-27 04:54:49,091] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96071: loss -178.7607
[2019-03-27 04:54:49,096] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96071: learning rate 0.0000
[2019-03-27 04:54:49,204] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 96120: loss -144.2297
[2019-03-27 04:54:49,207] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 96121: learning rate 0.0000
[2019-03-27 04:54:49,211] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96121: loss -140.9224
[2019-03-27 04:54:49,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96122: learning rate 0.0000
[2019-03-27 04:54:49,286] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 96159: loss -169.5327
[2019-03-27 04:54:49,287] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 96159: learning rate 0.0000
[2019-03-27 04:54:49,307] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 96164: loss -192.4329
[2019-03-27 04:54:49,311] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 96165: learning rate 0.0000
[2019-03-27 04:54:52,359] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.9198891e-12 1.0000000e+00 4.2238500e-21 3.5081686e-12 1.1431250e-16], sum to 1.0000
[2019-03-27 04:54:52,374] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7421
[2019-03-27 04:54:52,388] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 75.66666666666666, 1.0, 2.0, 0.76585292453172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1070347.662536968, 1070347.662536968, 235698.4606344045], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3652800.0000, 
sim time next is 3653400.0000, 
raw observation next is [27.83333333333334, 74.83333333333334, 1.0, 2.0, 0.7949907992404762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1111091.742310304, 1111091.742310304, 242681.8235789408], 
processed observation next is [1.0, 0.2608695652173913, 0.5181674565560824, 0.7483333333333334, 1.0, 1.0, 0.7530009629403328, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30863659508619556, 0.30863659508619556, 0.3622116769834937], 
reward next is 0.6378, 
noisyNet noise sample is [array([-1.1280698], dtype=float32), -1.0394899]. 
=============================================
[2019-03-27 04:54:57,775] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 04:54:57,776] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:54:57,777] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:54:57,777] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:54:57,779] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:54:57,779] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:54:57,778] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:54:57,780] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:54:57,784] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:54:57,785] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:54:57,782] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:54:57,803] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run5
[2019-03-27 04:54:57,820] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run5
[2019-03-27 04:54:57,843] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run5
[2019-03-27 04:54:57,843] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run5
[2019-03-27 04:54:57,880] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run5
[2019-03-27 04:55:42,807] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033969887]
[2019-03-27 04:55:42,808] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 100.0, 1.0, 2.0, 0.6057840286348392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912274.0170086146, 912274.0170086146, 210012.8193050214]
[2019-03-27 04:55:42,809] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:55:42,813] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8842071e-11 1.0000000e+00 1.3763322e-18 1.9124419e-10 8.1506584e-15], sampled 0.46006436405323836
[2019-03-27 04:55:48,524] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033969887]
[2019-03-27 04:55:48,526] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.5, 81.5, 1.0, 2.0, 0.5183031464538396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724256.8383605416, 724256.8383605416, 186513.2240466001]
[2019-03-27 04:55:48,527] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:55:48,530] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.3408567e-12 1.0000000e+00 7.4573098e-20 4.9167403e-11 5.7141314e-16], sampled 0.17535211178719656
[2019-03-27 04:55:52,771] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033969887]
[2019-03-27 04:55:52,772] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.23333333333333, 64.83333333333334, 1.0, 2.0, 0.8558998401424015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1196267.136757951, 1196267.136757951, 258095.3793921491]
[2019-03-27 04:55:52,774] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:55:52,777] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.863821e-12 1.000000e+00 1.120436e-19 6.124891e-11 9.051046e-16], sampled 0.3546858973720861
[2019-03-27 04:56:35,543] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033969887]
[2019-03-27 04:56:35,544] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.25, 83.5, 1.0, 2.0, 0.3659360682601785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579438.8657669369, 579438.8657669376, 173408.5297280036]
[2019-03-27 04:56:35,545] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:56:35,548] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.2621706e-11 1.0000000e+00 4.0825801e-18 3.4978159e-10 1.7460605e-14], sampled 0.5843252522625355
[2019-03-27 04:56:51,988] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-03-27 04:56:52,096] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:56:52,142] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:56:52,310] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1204 3007777057.7724 1766.0000
[2019-03-27 04:56:52,391] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7724 2842493333.3205 1131.0000
[2019-03-27 04:56:53,405] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 100000, evaluation results [100000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.120417058023, 3007777057.7724214, 1766.0, 8496.772351911874, 2842493333.320488, 1131.0]
[2019-03-27 04:56:58,109] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5473976e-12 1.0000000e+00 2.0904343e-18 5.2720557e-11 3.3356788e-14], sum to 1.0000
[2019-03-27 04:56:58,117] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8206
[2019-03-27 04:56:58,122] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5434979214518272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759475.6317513906, 759475.6317513912, 190692.0933461548], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3826800.0000, 
sim time next is 3827400.0000, 
raw observation next is [28.33333333333334, 82.5, 1.0, 2.0, 0.547343449913913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764851.2487339274, 764851.2487339268, 191346.3796811566], 
processed observation next is [0.0, 0.30434782608695654, 0.5418641390205374, 0.825, 1.0, 1.0, 0.45463066254688317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2124586802038687, 0.21245868020386854, 0.2855916114644128], 
reward next is 0.7144, 
noisyNet noise sample is [array([0.62445754], dtype=float32), -0.057579882]. 
=============================================
[2019-03-27 04:57:01,379] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103762: loss 0.0697
[2019-03-27 04:57:01,384] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103763: learning rate 0.0000
[2019-03-27 04:57:01,390] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103763: loss 0.0205
[2019-03-27 04:57:01,391] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103764: loss 0.0112
[2019-03-27 04:57:01,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103765: learning rate 0.0000
[2019-03-27 04:57:01,394] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103764: learning rate 0.0000
[2019-03-27 04:57:01,416] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103774: loss 0.0370
[2019-03-27 04:57:01,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103774: learning rate 0.0000
[2019-03-27 04:57:01,722] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 103923: loss 0.0277
[2019-03-27 04:57:01,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 103924: learning rate 0.0000
[2019-03-27 04:57:01,850] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 103982: loss 0.0090
[2019-03-27 04:57:01,853] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 103983: learning rate 0.0000
[2019-03-27 04:57:01,863] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103987: loss 0.0339
[2019-03-27 04:57:01,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103989: learning rate 0.0000
[2019-03-27 04:57:01,901] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 104002: loss 0.0160
[2019-03-27 04:57:01,903] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 104002: learning rate 0.0000
[2019-03-27 04:57:01,936] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104020: loss 0.0160
[2019-03-27 04:57:01,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104020: learning rate 0.0000
[2019-03-27 04:57:01,998] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104048: loss 0.0066
[2019-03-27 04:57:02,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104048: learning rate 0.0000
[2019-03-27 04:57:02,040] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104067: loss 0.0235
[2019-03-27 04:57:02,042] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104068: learning rate 0.0000
[2019-03-27 04:57:02,142] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104117: loss 0.0032
[2019-03-27 04:57:02,148] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104117: learning rate 0.0000
[2019-03-27 04:57:02,164] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 104127: loss 0.0153
[2019-03-27 04:57:02,168] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 104128: learning rate 0.0000
[2019-03-27 04:57:02,173] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104129: loss 0.0208
[2019-03-27 04:57:02,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104129: learning rate 0.0000
[2019-03-27 04:57:02,202] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 104144: loss 0.0028
[2019-03-27 04:57:02,205] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 104144: learning rate 0.0000
[2019-03-27 04:57:02,297] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104186: loss 0.0220
[2019-03-27 04:57:02,301] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104187: learning rate 0.0000
[2019-03-27 04:57:04,108] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4531770e-12 1.0000000e+00 1.5003087e-21 9.5269322e-13 1.0503042e-15], sum to 1.0000
[2019-03-27 04:57:04,118] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1517
[2019-03-27 04:57:04,123] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 0.5927436550983791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 828317.6943218128, 828317.6943218135, 199402.568686852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3931200.0000, 
sim time next is 3931800.0000, 
raw observation next is [34.16666666666667, 59.33333333333333, 1.0, 2.0, 0.6513293883865268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 910222.2297468415, 910222.2297468421, 210695.5985835136], 
processed observation next is [0.0, 0.5217391304347826, 0.8183254344391787, 0.5933333333333333, 1.0, 1.0, 0.5799149257668997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25283950826301155, 0.25283950826301166, 0.31447104266196063], 
reward next is 0.6855, 
noisyNet noise sample is [array([-0.66947305], dtype=float32), -0.8520134]. 
=============================================
[2019-03-27 04:57:04,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1857804e-13 1.0000000e+00 2.9490840e-20 7.8537822e-11 1.6385076e-15], sum to 1.0000
[2019-03-27 04:57:04,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2727
[2019-03-27 04:57:04,261] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.33333333333333, 60.66666666666667, 1.0, 2.0, 0.6068088127332675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847980.6119475391, 847980.6119475391, 202022.2186594015], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3948000.0000, 
sim time next is 3948600.0000, 
raw observation next is [34.16666666666667, 61.83333333333333, 1.0, 2.0, 0.6104027810238561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 853004.9938681923, 853004.993868193, 202700.9115656583], 
processed observation next is [0.0, 0.6956521739130435, 0.8183254344391787, 0.6183333333333333, 1.0, 1.0, 0.5306057602697062, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23694583163005342, 0.23694583163005362, 0.30253867397859446], 
reward next is 0.6975, 
noisyNet noise sample is [array([1.5463399], dtype=float32), -0.8770981]. 
=============================================
[2019-03-27 04:57:04,423] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5068287e-13 1.0000000e+00 2.4902828e-21 1.2970042e-12 1.5174470e-17], sum to 1.0000
[2019-03-27 04:57:04,431] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4462
[2019-03-27 04:57:04,436] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5942153171896634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830375.0427994833, 830375.0427994839, 199674.0139918176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3938400.0000, 
sim time next is 3939000.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.6102890567413877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852846.0065062258, 852846.0065062258, 202678.21304662], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5304687430619128, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2369016684739516, 0.2369016684739516, 0.30250479559197013], 
reward next is 0.6975, 
noisyNet noise sample is [array([0.18845761], dtype=float32), -0.04633043]. 
=============================================
[2019-03-27 04:57:04,447] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[71.50062 ]
 [71.454094]
 [71.45268 ]
 [71.4317  ]
 [71.36356 ]], R is [[71.53164673]
 [71.51831055]
 [71.50502014]
 [71.49173737]
 [71.47830963]].
[2019-03-27 04:57:18,128] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111728: loss -217.3734
[2019-03-27 04:57:18,130] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111728: learning rate 0.0000
[2019-03-27 04:57:18,330] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111819: loss -172.2810
[2019-03-27 04:57:18,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111821: learning rate 0.0000
[2019-03-27 04:57:18,351] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111829: loss -208.6272
[2019-03-27 04:57:18,354] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111829: learning rate 0.0000
[2019-03-27 04:57:18,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5898593e-12 1.0000000e+00 7.1730437e-21 2.4659157e-11 2.8384609e-16], sum to 1.0000
[2019-03-27 04:57:18,417] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0563
[2019-03-27 04:57:18,428] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111862: loss -130.5840
[2019-03-27 04:57:18,431] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3323494.466804633 W.
[2019-03-27 04:57:18,432] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111862: learning rate 0.0000
[2019-03-27 04:57:18,437] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.0, 55.66666666666667, 1.0, 2.0, 0.9426265762400092, 1.0, 2.0, 0.7919033276342671, 1.0, 1.0, 1.03, 7.005116870839537, 6.9112, 170.5573041426782, 3323494.466804633, 3256217.991928221, 608925.5757567051], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4196400.0000, 
sim time next is 4197000.0000, 
raw observation next is [36.0, 56.33333333333333, 1.0, 2.0, 0.8998330062737582, 1.0, 2.0, 0.7705065426511415, 1.0, 2.0, 1.03, 7.005113494095834, 6.9112, 170.5573041426782, 3233579.238983064, 3166305.183005524, 591880.8416329257], 
processed observation next is [1.0, 0.5652173913043478, 0.9052132701421801, 0.5633333333333332, 1.0, 1.0, 0.8793168750286243, 1.0, 1.0, 0.7235018586158332, 1.0, 1.0, 1.0365853658536586, 0.009391349409583416, 0.0, 0.8375144448122397, 0.8982164552730734, 0.8795292175015345, 0.8834042412431726], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9174503], dtype=float32), 0.6706785]. 
=============================================
[2019-03-27 04:57:18,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[60.006004]
 [60.038013]
 [59.684124]
 [59.298153]
 [59.51794 ]], R is [[59.60486603]
 [59.00881958]
 [58.41873169]
 [57.83454514]
 [57.25619888]].
[2019-03-27 04:57:18,624] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111946: loss -121.5281
[2019-03-27 04:57:18,629] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111946: learning rate 0.0000
[2019-03-27 04:57:18,641] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111953: loss -171.9833
[2019-03-27 04:57:18,641] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111953: learning rate 0.0000
[2019-03-27 04:57:18,679] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 111971: loss -196.6376
[2019-03-27 04:57:18,682] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 111972: learning rate 0.0000
[2019-03-27 04:57:18,726] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111995: loss -163.9375
[2019-03-27 04:57:18,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111998: learning rate 0.0000
[2019-03-27 04:57:18,782] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112018: loss -132.2277
[2019-03-27 04:57:18,784] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112018: learning rate 0.0000
[2019-03-27 04:57:18,786] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 112018: loss -218.9163
[2019-03-27 04:57:18,788] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 112018: learning rate 0.0000
[2019-03-27 04:57:18,925] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 112084: loss -215.6371
[2019-03-27 04:57:18,927] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 112084: learning rate 0.0000
[2019-03-27 04:57:18,929] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 112086: loss -157.0872
[2019-03-27 04:57:18,935] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 112086: learning rate 0.0000
[2019-03-27 04:57:18,954] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112096: loss -257.4694
[2019-03-27 04:57:18,955] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112096: learning rate 0.0000
[2019-03-27 04:57:18,958] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112096: loss -197.7511
[2019-03-27 04:57:18,961] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112097: learning rate 0.0000
[2019-03-27 04:57:19,097] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 112162: loss -136.5901
[2019-03-27 04:57:19,099] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 112163: learning rate 0.0000
[2019-03-27 04:57:19,111] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112170: loss -162.3955
[2019-03-27 04:57:19,114] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112171: learning rate 0.0000
[2019-03-27 04:57:22,595] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5903446e-12 1.0000000e+00 6.2607196e-19 9.1935279e-12 7.6877710e-15], sum to 1.0000
[2019-03-27 04:57:22,604] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0195
[2019-03-27 04:57:22,609] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2553069.822959731 W.
[2019-03-27 04:57:22,619] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.66666666666667, 81.0, 1.0, 2.0, 0.6085183487097162, 1.0, 2.0, 0.6085183487097162, 1.0, 1.0, 1.03, 6.941322215763321, 6.9112, 170.5573041426782, 2553069.822959731, 2531492.054515068, 491090.2079900011], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4263600.0000, 
sim time next is 4264200.0000, 
raw observation next is [32.0, 79.5, 1.0, 2.0, 1.016034479090995, 1.0, 2.0, 1.016034479090995, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2842211.572499648, 2842211.572499649, 538764.9138806438], 
processed observation next is [1.0, 0.34782608695652173, 0.7156398104265403, 0.795, 1.0, 1.0, 1.0193186495072228, 1.0, 1.0, 1.0193186495072228, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7895032145832356, 0.7895032145832359, 0.8041267371352893], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.004146], dtype=float32), 0.55224466]. 
=============================================
[2019-03-27 04:57:23,236] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3250947e-11 1.0000000e+00 2.1943181e-19 2.2410917e-11 8.1875200e-16], sum to 1.0000
[2019-03-27 04:57:23,242] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0426
[2019-03-27 04:57:23,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3065690.052805917 W.
[2019-03-27 04:57:23,260] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [38.0, 53.66666666666666, 1.0, 2.0, 0.8199208551887539, 1.0, 2.0, 0.7305504671086396, 1.0, 1.0, 1.03, 7.005107189709724, 6.9112, 170.5573041426782, 3065690.052805917, 2998420.512916593, 561883.7391768019], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4281000.0000, 
sim time next is 4281600.0000, 
raw observation next is [38.0, 53.33333333333334, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 8.68906832243933, 6.9112, 170.5573041426782, 4184373.067435424, 2910813.66901257, 542336.254655293], 
processed observation next is [1.0, 0.5652173913043478, 1.0, 0.5333333333333334, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.17778683224393294, 0.0, 0.8375144448122397, 1.1623258520653954, 0.8085593525034916, 0.8094570965004373], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.80907327], dtype=float32), 0.3799018]. 
=============================================
[2019-03-27 04:57:26,487] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0756496e-15 1.0000000e+00 1.1325653e-22 1.1772327e-14 1.1793120e-17], sum to 1.0000
[2019-03-27 04:57:26,495] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9155
[2019-03-27 04:57:26,499] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 81.5, 1.0, 2.0, 0.6211749877591769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868064.7239830779, 868064.7239830779, 204757.4684565242], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4321800.0000, 
sim time next is 4322400.0000, 
raw observation next is [30.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6198830505090315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866258.5622388503, 866258.5622388503, 204508.9283890536], 
processed observation next is [1.0, 0.0, 0.6366508688783573, 0.8233333333333335, 1.0, 1.0, 0.5420277716976283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24062737839968062, 0.24062737839968062, 0.30523720655082626], 
reward next is 0.6948, 
noisyNet noise sample is [array([-0.02485823], dtype=float32), 1.7708049]. 
=============================================
[2019-03-27 04:57:28,253] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0031921e-12 1.0000000e+00 1.4646965e-17 2.1838220e-11 1.3272083e-14], sum to 1.0000
[2019-03-27 04:57:28,263] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5504
[2019-03-27 04:57:28,271] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3066984.17565208 W.
[2019-03-27 04:57:28,276] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 60.0, 1.0, 2.0, 0.8205368748411319, 1.0, 2.0, 0.7308584769348284, 1.0, 2.0, 1.03, 7.005107238301933, 6.9112, 170.5573041426782, 3066984.17565208, 2999714.60095418, 562105.4029907304], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4359600.0000, 
sim time next is 4360200.0000, 
raw observation next is [36.16666666666666, 59.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 8.534343052833666, 6.9112, 170.5573041426782, 4073407.861262971, 2910684.467280218, 543514.5005138316], 
processed observation next is [1.0, 0.4782608695652174, 0.9131121642969979, 0.59, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.1623143052833666, 0.0, 0.8375144448122397, 1.1315021836841586, 0.8085234631333938, 0.811215672408704], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1798737], dtype=float32), 0.6849294]. 
=============================================
[2019-03-27 04:57:33,563] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6977805e-13 1.0000000e+00 5.8913464e-20 5.2336976e-13 1.3670302e-16], sum to 1.0000
[2019-03-27 04:57:33,571] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9551
[2019-03-27 04:57:33,574] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 70.33333333333334, 1.0, 2.0, 0.6710449508241483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 937786.5720021, 937786.5720021005, 214727.2930612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4450200.0000, 
sim time next is 4450800.0000, 
raw observation next is [33.0, 69.66666666666667, 1.0, 2.0, 0.6439446792931339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 899897.8467283173, 899897.8467283173, 209218.5528079979], 
processed observation next is [0.0, 0.5217391304347826, 0.7630331753554502, 0.6966666666666668, 1.0, 1.0, 0.5710176858953421, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24997162409119925, 0.24997162409119925, 0.3122664967283551], 
reward next is 0.6877, 
noisyNet noise sample is [array([0.43464512], dtype=float32), 0.0002985085]. 
=============================================
[2019-03-27 04:57:35,684] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119646: loss 0.9516
[2019-03-27 04:57:35,686] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119647: learning rate 0.0000
[2019-03-27 04:57:35,887] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 119738: loss 1.0910
[2019-03-27 04:57:35,894] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 119739: learning rate 0.0000
[2019-03-27 04:57:35,956] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119768: loss 1.0653
[2019-03-27 04:57:35,958] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119768: learning rate 0.0000
[2019-03-27 04:57:35,985] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119777: loss 0.7926
[2019-03-27 04:57:35,988] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119777: learning rate 0.0000
[2019-03-27 04:57:36,285] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119917: loss 0.8690
[2019-03-27 04:57:36,288] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119917: learning rate 0.0000
[2019-03-27 04:57:36,339] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 119939: loss 0.9421
[2019-03-27 04:57:36,340] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 119939: learning rate 0.0000
[2019-03-27 04:57:36,459] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 119997: loss 0.7802
[2019-03-27 04:57:36,463] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 119999: learning rate 0.0000
[2019-03-27 04:57:36,526] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 120024: loss 0.7845
[2019-03-27 04:57:36,530] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 120025: learning rate 0.0000
[2019-03-27 04:57:36,539] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120030: loss 0.9229
[2019-03-27 04:57:36,541] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120030: learning rate 0.0000
[2019-03-27 04:57:36,546] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120032: loss 1.0544
[2019-03-27 04:57:36,548] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120033: learning rate 0.0000
[2019-03-27 04:57:36,635] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 120073: loss 0.8831
[2019-03-27 04:57:36,639] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 120073: learning rate 0.0000
[2019-03-27 04:57:36,721] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 120108: loss 0.9052
[2019-03-27 04:57:36,723] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 120108: learning rate 0.0000
[2019-03-27 04:57:36,755] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120122: loss 0.6021
[2019-03-27 04:57:36,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120122: learning rate 0.0000
[2019-03-27 04:57:36,860] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120173: loss 0.9621
[2019-03-27 04:57:36,868] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120176: learning rate 0.0000
[2019-03-27 04:57:36,963] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 120219: loss 0.8898
[2019-03-27 04:57:36,968] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 120219: learning rate 0.0000
[2019-03-27 04:57:37,055] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 120260: loss 0.7981
[2019-03-27 04:57:37,061] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 120262: learning rate 0.0000
[2019-03-27 04:57:37,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4870367e-12 1.0000000e+00 1.4103462e-20 4.3909902e-12 2.9512621e-16], sum to 1.0000
[2019-03-27 04:57:37,808] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4263
[2019-03-27 04:57:37,812] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5469804242961529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764343.7785209075, 764343.7785209082, 191284.746120626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4532400.0000, 
sim time next is 4533000.0000, 
raw observation next is [29.33333333333334, 76.83333333333334, 1.0, 2.0, 0.5503004820828512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768984.8661690967, 768984.8661690967, 191852.4994481579], 
processed observation next is [0.0, 0.4782608695652174, 0.5892575039494474, 0.7683333333333334, 1.0, 1.0, 0.45819335190704963, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2136069072691935, 0.2136069072691935, 0.2863470141017282], 
reward next is 0.7137, 
noisyNet noise sample is [array([-1.1124679], dtype=float32), -0.9072321]. 
=============================================
[2019-03-27 04:57:37,822] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[63.24305 ]
 [63.167747]
 [63.110523]
 [63.05315 ]
 [62.99139 ]], R is [[63.40114975]
 [63.48163986]
 [63.5627861 ]
 [63.6445694 ]
 [63.72691345]].
[2019-03-27 04:57:38,142] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.9565881e-12 1.0000000e+00 1.3082146e-19 3.7001402e-11 2.1033067e-16], sum to 1.0000
[2019-03-27 04:57:38,151] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6298
[2019-03-27 04:57:38,156] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5134148259972892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717423.7810905465, 717423.7810905459, 185724.3795693072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4521000.0000, 
sim time next is 4521600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5131814695597957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717097.5887832848, 717097.5887832848, 185686.9122256765], 
processed observation next is [0.0, 0.34782608695652173, 0.4786729857819906, 0.84, 1.0, 1.0, 0.413471650072043, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19919377466202354, 0.19919377466202354, 0.27714464511295], 
reward next is 0.7229, 
noisyNet noise sample is [array([-1.0465162], dtype=float32), 0.59649324]. 
=============================================
[2019-03-27 04:57:42,555] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7493349e-13 1.0000000e+00 5.8814572e-23 6.3614663e-13 1.7154617e-19], sum to 1.0000
[2019-03-27 04:57:42,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5179
[2019-03-27 04:57:42,573] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3299808.912484923 W.
[2019-03-27 04:57:42,578] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.66666666666667, 81.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 8.3422645127832, 6.9112, 168.9042683175111, 3299808.912484923, 2284614.713808356, 472574.1105996352], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4609200.0000, 
sim time next is 4609800.0000, 
raw observation next is [31.0, 79.5, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 0.0, 1.0, 0.0, 7.467403212464346, 6.9112, 170.5573041426782, 3308224.828726287, 2909793.842737939, 550629.1728286407], 
processed observation next is [1.0, 0.34782608695652173, 0.6682464454976303, 0.795, 1.0, 1.0, 1.0481927710843375, 1.0, 0.5, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.05562032124643457, 0.0, 0.8375144448122397, 0.9189513413128575, 0.8082760674272054, 0.8218345863114039], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49693236], dtype=float32), 1.1216217]. 
=============================================
[2019-03-27 04:57:47,442] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7164590e-14 1.0000000e+00 1.2962315e-21 2.7963143e-13 1.8506530e-17], sum to 1.0000
[2019-03-27 04:57:47,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6094
[2019-03-27 04:57:47,456] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7864823679441934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1099194.069068189, 1099194.069068189, 240619.8717489485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4681200.0000, 
sim time next is 4681800.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7798236279340122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1089882.984695318, 1089882.984695318, 239018.6264797775], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.734727262571099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3027452735264772, 0.3027452735264772, 0.3567442186265336], 
reward next is 0.6433, 
noisyNet noise sample is [array([-0.31385303], dtype=float32), -0.6165031]. 
=============================================
[2019-03-27 04:57:47,532] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 04:57:47,534] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:57:47,537] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:57:47,537] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:57:47,539] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:57:47,539] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:57:47,541] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:57:47,541] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:57:47,542] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:57:47,542] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:57:47,544] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:57:47,559] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run6
[2019-03-27 04:57:47,559] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run6
[2019-03-27 04:57:47,577] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run6
[2019-03-27 04:57:47,619] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run6
[2019-03-27 04:57:47,619] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run6
[2019-03-27 04:58:02,247] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.034965828]
[2019-03-27 04:58:02,249] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 93.0, 1.0, 2.0, 0.3292132708040946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510337.8854934814, 510337.8854934814, 167546.445712559]
[2019-03-27 04:58:02,252] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:58:02,254] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.2448800e-12 1.0000000e+00 2.2888959e-19 1.0270686e-11 1.4456967e-15], sampled 0.7052353345614565
[2019-03-27 04:58:07,379] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.034965828]
[2019-03-27 04:58:07,381] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.2, 82.0, 1.0, 2.0, 0.2345769611785446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 389812.4069394476, 389812.4069394483, 159121.8492399324]
[2019-03-27 04:58:07,382] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:58:07,385] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6710549e-11 1.0000000e+00 2.4584211e-18 4.6303170e-11 1.0054129e-14], sampled 0.8015902232573284
[2019-03-27 04:58:16,441] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.034965828]
[2019-03-27 04:58:16,442] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.72369785, 68.10500395, 1.0, 2.0, 0.4219384295432772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687231.9589208621, 687231.9589208628, 182643.3670751469]
[2019-03-27 04:58:16,443] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:58:16,445] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7113076e-12 1.0000000e+00 2.3544370e-20 3.0185483e-12 2.2274830e-16], sampled 0.8573050900603312
[2019-03-27 04:58:25,993] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.034965828]
[2019-03-27 04:58:25,994] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.66666666666666, 94.83333333333333, 1.0, 2.0, 0.4957848750506703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692780.4131373968, 692780.4131373974, 182937.2439685969]
[2019-03-27 04:58:25,994] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:58:25,998] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5609818e-13 1.0000000e+00 1.1957718e-21 5.3945417e-13 1.8562669e-17], sampled 0.5845941529896107
[2019-03-27 04:58:26,970] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.034965828]
[2019-03-27 04:58:26,974] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.59465868, 81.71736828, 1.0, 2.0, 0.4217072934253147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 624483.8856567002, 624483.8856567008, 176710.1175233934]
[2019-03-27 04:58:26,978] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:58:26,980] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.5476616e-13 1.0000000e+00 3.0565285e-21 8.8016839e-13 3.9659959e-17], sampled 0.46700759893657573
[2019-03-27 04:58:38,162] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.034965828]
[2019-03-27 04:58:38,165] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.53333333333333, 71.0, 1.0, 2.0, 0.8428471004100768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1178013.569221269, 1178013.569221269, 254696.9519297469]
[2019-03-27 04:58:38,167] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:58:38,171] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.9899866e-14 1.0000000e+00 8.3717775e-23 1.2733614e-13 2.0392459e-18], sampled 0.043110981314865016
[2019-03-27 04:58:53,879] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.034965828]
[2019-03-27 04:58:53,879] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.95, 73.5, 1.0, 2.0, 0.585910429295944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 818765.0599557302, 818765.0599557295, 198150.6407699384]
[2019-03-27 04:58:53,881] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:58:53,884] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1929410e-12 1.0000000e+00 1.2144085e-20 1.9708078e-12 1.2839220e-16], sampled 0.8228253336953497
[2019-03-27 04:59:30,028] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.034965828]
[2019-03-27 04:59:30,029] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.8, 71.0, 1.0, 2.0, 0.5990558705190031, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9986542588652546, 6.911199999999999, 6.9112, 168.9129559133916, 1679364.148991898, 1679364.148991898, 357937.7259576876]
[2019-03-27 04:59:30,030] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:59:30,033] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.0168679e-13 1.0000000e+00 7.5783832e-21 1.3099721e-12 9.0211219e-17], sampled 0.8316587898269083
[2019-03-27 04:59:30,035] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1679364.148991898 W.
[2019-03-27 04:59:41,559] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164134907.6624 1778.0000
[2019-03-27 04:59:42,003] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7281 2779199535.5119 933.0000
[2019-03-27 04:59:42,034] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8604 2842435242.9853 1131.0000
[2019-03-27 04:59:42,049] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 04:59:42,054] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4776 3007718284.7998 1766.0000
[2019-03-27 04:59:43,067] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 125000, evaluation results [125000.0, 7883.418813853433, 3164134907.662386, 1778.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8660.728066421085, 2779199535.5119066, 933.0, 7997.477595799005, 3007718284.7998276, 1766.0, 8496.860364057107, 2842435242.9852624, 1131.0]
[2019-03-27 04:59:47,171] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.3703040e-14 1.0000000e+00 8.2544492e-22 2.3865387e-12 7.4221580e-17], sum to 1.0000
[2019-03-27 04:59:47,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7458
[2019-03-27 04:59:47,184] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4847111144948532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677301.663182531, 677301.6631825304, 181235.0571620968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4758000.0000, 
sim time next is 4758600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4846000987575116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677146.4880560861, 677146.4880560861, 181218.174291246], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 1.0, 1.0, 0.37903626356326703, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18809624668224614, 0.18809624668224614, 0.2704748870018597], 
reward next is 0.7295, 
noisyNet noise sample is [array([-0.47145602], dtype=float32), -0.35493514]. 
=============================================
[2019-03-27 04:59:48,824] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127724: loss -97.1659
[2019-03-27 04:59:48,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127724: learning rate 0.0000
[2019-03-27 04:59:48,871] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127745: loss -99.2528
[2019-03-27 04:59:48,876] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127748: learning rate 0.0000
[2019-03-27 04:59:48,933] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127774: loss -73.2402
[2019-03-27 04:59:48,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127774: learning rate 0.0000
[2019-03-27 04:59:49,047] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127825: loss -76.6125
[2019-03-27 04:59:49,048] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127825: learning rate 0.0000
[2019-03-27 04:59:49,230] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 127908: loss -47.0164
[2019-03-27 04:59:49,232] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 127909: learning rate 0.0000
[2019-03-27 04:59:49,272] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127926: loss -75.8661
[2019-03-27 04:59:49,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127927: learning rate 0.0000
[2019-03-27 04:59:49,350] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127965: loss -74.9909
[2019-03-27 04:59:49,353] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127967: learning rate 0.0000
[2019-03-27 04:59:49,397] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127989: loss -41.6998
[2019-03-27 04:59:49,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127989: learning rate 0.0000
[2019-03-27 04:59:49,438] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128006: loss -53.9407
[2019-03-27 04:59:49,442] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128006: learning rate 0.0000
[2019-03-27 04:59:49,517] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128046: loss -71.9233
[2019-03-27 04:59:49,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128046: learning rate 0.0000
[2019-03-27 04:59:49,525] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128048: loss -2.7422
[2019-03-27 04:59:49,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128049: learning rate 0.0000
[2019-03-27 04:59:49,597] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 128081: loss -98.0469
[2019-03-27 04:59:49,598] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 128081: learning rate 0.0000
[2019-03-27 04:59:49,707] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 128131: loss -64.4715
[2019-03-27 04:59:49,708] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 128131: learning rate 0.0000
[2019-03-27 04:59:49,717] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128136: loss -93.8449
[2019-03-27 04:59:49,720] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128136: learning rate 0.0000
[2019-03-27 04:59:49,827] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 128183: loss -73.9867
[2019-03-27 04:59:49,831] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 128186: learning rate 0.0000
[2019-03-27 04:59:49,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6234863e-11 1.0000000e+00 8.7192852e-18 7.6817913e-11 4.4055237e-14], sum to 1.0000
[2019-03-27 04:59:49,939] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6354
[2019-03-27 04:59:49,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2481612.143195448 W.
[2019-03-27 04:59:49,953] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.66666666666667, 64.0, 1.0, 2.0, 0.591503470798528, 1.0, 2.0, 0.591503470798528, 1.0, 2.0, 1.027245915590105, 6.911199999999999, 6.9112, 170.5573041426782, 2481612.143195448, 2481612.143195448, 484187.0093019378], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4810800.0000, 
sim time next is 4811400.0000, 
raw observation next is [31.5, 64.5, 1.0, 2.0, 0.8775043607176601, 1.0, 2.0, 0.8775043607176601, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2454312.691389275, 2454312.691389275, 459364.3786527445], 
processed observation next is [1.0, 0.6956521739130435, 0.6919431279620853, 0.645, 1.0, 1.0, 0.8524148924309158, 1.0, 1.0, 0.8524148924309158, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6817535253859096, 0.6817535253859096, 0.6856184756011112], 
reward next is 0.3144, 
noisyNet noise sample is [array([-0.5297004], dtype=float32), -0.007989945]. 
=============================================
[2019-03-27 04:59:50,027] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 128276: loss -103.7935
[2019-03-27 04:59:50,029] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 128276: learning rate 0.0000
[2019-03-27 04:59:50,310] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.1382699e-10 1.0000000e+00 1.3791710e-17 3.6218126e-10 3.0455232e-14], sum to 1.0000
[2019-03-27 04:59:50,319] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9660
[2019-03-27 04:59:50,326] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2347138.731207923 W.
[2019-03-27 04:59:50,330] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 65.0, 1.0, 2.0, 0.8392217986784745, 1.0, 1.0, 0.8392217986784745, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2347138.731207923, 2347138.731207922, 439415.433151254], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4807200.0000, 
sim time next is 4807800.0000, 
raw observation next is [31.5, 64.5, 1.0, 2.0, 0.5168406849865145, 1.0, 2.0, 0.5168406849865145, 1.0, 1.0, 0.8973845022638741, 6.9112, 6.9112, 170.5573041426782, 2168091.396595747, 2168091.396595747, 426949.8942253987], 
processed observation next is [1.0, 0.6521739130434783, 0.6919431279620853, 0.645, 1.0, 1.0, 0.41788034335724633, 1.0, 1.0, 0.41788034335724633, 1.0, 0.5, 0.8748591491022853, 0.0, 0.0, 0.8375144448122397, 0.6022476101654853, 0.6022476101654853, 0.63723864809761], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03833378], dtype=float32), 1.0608026]. 
=============================================
[2019-03-27 04:59:50,831] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1651209e-14 1.0000000e+00 1.4485738e-21 2.4516001e-13 5.0075251e-17], sum to 1.0000
[2019-03-27 04:59:50,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3138
[2019-03-27 04:59:50,845] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.91666666666667, 74.5, 1.0, 2.0, 0.4919079443026316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687361.2670865305, 687361.2670865298, 182337.7432771303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4831800.0000, 
sim time next is 4832400.0000, 
raw observation next is [27.83333333333334, 75.0, 1.0, 2.0, 0.4930706138307717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688986.4353290875, 688986.435329087, 182517.2528753647], 
processed observation next is [1.0, 0.9565217391304348, 0.5181674565560824, 0.75, 1.0, 1.0, 0.38924170341056835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19138512092474655, 0.19138512092474638, 0.2724138102617384], 
reward next is 0.7276, 
noisyNet noise sample is [array([1.3294762], dtype=float32), 1.419478]. 
=============================================
[2019-03-27 05:00:02,906] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8768077e-12 1.0000000e+00 1.4252308e-19 3.2471151e-11 5.8763874e-16], sum to 1.0000
[2019-03-27 05:00:02,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5685
[2019-03-27 05:00:02,918] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4997780815167864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698362.1174565919, 698362.1174565919, 183561.3093507194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5032800.0000, 
sim time next is 5033400.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5011830809076951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700326.0326932248, 700326.0326932248, 183781.6750144393], 
processed observation next is [0.0, 0.2608695652173913, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.39901576012975315, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19453500908145135, 0.19453500908145135, 0.27430100748423775], 
reward next is 0.7257, 
noisyNet noise sample is [array([0.87114626], dtype=float32), -1.1223404]. 
=============================================
[2019-03-27 05:00:03,412] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3971649e-11 1.0000000e+00 3.0610240e-19 3.4167027e-11 8.9172177e-16], sum to 1.0000
[2019-03-27 05:00:03,419] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7214
[2019-03-27 05:00:03,425] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.512187673146058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715708.4328341713, 715708.4328341719, 185527.516551328], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5043600.0000, 
sim time next is 5044200.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5090546872603902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711329.0709204307, 711329.0709204313, 185026.9719908704], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.4084996232052893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19759140858900853, 0.1975914085890087, 0.27615965968786627], 
reward next is 0.7238, 
noisyNet noise sample is [array([0.32986167], dtype=float32), -0.07558125]. 
=============================================
[2019-03-27 05:00:03,649] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.8944702e-13 1.0000000e+00 1.5909116e-20 3.0730079e-13 4.4206428e-17], sum to 1.0000
[2019-03-27 05:00:03,655] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8290
[2019-03-27 05:00:03,660] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5069270928919654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708355.0793879728, 708355.0793879734, 184688.7124087242], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5076443043177187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709357.6101144654, 709357.6101144648, 184802.5896972851], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.4068003666478538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19704378058735147, 0.19704378058735134, 0.27582476074221657], 
reward next is 0.7242, 
noisyNet noise sample is [array([-1.5295402], dtype=float32), -0.72773516]. 
=============================================
[2019-03-27 05:00:05,453] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 135682: loss 0.5433
[2019-03-27 05:00:05,456] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 135682: learning rate 0.0000
[2019-03-27 05:00:05,511] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 135706: loss 0.4903
[2019-03-27 05:00:05,515] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 135707: learning rate 0.0000
[2019-03-27 05:00:05,536] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 135714: loss 0.4484
[2019-03-27 05:00:05,539] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 135714: learning rate 0.0000
[2019-03-27 05:00:05,565] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 135726: loss 0.5436
[2019-03-27 05:00:05,566] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 135727: learning rate 0.0000
[2019-03-27 05:00:05,907] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 135885: loss 0.6148
[2019-03-27 05:00:05,911] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 135887: learning rate 0.0000
[2019-03-27 05:00:05,969] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 135913: loss 0.5443
[2019-03-27 05:00:05,971] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 135914: learning rate 0.0000
[2019-03-27 05:00:06,040] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 135946: loss 0.5825
[2019-03-27 05:00:06,044] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 135946: learning rate 0.0000
[2019-03-27 05:00:06,185] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136010: loss 0.6777
[2019-03-27 05:00:06,186] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136011: learning rate 0.0000
[2019-03-27 05:00:06,231] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 136029: loss 0.5993
[2019-03-27 05:00:06,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 136029: learning rate 0.0000
[2019-03-27 05:00:06,298] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 136058: loss 0.5424
[2019-03-27 05:00:06,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 136060: learning rate 0.0000
[2019-03-27 05:00:06,382] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 136096: loss 0.5547
[2019-03-27 05:00:06,385] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 136096: learning rate 0.0000
[2019-03-27 05:00:06,425] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 136118: loss 0.4422
[2019-03-27 05:00:06,427] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 136118: learning rate 0.0000
[2019-03-27 05:00:06,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2915038e-13 1.0000000e+00 6.8766744e-22 9.4257338e-13 6.7384061e-19], sum to 1.0000
[2019-03-27 05:00:06,439] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6291
[2019-03-27 05:00:06,445] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5131314479582318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717027.6671699791, 717027.6671699791, 185678.4695288258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5100000.0000, 
sim time next is 5100600.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.511471669805933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714707.5848065187, 714707.5848065187, 185412.3566504704], 
processed observation next is [0.0, 0.0, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.4114116503685939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19852988466847743, 0.19852988466847743, 0.2767348606723439], 
reward next is 0.7233, 
noisyNet noise sample is [array([-0.10745738], dtype=float32), 1.3584503]. 
=============================================
[2019-03-27 05:00:06,496] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136144: loss 0.6388
[2019-03-27 05:00:06,498] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136144: learning rate 0.0000
[2019-03-27 05:00:06,529] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 136160: loss 0.6247
[2019-03-27 05:00:06,531] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 136161: learning rate 0.0000
[2019-03-27 05:00:06,736] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 136255: loss 0.5141
[2019-03-27 05:00:06,743] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 136258: learning rate 0.0000
[2019-03-27 05:00:06,941] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 136349: loss 0.5213
[2019-03-27 05:00:06,943] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 136349: learning rate 0.0000
[2019-03-27 05:00:06,970] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4375296e-12 1.0000000e+00 6.1125351e-21 6.5999940e-13 1.4581967e-16], sum to 1.0000
[2019-03-27 05:00:06,981] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2029
[2019-03-27 05:00:06,989] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333333, 82.33333333333333, 1.0, 2.0, 0.5122393071937514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715780.6082808475, 715780.6082808468, 185535.990862769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5123400.0000, 
sim time next is 5124000.0000, 
raw observation next is [27.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5150709156731073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719738.71351649, 719738.71351649, 185991.1029217095], 
processed observation next is [0.0, 0.30434782608695654, 0.5102685624012641, 0.8066666666666668, 1.0, 1.0, 0.41574809117241834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1999274204212472, 0.1999274204212472, 0.27759866107717834], 
reward next is 0.7224, 
noisyNet noise sample is [array([0.8921997], dtype=float32), 0.56074345]. 
=============================================
[2019-03-27 05:00:07,007] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[64.19011 ]
 [64.12611 ]
 [64.07313 ]
 [64.039856]
 [64.00606 ]], R is [[64.33946228]
 [64.41915131]
 [64.49901581]
 [64.57936096]
 [64.66014099]].
[2019-03-27 05:00:08,122] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.47035352e-13 1.00000000e+00 1.96167437e-21 1.00288145e-13
 9.21218078e-17], sum to 1.0000
[2019-03-27 05:00:08,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4368
[2019-03-27 05:00:08,135] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5230494490516497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730891.4203920225, 730891.4203920232, 187285.3622219982], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5130600.0000, 
sim time next is 5131200.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5121259760916383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715622.1910551882, 715622.1910551875, 185517.6304398416], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.41219997119474494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1987839419597745, 0.1987839419597743, 0.27689198573110685], 
reward next is 0.7231, 
noisyNet noise sample is [array([0.49340096], dtype=float32), -0.075973816]. 
=============================================
[2019-03-27 05:00:23,070] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3952658e-13 1.0000000e+00 2.4916021e-22 6.8417839e-14 8.4906006e-19], sum to 1.0000
[2019-03-27 05:00:23,077] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7737
[2019-03-27 05:00:23,085] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2293373.5438631 W.
[2019-03-27 05:00:23,090] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.3, 66.66666666666667, 1.0, 2.0, 0.5466770835185456, 1.0, 2.0, 0.5466770835185456, 1.0, 2.0, 0.9493973051975781, 6.9112, 6.9112, 170.5573041426782, 2293373.5438631, 2293373.5438631, 448926.6980353944], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5390400.0000, 
sim time next is 5391000.0000, 
raw observation next is [33.5, 66.0, 1.0, 2.0, 0.5515430469415388, 1.0, 2.0, 0.5515430469415388, 1.0, 2.0, 0.9578478744646233, 6.9112, 6.9112, 170.5573041426782, 2313805.720062277, 2313805.720062277, 452620.7948708776], 
processed observation next is [1.0, 0.391304347826087, 0.7867298578199052, 0.66, 1.0, 1.0, 0.4596904180018539, 1.0, 1.0, 0.4596904180018539, 1.0, 1.0, 0.9485949688592967, 0.0, 0.0, 0.8375144448122397, 0.6427238111284103, 0.6427238111284103, 0.6755534251804143], 
reward next is 0.3244, 
noisyNet noise sample is [array([0.05323731], dtype=float32), -0.5088895]. 
=============================================
[2019-03-27 05:00:23,101] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[59.916553]
 [61.15894 ]
 [61.90511 ]
 [61.71494 ]
 [62.881214]], R is [[59.22222519]
 [58.95996475]
 [58.37036514]
 [57.78666306]
 [57.20879745]].
[2019-03-27 05:00:23,337] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 143720: loss -231.0497
[2019-03-27 05:00:23,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 143720: learning rate 0.0000
[2019-03-27 05:00:23,424] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 143758: loss -210.1865
[2019-03-27 05:00:23,426] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 143758: learning rate 0.0000
[2019-03-27 05:00:23,480] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 143782: loss -200.5463
[2019-03-27 05:00:23,481] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 143783: learning rate 0.0000
[2019-03-27 05:00:23,593] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 143832: loss -184.9123
[2019-03-27 05:00:23,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 143832: learning rate 0.0000
[2019-03-27 05:00:23,660] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 143860: loss -112.2330
[2019-03-27 05:00:23,663] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 143860: learning rate 0.0000
[2019-03-27 05:00:23,711] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 143882: loss -202.3126
[2019-03-27 05:00:23,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 143882: learning rate 0.0000
[2019-03-27 05:00:23,961] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 143998: loss -212.0614
[2019-03-27 05:00:23,966] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 143999: learning rate 0.0000
[2019-03-27 05:00:23,967] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 143999: loss -186.2710
[2019-03-27 05:00:23,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144000: learning rate 0.0000
[2019-03-27 05:00:24,037] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 144030: loss -159.0110
[2019-03-27 05:00:24,039] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 144030: learning rate 0.0000
[2019-03-27 05:00:24,067] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 144044: loss -161.2090
[2019-03-27 05:00:24,068] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 144044: learning rate 0.0000
[2019-03-27 05:00:24,101] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 144060: loss -228.6101
[2019-03-27 05:00:24,103] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 144061: learning rate 0.0000
[2019-03-27 05:00:24,136] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 144076: loss -240.3299
[2019-03-27 05:00:24,138] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 144077: learning rate 0.0000
[2019-03-27 05:00:24,151] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144081: loss -182.4736
[2019-03-27 05:00:24,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144081: learning rate 0.0000
[2019-03-27 05:00:24,174] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 144091: loss -216.7865
[2019-03-27 05:00:24,176] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 144091: learning rate 0.0000
[2019-03-27 05:00:24,608] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 144284: loss -166.3182
[2019-03-27 05:00:24,611] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 144284: learning rate 0.0000
[2019-03-27 05:00:24,649] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 144301: loss -176.8450
[2019-03-27 05:00:24,650] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 144302: learning rate 0.0000
[2019-03-27 05:00:24,689] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.7333632e-13 1.0000000e+00 3.4721821e-19 2.4554670e-12 1.2226344e-15], sum to 1.0000
[2019-03-27 05:00:24,700] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5233
[2019-03-27 05:00:24,711] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3495859.979947878 W.
[2019-03-27 05:00:24,714] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.0, 64.0, 1.0, 2.0, 1.024652293079779, 1.0, 2.0, 0.8329161860541521, 1.0, 2.0, 1.03, 7.005123344661879, 6.9112, 170.5573041426782, 3495859.979947878, 3428578.867609185, 643513.3436156289], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5400000.0000, 
sim time next is 5400600.0000, 
raw observation next is [36.16666666666666, 63.33333333333334, 1.0, 2.0, 1.021321151892515, 1.0, 2.0, 0.8312506154605203, 1.0, 2.0, 1.03, 7.005123081719091, 6.9112, 170.5573041426782, 3488859.591454439, 3421578.66747236, 642059.7600386691], 
processed observation next is [1.0, 0.5217391304347826, 0.9131121642969979, 0.6333333333333334, 1.0, 1.0, 1.025688134810259, 1.0, 1.0, 0.7966874885066509, 1.0, 1.0, 1.0365853658536586, 0.009392308171909124, 0.0, 0.8375144448122397, 0.9691276642928996, 0.9504385187423223, 0.9582981493114464], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5325851], dtype=float32), -1.0684674]. 
=============================================
[2019-03-27 05:00:26,822] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3739994e-12 1.0000000e+00 3.0663618e-21 4.8849911e-13 3.4219033e-17], sum to 1.0000
[2019-03-27 05:00:26,837] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1019
[2019-03-27 05:00:26,844] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.936888718743205, 6.9112, 168.9126344409556, 1471991.834952635, 1453767.40848766, 311353.9103205313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5454600.0000, 
sim time next is 5455200.0000, 
raw observation next is [27.83333333333334, 92.0, 1.0, 2.0, 0.9811337949457283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129355935981, 1371416.163929026, 1371416.163929026, 293233.7382016178], 
processed observation next is [1.0, 0.13043478260869565, 0.5181674565560824, 0.92, 1.0, 1.0, 0.9772696324647329, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294398424410774, 0.3809489344247295, 0.3809489344247295, 0.4376622958233101], 
reward next is 0.5623, 
noisyNet noise sample is [array([-1.0248842], dtype=float32), 1.0775107]. 
=============================================
[2019-03-27 05:00:33,986] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0355675e-12 1.0000000e+00 9.8578668e-21 1.3313094e-12 2.6187559e-16], sum to 1.0000
[2019-03-27 05:00:33,993] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7139
[2019-03-27 05:00:34,000] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2452480.864840047 W.
[2019-03-27 05:00:34,006] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.1, 64.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.145416054533348, 6.9112, 168.911117977754, 2452480.864840047, 2286321.743237867, 475699.6460498896], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5568000.0000, 
sim time next is 5568600.0000, 
raw observation next is [32.3, 63.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.157093263476037, 6.9112, 168.9111187005158, 2460798.782943252, 2286355.536841688, 475677.0249947009], 
processed observation next is [1.0, 0.43478260869565216, 0.7298578199052131, 0.63, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.02458932634760371, 0.0, 0.829430920664266, 0.6835552174842366, 0.6350987602338023, 0.7099657089473148], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6977798], dtype=float32), 0.31593323]. 
=============================================
[2019-03-27 05:00:37,278] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 05:00:37,280] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:00:37,282] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:00:37,282] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:00:37,284] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:00:37,283] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:00:37,284] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:00:37,287] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:00:37,287] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:00:37,289] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:00:37,290] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:00:37,309] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run7
[2019-03-27 05:00:37,309] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run7
[2019-03-27 05:00:37,329] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run7
[2019-03-27 05:00:37,348] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run7
[2019-03-27 05:00:37,384] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run7
[2019-03-27 05:00:50,601] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033503402]
[2019-03-27 05:00:50,603] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.7, 96.0, 1.0, 2.0, 0.4002298126347276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 598518.648910109, 598518.648910109, 174437.8733917987]
[2019-03-27 05:00:50,603] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:00:50,605] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1511945e-12 1.0000000e+00 2.2649914e-20 9.5421175e-13 1.6100180e-16], sampled 0.8922687723773494
[2019-03-27 05:01:01,540] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033503402]
[2019-03-27 05:01:01,541] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.4, 94.0, 1.0, 2.0, 0.4386847782915002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645358.9268643935, 645358.9268643935, 178651.9325906116]
[2019-03-27 05:01:01,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:01:01,544] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4644236e-12 1.0000000e+00 2.7867098e-20 1.2085350e-12 2.0412872e-16], sampled 0.5835526364735316
[2019-03-27 05:01:06,039] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033503402]
[2019-03-27 05:01:06,041] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.2, 89.33333333333334, 1.0, 2.0, 0.4079267947042723, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595140.6724725199, 595140.6724725205, 173676.075033264]
[2019-03-27 05:01:06,043] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:01:06,047] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.6658437e-12 1.0000000e+00 1.0668216e-19 2.5378952e-12 4.6020952e-16], sampled 0.0376104506758016
[2019-03-27 05:01:22,367] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033503402]
[2019-03-27 05:01:22,369] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.01430589333334, 99.89628807333334, 1.0, 2.0, 0.440634698466415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 631569.2502596813, 631569.2502596818, 176861.7208614246]
[2019-03-27 05:01:22,370] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:01:22,372] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.2535064e-12 1.0000000e+00 5.2839134e-19 5.8027771e-12 2.0435710e-15], sampled 0.8930789585173287
[2019-03-27 05:01:33,587] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033503402]
[2019-03-27 05:01:33,589] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.77447453, 58.5429552, 1.0, 2.0, 0.7210878599282079, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.996022585943309, 6.9112, 168.9123755160181, 1904638.58623682, 1844462.731382617, 389666.6414589692]
[2019-03-27 05:01:33,589] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:01:33,592] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6988008e-11 1.0000000e+00 7.6367796e-19 6.8538335e-12 2.8462365e-15], sampled 0.46845114042631764
[2019-03-27 05:01:33,593] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1904638.58623682 W.
[2019-03-27 05:01:38,874] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.033503402]
[2019-03-27 05:01:38,877] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.33333333333333, 73.66666666666667, 1.0, 2.0, 0.6003760123385354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 838987.5908322779, 838987.5908322772, 200816.3024063902]
[2019-03-27 05:01:38,878] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:01:38,881] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2693216e-13 1.0000000e+00 3.9698457e-22 1.0765095e-13 6.2319866e-18], sampled 0.8667014828884851
[2019-03-27 05:02:31,685] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 05:02:31,766] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9447 2927326650.9760 1338.0000
[2019-03-27 05:02:31,913] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7281 2779199535.5119 933.0000
[2019-03-27 05:02:31,949] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1204 3007777057.7724 1766.0000
[2019-03-27 05:02:32,069] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8618 2842441618.2790 1131.0000
[2019-03-27 05:02:33,083] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 150000, evaluation results [150000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8252.944688853477, 2927326650.975955, 1338.0, 8660.728066421085, 2779199535.5119066, 933.0, 7998.120417058023, 3007777057.7724214, 1766.0, 8496.861836459404, 2842441618.2789946, 1131.0]
[2019-03-27 05:02:35,300] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.4465523e-13 1.0000000e+00 1.8405893e-20 7.6580262e-13 2.7344214e-16], sum to 1.0000
[2019-03-27 05:02:35,311] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8550
[2019-03-27 05:02:35,316] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.75, 65.33333333333334, 1.0, 2.0, 0.5579412373779853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779665.906399961, 779665.9063999616, 193172.5295775913], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5658600.0000, 
sim time next is 5659200.0000, 
raw observation next is [31.8, 65.0, 1.0, 2.0, 0.5583092521566168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780180.358338228, 780180.358338228, 193236.4637166237], 
processed observation next is [0.0, 0.5217391304347826, 0.7061611374407584, 0.65, 1.0, 1.0, 0.4678424724778515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21671676620506333, 0.21671676620506333, 0.2884126324128712], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.6912614], dtype=float32), -0.8089187]. 
=============================================
[2019-03-27 05:02:36,663] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 151684: loss 0.0662
[2019-03-27 05:02:36,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 151684: learning rate 0.0000
[2019-03-27 05:02:36,724] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 151713: loss 0.0828
[2019-03-27 05:02:36,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 151713: learning rate 0.0000
[2019-03-27 05:02:36,748] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 151728: loss 0.0468
[2019-03-27 05:02:36,752] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 151729: learning rate 0.0000
[2019-03-27 05:02:36,885] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 151792: loss 0.0529
[2019-03-27 05:02:36,886] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 151792: learning rate 0.0000
[2019-03-27 05:02:36,920] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 151808: loss 0.0433
[2019-03-27 05:02:36,922] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 151808: learning rate 0.0000
[2019-03-27 05:02:37,099] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 151893: loss 0.0371
[2019-03-27 05:02:37,102] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 151894: learning rate 0.0000
[2019-03-27 05:02:37,307] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 151993: loss 0.0314
[2019-03-27 05:02:37,314] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 151995: learning rate 0.0000
[2019-03-27 05:02:37,346] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152010: loss 0.0239
[2019-03-27 05:02:37,347] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152010: learning rate 0.0000
[2019-03-27 05:02:37,358] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 152014: loss 0.0637
[2019-03-27 05:02:37,359] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 152014: learning rate 0.0000
[2019-03-27 05:02:37,390] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 152025: loss 0.0615
[2019-03-27 05:02:37,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 152027: learning rate 0.0000
[2019-03-27 05:02:37,499] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152079: loss 0.0345
[2019-03-27 05:02:37,501] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152080: learning rate 0.0000
[2019-03-27 05:02:37,627] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 152137: loss 0.0087
[2019-03-27 05:02:37,629] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 152138: learning rate 0.0000
[2019-03-27 05:02:37,675] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 152159: loss 0.0379
[2019-03-27 05:02:37,677] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 152161: learning rate 0.0000
[2019-03-27 05:02:37,719] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 152180: loss 0.0258
[2019-03-27 05:02:37,721] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 152181: learning rate 0.0000
[2019-03-27 05:02:37,978] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 152299: loss 0.0026
[2019-03-27 05:02:37,981] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 152300: loss 0.0433
[2019-03-27 05:02:37,983] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 152300: learning rate 0.0000
[2019-03-27 05:02:37,983] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 152300: learning rate 0.0000
[2019-03-27 05:02:38,862] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8282771e-12 1.0000000e+00 1.9878905e-20 3.6814234e-14 1.3721286e-17], sum to 1.0000
[2019-03-27 05:02:38,872] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4374
[2019-03-27 05:02:38,876] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.36666666666667, 65.66666666666667, 1.0, 2.0, 0.5187050071485437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724818.5745895375, 724818.5745895375, 186578.7846418523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5737200.0000, 
sim time next is 5737800.0000, 
raw observation next is [30.53333333333333, 64.83333333333333, 1.0, 2.0, 0.5189555364224218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725168.7741283949, 725168.7741283949, 186619.4367274168], 
processed observation next is [0.0, 0.391304347826087, 0.646129541864139, 0.6483333333333333, 1.0, 1.0, 0.42042835713544785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2014357705912208, 0.2014357705912208, 0.27853647272748777], 
reward next is 0.7215, 
noisyNet noise sample is [array([0.63096434], dtype=float32), 0.49632168]. 
=============================================
[2019-03-27 05:02:44,776] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3574480e-12 1.0000000e+00 3.3447743e-19 1.1702312e-12 1.6910871e-15], sum to 1.0000
[2019-03-27 05:02:44,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8495
[2019-03-27 05:02:44,790] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2889050.759756842 W.
[2019-03-27 05:02:44,914] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.8, 61.33333333333334, 1.0, 2.0, 0.7358322175900829, 1.0, 2.0, 0.6885061483093039, 1.0, 2.0, 1.03, 7.00510055766311, 6.9112, 170.5573041426782, 2889050.759756842, 2821785.970672283, 532884.8098455379], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5840400.0000, 
sim time next is 5841000.0000, 
raw observation next is [32.75, 61.5, 1.0, 2.0, 0.7426716798051465, 1.0, 2.0, 0.6919258794168357, 1.0, 2.0, 1.03, 7.00510109701949, 6.9112, 170.5573041426782, 2903417.020547639, 2836151.845100165, 535144.8587186998], 
processed observation next is [1.0, 0.6086956521739131, 0.7511848341232228, 0.615, 1.0, 1.0, 0.689965879283309, 1.0, 1.0, 0.6288263607431755, 1.0, 1.0, 1.0365853658536586, 0.009390109701949001, 0.0, 0.8375144448122397, 0.8065047279298998, 0.787819956972268, 0.7987236697294027], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.31332105], dtype=float32), 0.37247726]. 
=============================================
[2019-03-27 05:02:44,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[54.71782 ]
 [55.124474]
 [54.69222 ]
 [55.17719 ]
 [55.560337]], R is [[52.76685715]
 [52.23918915]
 [51.71679688]
 [51.19963074]
 [50.68763351]].
[2019-03-27 05:02:53,676] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 159750: loss -255.7079
[2019-03-27 05:02:53,678] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 159751: learning rate 0.0000
[2019-03-27 05:02:53,708] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 159768: loss -301.5426
[2019-03-27 05:02:53,709] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 159768: learning rate 0.0000
[2019-03-27 05:02:53,761] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 159799: loss -287.7036
[2019-03-27 05:02:53,762] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 159799: learning rate 0.0000
[2019-03-27 05:02:53,819] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 159838: loss -339.3972
[2019-03-27 05:02:53,820] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 159838: loss -319.7443
[2019-03-27 05:02:53,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 159838: learning rate 0.0000
[2019-03-27 05:02:53,822] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 159838: learning rate 0.0000
[2019-03-27 05:02:53,957] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159930: loss -274.1343
[2019-03-27 05:02:53,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159930: learning rate 0.0000
[2019-03-27 05:02:54,001] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 159957: loss -257.0219
[2019-03-27 05:02:54,002] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 159957: learning rate 0.0000
[2019-03-27 05:02:54,012] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 159962: loss -347.6800
[2019-03-27 05:02:54,012] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 159962: learning rate 0.0000
[2019-03-27 05:02:54,101] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 160023: loss -276.8373
[2019-03-27 05:02:54,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 160023: learning rate 0.0000
[2019-03-27 05:02:54,125] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 160038: loss -339.2136
[2019-03-27 05:02:54,127] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 160039: learning rate 0.0000
[2019-03-27 05:02:54,225] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160093: loss -260.3024
[2019-03-27 05:02:54,227] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160093: learning rate 0.0000
[2019-03-27 05:02:54,273] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 160122: loss -173.5229
[2019-03-27 05:02:54,273] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 160122: learning rate 0.0000
[2019-03-27 05:02:54,286] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 160125: loss -328.2950
[2019-03-27 05:02:54,288] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 160127: learning rate 0.0000
[2019-03-27 05:02:54,304] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 160133: loss -288.5956
[2019-03-27 05:02:54,305] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 160133: learning rate 0.0000
[2019-03-27 05:02:54,534] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 160215: loss -278.6109
[2019-03-27 05:02:54,537] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 160216: learning rate 0.0000
[2019-03-27 05:02:54,558] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 160225: loss -269.2437
[2019-03-27 05:02:54,559] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 160225: learning rate 0.0000
[2019-03-27 05:02:57,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1005527e-15 1.0000000e+00 2.5397272e-22 5.6431507e-14 6.8009814e-18], sum to 1.0000
[2019-03-27 05:02:57,541] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4718
[2019-03-27 05:02:57,548] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 92.0, 1.0, 2.0, 0.5304143210591867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741186.4312685619, 741186.4312685619, 188498.5566707459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6051600.0000, 
sim time next is 6052200.0000, 
raw observation next is [26.36666666666667, 92.16666666666667, 1.0, 2.0, 0.5297481658025573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740255.2398190206, 740255.2398190206, 188388.2603690794], 
processed observation next is [1.0, 0.043478260869565216, 0.4486571879936811, 0.9216666666666667, 1.0, 1.0, 0.43343152506332205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2056264555052835, 0.2056264555052835, 0.2811765080135513], 
reward next is 0.7188, 
noisyNet noise sample is [array([0.0739834], dtype=float32), 0.71482]. 
=============================================
[2019-03-27 05:02:58,001] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0193786e-12 1.0000000e+00 2.5504386e-20 2.0670520e-12 2.8437702e-17], sum to 1.0000
[2019-03-27 05:02:58,010] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9219
[2019-03-27 05:02:58,019] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.6993744570760846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 977395.3280066288, 977395.3280066282, 220709.5056729922], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6065400.0000, 
sim time next is 6066000.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.6735150494075706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 941240.0690682312, 941240.0690682305, 215231.6120407878], 
processed observation next is [1.0, 0.21739130434782608, 0.4360189573459717, 0.93, 1.0, 1.0, 0.6066446378404464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2614555747411753, 0.26145557474117515, 0.3212412120011758], 
reward next is 0.6788, 
noisyNet noise sample is [array([0.8385649], dtype=float32), 1.0944898]. 
=============================================
[2019-03-27 05:02:58,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[63.725674]
 [63.67145 ]
 [63.676235]
 [63.680286]
 [63.70729 ]], R is [[63.91547394]
 [63.94690323]
 [63.97348785]
 [63.99430084]
 [64.02986145]].
[2019-03-27 05:03:11,010] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 167684: loss 0.1561
[2019-03-27 05:03:11,012] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 167684: learning rate 0.0000
[2019-03-27 05:03:11,095] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 167716: loss 0.1175
[2019-03-27 05:03:11,097] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 167717: learning rate 0.0000
[2019-03-27 05:03:11,153] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 167744: loss 0.1248
[2019-03-27 05:03:11,155] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 167744: learning rate 0.0000
[2019-03-27 05:03:11,219] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167772: loss 0.1495
[2019-03-27 05:03:11,231] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167775: learning rate 0.0000
[2019-03-27 05:03:11,240] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 167781: loss 0.1443
[2019-03-27 05:03:11,243] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 167781: learning rate 0.0000
[2019-03-27 05:03:11,553] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 167923: loss 0.0712
[2019-03-27 05:03:11,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 167923: learning rate 0.0000
[2019-03-27 05:03:11,660] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 167975: loss 0.1061
[2019-03-27 05:03:11,668] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 167979: learning rate 0.0000
[2019-03-27 05:03:11,670] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 167979: loss 0.1762
[2019-03-27 05:03:11,674] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 167979: learning rate 0.0000
[2019-03-27 05:03:11,691] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 167988: loss 0.1055
[2019-03-27 05:03:11,694] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 167988: learning rate 0.0000
[2019-03-27 05:03:11,720] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168000: loss 0.1341
[2019-03-27 05:03:11,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168000: learning rate 0.0000
[2019-03-27 05:03:11,983] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 168120: loss 0.0795
[2019-03-27 05:03:11,987] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 168120: learning rate 0.0000
[2019-03-27 05:03:12,100] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168173: loss 0.1521
[2019-03-27 05:03:12,103] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168173: learning rate 0.0000
[2019-03-27 05:03:12,119] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 168181: loss 0.0692
[2019-03-27 05:03:12,122] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 168182: learning rate 0.0000
[2019-03-27 05:03:12,142] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 168191: loss 0.0983
[2019-03-27 05:03:12,144] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 168191: learning rate 0.0000
[2019-03-27 05:03:12,269] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 168248: loss 0.0755
[2019-03-27 05:03:12,271] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 168248: learning rate 0.0000
[2019-03-27 05:03:12,397] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 168305: loss 0.0556
[2019-03-27 05:03:12,402] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 168306: learning rate 0.0000
[2019-03-27 05:03:16,516] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4024827e-14 1.0000000e+00 9.9296308e-25 4.0371893e-17 6.3967525e-20], sum to 1.0000
[2019-03-27 05:03:16,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5445
[2019-03-27 05:03:16,527] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 77.0, 1.0, 2.0, 0.522073797086498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729527.6095259741, 729527.6095259741, 187126.5448271005], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6373800.0000, 
sim time next is 6374400.0000, 
raw observation next is [28.23333333333333, 77.66666666666667, 1.0, 2.0, 0.5243435756053749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 732700.4122814663, 732700.4122814657, 187497.7452966479], 
processed observation next is [0.0, 0.782608695652174, 0.537124802527646, 0.7766666666666667, 1.0, 1.0, 0.4269199706088854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2035278923004073, 0.20352789230040713, 0.27984738103977297], 
reward next is 0.7202, 
noisyNet noise sample is [array([-2.5379066], dtype=float32), -0.3397322]. 
=============================================
[2019-03-27 05:03:27,180] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 05:03:27,182] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:03:27,183] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:03:27,183] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:03:27,186] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:03:27,186] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:03:27,184] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:03:27,187] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:03:27,191] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:03:27,193] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:03:27,193] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:03:27,210] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run8
[2019-03-27 05:03:27,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run8
[2019-03-27 05:03:27,244] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run8
[2019-03-27 05:03:27,245] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run8
[2019-03-27 05:03:27,245] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run8
[2019-03-27 05:03:40,176] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035353176]
[2019-03-27 05:03:40,180] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.53333333333333, 96.0, 1.0, 2.0, 0.3892447251647098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 585429.902525762, 585429.9025257614, 173339.4165852518]
[2019-03-27 05:03:40,182] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:03:40,185] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.9920063e-14 1.0000000e+00 2.3848520e-22 2.1272619e-14 2.5369243e-18], sampled 0.964108877559742
[2019-03-27 05:03:59,629] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035353176]
[2019-03-27 05:03:59,631] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.15, 94.5, 1.0, 2.0, 0.562948585415229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786665.7550940972, 786665.7550940979, 194041.5687225549]
[2019-03-27 05:03:59,632] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:03:59,636] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.7546371e-14 1.0000000e+00 1.9591457e-22 1.8440280e-14 2.1935074e-18], sampled 0.9117357693272986
[2019-03-27 05:04:00,084] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035353176]
[2019-03-27 05:04:00,086] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.09779958333333, 80.29434389, 1.0, 2.0, 0.7116761124272496, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005976409447884, 6.9112, 168.9116092681413, 1891467.943328253, 1824230.832798159, 387200.7563238172]
[2019-03-27 05:04:00,087] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:04:00,090] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.4124186e-14 1.0000000e+00 5.7123457e-23 8.5104636e-15 8.9104030e-19], sampled 0.280234887201418
[2019-03-27 05:04:00,091] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1891467.943328253 W.
[2019-03-27 05:04:00,466] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035353176]
[2019-03-27 05:04:00,466] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.3, 82.33333333333334, 1.0, 2.0, 0.5445081688488133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 760887.8415388272, 760887.8415388279, 190863.5972684024]
[2019-03-27 05:04:00,468] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:04:00,472] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4247235e-14 1.0000000e+00 2.1879529e-23 7.4737037e-15 2.8652293e-19], sampled 0.4125103199594844
[2019-03-27 05:04:42,341] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035353176]
[2019-03-27 05:04:42,342] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.355767375, 58.52188377, 1.0, 2.0, 0.4036003734967281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594711.694154626, 594711.6941546266, 173825.4527671436]
[2019-03-27 05:04:42,343] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:04:42,347] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2975329e-13 1.0000000e+00 5.1447948e-22 3.2434765e-14 4.9437111e-18], sampled 0.28573597979291454
[2019-03-27 05:04:46,488] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035353176]
[2019-03-27 05:04:46,491] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.09208002833333, 69.24526831333333, 1.0, 2.0, 1.035805716051217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912756227813, 1447887.986645726, 1447887.986645725, 310032.7391406979]
[2019-03-27 05:04:46,492] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:04:46,495] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.0930048e-14 1.0000000e+00 7.3169676e-23 8.7939223e-15 1.1741851e-18], sampled 0.0750643689887851
[2019-03-27 05:04:59,309] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035353176]
[2019-03-27 05:04:59,310] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.6, 87.0, 1.0, 2.0, 0.5935798111127145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 829486.6199834307, 829486.6199834314, 199555.2491812233]
[2019-03-27 05:04:59,312] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:04:59,315] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.5224146e-14 1.0000000e+00 1.9052614e-22 1.7476197e-14 1.9815480e-18], sampled 0.5207156215715719
[2019-03-27 05:05:12,530] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035353176]
[2019-03-27 05:05:12,532] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.37429901166666, 97.50610709666668, 1.0, 2.0, 0.2865824765677171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 464824.2121627074, 464824.2121627074, 164577.5443416063]
[2019-03-27 05:05:12,533] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:05:12,536] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.9826213e-13 1.0000000e+00 5.9788704e-21 1.5410348e-13 3.7027129e-17], sampled 0.8992488694162886
[2019-03-27 05:05:19,101] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035353176]
[2019-03-27 05:05:19,102] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.14203241333333, 62.71751087333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.008469572377579, 6.9112, 168.9123363819751, 1522808.31395889, 1453802.184896365, 311349.7676181328]
[2019-03-27 05:05:19,106] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:05:19,108] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.4983145e-14 1.0000000e+00 1.4247231e-22 1.5242051e-14 1.5737707e-18], sampled 0.20719143241079507
[2019-03-27 05:05:21,168] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4596 3007684600.1831 1766.0000
[2019-03-27 05:05:21,932] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.9093 3164121676.4512 1778.0000
[2019-03-27 05:05:21,968] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8612 2842434274.6064 1131.0000
[2019-03-27 05:05:22,004] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9509 2779224265.4428 933.0000
[2019-03-27 05:05:22,043] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 05:05:23,056] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 175000, evaluation results [175000.0, 7881.909338666438, 3164121676.451171, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8659.950916828071, 2779224265.4427943, 933.0, 7997.459639405762, 3007684600.183132, 1766.0, 8496.8611576407, 2842434274.606407, 1131.0]
[2019-03-27 05:05:24,564] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 175710: loss -404.8210
[2019-03-27 05:05:24,568] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 175710: learning rate 0.0000
[2019-03-27 05:05:24,608] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 175731: loss -494.6940
[2019-03-27 05:05:24,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 175731: learning rate 0.0000
[2019-03-27 05:05:24,685] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 175766: loss -394.7703
[2019-03-27 05:05:24,686] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 175767: learning rate 0.0000
[2019-03-27 05:05:24,713] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 175779: loss -478.5113
[2019-03-27 05:05:24,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 175779: learning rate 0.0000
[2019-03-27 05:05:24,909] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 175873: loss -463.7471
[2019-03-27 05:05:24,911] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 175874: learning rate 0.0000
[2019-03-27 05:05:24,998] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 175912: loss -432.5821
[2019-03-27 05:05:25,001] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 175912: learning rate 0.0000
[2019-03-27 05:05:25,068] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 175947: loss -387.2612
[2019-03-27 05:05:25,070] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 175947: learning rate 0.0000
[2019-03-27 05:05:25,105] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 175965: loss -465.7046
[2019-03-27 05:05:25,110] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 175965: learning rate 0.0000
[2019-03-27 05:05:25,159] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 175987: loss -433.1859
[2019-03-27 05:05:25,162] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 175988: learning rate 0.0000
[2019-03-27 05:05:25,295] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176052: loss -399.2941
[2019-03-27 05:05:25,297] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176052: learning rate 0.0000
[2019-03-27 05:05:25,399] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 176099: loss -437.3701
[2019-03-27 05:05:25,402] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 176099: learning rate 0.0000
[2019-03-27 05:05:25,454] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 176126: loss -441.9734
[2019-03-27 05:05:25,456] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 176126: learning rate 0.0000
[2019-03-27 05:05:25,468] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 176133: loss -444.5100
[2019-03-27 05:05:25,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 176133: learning rate 0.0000
[2019-03-27 05:05:25,512] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 176149: loss -469.4504
[2019-03-27 05:05:25,513] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 176149: learning rate 0.0000
[2019-03-27 05:05:25,601] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 176194: loss -440.0163
[2019-03-27 05:05:25,605] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 176196: learning rate 0.0000
[2019-03-27 05:05:25,701] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 176239: loss -445.7003
[2019-03-27 05:05:25,705] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 176239: learning rate 0.0000
[2019-03-27 05:05:34,260] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3549389e-12 1.0000000e+00 3.0511116e-21 3.9338121e-13 9.5203195e-17], sum to 1.0000
[2019-03-27 05:05:34,270] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6740
[2019-03-27 05:05:34,275] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.41666666666667, 82.33333333333334, 1.0, 2.0, 0.3960332664974838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627250.7252275207, 627250.7252275207, 177594.4617805309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6761400.0000, 
sim time next is 6762000.0000, 
raw observation next is [22.53333333333333, 81.66666666666667, 1.0, 2.0, 0.3487074423974256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 551949.6496468761, 551949.6496468768, 171103.700153481], 
processed observation next is [1.0, 0.2608695652173913, 0.26698262243285936, 0.8166666666666668, 1.0, 1.0, 0.2153101715631634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15331934712413225, 0.15331934712413245, 0.25537865694549405], 
reward next is 0.7446, 
noisyNet noise sample is [array([0.75080353], dtype=float32), 2.5827522]. 
=============================================
[2019-03-27 05:05:34,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[65.32908 ]
 [65.236946]
 [65.144005]
 [65.13933 ]
 [65.14829 ]], R is [[65.48443604]
 [65.56452179]
 [65.64886475]
 [65.73971558]
 [65.83001709]].
[2019-03-27 05:05:39,005] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5089991e-13 1.0000000e+00 3.8294484e-21 9.9610858e-13 7.0404929e-17], sum to 1.0000
[2019-03-27 05:05:39,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8182
[2019-03-27 05:05:39,015] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 84.0, 1.0, 2.0, 0.3425703354289968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532022.6739180152, 532022.6739180159, 169290.5005890343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6843600.0000, 
sim time next is 6844200.0000, 
raw observation next is [23.1, 83.66666666666667, 1.0, 2.0, 0.3446965865270571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534664.7639009362, 534664.7639009355, 169486.1890551904], 
processed observation next is [0.0, 0.21739130434782608, 0.2938388625592418, 0.8366666666666667, 1.0, 1.0, 0.21047781509283986, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14851798997248228, 0.14851798997248208, 0.2529644612764036], 
reward next is 0.7470, 
noisyNet noise sample is [array([-1.5760664], dtype=float32), -0.11720285]. 
=============================================
[2019-03-27 05:05:41,513] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 183691: loss 0.3589
[2019-03-27 05:05:41,515] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 183692: learning rate 0.0000
[2019-03-27 05:05:41,607] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 183734: loss 0.4192
[2019-03-27 05:05:41,612] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 183735: learning rate 0.0000
[2019-03-27 05:05:41,663] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 183762: loss 0.4607
[2019-03-27 05:05:41,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 183762: learning rate 0.0000
[2019-03-27 05:05:41,712] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 183784: loss 0.2821
[2019-03-27 05:05:41,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 183784: learning rate 0.0000
[2019-03-27 05:05:41,763] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 183807: loss 0.3121
[2019-03-27 05:05:41,770] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 183807: learning rate 0.0000
[2019-03-27 05:05:41,834] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9750285e-15 1.0000000e+00 1.0757081e-22 1.6797273e-14 1.1597707e-18], sum to 1.0000
[2019-03-27 05:05:41,843] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2073
[2019-03-27 05:05:41,847] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.13333333333333, 55.66666666666667, 1.0, 2.0, 0.3564536971821201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546862.1566660238, 546862.1566660238, 170319.1914857418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6891600.0000, 
sim time next is 6892200.0000, 
raw observation next is [28.0, 56.5, 1.0, 2.0, 0.3590881788372061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 550403.7658441856, 550403.765844185, 170601.182203602], 
processed observation next is [0.0, 0.782608695652174, 0.5260663507109005, 0.565, 1.0, 1.0, 0.2278170829363929, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1528899349567182, 0.15288993495671807, 0.25462863015462983], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.9582183], dtype=float32), 0.62956476]. 
=============================================
[2019-03-27 05:05:41,884] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 183863: loss 0.3104
[2019-03-27 05:05:41,886] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 183864: learning rate 0.0000
[2019-03-27 05:05:41,923] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 183880: loss 0.3230
[2019-03-27 05:05:41,926] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 183881: learning rate 0.0000
[2019-03-27 05:05:42,064] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 183948: loss 0.3363
[2019-03-27 05:05:42,066] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 183949: learning rate 0.0000
[2019-03-27 05:05:42,153] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 183985: loss 0.3525
[2019-03-27 05:05:42,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 183986: learning rate 0.0000
[2019-03-27 05:05:42,272] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184044: loss 0.2150
[2019-03-27 05:05:42,274] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184044: learning rate 0.0000
[2019-03-27 05:05:42,432] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 184122: loss 0.3855
[2019-03-27 05:05:42,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 184124: learning rate 0.0000
[2019-03-27 05:05:42,536] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 184168: loss 0.4569
[2019-03-27 05:05:42,539] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 184170: learning rate 0.0000
[2019-03-27 05:05:42,588] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184193: loss 0.3732
[2019-03-27 05:05:42,591] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184194: learning rate 0.0000
[2019-03-27 05:05:42,631] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 184214: loss 0.3311
[2019-03-27 05:05:42,633] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 184215: learning rate 0.0000
[2019-03-27 05:05:42,764] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 184275: loss 0.2715
[2019-03-27 05:05:42,767] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 184277: learning rate 0.0000
[2019-03-27 05:05:42,828] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 184302: loss 0.2576
[2019-03-27 05:05:42,830] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 184302: learning rate 0.0000
[2019-03-27 05:05:45,855] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7508469e-15 1.0000000e+00 2.4123079e-24 2.7776957e-15 2.3198455e-20], sum to 1.0000
[2019-03-27 05:05:45,862] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4112
[2019-03-27 05:05:45,866] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.51666666666667, 57.83333333333334, 1.0, 2.0, 0.4310396285639245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 625009.0755310854, 625009.075531086, 176413.4769398913], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6977400.0000, 
sim time next is 6978000.0000, 
raw observation next is [29.43333333333334, 57.66666666666667, 1.0, 2.0, 0.4269511401382279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 621593.6425464626, 621593.6425464626, 176150.6303184638], 
processed observation next is [0.0, 0.782608695652174, 0.5939968404423385, 0.5766666666666667, 1.0, 1.0, 0.30957968691352755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17266490070735072, 0.17266490070735072, 0.2629113885350206], 
reward next is 0.7371, 
noisyNet noise sample is [array([-0.22436443], dtype=float32), 0.3272889]. 
=============================================
[2019-03-27 05:05:45,880] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.54015 ]
 [78.501564]
 [78.46697 ]
 [78.4399  ]
 [78.403145]], R is [[78.53110504]
 [78.48249054]
 [78.43317413]
 [78.38361359]
 [78.33392334]].
[2019-03-27 05:05:50,489] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5368802e-11 1.0000000e+00 1.7658910e-18 2.3703478e-12 4.2416059e-16], sum to 1.0000
[2019-03-27 05:05:50,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5973
[2019-03-27 05:05:50,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1704745.794043347 W.
[2019-03-27 05:05:50,506] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.6, 47.66666666666667, 1.0, 2.0, 0.5874178230848089, 0.0, 2.0, 0.0, 1.0, 1.0, 1.003100750640886, 6.9112, 6.9112, 168.9126696020556, 1704745.794043347, 1704745.794043347, 360548.7484547918], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7044600.0000, 
sim time next is 7045200.0000, 
raw observation next is [31.7, 47.0, 1.0, 2.0, 0.4183599213796342, 1.0, 1.0, 0.4183599213796342, 1.0, 2.0, 0.7017193552488974, 6.9112, 6.9112, 170.5573041426782, 1774709.019979654, 1774709.019979654, 361218.7497193368], 
processed observation next is [1.0, 0.5652173913043478, 0.7014218009478673, 0.47, 1.0, 1.0, 0.2992288209393183, 1.0, 0.5, 0.2992288209393183, 1.0, 1.0, 0.636243116157192, 0.0, 0.0, 0.8375144448122397, 0.49297472777212614, 0.49297472777212614, 0.5391324622676669], 
reward next is 0.4609, 
noisyNet noise sample is [array([-2.1366858], dtype=float32), 1.4959409]. 
=============================================
[2019-03-27 05:05:58,359] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5233977e-14 1.0000000e+00 1.9624977e-21 3.6303673e-14 6.3768506e-19], sum to 1.0000
[2019-03-27 05:05:58,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9075
[2019-03-27 05:05:58,371] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 88.0, 1.0, 2.0, 0.5673907862701171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792875.6144799187, 792875.6144799181, 194825.0522412113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7194000.0000, 
sim time next is 7194600.0000, 
raw observation next is [27.0, 87.5, 1.0, 2.0, 0.5667284434632494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791949.7072414268, 791949.7072414268, 194708.3262203799], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.875, 1.0, 1.0, 0.4779860764617462, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21998602978928522, 0.21998602978928522, 0.29060944211997003], 
reward next is 0.7094, 
noisyNet noise sample is [array([-0.14939784], dtype=float32), 1.1359067]. 
=============================================
[2019-03-27 05:05:58,660] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 191683: loss 0.0563
[2019-03-27 05:05:58,663] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 191683: learning rate 0.0000
[2019-03-27 05:05:58,800] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191744: loss 0.0147
[2019-03-27 05:05:58,803] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191744: learning rate 0.0000
[2019-03-27 05:05:58,837] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 191760: loss 0.0338
[2019-03-27 05:05:58,841] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 191760: learning rate 0.0000
[2019-03-27 05:05:58,890] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 191782: loss 0.0326
[2019-03-27 05:05:58,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 191782: learning rate 0.0000
[2019-03-27 05:05:59,106] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 191880: loss 0.0140
[2019-03-27 05:05:59,107] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 191880: loss 0.0496
[2019-03-27 05:05:59,111] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 191880: learning rate 0.0000
[2019-03-27 05:05:59,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 191883: learning rate 0.0000
[2019-03-27 05:05:59,126] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 191888: loss 0.0349
[2019-03-27 05:05:59,128] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 191890: learning rate 0.0000
[2019-03-27 05:05:59,224] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 191936: loss 0.0460
[2019-03-27 05:05:59,226] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 191936: learning rate 0.0000
[2019-03-27 05:05:59,350] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 191988: loss 0.0404
[2019-03-27 05:05:59,354] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 191992: learning rate 0.0000
[2019-03-27 05:05:59,397] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192014: loss 0.0236
[2019-03-27 05:05:59,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192014: learning rate 0.0000
[2019-03-27 05:05:59,481] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 192050: loss 0.0310
[2019-03-27 05:05:59,485] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 192050: learning rate 0.0000
[2019-03-27 05:05:59,599] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 192103: loss 0.0224
[2019-03-27 05:05:59,600] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 192104: learning rate 0.0000
[2019-03-27 05:05:59,664] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 192132: loss 0.0154
[2019-03-27 05:05:59,666] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 192132: learning rate 0.0000
[2019-03-27 05:05:59,753] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 192172: loss 0.0275
[2019-03-27 05:05:59,757] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 192174: learning rate 0.0000
[2019-03-27 05:05:59,908] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 192240: loss 0.0246
[2019-03-27 05:05:59,910] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 192241: learning rate 0.0000
[2019-03-27 05:05:59,964] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 192265: loss 0.0728
[2019-03-27 05:05:59,967] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 192265: learning rate 0.0000
[2019-03-27 05:06:08,882] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6261049e-14 1.0000000e+00 1.5759301e-22 6.4447539e-14 4.7470717e-18], sum to 1.0000
[2019-03-27 05:06:08,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4535
[2019-03-27 05:06:08,899] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 90.0, 1.0, 2.0, 0.3846878198389174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 593244.4596555493, 593244.4596555493, 174411.0307059647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7365600.0000, 
sim time next is 7366200.0000, 
raw observation next is [22.21666666666667, 90.16666666666667, 1.0, 2.0, 0.369317293870904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572208.7360456684, 572208.7360456684, 172614.1195678542], 
processed observation next is [1.0, 0.2608695652173913, 0.2519747235387047, 0.9016666666666667, 1.0, 1.0, 0.2401413179167518, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15894687112379677, 0.15894687112379677, 0.25763301428037944], 
reward next is 0.7424, 
noisyNet noise sample is [array([1.4275262], dtype=float32), -0.3827274]. 
=============================================
[2019-03-27 05:06:12,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.6351465e-13 1.0000000e+00 1.0841911e-21 5.6399120e-14 4.7014771e-18], sum to 1.0000
[2019-03-27 05:06:12,775] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1466
[2019-03-27 05:06:12,782] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 92.66666666666666, 1.0, 2.0, 0.3170576002367826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500480.1425461788, 500480.1425461788, 167043.2197491211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7432800.0000, 
sim time next is 7433400.0000, 
raw observation next is [21.26666666666667, 92.33333333333333, 1.0, 2.0, 0.3164641826768507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499628.4358157842, 499628.4358157848, 166981.1764986522], 
processed observation next is [0.0, 0.0, 0.2069510268562403, 0.9233333333333333, 1.0, 1.0, 0.17646287069500086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13878567661549562, 0.13878567661549576, 0.24922563656515254], 
reward next is 0.7508, 
noisyNet noise sample is [array([0.5613426], dtype=float32), 0.23804256]. 
=============================================
[2019-03-27 05:06:16,104] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8896799e-15 1.0000000e+00 1.0618464e-22 2.8767472e-14 1.0669076e-18], sum to 1.0000
[2019-03-27 05:06:16,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1020
[2019-03-27 05:06:16,117] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 75.5, 1.0, 2.0, 0.4208928791457533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610439.0602725557, 610439.0602725557, 175008.0735928292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7489800.0000, 
sim time next is 7490400.0000, 
raw observation next is [26.46666666666667, 75.33333333333334, 1.0, 2.0, 0.4226127910491406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 612005.3843742938, 612005.3843742944, 175130.3512210485], 
processed observation next is [0.0, 0.6956521739130435, 0.45339652448657203, 0.7533333333333334, 1.0, 1.0, 0.3043527603001694, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17000149565952608, 0.17000149565952624, 0.2613885839120127], 
reward next is 0.7386, 
noisyNet noise sample is [array([0.87536806], dtype=float32), -0.7392003]. 
=============================================
[2019-03-27 05:06:16,320] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199682: loss 0.0117
[2019-03-27 05:06:16,322] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199683: learning rate 0.0000
[2019-03-27 05:06:16,326] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 199684: loss 0.0363
[2019-03-27 05:06:16,328] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 199684: learning rate 0.0000
[2019-03-27 05:06:16,424] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 199728: loss 0.0227
[2019-03-27 05:06:16,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 199729: learning rate 0.0000
[2019-03-27 05:06:16,673] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 199843: loss 0.0107
[2019-03-27 05:06:16,675] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 199843: learning rate 0.0000
[2019-03-27 05:06:16,707] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 199856: loss 0.0296
[2019-03-27 05:06:16,709] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 199856: learning rate 0.0000
[2019-03-27 05:06:16,728] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 199864: loss 0.0209
[2019-03-27 05:06:16,731] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 199864: learning rate 0.0000
[2019-03-27 05:06:16,802] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 199898: loss 0.0101
[2019-03-27 05:06:16,804] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 199898: learning rate 0.0000
[2019-03-27 05:06:17,023] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 05:06:17,026] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:06:17,028] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:06:17,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:06:17,029] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:06:17,029] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:06:17,030] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:06:17,032] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:06:17,031] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:06:17,035] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:06:17,037] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:06:17,047] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run9
[2019-03-27 05:06:17,048] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run9
[2019-03-27 05:06:17,096] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run9
[2019-03-27 05:06:17,097] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run9
[2019-03-27 05:06:17,132] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run9
[2019-03-27 05:06:21,898] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03691508]
[2019-03-27 05:06:21,899] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.16868614333333, 90.31698352000001, 1.0, 2.0, 0.3547257374806767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539564.2998532534, 539564.2998532534, 169561.2185195637]
[2019-03-27 05:06:21,900] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:06:21,903] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8064084e-15 1.0000000e+00 3.3658210e-24 9.9729522e-16 6.0924948e-20], sampled 0.683956141632165
[2019-03-27 05:06:30,226] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03691508]
[2019-03-27 05:06:30,228] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.15, 95.5, 1.0, 2.0, 0.373675692761721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569941.3383510178, 569941.3383510178, 172195.2753665776]
[2019-03-27 05:06:30,229] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:06:30,232] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.2560843e-15 1.0000000e+00 7.6082645e-24 1.7317316e-15 1.2173636e-19], sampled 0.3576655907641837
[2019-03-27 05:07:06,147] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03691508]
[2019-03-27 05:07:06,151] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.618163935, 82.510626045, 1.0, 2.0, 0.50243375390885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702074.2327782661, 702074.2327782655, 183977.3909011964]
[2019-03-27 05:07:06,152] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:07:06,154] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3871645e-14 1.0000000e+00 4.0443447e-23 4.7373282e-15 4.8054820e-19], sampled 0.803810209003006
[2019-03-27 05:07:37,214] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03691508]
[2019-03-27 05:07:37,215] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.20995286, 51.59198354, 1.0, 2.0, 0.9587094135363573, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564503125, 1340051.869924022, 1340051.869924021, 286594.0858764286]
[2019-03-27 05:07:37,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:07:37,221] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8488449e-14 1.0000000e+00 3.4462585e-23 4.6410240e-15 2.6803529e-19], sampled 0.7068987174109478
[2019-03-27 05:07:49,865] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03691508]
[2019-03-27 05:07:49,867] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.40977096, 69.66635115666668, 1.0, 2.0, 0.5167148468882607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722036.6554664708, 722036.6554664708, 186256.1269831349]
[2019-03-27 05:07:49,868] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:07:49,870] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1923784e-14 1.0000000e+00 3.6439202e-23 4.2375004e-15 4.3168181e-19], sampled 0.7856657179190216
[2019-03-27 05:08:06,813] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03691508]
[2019-03-27 05:08:06,814] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.20811737666666, 63.11931239333333, 1.0, 2.0, 0.3484078956830046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540276.3655134988, 540276.3655134988, 169939.1314423248]
[2019-03-27 05:08:06,814] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:08:06,821] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1564095e-14 1.0000000e+00 3.2644333e-23 4.0756151e-15 4.0860212e-19], sampled 0.9759065921983358
[2019-03-27 05:08:10,444] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9447 2927328032.1726 1338.0000
[2019-03-27 05:08:10,987] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9775 2779206587.5399 933.0000
[2019-03-27 05:08:11,289] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 05:08:11,417] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-27 05:08:11,436] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0954 2842517464.9746 1131.0000
[2019-03-27 05:08:12,451] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 200000, evaluation results [200000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8252.944690175233, 2927328032.1725616, 1338.0, 8659.977453803283, 2779206587.539943, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.095384209508, 2842517464.974608, 1131.0]
[2019-03-27 05:08:12,488] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200019: loss 0.0021
[2019-03-27 05:08:12,492] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200020: learning rate 0.0000
[2019-03-27 05:08:12,513] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 200030: loss 0.0156
[2019-03-27 05:08:12,515] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200030: loss 0.0169
[2019-03-27 05:08:12,518] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 200030: learning rate 0.0000
[2019-03-27 05:08:12,519] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200030: learning rate 0.0000
[2019-03-27 05:08:12,596] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 200064: loss 0.0136
[2019-03-27 05:08:12,598] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 200065: learning rate 0.0000
[2019-03-27 05:08:12,724] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 200126: loss 0.0020
[2019-03-27 05:08:12,726] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 200126: learning rate 0.0000
[2019-03-27 05:08:12,911] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 200214: loss 0.0008
[2019-03-27 05:08:12,914] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 200217: learning rate 0.0000
[2019-03-27 05:08:12,950] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 200233: loss 0.0170
[2019-03-27 05:08:12,952] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 200233: learning rate 0.0000
[2019-03-27 05:08:12,959] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200235: loss 0.0000
[2019-03-27 05:08:12,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200235: learning rate 0.0000
[2019-03-27 05:08:13,099] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 200304: loss 0.0004
[2019-03-27 05:08:13,102] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 200305: learning rate 0.0000
[2019-03-27 05:08:23,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6633098e-14 1.0000000e+00 7.4726509e-22 1.8472914e-14 3.1671673e-19], sum to 1.0000
[2019-03-27 05:08:23,835] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1066
[2019-03-27 05:08:23,841] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4832435453360206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679215.9111065073, 679215.9111065079, 181522.7966113308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7707000.0000, 
sim time next is 7707600.0000, 
raw observation next is [24.5, 94.0, 1.0, 2.0, 0.4839619989981175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680227.970374999, 680227.9703749996, 181632.8029373225], 
processed observation next is [1.0, 0.21739130434782608, 0.3601895734597157, 0.94, 1.0, 1.0, 0.3782674686724307, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18895221399305528, 0.18895221399305542, 0.271093735727347], 
reward next is 0.7289, 
noisyNet noise sample is [array([0.18657772], dtype=float32), -0.6783854]. 
=============================================
[2019-03-27 05:08:28,562] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207602: loss 0.0362
[2019-03-27 05:08:28,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207603: learning rate 0.0000
[2019-03-27 05:08:28,693] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 207664: loss 0.0291
[2019-03-27 05:08:28,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 207665: learning rate 0.0000
[2019-03-27 05:08:28,794] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 207710: loss 0.0190
[2019-03-27 05:08:28,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 207711: learning rate 0.0000
[2019-03-27 05:08:28,947] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 207779: loss 0.0308
[2019-03-27 05:08:28,950] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 207779: learning rate 0.0000
[2019-03-27 05:08:29,070] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 207839: loss 0.0399
[2019-03-27 05:08:29,073] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 207840: learning rate 0.0000
[2019-03-27 05:08:29,098] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 207851: loss 0.0369
[2019-03-27 05:08:29,100] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 207851: learning rate 0.0000
[2019-03-27 05:08:29,198] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 207899: loss 0.0468
[2019-03-27 05:08:29,200] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 207899: learning rate 0.0000
[2019-03-27 05:08:29,379] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 207982: loss 0.0617
[2019-03-27 05:08:29,381] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 207983: learning rate 0.0000
[2019-03-27 05:08:29,483] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208031: loss 0.0244
[2019-03-27 05:08:29,486] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208031: learning rate 0.0000
[2019-03-27 05:08:29,568] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 208068: loss 0.0465
[2019-03-27 05:08:29,570] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 208069: learning rate 0.0000
[2019-03-27 05:08:29,580] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 208072: loss 0.0362
[2019-03-27 05:08:29,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 208073: learning rate 0.0000
[2019-03-27 05:08:29,681] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 208125: loss 0.0405
[2019-03-27 05:08:29,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 208125: learning rate 0.0000
[2019-03-27 05:08:29,870] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 208214: loss 0.0685
[2019-03-27 05:08:29,871] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 208215: learning rate 0.0000
[2019-03-27 05:08:29,964] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208255: loss 0.0148
[2019-03-27 05:08:29,966] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208256: learning rate 0.0000
[2019-03-27 05:08:29,980] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 208264: loss 0.0230
[2019-03-27 05:08:29,982] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 208264: learning rate 0.0000
[2019-03-27 05:08:30,005] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 208275: loss 0.0533
[2019-03-27 05:08:30,010] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 208276: learning rate 0.0000
[2019-03-27 05:08:30,229] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1223787e-15 1.0000000e+00 8.4049286e-23 2.7607792e-14 8.1790204e-18], sum to 1.0000
[2019-03-27 05:08:30,238] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6061
[2019-03-27 05:08:30,254] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1699596.896003305 W.
[2019-03-27 05:08:30,259] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.41666666666667, 78.33333333333334, 1.0, 2.0, 0.6078659463117041, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.91880344412935, 6.9112, 168.9128653201676, 1699596.896003305, 1694202.753959706, 366820.8984968489], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7809000.0000, 
sim time next is 7809600.0000, 
raw observation next is [28.53333333333334, 77.66666666666667, 1.0, 2.0, 0.6127895261733722, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.927309158907368, 6.9112, 168.9128195017743, 1713374.368423592, 1701945.98593004, 367783.878749861], 
processed observation next is [1.0, 0.391304347826087, 0.5513428120063194, 0.7766666666666667, 1.0, 1.0, 0.5334813568353882, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0016109158907368303, 0.0, 0.8294392723770754, 0.4759373245621089, 0.47276277386945553, 0.5489311623132254], 
reward next is 0.3705, 
noisyNet noise sample is [array([-2.730623], dtype=float32), -0.93496245]. 
=============================================
[2019-03-27 05:08:34,947] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.5488143e-14 1.0000000e+00 7.8105876e-24 1.4777688e-15 2.4274914e-18], sum to 1.0000
[2019-03-27 05:08:34,958] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3714
[2019-03-27 05:08:34,963] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 87.0, 1.0, 2.0, 0.5981795680841518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 835916.9907559324, 835916.9907559331, 200400.9850089962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7884000.0000, 
sim time next is 7884600.0000, 
raw observation next is [26.66666666666667, 86.5, 1.0, 2.0, 0.6033662351347145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 843167.8971018008, 843167.8971018002, 201368.0948255169], 
processed observation next is [1.0, 0.2608695652173913, 0.4628751974723541, 0.865, 1.0, 1.0, 0.5221279941382103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2342133047505002, 0.23421330475050006, 0.3005493952619655], 
reward next is 0.6995, 
noisyNet noise sample is [array([-1.5752527], dtype=float32), 1.052419]. 
=============================================
[2019-03-27 05:08:37,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:37,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:37,730] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-03-27 05:08:37,828] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:37,829] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:37,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-03-27 05:08:37,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:37,941] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:37,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-03-27 05:08:37,975] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:37,976] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:37,977] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-03-27 05:08:38,036] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,038] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-03-27 05:08:38,160] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,161] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,162] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-03-27 05:08:38,192] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,192] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-03-27 05:08:38,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,286] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-03-27 05:08:38,306] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,306] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,308] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-03-27 05:08:38,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-03-27 05:08:38,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,400] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-03-27 05:08:38,444] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,444] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,446] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-03-27 05:08:38,473] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,474] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,475] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-03-27 05:08:38,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,503] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-03-27 05:08:38,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:08:38,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:08:38,525] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-03-27 05:08:38,544] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-03-27 05:08:40,297] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7376443e-03 9.9593580e-01 3.9478677e-05 2.0416768e-03 2.4536575e-04], sum to 1.0000
[2019-03-27 05:08:40,303] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2763
[2019-03-27 05:08:40,305] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 84.0, 1.0, 2.0, 0.2705444592494824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 442899.9157403874, 442899.9157403874, 162995.909252409], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3600.0000, 
sim time next is 4200.0000, 
raw observation next is [20.26666666666667, 84.16666666666667, 1.0, 2.0, 0.2666885580273808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 437109.0718663594, 437109.0718663594, 162598.845847985], 
processed observation next is [1.0, 0.043478260869565216, 0.15955766192733034, 0.8416666666666667, 1.0, 1.0, 0.11649223858720575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12141918662954428, 0.12141918662954428, 0.24268484454923137], 
reward next is 0.7573, 
noisyNet noise sample is [array([-1.0337967], dtype=float32), -1.7299168]. 
=============================================
[2019-03-27 05:08:43,657] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5405008e-12 1.0000000e+00 1.2911336e-21 3.9050584e-14 2.8071340e-18], sum to 1.0000
[2019-03-27 05:08:43,669] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4141
[2019-03-27 05:08:43,677] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333333, 71.66666666666667, 1.0, 2.0, 0.5299709020944569, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564271041, 777632.4986622494, 777632.4986622494, 193080.821041168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 61800.0000, 
sim time next is 62400.0000, 
raw observation next is [26.26666666666667, 72.33333333333334, 1.0, 2.0, 0.3824038351299068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104102, 566361.1063872641, 566361.1063872635, 171341.4910205384], 
processed observation next is [1.0, 0.7391304347826086, 0.44391785150079005, 0.7233333333333334, 1.0, 1.0, 0.2559082350962732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451522005, 0.1573225295520178, 0.15732252955201764, 0.25573356868737074], 
reward next is 0.7443, 
noisyNet noise sample is [array([-0.86839193], dtype=float32), 1.3451948]. 
=============================================
[2019-03-27 05:08:43,816] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.9696481e-15 1.0000000e+00 1.2344588e-22 1.7453946e-14 8.7141937e-19], sum to 1.0000
[2019-03-27 05:08:43,822] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5036
[2019-03-27 05:08:43,827] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 68.5, 1.0, 2.0, 0.9955620027209112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1470254.569906476, 1470254.569906476, 309575.5844735556], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 58200.0000, 
sim time next is 58800.0000, 
raw observation next is [26.93333333333334, 69.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.975154725255804, 6.9112, 168.9124909160467, 1581166.632489665, 1535795.074304666, 324178.2410481563], 
processed observation next is [1.0, 0.6956521739130435, 0.4755134281200636, 0.69, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.00639547252558037, 0.0, 0.8294376588707257, 0.43921295346935135, 0.4266097428624072, 0.48384812096739743], 
reward next is 0.1964, 
noisyNet noise sample is [array([0.689288], dtype=float32), 0.04552119]. 
=============================================
[2019-03-27 05:08:51,669] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8300672e-13 1.0000000e+00 1.2577569e-19 6.6316673e-13 4.6148555e-17], sum to 1.0000
[2019-03-27 05:08:51,676] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1001
[2019-03-27 05:08:51,679] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666666, 93.0, 1.0, 2.0, 0.2972232293653841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 474875.4220113505, 474875.4220113511, 165279.0807824007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 208200.0000, 
sim time next is 208800.0000, 
raw observation next is [20.7, 93.0, 1.0, 2.0, 0.296681675757461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 473676.569458692, 473676.5694586913, 165190.2805393742], 
processed observation next is [0.0, 0.43478260869565216, 0.18009478672985785, 0.93, 1.0, 1.0, 0.15262852500898916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13157682484963668, 0.13157682484963648, 0.246552657521454], 
reward next is 0.7534, 
noisyNet noise sample is [array([-1.7617693], dtype=float32), -2.134034]. 
=============================================
[2019-03-27 05:09:07,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0642637e-16 1.0000000e+00 1.7618889e-24 2.3980519e-15 1.6728801e-20], sum to 1.0000
[2019-03-27 05:09:07,624] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0383
[2019-03-27 05:09:07,629] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.03333333333334, 53.0, 1.0, 2.0, 0.5232164997916068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 857609.5708894643, 857609.5708894636, 200104.891505163], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 487200.0000, 
sim time next is 487800.0000, 
raw observation next is [25.0, 53.0, 1.0, 2.0, 0.5412676434937735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887544.5930901167, 887544.5930901167, 203569.2083189036], 
processed observation next is [1.0, 0.6521739130434783, 0.38388625592417064, 0.53, 1.0, 1.0, 0.4473104138479199, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24654016474725465, 0.24654016474725465, 0.3038346392819457], 
reward next is 0.6962, 
noisyNet noise sample is [array([-0.3863458], dtype=float32), 0.498515]. 
=============================================
[2019-03-27 05:09:08,083] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 05:09:08,085] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:09:08,086] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:09:08,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:09:08,087] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:09:08,088] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:09:08,089] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:09:08,087] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:09:08,092] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:09:08,093] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:09:08,092] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:09:08,109] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run10
[2019-03-27 05:09:08,129] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run10
[2019-03-27 05:09:08,150] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run10
[2019-03-27 05:09:08,152] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run10
[2019-03-27 05:09:08,152] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run10
[2019-03-27 05:09:23,372] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03822539]
[2019-03-27 05:09:23,372] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.46666666666667, 81.0, 1.0, 2.0, 0.296220452931759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479786.5376183916, 479786.5376183916, 165622.9472847047]
[2019-03-27 05:09:23,376] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:09:23,379] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5171598e-14 1.0000000e+00 1.5895139e-23 1.6380576e-15 2.0117769e-19], sampled 0.15430396329906926
[2019-03-27 05:09:33,817] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03822539]
[2019-03-27 05:09:33,819] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.796436035, 81.97413475333335, 1.0, 2.0, 0.5733430319163423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801196.4587029597, 801196.4587029597, 195884.0581329123]
[2019-03-27 05:09:33,822] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:09:33,824] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3634676e-16 1.0000000e+00 1.6456081e-26 2.3767025e-17 5.8255921e-22], sampled 0.8578615300269761
[2019-03-27 05:10:15,206] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03822539]
[2019-03-27 05:10:15,208] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.475321365, 61.566474175, 1.0, 2.0, 0.5064353357964494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707667.6932304576, 707667.6932304582, 184609.9088866519]
[2019-03-27 05:10:15,209] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:10:15,213] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.6045507e-16 1.0000000e+00 5.0326845e-26 5.4873891e-17 1.7214646e-21], sampled 0.043029359635021636
[2019-03-27 05:10:39,926] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03822539]
[2019-03-27 05:10:39,926] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 78.0, 1.0, 2.0, 0.5766652977651024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805840.7941277992, 805840.7941277992, 196478.3577732962]
[2019-03-27 05:10:39,927] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:10:39,931] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.4117166e-16 1.0000000e+00 4.0247465e-26 4.1226933e-17 1.3526011e-21], sampled 0.532838862619269
[2019-03-27 05:10:44,405] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03822539]
[2019-03-27 05:10:44,406] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.83333333333333, 67.33333333333333, 1.0, 2.0, 0.7316045816709295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1022459.5102967, 1022459.510296701, 227815.830840007]
[2019-03-27 05:10:44,409] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:10:44,413] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9507557e-16 1.0000000e+00 2.0062059e-26 2.8587799e-17 7.6493926e-22], sampled 0.9187721862565057
[2019-03-27 05:11:00,794] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03822539]
[2019-03-27 05:11:00,795] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.25636552666667, 87.66284898, 1.0, 2.0, 0.5563610728882549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777456.9797763854, 777456.9797763848, 192896.395697583]
[2019-03-27 05:11:00,796] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:11:00,799] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5446905e-16 1.0000000e+00 1.4293910e-26 2.5028399e-17 5.3509851e-22], sampled 0.5521810274693603
[2019-03-27 05:11:02,899] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 05:11:03,020] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4235 3164023464.4496 1778.0000
[2019-03-27 05:11:03,051] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8724 2842497929.8887 1131.0000
[2019-03-27 05:11:03,072] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 05:11:03,076] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-27 05:11:04,092] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 225000, evaluation results [225000.0, 7883.4234960597905, 3164023464.449587, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8496.87235025343, 2842497929.888741, 1131.0]
[2019-03-27 05:11:18,122] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.4776838e-14 1.0000000e+00 4.0433595e-22 1.4190390e-14 3.5943102e-19], sum to 1.0000
[2019-03-27 05:11:18,130] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1561
[2019-03-27 05:11:18,139] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.05, 71.0, 1.0, 2.0, 0.4842114114022496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 793551.9730142583, 793551.9730142589, 193007.2105690312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 721800.0000, 
sim time next is 722400.0000, 
raw observation next is [22.23333333333333, 70.0, 1.0, 2.0, 0.4933154680641298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 808044.4208022965, 808044.4208022971, 194610.3667792293], 
processed observation next is [1.0, 0.34782608695652173, 0.25276461295418634, 0.7, 1.0, 1.0, 0.3895367085109998, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22445678355619347, 0.22445678355619364, 0.2904632339988497], 
reward next is 0.7095, 
noisyNet noise sample is [array([-2.3211334], dtype=float32), -0.23506361]. 
=============================================
[2019-03-27 05:11:19,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6105786e-14 1.0000000e+00 6.6127725e-24 3.3751081e-14 4.7472453e-21], sum to 1.0000
[2019-03-27 05:11:19,413] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2431
[2019-03-27 05:11:19,419] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.88333333333333, 69.83333333333334, 1.0, 2.0, 0.249177029053214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 410153.8110339758, 410153.8110339764, 160818.9242645232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 760200.0000, 
sim time next is 760800.0000, 
raw observation next is [21.66666666666667, 71.66666666666667, 1.0, 2.0, 0.2502482864269294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 411660.5502184638, 411660.5502184638, 160928.9597454255], 
processed observation next is [1.0, 0.8260869565217391, 0.22590837282780438, 0.7166666666666667, 1.0, 1.0, 0.09668468244208363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11435015283846217, 0.11435015283846217, 0.24019247723197837], 
reward next is 0.7598, 
noisyNet noise sample is [array([-0.06976537], dtype=float32), 0.49957442]. 
=============================================
[2019-03-27 05:11:20,817] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6737396e-15 1.0000000e+00 8.7754130e-25 6.3095230e-15 3.9209660e-20], sum to 1.0000
[2019-03-27 05:11:20,827] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5320
[2019-03-27 05:11:20,832] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.73333333333333, 87.0, 1.0, 2.0, 0.2569188624737023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 421992.8074784896, 421992.807478489, 161600.1278023405], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 772800.0000, 
sim time next is 773400.0000, 
raw observation next is [19.66666666666667, 87.5, 1.0, 2.0, 0.2563921792308387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 421159.7838542316, 421159.7838542322, 161546.9231726904], 
processed observation next is [1.0, 0.9565217391304348, 0.1311216429699845, 0.875, 1.0, 1.0, 0.10408696292872129, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11698882884839766, 0.11698882884839784, 0.24111481070550808], 
reward next is 0.7589, 
noisyNet noise sample is [array([-0.6367069], dtype=float32), 0.26668367]. 
=============================================
[2019-03-27 05:11:22,033] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.1437888e-14 1.0000000e+00 4.7829044e-23 6.1553745e-15 1.9736945e-18], sum to 1.0000
[2019-03-27 05:11:22,045] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1115
[2019-03-27 05:11:22,050] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 93.0, 1.0, 2.0, 0.2613040230641719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 426970.1042571639, 426970.1042571633, 162020.1651532058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 795000.0000, 
sim time next is 795600.0000, 
raw observation next is [19.4, 93.0, 1.0, 2.0, 0.2615543259106438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 427379.0794828273, 427379.0794828273, 162045.7391234416], 
processed observation next is [0.0, 0.21739130434782608, 0.11848341232227487, 0.93, 1.0, 1.0, 0.11030641675981182, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11871641096745202, 0.11871641096745202, 0.2418593121245397], 
reward next is 0.7581, 
noisyNet noise sample is [array([0.36906087], dtype=float32), -0.85369194]. 
=============================================
[2019-03-27 05:11:23,312] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2804658e-13 1.0000000e+00 1.8703429e-22 1.8725626e-15 8.9155329e-20], sum to 1.0000
[2019-03-27 05:11:23,321] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3570
[2019-03-27 05:11:23,326] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 65.0, 1.0, 2.0, 0.291832647351383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466766.9952571982, 466766.9952571976, 164717.0981647818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 814800.0000, 
sim time next is 815400.0000, 
raw observation next is [24.7, 64.0, 1.0, 2.0, 0.2917469936813316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466636.1393664615, 466636.1393664609, 164708.0811510054], 
processed observation next is [0.0, 0.43478260869565216, 0.3696682464454976, 0.64, 1.0, 1.0, 0.14668312491726698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12962114982401707, 0.12962114982401693, 0.2458329569417991], 
reward next is 0.7542, 
noisyNet noise sample is [array([-0.8767401], dtype=float32), 0.90502006]. 
=============================================
[2019-03-27 05:11:26,135] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8051549e-14 1.0000000e+00 8.4605750e-24 1.3693423e-16 3.0101419e-20], sum to 1.0000
[2019-03-27 05:11:26,139] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8710
[2019-03-27 05:11:26,148] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.36666666666667, 89.0, 1.0, 2.0, 0.3026761867764391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481203.8092262834, 481203.8092262834, 165695.3336316373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 865200.0000, 
sim time next is 865800.0000, 
raw observation next is [21.35, 89.0, 1.0, 2.0, 0.3020354382085937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480360.7102689046, 480360.710268904, 165637.8022522679], 
processed observation next is [0.0, 0.0, 0.2109004739336494, 0.89, 1.0, 1.0, 0.1590788412151731, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13343353063025126, 0.1334335306302511, 0.24722060037651925], 
reward next is 0.7528, 
noisyNet noise sample is [array([1.2397465], dtype=float32), 1.6999972]. 
=============================================
[2019-03-27 05:11:29,181] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3535251e-14 1.0000000e+00 3.5128093e-23 1.5022727e-14 1.0080564e-19], sum to 1.0000
[2019-03-27 05:11:29,191] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3789
[2019-03-27 05:11:29,203] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333334, 82.83333333333334, 1.0, 2.0, 0.3303007847433387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 514730.1321471648, 514730.1321471654, 167969.7924274406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 933000.0000, 
sim time next is 933600.0000, 
raw observation next is [22.96666666666667, 83.66666666666667, 1.0, 2.0, 0.3323981950555167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 517377.7231322267, 517377.7231322261, 168159.1274346377], 
processed observation next is [0.0, 0.8260869565217391, 0.2875197472353872, 0.8366666666666667, 1.0, 1.0, 0.19566047597050207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1437160342033963, 0.14371603420339613, 0.250983772290504], 
reward next is 0.7490, 
noisyNet noise sample is [array([-1.8730394], dtype=float32), -0.5543425]. 
=============================================
[2019-03-27 05:11:36,289] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2139172e-13 1.0000000e+00 2.0582004e-23 3.6979296e-16 2.5098251e-20], sum to 1.0000
[2019-03-27 05:11:36,298] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4060
[2019-03-27 05:11:36,306] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.0, 1.0, 2.0, 0.3497808518227349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 537152.0915332764, 537152.091533277, 169531.2477243921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1047600.0000, 
sim time next is 1048200.0000, 
raw observation next is [21.56666666666667, 96.83333333333334, 1.0, 2.0, 0.3545090253303371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546747.7960604882, 546747.7960604876, 170392.7842107117], 
processed observation next is [1.0, 0.13043478260869565, 0.22116903633491333, 0.9683333333333334, 1.0, 1.0, 0.22230003051847846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15187438779458007, 0.1518743877945799, 0.25431758837419655], 
reward next is 0.7457, 
noisyNet noise sample is [array([-0.01970682], dtype=float32), 0.19885494]. 
=============================================
[2019-03-27 05:11:37,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6133335e-14 1.0000000e+00 1.1204209e-23 3.2136275e-15 4.6526935e-20], sum to 1.0000
[2019-03-27 05:11:37,531] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4144
[2019-03-27 05:11:37,538] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 90.66666666666667, 1.0, 2.0, 0.2876360137193449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462568.0272232793, 462568.0272232793, 164447.0240701933], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1129200.0000, 
sim time next is 1129800.0000, 
raw observation next is [20.55, 90.83333333333334, 1.0, 2.0, 0.2876488060305038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462853.398360409, 462853.398360409, 164467.2992103646], 
processed observation next is [1.0, 0.043478260869565216, 0.17298578199052142, 0.9083333333333334, 1.0, 1.0, 0.14174554943434192, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12857038843344695, 0.12857038843344695, 0.24547358091099195], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.29204997], dtype=float32), 1.2927258]. 
=============================================
[2019-03-27 05:11:37,760] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9983889e-14 1.0000000e+00 1.4088814e-23 2.7640354e-14 2.3939169e-18], sum to 1.0000
[2019-03-27 05:11:37,771] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9401
[2019-03-27 05:11:37,775] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333334, 69.5, 1.0, 2.0, 0.6672565713197245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1028934.593009282, 1028934.593009282, 225863.8729400148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1086600.0000, 
sim time next is 1087200.0000, 
raw observation next is [25.6, 69.0, 1.0, 2.0, 0.7022262944596471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080421.196625199, 1080421.196625199, 233784.8565052438], 
processed observation next is [1.0, 0.6086956521739131, 0.4123222748815167, 0.69, 1.0, 1.0, 0.6412364993489724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30011699906255523, 0.30011699906255523, 0.34893262164961764], 
reward next is 0.6511, 
noisyNet noise sample is [array([-0.2344335], dtype=float32), 0.5340029]. 
=============================================
[2019-03-27 05:11:38,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.31604568e-15 1.00000000e+00 1.01893023e-22 4.81933371e-15
 1.09874801e-19], sum to 1.0000
[2019-03-27 05:11:38,358] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8594
[2019-03-27 05:11:38,361] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.43333333333334, 72.66666666666666, 1.0, 2.0, 0.4005005941091468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 625421.3745369897, 625421.3745369904, 177405.6012683179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1083000.0000, 
sim time next is 1083600.0000, 
raw observation next is [24.6, 72.0, 1.0, 2.0, 0.446095451224536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695558.2386561892, 695558.2386561886, 184143.6675486088], 
processed observation next is [1.0, 0.5652173913043478, 0.36492890995260674, 0.72, 1.0, 1.0, 0.33264512195727225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19321062184894144, 0.19321062184894128, 0.2748412948486698], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.48453942], dtype=float32), -0.34118155]. 
=============================================
[2019-03-27 05:11:39,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.9536485e-15 1.0000000e+00 3.9866715e-24 2.8290239e-16 1.4682800e-20], sum to 1.0000
[2019-03-27 05:11:39,252] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7936
[2019-03-27 05:11:39,256] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 67.5, 1.0, 2.0, 0.4232138857200688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653470.7032023642, 653470.7032023648, 179974.1591324478], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1098600.0000, 
sim time next is 1099200.0000, 
raw observation next is [25.53333333333333, 68.0, 1.0, 2.0, 0.3327323270616147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514524.5463198539, 514524.5463198539, 167832.2459973225], 
processed observation next is [1.0, 0.7391304347826086, 0.4091627172195892, 0.68, 1.0, 1.0, 0.19606304465254784, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1429234850888483, 0.1429234850888483, 0.25049588954824253], 
reward next is 0.7495, 
noisyNet noise sample is [array([1.1265608], dtype=float32), -0.1822005]. 
=============================================
[2019-03-27 05:11:39,712] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.4885714e-15 1.0000000e+00 1.4586240e-25 1.2591408e-17 1.7687747e-20], sum to 1.0000
[2019-03-27 05:11:39,721] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4350
[2019-03-27 05:11:39,727] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 70.0, 1.0, 2.0, 0.3344799801869318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519988.7483065563, 519988.7483065563, 168346.3379643153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1101600.0000, 
sim time next is 1102200.0000, 
raw observation next is [24.86666666666667, 70.5, 1.0, 2.0, 0.3363306403399798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 523551.3131005259, 523551.3131005265, 168646.9554806755], 
processed observation next is [1.0, 0.782608695652174, 0.3775671406003162, 0.705, 1.0, 1.0, 0.20039836185539733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14543092030570162, 0.14543092030570182, 0.2517118738517545], 
reward next is 0.7483, 
noisyNet noise sample is [array([-0.9492258], dtype=float32), -2.2419827]. 
=============================================
[2019-03-27 05:11:40,398] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.1431637e-14 1.0000000e+00 4.9354428e-24 1.7757215e-16 4.4598987e-20], sum to 1.0000
[2019-03-27 05:11:40,406] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3156
[2019-03-27 05:11:40,414] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 86.66666666666667, 1.0, 2.0, 0.3026724876742927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482481.6111709088, 482481.6111709088, 165806.939938598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1118400.0000, 
sim time next is 1119000.0000, 
raw observation next is [21.41666666666666, 87.33333333333334, 1.0, 2.0, 0.301479053093824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480898.9324702948, 480898.9324702942, 165697.7298154532], 
processed observation next is [1.0, 0.9565217391304348, 0.21406003159557638, 0.8733333333333334, 1.0, 1.0, 0.1584084977034024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13358303679730413, 0.13358303679730393, 0.24731004450067642], 
reward next is 0.7527, 
noisyNet noise sample is [array([-2.0391407], dtype=float32), 0.440513]. 
=============================================
[2019-03-27 05:11:40,423] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.63889 ]
 [73.60441 ]
 [73.5569  ]
 [73.50197 ]
 [73.449234]], R is [[73.68764496]
 [73.70329285]
 [73.71862793]
 [73.73366547]
 [73.74839783]].
[2019-03-27 05:11:43,263] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2680688e-15 1.0000000e+00 5.8919799e-26 1.1052024e-16 2.2007540e-21], sum to 1.0000
[2019-03-27 05:11:43,271] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7266
[2019-03-27 05:11:43,279] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.88333333333333, 63.5, 1.0, 2.0, 0.5262258908200603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801873.4452989831, 801873.4452989837, 195883.366619986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1169400.0000, 
sim time next is 1170000.0000, 
raw observation next is [27.0, 63.0, 1.0, 2.0, 0.5250589993288421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799527.1029328268, 799527.1029328268, 195610.9273368673], 
processed observation next is [1.0, 0.5652173913043478, 0.4786729857819906, 0.63, 1.0, 1.0, 0.42778192690221944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22209086192578523, 0.22209086192578523, 0.29195660796547357], 
reward next is 0.7080, 
noisyNet noise sample is [array([0.74873096], dtype=float32), -0.9233905]. 
=============================================
[2019-03-27 05:11:43,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.655525]
 [80.615974]
 [80.50957 ]
 [80.38156 ]
 [80.17465 ]], R is [[80.55395508]
 [80.45604706]
 [80.35479736]
 [80.22676849]
 [80.0923996 ]].
[2019-03-27 05:11:45,951] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.0506873e-13 1.0000000e+00 3.4484417e-23 2.3392473e-14 4.0243135e-20], sum to 1.0000
[2019-03-27 05:11:45,961] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5912
[2019-03-27 05:11:45,965] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 93.0, 1.0, 2.0, 0.3475140407416505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540193.0937402694, 540193.0937402694, 169965.9488089769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1224000.0000, 
sim time next is 1224600.0000, 
raw observation next is [21.76666666666667, 93.33333333333333, 1.0, 2.0, 0.3797506007495984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590201.4224331676, 590201.4224331676, 174223.4194236858], 
processed observation next is [1.0, 0.17391304347826086, 0.23064770932069528, 0.9333333333333332, 1.0, 1.0, 0.2527115671681908, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16394483956476877, 0.16394483956476877, 0.26003495436371016], 
reward next is 0.7400, 
noisyNet noise sample is [array([-0.9968387], dtype=float32), -0.20831394]. 
=============================================
[2019-03-27 05:11:49,141] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.01379105e-11 1.00000000e+00 4.29367322e-21 9.04474846e-12
 1.74909286e-15], sum to 1.0000
[2019-03-27 05:11:49,151] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7990
[2019-03-27 05:11:49,161] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2097073.131146631 W.
[2019-03-27 05:11:49,166] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.05, 74.83333333333333, 1.0, 2.0, 0.858588273533946, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.96974013836522, 6.9112, 168.9126077652816, 2097073.131146631, 2055542.82793975, 424036.3299095338], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1266600.0000, 
sim time next is 1267200.0000, 
raw observation next is [28.0, 75.0, 1.0, 2.0, 0.4723608229101511, 1.0, 1.0, 0.4723608229101511, 1.0, 2.0, 0.801572198185797, 6.9112, 6.9112, 170.5573041426782, 1981330.578607175, 1981330.578607175, 393331.0502679248], 
processed observation next is [1.0, 0.6956521739130435, 0.5260663507109005, 0.75, 1.0, 1.0, 0.3642901480845194, 1.0, 0.5, 0.3642901480845194, 1.0, 1.0, 0.7580148758363376, 0.0, 0.0, 0.8375144448122397, 0.5503696051686597, 0.5503696051686597, 0.5870612690566042], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7450478], dtype=float32), -0.47873047]. 
=============================================
[2019-03-27 05:11:53,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3368002e-16 1.0000000e+00 6.1509006e-27 1.7410362e-17 3.3108639e-21], sum to 1.0000
[2019-03-27 05:11:53,518] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9482
[2019-03-27 05:11:53,527] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.96666666666667, 91.16666666666667, 1.0, 2.0, 0.6389362483018863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1018434.12291141, 1018434.122911409, 222670.5488732789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1354200.0000, 
sim time next is 1354800.0000, 
raw observation next is [20.93333333333334, 91.33333333333334, 1.0, 2.0, 0.6037842607230055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 962711.6791604629, 962711.6791604629, 214989.1359678286], 
processed observation next is [1.0, 0.6956521739130435, 0.19115323854660388, 0.9133333333333334, 1.0, 1.0, 0.5226316394253078, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26741991087790634, 0.26741991087790634, 0.3208793074146695], 
reward next is 0.6791, 
noisyNet noise sample is [array([0.8719227], dtype=float32), 0.5135332]. 
=============================================
[2019-03-27 05:11:57,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4135560e-13 1.0000000e+00 5.5858577e-22 9.3300164e-14 1.0375674e-18], sum to 1.0000
[2019-03-27 05:11:57,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7139
[2019-03-27 05:11:57,139] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333334, 93.66666666666667, 1.0, 2.0, 0.3272875263374947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 512773.1299449203, 512773.1299449197, 167891.275647502], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1406400.0000, 
sim time next is 1407000.0000, 
raw observation next is [21.51666666666667, 93.33333333333333, 1.0, 2.0, 0.3283729307895555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513909.2603748629, 513909.2603748622, 167964.9492477559], 
processed observation next is [0.0, 0.2608695652173913, 0.21879936808846778, 0.9333333333333332, 1.0, 1.0, 0.19081075998741623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1427525723263508, 0.1427525723263506, 0.2506939541011282], 
reward next is 0.7493, 
noisyNet noise sample is [array([-0.6324227], dtype=float32), 0.12414294]. 
=============================================
[2019-03-27 05:11:57,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.414955]
 [73.37239 ]
 [73.3173  ]
 [73.25729 ]
 [73.20373 ]], R is [[73.47045898]
 [73.48516846]
 [73.49974823]
 [73.51426697]
 [73.52886963]].
[2019-03-27 05:11:57,623] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1925058e-14 1.0000000e+00 3.0430216e-21 9.6245180e-13 9.5237655e-18], sum to 1.0000
[2019-03-27 05:11:57,633] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7525
[2019-03-27 05:11:57,638] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 89.33333333333333, 1.0, 2.0, 0.336753372377344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523885.4917722377, 523885.4917722377, 168664.6677373519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1411800.0000, 
sim time next is 1412400.0000, 
raw observation next is [22.4, 88.66666666666667, 1.0, 2.0, 0.3411991142400327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 529893.099511375, 529893.0995113757, 169119.2530012694], 
processed observation next is [0.0, 0.34782608695652173, 0.2606635071090047, 0.8866666666666667, 1.0, 1.0, 0.20626399306028034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1471925276420486, 0.1471925276420488, 0.25241679552428264], 
reward next is 0.7476, 
noisyNet noise sample is [array([-0.28806964], dtype=float32), -0.44442087]. 
=============================================
[2019-03-27 05:11:57,909] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 05:11:57,914] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:11:57,915] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:11:57,916] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:11:57,917] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:11:57,918] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:11:57,920] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:11:57,921] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:11:57,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:11:57,918] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:11:57,926] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:11:57,943] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run11
[2019-03-27 05:11:57,943] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run11
[2019-03-27 05:11:57,982] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run11
[2019-03-27 05:11:57,982] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run11
[2019-03-27 05:11:58,033] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run11
[2019-03-27 05:12:18,941] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03773641]
[2019-03-27 05:12:18,943] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.9, 93.0, 1.0, 2.0, 0.3099032748595594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 492672.6424365093, 492672.6424365099, 166528.201158081]
[2019-03-27 05:12:18,945] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:12:18,949] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7878548e-15 1.0000000e+00 8.3049718e-25 3.7463754e-16 1.7042811e-20], sampled 0.9552908021064107
[2019-03-27 05:12:54,983] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03773641]
[2019-03-27 05:12:54,985] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.73910662666667, 82.67271511333334, 1.0, 2.0, 0.5026119384393147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702323.3006831987, 702323.3006831987, 184005.7524385035]
[2019-03-27 05:12:54,987] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:12:54,990] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2316432e-15 1.0000000e+00 3.5896815e-25 2.5244639e-16 9.8688536e-21], sampled 0.9739946132132552
[2019-03-27 05:13:32,817] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03773641]
[2019-03-27 05:13:32,818] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.81666666666667, 69.5, 1.0, 2.0, 0.5345913394113851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747025.3346332994, 747025.3346332994, 189192.6567046977]
[2019-03-27 05:13:32,819] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:13:32,825] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.3175332e-16 1.0000000e+00 8.6144461e-26 8.8302854e-17 2.1169369e-21], sampled 0.0008628930034607629
[2019-03-27 05:13:35,810] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03773641]
[2019-03-27 05:13:35,811] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.11978778666667, 63.87075599333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.937926947447855, 6.9112, 168.9125084199044, 1472728.879690206, 1453767.913599745, 311347.916175983]
[2019-03-27 05:13:35,814] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:13:35,816] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9366824e-15 1.0000000e+00 2.9678900e-25 2.0663894e-16 8.4277321e-21], sampled 0.09318466064616604
[2019-03-27 05:13:46,757] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.03773641]
[2019-03-27 05:13:46,759] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.7, 80.66666666666667, 1.0, 2.0, 0.4993224534033227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697725.2389606611, 697725.2389606618, 183488.866567977]
[2019-03-27 05:13:46,759] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:13:46,762] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.3273964e-15 1.0000000e+00 5.0121705e-24 1.0464370e-15 8.2623653e-20], sampled 0.8796198147984012
[2019-03-27 05:13:52,957] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 05:13:53,190] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 05:13:53,254] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5752 2842451949.6408 1131.0000
[2019-03-27 05:13:53,340] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 05:13:53,376] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 05:13:54,391] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 250000, evaluation results [250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.575228127671, 2842451949.640752, 1131.0]
[2019-03-27 05:13:54,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3612754e-15 1.0000000e+00 1.0395813e-23 1.0007818e-16 1.1840716e-18], sum to 1.0000
[2019-03-27 05:13:54,834] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4846
[2019-03-27 05:13:54,846] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 70.66666666666667, 1.0, 2.0, 0.4270259918218965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617269.5286702916, 617269.5286702916, 175605.094778121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1433400.0000, 
sim time next is 1434000.0000, 
raw observation next is [27.33333333333334, 70.33333333333334, 1.0, 2.0, 0.4272811100127285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617414.2181239652, 617414.2181239659, 175612.5894926342], 
processed observation next is [0.0, 0.6086956521739131, 0.4944707740916275, 0.7033333333333335, 1.0, 1.0, 0.30997724097919094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17150394947887923, 0.17150394947887942, 0.2621083425263197], 
reward next is 0.7379, 
noisyNet noise sample is [array([-1.8418438], dtype=float32), -0.2076852]. 
=============================================
[2019-03-27 05:13:54,864] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.55171 ]
 [78.488754]
 [78.418564]
 [78.358795]
 [78.290215]], R is [[78.55953217]
 [78.51184082]
 [78.46461487]
 [78.41777802]
 [78.37129974]].
[2019-03-27 05:13:59,228] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0015066e-13 1.0000000e+00 2.2456146e-22 1.0923228e-15 1.9055561e-18], sum to 1.0000
[2019-03-27 05:13:59,237] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7424
[2019-03-27 05:13:59,244] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.43333333333333, 51.0, 1.0, 2.0, 0.356860421157582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543278.6023775876, 543278.6023775876, 169886.0060121602], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1515000.0000, 
sim time next is 1515600.0000, 
raw observation next is [29.5, 51.0, 1.0, 2.0, 0.3592854886153388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546012.8346006966, 546012.8346006966, 170083.2172065805], 
processed observation next is [0.0, 0.5652173913043478, 0.5971563981042655, 0.51, 1.0, 1.0, 0.22805480556064917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15167023183352682, 0.15167023183352682, 0.25385554806952315], 
reward next is 0.7461, 
noisyNet noise sample is [array([0.7970343], dtype=float32), -1.142461]. 
=============================================
[2019-03-27 05:14:06,259] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4924208e-15 1.0000000e+00 3.2124238e-24 9.2013639e-17 9.2439849e-21], sum to 1.0000
[2019-03-27 05:14:06,269] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4476
[2019-03-27 05:14:06,274] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 99.0, 1.0, 2.0, 0.4684580988681915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669463.7835423907, 669463.7835423913, 180708.0322407645], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1663200.0000, 
sim time next is 1663800.0000, 
raw observation next is [23.51666666666667, 98.83333333333333, 1.0, 2.0, 0.5287613154827693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755556.0521053362, 755556.0521053356, 190353.0675579442], 
processed observation next is [1.0, 0.2608695652173913, 0.31358609794628767, 0.9883333333333333, 1.0, 1.0, 0.4322425487744208, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20987668114037117, 0.209876681140371, 0.2841090560566331], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.56545264], dtype=float32), 0.83984655]. 
=============================================
[2019-03-27 05:14:08,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3415157e-15 1.0000000e+00 1.2502721e-24 2.5003314e-16 5.3259016e-20], sum to 1.0000
[2019-03-27 05:14:08,676] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6939
[2019-03-27 05:14:08,681] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.68333333333333, 98.0, 1.0, 2.0, 0.4636623486231898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660898.7693431445, 660898.7693431451, 179770.3064371699], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1669800.0000, 
sim time next is 1670400.0000, 
raw observation next is [23.7, 98.0, 1.0, 2.0, 0.4674243400664169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665774.3025583858, 665774.3025583858, 180271.479460674], 
processed observation next is [1.0, 0.34782608695652173, 0.3222748815165877, 0.98, 1.0, 1.0, 0.35834257839327344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18493730626621827, 0.18493730626621827, 0.269061909642797], 
reward next is 0.7309, 
noisyNet noise sample is [array([-1.1510732], dtype=float32), -0.91376346]. 
=============================================
[2019-03-27 05:14:14,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6245675e-15 1.0000000e+00 7.0881124e-24 2.6263090e-16 6.5104661e-19], sum to 1.0000
[2019-03-27 05:14:14,506] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7406
[2019-03-27 05:14:14,513] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 92.0, 1.0, 2.0, 0.6004387425506822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 953896.1516256345, 953896.1516256345, 213998.0517939191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1782000.0000, 
sim time next is 1782600.0000, 
raw observation next is [21.0, 92.33333333333334, 1.0, 2.0, 0.611183759918883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970856.4212880665, 970856.4212880665, 216282.768564925], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.9233333333333335, 1.0, 1.0, 0.5315466986974493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26968233924668517, 0.26968233924668517, 0.32281010233570895], 
reward next is 0.6772, 
noisyNet noise sample is [array([1.2531909], dtype=float32), -0.9662865]. 
=============================================
[2019-03-27 05:14:30,188] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.8741040e-15 1.0000000e+00 1.6354083e-25 1.4177451e-15 2.3898712e-21], sum to 1.0000
[2019-03-27 05:14:30,201] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5293
[2019-03-27 05:14:30,209] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 86.0, 1.0, 2.0, 0.5010387839478553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 700124.3335075019, 700124.3335075025, 183759.0823737311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2048400.0000, 
sim time next is 2049000.0000, 
raw observation next is [26.43333333333334, 86.16666666666667, 1.0, 2.0, 0.4998102840773837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698407.130317596, 698407.130317596, 183566.4228598553], 
processed observation next is [0.0, 0.7391304347826086, 0.4518167456556086, 0.8616666666666667, 1.0, 1.0, 0.3973617880450406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19400198064377666, 0.19400198064377666, 0.2739797356117243], 
reward next is 0.7260, 
noisyNet noise sample is [array([-1.4177318], dtype=float32), -0.22531538]. 
=============================================
[2019-03-27 05:14:30,227] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[76.50114 ]
 [76.471825]
 [76.45987 ]
 [76.44918 ]
 [76.42365 ]], R is [[76.48590088]
 [76.44676971]
 [76.4078064 ]
 [76.36894989]
 [76.33011627]].
[2019-03-27 05:14:32,919] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9725159e-15 1.0000000e+00 6.0665545e-24 2.5226458e-15 1.7615160e-20], sum to 1.0000
[2019-03-27 05:14:32,927] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2851
[2019-03-27 05:14:32,933] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 84.83333333333333, 1.0, 2.0, 0.5162378622258509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721369.9098649976, 721369.9098649976, 186179.9780078104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2105400.0000, 
sim time next is 2106000.0000, 
raw observation next is [27.5, 84.0, 1.0, 2.0, 0.5193863155730455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725770.9342080911, 725770.9342080904, 186689.897721524], 
processed observation next is [0.0, 0.391304347826087, 0.5023696682464456, 0.84, 1.0, 1.0, 0.42094736816029577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20160303728002532, 0.20160303728002513, 0.27864163839033435], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.95369333], dtype=float32), -1.1932119]. 
=============================================
[2019-03-27 05:14:32,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.121254]
 [73.07569 ]
 [73.029785]
 [72.97236 ]
 [72.90954 ]], R is [[73.17182922]
 [73.16223145]
 [73.15344238]
 [73.1453476 ]
 [73.13793945]].
[2019-03-27 05:14:34,241] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.56941534e-14 1.00000000e+00 5.97550364e-26 1.00169845e-16
 6.10207462e-21], sum to 1.0000
[2019-03-27 05:14:34,250] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9135
[2019-03-27 05:14:34,254] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.05, 76.83333333333334, 1.0, 2.0, 0.5691461926551131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795329.5492814106, 795329.5492814106, 195139.2908913219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2124600.0000, 
sim time next is 2125200.0000, 
raw observation next is [30.1, 76.66666666666667, 1.0, 2.0, 0.5696814698117408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796077.8302688397, 796077.8302688404, 195234.1858322718], 
processed observation next is [0.0, 0.6086956521739131, 0.6255924170616115, 0.7666666666666667, 1.0, 1.0, 0.48154393953221786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22113273063023325, 0.22113273063023345, 0.29139430721234594], 
reward next is 0.7086, 
noisyNet noise sample is [array([-1.3654969], dtype=float32), -0.7174308]. 
=============================================
[2019-03-27 05:14:34,514] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.6302738e-17 1.0000000e+00 1.1676373e-25 1.6739887e-15 9.7870594e-19], sum to 1.0000
[2019-03-27 05:14:34,522] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9357
[2019-03-27 05:14:34,528] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 75.66666666666667, 1.0, 2.0, 0.5775610225557228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807092.9690326591, 807092.9690326591, 196640.2359953894], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2128200.0000, 
sim time next is 2128800.0000, 
raw observation next is [30.36666666666667, 75.33333333333334, 1.0, 2.0, 0.5736805237774774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 801668.2518703135, 801668.251870313, 195945.5078429384], 
processed observation next is [0.0, 0.6521739130434783, 0.6382306477093209, 0.7533333333333334, 1.0, 1.0, 0.48636207684033417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22268562551953153, 0.2226856255195314, 0.29245598185513194], 
reward next is 0.7075, 
noisyNet noise sample is [array([0.2575014], dtype=float32), -1.246346]. 
=============================================
[2019-03-27 05:14:35,517] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4804660e-15 1.0000000e+00 9.6669160e-26 9.3403880e-16 2.8220567e-19], sum to 1.0000
[2019-03-27 05:14:35,529] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5739
[2019-03-27 05:14:35,535] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 83.66666666666667, 1.0, 2.0, 0.553274880189025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773142.7755715012, 773142.7755715012, 192364.2066249187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2143200.0000, 
sim time next is 2143800.0000, 
raw observation next is [28.15, 84.5, 1.0, 2.0, 0.5519986809863414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771358.7754463197, 771358.7754463197, 192144.2854512214], 
processed observation next is [0.0, 0.8260869565217391, 0.533175355450237, 0.845, 1.0, 1.0, 0.46023937468233905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2142663265128666, 0.2142663265128666, 0.2867825155988379], 
reward next is 0.7132, 
noisyNet noise sample is [array([-0.77845067], dtype=float32), 0.4715387]. 
=============================================
[2019-03-27 05:14:37,623] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.05023324e-13 1.00000000e+00 2.33898344e-22 9.18065215e-15
 3.58892922e-17], sum to 1.0000
[2019-03-27 05:14:37,631] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1010
[2019-03-27 05:14:37,638] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 97.0, 1.0, 2.0, 0.558473430911198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780409.8657194518, 780409.8657194518, 193259.4443489104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2178000.0000, 
sim time next is 2178600.0000, 
raw observation next is [24.71666666666667, 96.16666666666667, 1.0, 2.0, 0.5331504626619314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745011.1813455194, 745011.1813455194, 188950.06243114], 
processed observation next is [1.0, 0.21739130434782608, 0.3704581358609796, 0.9616666666666667, 1.0, 1.0, 0.43753067790594147, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2069475503737554, 0.2069475503737554, 0.2820150185539403], 
reward next is 0.7180, 
noisyNet noise sample is [array([-0.61836416], dtype=float32), 0.7513628]. 
=============================================
[2019-03-27 05:14:37,784] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.0532414e-14 1.0000000e+00 3.5682076e-24 3.7643396e-16 1.6192661e-20], sum to 1.0000
[2019-03-27 05:14:37,794] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8130
[2019-03-27 05:14:37,800] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 95.83333333333333, 1.0, 2.0, 0.6042654707323429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844425.0241070074, 844425.0241070081, 201535.2731236075], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2173800.0000, 
sim time next is 2174400.0000, 
raw observation next is [24.8, 96.0, 1.0, 2.0, 0.6021880596426645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 841520.8157285784, 841520.8157285777, 201146.3751646629], 
processed observation next is [1.0, 0.17391304347826086, 0.3744075829383887, 0.96, 1.0, 1.0, 0.5207085055935717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23375578214682732, 0.23375578214682713, 0.30021847039501925], 
reward next is 0.6998, 
noisyNet noise sample is [array([-0.8728204], dtype=float32), -2.0810921]. 
=============================================
[2019-03-27 05:14:40,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.1015888e-11 1.0000000e+00 3.4995252e-17 4.0706515e-11 3.4315687e-14], sum to 1.0000
[2019-03-27 05:14:40,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0616
[2019-03-27 05:14:40,265] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2140821.833457436 W.
[2019-03-27 05:14:40,272] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333334, 67.66666666666666, 1.0, 2.0, 0.7655197756013565, 1.0, 2.0, 0.7655197756013565, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2140821.833457436, 2140821.833457436, 403440.2620911021], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2220000.0000, 
sim time next is 2220600.0000, 
raw observation next is [31.81666666666667, 67.83333333333334, 1.0, 2.0, 0.716077908017359, 1.0, 2.0, 0.716077908017359, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2002425.465754903, 2002425.465754903, 381090.4654220814], 
processed observation next is [1.0, 0.6956521739130435, 0.7069510268562403, 0.6783333333333335, 1.0, 1.0, 0.6579251903823603, 1.0, 1.0, 0.6579251903823603, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5562292960430286, 0.5562292960430286, 0.5687917394359424], 
reward next is 0.4312, 
noisyNet noise sample is [array([-0.75228244], dtype=float32), 1.5877783]. 
=============================================
[2019-03-27 05:14:47,693] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.13646126e-13 1.00000000e+00 6.37158889e-22 2.00643623e-14
 8.95159246e-19], sum to 1.0000
[2019-03-27 05:14:47,703] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5639
[2019-03-27 05:14:47,713] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.13333333333333, 82.0, 1.0, 2.0, 0.6682611215593884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 933894.4548958407, 933894.4548958407, 214141.528037051], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2350200.0000, 
sim time next is 2350800.0000, 
raw observation next is [27.1, 82.0, 1.0, 2.0, 0.6234163866440692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 871198.2649760069, 871198.2649760064, 205181.0847337015], 
processed observation next is [1.0, 0.21739130434782608, 0.4834123222748816, 0.82, 1.0, 1.0, 0.5462848031856254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2419995180488908, 0.24199951804889067, 0.3062404249756739], 
reward next is 0.6938, 
noisyNet noise sample is [array([-1.0912119], dtype=float32), 0.20260568]. 
=============================================
[2019-03-27 05:14:48,319] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 05:14:48,321] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:14:48,322] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:14:48,322] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:14:48,324] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:14:48,325] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:14:48,323] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:14:48,325] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:14:48,326] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:14:48,329] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:14:48,329] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:14:48,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run12
[2019-03-27 05:14:48,360] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run12
[2019-03-27 05:14:48,362] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run12
[2019-03-27 05:14:48,363] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run12
[2019-03-27 05:14:48,420] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run12
[2019-03-27 05:15:06,785] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036185622]
[2019-03-27 05:15:06,787] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [17.43333333333333, 92.33333333333334, 1.0, 2.0, 0.2198320375477427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 366690.7959046554, 366690.7959046554, 157427.5968761826]
[2019-03-27 05:15:06,789] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:15:06,790] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.8690584e-13 1.0000000e+00 1.1851821e-20 1.8941463e-13 5.0062837e-17], sampled 0.32351402453905165
[2019-03-27 05:15:21,644] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036185622]
[2019-03-27 05:15:21,645] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.08906851166667, 81.57975990666667, 1.0, 2.0, 0.604022896927727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844085.9069578116, 844085.9069578116, 201498.1135377415]
[2019-03-27 05:15:21,647] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:15:21,650] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.9174778e-15 1.0000000e+00 3.5917105e-24 1.4421054e-15 4.8753673e-20], sampled 0.34358769655342425
[2019-03-27 05:15:28,356] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036185622]
[2019-03-27 05:15:28,357] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.65170231, 91.79557309, 1.0, 2.0, 0.3802989154965522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577458.234232375, 577458.234232375, 172784.3057640703]
[2019-03-27 05:15:28,359] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:15:28,363] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8628985e-14 1.0000000e+00 3.1948154e-23 5.1039385e-15 3.3986525e-19], sampled 0.18758145490686629
[2019-03-27 05:15:57,500] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036185622]
[2019-03-27 05:15:57,503] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.753095135, 98.75657918166668, 1.0, 2.0, 0.5176268116191072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723311.4323651663, 723311.4323651668, 186402.8850906368]
[2019-03-27 05:15:57,504] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:15:57,507] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2654608e-14 1.0000000e+00 1.8956040e-23 4.4857425e-15 2.2472721e-19], sampled 0.9820017939991225
[2019-03-27 05:16:27,654] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036185622]
[2019-03-27 05:16:27,655] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 84.33333333333334, 1.0, 2.0, 0.3452182871015645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548351.6535439709, 548351.6535439703, 170826.0941097662]
[2019-03-27 05:16:27,656] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:16:27,659] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.8774138e-13 1.0000000e+00 2.3991517e-21 7.0732787e-14 1.1231502e-17], sampled 0.43075655829387616
[2019-03-27 05:16:29,111] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036185622]
[2019-03-27 05:16:29,112] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.2, 70.0, 1.0, 2.0, 0.5608036151571666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783667.2625929243, 783667.2625929243, 193669.4815470427]
[2019-03-27 05:16:29,114] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:16:29,116] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.03322485e-14 1.00000000e+00 4.06119399e-24 1.57283081e-15
 6.02845612e-20], sampled 0.28468359621248984
[2019-03-27 05:16:29,655] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.036185622]
[2019-03-27 05:16:29,656] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.06666666666667, 62.33333333333333, 1.0, 2.0, 0.3680216301841056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561263.2853430847, 561263.2853430847, 171441.8689189892]
[2019-03-27 05:16:29,656] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:16:29,660] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4893550e-13 1.0000000e+00 3.6153579e-22 2.7589632e-14 2.9440514e-18], sampled 0.7904516774244076
[2019-03-27 05:16:43,065] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 05:16:43,370] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 05:16:43,422] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.7395 3007736496.5946 1766.0000
[2019-03-27 05:16:43,632] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 05:16:43,637] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7724 2842493333.3205 1131.0000
[2019-03-27 05:16:44,652] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 275000, evaluation results [275000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7996.739503040816, 3007736496.5946302, 1766.0, 8496.772351911874, 2842493333.320488, 1131.0]
[2019-03-27 05:16:46,479] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4095654e-10 1.0000000e+00 2.0269388e-17 3.0630121e-10 3.1773951e-14], sum to 1.0000
[2019-03-27 05:16:46,487] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4274
[2019-03-27 05:16:46,499] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2654950.112371637 W.
[2019-03-27 05:16:46,503] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 61.5, 1.0, 2.0, 0.9491632646434933, 1.0, 2.0, 0.9491632646434933, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2654950.112371637, 2654950.112371637, 499024.0285177197], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2388600.0000, 
sim time next is 2389200.0000, 
raw observation next is [33.03333333333333, 61.33333333333333, 1.0, 2.0, 0.9519096544077423, 1.0, 2.0, 0.9519096544077423, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2662640.356996303, 2662640.356996304, 500603.8311558728], 
processed observation next is [1.0, 0.6521739130434783, 0.7646129541864138, 0.6133333333333333, 1.0, 1.0, 0.9420598245876414, 1.0, 1.0, 0.9420598245876414, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7396223213878619, 0.7396223213878622, 0.7471698972475713], 
reward next is 0.2528, 
noisyNet noise sample is [array([1.2807369], dtype=float32), 0.02903841]. 
=============================================
[2019-03-27 05:16:53,512] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3228116e-13 1.0000000e+00 7.2025121e-23 3.7627213e-14 1.2037508e-18], sum to 1.0000
[2019-03-27 05:16:53,520] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5705
[2019-03-27 05:16:53,528] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 96.5, 1.0, 2.0, 0.6952171063192489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 971582.6565672379, 971582.6565672379, 219817.1580888292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2521800.0000, 
sim time next is 2522400.0000, 
raw observation next is [26.23333333333333, 96.66666666666666, 1.0, 2.0, 0.6515346744744029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 910509.2366906371, 910509.2366906371, 210732.8218617607], 
processed observation next is [1.0, 0.17391304347826086, 0.44233807266982617, 0.9666666666666666, 1.0, 1.0, 0.580162258402895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25291923241406583, 0.25291923241406583, 0.31452659979367265], 
reward next is 0.6855, 
noisyNet noise sample is [array([0.93096536], dtype=float32), -0.40601856]. 
=============================================
[2019-03-27 05:17:03,502] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.5459336e-13 1.0000000e+00 4.1698799e-22 5.7729570e-13 5.8425404e-18], sum to 1.0000
[2019-03-27 05:17:03,512] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2844
[2019-03-27 05:17:03,520] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4241021189886448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616576.9840397829, 616576.9840397829, 175641.1058964175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2682000.0000, 
sim time next is 2682600.0000, 
raw observation next is [23.16666666666667, 99.00000000000001, 1.0, 2.0, 0.4280157911785641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 620524.527587044, 620524.5275870446, 175973.5112063584], 
processed observation next is [0.0, 0.043478260869565216, 0.2969984202211693, 0.9900000000000001, 1.0, 1.0, 0.3108623990103182, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17236792432973447, 0.17236792432973463, 0.2626470316512812], 
reward next is 0.7374, 
noisyNet noise sample is [array([0.40035772], dtype=float32), -0.951302]. 
=============================================
[2019-03-27 05:17:04,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9553756e-14 1.0000000e+00 5.5270514e-25 9.0156611e-15 1.1789144e-20], sum to 1.0000
[2019-03-27 05:17:04,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6397
[2019-03-27 05:17:04,650] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 100.0, 1.0, 2.0, 0.4157842069799606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610144.6145788797, 610144.6145788797, 175191.8476672653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [22.5, 100.0, 1.0, 2.0, 0.409181697380103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604254.4559888901, 604254.4559888901, 174749.847841923], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 1.0, 1.0, 1.0, 0.28817071973506386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16784845999691392, 0.16784845999691392, 0.2608206684207806], 
reward next is 0.7392, 
noisyNet noise sample is [array([-1.2831285], dtype=float32), 0.98770416]. 
=============================================
[2019-03-27 05:17:04,760] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4713852e-14 1.0000000e+00 7.9166870e-24 1.0264628e-16 1.4517895e-19], sum to 1.0000
[2019-03-27 05:17:04,768] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6769
[2019-03-27 05:17:04,778] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.428758649458126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 622337.012059926, 622337.0120599266, 176170.4536607454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2714400.0000, 
sim time next is 2715000.0000, 
raw observation next is [22.83333333333334, 100.0, 1.0, 2.0, 0.422891704354851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616621.4234102467, 616621.4234102473, 175696.9767287749], 
processed observation next is [0.0, 0.43478260869565216, 0.2812006319115327, 1.0, 1.0, 1.0, 0.3046888004275313, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17128372872506853, 0.1712837287250687, 0.26223429362503714], 
reward next is 0.7378, 
noisyNet noise sample is [array([0.6921577], dtype=float32), -0.11874109]. 
=============================================
[2019-03-27 05:17:04,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[74.12112 ]
 [74.099815]
 [74.09621 ]
 [74.08993 ]
 [74.069435]], R is [[74.12874603]
 [74.12451935]
 [74.12007141]
 [74.11555481]
 [74.11123657]].
[2019-03-27 05:17:05,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0357222e-15 1.0000000e+00 3.8065928e-23 1.1565959e-15 3.3589160e-19], sum to 1.0000
[2019-03-27 05:17:05,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3323
[2019-03-27 05:17:05,513] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 97.0, 1.0, 2.0, 0.3908136046864867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586100.983660108, 586100.983660108, 173350.5651956135], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2730600.0000, 
sim time next is 2731200.0000, 
raw observation next is [22.66666666666666, 96.0, 1.0, 2.0, 0.3907705648250304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585125.0563520117, 585125.0563520123, 173234.6026738832], 
processed observation next is [0.0, 0.6086956521739131, 0.27330173775671385, 0.96, 1.0, 1.0, 0.26598863231931374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1625347378755588, 0.16253473787555897, 0.2585591084684824], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.06100593], dtype=float32), -1.1747978]. 
=============================================
[2019-03-27 05:17:16,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0441849e-12 1.0000000e+00 2.2067216e-22 3.6541522e-15 5.3211722e-19], sum to 1.0000
[2019-03-27 05:17:16,871] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4951
[2019-03-27 05:17:16,880] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 97.0, 1.0, 2.0, 0.31207332160021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495274.6924825286, 495274.6924825286, 166706.4719911926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2926800.0000, 
sim time next is 2927400.0000, 
raw observation next is [20.58333333333334, 96.50000000000001, 1.0, 2.0, 0.312656314054623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495910.2715004249, 495910.2715004249, 166748.6065074436], 
processed observation next is [1.0, 0.9130434782608695, 0.17456556082148533, 0.9650000000000002, 1.0, 1.0, 0.1718750771742446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13775285319456246, 0.13775285319456246, 0.24887851717528894], 
reward next is 0.7511, 
noisyNet noise sample is [array([2.2301512], dtype=float32), 1.1810509]. 
=============================================
[2019-03-27 05:17:17,853] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0635678e-13 1.0000000e+00 6.2397490e-23 7.8832760e-14 3.9856325e-19], sum to 1.0000
[2019-03-27 05:17:17,859] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9595
[2019-03-27 05:17:17,865] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.00000000000001, 1.0, 2.0, 0.3110690049642909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491912.914583405, 491912.914583405, 166424.9596614277], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2949000.0000, 
sim time next is 2949600.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3081062899933056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 487217.4252841523, 487217.425284153, 166080.1343062757], 
processed observation next is [1.0, 0.13043478260869565, 0.19431279620853087, 0.94, 1.0, 1.0, 0.16639312047386215, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1353381736900423, 0.1353381736900425, 0.24788079747205327], 
reward next is 0.7521, 
noisyNet noise sample is [array([-0.8748508], dtype=float32), -0.36451256]. 
=============================================
[2019-03-27 05:17:18,660] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5673969e-13 1.0000000e+00 2.6249891e-22 7.8222259e-14 1.1569207e-18], sum to 1.0000
[2019-03-27 05:17:18,665] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6718
[2019-03-27 05:17:18,669] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3081062899933056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 487217.4252841523, 487217.425284153, 166080.1343062757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2949600.0000, 
sim time next is 2950200.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3102961502405507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490680.4228398422, 490680.4228398422, 166333.9938178167], 
processed observation next is [1.0, 0.13043478260869565, 0.19431279620853087, 0.94, 1.0, 1.0, 0.16903150631391647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13630011745551174, 0.13630011745551174, 0.24825969226539804], 
reward next is 0.7517, 
noisyNet noise sample is [array([0.14861731], dtype=float32), 0.88389057]. 
=============================================
[2019-03-27 05:17:26,680] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2325787e-16 1.0000000e+00 1.6084715e-23 9.4117378e-17 2.1391810e-20], sum to 1.0000
[2019-03-27 05:17:26,690] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8933
[2019-03-27 05:17:26,696] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4290735432712089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 622794.3850525762, 622794.3850525768, 176215.0042449138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3092400.0000, 
sim time next is 3093000.0000, 
raw observation next is [23.0, 99.00000000000001, 1.0, 2.0, 0.4264394184050594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 621010.880074416, 621010.8800744154, 176098.6453409253], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.9900000000000001, 1.0, 1.0, 0.3089631547048908, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17250302224289335, 0.17250302224289316, 0.2628337990163064], 
reward next is 0.7372, 
noisyNet noise sample is [array([0.84304583], dtype=float32), 0.7957846]. 
=============================================
[2019-03-27 05:17:26,714] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.02637 ]
 [77.15173 ]
 [77.263435]
 [77.46725 ]
 [77.710915]], R is [[76.8164444 ]
 [76.78527069]
 [76.75436401]
 [76.72368622]
 [76.69263458]].
[2019-03-27 05:17:32,436] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4849490e-15 1.0000000e+00 6.4651312e-25 1.3228506e-15 1.6115448e-20], sum to 1.0000
[2019-03-27 05:17:32,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3488
[2019-03-27 05:17:32,452] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4865236335153158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679835.1618634638, 679835.1618634638, 181511.5270902766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3196800.0000, 
sim time next is 3197400.0000, 
raw observation next is [25.0, 94.00000000000001, 1.0, 2.0, 0.4865847357625614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679920.5693278117, 679920.5693278117, 181520.8536546495], 
processed observation next is [0.0, 0.0, 0.38388625592417064, 0.9400000000000002, 1.0, 1.0, 0.3814273924850138, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18886682481328104, 0.18886682481328104, 0.2709266472457455], 
reward next is 0.7291, 
noisyNet noise sample is [array([-1.7849698], dtype=float32), 1.247213]. 
=============================================
[2019-03-27 05:17:34,798] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.4567871e-14 1.0000000e+00 4.5566947e-23 1.7292562e-15 3.5244000e-19], sum to 1.0000
[2019-03-27 05:17:34,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2941
[2019-03-27 05:17:34,818] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 77.33333333333334, 1.0, 2.0, 0.4801970682299169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670992.0562059153, 670992.0562059153, 180551.6786721637], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3226800.0000, 
sim time next is 3227400.0000, 
raw observation next is [27.5, 76.5, 1.0, 2.0, 0.4813187504993471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672559.9086517622, 672559.9086517622, 180721.0391912017], 
processed observation next is [0.0, 0.34782608695652173, 0.5023696682464456, 0.765, 1.0, 1.0, 0.3750828319269242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1868221968477117, 0.1868221968477117, 0.26973289431522646], 
reward next is 0.7303, 
noisyNet noise sample is [array([-0.22910999], dtype=float32), 1.6326025]. 
=============================================
[2019-03-27 05:17:38,550] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 05:17:38,552] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:17:38,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:17:38,553] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:17:38,555] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:17:38,558] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:17:38,559] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:17:38,560] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:17:38,560] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:17:38,558] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:17:38,561] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:17:38,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run13
[2019-03-27 05:17:38,602] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run13
[2019-03-27 05:17:38,603] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run13
[2019-03-27 05:17:38,641] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run13
[2019-03-27 05:17:38,642] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run13
[2019-03-27 05:17:54,959] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:17:54,961] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.512405765, 94.198211165, 1.0, 2.0, 0.3082236813805302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491558.1514482602, 491558.1514482602, 166467.1662144987]
[2019-03-27 05:17:54,963] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:17:54,967] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3960167e-13 1.0000000e+00 4.8145572e-22 2.6027442e-14 2.2132512e-18], sampled 0.5187993926673325
[2019-03-27 05:18:04,229] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:18:04,231] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.7321083217970223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024919.433990161, 1024919.433990161, 228165.068627696]
[2019-03-27 05:18:04,232] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:18:04,236] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.5301457e-14 1.0000000e+00 6.6824350e-23 8.6852717e-15 4.1327983e-19], sampled 0.5356541697946086
[2019-03-27 05:18:06,354] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:18:06,358] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.26666666666667, 93.66666666666667, 1.0, 2.0, 0.3736999967301641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571673.26645935, 571673.2664593506, 172392.7189517926]
[2019-03-27 05:18:06,359] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:18:06,360] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2857545e-13 1.0000000e+00 3.3733823e-22 2.4094325e-14 2.0060075e-18], sampled 0.008471543315790364
[2019-03-27 05:18:08,722] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:18:08,724] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.46666666666667, 94.66666666666666, 1.0, 2.0, 0.3914396018152642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592883.8028149763, 592883.8028149757, 174130.8068028491]
[2019-03-27 05:18:08,727] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:18:08,730] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.6120470e-14 1.0000000e+00 5.6247707e-23 9.4971867e-15 3.7954937e-19], sampled 0.7429791620086211
[2019-03-27 05:18:47,971] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:18:47,972] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.64800459, 66.42985535, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.127468609728687, 6.9112, 168.9117696520684, 1607287.319849076, 1453860.002773323, 311352.8303746596]
[2019-03-27 05:18:47,974] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:18:47,977] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.0260926e-15 1.0000000e+00 1.8016972e-24 1.2123795e-15 2.2193786e-20], sampled 0.17685855462181177
[2019-03-27 05:19:00,800] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:19:00,802] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.64101397, 76.64335672, 1.0, 2.0, 0.5262658748193572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735387.4995965022, 735387.4995965029, 187811.5337682361]
[2019-03-27 05:19:00,805] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:19:00,810] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2189746e-14 1.0000000e+00 5.6150780e-24 2.2596897e-15 5.0641293e-20], sampled 0.11684390583140736
[2019-03-27 05:19:05,812] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:19:05,813] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.41666666666666, 82.83333333333334, 1.0, 2.0, 0.7095915001286176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 991680.5975881364, 991680.5975881371, 222933.29686176]
[2019-03-27 05:19:05,813] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:19:05,815] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.5508783e-14 1.0000000e+00 1.0118690e-22 9.0641114e-15 4.8081409e-19], sampled 0.0012241177040922535
[2019-03-27 05:19:08,014] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:19:08,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.6, 92.0, 1.0, 2.0, 0.6861760957332826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 958941.9219953931, 958941.9219953924, 217889.8840422331]
[2019-03-27 05:19:08,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:19:08,018] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5860143e-14 1.0000000e+00 8.1689860e-24 2.5619416e-15 8.9298283e-20], sampled 0.5540738647586547
[2019-03-27 05:19:09,335] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:19:09,338] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.488189655, 84.766089535, 1.0, 2.0, 0.5122564076404725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715804.5117416723, 715804.5117416716, 185537.6479441572]
[2019-03-27 05:19:09,340] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:19:09,343] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.7268113e-15 1.0000000e+00 1.9569850e-24 1.2094648e-15 2.1717412e-20], sampled 0.9028128833616621
[2019-03-27 05:19:25,267] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:19:25,268] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.81666666666667, 60.83333333333334, 1.0, 2.0, 0.9338760829062352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1305319.333941575, 1305319.333941574, 279431.0033422097]
[2019-03-27 05:19:25,269] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:19:25,272] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0364540e-14 1.0000000e+00 4.4480474e-24 2.1140303e-15 5.1320004e-20], sampled 0.7258121890795117
[2019-03-27 05:19:31,062] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.037082367]
[2019-03-27 05:19:31,064] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.2, 88.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.12005409247243, 6.9112, 168.9112171151211, 2431981.059478477, 2283814.303850709, 475616.8272078943]
[2019-03-27 05:19:31,066] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:19:31,069] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.8987012e-12 1.0000000e+00 1.9445403e-19 2.2057859e-12 3.6932352e-16], sampled 0.93067201718581
[2019-03-27 05:19:31,070] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2431981.059478477 W.
[2019-03-27 05:19:33,418] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2362 2779204645.2033 933.0000
[2019-03-27 05:19:33,439] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 05:19:33,669] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8612 2842434274.6064 1131.0000
[2019-03-27 05:19:33,691] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 05:19:33,775] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 05:19:34,793] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 300000, evaluation results [300000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8659.236211692969, 2779204645.2032776, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.8611576407, 2842434274.606407, 1131.0]
[2019-03-27 05:19:38,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.4493254e-15 1.0000000e+00 1.7016929e-23 1.8067616e-14 1.1821529e-19], sum to 1.0000
[2019-03-27 05:19:38,313] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1913
[2019-03-27 05:19:38,318] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 88.16666666666667, 1.0, 2.0, 0.8604299167453348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1202602.283125204, 1202602.283125204, 259287.6254139024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3395400.0000, 
sim time next is 3396000.0000, 
raw observation next is [28.33333333333334, 87.33333333333334, 1.0, 2.0, 0.8712014140019531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217665.979690903, 1217665.979690903, 262141.6740370762], 
processed observation next is [1.0, 0.30434782608695654, 0.5418641390205374, 0.8733333333333334, 1.0, 1.0, 0.8448209807252446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33824054991413977, 0.33824054991413977, 0.3912562299060839], 
reward next is 0.6087, 
noisyNet noise sample is [array([-1.1244699], dtype=float32), -1.0016134]. 
=============================================
[2019-03-27 05:19:38,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.43489 ]
 [73.196495]
 [73.01896 ]
 [72.95638 ]
 [72.95752 ]], R is [[73.32013702]
 [73.19994354]
 [73.08105469]
 [72.97106934]
 [72.87219238]].
[2019-03-27 05:19:41,309] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9142410e-10 1.0000000e+00 7.6988274e-17 2.2426322e-10 9.0689927e-14], sum to 1.0000
[2019-03-27 05:19:41,315] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0572
[2019-03-27 05:19:41,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2216044.611268362 W.
[2019-03-27 05:19:41,324] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.5282603240177026, 1.0, 1.0, 0.5282603240177026, 1.0, 2.0, 0.9174134844600493, 6.9112, 6.9112, 170.5573041426782, 2216044.611268362, 2216044.611268362, 435240.3722112469], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3415800.0000, 
sim time next is 3416400.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.4897135181494841, 1.0, 2.0, 0.4897135181494841, 1.0, 2.0, 0.8504704302904496, 6.9112, 6.9112, 170.5573041426782, 2054186.723347741, 2054186.723347741, 408106.9563652831], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.67, 1.0, 1.0, 0.38519700981865557, 1.0, 1.0, 0.38519700981865557, 1.0, 1.0, 0.8176468662078653, 0.0, 0.0, 0.8375144448122397, 0.5706074231521503, 0.5706074231521503, 0.6091148602466913], 
reward next is 0.3909, 
noisyNet noise sample is [array([1.340012], dtype=float32), -0.5225455]. 
=============================================
[2019-03-27 05:19:54,652] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9221373e-14 1.0000000e+00 2.4801565e-22 3.1576228e-13 4.2240177e-18], sum to 1.0000
[2019-03-27 05:19:54,659] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3469
[2019-03-27 05:19:54,666] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.7734705947839404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1080999.454055136, 1080999.454055136, 237499.5673800751], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3640200.0000, 
sim time next is 3640800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.759871013071652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1061983.228120634, 1061983.228120635, 234295.6190430396], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7106879675562071, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29499534114462056, 0.29499534114462084, 0.34969495379558146], 
reward next is 0.6503, 
noisyNet noise sample is [array([0.03678466], dtype=float32), 0.6042286]. 
=============================================
[2019-03-27 05:19:58,717] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0536607e-12 1.0000000e+00 7.4990692e-21 1.7659293e-13 2.6402503e-17], sum to 1.0000
[2019-03-27 05:19:58,722] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8757
[2019-03-27 05:19:58,729] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.4579791345538589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653507.9042580575, 653507.9042580575, 179018.0397899096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3718800.0000, 
sim time next is 3719400.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.4547525033161577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650008.1057268956, 650008.1057268963, 178683.9311878362], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.74, 1.0, 1.0, 0.34307530520019, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18055780714635988, 0.18055780714636008, 0.2666924346087107], 
reward next is 0.7333, 
noisyNet noise sample is [array([-0.07007968], dtype=float32), -0.79281265]. 
=============================================
[2019-03-27 05:20:09,817] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6259454e-14 1.0000000e+00 4.6057995e-24 9.0337690e-15 7.6761178e-20], sum to 1.0000
[2019-03-27 05:20:09,823] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1539
[2019-03-27 05:20:09,827] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.579580615603349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809916.2565093081, 809916.2565093081, 197003.6025038642], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3913200.0000, 
sim time next is 3913800.0000, 
raw observation next is [29.16666666666667, 83.16666666666667, 1.0, 2.0, 0.5860418472139938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818948.7772540991, 818948.7772540991, 198174.1133729149], 
processed observation next is [0.0, 0.30434782608695654, 0.581358609794629, 0.8316666666666667, 1.0, 1.0, 0.5012552376072213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.227485771459472, 0.227485771459472, 0.29578225876554465], 
reward next is 0.7042, 
noisyNet noise sample is [array([-0.2836681], dtype=float32), 0.05464738]. 
=============================================
[2019-03-27 05:20:11,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7033215e-13 1.0000000e+00 1.8703643e-22 1.7339545e-13 1.2037560e-17], sum to 1.0000
[2019-03-27 05:20:11,293] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1605
[2019-03-27 05:20:11,296] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5943540290937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830568.9589360296, 830568.9589360302, 199699.6121076522], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3940800.0000, 
sim time next is 3941400.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5944829802785813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 830749.229837461, 830749.2298374603, 199723.4140715139], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5114252774440737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23076367495485028, 0.2307636749548501, 0.29809464786793116], 
reward next is 0.7019, 
noisyNet noise sample is [array([0.58263427], dtype=float32), 1.240871]. 
=============================================
[2019-03-27 05:20:12,553] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9677117e-14 1.0000000e+00 7.7488040e-23 1.6337463e-14 4.0398638e-20], sum to 1.0000
[2019-03-27 05:20:12,563] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8579
[2019-03-27 05:20:12,568] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 62.00000000000001, 1.0, 2.0, 0.6138143539392955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 857774.4083206037, 857774.4083206031, 203348.1752246291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3950400.0000, 
sim time next is 3951000.0000, 
raw observation next is [34.0, 61.5, 1.0, 2.0, 0.6090855167490602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851163.4504296836, 851163.4504296836, 202451.2057352744], 
processed observation next is [0.0, 0.7391304347826086, 0.8104265402843602, 0.615, 1.0, 1.0, 0.5290186948783857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2364342917860232, 0.2364342917860232, 0.30216597870936474], 
reward next is 0.6978, 
noisyNet noise sample is [array([-0.49998766], dtype=float32), -0.005941301]. 
=============================================
[2019-03-27 05:20:12,591] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.695984]
 [72.648834]
 [72.6091  ]
 [72.52832 ]
 [72.520645]], R is [[72.7338562 ]
 [72.70301056]
 [72.66891479]
 [72.6386795 ]
 [72.60975647]].
[2019-03-27 05:20:14,956] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7055828e-10 1.0000000e+00 1.6253157e-16 1.0116667e-10 3.4439286e-14], sum to 1.0000
[2019-03-27 05:20:14,964] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1049
[2019-03-27 05:20:14,974] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1689659.686480953 W.
[2019-03-27 05:20:14,983] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.4028795566161121, 1.0, 2.0, 0.4028795566161121, 1.0, 2.0, 0.6996685555368743, 6.911200000000001, 6.9112, 170.5573041426782, 1689659.686480953, 1689659.686480953, 354488.7079443578], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3991800.0000, 
sim time next is 3992400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.3922937161908797, 1.0, 2.0, 0.3922937161908797, 1.0, 2.0, 0.6812844515091695, 6.9112, 6.9112, 170.5573041426782, 1645229.043443153, 1645229.043443153, 348664.0499019554], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.84, 1.0, 1.0, 0.26782375444684303, 1.0, 1.0, 0.26782375444684303, 1.0, 1.0, 0.6113225018404504, 0.0, 0.0, 0.8375144448122397, 0.45700806762309804, 0.45700806762309804, 0.5203941043312768], 
reward next is 0.4796, 
noisyNet noise sample is [array([0.73994327], dtype=float32), -1.1298548]. 
=============================================
[2019-03-27 05:20:21,924] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.30526585e-08 1.00000000e+00 8.62784711e-15 1.82355144e-08
 3.25005040e-12], sum to 1.0000
[2019-03-27 05:20:21,931] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9673
[2019-03-27 05:20:21,938] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 69.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 11.62012362676609, 6.9112, 168.8713779941689, 5626614.467118903, 2286764.295889491, 459549.2891646721], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4108800.0000, 
sim time next is 4109400.0000, 
raw observation next is [34.0, 69.0, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 1.0, 2.0, 1.03, 9.083356162552837, 6.9112, 170.5573041426782, 5298472.149108404, 3742468.332208353, 692384.8849614355], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.69, 1.0, 1.0, 1.0481927710843375, 1.0, 0.5, 1.0481927710843375, 1.0, 1.0, 1.0365853658536586, 0.2172156162552837, 0.0, 0.8375144448122397, 1.4717978191967789, 1.0395745367245426, 1.033410276061844], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9838756], dtype=float32), 0.45257986]. 
=============================================
[2019-03-27 05:20:23,282] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9071555e-11 1.0000000e+00 1.0840723e-19 4.1415663e-11 4.5936036e-17], sum to 1.0000
[2019-03-27 05:20:23,290] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5727
[2019-03-27 05:20:23,298] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 77.66666666666667, 1.0, 2.0, 0.6134229029188494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857227.1546310324, 857227.1546310324, 203273.6847184631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4130400.0000, 
sim time next is 4131000.0000, 
raw observation next is [31.0, 77.0, 1.0, 2.0, 0.6090133766131698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851062.5981747742, 851062.5981747742, 202437.5336869241], 
processed observation next is [1.0, 0.8260869565217391, 0.6682464454976303, 0.77, 1.0, 1.0, 0.5289317790520117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23640627727077063, 0.23640627727077063, 0.3021455726670509], 
reward next is 0.6979, 
noisyNet noise sample is [array([0.06146969], dtype=float32), 0.7144404]. 
=============================================
[2019-03-27 05:20:23,316] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[58.23092 ]
 [57.34464 ]
 [55.647682]
 [54.033   ]
 [51.87071 ]], R is [[60.02658844]
 [60.12292862]
 [60.2171402 ]
 [60.31015778]
 [60.40197754]].
[2019-03-27 05:20:25,429] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3592508e-13 1.0000000e+00 2.1285153e-22 2.6557006e-13 2.6970416e-18], sum to 1.0000
[2019-03-27 05:20:25,440] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0613
[2019-03-27 05:20:25,447] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.6004596397817472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839104.5010695005, 839104.5010695005, 200831.6371857367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4231200.0000, 
sim time next is 4231800.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.6014617631809732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840505.459457621, 840505.459457621, 201018.551850352], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5198334496156304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23347373873822805, 0.23347373873822805, 0.3000276893288836], 
reward next is 0.7000, 
noisyNet noise sample is [array([-2.06275], dtype=float32), -1.7729968]. 
=============================================
[2019-03-27 05:20:27,446] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2097884e-11 1.0000000e+00 8.3812850e-19 2.1961653e-11 3.4589259e-17], sum to 1.0000
[2019-03-27 05:20:27,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9278
[2019-03-27 05:20:27,460] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.16666666666667, 59.33333333333333, 1.0, 2.0, 0.5857031586283181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818475.3034357281, 818475.3034357287, 198113.9034681573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4215000.0000, 
sim time next is 4215600.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.5891443359956859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823285.9489174172, 823285.9489174172, 198741.9055793784], 
processed observation next is [1.0, 0.8260869565217391, 0.8104265402843602, 0.6, 1.0, 1.0, 0.5049931758984167, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2286905413659492, 0.2286905413659492, 0.2966297098199677], 
reward next is 0.7034, 
noisyNet noise sample is [array([-1.155254], dtype=float32), 0.87786096]. 
=============================================
[2019-03-27 05:20:28,774] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 05:20:28,776] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:20:28,778] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:20:28,778] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:20:28,780] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:20:28,782] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:20:28,779] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:20:28,782] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:20:28,781] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:20:28,784] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:20:28,786] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:20:28,805] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run14
[2019-03-27 05:20:28,828] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run14
[2019-03-27 05:20:28,845] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run14
[2019-03-27 05:20:28,846] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run14
[2019-03-27 05:20:28,887] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run14
[2019-03-27 05:20:46,408] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035601366]
[2019-03-27 05:20:46,409] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.62556146, 100.0, 1.0, 2.0, 0.3418025349504175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 525703.501042766, 525703.5010427654, 168628.2135587927]
[2019-03-27 05:20:46,412] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:20:46,414] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.5557130e-12 1.0000000e+00 3.3986635e-20 1.5122586e-12 2.4113964e-17], sampled 0.4862317483446257
[2019-03-27 05:21:07,298] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035601366]
[2019-03-27 05:21:07,299] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.43580275, 95.79226480666667, 1.0, 2.0, 0.4936549694474403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689803.242907808, 689803.242907808, 182606.7367421842]
[2019-03-27 05:21:07,300] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:21:07,302] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.6845860e-13 1.0000000e+00 5.1186635e-21 5.8503723e-13 4.0751425e-18], sampled 0.8090943965216814
[2019-03-27 05:21:37,271] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035601366]
[2019-03-27 05:21:37,272] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.7, 48.0, 1.0, 2.0, 0.5079989654742645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709853.3620572198, 709853.3620572193, 184858.4702162784]
[2019-03-27 05:21:37,274] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:21:37,278] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6843824e-13 1.0000000e+00 5.5127620e-22 2.4007865e-13 7.0345528e-19], sampled 0.7502618123607652
[2019-03-27 05:21:39,080] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035601366]
[2019-03-27 05:21:39,084] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.55, 62.0, 1.0, 2.0, 0.5796621027807388, 0.0, 2.0, 0.0, 1.0, 2.0, 1.006681375343449, 6.9112, 6.9112, 168.9129325558338, 1620678.566306754, 1620678.566306754, 354787.7750110411]
[2019-03-27 05:21:39,085] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:21:39,089] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.0475910e-11 1.0000000e+00 7.4525923e-19 2.4046156e-11 3.6977178e-16], sampled 0.004529930857045783
[2019-03-27 05:21:39,309] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035601366]
[2019-03-27 05:21:39,313] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.73494725, 74.895281565, 1.0, 2.0, 0.5342409517950838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746535.5391833925, 746535.5391833925, 189133.8457175872]
[2019-03-27 05:21:39,315] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:21:39,319] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4635250e-13 1.0000000e+00 3.9843494e-22 2.0525735e-13 5.0718455e-19], sampled 0.3630115751712003
[2019-03-27 05:22:07,233] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035601366]
[2019-03-27 05:22:07,235] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.28186188666666, 87.38650403000001, 1.0, 2.0, 0.5358145548072129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 835762.6493443276, 835762.649344327, 199630.8770703967]
[2019-03-27 05:22:07,236] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:22:07,240] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0647804e-12 1.0000000e+00 4.3570155e-21 6.2315745e-13 5.1966613e-18], sampled 0.06405359023634771
[2019-03-27 05:22:11,491] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([3.948567e-05], dtype=float32), 0.035601366]
[2019-03-27 05:22:11,494] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.9, 88.0, 1.0, 2.0, 0.5702972909694529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 796938.7069330532, 796938.7069330526, 195342.3032144388]
[2019-03-27 05:22:11,496] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:22:11,499] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0428690e-13 1.0000000e+00 7.4025152e-22 2.7736114e-13 9.1372749e-19], sampled 0.48290898176464503
[2019-03-27 05:22:22,867] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 05:22:23,275] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8724 2842497929.8887 1131.0000
[2019-03-27 05:22:23,309] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6679 2927337855.8174 1338.0000
[2019-03-27 05:22:23,416] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 05:22:23,437] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 05:22:24,451] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 325000, evaluation results [325000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8253.667897652565, 2927337855.817377, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.87235025343, 2842497929.888741, 1131.0]
[2019-03-27 05:22:28,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2395961e-13 1.0000000e+00 5.0876085e-23 5.4325264e-14 7.5969075e-21], sum to 1.0000
[2019-03-27 05:22:28,608] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7629
[2019-03-27 05:22:28,614] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.621937577563629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869130.8457266962, 869130.8457266962, 204904.5806662329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4315200.0000, 
sim time next is 4315800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6229624248749964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870563.612440204, 870563.612440204, 205102.2556775321], 
processed observation next is [1.0, 0.9565217391304348, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5457378612951764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24182322567783446, 0.24182322567783446, 0.30612276966795837], 
reward next is 0.6939, 
noisyNet noise sample is [array([-0.7365325], dtype=float32), 0.31875786]. 
=============================================
[2019-03-27 05:22:35,966] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.8453391e-14 1.0000000e+00 3.8797909e-23 8.6700490e-14 1.7149849e-18], sum to 1.0000
[2019-03-27 05:22:35,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2472
[2019-03-27 05:22:35,979] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 79.0, 1.0, 2.0, 0.5745304247726876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 802856.3628796691, 802856.3628796698, 196096.1862347673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4473600.0000, 
sim time next is 4474200.0000, 
raw observation next is [29.16666666666667, 79.0, 1.0, 2.0, 0.5698910863366542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796370.8600295669, 796370.8600295669, 195269.9276933215], 
processed observation next is [0.0, 0.782608695652174, 0.581358609794629, 0.79, 1.0, 1.0, 0.4817964895622339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2212141277859908, 0.2212141277859908, 0.29144765327361416], 
reward next is 0.7086, 
noisyNet noise sample is [array([-1.1291862], dtype=float32), 1.1474391]. 
=============================================
[2019-03-27 05:22:37,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8259474e-14 1.0000000e+00 6.2959643e-22 6.4653431e-13 9.5288066e-20], sum to 1.0000
[2019-03-27 05:22:37,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7521
[2019-03-27 05:22:37,981] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 79.0, 1.0, 2.0, 0.5698910863366542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796370.8600295669, 796370.8600295669, 195269.9276933215], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4474200.0000, 
sim time next is 4474800.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5642307618087112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788458.1365767573, 788458.1365767573, 194270.5614235827], 
processed observation next is [0.0, 0.8260869565217391, 0.5734597156398105, 0.79, 1.0, 1.0, 0.47497682145627856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21901614904909925, 0.21901614904909925, 0.28995606182624284], 
reward next is 0.7100, 
noisyNet noise sample is [array([-1.3988552], dtype=float32), -0.21182282]. 
=============================================
[2019-03-27 05:22:39,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6568031e-12 1.0000000e+00 3.0339560e-22 6.2874519e-12 1.4887491e-18], sum to 1.0000
[2019-03-27 05:22:39,576] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7042
[2019-03-27 05:22:39,583] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 84.0, 1.0, 2.0, 0.5371457553079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750596.0763388888, 750596.0763388888, 189620.0435537461], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4487400.0000, 
sim time next is 4488000.0000, 
raw observation next is [27.33333333333334, 84.0, 1.0, 2.0, 0.5319539987084964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743338.6874111893, 743338.6874111893, 188753.3489196623], 
processed observation next is [0.0, 0.9565217391304348, 0.4944707740916275, 0.84, 1.0, 1.0, 0.4360891550704775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20648296872533037, 0.20648296872533037, 0.2817214162980034], 
reward next is 0.7183, 
noisyNet noise sample is [array([1.3488628], dtype=float32), -1.6721897]. 
=============================================
[2019-03-27 05:22:39,606] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.35203 ]
 [71.35069 ]
 [71.34998 ]
 [71.34825 ]
 [71.326035]], R is [[71.33676147]
 [71.34038544]
 [71.34275055]
 [71.34380341]
 [71.3440094 ]].
[2019-03-27 05:22:40,095] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1378355e-12 1.0000000e+00 3.4780496e-22 1.5369743e-14 3.6651316e-19], sum to 1.0000
[2019-03-27 05:22:40,108] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5588
[2019-03-27 05:22:40,118] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5025154840471426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702188.4758776496, 702188.4758776496, 183990.9712561955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4510800.0000, 
sim time next is 4511400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5042153837952326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704564.6133413261, 704564.6133413261, 184258.9049253129], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4026691371026898, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19571239259481282, 0.19571239259481282, 0.27501329093330285], 
reward next is 0.7250, 
noisyNet noise sample is [array([1.0915474], dtype=float32), -0.29313222]. 
=============================================
[2019-03-27 05:22:48,652] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8045936e-13 1.0000000e+00 6.2675986e-20 3.7324037e-12 5.4492725e-19], sum to 1.0000
[2019-03-27 05:22:48,658] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3025
[2019-03-27 05:22:48,661] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5586445498791025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780649.0749579842, 780649.0749579842, 193295.1489402235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4647000.0000, 
sim time next is 4647600.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5608018338740058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783664.772508675, 783664.7725086757, 193671.1778542918], 
processed observation next is [1.0, 0.8260869565217391, 0.6682464454976303, 0.7, 1.0, 1.0, 0.4708455829807299, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2176846590301875, 0.2176846590301877, 0.2890614594840176], 
reward next is 0.7109, 
noisyNet noise sample is [array([-1.6060297], dtype=float32), 0.15165739]. 
=============================================
[2019-03-27 05:22:49,470] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1547301e-13 1.0000000e+00 5.7912044e-22 7.7112318e-13 2.0758355e-19], sum to 1.0000
[2019-03-27 05:22:49,479] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4008
[2019-03-27 05:22:49,485] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 90.66666666666667, 1.0, 2.0, 0.5004445329501753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 699293.6861796705, 699293.6861796705, 183665.4725159302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4666800.0000, 
sim time next is 4667400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5023556926498313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701965.1180817671, 701965.1180817671, 183965.8295202671], 
processed observation next is [1.0, 0.0, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40042854536124245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19499031057826866, 0.19499031057826866, 0.2745758649556226], 
reward next is 0.7254, 
noisyNet noise sample is [array([0.8599511], dtype=float32), 0.057599295]. 
=============================================
[2019-03-27 05:22:57,276] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0218432e-12 1.0000000e+00 9.1120386e-19 8.6487893e-11 2.0853231e-16], sum to 1.0000
[2019-03-27 05:22:57,283] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1856
[2019-03-27 05:22:57,287] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 70.0, 1.0, 2.0, 0.5041326307496193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704448.9401545095, 704448.9401545101, 184246.6101248209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4819200.0000, 
sim time next is 4819800.0000, 
raw observation next is [29.16666666666667, 70.0, 1.0, 2.0, 0.5025149560154667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702187.737790333, 702187.7377903324, 183991.2982153274], 
processed observation next is [1.0, 0.782608695652174, 0.581358609794629, 0.7, 1.0, 1.0, 0.40062042893429717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1950521493862036, 0.19505214938620344, 0.2746138779333245], 
reward next is 0.7254, 
noisyNet noise sample is [array([0.9477914], dtype=float32), 2.4937522]. 
=============================================
[2019-03-27 05:23:00,064] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3160241e-14 1.0000000e+00 1.4633140e-21 1.1576145e-13 1.2484023e-19], sum to 1.0000
[2019-03-27 05:23:00,073] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9733
[2019-03-27 05:23:00,076] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4848186778890622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677452.0127284338, 677452.0127284338, 181251.4184775893], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4845600.0000, 
sim time next is 4846200.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.9806421946334349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1370728.567619118, 1370728.567619118, 293079.7629098151], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.9766773429318493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38075793544975495, 0.38075793544975495, 0.43743248195494794], 
reward next is 0.5626, 
noisyNet noise sample is [array([0.09498411], dtype=float32), 0.45575547]. 
=============================================
[2019-03-27 05:23:01,759] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4981911e-08 1.0000000e+00 3.1883999e-15 1.0520684e-08 2.3307419e-13], sum to 1.0000
[2019-03-27 05:23:01,766] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9591
[2019-03-27 05:23:01,777] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1858884.49889493 W.
[2019-03-27 05:23:01,783] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.664791393026437, 1.0, 2.0, 0.664791393026437, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1858884.49889493, 1858884.49889493, 359416.2545789533], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4882800.0000, 
sim time next is 4883400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4147414246580975, 1.0, 2.0, 0.4147414246580975, 1.0, 1.0, 0.7166668090654802, 6.9112, 6.9112, 170.5573041426782, 1739448.231634422, 1739448.231634422, 360674.418099232], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.2948691863350572, 1.0, 1.0, 0.2948691863350572, 1.0, 0.5, 0.6544717183725368, 0.0, 0.0, 0.8375144448122397, 0.483180064342895, 0.483180064342895, 0.5383200270137791], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.10399222], dtype=float32), 0.20619549]. 
=============================================
[2019-03-27 05:23:03,014] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0378141e-08 9.9999988e-01 1.6262164e-14 8.8273616e-08 2.3305698e-12], sum to 1.0000
[2019-03-27 05:23:03,023] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9141
[2019-03-27 05:23:03,034] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1910994.346680752 W.
[2019-03-27 05:23:03,037] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.16666666666667, 65.16666666666667, 1.0, 2.0, 0.6834107919622631, 1.0, 2.0, 0.6834107919622631, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1910994.346680752, 1910994.346680752, 367097.1746551011], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4967400.0000, 
sim time next is 4968000.0000, 
raw observation next is [30.2, 65.0, 1.0, 2.0, 0.4560869336283865, 1.0, 2.0, 0.4560869336283865, 1.0, 1.0, 0.7782221623285754, 6.9112, 6.9112, 170.5573041426782, 1913008.355600218, 1913008.355600218, 383830.1767957154], 
processed observation next is [1.0, 0.5217391304347826, 0.6303317535545023, 0.65, 1.0, 1.0, 0.344683052564321, 1.0, 1.0, 0.344683052564321, 1.0, 0.5, 0.7295392223519213, 0.0, 0.0, 0.8375144448122397, 0.5313912098889495, 0.5313912098889495, 0.5728808608891275], 
reward next is 0.4271, 
noisyNet noise sample is [array([0.31259325], dtype=float32), -0.076569274]. 
=============================================
[2019-03-27 05:23:03,047] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[36.27504 ]
 [35.981583]
 [35.316124]
 [36.845474]
 [37.59427 ]], R is [[35.46806335]
 [35.56547928]
 [35.66792679]
 [35.31124878]
 [34.95813751]].
[2019-03-27 05:23:04,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.641768e-14 1.000000e+00 4.555756e-23 1.227847e-13 6.106086e-19], sum to 1.0000
[2019-03-27 05:23:04,979] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4374
[2019-03-27 05:23:04,987] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.8223806614057153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1149392.966333013, 1149392.966333013, 249476.1266131804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4945200.0000, 
sim time next is 4945800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.8657238211015147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1210005.662553092, 1210005.662553092, 260680.6476420635], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.8382214712066441, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33611268404252553, 0.33611268404252553, 0.38907559349561716], 
reward next is 0.6109, 
noisyNet noise sample is [array([-0.6825117], dtype=float32), 0.4339554]. 
=============================================
[2019-03-27 05:23:11,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5702225e-13 1.0000000e+00 7.8478983e-22 2.1238059e-12 7.0088887e-20], sum to 1.0000
[2019-03-27 05:23:11,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4098
[2019-03-27 05:23:11,218] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 84.0, 1.0, 2.0, 0.5230875223035841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730944.6409575349, 730944.6409575342, 187292.8553258931], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5038200.0000, 
sim time next is 5038800.0000, 
raw observation next is [27.66666666666666, 84.0, 1.0, 2.0, 0.5270428143030901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736473.5469318789, 736473.5469318789, 187942.1343033622], 
processed observation next is [0.0, 0.30434782608695654, 0.5102685624012636, 0.84, 1.0, 1.0, 0.4301720654254097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20457598525885526, 0.20457598525885526, 0.2805106482139734], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.3020927], dtype=float32), -0.5229759]. 
=============================================
[2019-03-27 05:23:14,244] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2070278e-13 1.0000000e+00 1.3022576e-22 4.4094496e-14 2.4219577e-20], sum to 1.0000
[2019-03-27 05:23:14,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1695
[2019-03-27 05:23:14,261] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5072402645211846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708792.8360748034, 708792.8360748028, 184737.793928313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5102400.0000, 
sim time next is 5103000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5066312681426557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707941.5706337645, 707941.5706337652, 184641.1618322011], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4055798411357297, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1966504362871568, 0.196650436287157, 0.2755838236301509], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.33601627], dtype=float32), 0.1127327]. 
=============================================
[2019-03-27 05:23:14,277] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.31766]
 [73.63809]
 [73.82836]
 [73.99604]
 [74.17939]], R is [[73.03030396]
 [73.02427673]
 [73.018013  ]
 [73.0114975 ]
 [73.0046463 ]].
[2019-03-27 05:23:14,416] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6030632e-13 1.0000000e+00 1.5415157e-22 3.7787597e-14 1.0071910e-20], sum to 1.0000
[2019-03-27 05:23:14,424] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2352
[2019-03-27 05:23:14,428] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5084905153081465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710540.4599146695, 710540.4599146689, 184936.5208529279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5101800.0000, 
sim time next is 5102400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5072402645211846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708792.8360748034, 708792.8360748028, 184737.793928313], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4063135717122706, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1968868989096676, 0.19688689890966743, 0.27572805063927314], 
reward next is 0.7243, 
noisyNet noise sample is [array([-0.98603374], dtype=float32), -0.14891401]. 
=============================================
[2019-03-27 05:23:18,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5182056e-13 1.0000000e+00 7.2583370e-23 2.0332301e-14 2.0570576e-20], sum to 1.0000
[2019-03-27 05:23:18,374] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3163
[2019-03-27 05:23:18,380] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5427956296079233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758493.9095505218, 758493.9095505225, 190572.9430491542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5160000.0000, 
sim time next is 5160600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5417049376440679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756969.2510182518, 756969.2510182523, 190388.4717676915], 
processed observation next is [0.0, 0.7391304347826086, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4478372742699613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2102692363939588, 0.21026923639395897, 0.28416189816073356], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.34113273], dtype=float32), -0.7844247]. 
=============================================
[2019-03-27 05:23:18,495] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 05:23:18,497] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:23:18,498] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:23:18,499] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:23:18,500] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:23:18,501] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:23:18,503] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:23:18,504] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:23:18,504] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:23:18,505] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:23:18,504] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:23:18,526] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run15
[2019-03-27 05:23:18,526] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run15
[2019-03-27 05:23:18,527] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run15
[2019-03-27 05:23:18,577] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run15
[2019-03-27 05:23:18,596] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run15
[2019-03-27 05:24:15,855] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([5.7318648e-05], dtype=float32), 0.03665187]
[2019-03-27 05:24:15,856] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.91666666666667, 60.5, 1.0, 2.0, 0.5761894814342273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805175.6291827959, 805175.6291827959, 196393.4161199305]
[2019-03-27 05:24:15,857] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:24:15,859] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.5622268e-14 1.0000000e+00 5.6202452e-23 5.8201077e-14 2.6984178e-20], sampled 0.6352314006527542
[2019-03-27 05:24:41,101] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([5.7318648e-05], dtype=float32), 0.03665187]
[2019-03-27 05:24:41,102] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.76666666666667, 91.66666666666666, 1.0, 2.0, 0.6207986674549771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867538.6181319352, 867538.6181319352, 204684.5773250865]
[2019-03-27 05:24:41,103] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:24:41,105] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.5432983e-14 1.0000000e+00 5.8665112e-23 5.1430888e-14 2.0453596e-20], sampled 0.25052908872114943
[2019-03-27 05:24:47,131] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([5.7318648e-05], dtype=float32), 0.03665187]
[2019-03-27 05:24:47,132] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.84382150166667, 82.48809669333333, 1.0, 2.0, 0.9764370453849414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104262, 1364846.887672975, 1364846.887672974, 291823.8458802854]
[2019-03-27 05:24:47,133] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:24:47,134] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.0798730e-11 1.0000000e+00 1.5947167e-18 5.0673701e-11 1.6967403e-16], sampled 0.3243725433057134
[2019-03-27 05:25:13,124] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6365 2927358667.7485 1338.0000
[2019-03-27 05:25:13,400] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 05:25:13,482] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 05:25:13,585] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5735 3007655529.2020 1766.0000
[2019-03-27 05:25:13,619] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1321 2842492854.9742 1131.0000
[2019-03-27 05:25:14,635] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 350000, evaluation results [350000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8253.636471839933, 2927358667.748515, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7997.5734676127495, 3007655529.2019873, 1766.0, 8496.132107770307, 2842492854.9741545, 1131.0]
[2019-03-27 05:25:18,152] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.98108296e-07 9.99999166e-01 3.24548656e-13 6.02279044e-07
 1.12456745e-11], sum to 1.0000
[2019-03-27 05:25:18,158] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4905
[2019-03-27 05:25:18,165] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2237188.706089075 W.
[2019-03-27 05:25:18,172] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.9586944153361294, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991148760808, 6.9112, 168.912315940265, 2237188.706089075, 2169940.857724204, 450844.2808028855], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5230800.0000, 
sim time next is 5231400.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.7425624639392383, 1.0, 1.0, 0.7425624639392383, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2076558.110410291, 2076558.110410291, 392884.3574329398], 
processed observation next is [1.0, 0.5652173913043478, 0.7156398104265403, 0.67, 1.0, 1.0, 0.6898342939026967, 1.0, 0.5, 0.6898342939026967, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5768216973361919, 0.5768216973361919, 0.586394563332746], 
reward next is 0.4136, 
noisyNet noise sample is [array([-1.7235192], dtype=float32), 0.14552844]. 
=============================================
[2019-03-27 05:25:19,865] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.5145630e-08 9.9999714e-01 6.7208517e-12 2.7718495e-06 6.6128984e-11], sum to 1.0000
[2019-03-27 05:25:19,875] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0751
[2019-03-27 05:25:19,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2862425.451871767 W.
[2019-03-27 05:25:19,890] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.1, 53.0, 1.0, 2.0, 0.7231562834781129, 1.0, 2.0, 0.6821681812533189, 1.0, 2.0, 1.03, 7.005099558078423, 6.9112, 170.5573041426782, 2862425.451871767, 2795161.378830379, 528743.4530764375], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5321400.0000, 
sim time next is 5322000.0000, 
raw observation next is [36.1, 53.0, 1.0, 2.0, 0.7767105926711161, 1.0, 2.0, 0.7089453358498206, 1.0, 2.0, 1.03, 7.005103781494618, 6.9112, 170.5573041426782, 2974918.067319323, 2907650.968873118, 546653.0258913477], 
processed observation next is [1.0, 0.6086956521739131, 0.9099526066350712, 0.53, 1.0, 1.0, 0.7309766176760435, 1.0, 1.0, 0.6493317299395428, 1.0, 1.0, 1.0365853658536586, 0.009390378149461842, 0.0, 0.8375144448122397, 0.8263661298109231, 0.8076808246869772, 0.8159000386438024], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3441999], dtype=float32), 0.29669356]. 
=============================================
[2019-03-27 05:25:19,907] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[25.183224]
 [25.569584]
 [26.013773]
 [24.986668]
 [23.662481]], R is [[24.14236832]
 [23.90094566]
 [23.66193581]
 [23.42531586]
 [23.19106293]].
[2019-03-27 05:25:25,215] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.22987553e-15 1.00000000e+00 1.50101777e-22 4.20757450e-14
 1.00646894e-20], sum to 1.0000
[2019-03-27 05:25:25,222] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6179
[2019-03-27 05:25:25,226] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 85.0, 1.0, 2.0, 0.6026620106718366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842183.3957562943, 842183.3957562943, 201243.2119418286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5360400.0000, 
sim time next is 5361000.0000, 
raw observation next is [29.46666666666667, 85.16666666666667, 1.0, 2.0, 0.6027389735796208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842290.9893702389, 842290.9893702389, 201257.5864045632], 
processed observation next is [1.0, 0.043478260869565216, 0.5955766192733019, 0.8516666666666667, 1.0, 1.0, 0.5213722573248443, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2339697192695108, 0.2339697192695108, 0.30038445732024355], 
reward next is 0.6996, 
noisyNet noise sample is [array([0.7000338], dtype=float32), -0.63870835]. 
=============================================
[2019-03-27 05:25:25,238] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[68.67444 ]
 [68.83074 ]
 [69.015   ]
 [69.249275]
 [69.506065]], R is [[68.48570251]
 [68.50048828]
 [68.5150528 ]
 [68.52934265]
 [68.54327393]].
[2019-03-27 05:25:28,015] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.57446462e-13 1.00000000e+00 1.25256415e-23 1.06103684e-14
 8.29564815e-21], sum to 1.0000
[2019-03-27 05:25:28,027] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0722
[2019-03-27 05:25:28,034] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1800026.595570599 W.
[2019-03-27 05:25:28,038] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.7, 81.83333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.398970201970895, 6.9112, 168.9098758912226, 1800026.595570599, 1453991.937601569, 311354.7977091381], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5467800.0000, 
sim time next is 5468400.0000, 
raw observation next is [29.9, 81.0, 1.0, 2.0, 0.5528279850065794, 1.0, 1.0, 0.5528279850065794, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1545587.520801519, 1545587.520801519, 317493.7887383871], 
processed observation next is [1.0, 0.30434782608695654, 0.6161137440758293, 0.81, 1.0, 1.0, 0.46123853615250526, 1.0, 0.5, 0.46123853615250526, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.42932986688931085, 0.42932986688931085, 0.47387132647520464], 
reward next is 0.5261, 
noisyNet noise sample is [array([0.6899276], dtype=float32), 0.5364304]. 
=============================================
[2019-03-27 05:25:34,483] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2408636e-06 9.9999511e-01 1.0972937e-11 3.6568981e-06 1.3642303e-10], sum to 1.0000
[2019-03-27 05:25:34,493] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6793
[2019-03-27 05:25:34,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2327511.913775438 W.
[2019-03-27 05:25:34,508] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.1, 48.66666666666666, 1.0, 2.0, 1.02322632130563, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.982588116423358, 6.9112, 168.9125322485755, 2327511.913775438, 2276866.854201831, 471373.9326357753], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5579400.0000, 
sim time next is 5580000.0000, 
raw observation next is [34.2, 48.0, 1.0, 2.0, 0.5685629145366035, 1.0, 1.0, 0.5685629145366035, 1.0, 2.0, 0.9751140669395992, 6.911200000000001, 6.9112, 170.5573041426782, 2385274.757195194, 2385274.757195193, 463221.7103096198], 
processed observation next is [1.0, 0.6086956521739131, 0.8199052132701423, 0.48, 1.0, 1.0, 0.48019628257422103, 1.0, 0.5, 0.48019628257422103, 1.0, 1.0, 0.9696513011458527, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6625763214431094, 0.6625763214431091, 0.6913756870292832], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.73564845], dtype=float32), -0.33370152]. 
=============================================
[2019-03-27 05:25:34,520] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[22.098764]
 [21.229235]
 [22.141731]
 [22.639212]
 [23.40677 ]], R is [[21.0521946 ]
 [20.8416729 ]
 [20.96629333]
 [21.08415794]
 [21.2264061 ]].
[2019-03-27 05:25:40,512] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9676309e-12 1.0000000e+00 1.7367277e-21 2.0904475e-13 8.8570920e-20], sum to 1.0000
[2019-03-27 05:25:40,522] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7157
[2019-03-27 05:25:40,528] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 91.0, 1.0, 2.0, 0.4978311659426183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695640.7150467213, 695640.7150467207, 183256.7908125539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5634000.0000, 
sim time next is 5634600.0000, 
raw observation next is [25.76666666666667, 90.16666666666667, 1.0, 2.0, 0.4987063692652957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696864.0751059032, 696864.0751059025, 183393.5386969224], 
processed observation next is [0.0, 0.21739130434782608, 0.42022116903633505, 0.9016666666666667, 1.0, 1.0, 0.3960317701991515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19357335419608424, 0.19357335419608404, 0.27372169954764536], 
reward next is 0.7263, 
noisyNet noise sample is [array([1.2623241], dtype=float32), -1.1045805]. 
=============================================
[2019-03-27 05:25:45,304] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0885116e-13 1.0000000e+00 5.4481808e-23 2.0320942e-14 4.5461627e-20], sum to 1.0000
[2019-03-27 05:25:45,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6794
[2019-03-27 05:25:45,321] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333333, 87.0, 1.0, 2.0, 0.5113840476243214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714585.1043146537, 714585.1043146544, 185398.5746482973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5708400.0000, 
sim time next is 5709000.0000, 
raw observation next is [26.41666666666666, 87.0, 1.0, 2.0, 0.5106017814643823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713491.6336039457, 713491.6336039457, 185273.4712012899], 
processed observation next is [0.0, 0.043478260869565216, 0.4510268562401261, 0.87, 1.0, 1.0, 0.4103635921257618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19819212044554047, 0.19819212044554047, 0.27652756895714914], 
reward next is 0.7235, 
noisyNet noise sample is [array([-0.18457343], dtype=float32), 1.2195317]. 
=============================================
[2019-03-27 05:25:45,338] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.02586]
 [74.31218]
 [74.62739]
 [74.96676]
 [75.20055]], R is [[73.73970032]
 [73.72559357]
 [73.71142578]
 [73.69715118]
 [73.68264771]].
[2019-03-27 05:25:59,796] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2810647e-13 1.0000000e+00 3.5309497e-21 2.1524000e-13 3.1640259e-20], sum to 1.0000
[2019-03-27 05:25:59,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1387
[2019-03-27 05:25:59,809] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 88.0, 1.0, 2.0, 0.5579893811815649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 779733.2071424638, 779733.2071424631, 193180.4869477111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5953200.0000, 
sim time next is 5953800.0000, 
raw observation next is [27.68333333333334, 88.5, 1.0, 2.0, 0.5570933010978342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778480.5678235178, 778480.5678235178, 193024.8443718945], 
processed observation next is [1.0, 0.9130434782608695, 0.511058451816746, 0.885, 1.0, 1.0, 0.4663774712022098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2162446021731994, 0.2162446021731994, 0.28809678264461863], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.8859903], dtype=float32), -1.0776533]. 
=============================================
[2019-03-27 05:26:01,333] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8546280e-12 1.0000000e+00 7.8368634e-21 1.9036210e-13 5.3513814e-19], sum to 1.0000
[2019-03-27 05:26:01,341] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4068
[2019-03-27 05:26:01,347] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2099962.375537966 W.
[2019-03-27 05:26:01,351] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.03333333333333, 82.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.821479272219213, 6.9112, 168.9081532308424, 2099962.375537966, 1454197.294199544, 311353.450260544], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5991600.0000, 
sim time next is 5992200.0000, 
raw observation next is [29.2, 81.5, 1.0, 2.0, 0.709932007011547, 1.0, 1.0, 0.709932007011547, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1985223.271569236, 1985223.271569236, 378411.1683513144], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.815, 1.0, 1.0, 0.6505204903753579, 1.0, 0.5, 0.6505204903753579, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5514509087692322, 0.5514509087692322, 0.5647927885840514], 
reward next is 0.4352, 
noisyNet noise sample is [array([0.5922625], dtype=float32), 0.092061214]. 
=============================================
[2019-03-27 05:26:04,120] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2057332e-13 1.0000000e+00 2.2235033e-22 2.4884980e-13 4.7571254e-21], sum to 1.0000
[2019-03-27 05:26:04,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0669
[2019-03-27 05:26:04,131] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 86.66666666666667, 1.0, 2.0, 0.540030127717527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754628.068830808, 754628.068830808, 190105.7250444031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6038400.0000, 
sim time next is 6039000.0000, 
raw observation next is [27.4, 87.0, 1.0, 2.0, 0.5402887407433934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754989.5782987179, 754989.5782987179, 190149.2567552365], 
processed observation next is [1.0, 0.9130434782608695, 0.4976303317535545, 0.87, 1.0, 1.0, 0.44613101294384744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2097193273051994, 0.2097193273051994, 0.2838048608287112], 
reward next is 0.7162, 
noisyNet noise sample is [array([0.58737373], dtype=float32), 1.0573403]. 
=============================================
[2019-03-27 05:26:04,144] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.58842 ]
 [67.96508 ]
 [68.366196]
 [67.947075]
 [67.494064]], R is [[68.52440643]
 [68.55542755]
 [68.58629608]
 [68.61702728]
 [68.64755249]].
[2019-03-27 05:26:07,273] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3674502e-12 1.0000000e+00 4.8176829e-21 2.0888442e-14 3.9911705e-19], sum to 1.0000
[2019-03-27 05:26:07,281] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0695
[2019-03-27 05:26:07,285] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 89.66666666666667, 1.0, 2.0, 0.674263405035288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 942286.3631138303, 942286.3631138309, 215387.915329292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6069000.0000, 
sim time next is 6069600.0000, 
raw observation next is [26.9, 89.0, 1.0, 2.0, 0.6834732248677882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 955162.9186698908, 955162.9186698901, 217318.3066567561], 
processed observation next is [1.0, 0.2608695652173913, 0.4739336492890995, 0.89, 1.0, 1.0, 0.6186424395997447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26532303296385856, 0.2653230329638584, 0.32435568157724787], 
reward next is 0.6756, 
noisyNet noise sample is [array([-1.8400668], dtype=float32), 0.86098814]. 
=============================================
[2019-03-27 05:26:08,792] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 05:26:08,793] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:26:08,794] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:26:08,795] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:26:08,796] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:26:08,798] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:26:08,800] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:26:08,797] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:26:08,800] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:26:08,801] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:26:08,804] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:26:08,820] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run16
[2019-03-27 05:26:08,821] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run16
[2019-03-27 05:26:08,822] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run16
[2019-03-27 05:26:08,867] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run16
[2019-03-27 05:26:08,903] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run16
[2019-03-27 05:26:20,129] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.0358582]
[2019-03-27 05:26:20,131] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.4, 74.0, 1.0, 2.0, 0.3523840997780265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560625.3570253667, 560625.3570253667, 171835.6501998944]
[2019-03-27 05:26:20,132] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:26:20,134] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5599622e-12 1.0000000e+00 3.4598710e-20 1.4735296e-12 2.4916928e-18], sampled 0.7992156010022868
[2019-03-27 05:26:45,517] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.0358582]
[2019-03-27 05:26:45,520] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.86666666666667, 88.5, 1.0, 2.0, 0.3962181821092796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588399.0240427286, 588399.0240427292, 173384.3455146827]
[2019-03-27 05:26:45,522] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:26:45,524] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1112691e-12 1.0000000e+00 9.8666330e-21 6.5200671e-13 7.2267177e-19], sampled 0.5080559480105671
[2019-03-27 05:27:08,996] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.0358582]
[2019-03-27 05:27:08,997] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.10686914, 71.435059215, 1.0, 2.0, 0.7215329372138084, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976628009609, 6.9112, 168.9123160332099, 1905261.424999761, 1838023.878079493, 389395.3592577161]
[2019-03-27 05:27:08,999] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:27:09,002] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.8107352e-10 1.0000000e+00 7.3743780e-17 6.0155192e-10 2.9722102e-15], sampled 0.49600967361701
[2019-03-27 05:27:09,003] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1905261.424999761 W.
[2019-03-27 05:27:24,876] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.0358582]
[2019-03-27 05:27:24,877] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.03333333333333, 73.33333333333334, 1.0, 2.0, 0.6071775903924442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 848496.1634830114, 848496.1634830114, 202089.6746038916]
[2019-03-27 05:27:24,878] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:27:24,880] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9899878e-13 1.0000000e+00 7.9781469e-22 1.7747964e-13 6.7562400e-20], sampled 0.7266473865619912
[2019-03-27 05:27:30,330] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.0358582]
[2019-03-27 05:27:30,331] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.39486710666667, 82.27088463666666, 1.0, 2.0, 0.5249166985827941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733501.5519293679, 733501.5519293674, 187591.4025384469]
[2019-03-27 05:27:30,334] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:27:30,336] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2901862e-13 1.0000000e+00 1.7251451e-22 1.1804487e-13 1.5758657e-20], sampled 0.5092723150684321
[2019-03-27 05:28:01,514] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.0358582]
[2019-03-27 05:28:01,517] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.5, 90.33333333333334, 1.0, 2.0, 0.5881361047409477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821876.4746654474, 821876.4746654474, 198549.443549744]
[2019-03-27 05:28:01,518] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:28:01,521] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.1279288e-13 1.0000000e+00 1.9215391e-21 2.9928975e-13 1.6918063e-19], sampled 0.6578065254797925
[2019-03-27 05:28:03,514] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0344 2842500170.3949 1131.0000
[2019-03-27 05:28:03,653] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6871 3164041167.3990 1778.0000
[2019-03-27 05:28:03,750] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 05:28:03,955] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.7806 3007626493.1219 1766.0000
[2019-03-27 05:28:03,982] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9878 2779131868.1217 933.0000
[2019-03-27 05:28:04,997] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 375000, evaluation results [375000.0, 7882.687094126012, 3164041167.398958, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8659.98775396918, 2779131868.121704, 933.0, 7996.780628304828, 3007626493.121923, 1766.0, 8496.034361333246, 2842500170.394918, 1131.0]
[2019-03-27 05:28:05,244] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.08304505e-07 9.99999523e-01 5.16316915e-13 3.39043254e-07
 9.50074828e-12], sum to 1.0000
[2019-03-27 05:28:05,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5389
[2019-03-27 05:28:05,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2018916.34657564 W.
[2019-03-27 05:28:05,265] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.3, 77.0, 1.0, 2.0, 0.4813130549512784, 1.0, 2.0, 0.4813130549512784, 1.0, 2.0, 0.8358816037907973, 6.9112, 6.9112, 170.5573041426782, 2018916.34657564, 2018916.34657564, 402463.5757772183], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6174000.0000, 
sim time next is 6174600.0000, 
raw observation next is [29.38333333333333, 76.5, 1.0, 2.0, 0.4447868949028854, 1.0, 2.0, 0.4447868949028854, 1.0, 2.0, 0.7724477431724505, 6.9112, 6.9112, 170.5573041426782, 1865570.275780265, 1865570.275780265, 379065.3873437668], 
processed observation next is [1.0, 0.4782608695652174, 0.5916271721958924, 0.765, 1.0, 1.0, 0.3310685480757656, 1.0, 1.0, 0.3310685480757656, 1.0, 1.0, 0.722497247771281, 0.0, 0.0, 0.8375144448122397, 0.518213965494518, 0.518213965494518, 0.565769234841443], 
reward next is 0.4342, 
noisyNet noise sample is [array([-0.00021795], dtype=float32), -0.8525093]. 
=============================================
[2019-03-27 05:28:06,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6800167e-13 1.0000000e+00 5.4009441e-21 4.9770907e-14 4.9864541e-20], sum to 1.0000
[2019-03-27 05:28:06,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1740
[2019-03-27 05:28:06,280] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 90.66666666666667, 1.0, 2.0, 0.5369747140129834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750356.9824451448, 750356.9824451454, 189592.1813109072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6139200.0000, 
sim time next is 6139800.0000, 
raw observation next is [26.8, 91.0, 1.0, 2.0, 0.538071823698464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751890.6026538422, 751890.6026538422, 189776.2377056621], 
processed observation next is [1.0, 0.043478260869565216, 0.4691943127962086, 0.91, 1.0, 1.0, 0.4434600285523662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20885850073717838, 0.20885850073717838, 0.2832481159786001], 
reward next is 0.7168, 
noisyNet noise sample is [array([0.876948], dtype=float32), 0.09373376]. 
=============================================
[2019-03-27 05:28:08,867] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.2618769e-08 9.9999964e-01 5.4067126e-14 2.6491486e-07 1.8318754e-12], sum to 1.0000
[2019-03-27 05:28:08,879] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7226
[2019-03-27 05:28:08,885] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1980306.60762465 W.
[2019-03-27 05:28:08,890] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.05, 78.5, 1.0, 2.0, 0.7081753914603695, 1.0, 2.0, 0.7081753914603695, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1980306.60762465, 1980306.60762465, 377645.4428064942], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6172200.0000, 
sim time next is 6172800.0000, 
raw observation next is [29.13333333333333, 78.0, 1.0, 2.0, 0.7382319280349517, 1.0, 2.0, 0.7382319280349517, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2064436.201623176, 2064436.201623176, 390921.6887928167], 
processed observation next is [1.0, 0.43478260869565216, 0.5797788309636649, 0.78, 1.0, 1.0, 0.6846167807650021, 1.0, 1.0, 0.6846167807650021, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5734545004508822, 0.5734545004508822, 0.5834652071534577], 
reward next is 0.4165, 
noisyNet noise sample is [array([-1.622323], dtype=float32), -0.8439421]. 
=============================================
[2019-03-27 05:28:13,121] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3583151e-13 1.0000000e+00 1.4023038e-21 1.4365236e-12 1.1992630e-19], sum to 1.0000
[2019-03-27 05:28:13,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7936
[2019-03-27 05:28:13,132] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.55, 74.83333333333334, 1.0, 2.0, 0.5418757336280602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757208.0034848315, 757208.0034848315, 190417.7333256967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6256200.0000, 
sim time next is 6256800.0000, 
raw observation next is [29.7, 74.0, 1.0, 2.0, 0.5419591470242147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757324.6055087395, 757324.6055087395, 190431.8515311663], 
processed observation next is [0.0, 0.43478260869565216, 0.6066350710900474, 0.74, 1.0, 1.0, 0.4481435506315839, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21036794597464986, 0.21036794597464986, 0.2842266440763676], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.6989249], dtype=float32), 1.1197017]. 
=============================================
[2019-03-27 05:28:15,355] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.8094889e-14 1.0000000e+00 5.7861004e-23 8.1049810e-13 4.3171781e-20], sum to 1.0000
[2019-03-27 05:28:15,362] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9511
[2019-03-27 05:28:15,367] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 62.33333333333334, 1.0, 2.0, 0.5063617427186132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707564.8236539173, 707564.8236539173, 184598.9760853695], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6276000.0000, 
sim time next is 6276600.0000, 
raw observation next is [30.65, 62.5, 1.0, 2.0, 0.5083801552382119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710386.1964374154, 710386.1964374154, 184919.5407924289], 
processed observation next is [0.0, 0.6521739130434783, 0.6516587677725119, 0.625, 1.0, 1.0, 0.407686934021942, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19732949901039318, 0.19732949901039318, 0.2759993146155655], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.7843234], dtype=float32), 1.7309309]. 
=============================================
[2019-03-27 05:28:18,799] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9255864e-13 1.0000000e+00 4.9883679e-23 1.0381317e-14 8.4936094e-20], sum to 1.0000
[2019-03-27 05:28:18,810] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3689
[2019-03-27 05:28:18,816] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.75, 62.16666666666667, 1.0, 2.0, 0.5132764499090148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717230.3550217845, 717230.355021784, 185702.1646817968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6357000.0000, 
sim time next is 6357600.0000, 
raw observation next is [30.7, 62.0, 1.0, 2.0, 0.5106630640326387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713577.2958313425, 713577.2958313425, 185283.4173182215], 
processed observation next is [0.0, 0.6086956521739131, 0.6540284360189573, 0.62, 1.0, 1.0, 0.4104374265453478, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19821591550870624, 0.19821591550870624, 0.27654241390779327], 
reward next is 0.7235, 
noisyNet noise sample is [array([0.643025], dtype=float32), 0.57280564]. 
=============================================
[2019-03-27 05:28:18,821] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3145956e-12 1.0000000e+00 5.4212319e-22 3.3580164e-13 2.4613196e-19], sum to 1.0000
[2019-03-27 05:28:18,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3332
[2019-03-27 05:28:18,838] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.08333333333333, 84.66666666666667, 1.0, 2.0, 0.8095661276354895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1131473.305538636, 1131473.305538636, 246271.0952812609], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6415800.0000, 
sim time next is 6416400.0000, 
raw observation next is [27.16666666666667, 84.33333333333334, 1.0, 2.0, 0.8174522797683645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1142501.152132035, 1142501.152132035, 248238.4832001137], 
processed observation next is [1.0, 0.2608695652173913, 0.4865718799368091, 0.8433333333333334, 1.0, 1.0, 0.7800629876727283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3173614311477875, 0.3173614311477875, 0.37050519880613986], 
reward next is 0.6295, 
noisyNet noise sample is [array([-0.82526433], dtype=float32), -0.37107316]. 
=============================================
[2019-03-27 05:28:19,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0695430e-14 1.0000000e+00 1.4916395e-22 7.0602752e-13 6.7018470e-20], sum to 1.0000
[2019-03-27 05:28:19,334] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5104
[2019-03-27 05:28:19,344] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 62.33333333333333, 1.0, 2.0, 0.5156978665998968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720615.0858719458, 720615.0858719453, 186091.9705653025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6356400.0000, 
sim time next is 6357000.0000, 
raw observation next is [30.75, 62.16666666666667, 1.0, 2.0, 0.5132764499090148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717230.3550217845, 717230.355021784, 185702.1646817968], 
processed observation next is [0.0, 0.5652173913043478, 0.6563981042654029, 0.6216666666666667, 1.0, 1.0, 0.41358608422772863, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19923065417271793, 0.19923065417271776, 0.27716740997283107], 
reward next is 0.7228, 
noisyNet noise sample is [array([0.44856173], dtype=float32), 0.366361]. 
=============================================
[2019-03-27 05:28:19,355] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.189384]
 [73.18972 ]
 [73.163994]
 [73.143135]
 [73.13138 ]], R is [[73.17423248]
 [73.16474152]
 [73.15483856]
 [73.1445694 ]
 [73.13208008]].
[2019-03-27 05:28:20,132] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6571525e-14 1.0000000e+00 2.0507159e-23 4.4485429e-15 8.4287887e-22], sum to 1.0000
[2019-03-27 05:28:20,139] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1058
[2019-03-27 05:28:20,145] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 79.0, 1.0, 2.0, 0.5273614019155969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736918.8859631692, 736918.8859631697, 187993.7105211481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6375600.0000, 
sim time next is 6376200.0000, 
raw observation next is [28.03333333333333, 79.50000000000001, 1.0, 2.0, 0.5286746343410111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738754.5953918314, 738754.5953918314, 188210.3184232313], 
processed observation next is [0.0, 0.8260869565217391, 0.5276461295418641, 0.7950000000000002, 1.0, 1.0, 0.43213811366386884, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20520960983106426, 0.20520960983106426, 0.28091092301974824], 
reward next is 0.7191, 
noisyNet noise sample is [array([-0.737766], dtype=float32), -0.042574205]. 
=============================================
[2019-03-27 05:28:28,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3233643e-13 1.0000000e+00 5.5159780e-23 1.8280914e-13 7.0655849e-20], sum to 1.0000
[2019-03-27 05:28:28,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1081
[2019-03-27 05:28:28,850] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 84.33333333333334, 1.0, 2.0, 0.5178707242791181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723652.3824592204, 723652.3824592197, 186443.6304068216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6562200.0000, 
sim time next is 6562800.0000, 
raw observation next is [27.2, 85.0, 1.0, 2.0, 0.5191763705832839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725477.46478193, 725477.4647819294, 186655.4129660442], 
processed observation next is [1.0, 1.0, 0.4881516587677725, 0.85, 1.0, 1.0, 0.42069442238949867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20152151799498055, 0.2015215179949804, 0.27859016860603614], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.7925707], dtype=float32), 1.2605091]. 
=============================================
[2019-03-27 05:28:41,512] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.7615703e-12 1.0000000e+00 2.7042504e-20 7.4974407e-13 6.6996268e-19], sum to 1.0000
[2019-03-27 05:28:41,519] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4004
[2019-03-27 05:28:41,524] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.23333333333333, 71.33333333333334, 1.0, 2.0, 0.3616199689664652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 555564.3332227304, 555564.3332227311, 171073.382508832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6734400.0000, 
sim time next is 6735000.0000, 
raw observation next is [25.11666666666667, 71.66666666666666, 1.0, 2.0, 0.3588082607352506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 552194.5937708472, 552194.5937708472, 170815.5840962378], 
processed observation next is [1.0, 0.9565217391304348, 0.38941548183254365, 0.7166666666666666, 1.0, 1.0, 0.22747983221114526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15338738715856867, 0.15338738715856867, 0.2549486329794594], 
reward next is 0.7451, 
noisyNet noise sample is [array([-0.27161583], dtype=float32), 0.8192088]. 
=============================================
[2019-03-27 05:28:41,537] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.848976]
 [70.868835]
 [70.87773 ]
 [70.88654 ]
 [70.83046 ]], R is [[70.86628723]
 [70.90229034]
 [70.93785858]
 [70.97288513]
 [71.00717926]].
[2019-03-27 05:28:41,600] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.9580617e-13 1.0000000e+00 3.3715573e-21 2.8360951e-13 6.9476632e-20], sum to 1.0000
[2019-03-27 05:28:41,610] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2475
[2019-03-27 05:28:41,613] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 70.66666666666667, 1.0, 2.0, 0.3646489682755514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 558275.8642813705, 558275.86428137, 171249.0850692877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6733200.0000, 
sim time next is 6733800.0000, 
raw observation next is [25.35, 71.0, 1.0, 2.0, 0.3628508173431224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556491.2114429169, 556491.2114429175, 171124.9044373804], 
processed observation next is [1.0, 0.9565217391304348, 0.4004739336492892, 0.71, 1.0, 1.0, 0.23235038234111133, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1545808920674769, 0.15458089206747708, 0.2554103051304185], 
reward next is 0.7446, 
noisyNet noise sample is [array([-2.4087923], dtype=float32), -0.30317524]. 
=============================================
[2019-03-27 05:28:49,022] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.37400897e-13 1.00000000e+00 6.32326224e-21 4.73605042e-13
 1.03593353e-19], sum to 1.0000
[2019-03-27 05:28:49,033] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0993
[2019-03-27 05:28:49,037] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.9, 30.0, 1.0, 2.0, 0.2544687688644385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419707.3587315264, 419707.3587315264, 161319.5096549368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6876000.0000, 
sim time next is 6876600.0000, 
raw observation next is [29.86666666666667, 31.16666666666666, 1.0, 2.0, 0.2530402991085502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 416885.9759888996, 416885.9759889003, 161192.1970138435], 
processed observation next is [0.0, 0.6086956521739131, 0.6145339652448659, 0.3116666666666666, 1.0, 1.0, 0.10004855314283155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11580165999691655, 0.11580165999691674, 0.24058536867737837], 
reward next is 0.7594, 
noisyNet noise sample is [array([1.3233217], dtype=float32), -1.6059391]. 
=============================================
[2019-03-27 05:28:51,941] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.9255161e-15 1.0000000e+00 5.2829526e-24 4.8001579e-16 6.0435550e-22], sum to 1.0000
[2019-03-27 05:28:51,952] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3438
[2019-03-27 05:28:51,958] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 90.0, 1.0, 2.0, 0.4165771048668537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612831.3522172116, 612831.3522172116, 175490.061568499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6926400.0000, 
sim time next is 6927000.0000, 
raw observation next is [23.85, 90.33333333333333, 1.0, 2.0, 0.416518356706541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613002.4067127772, 613002.4067127772, 175513.5447305596], 
processed observation next is [0.0, 0.17391304347826086, 0.3293838862559243, 0.9033333333333333, 1.0, 1.0, 0.29701006832113375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17027844630910477, 0.17027844630910477, 0.2619605145232233], 
reward next is 0.7380, 
noisyNet noise sample is [array([-1.4328201], dtype=float32), -0.3087506]. 
=============================================
[2019-03-27 05:28:51,976] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[76.44955]
 [76.3767 ]
 [76.33282]
 [76.2585 ]
 [76.25119]], R is [[76.48793793]
 [76.46113586]
 [76.43445587]
 [76.40790558]
 [76.38144684]].
[2019-03-27 05:28:59,101] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 05:28:59,103] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:28:59,103] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:28:59,104] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:28:59,104] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:28:59,106] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:28:59,105] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:28:59,107] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:28:59,109] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:28:59,109] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:28:59,112] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:28:59,132] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run17
[2019-03-27 05:28:59,153] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run17
[2019-03-27 05:28:59,155] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run17
[2019-03-27 05:28:59,174] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run17
[2019-03-27 05:28:59,174] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run17
[2019-03-27 05:29:21,742] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.03631179]
[2019-03-27 05:29:21,743] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.85, 57.33333333333333, 1.0, 2.0, 0.3572748588014932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546906.9123869502, 546906.9123869502, 170285.8858871213]
[2019-03-27 05:29:21,745] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:29:21,748] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.1111274e-13 1.0000000e+00 2.5092112e-21 1.7532731e-13 1.1181475e-19], sampled 0.25823201715635846
[2019-03-27 05:29:31,410] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.03631179]
[2019-03-27 05:29:31,410] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.7, 75.5, 1.0, 2.0, 0.9109512674912407, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997317490489197, 6.9112, 168.9123747944431, 2170365.357789699, 2109270.856504254, 437439.9526290587]
[2019-03-27 05:29:31,411] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:29:31,416] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5773181e-10 1.0000000e+00 3.7969023e-17 2.3932692e-10 9.3369681e-16], sampled 0.7651991719600809
[2019-03-27 05:29:31,417] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2170365.357789699 W.
[2019-03-27 05:29:36,981] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.03631179]
[2019-03-27 05:29:36,984] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.96960895, 91.01245955, 1.0, 2.0, 0.6715626983706814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1008120.800767167, 1008120.800767166, 223717.0236146703]
[2019-03-27 05:29:36,987] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:29:36,989] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2442318e-13 1.0000000e+00 3.3447692e-22 5.6631739e-14 1.7198116e-20], sampled 0.6889432721397908
[2019-03-27 05:29:57,762] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.03631179]
[2019-03-27 05:29:57,763] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.66666666666666, 63.5, 1.0, 2.0, 0.9462364749355944, 1.0, 2.0, 0.9462364749355944, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2646754.776800658, 2646754.776800659, 497345.9777385553]
[2019-03-27 05:29:57,764] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:29:57,768] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.7825711e-08 9.9999988e-01 6.0769728e-14 1.1049164e-07 9.4680578e-13], sampled 0.26726799230201514
[2019-03-27 05:29:57,768] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2646754.776800658 W.
[2019-03-27 05:30:30,275] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.03631179]
[2019-03-27 05:30:30,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.63455725, 85.10387138000002, 1.0, 2.0, 0.5157811949162611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720731.5649517197, 720731.5649517191, 186104.778040906]
[2019-03-27 05:30:30,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:30:30,279] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.5914874e-14 1.0000000e+00 8.5473559e-23 3.2002396e-14 3.6333747e-21], sampled 0.0401385315192172
[2019-03-27 05:30:34,131] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.03631179]
[2019-03-27 05:30:34,134] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.03333333333333, 81.66666666666667, 1.0, 2.0, 0.7794797584748636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1089402.145560576, 1089402.145560576, 238936.1462444799]
[2019-03-27 05:30:34,137] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:30:34,141] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3391014e-13 1.0000000e+00 1.1871498e-21 1.0441453e-13 3.8723594e-20], sampled 0.9493208395785487
[2019-03-27 05:30:44,657] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.03631179]
[2019-03-27 05:30:44,658] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.30057316, 85.41489793666666, 1.0, 2.0, 0.4000944758065331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 593366.8722192447, 593366.8722192447, 173818.4024879465]
[2019-03-27 05:30:44,659] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:30:44,662] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1085038e-13 1.0000000e+00 3.4507331e-22 5.4736828e-14 1.3676325e-20], sampled 0.005511508404784982
[2019-03-27 05:30:48,679] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00015141], dtype=float32), 0.03631179]
[2019-03-27 05:30:48,680] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.13333333333333, 92.0, 1.0, 2.0, 0.4824504968940798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674141.8307629039, 674141.8307629032, 180891.9996816422]
[2019-03-27 05:30:48,682] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:30:48,688] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1689657e-14 1.0000000e+00 1.7471194e-22 3.4862328e-14 8.0474140e-21], sampled 0.1422696493335689
[2019-03-27 05:30:53,319] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 05:30:53,465] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4216 3164121225.5141 1778.0000
[2019-03-27 05:30:53,867] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5368 3007675498.4503 1766.0000
[2019-03-27 05:30:53,961] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 05:30:53,972] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 05:30:54,987] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 400000, evaluation results [400000.0, 7883.421573052634, 3164121225.5140734, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7997.536809851376, 3007675498.45028, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 05:31:01,721] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2334798e-14 1.0000000e+00 2.4068585e-23 1.8688501e-13 4.0621942e-20], sum to 1.0000
[2019-03-27 05:31:01,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8296369e-15 1.0000000e+00 2.6257675e-24 3.5481897e-15 9.7255196e-23], sum to 1.0000
[2019-03-27 05:31:01,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1076
[2019-03-27 05:31:01,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5880
[2019-03-27 05:31:01,736] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.75, 86.0, 1.0, 2.0, 0.4740798169992675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662441.5942000849, 662441.5942000843, 179634.3761610241], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7173000.0000, 
sim time next is 7173600.0000, 
raw observation next is [25.76666666666667, 86.0, 1.0, 2.0, 0.4748253106977512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 663483.6133647504, 663483.6133647497, 179745.5331999008], 
processed observation next is [1.0, 0.0, 0.42022116903633505, 0.86, 1.0, 1.0, 0.36725941047921834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18430100371243066, 0.18430100371243047, 0.26827691522373254], 
reward next is 0.7317, 
noisyNet noise sample is [array([0.14411283], dtype=float32), -0.8232651]. 
=============================================
[2019-03-27 05:31:01,738] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.75, 84.5, 1.0, 2.0, 0.4685609498154886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660048.8260676397, 660048.8260676397, 179503.4029037608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7165800.0000, 
sim time next is 7166400.0000, 
raw observation next is [25.73333333333333, 84.66666666666667, 1.0, 2.0, 0.4683753990956585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659578.6357287432, 659578.6357287432, 179448.9864789623], 
processed observation next is [1.0, 0.9565217391304348, 0.41864139020537117, 0.8466666666666667, 1.0, 1.0, 0.35948843264537167, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18321628770242865, 0.18321628770242865, 0.26783430817755566], 
reward next is 0.7322, 
noisyNet noise sample is [array([1.8722879], dtype=float32), -0.35638705]. 
=============================================
[2019-03-27 05:31:15,928] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9382644e-14 1.0000000e+00 3.4289619e-23 1.5771423e-14 2.6797877e-21], sum to 1.0000
[2019-03-27 05:31:15,941] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2373
[2019-03-27 05:31:15,945] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 89.0, 1.0, 2.0, 0.5103061967884225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826402.2734230156, 826402.2734230156, 197352.1683664757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7405200.0000, 
sim time next is 7405800.0000, 
raw observation next is [20.46666666666667, 88.5, 1.0, 2.0, 0.3238150250920718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524529.2638232014, 524529.263823202, 168898.2585364995], 
processed observation next is [1.0, 0.7391304347826086, 0.16903633491311232, 0.885, 1.0, 1.0, 0.1853193073398455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14570257328422262, 0.14570257328422279, 0.2520869530395515], 
reward next is 0.7479, 
noisyNet noise sample is [array([0.7723527], dtype=float32), -0.92970175]. 
=============================================
[2019-03-27 05:31:32,196] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3350142e-12 1.0000000e+00 2.6216179e-21 1.5073710e-13 3.5420546e-20], sum to 1.0000
[2019-03-27 05:31:32,206] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7508
[2019-03-27 05:31:32,212] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 93.0, 1.0, 2.0, 0.495363730226292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692191.7385777482, 692191.7385777482, 182871.6693297447], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7708800.0000, 
sim time next is 7709400.0000, 
raw observation next is [25.0, 92.5, 1.0, 2.0, 0.4969029648518459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 694343.2761949665, 694343.2761949671, 183111.2153675945], 
processed observation next is [1.0, 0.21739130434782608, 0.38388625592417064, 0.925, 1.0, 1.0, 0.3938589937974047, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1928731322763796, 0.19287313227637976, 0.2733003214441709], 
reward next is 0.7267, 
noisyNet noise sample is [array([-0.44553205], dtype=float32), 0.95692015]. 
=============================================
[2019-03-27 05:31:39,747] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.74054684e-11 1.00000000e+00 1.10446485e-16 1.35854758e-10
 5.74102732e-16], sum to 1.0000
[2019-03-27 05:31:39,758] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8283
[2019-03-27 05:31:39,767] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1953116.850537899 W.
[2019-03-27 05:31:39,772] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.2, 74.0, 1.0, 2.0, 0.6984609455777043, 1.0, 1.0, 0.6984609455777043, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1953116.850537899, 1953116.850537899, 373464.1150703757], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7813200.0000, 
sim time next is 7813800.0000, 
raw observation next is [29.3, 73.5, 1.0, 2.0, 0.4844136866004233, 1.0, 2.0, 0.4844136866004233, 1.0, 1.0, 0.8319280292169858, 6.911200000000001, 6.9112, 170.5573041426782, 2031934.594276317, 2031934.594276317, 402900.3882763592], 
processed observation next is [1.0, 0.43478260869565216, 0.5876777251184835, 0.735, 1.0, 1.0, 0.3788116706029196, 1.0, 1.0, 0.3788116706029196, 1.0, 0.5, 0.7950341819719339, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5644262761878658, 0.5644262761878658, 0.6013438630990435], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.05700459], dtype=float32), 0.13253462]. 
=============================================
[2019-03-27 05:31:44,389] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:44,391] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:44,430] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-03-27 05:31:45,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:45,227] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:45,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-03-27 05:31:46,485] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:46,485] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:46,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-03-27 05:31:46,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:46,589] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:46,599] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-03-27 05:31:46,699] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:46,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:46,708] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-03-27 05:31:46,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:46,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:46,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-03-27 05:31:46,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:46,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:46,844] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-03-27 05:31:46,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:46,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:46,866] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:46,866] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:46,870] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-03-27 05:31:46,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-03-27 05:31:46,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:46,950] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:46,953] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-03-27 05:31:46,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:46,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:46,993] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-03-27 05:31:47,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:47,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:47,114] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-03-27 05:31:47,135] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:47,136] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:47,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-03-27 05:31:47,159] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:47,159] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:47,161] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-03-27 05:31:47,185] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:47,186] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:47,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-03-27 05:31:47,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:31:47,227] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:47,228] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-03-27 05:31:49,068] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 05:31:49,069] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:31:49,070] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:31:49,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:49,071] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:49,072] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:31:49,071] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:31:49,074] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:31:49,075] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:49,075] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:49,076] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:31:49,096] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run18
[2019-03-27 05:31:49,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run18
[2019-03-27 05:31:49,136] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run18
[2019-03-27 05:31:49,154] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run18
[2019-03-27 05:31:49,171] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run18
[2019-03-27 05:31:57,800] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037249386]
[2019-03-27 05:31:57,801] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.5, 76.0, 1.0, 2.0, 0.2152994609686631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 358897.6653727277, 358897.6653727283, 157115.7193498064]
[2019-03-27 05:31:57,802] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:31:57,804] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.1830144e-13 1.0000000e+00 6.7520625e-21 9.3694638e-14 1.5343431e-19], sampled 0.3841551051296297
[2019-03-27 05:32:02,715] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037249386]
[2019-03-27 05:32:02,716] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.96666666666667, 84.33333333333333, 1.0, 2.0, 0.3033964635681946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482440.7962521837, 482440.7962521837, 165785.8971832283]
[2019-03-27 05:32:02,716] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:32:02,720] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1260499e-14 1.0000000e+00 1.9919648e-22 1.2892335e-14 4.9778610e-21], sampled 0.5165603281187129
[2019-03-27 05:32:10,357] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037249386]
[2019-03-27 05:32:10,359] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.680579955, 88.27558598499999, 1.0, 2.0, 0.3776282670482094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579743.6775730613, 579743.6775730619, 173152.2153068141]
[2019-03-27 05:32:10,360] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:32:10,362] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.6073875e-14 1.0000000e+00 3.2803883e-22 1.3882149e-14 8.6851460e-21], sampled 0.24999523681859093
[2019-03-27 05:32:18,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037249386]
[2019-03-27 05:32:18,385] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.66666666666667, 89.33333333333334, 1.0, 2.0, 0.779953755934396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1090064.945183442, 1090064.945183443, 239046.6281250679]
[2019-03-27 05:32:18,386] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:32:18,391] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7073861e-14 1.0000000e+00 3.8453271e-23 6.3009121e-15 9.2056236e-22], sampled 0.7733593268542285
[2019-03-27 05:32:23,905] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037249386]
[2019-03-27 05:32:23,906] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.8, 83.5, 1.0, 2.0, 0.3773487480699789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 573616.1674544162, 573616.1674544155, 172463.1816279616]
[2019-03-27 05:32:23,907] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:32:23,910] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9000714e-14 1.0000000e+00 8.2018392e-23 9.3255465e-15 2.0140860e-21], sampled 0.7939279115408375
[2019-03-27 05:32:55,323] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037249386]
[2019-03-27 05:32:55,325] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.53333333333333, 71.33333333333334, 1.0, 2.0, 0.5735319379322263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801460.5378382369, 801460.5378382369, 195917.6304204279]
[2019-03-27 05:32:55,325] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:32:55,328] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.6750981e-14 1.0000000e+00 6.9214902e-23 1.1574527e-14 1.8710361e-21], sampled 0.20824422636068263
[2019-03-27 05:32:59,934] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037249386]
[2019-03-27 05:32:59,936] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.22373562, 78.54192441, 1.0, 2.0, 0.4194885404802042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 622258.8603758503, 622258.860375851, 176524.3808225587]
[2019-03-27 05:32:59,937] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:32:59,941] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1875915e-14 1.0000000e+00 2.6186268e-23 5.4738471e-15 6.5252004e-22], sampled 0.12902719215055125
[2019-03-27 05:33:04,069] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037249386]
[2019-03-27 05:33:04,070] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.85, 79.0, 1.0, 2.0, 0.5972527030267215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834621.2483028665, 834621.2483028665, 200234.2665210609]
[2019-03-27 05:33:04,071] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:33:04,074] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2307592e-14 1.0000000e+00 1.9452931e-23 4.5756821e-15 5.3863332e-22], sampled 0.5669730881092961
[2019-03-27 05:33:13,888] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037249386]
[2019-03-27 05:33:13,889] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.55, 72.5, 1.0, 2.0, 0.6112658661578394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 854211.5938470138, 854211.5938470138, 202863.1462390345]
[2019-03-27 05:33:13,889] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:33:13,895] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8478976e-14 1.0000000e+00 1.5090030e-23 4.3752496e-15 4.3169760e-22], sampled 0.6294103881727442
[2019-03-27 05:33:44,124] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7168 2779131445.1674 933.0000
[2019-03-27 05:33:44,334] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 05:33:44,384] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.9131 3164055569.9463 1778.0000
[2019-03-27 05:33:44,387] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 05:33:44,398] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 05:33:45,413] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 425000, evaluation results [425000.0, 7881.913059455902, 3164055569.946269, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8660.716805456479, 2779131445.1673803, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 05:33:51,420] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1669118e-15 1.0000000e+00 1.2623923e-24 7.1123315e-15 2.3566002e-22], sum to 1.0000
[2019-03-27 05:33:51,429] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9608
[2019-03-27 05:33:51,436] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.81666666666667, 94.83333333333333, 1.0, 2.0, 0.6812387293344793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1019081.761564686, 1019081.761564686, 225470.4249203383], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 125400.0000, 
sim time next is 126000.0000, 
raw observation next is [22.8, 95.0, 1.0, 2.0, 0.7321840579104695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1095165.905318714, 1095165.905318714, 237395.6173608549], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.95, 1.0, 1.0, 0.6773301902535777, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30421275147742055, 0.30421275147742055, 0.35432181695649984], 
reward next is 0.6457, 
noisyNet noise sample is [array([-1.8821476], dtype=float32), 1.0340438]. 
=============================================
[2019-03-27 05:33:51,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.07948 ]
 [75.90886 ]
 [75.75656 ]
 [75.593575]
 [75.42322 ]], R is [[76.0036087 ]
 [75.90705109]
 [75.80595398]
 [75.68280029]
 [75.56276703]].
[2019-03-27 05:33:52,478] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.8942176e-16 1.0000000e+00 2.0208565e-23 1.6981332e-15 2.9066732e-22], sum to 1.0000
[2019-03-27 05:33:52,485] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8308
[2019-03-27 05:33:52,490] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 95.83333333333333, 1.0, 2.0, 0.8993225992315843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1340447.958229606, 1340447.958229607, 281729.6384257205], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 129000.0000, 
sim time next is 129600.0000, 
raw observation next is [22.8, 96.0, 1.0, 2.0, 0.9100219829494143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1355436.174520329, 1355436.174520329, 284768.7342746886], 
processed observation next is [1.0, 0.5217391304347826, 0.2796208530805688, 0.96, 1.0, 1.0, 0.891592750541463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3765100484778692, 0.3765100484778692, 0.4250279616040128], 
reward next is 0.5750, 
noisyNet noise sample is [array([0.3420275], dtype=float32), 0.49191236]. 
=============================================
[2019-03-27 05:33:58,340] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6133778e-13 1.0000000e+00 2.0958836e-22 2.0923013e-14 1.4590848e-19], sum to 1.0000
[2019-03-27 05:33:58,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4433
[2019-03-27 05:33:58,355] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.96666666666667, 89.66666666666667, 1.0, 2.0, 0.2937403827338948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 470170.5035380226, 470170.5035380226, 164958.0196166112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 247200.0000, 
sim time next is 247800.0000, 
raw observation next is [20.93333333333333, 89.83333333333333, 1.0, 2.0, 0.2929882873432826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469089.0690366681, 469089.0690366681, 164883.7680691813], 
processed observation next is [0.0, 0.8695652173913043, 0.19115323854660338, 0.8983333333333333, 1.0, 1.0, 0.14817865944973804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13030251917685226, 0.13030251917685226, 0.24609517622265867], 
reward next is 0.7539, 
noisyNet noise sample is [array([1.0379096], dtype=float32), -1.1073123]. 
=============================================
[2019-03-27 05:34:00,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2301754e-12 1.0000000e+00 4.1510589e-21 7.3387626e-14 5.9347450e-20], sum to 1.0000
[2019-03-27 05:34:00,581] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6909
[2019-03-27 05:34:00,584] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 92.0, 1.0, 2.0, 0.2706050916224741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 438197.227455016, 438197.227455016, 162818.2025584676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 280800.0000, 
sim time next is 281400.0000, 
raw observation next is [20.25, 91.33333333333334, 1.0, 2.0, 0.2725676136235184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440822.8523189685, 440822.8523189685, 162992.3844759037], 
processed observation next is [0.0, 0.2608695652173913, 0.1587677725118484, 0.9133333333333334, 1.0, 1.0, 0.12357543810062456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12245079231082459, 0.12245079231082459, 0.24327221563567716], 
reward next is 0.7567, 
noisyNet noise sample is [array([0.29098955], dtype=float32), -0.21841908]. 
=============================================
[2019-03-27 05:34:01,154] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.3919279e-16 1.0000000e+00 1.3351704e-22 6.9070343e-16 1.1932479e-20], sum to 1.0000
[2019-03-27 05:34:01,164] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0666
[2019-03-27 05:34:01,169] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.71666666666667, 79.5, 1.0, 2.0, 0.3029723686281392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480799.7242377104, 480799.7242377104, 165650.3145247471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 294600.0000, 
sim time next is 295200.0000, 
raw observation next is [22.8, 79.0, 1.0, 2.0, 0.3031729866589544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480976.1166820957, 480976.1166820964, 165660.2373612459], 
processed observation next is [0.0, 0.43478260869565216, 0.2796208530805688, 0.79, 1.0, 1.0, 0.16044938151681254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1336044768561377, 0.13360447685613788, 0.24725408561379986], 
reward next is 0.7527, 
noisyNet noise sample is [array([-2.6685808], dtype=float32), -2.2205675]. 
=============================================
[2019-03-27 05:34:35,785] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7787516e-14 1.0000000e+00 4.1614261e-22 1.9055927e-14 5.2924615e-21], sum to 1.0000
[2019-03-27 05:34:35,793] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3696
[2019-03-27 05:34:35,804] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2938548681027904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469072.5909474737, 469072.5909474737, 164865.7112779409], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 898800.0000, 
sim time next is 899400.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2940161646984426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469329.9655398098, 469329.9655398098, 164883.7091789673], 
processed observation next is [0.0, 0.391304347826087, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14941706590173806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13036943487216937, 0.13036943487216937, 0.2460950883268169], 
reward next is 0.7539, 
noisyNet noise sample is [array([0.56731546], dtype=float32), -1.4012032]. 
=============================================
[2019-03-27 05:34:39,341] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 05:34:39,345] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:34:39,346] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:34:39,346] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:34:39,346] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:34:39,347] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:34:39,347] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:34:39,348] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:34:39,349] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:34:39,351] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:34:39,354] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:34:39,366] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run19
[2019-03-27 05:34:39,386] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run19
[2019-03-27 05:34:39,386] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run19
[2019-03-27 05:34:39,387] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run19
[2019-03-27 05:34:39,455] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run19
[2019-03-27 05:34:44,335] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037997164]
[2019-03-27 05:34:44,337] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.7, 80.0, 1.0, 2.0, 0.260472440547457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427395.153746268, 427395.1537462687, 161960.8703752357]
[2019-03-27 05:34:44,339] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:34:44,343] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2383721e-13 1.0000000e+00 7.6478462e-22 2.0810339e-14 2.0158627e-20], sampled 0.5812269859362773
[2019-03-27 05:34:51,570] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037997164]
[2019-03-27 05:34:51,571] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.06405606333334, 86.14180276333335, 1.0, 2.0, 0.243111112659327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403025.4980777298, 403025.4980777298, 160059.9971032358]
[2019-03-27 05:34:51,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:34:51,575] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.5034163e-14 1.0000000e+00 3.1558743e-22 1.4941172e-14 8.3539846e-21], sampled 0.07132234258317571
[2019-03-27 05:35:00,466] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037997164]
[2019-03-27 05:35:00,466] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [18.2, 88.0, 1.0, 2.0, 0.225837679223153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 376050.5513806483, 376050.5513806483, 158160.911277381]
[2019-03-27 05:35:00,468] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:35:00,470] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4953438e-13 1.0000000e+00 1.0016501e-21 2.7838656e-14 2.5013472e-20], sampled 0.5661762688772395
[2019-03-27 05:35:10,602] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037997164]
[2019-03-27 05:35:10,603] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.86666666666667, 92.66666666666666, 1.0, 2.0, 0.4836854115220442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675867.9611099224, 675867.961109923, 181078.8060475196]
[2019-03-27 05:35:10,604] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:35:10,605] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7785767e-14 1.0000000e+00 6.1094986e-23 4.1321918e-15 1.5847024e-21], sampled 0.31106547046137933
[2019-03-27 05:36:15,255] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037997164]
[2019-03-27 05:36:15,255] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.85, 73.0, 1.0, 2.0, 0.5551985343019431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775831.8592169048, 775831.8592169042, 192695.8013238962]
[2019-03-27 05:36:15,256] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:36:15,259] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1184020e-14 1.0000000e+00 8.6509753e-24 2.5126479e-15 2.0607311e-22], sampled 0.7083401302666594
[2019-03-27 05:36:15,603] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037997164]
[2019-03-27 05:36:15,603] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.36666666666667, 87.0, 1.0, 2.0, 0.6433942664929133, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988022703474197, 6.9112, 168.9124362671253, 1795920.295146946, 1741419.793128241, 373529.3476373397]
[2019-03-27 05:36:15,604] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:36:15,606] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4199563e-11 1.0000000e+00 6.9578778e-19 6.6742948e-12 1.3489813e-17], sampled 0.3927941787965564
[2019-03-27 05:36:15,606] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1795920.295146946 W.
[2019-03-27 05:36:21,969] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.037997164]
[2019-03-27 05:36:21,970] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.30477893, 55.98821942333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.835768364744527, 6.9112, 168.9079008117323, 2110105.228352997, 1454204.241440074, 311349.3373043552]
[2019-03-27 05:36:21,971] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:36:21,972] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7816764e-12 1.0000000e+00 3.3525265e-20 1.5301898e-12 6.3247884e-19], sampled 0.2864979348564013
[2019-03-27 05:36:21,976] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2110105.228352997 W.
[2019-03-27 05:36:34,172] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.8169 3007629189.2715 1766.0000
[2019-03-27 05:36:34,363] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 05:36:34,377] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 05:36:34,387] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1488 2779272531.6375 933.0000
[2019-03-27 05:36:34,426] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5723 2927391596.7895 1338.0000
[2019-03-27 05:36:35,441] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 450000, evaluation results [450000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8253.572272036794, 2927391596.789537, 1338.0, 8659.14877505413, 2779272531.637507, 933.0, 7996.81694110405, 3007629189.271529, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 05:36:36,791] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7210014e-14 1.0000000e+00 1.4714634e-22 5.5130738e-16 2.5639673e-21], sum to 1.0000
[2019-03-27 05:36:36,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8965
[2019-03-27 05:36:36,802] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.3350384879186246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519354.9549688383, 519354.9549688377, 168252.030569168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 978000.0000, 
sim time next is 978600.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3329765268366439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516161.5966468404, 516161.5966468398, 168000.7902355249], 
processed observation next is [1.0, 0.30434782608695654, 0.23696682464454974, 0.93, 1.0, 1.0, 0.19635726124896857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.143378221290789, 0.14337822129078884, 0.25074744811272376], 
reward next is 0.7493, 
noisyNet noise sample is [array([0.9675953], dtype=float32), 0.42018467]. 
=============================================
[2019-03-27 05:36:41,090] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.2842023e-13 1.0000000e+00 1.8564623e-20 5.2180956e-14 5.4094833e-20], sum to 1.0000
[2019-03-27 05:36:41,096] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1916
[2019-03-27 05:36:41,101] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.25, 95.16666666666667, 1.0, 2.0, 0.2912758945374515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466920.9517367519, 466920.9517367513, 164738.2797270654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1057800.0000, 
sim time next is 1058400.0000, 
raw observation next is [20.3, 95.0, 1.0, 2.0, 0.2932622200729397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469811.226344271, 469811.226344271, 164936.77485295], 
processed observation next is [1.0, 0.2608695652173913, 0.16113744075829392, 0.95, 1.0, 1.0, 0.1485086988830599, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13050311842896417, 0.13050311842896417, 0.24617429082529851], 
reward next is 0.7538, 
noisyNet noise sample is [array([-1.4632174], dtype=float32), 0.8192569]. 
=============================================
[2019-03-27 05:36:41,445] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9606500e-12 1.0000000e+00 9.7843516e-23 5.4915335e-14 8.2594029e-20], sum to 1.0000
[2019-03-27 05:36:41,457] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5350
[2019-03-27 05:36:41,461] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3304369664384131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522451.8271187349, 522451.8271187343, 168738.7956749833], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1066200.0000, 
sim time next is 1066800.0000, 
raw observation next is [21.1, 93.0, 1.0, 2.0, 0.4328314631610836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684621.5134002778, 684621.5134002785, 182991.014045255], 
processed observation next is [1.0, 0.34782608695652173, 0.1990521327014219, 0.93, 1.0, 1.0, 0.31666441344708873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1901726426111883, 0.1901726426111885, 0.2731209164854552], 
reward next is 0.7269, 
noisyNet noise sample is [array([0.989713], dtype=float32), 1.3911284]. 
=============================================
[2019-03-27 05:36:45,631] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4595638e-14 1.0000000e+00 2.1359427e-21 4.9058664e-14 1.6298633e-20], sum to 1.0000
[2019-03-27 05:36:45,640] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6451
[2019-03-27 05:36:45,643] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 96.0, 1.0, 2.0, 0.2735505203460503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 443018.5352876315, 443018.5352876315, 163131.2002154956], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1141200.0000, 
sim time next is 1141800.0000, 
raw observation next is [19.75, 95.16666666666667, 1.0, 2.0, 0.2720137407407139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440269.0698627054, 440269.0698627054, 162954.1796970875], 
processed observation next is [1.0, 0.21739130434782608, 0.13507109004739343, 0.9516666666666667, 1.0, 1.0, 0.12290812137435407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1222969638507515, 0.1222969638507515, 0.24321519357774254], 
reward next is 0.7568, 
noisyNet noise sample is [array([-0.838213], dtype=float32), -1.1613685]. 
=============================================
[2019-03-27 05:36:49,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4793118e-14 1.0000000e+00 3.4644239e-22 2.5189434e-15 2.4711294e-21], sum to 1.0000
[2019-03-27 05:36:49,059] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0465
[2019-03-27 05:36:49,065] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 72.33333333333334, 1.0, 2.0, 0.4279153722120074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 618066.4080205425, 618066.4080205419, 175668.1792200477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1432200.0000, 
sim time next is 1432800.0000, 
raw observation next is [27.2, 71.0, 1.0, 2.0, 0.4270633068830873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617325.7241782949, 617325.7241782949, 175610.5870608765], 
processed observation next is [0.0, 0.6086956521739131, 0.4881516587677725, 0.71, 1.0, 1.0, 0.30971482756998475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17147936782730414, 0.17147936782730414, 0.26210535382220373], 
reward next is 0.7379, 
noisyNet noise sample is [array([-0.2104209], dtype=float32), -0.4946884]. 
=============================================
[2019-03-27 05:36:51,489] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7462363e-14 1.0000000e+00 5.4959013e-22 4.4899201e-15 4.6316007e-21], sum to 1.0000
[2019-03-27 05:36:51,502] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7546
[2019-03-27 05:36:51,506] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 76.66666666666667, 1.0, 2.0, 0.9627624593586369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1386511.577371322, 1386511.577371322, 293996.219844892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1244400.0000, 
sim time next is 1245000.0000, 
raw observation next is [26.6, 75.83333333333333, 1.0, 2.0, 0.9844087125408639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1413843.379648927, 1413843.379648927, 300052.910774734], 
processed observation next is [1.0, 0.391304347826087, 0.4597156398104266, 0.7583333333333333, 1.0, 1.0, 0.9812153163142939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3927342721247019, 0.3927342721247019, 0.44784016533542387], 
reward next is 0.5522, 
noisyNet noise sample is [array([-0.6232112], dtype=float32), 0.62386]. 
=============================================
[2019-03-27 05:36:51,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.58445 ]
 [73.51841 ]
 [73.570984]
 [73.57219 ]
 [73.525   ]], R is [[73.49977875]
 [73.32598114]
 [73.16516876]
 [73.00556183]
 [72.86617279]].
[2019-03-27 05:36:58,285] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2668937e-12 1.0000000e+00 1.4758370e-22 2.0271865e-13 1.4553937e-19], sum to 1.0000
[2019-03-27 05:36:58,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0627
[2019-03-27 05:36:58,305] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 93.83333333333334, 1.0, 2.0, 0.3206230442502246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506670.1759969085, 506670.1759969085, 167521.2245619837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1363800.0000, 
sim time next is 1364400.0000, 
raw observation next is [21.1, 94.0, 1.0, 2.0, 0.3215548565950076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507499.2158023393, 507499.2158023393, 167571.6449226296], 
processed observation next is [1.0, 0.8260869565217391, 0.1990521327014219, 0.94, 1.0, 1.0, 0.18259621276506938, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1409720043895387, 0.1409720043895387, 0.2501069327203427], 
reward next is 0.7499, 
noisyNet noise sample is [array([-0.38656828], dtype=float32), 0.7059264]. 
=============================================
[2019-03-27 05:37:08,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1561559e-13 1.0000000e+00 3.8246453e-21 1.6551363e-15 8.8797244e-20], sum to 1.0000
[2019-03-27 05:37:08,798] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1844
[2019-03-27 05:37:08,806] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333334, 85.5, 1.0, 2.0, 0.3573930548972085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548950.372092193, 548950.3720921937, 170512.9621013683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [23.1, 86.0, 1.0, 2.0, 0.3561579235170261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547306.5807380031, 547306.5807380031, 170382.9207291479], 
processed observation next is [0.0, 0.9130434782608695, 0.2938388625592418, 0.86, 1.0, 1.0, 0.22428665483979046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15202960576055644, 0.15202960576055644, 0.25430286675992225], 
reward next is 0.7457, 
noisyNet noise sample is [array([-1.0415515], dtype=float32), 1.494111]. 
=============================================
[2019-03-27 05:37:13,382] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.2142356e-14 1.0000000e+00 6.0111817e-22 3.5409982e-14 3.7630381e-21], sum to 1.0000
[2019-03-27 05:37:13,391] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9085
[2019-03-27 05:37:13,400] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333333, 96.0, 1.0, 2.0, 0.39572206576622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584570.6059960261, 584570.6059960267, 172933.9716455378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1617600.0000, 
sim time next is 1618200.0000, 
raw observation next is [23.05, 96.0, 1.0, 2.0, 0.3934647885725295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580937.2404520125, 580937.2404520125, 172591.8668708229], 
processed observation next is [1.0, 0.7391304347826086, 0.2914691943127963, 0.96, 1.0, 1.0, 0.2692346850271439, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16137145568111458, 0.16137145568111458, 0.2575998012997357], 
reward next is 0.7424, 
noisyNet noise sample is [array([-1.2885529], dtype=float32), 0.2565913]. 
=============================================
[2019-03-27 05:37:16,237] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.6264364e-14 1.0000000e+00 1.2664694e-22 6.1386348e-14 6.5832336e-21], sum to 1.0000
[2019-03-27 05:37:16,248] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1048
[2019-03-27 05:37:16,252] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.51666666666667, 98.83333333333333, 1.0, 2.0, 0.5287613154827693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755556.0521053362, 755556.0521053356, 190353.0675579442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1663800.0000, 
sim time next is 1664400.0000, 
raw observation next is [23.53333333333333, 98.66666666666669, 1.0, 2.0, 0.4825703655118966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689628.8592915381, 689628.8592915375, 182868.4920696368], 
processed observation next is [1.0, 0.2608695652173913, 0.3143759873617693, 0.9866666666666668, 1.0, 1.0, 0.3765908018215621, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19156357202542726, 0.1915635720254271, 0.27293804786512954], 
reward next is 0.7271, 
noisyNet noise sample is [array([0.8653444], dtype=float32), -0.97238743]. 
=============================================
[2019-03-27 05:37:29,193] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 05:37:29,196] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:37:29,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:37:29,200] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:37:29,201] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:37:29,203] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:37:29,204] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:37:29,206] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:37:29,206] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:37:29,207] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:37:29,209] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:37:29,228] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run20
[2019-03-27 05:37:29,229] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run20
[2019-03-27 05:37:29,229] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run20
[2019-03-27 05:37:29,230] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run20
[2019-03-27 05:37:29,305] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run20
[2019-03-27 05:37:42,493] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.0372958]
[2019-03-27 05:37:42,495] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.16970908, 78.596135995, 1.0, 2.0, 0.403661985446812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600267.7640519238, 600267.7640519238, 174502.1938084152]
[2019-03-27 05:37:42,496] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:37:42,500] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0174628e-13 1.0000000e+00 4.8473970e-22 1.7725660e-14 1.4684705e-20], sampled 0.2964576238704747
[2019-03-27 05:37:49,772] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.0372958]
[2019-03-27 05:37:49,774] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.26666666666667, 69.66666666666667, 1.0, 2.0, 0.246648807297754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 408555.1465528188, 408555.1465528188, 160435.9209213874]
[2019-03-27 05:37:49,775] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:37:49,778] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4959087e-13 1.0000000e+00 9.0439780e-22 3.1551347e-14 2.6097553e-20], sampled 0.6754887498349338
[2019-03-27 05:38:07,394] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.0372958]
[2019-03-27 05:38:07,396] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.710652765, 69.165523345, 1.0, 2.0, 0.7600208780139295, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005979031433299, 6.9112, 168.9123160215585, 1959122.573429755, 1891883.321449105, 398197.5831729218]
[2019-03-27 05:38:07,398] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:38:07,401] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7314106e-12 1.0000000e+00 2.3275486e-20 9.8575683e-13 6.7206160e-19], sampled 0.9798634039304895
[2019-03-27 05:38:07,403] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1959122.573429755 W.
[2019-03-27 05:39:04,628] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00044564], dtype=float32), 0.0372958]
[2019-03-27 05:39:04,629] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.33333333333334, 87.0, 1.0, 2.0, 0.6198944744218886, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.945454579320197, 6.9112, 168.9126644141301, 1733256.199818974, 1708954.864321264, 368992.3905252349]
[2019-03-27 05:39:04,632] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:39:04,633] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3955130e-10 1.0000000e+00 2.2344008e-17 7.4087438e-11 4.3950030e-16], sampled 0.11533071511398574
[2019-03-27 05:39:04,635] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1733256.199818974 W.
[2019-03-27 05:39:23,491] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 05:39:23,813] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2178 3007719775.7786 1766.0000
[2019-03-27 05:39:23,845] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8506 2842512529.7895 1131.0000
[2019-03-27 05:39:23,937] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9883 2779274382.2099 933.0000
[2019-03-27 05:39:24,125] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6701 3164128525.6280 1778.0000
[2019-03-27 05:39:25,140] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 475000, evaluation results [475000.0, 7882.6701280147, 3164128525.6279716, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8659.988291364358, 2779274382.209945, 933.0, 7998.217808978373, 3007719775.7785764, 1766.0, 8496.850559356773, 2842512529.789502, 1131.0]
[2019-03-27 05:39:27,041] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3089881e-14 1.0000000e+00 5.5276237e-24 3.4272105e-15 1.9955547e-22], sum to 1.0000
[2019-03-27 05:39:27,053] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1604
[2019-03-27 05:39:27,058] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 82.66666666666667, 1.0, 2.0, 0.8095534257718462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1160609.468294198, 1160609.468294198, 250365.9228827418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1930800.0000, 
sim time next is 1931400.0000, 
raw observation next is [25.65, 82.5, 1.0, 2.0, 0.9261565077601371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1326232.476951096, 1326232.476951096, 281979.1999425621], 
processed observation next is [1.0, 0.34782608695652173, 0.41469194312796204, 0.825, 1.0, 1.0, 0.9110319370604062, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36839791026419333, 0.36839791026419333, 0.4208644775262121], 
reward next is 0.5791, 
noisyNet noise sample is [array([-2.3117225], dtype=float32), 0.6220668]. 
=============================================
[2019-03-27 05:39:32,476] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7765711e-13 1.0000000e+00 1.2909705e-22 1.7696643e-14 6.0672147e-20], sum to 1.0000
[2019-03-27 05:39:32,485] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9840
[2019-03-27 05:39:32,491] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 96.0, 1.0, 2.0, 0.4763922924712487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665673.8767111677, 665673.8767111677, 179979.8213440143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2008800.0000, 
sim time next is 2009400.0000, 
raw observation next is [24.58333333333334, 95.83333333333333, 1.0, 2.0, 0.478267751760229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 668295.3233105448, 668295.3233105455, 180261.1107287601], 
processed observation next is [0.0, 0.2608695652173913, 0.3641390205371251, 0.9583333333333333, 1.0, 1.0, 0.37140692983160123, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1856375898084847, 0.18563758980848488, 0.26904643392352257], 
reward next is 0.7310, 
noisyNet noise sample is [array([-0.28390586], dtype=float32), 0.009585316]. 
=============================================
[2019-03-27 05:39:34,306] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5609950e-14 1.0000000e+00 1.5282540e-22 1.2263385e-15 1.1705602e-22], sum to 1.0000
[2019-03-27 05:39:34,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1463
[2019-03-27 05:39:34,318] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 91.66666666666666, 1.0, 2.0, 0.4765954530357104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665957.8466101834, 665957.8466101841, 180009.9954166866], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2065200.0000, 
sim time next is 2065800.0000, 
raw observation next is [24.93333333333333, 91.83333333333333, 1.0, 2.0, 0.476418763859153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665710.8773785756, 665710.8773785762, 179983.5260085615], 
processed observation next is [0.0, 0.9130434782608695, 0.38072669826224315, 0.9183333333333333, 1.0, 1.0, 0.3691792335652446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18491968816071544, 0.1849196881607156, 0.26863212837098727], 
reward next is 0.7314, 
noisyNet noise sample is [array([0.92581564], dtype=float32), 0.041685104]. 
=============================================
[2019-03-27 05:39:36,389] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5802206e-13 1.0000000e+00 5.9784337e-22 3.4956728e-15 3.5741121e-21], sum to 1.0000
[2019-03-27 05:39:36,399] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6794
[2019-03-27 05:39:36,405] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 89.0, 1.0, 2.0, 0.5041720142207456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704503.9908351148, 704503.9908351141, 184252.7491142351], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2102400.0000, 
sim time next is 2103000.0000, 
raw observation next is [26.5, 88.16666666666667, 1.0, 2.0, 0.5059188884544717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706945.7951191511, 706945.7951191511, 184529.1445501473], 
processed observation next is [0.0, 0.34782608695652173, 0.4549763033175356, 0.8816666666666667, 1.0, 1.0, 0.4047215523547852, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.196373831977542, 0.196373831977542, 0.27541663365693625], 
reward next is 0.7246, 
noisyNet noise sample is [array([-0.05062174], dtype=float32), -0.262339]. 
=============================================
[2019-03-27 05:39:36,431] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.67838 ]
 [75.65566 ]
 [75.66076 ]
 [75.659195]
 [75.64414 ]], R is [[75.65576172]
 [75.62420654]
 [75.59323883]
 [75.56302643]
 [75.53387451]].
[2019-03-27 05:39:42,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4336429e-10 1.0000000e+00 8.8473749e-18 1.2661873e-10 4.2888365e-17], sum to 1.0000
[2019-03-27 05:39:42,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4417
[2019-03-27 05:39:42,151] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1769071.398642543 W.
[2019-03-27 05:39:42,160] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.01666666666667, 78.50000000000001, 1.0, 2.0, 0.632698098389285, 1.0, 2.0, 0.632698098389285, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1769071.398642543, 1769071.398642543, 346645.2321555794], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2193000.0000, 
sim time next is 2193600.0000, 
raw observation next is [29.13333333333334, 78.0, 1.0, 2.0, 0.796294270634116, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.995741311354831, 6.9112, 168.9124541256041, 2009887.119787435, 1949910.782208482, 407313.9643402617], 
processed observation next is [1.0, 0.391304347826087, 0.5797788309636654, 0.78, 1.0, 1.0, 0.7545714104025494, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008454131135483055, 0.0, 0.8294374782128099, 0.5583019777187319, 0.5416418839468006, 0.607931290060092], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3775806], dtype=float32), -0.6886403]. 
=============================================
[2019-03-27 05:39:44,216] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.5939933e-14 1.0000000e+00 2.6994254e-21 3.0830294e-13 7.2714257e-21], sum to 1.0000
[2019-03-27 05:39:44,223] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3675
[2019-03-27 05:39:44,228] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 84.33333333333334, 1.0, 2.0, 0.533385824450514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745340.1854358703, 745340.1854358697, 188992.3759554533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2240400.0000, 
sim time next is 2241000.0000, 
raw observation next is [27.55, 84.5, 1.0, 2.0, 0.531361632074012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742510.6403696458, 742510.6403696451, 188655.7856488573], 
processed observation next is [1.0, 0.9565217391304348, 0.504739336492891, 0.845, 1.0, 1.0, 0.4353754603301349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20625295565823493, 0.20625295565823476, 0.28157579947590644], 
reward next is 0.7184, 
noisyNet noise sample is [array([2.1289485], dtype=float32), -0.36021712]. 
=============================================
[2019-03-27 05:39:44,239] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[67.05238 ]
 [67.015854]
 [66.74507 ]
 [67.356026]
 [66.94859 ]], R is [[67.13916779]
 [67.18569183]
 [67.23104858]
 [67.27542114]
 [67.31917572]].
[2019-03-27 05:39:49,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0907130e-12 1.0000000e+00 2.5524921e-20 9.1223437e-13 1.9198486e-19], sum to 1.0000
[2019-03-27 05:39:49,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4481
[2019-03-27 05:39:49,384] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.03333333333333, 77.33333333333334, 1.0, 2.0, 0.5760035941345235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 804915.7691199516, 804915.7691199511, 196360.841515998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2319600.0000, 
sim time next is 2320200.0000, 
raw observation next is [29.95, 77.5, 1.0, 2.0, 0.5752929894162296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803922.383998383, 803922.383998383, 196233.4834917669], 
processed observation next is [1.0, 0.8695652173913043, 0.6184834123222749, 0.775, 1.0, 1.0, 0.48830480652557784, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22331177333288416, 0.22331177333288416, 0.2928857962563685], 
reward next is 0.7071, 
noisyNet noise sample is [array([-0.14886145], dtype=float32), -0.7188437]. 
=============================================
[2019-03-27 05:39:50,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9560295e-13 1.0000000e+00 2.7310683e-21 3.0899290e-13 1.2429898e-19], sum to 1.0000
[2019-03-27 05:39:50,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5159
[2019-03-27 05:39:50,374] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 82.0, 1.0, 2.0, 0.6725323260219531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 939866.1013753957, 939866.1013753957, 215026.5012050357], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2346600.0000, 
sim time next is 2347200.0000, 
raw observation next is [27.3, 82.0, 1.0, 2.0, 0.670485267407717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 937004.0683421745, 937004.0683421738, 214601.8070888437], 
processed observation next is [1.0, 0.17391304347826086, 0.4928909952606636, 0.82, 1.0, 1.0, 0.6029942980815867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26027890787282626, 0.26027890787282604, 0.32030120461021444], 
reward next is 0.6797, 
noisyNet noise sample is [array([1.1515685], dtype=float32), 0.118901126]. 
=============================================
[2019-03-27 05:39:51,027] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4731410e-13 1.0000000e+00 1.0403480e-21 8.7815233e-14 5.1137285e-20], sum to 1.0000
[2019-03-27 05:39:51,036] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8850
[2019-03-27 05:39:51,040] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.05, 81.0, 1.0, 2.0, 0.5323853369376412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 743941.6393067535, 743941.6393067541, 188825.6811382481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2337000.0000, 
sim time next is 2337600.0000, 
raw observation next is [28.0, 81.0, 1.0, 2.0, 0.5300839181191508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740724.5742801601, 740724.5742801601, 188443.8136541813], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.81, 1.0, 1.0, 0.4338360459266877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20575682618893337, 0.20575682618893337, 0.2812594233644497], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.48026398], dtype=float32), 0.030851742]. 
=============================================
[2019-03-27 05:39:52,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1831330e-05 9.9995565e-01 2.6425007e-10 3.2488984e-05 5.1554943e-09], sum to 1.0000
[2019-03-27 05:39:52,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1633
[2019-03-27 05:39:52,529] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2140205.748596165 W.
[2019-03-27 05:39:52,537] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.43333333333334, 63.83333333333334, 1.0, 2.0, 0.8894044343546219, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005987069370525, 6.9112, 168.9123931659733, 2140205.748596165, 2072960.763536194, 431118.8752005637], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2376600.0000, 
sim time next is 2377200.0000, 
raw observation next is [32.46666666666667, 63.66666666666667, 1.0, 2.0, 0.6486657320360525, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005972078389947, 6.9112, 168.9123160752159, 1803296.417366243, 1736062.098073791, 373907.2740788051], 
processed observation next is [1.0, 0.5217391304347826, 0.7377567140600317, 0.6366666666666667, 1.0, 1.0, 0.576705701248256, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009477207838994684, 0.0, 0.829436800322204, 0.500915671490623, 0.4822394716871642, 0.5580705583265748], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.53984386], dtype=float32), -1.5599923]. 
=============================================
[2019-03-27 05:39:53,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.54642248e-13 1.00000000e+00 7.43333498e-21 2.05789010e-12
 1.18070565e-20], sum to 1.0000
[2019-03-27 05:39:53,594] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1970
[2019-03-27 05:39:53,600] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.81666666666667, 74.33333333333334, 1.0, 2.0, 0.5856821183125834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818445.8898779831, 818445.8898779831, 198109.0939281125], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2405400.0000, 
sim time next is 2406000.0000, 
raw observation next is [30.73333333333333, 74.66666666666667, 1.0, 2.0, 0.5842371234322484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 816425.8441769247, 816425.8441769254, 197846.3589911139], 
processed observation next is [1.0, 0.8695652173913043, 0.6556082148499209, 0.7466666666666667, 1.0, 1.0, 0.49908087160511855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22678495671581242, 0.2267849567158126, 0.2952930731210655], 
reward next is 0.7047, 
noisyNet noise sample is [array([-0.98943806], dtype=float32), 0.3911369]. 
=============================================
[2019-03-27 05:39:53,613] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[66.26351 ]
 [66.42369 ]
 [65.21949 ]
 [64.28764 ]
 [62.658077]], R is [[66.94329071]
 [66.9781723 ]
 [67.01277924]
 [67.04769135]
 [67.08326721]].
[2019-03-27 05:40:04,846] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6438538e-13 1.0000000e+00 3.2655307e-22 7.6210300e-13 4.1590021e-20], sum to 1.0000
[2019-03-27 05:40:04,857] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4421
[2019-03-27 05:40:04,861] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 92.66666666666667, 1.0, 2.0, 0.4683071221736013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660557.0414491901, 660557.0414491895, 179576.8516253193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2596800.0000, 
sim time next is 2597400.0000, 
raw observation next is [24.5, 92.5, 1.0, 2.0, 0.46358690747478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 656448.3001028345, 656448.3001028338, 179205.0183926996], 
processed observation next is [0.0, 0.043478260869565216, 0.3601895734597157, 0.925, 1.0, 1.0, 0.3537191656322651, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18234675002856512, 0.18234675002856493, 0.2674701767055218], 
reward next is 0.7325, 
noisyNet noise sample is [array([0.3629529], dtype=float32), 0.2646346]. 
=============================================
[2019-03-27 05:40:09,203] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.7578425e-13 1.0000000e+00 3.8169789e-21 9.0243534e-14 1.0310803e-19], sum to 1.0000
[2019-03-27 05:40:09,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0950
[2019-03-27 05:40:09,216] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 84.0, 1.0, 2.0, 0.4886596575628824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682820.8564156874, 682820.856415688, 181838.4814686216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2647800.0000, 
sim time next is 2648400.0000, 
raw observation next is [26.33333333333334, 85.66666666666666, 1.0, 2.0, 0.490508351975817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685404.9337410438, 685404.9337410444, 182122.4945417789], 
processed observation next is [0.0, 0.6521739130434783, 0.44707740916271754, 0.8566666666666666, 1.0, 1.0, 0.38615464093471924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19039025937251217, 0.19039025937251233, 0.271824618719073], 
reward next is 0.7282, 
noisyNet noise sample is [array([0.10575841], dtype=float32), -0.97252744]. 
=============================================
[2019-03-27 05:40:09,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.01321019e-14 1.00000000e+00 1.23287332e-21 1.07793955e-13
 1.69208939e-20], sum to 1.0000
[2019-03-27 05:40:09,287] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3080
[2019-03-27 05:40:09,291] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333333, 92.33333333333334, 1.0, 2.0, 0.404807159499356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 601011.171667493, 601011.1716674924, 174543.0719971536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2662800.0000, 
sim time next is 2663400.0000, 
raw observation next is [23.16666666666667, 93.16666666666667, 1.0, 2.0, 0.4018787071855204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597938.4926027788, 597938.4926027788, 174297.4293140701], 
processed observation next is [0.0, 0.8260869565217391, 0.2969984202211693, 0.9316666666666668, 1.0, 1.0, 0.2793719363680968, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16609402572299412, 0.16609402572299412, 0.26014541688667175], 
reward next is 0.7399, 
noisyNet noise sample is [array([0.07133757], dtype=float32), 0.16019508]. 
=============================================
[2019-03-27 05:40:12,758] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.502211e-14 1.000000e+00 5.410719e-21 4.539762e-15 1.379547e-21], sum to 1.0000
[2019-03-27 05:40:12,771] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4239
[2019-03-27 05:40:12,777] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.393498388311652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587159.3775001465, 587159.3775001465, 173357.6295001984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2734200.0000, 
sim time next is 2734800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3939020088306986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587761.3349344825, 587761.3349344818, 173412.6244485988], 
processed observation next is [0.0, 0.6521739130434783, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2697614564225284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1632670374818007, 0.1632670374818005, 0.25882481260984896], 
reward next is 0.7412, 
noisyNet noise sample is [array([-1.0931138], dtype=float32), -0.7322358]. 
=============================================
[2019-03-27 05:40:13,052] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8327138e-13 1.0000000e+00 5.7138743e-22 1.9384869e-14 2.0422566e-20], sum to 1.0000
[2019-03-27 05:40:13,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7939
[2019-03-27 05:40:13,068] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3926703583748353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585939.7882578761, 585939.7882578761, 173246.8439187614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2733000.0000, 
sim time next is 2733600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3930336420206463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586467.0610255925, 586467.0610255925, 173294.4695076119], 
processed observation next is [0.0, 0.6521739130434783, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2687152313501762, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16290751695155348, 0.16290751695155348, 0.2586484619516596], 
reward next is 0.7414, 
noisyNet noise sample is [array([1.5132569], dtype=float32), -0.19959873]. 
=============================================
[2019-03-27 05:40:18,941] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 05:40:18,943] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:40:18,944] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:40:18,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:40:18,945] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:40:18,946] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:40:18,947] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:40:18,948] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:40:18,949] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:40:18,953] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:40:18,954] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:40:18,956] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run21
[2019-03-27 05:40:18,975] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run21
[2019-03-27 05:40:18,994] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run21
[2019-03-27 05:40:18,996] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run21
[2019-03-27 05:40:19,031] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run21
[2019-03-27 05:40:21,707] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00107419], dtype=float32), 0.0369969]
[2019-03-27 05:40:21,708] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.17844772, 86.90876574, 1.0, 2.0, 0.4957322082689709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785018.0020553758, 785018.0020553758, 193469.3591655472]
[2019-03-27 05:40:21,710] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:40:21,713] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.2343599e-13 1.0000000e+00 5.2033578e-21 5.7715883e-14 5.9589044e-20], sampled 0.3109234539603012
[2019-03-27 05:40:42,609] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00107419], dtype=float32), 0.0369969]
[2019-03-27 05:40:42,611] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.64736871333333, 67.27165645, 1.0, 2.0, 0.3896168327709182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586000.1437982974, 586000.1437982974, 173391.2940416819]
[2019-03-27 05:40:42,612] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:40:42,617] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4479997e-13 1.0000000e+00 1.0969586e-21 3.0441297e-14 1.3777596e-20], sampled 0.9934948834139232
[2019-03-27 05:40:50,305] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00107419], dtype=float32), 0.0369969]
[2019-03-27 05:40:50,307] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.21666666666667, 90.83333333333334, 1.0, 2.0, 0.4914706646750814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686750.0424878362, 686750.0424878356, 182269.7393594605]
[2019-03-27 05:40:50,310] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:40:50,312] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2401748e-14 1.0000000e+00 1.1829767e-22 7.0486698e-15 1.3307219e-21], sampled 0.923630399278791
[2019-03-27 05:40:57,539] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00107419], dtype=float32), 0.0369969]
[2019-03-27 05:40:57,542] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.27251383, 79.33985786, 1.0, 2.0, 0.9987643541710115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1396076.136475106, 1396076.136475106, 298551.8255594524]
[2019-03-27 05:40:57,544] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:40:57,548] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.5321331e-14 1.0000000e+00 3.4196502e-22 2.1244559e-14 4.5040929e-21], sampled 0.3879028718156168
[2019-03-27 05:41:09,828] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00107419], dtype=float32), 0.0369969]
[2019-03-27 05:41:09,829] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.08333333333334, 84.0, 1.0, 2.0, 0.5231707699830984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731061.0084520311, 731061.0084520316, 187305.4002619424]
[2019-03-27 05:41:09,830] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:41:09,832] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.7368169e-14 1.0000000e+00 2.7320151e-22 2.1214150e-14 2.6016133e-21], sampled 0.26099396495548866
[2019-03-27 05:41:13,041] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00107419], dtype=float32), 0.0369969]
[2019-03-27 05:41:13,042] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.758548635, 75.27035244000001, 1.0, 2.0, 0.7528836062414976, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997068285461264, 6.9112, 168.9123774747808, 1949134.259705081, 1888216.551467911, 396877.062107781]
[2019-03-27 05:41:13,044] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:41:13,047] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.28082567e-08 1.00000000e+00 2.13587286e-14 1.33938345e-08
 2.28881123e-13], sampled 0.9839739065192654
[2019-03-27 05:41:13,049] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1949134.259705081 W.
[2019-03-27 05:41:22,987] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00107419], dtype=float32), 0.0369969]
[2019-03-27 05:41:22,988] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.14184550666667, 87.26692186666666, 1.0, 2.0, 0.6396639682608953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893913.1342100556, 893913.1342100556, 208368.7849214028]
[2019-03-27 05:41:22,989] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:41:22,991] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.6745484e-14 1.0000000e+00 3.4065909e-22 2.4028329e-14 3.9323633e-21], sampled 0.10234682369254344
[2019-03-27 05:41:27,720] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00107419], dtype=float32), 0.0369969]
[2019-03-27 05:41:27,721] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 70.0, 1.0, 2.0, 0.3964572762549383, 1.0, 2.0, 0.3964572762549383, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1108182.857707493, 1108182.857707493, 271245.4948300262]
[2019-03-27 05:41:27,725] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:41:27,730] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6696864e-08 9.9999988e-01 2.3671960e-14 1.7806438e-07 1.9802152e-13], sampled 0.056987183019205134
[2019-03-27 05:41:31,905] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00107419], dtype=float32), 0.0369969]
[2019-03-27 05:41:31,907] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.40510096, 69.38230906, 1.0, 2.0, 0.4796267687145012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 670194.910241183, 670194.9102411823, 180465.1926387742]
[2019-03-27 05:41:31,909] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:41:31,911] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.8262907e-14 1.0000000e+00 3.7894841e-22 3.0030286e-14 3.1792204e-21], sampled 0.3037447303640216
[2019-03-27 05:42:14,341] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6186 2779197265.0983 933.0000
[2019-03-27 05:42:14,375] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.0936 3007673216.5125 1766.0000
[2019-03-27 05:42:14,402] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8250.5251 2927489876.1579 1338.0000
[2019-03-27 05:42:14,446] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6673 3164095478.5335 1778.0000
[2019-03-27 05:42:14,608] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8604 2842435242.9853 1131.0000
[2019-03-27 05:42:15,624] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 500000, evaluation results [500000.0, 7882.667340453697, 3164095478.53349, 1778.0, 8250.525087712145, 2927489876.1578894, 1338.0, 8660.618565999279, 2779197265.0982533, 933.0, 7996.093556134373, 3007673216.512464, 1766.0, 8496.860364057107, 2842435242.9852624, 1131.0]
[2019-03-27 05:42:21,880] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.6944317e-14 1.0000000e+00 1.9790454e-20 3.6018427e-13 9.7289801e-21], sum to 1.0000
[2019-03-27 05:42:21,894] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1259
[2019-03-27 05:42:21,901] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 97.0, 1.0, 2.0, 0.3043100954086112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 483045.0779612894, 483045.0779612888, 165814.5117032176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2946600.0000, 
sim time next is 2947200.0000, 
raw observation next is [20.66666666666667, 96.0, 1.0, 2.0, 0.3078368523663025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488082.7785863078, 488082.7785863078, 166169.8120022653], 
processed observation next is [1.0, 0.08695652173913043, 0.17851500789889443, 0.96, 1.0, 1.0, 0.16606849682687044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13557854960730772, 0.13557854960730772, 0.24801464477950044], 
reward next is 0.7520, 
noisyNet noise sample is [array([1.3324213], dtype=float32), 0.74245524]. 
=============================================
[2019-03-27 05:42:27,455] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4050150e-13 1.0000000e+00 3.2982439e-22 7.2248582e-15 1.2381668e-20], sum to 1.0000
[2019-03-27 05:42:27,467] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3814
[2019-03-27 05:42:27,472] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3023282602096846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481441.2584630319, 481441.2584630319, 165725.1783687972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3029400.0000, 
sim time next is 3030000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3014658689513921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480068.0505538484, 480068.0505538478, 165626.7477637717], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.1583926131944483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13335223626495787, 0.13335223626495774, 0.24720410113995778], 
reward next is 0.7528, 
noisyNet noise sample is [array([0.6656515], dtype=float32), -0.50410277]. 
=============================================
[2019-03-27 05:42:27,490] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.89422 ]
 [74.1227  ]
 [74.41321 ]
 [74.62057 ]
 [74.768715]], R is [[73.71516418]
 [73.73065948]
 [73.74595642]
 [73.76121521]
 [73.77638245]].
[2019-03-27 05:42:38,993] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4735554e-14 1.0000000e+00 1.5650097e-21 5.7711262e-14 1.9169350e-20], sum to 1.0000
[2019-03-27 05:42:39,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0443
[2019-03-27 05:42:39,008] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.66666666666667, 65.66666666666667, 1.0, 2.0, 0.5863683956701757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819405.2798309708, 819405.2798309708, 198234.6409096399], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3255600.0000, 
sim time next is 3256200.0000, 
raw observation next is [32.5, 67.0, 1.0, 2.0, 0.5876854137181252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 821246.4239906766, 821246.4239906761, 198475.0997085562], 
processed observation next is [0.0, 0.6956521739130435, 0.7393364928909952, 0.67, 1.0, 1.0, 0.5032354382146087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22812400666407684, 0.22812400666407667, 0.2962314921023227], 
reward next is 0.7038, 
noisyNet noise sample is [array([0.77089393], dtype=float32), 3.6647954]. 
=============================================
[2019-03-27 05:42:39,035] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0626793e-13 1.0000000e+00 9.9167704e-23 6.5126683e-14 1.0119042e-21], sum to 1.0000
[2019-03-27 05:42:39,044] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6903
[2019-03-27 05:42:39,050] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.5786776747525947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808653.99059273, 808653.99059273, 196841.6482538433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3249600.0000, 
sim time next is 3250200.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.5808133630351888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811639.5778681749, 811639.5778681749, 197226.576163021], 
processed observation next is [0.0, 0.6086956521739131, 0.7630331753554502, 0.63, 1.0, 1.0, 0.4949558590785407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22545543829671524, 0.22545543829671524, 0.29436802412391194], 
reward next is 0.7056, 
noisyNet noise sample is [array([1.995681], dtype=float32), 1.6905798]. 
=============================================
[2019-03-27 05:42:40,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4925370e-12 1.0000000e+00 6.4522552e-22 3.2488508e-14 2.5097172e-22], sum to 1.0000
[2019-03-27 05:42:40,717] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0307
[2019-03-27 05:42:40,721] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5171671507583908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722668.9017923947, 722668.9017923947, 186329.605471932], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3276000.0000, 
sim time next is 3276600.0000, 
raw observation next is [27.83333333333334, 79.00000000000001, 1.0, 2.0, 0.5146523632729149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719153.6477883166, 719153.647788316, 185923.4973853219], 
processed observation next is [0.0, 0.9565217391304348, 0.5181674565560824, 0.7900000000000001, 1.0, 1.0, 0.41524381117218656, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19976490216342127, 0.1997649021634211, 0.27749775729152526], 
reward next is 0.7225, 
noisyNet noise sample is [array([-0.6627525], dtype=float32), 0.021069264]. 
=============================================
[2019-03-27 05:42:41,803] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.8101954e-14 1.0000000e+00 2.0738953e-20 1.8345595e-14 5.5687548e-21], sum to 1.0000
[2019-03-27 05:42:41,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7598
[2019-03-27 05:42:41,814] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 83.66666666666667, 1.0, 2.0, 0.4643756041318868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 658046.0924876723, 658046.092487673, 179383.2837849105], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3295200.0000, 
sim time next is 3295800.0000, 
raw observation next is [25.5, 83.5, 1.0, 2.0, 0.4553574347421961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650414.1350752919, 650414.1350752924, 178714.4579361897], 
processed observation next is [0.0, 0.13043478260869565, 0.40758293838862564, 0.835, 1.0, 1.0, 0.3438041382436098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18067059307646996, 0.18067059307647013, 0.2667379969196861], 
reward next is 0.7333, 
noisyNet noise sample is [array([-2.2673743], dtype=float32), 0.2051945]. 
=============================================
[2019-03-27 05:42:44,457] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8909139e-12 1.0000000e+00 3.2233380e-23 6.6111667e-14 4.3802106e-22], sum to 1.0000
[2019-03-27 05:42:44,466] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9511
[2019-03-27 05:42:44,472] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 80.66666666666667, 1.0, 2.0, 0.553952466700862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774089.9756430094, 774089.9756430094, 192480.7351894168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3352800.0000, 
sim time next is 3353400.0000, 
raw observation next is [28.5, 81.5, 1.0, 2.0, 0.5522574443033643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771720.5007004391, 771720.5007004397, 192188.5110260873], 
processed observation next is [0.0, 0.8260869565217391, 0.5497630331753555, 0.815, 1.0, 1.0, 0.46055113771489675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21436680575012196, 0.21436680575012212, 0.2868485239195333], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.6789085], dtype=float32), 0.012418768]. 
=============================================
[2019-03-27 05:42:48,858] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8488769e-06 9.9983454e-01 1.2916168e-10 1.5657225e-04 8.4000840e-10], sum to 1.0000
[2019-03-27 05:42:48,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3277
[2019-03-27 05:42:48,875] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2572902.550401304 W.
[2019-03-27 05:42:48,880] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.66666666666666, 64.33333333333334, 1.0, 2.0, 0.6132405718465964, 1.0, 2.0, 0.6132405718465964, 1.0, 1.0, 1.03, 6.950541757705058, 6.9112, 170.5573041426782, 2572902.550401304, 2544720.449036786, 492830.6586801425], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3418800.0000, 
sim time next is 3419400.0000, 
raw observation next is [33.83333333333334, 63.66666666666666, 1.0, 2.0, 0.9001270619504819, 1.0, 2.0, 0.9001270619504819, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2517650.403933862, 2517650.403933862, 471571.1778299312], 
processed observation next is [1.0, 0.5652173913043478, 0.8025276461295423, 0.6366666666666666, 1.0, 1.0, 0.8796711589764842, 1.0, 1.0, 0.8796711589764842, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6993473344260729, 0.6993473344260729, 0.7038375788506436], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8030055], dtype=float32), 1.4905624]. 
=============================================
[2019-03-27 05:42:51,165] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.0473862e-13 1.0000000e+00 7.3722007e-20 2.5462492e-12 8.0444142e-19], sum to 1.0000
[2019-03-27 05:42:51,174] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2168
[2019-03-27 05:42:51,184] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5113292983090572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714508.5743503075, 714508.574350308, 185390.1024351827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3451200.0000, 
sim time next is 3451800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5110902771966108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714174.4647164326, 714174.4647164332, 185351.8733841149], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4109521412007359, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1983817957545646, 0.19838179575456477, 0.27664458714047], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.7111204], dtype=float32), 0.22500029]. 
=============================================
[2019-03-27 05:43:00,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.03681994e-10 1.00000000e+00 5.05741866e-17 3.44005890e-08
 1.36327544e-16], sum to 1.0000
[2019-03-27 05:43:00,229] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4545
[2019-03-27 05:43:00,233] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 66.33333333333334, 1.0, 2.0, 0.5509519625397519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769895.5689331697, 769895.5689331697, 191964.5982093486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3609600.0000, 
sim time next is 3610200.0000, 
raw observation next is [31.16666666666667, 66.16666666666666, 1.0, 2.0, 0.5461988551441728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763251.2309266883, 763251.2309266883, 191151.2970614113], 
processed observation next is [1.0, 0.782608695652174, 0.6761453396524489, 0.6616666666666666, 1.0, 1.0, 0.45325163270382257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21201423081296897, 0.21201423081296897, 0.28530044337524074], 
reward next is 0.7147, 
noisyNet noise sample is [array([-1.2573836], dtype=float32), -0.46360776]. 
=============================================
[2019-03-27 05:43:05,180] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1058395e-05 9.9598396e-01 1.1694659e-09 4.0049842e-03 1.0371610e-09], sum to 1.0000
[2019-03-27 05:43:05,192] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9693
[2019-03-27 05:43:05,199] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3044637.870343423 W.
[2019-03-27 05:43:05,205] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.978401712610214, 6.9112, 168.9070063766172, 3044637.870343423, 2287554.915140064, 473752.0326812895], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3682800.0000, 
sim time next is 3683400.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 0.0, 1.0, 0.0, 6.952772812458382, 6.9112, 170.5573041426782, 2939144.748379644, 2909364.451741815, 553414.219497767], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.59, 1.0, 1.0, 1.0481927710843375, 1.0, 0.5, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.0041572812458381845, 0.0, 0.8375144448122397, 0.8164290967721234, 0.8081567921505042, 0.825991372384727], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.30669168], dtype=float32), -0.74582094]. 
=============================================
[2019-03-27 05:43:09,506] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 05:43:09,508] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:43:09,509] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:43:09,509] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:43:09,511] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:43:09,510] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:43:09,513] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:43:09,512] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:43:09,515] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:43:09,516] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:43:09,513] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:43:09,534] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run22
[2019-03-27 05:43:09,534] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run22
[2019-03-27 05:43:09,552] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run22
[2019-03-27 05:43:09,587] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run22
[2019-03-27 05:43:09,609] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run22
[2019-03-27 05:43:21,404] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:43:21,408] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.35, 55.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.981304886551094, 6.9112, 168.9120459409178, 1503523.540787014, 1453788.98955225, 311346.9553951104]
[2019-03-27 05:43:21,408] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:43:21,413] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.3052350e-13 1.0000000e+00 1.2516776e-20 1.4865291e-12 3.8101290e-20], sampled 0.7600640552769296
[2019-03-27 05:43:25,427] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:43:25,428] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.95097154166667, 94.68491774833333, 1.0, 2.0, 0.2968744705729731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478312.8238815715, 478312.8238815709, 165541.0203552437]
[2019-03-27 05:43:25,431] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:43:25,435] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.8808362e-13 1.0000000e+00 3.0256606e-20 4.4421373e-13 8.0499666e-20], sampled 0.792125415347397
[2019-03-27 05:43:36,590] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:43:36,591] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.75, 82.0, 1.0, 2.0, 0.4680762628345708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668949.3676758202, 668949.3676758202, 180654.3150869421]
[2019-03-27 05:43:36,593] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:43:36,596] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6377289e-13 1.0000000e+00 4.4514151e-21 3.3586057e-13 1.2788503e-20], sampled 0.7621412271754192
[2019-03-27 05:43:55,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:43:55,413] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3761645034133674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 579443.1204369199, 579443.1204369199, 173171.4684380812]
[2019-03-27 05:43:55,416] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:43:55,420] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.2406498e-13 1.0000000e+00 2.7303403e-20 4.5591000e-13 8.2304355e-20], sampled 0.31794770839465447
[2019-03-27 05:44:07,413] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:44:07,416] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.23333333333333, 67.66666666666667, 1.0, 2.0, 0.5330178514402666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744825.8087578397, 744825.8087578404, 188930.6855104732]
[2019-03-27 05:44:07,416] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:44:07,419] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.633021e-13 1.000000e+00 1.891165e-20 5.484418e-13 5.925063e-20], sampled 0.8680458065309042
[2019-03-27 05:44:19,149] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:44:19,151] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [38.46666666666667, 39.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.64376346804138, 6.9112, 168.9087803828023, 1973803.798737426, 1454110.910589991, 311353.0591335294]
[2019-03-27 05:44:19,154] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:44:19,160] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9407531e-12 1.0000000e+00 8.8362697e-20 1.3201815e-11 2.9469938e-19], sampled 0.4549774487472882
[2019-03-27 05:44:19,160] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1973803.798737426 W.
[2019-03-27 05:44:26,669] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:44:26,671] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.16666666666667, 77.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 11.75291539253871, 6.9112, 168.8685630710071, 4889822.936822151, 1455846.148047635, 299196.5386035157]
[2019-03-27 05:44:26,673] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:44:26,676] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.5905402e-12 1.0000000e+00 4.1734191e-19 2.1351879e-11 1.1553770e-18], sampled 0.38761429283397497
[2019-03-27 05:44:26,677] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 4889822.936822151 W.
[2019-03-27 05:44:33,553] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:44:33,555] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.25, 94.5, 1.0, 2.0, 0.6214228555928781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868411.2499869526, 868411.2499869526, 204804.4124883975]
[2019-03-27 05:44:33,556] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:44:33,558] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.4119693e-14 1.0000000e+00 6.2473977e-22 9.8035255e-14 1.6431031e-21], sampled 0.22700833810427712
[2019-03-27 05:44:43,589] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:44:43,590] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.63333333333333, 90.33333333333334, 1.0, 2.0, 0.512271098554551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715825.0470938901, 715825.0470938896, 185539.8403558284]
[2019-03-27 05:44:43,592] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:44:43,595] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.7208808e-13 1.0000000e+00 1.1019501e-20 4.9035488e-13 3.4660532e-20], sampled 0.8548302430284533
[2019-03-27 05:44:52,586] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00255952], dtype=float32), 0.035779875]
[2019-03-27 05:44:52,588] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.48917941333333, 79.32139695666666, 1.0, 2.0, 0.3807165979188887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 579768.5218488242, 579768.5218488242, 173035.7728941596]
[2019-03-27 05:44:52,588] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:44:52,590] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.9235700e-13 1.0000000e+00 1.6386285e-20 5.6209280e-13 4.7474290e-20], sampled 0.3087325580765332
[2019-03-27 05:45:03,932] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.1591 3164148502.0330 1778.0000
[2019-03-27 05:45:04,170] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8656.2544 2779427750.0743 933.0000
[2019-03-27 05:45:04,647] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-27 05:45:04,654] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5578 2927401370.6864 1338.0000
[2019-03-27 05:45:04,675] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0954 2842517464.9746 1131.0000
[2019-03-27 05:45:05,687] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 525000, evaluation results [525000.0, 7881.1590859285025, 3164148502.033024, 1778.0, 8253.55780182101, 2927401370.6864357, 1338.0, 8656.254423096752, 2779427750.074331, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8496.095384209508, 2842517464.974608, 1131.0]
[2019-03-27 05:45:10,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5207540e-14 1.0000000e+00 1.6906055e-20 4.1861396e-13 1.5421585e-20], sum to 1.0000
[2019-03-27 05:45:10,815] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2661
[2019-03-27 05:45:10,823] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 58.66666666666667, 1.0, 2.0, 0.6194142991590283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865603.2364676001, 865603.2364676001, 204419.2682650468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3853200.0000, 
sim time next is 3853800.0000, 
raw observation next is [35.0, 58.0, 1.0, 2.0, 0.6138748972006653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857859.0486237034, 857859.0486237034, 203359.9688358732], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.58, 1.0, 1.0, 0.5347890327718859, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23829418017325096, 0.23829418017325096, 0.30352234154607943], 
reward next is 0.6965, 
noisyNet noise sample is [array([-0.3351966], dtype=float32), 0.7258318]. 
=============================================
[2019-03-27 05:45:20,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7337228e-05 9.7453737e-01 6.1536247e-09 2.5425380e-02 2.5421897e-08], sum to 1.0000
[2019-03-27 05:45:20,268] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5080
[2019-03-27 05:45:20,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2915969.873467868 W.
[2019-03-27 05:45:20,280] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 6.920458694304476, 6.9112, 170.5573041426782, 2915969.873467868, 2909337.494117967, 553583.4820949784], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4024200.0000, 
sim time next is 4024800.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.02566235016972, 6.9112, 170.5573041426782, 2991419.298164278, 2909425.260706991, 553050.7802995844], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.6, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.01144623501697204, 0.0, 0.8375144448122397, 0.8309498050456328, 0.8081736835297197, 0.8254489258202752], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.42788184], dtype=float32), 1.5299895]. 
=============================================
[2019-03-27 05:45:26,103] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0510713e-05 9.7485811e-01 3.7096333e-09 2.5091415e-02 3.0935688e-08], sum to 1.0000
[2019-03-27 05:45:26,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5254
[2019-03-27 05:45:26,116] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3091809.692517083 W.
[2019-03-27 05:45:26,121] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.33333333333334, 65.0, 1.0, 2.0, 0.8323540268226124, 1.0, 2.0, 0.7367670529255687, 1.0, 2.0, 1.03, 7.005108170469493, 6.9112, 170.5573041426782, 3091809.692517083, 3024539.450069643, 566394.4914820186], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4113600.0000, 
sim time next is 4114200.0000, 
raw observation next is [35.66666666666666, 64.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.907646290414492, 6.9112, 170.5573041426782, 3623956.278740894, 2910161.268073408, 547918.6771823313], 
processed observation next is [1.0, 0.6086956521739131, 0.889415481832543, 0.645, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.09964462904144922, 0.0, 0.8375144448122397, 1.0066545218724705, 0.808378130020391, 0.81778907042139], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1120268], dtype=float32), 0.36142534]. 
=============================================
[2019-03-27 05:45:28,571] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2530912e-05 9.9951518e-01 2.6449060e-10 4.7226786e-04 6.8977313e-10], sum to 1.0000
[2019-03-27 05:45:28,580] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5305
[2019-03-27 05:45:28,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3516287.310523318 W.
[2019-03-27 05:45:28,594] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 59.33333333333333, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.757517079750074, 6.9112, 170.5573041426782, 3516287.310523318, 2910035.960320596, 548869.8148873077], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4183800.0000, 
sim time next is 4184400.0000, 
raw observation next is [35.0, 58.66666666666667, 1.0, 2.0, 0.9684650945721313, 1.0, 2.0, 0.8048225868003284, 1.0, 1.0, 1.03, 7.005118909933828, 6.9112, 170.5573041426782, 3377787.913742836, 3310509.978180239, 619548.4027066141], 
processed observation next is [1.0, 0.43478260869565216, 0.8578199052132701, 0.5866666666666667, 1.0, 1.0, 0.9620061380387124, 1.0, 1.0, 0.7648464901208776, 1.0, 0.5, 1.0365853658536586, 0.009391890993382824, 0.0, 0.8375144448122397, 0.9382744204841211, 0.9195861050500664, 0.9246991085173345], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.19194931], dtype=float32), -0.65834844]. 
=============================================
[2019-03-27 05:45:38,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3047604e-12 1.0000000e+00 1.4033915e-19 7.5795889e-12 1.2059523e-18], sum to 1.0000
[2019-03-27 05:45:38,573] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2349
[2019-03-27 05:45:38,582] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2602131.334153201 W.
[2019-03-27 05:45:38,589] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.16666666666666, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.528954585760246, 6.9112, 168.9040528137483, 2602131.334153201, 1454501.048982887, 310414.9103111032], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4345800.0000, 
sim time next is 4346400.0000, 
raw observation next is [30.33333333333334, 84.0, 1.0, 2.0, 0.5288076550731582, 1.0, 1.0, 0.5288076550731582, 1.0, 1.0, 0.9183640174982296, 6.911200000000001, 6.9112, 170.5573041426782, 2218342.695023546, 2218342.695023545, 435640.7172057435], 
processed observation next is [1.0, 0.30434782608695654, 0.6366508688783573, 0.84, 1.0, 1.0, 0.43229837960621464, 1.0, 0.5, 0.43229837960621464, 1.0, 0.5, 0.9004439237783286, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6162063041732072, 0.616206304173207, 0.6502100256802141], 
reward next is 0.3498, 
noisyNet noise sample is [array([-0.01932538], dtype=float32), 1.1950724]. 
=============================================
[2019-03-27 05:45:38,977] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3382656e-06 9.9933738e-01 1.7785580e-11 6.5837172e-04 8.5322027e-11], sum to 1.0000
[2019-03-27 05:45:38,985] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2682
[2019-03-27 05:45:38,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3293445.333232984 W.
[2019-03-27 05:45:39,006] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.5, 64.5, 1.0, 2.0, 0.9283255628633749, 1.0, 2.0, 0.7847528209459499, 1.0, 1.0, 1.03, 7.00511574232489, 6.9112, 170.5573041426782, 3293445.333232984, 3226169.666757519, 603153.7438595514], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4368600.0000, 
sim time next is 4369200.0000, 
raw observation next is [33.66666666666666, 68.0, 1.0, 2.0, 0.8642242537057864, 1.0, 2.0, 0.7527021663671556, 1.0, 2.0, 1.03, 7.005110684660186, 6.9112, 170.5573041426782, 3158765.243964495, 3091493.200499991, 578219.3984878542], 
processed observation next is [1.0, 0.5652173913043478, 0.7946287519747232, 0.68, 1.0, 1.0, 0.8364147635009475, 1.0, 1.0, 0.7020508028519947, 1.0, 1.0, 1.0365853658536586, 0.00939106846601856, 0.0, 0.8375144448122397, 0.8774347899901376, 0.8587481112499975, 0.8630140275938122], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.09090554], dtype=float32), 0.77440625]. 
=============================================
[2019-03-27 05:45:42,059] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.679423e-14 1.000000e+00 5.527804e-20 5.980309e-13 3.779438e-21], sum to 1.0000
[2019-03-27 05:45:42,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3536
[2019-03-27 05:45:42,070] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 65.66666666666667, 1.0, 2.0, 0.5321880424947747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 743665.8485963726, 743665.8485963732, 188793.6441412979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4392600.0000, 
sim time next is 4393200.0000, 
raw observation next is [31.0, 68.33333333333334, 1.0, 2.0, 0.5427259812815993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758396.5493518121, 758396.5493518121, 190562.2585090613], 
processed observation next is [1.0, 0.8695652173913043, 0.6682464454976303, 0.6833333333333335, 1.0, 1.0, 0.4490674473272281, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21066570815328114, 0.21066570815328114, 0.2844212813568079], 
reward next is 0.7156, 
noisyNet noise sample is [array([0.28415516], dtype=float32), 0.8639772]. 
=============================================
[2019-03-27 05:45:43,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.5945590e-15 1.0000000e+00 1.7710955e-22 3.8949123e-14 8.0295981e-21], sum to 1.0000
[2019-03-27 05:45:43,688] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7244
[2019-03-27 05:45:43,692] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 88.16666666666667, 1.0, 2.0, 0.6123978910827148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855794.1759007145, 855794.1759007145, 203078.4648208303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4416600.0000, 
sim time next is 4417200.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.6107698578499284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 853518.1700643079, 853518.1700643084, 202769.5700233819], 
processed observation next is [0.0, 0.13043478260869565, 0.5734597156398105, 0.89, 1.0, 1.0, 0.5310480215059379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23708838057341886, 0.23708838057341902, 0.30264114928862973], 
reward next is 0.6974, 
noisyNet noise sample is [array([-0.8425191], dtype=float32), -0.7701739]. 
=============================================
[2019-03-27 05:45:57,312] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9624105e-14 1.0000000e+00 2.8400907e-20 6.2916868e-12 2.4183848e-20], sum to 1.0000
[2019-03-27 05:45:57,322] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3062
[2019-03-27 05:45:57,325] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.83333333333333, 1.0, 2.0, 0.5101189259999527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712816.6870251385, 712816.6870251378, 185196.9682904144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4669800.0000, 
sim time next is 4670400.0000, 
raw observation next is [27.0, 85.66666666666667, 1.0, 2.0, 0.5136318924589645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717727.2028932939, 717727.2028932932, 185759.8312043077], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.8566666666666667, 1.0, 1.0, 0.41401432826381257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19936866747035942, 0.19936866747035922, 0.27725347940941447], 
reward next is 0.7227, 
noisyNet noise sample is [array([-0.43600723], dtype=float32), 0.44354758]. 
=============================================
[2019-03-27 05:45:57,503] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2796901e-15 1.0000000e+00 6.6566653e-22 1.1351305e-14 1.6238109e-22], sum to 1.0000
[2019-03-27 05:45:57,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0524
[2019-03-27 05:45:57,517] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 94.0, 1.0, 2.0, 0.4558572356147115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650958.3237914713, 650958.3237914713, 178766.5752197482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4656000.0000, 
sim time next is 4656600.0000, 
raw observation next is [24.25, 94.0, 1.0, 2.0, 0.4596692198115073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654075.6781767484, 654075.6781767484, 179033.3958349618], 
processed observation next is [1.0, 0.9130434782608695, 0.3483412322274882, 0.94, 1.0, 1.0, 0.3489990600138642, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18168768838243013, 0.18168768838243013, 0.2672140236342713], 
reward next is 0.7328, 
noisyNet noise sample is [array([0.31109154], dtype=float32), 1.0851526]. 
=============================================
[2019-03-27 05:45:59,618] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 05:45:59,620] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:45:59,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:45:59,621] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:45:59,622] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:45:59,623] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:45:59,624] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:45:59,624] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:45:59,624] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:45:59,627] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:45:59,627] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:45:59,645] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run23
[2019-03-27 05:45:59,664] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run23
[2019-03-27 05:45:59,686] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run23
[2019-03-27 05:45:59,714] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run23
[2019-03-27 05:45:59,735] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run23
[2019-03-27 05:46:10,670] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00331929], dtype=float32), 0.035945997]
[2019-03-27 05:46:10,671] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.07103556666667, 86.830197, 1.0, 2.0, 0.2170571998549825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 362108.2187675375, 362108.2187675375, 157167.7349191224]
[2019-03-27 05:46:10,673] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:46:10,677] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.4337293e-14 1.0000000e+00 2.0591213e-21 4.6720853e-14 8.6608900e-22], sampled 0.6127254694194385
[2019-03-27 05:46:41,692] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00331929], dtype=float32), 0.035945997]
[2019-03-27 05:46:41,693] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 89.0, 1.0, 2.0, 0.4091672451503332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602870.9496955557, 602870.9496955563, 174580.6887437804]
[2019-03-27 05:46:41,695] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:46:41,700] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8678371e-14 1.0000000e+00 6.0256912e-22 3.5640497e-14 1.8543654e-22], sampled 0.7804306187500974
[2019-03-27 05:46:47,121] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00331929], dtype=float32), 0.035945997]
[2019-03-27 05:46:47,122] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 89.66666666666667, 1.0, 2.0, 0.4618733234810526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651166.125079046, 651166.1250790454, 178585.4620427874]
[2019-03-27 05:46:47,126] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:46:47,129] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.5450999e-14 1.0000000e+00 1.9022775e-21 3.4223001e-14 6.4323738e-22], sampled 0.4916466991048495
[2019-03-27 05:47:02,659] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00331929], dtype=float32), 0.035945997]
[2019-03-27 05:47:02,659] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.810551707165815, 6.9112, 170.5573041426782, 3554322.436419871, 2910080.225288541, 548541.0378716155]
[2019-03-27 05:47:02,662] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:47:02,665] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6194259e-06 9.9913436e-01 1.0747918e-10 8.6305157e-04 7.7709332e-11], sampled 0.5969208177834361
[2019-03-27 05:47:02,666] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3554322.436419871 W.
[2019-03-27 05:47:54,192] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.8889 3007799306.8851 1766.0000
[2019-03-27 05:47:54,436] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.3799 2842507723.3256 1131.0000
[2019-03-27 05:47:54,535] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.4953 2779263801.8109 933.0000
[2019-03-27 05:47:54,568] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6612 3164046809.8528 1778.0000
[2019-03-27 05:47:54,683] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.8737 2927369435.7472 1338.0000
[2019-03-27 05:47:55,698] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 550000, evaluation results [550000.0, 7882.66122461977, 3164046809.8528395, 1778.0, 8252.873660369303, 2927369435.7472334, 1338.0, 8658.495312354024, 2779263801.81092, 933.0, 7995.888911741081, 3007799306.885107, 1766.0, 8495.379860653391, 2842507723.3256373, 1131.0]
[2019-03-27 05:48:01,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3250413e-05 9.8646784e-01 2.1057418e-09 1.3478923e-02 3.1640244e-09], sum to 1.0000
[2019-03-27 05:48:01,138] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2197
[2019-03-27 05:48:01,145] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2538699.874835012 W.
[2019-03-27 05:48:01,147] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.66666666666666, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.261883438061761, 6.9112, 168.9113543322479, 2538699.874835012, 2289915.334082427, 475635.9847928412], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4808400.0000, 
sim time next is 4809000.0000, 
raw observation next is [31.83333333333333, 63.5, 1.0, 2.0, 0.6037708075926304, 1.0, 1.0, 0.6037708075926304, 1.0, 2.0, 1.03, 6.932053391047007, 6.9112, 170.5573041426782, 2533131.080606393, 2518192.948337709, 489349.0072489235], 
processed observation next is [1.0, 0.6521739130434783, 0.7077409162717218, 0.635, 1.0, 1.0, 0.5226154308344944, 1.0, 0.5, 0.5226154308344944, 1.0, 1.0, 1.0365853658536586, 0.0020853391047007007, 0.0, 0.8375144448122397, 0.7036475223906647, 0.6994980412049192, 0.7303716526103335], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.64530927], dtype=float32), -1.1226141]. 
=============================================
[2019-03-27 05:48:01,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[11.7490835]
 [10.853892 ]
 [12.126494 ]
 [12.763312 ]
 [15.02842  ]], R is [[10.19752312]
 [10.09554768]
 [10.35950089]
 [10.60660744]
 [10.50054169]].
[2019-03-27 05:48:02,329] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1606519e-14 1.0000000e+00 2.0522586e-22 1.4202888e-12 2.1452036e-20], sum to 1.0000
[2019-03-27 05:48:02,340] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4324
[2019-03-27 05:48:02,345] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.58333333333334, 76.5, 1.0, 2.0, 0.4925415304471977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688246.8873747757, 688246.887374775, 182435.5013616573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4834200.0000, 
sim time next is 4834800.0000, 
raw observation next is [27.5, 77.0, 1.0, 2.0, 0.4933954913837593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689440.5464808679, 689440.5464808679, 182567.4639977381], 
processed observation next is [1.0, 1.0, 0.5023696682464456, 0.77, 1.0, 1.0, 0.38963312214910767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1915112629113522, 0.1915112629113522, 0.27248875223542995], 
reward next is 0.7275, 
noisyNet noise sample is [array([-0.145418], dtype=float32), 0.9179293]. 
=============================================
[2019-03-27 05:48:06,225] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1068554e-06 9.9765211e-01 1.7722032e-10 2.3418132e-03 6.6411276e-10], sum to 1.0000
[2019-03-27 05:48:06,232] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3278
[2019-03-27 05:48:06,243] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2343836.037375038 W.
[2019-03-27 05:48:06,249] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.8380420235695669, 1.0, 2.0, 0.8380420235695669, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2343836.037375038, 2343836.037375038, 438813.3211670219], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4881600.0000, 
sim time next is 4882200.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5397428572028911, 1.0, 2.0, 0.5397428572028911, 1.0, 1.0, 0.9349235763280397, 6.9112, 6.9112, 170.5573041426782, 2264257.310766682, 2264257.310766682, 443239.7167098761], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44547332193119404, 1.0, 1.0, 0.44547332193119404, 1.0, 0.5, 0.9206385077171215, 0.0, 0.0, 0.8375144448122397, 0.6289603641018561, 0.6289603641018561, 0.6615518159848897], 
reward next is 0.3384, 
noisyNet noise sample is [array([-1.3531963], dtype=float32), 2.0970366]. 
=============================================
[2019-03-27 05:48:06,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7182673e-14 1.0000000e+00 6.5545385e-22 1.3062381e-14 6.0221464e-24], sum to 1.0000
[2019-03-27 05:48:06,998] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2655
[2019-03-27 05:48:07,003] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 72.66666666666667, 1.0, 2.0, 0.5234998566177778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731521.0212447378, 731521.0212447371, 187359.871395574], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5127000.0000, 
sim time next is 5127600.0000, 
raw observation next is [29.33333333333334, 71.33333333333334, 1.0, 2.0, 0.5221001399677294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729564.4327851983, 729564.4327851989, 187131.0297538437], 
processed observation next is [0.0, 0.34782608695652173, 0.5892575039494474, 0.7133333333333334, 1.0, 1.0, 0.42421703610569805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2026567868847773, 0.20265678688477748, 0.2793000444087219], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.38988096], dtype=float32), 1.3664317]. 
=============================================
[2019-03-27 05:48:08,082] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.23270084e-13 1.00000000e+00 1.10707365e-20 1.07053114e-12
 4.87014177e-21], sum to 1.0000
[2019-03-27 05:48:08,092] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7623
[2019-03-27 05:48:08,097] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 86.5, 1.0, 2.0, 0.5061648481581459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707289.6013497357, 707289.6013497364, 184567.5614508101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4930200.0000, 
sim time next is 4930800.0000, 
raw observation next is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5048256229970798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705417.6133108041, 705417.6133108041, 184355.5239324152], 
processed observation next is [1.0, 0.043478260869565216, 0.44707740916271754, 0.8733333333333333, 1.0, 1.0, 0.4034043650567226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19594933703077894, 0.19594933703077894, 0.2751574984065899], 
reward next is 0.7248, 
noisyNet noise sample is [array([-0.8751559], dtype=float32), -1.5719365]. 
=============================================
[2019-03-27 05:48:08,310] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4953363e-13 1.0000000e+00 2.0347835e-20 4.7190235e-14 8.4921429e-22], sum to 1.0000
[2019-03-27 05:48:08,318] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1416
[2019-03-27 05:48:08,324] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.7493853896952534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1047321.46678686, 1047321.46678686, 231863.7486280597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4934400.0000, 
sim time next is 4935000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.723536433732406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1011178.44088567, 1011178.440885671, 226005.2025475325], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.6669113659426578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2808829002460195, 0.2808829002460197, 0.33732119783213804], 
reward next is 0.6627, 
noisyNet noise sample is [array([0.15288635], dtype=float32), -1.7871654]. 
=============================================
[2019-03-27 05:48:08,347] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[64.90762 ]
 [64.8004  ]
 [64.65759 ]
 [64.70271 ]
 [64.892944]], R is [[65.02670288]
 [65.03037262]
 [65.0249939 ]
 [64.9804306 ]
 [64.90422821]].
[2019-03-27 05:48:11,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4806448e-10 1.0000000e+00 3.1748171e-16 4.0101358e-10 2.2694231e-17], sum to 1.0000
[2019-03-27 05:48:11,040] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1874
[2019-03-27 05:48:11,048] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2071296.486838971 W.
[2019-03-27 05:48:11,052] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.83333333333333, 67.33333333333333, 1.0, 2.0, 0.4937885079243249, 1.0, 2.0, 0.4937885079243249, 1.0, 1.0, 0.8442276947010519, 6.9112, 6.9112, 170.5573041426782, 2071296.486838971, 2071296.486838971, 408484.9481852304], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4956600.0000, 
sim time next is 4957200.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.8553454061684787, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.977249542542909, 6.9112, 168.912562903163, 2092534.272790662, 2045676.563250744, 422974.1248411307], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.8257173568294923, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006604954254290885, 0.0, 0.8294380123604227, 0.5812595202196283, 0.5682434897918733, 0.6313046639419861], 
reward next is 0.0384, 
noisyNet noise sample is [array([-1.4061733], dtype=float32), 0.317397]. 
=============================================
[2019-03-27 05:48:14,536] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7007118e-13 1.0000000e+00 1.5322593e-21 2.0427937e-13 9.1447230e-22], sum to 1.0000
[2019-03-27 05:48:14,546] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8831
[2019-03-27 05:48:14,550] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 65.5, 1.0, 2.0, 0.5053654036161266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706172.1259651259, 706172.1259651259, 184441.4294424352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5047800.0000, 
sim time next is 5048400.0000, 
raw observation next is [30.33333333333334, 65.0, 1.0, 2.0, 0.508066914699337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709948.342770735, 709948.3427707345, 184870.064162707], 
processed observation next is [0.0, 0.43478260869565216, 0.6366508688783573, 0.65, 1.0, 1.0, 0.40730953578233364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19720787299187084, 0.19720787299187068, 0.2759254688995627], 
reward next is 0.7241, 
noisyNet noise sample is [array([0.30697778], dtype=float32), 1.4646196]. 
=============================================
[2019-03-27 05:48:17,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.1916953e-14 1.0000000e+00 1.1409661e-20 1.4143812e-13 6.8733701e-22], sum to 1.0000
[2019-03-27 05:48:17,462] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6392
[2019-03-27 05:48:17,468] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5308752258061428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741830.7118224282, 741830.7118224282, 188575.3002265263], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5076000.0000, 
sim time next is 5076600.0000, 
raw observation next is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.5286455297675907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738713.9113636625, 738713.9113636625, 188206.3161386328], 
processed observation next is [0.0, 0.782608695652174, 0.6129541864139019, 0.7066666666666667, 1.0, 1.0, 0.4321030479127599, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20519830871212846, 0.20519830871212846, 0.280904949460646], 
reward next is 0.7191, 
noisyNet noise sample is [array([0.07206503], dtype=float32), -0.32094327]. 
=============================================
[2019-03-27 05:48:20,678] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1576446e-15 1.0000000e+00 1.7738339e-22 1.8550840e-14 1.0928280e-21], sum to 1.0000
[2019-03-27 05:48:20,688] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1621
[2019-03-27 05:48:20,695] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5499997149519944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768564.4248759142, 768564.4248759142, 191801.2346867266], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5153400.0000, 
sim time next is 5154000.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5500360177157494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768615.1723702952, 768615.1723702945, 191807.4581109722], 
processed observation next is [0.0, 0.6521739130434783, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45787472013945707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2135042145473042, 0.213504214547304, 0.2862797882253316], 
reward next is 0.7137, 
noisyNet noise sample is [array([0.4347194], dtype=float32), 0.7386296]. 
=============================================
[2019-03-27 05:48:20,710] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.008965]
 [74.94103 ]
 [74.88017 ]
 [74.84048 ]
 [74.76257 ]], R is [[75.00921631]
 [74.97285461]
 [74.93595123]
 [74.89414215]
 [74.85871124]].
[2019-03-27 05:48:21,542] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8232225e-14 1.0000000e+00 3.0015291e-21 2.1767117e-14 3.4824041e-22], sum to 1.0000
[2019-03-27 05:48:21,553] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1356
[2019-03-27 05:48:21,562] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.66666666666666, 1.0, 2.0, 0.557372386554527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778870.7040187203, 778870.7040187209, 193073.4920540554], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5143800.0000, 
sim time next is 5144400.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.553675118974134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773702.2703814851, 773702.2703814845, 192433.2994364631], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.63, 1.0, 1.0, 0.4622591794869084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21491729732819032, 0.21491729732819015, 0.28721387975591506], 
reward next is 0.7128, 
noisyNet noise sample is [array([-2.2043395], dtype=float32), 0.39756468]. 
=============================================
[2019-03-27 05:48:26,894] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7560408e-08 9.9996078e-01 9.7968281e-13 3.9136958e-05 1.8253674e-14], sum to 1.0000
[2019-03-27 05:48:26,904] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6108
[2019-03-27 05:48:26,911] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 73.33333333333334, 1.0, 2.0, 0.537158088517535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750613.3165934792, 750613.3165934792, 189624.9711362056], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5247600.0000, 
sim time next is 5248200.0000, 
raw observation next is [30.16666666666666, 74.16666666666666, 1.0, 2.0, 0.5396405000564234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754083.4171146157, 754083.4171146157, 190041.9426662589], 
processed observation next is [1.0, 0.7391304347826086, 0.6287519747235385, 0.7416666666666666, 1.0, 1.0, 0.44535000006798, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20946761586517101, 0.20946761586517101, 0.2836446905466551], 
reward next is 0.7164, 
noisyNet noise sample is [array([-0.16853458], dtype=float32), 0.80282444]. 
=============================================
[2019-03-27 05:48:30,295] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2945491e-05 7.6604432e-01 5.9595006e-08 2.3386259e-01 1.7454515e-07], sum to 1.0000
[2019-03-27 05:48:30,306] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8401
[2019-03-27 05:48:30,315] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.33333333333333, 51.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.179420711241795, 6.9112, 170.5573041426782, 3101690.949781746, 2909553.543798291, 552243.2177817696], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5312400.0000, 
sim time next is 5313000.0000, 
raw observation next is [36.31666666666666, 51.83333333333334, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.469028641482371, 6.9112, 170.5573041426782, 3309390.546056663, 2909795.199144479, 550617.4168100285], 
processed observation next is [1.0, 0.4782608695652174, 0.9202211690363348, 0.5183333333333334, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.0557828641482371, 0.0, 0.8375144448122397, 0.9192751516824065, 0.8082764442067997, 0.8218170400149679], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5579374], dtype=float32), 1.4464438]. 
=============================================
[2019-03-27 05:48:30,325] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[ 5.6426435]
 [ 5.0581393]
 [ 6.2120776]
 [ 8.354631 ]
 [10.517687 ]], R is [[7.30338526]
 [7.23035145]
 [7.15804815]
 [7.08646774]
 [7.01560307]].
[2019-03-27 05:48:32,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.9896353e-13 1.0000000e+00 3.3751363e-20 3.0856489e-12 5.1200476e-22], sum to 1.0000
[2019-03-27 05:48:32,386] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0785
[2019-03-27 05:48:32,391] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 79.66666666666667, 1.0, 2.0, 0.6217062605328083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 868807.4579748324, 868807.457974833, 204859.7899359629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5347200.0000, 
sim time next is 5347800.0000, 
raw observation next is [30.7, 80.0, 1.0, 2.0, 0.6197149989281698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866023.6220010631, 866023.6220010631, 204476.6192047563], 
processed observation next is [1.0, 0.9130434782608695, 0.6540284360189573, 0.8, 1.0, 1.0, 0.5418252999134576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24056211722251752, 0.24056211722251752, 0.305188983887696], 
reward next is 0.6948, 
noisyNet noise sample is [array([-1.1726755], dtype=float32), -1.6453297]. 
=============================================
[2019-03-27 05:48:34,498] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9179302e-12 1.0000000e+00 8.0809268e-18 2.1634741e-11 6.0560697e-20], sum to 1.0000
[2019-03-27 05:48:34,506] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4586
[2019-03-27 05:48:34,514] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 75.0, 1.0, 2.0, 0.546834199335838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764139.3720458178, 764139.3720458171, 191259.9212479961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5598000.0000, 
sim time next is 5598600.0000, 
raw observation next is [29.43333333333333, 77.16666666666667, 1.0, 2.0, 0.549850754503204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768356.1936605874, 768356.1936605874, 191775.8128642236], 
processed observation next is [1.0, 0.8260869565217391, 0.5939968404423379, 0.7716666666666667, 1.0, 1.0, 0.45765151144964333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21343227601682985, 0.21343227601682985, 0.2862325565137666], 
reward next is 0.7138, 
noisyNet noise sample is [array([0.29560655], dtype=float32), -0.022850433]. 
=============================================
[2019-03-27 05:48:43,228] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7469465e-14 1.0000000e+00 1.2291066e-22 2.3831577e-13 1.0152173e-22], sum to 1.0000
[2019-03-27 05:48:43,237] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1162
[2019-03-27 05:48:43,242] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333334, 87.5, 1.0, 2.0, 0.5495289017941459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767906.2771858142, 767906.2771858149, 191719.8930627896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5526600.0000, 
sim time next is 5527200.0000, 
raw observation next is [27.46666666666667, 88.0, 1.0, 2.0, 0.5491351243688971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767355.8177284313, 767355.817728432, 191652.4824874598], 
processed observation next is [1.0, 1.0, 0.500789889415482, 0.88, 1.0, 1.0, 0.4567893064685507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21315439381345314, 0.21315439381345333, 0.2860484813245669], 
reward next is 0.7140, 
noisyNet noise sample is [array([0.35350475], dtype=float32), -0.23934674]. 
=============================================
[2019-03-27 05:48:46,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3839378e-06 9.9861622e-01 2.0359507e-10 1.3824004e-03 4.1126845e-11], sum to 1.0000
[2019-03-27 05:48:46,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0874
[2019-03-27 05:48:46,616] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2459003.551081871 W.
[2019-03-27 05:48:46,621] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.86666666666666, 49.66666666666667, 1.0, 2.0, 0.5861199078856641, 1.0, 2.0, 0.5861199078856641, 1.0, 1.0, 1.006973308885415, 6.911200000000001, 6.9112, 170.5573041426782, 2459003.551081871, 2459003.551081871, 477444.571172395], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5583000.0000, 
sim time next is 5583600.0000, 
raw observation next is [33.8, 50.0, 1.0, 2.0, 0.8548371381886073, 1.0, 2.0, 0.8548371381886073, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2390853.550507544, 2390853.550507544, 447443.3351538139], 
processed observation next is [1.0, 0.6521739130434783, 0.800947867298578, 0.5, 1.0, 1.0, 0.8251049857694064, 1.0, 1.0, 0.8251049857694064, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6641259862520956, 0.6641259862520956, 0.6678258733639013], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.218333], dtype=float32), 1.328121]. 
=============================================
[2019-03-27 05:48:49,668] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 05:48:49,671] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:48:49,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:48:49,673] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:48:49,674] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:48:49,674] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:48:49,675] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:48:49,675] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:48:49,678] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:48:49,676] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:48:49,680] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:48:49,698] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run24
[2019-03-27 05:48:49,699] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run24
[2019-03-27 05:48:49,732] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run24
[2019-03-27 05:48:49,751] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run24
[2019-03-27 05:48:49,775] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run24
[2019-03-27 05:49:18,547] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00398112], dtype=float32), 0.036147356]
[2019-03-27 05:49:18,549] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.7, 95.0, 1.0, 2.0, 0.3879540348807909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 582266.2124572822, 582266.2124572828, 173017.1129495691]
[2019-03-27 05:49:18,550] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:49:18,554] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6329072e-14 1.0000000e+00 4.4641177e-21 4.8097858e-14 3.4409401e-22], sampled 0.22144822375199436
[2019-03-27 05:49:23,461] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00398112], dtype=float32), 0.036147356]
[2019-03-27 05:49:23,462] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.73758738, 93.29024662333333, 1.0, 2.0, 0.575155878261168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803730.7104815161, 803730.7104815161, 196209.4930188654]
[2019-03-27 05:49:23,463] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:49:23,465] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5027112e-14 1.0000000e+00 1.4055331e-21 5.1539695e-14 8.2688476e-23], sampled 0.7856206659686515
[2019-03-27 05:49:23,963] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00398112], dtype=float32), 0.036147356]
[2019-03-27 05:49:23,965] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.04223147833333, 74.06987434333332, 1.0, 2.0, 0.4618245694042176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104262, 679528.8301247397, 679528.8301247404, 182139.9214769721]
[2019-03-27 05:49:23,966] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:49:23,970] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4866728e-12 1.0000000e+00 1.4865245e-18 3.9368633e-11 9.1281423e-20], sampled 0.4840407266836626
[2019-03-27 05:50:02,158] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00398112], dtype=float32), 0.036147356]
[2019-03-27 05:50:02,161] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.63333333333333, 78.33333333333334, 1.0, 2.0, 0.7101316361098168, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.004056923964308, 6.9112, 168.9114965847316, 1889306.644323255, 1823431.315943978, 386927.5026608849]
[2019-03-27 05:50:02,163] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:50:02,165] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2723466e-13 1.0000000e+00 1.9186323e-20 5.7476430e-13 1.9719156e-21], sampled 0.8289445337891553
[2019-03-27 05:50:02,166] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1889306.644323255 W.
[2019-03-27 05:50:02,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00398112], dtype=float32), 0.036147356]
[2019-03-27 05:50:02,360] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.6908605370632511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 965491.4784959515, 965491.4784959509, 218882.5216439698]
[2019-03-27 05:50:02,360] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:50:02,363] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8955188e-14 1.0000000e+00 1.9162907e-21 6.8321732e-14 1.3441289e-22], sampled 0.8922136654944212
[2019-03-27 05:50:09,333] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00398112], dtype=float32), 0.036147356]
[2019-03-27 05:50:09,334] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.60218965666667, 92.27189731666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.297609674437734, 6.9112, 168.9106909047811, 2558059.187182819, 2283930.541840229, 475261.9206077706]
[2019-03-27 05:50:09,338] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:50:09,343] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.11862825e-10 1.00000000e+00 2.69988549e-16 2.55171306e-09
 2.60389227e-17], sampled 0.832671967875013
[2019-03-27 05:50:09,344] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2558059.187182819 W.
[2019-03-27 05:50:44,709] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 05:50:44,797] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.3698 2842506997.8793 1131.0000
[2019-03-27 05:50:44,832] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4258 3164063616.5785 1778.0000
[2019-03-27 05:50:44,842] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.2438 3007867297.2953 1766.0000
[2019-03-27 05:50:45,040] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8788 2779338695.8653 933.0000
[2019-03-27 05:50:46,056] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 575000, evaluation results [575000.0, 7883.42583910815, 3164063616.5785074, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8659.87882874402, 2779338695.865296, 933.0, 7995.243848660214, 3007867297.2953196, 1766.0, 8495.369816379352, 2842506997.879306, 1131.0]
[2019-03-27 05:50:46,788] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0697755e-14 1.0000000e+00 6.9513408e-22 9.5961644e-16 3.7733532e-24], sum to 1.0000
[2019-03-27 05:50:46,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5803
[2019-03-27 05:50:46,794] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.75, 65.33333333333334, 1.0, 2.0, 0.5579412373779853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779665.906399961, 779665.9063999616, 193172.5295775913], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5658600.0000, 
sim time next is 5659200.0000, 
raw observation next is [31.8, 65.0, 1.0, 2.0, 0.5583092521566168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780180.358338228, 780180.358338228, 193236.4637166237], 
processed observation next is [0.0, 0.5217391304347826, 0.7061611374407584, 0.65, 1.0, 1.0, 0.4678424724778515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21671676620506333, 0.21671676620506333, 0.2884126324128712], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.16721772], dtype=float32), -0.57560587]. 
=============================================
[2019-03-27 05:50:48,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3930882e-15 1.0000000e+00 9.8398657e-22 4.1887971e-14 2.4636265e-22], sum to 1.0000
[2019-03-27 05:50:48,132] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9108
[2019-03-27 05:50:48,140] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.06666666666667, 59.83333333333333, 1.0, 2.0, 0.5413215084498287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756433.2628154951, 756433.2628154951, 190323.2964620353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5674200.0000, 
sim time next is 5674800.0000, 
raw observation next is [32.03333333333333, 59.66666666666667, 1.0, 2.0, 0.5327549306845658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744458.2811267487, 744458.2811267494, 188887.4690280051], 
processed observation next is [0.0, 0.6956521739130435, 0.7172195892575038, 0.5966666666666667, 1.0, 1.0, 0.4370541333548985, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2067939669796524, 0.2067939669796526, 0.2819215955641867], 
reward next is 0.7181, 
noisyNet noise sample is [array([-0.14780194], dtype=float32), -0.8481782]. 
=============================================
[2019-03-27 05:50:51,345] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.7379865e-14 1.0000000e+00 2.0982377e-20 1.0473526e-13 7.0611316e-22], sum to 1.0000
[2019-03-27 05:50:51,352] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5565
[2019-03-27 05:50:51,357] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.85, 63.33333333333334, 1.0, 2.0, 0.520535416863393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727377.194963408, 727377.1949634086, 186876.1747561599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5739000.0000, 
sim time next is 5739600.0000, 
raw observation next is [31.0, 62.66666666666667, 1.0, 2.0, 0.5203874188900699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727170.3172103333, 727170.3172103326, 186852.1414968371], 
processed observation next is [0.0, 0.43478260869565216, 0.6682464454976303, 0.6266666666666667, 1.0, 1.0, 0.42215351673502394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20199175478064815, 0.20199175478064796, 0.27888379327886137], 
reward next is 0.7211, 
noisyNet noise sample is [array([1.0180832], dtype=float32), 0.5863726]. 
=============================================
[2019-03-27 05:50:52,834] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2520761e-13 1.0000000e+00 1.4777427e-20 8.0926341e-14 1.4275186e-23], sum to 1.0000
[2019-03-27 05:50:52,840] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5140
[2019-03-27 05:50:52,846] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 79.0, 1.0, 2.0, 0.5495153097932858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767887.2769922784, 767887.276992279, 191717.473560603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5772600.0000, 
sim time next is 5773200.0000, 
raw observation next is [28.6, 80.0, 1.0, 2.0, 0.5487683742202893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766843.1398199901, 766843.1398199908, 191589.5144488959], 
processed observation next is [0.0, 0.8260869565217391, 0.5545023696682465, 0.8, 1.0, 1.0, 0.4563474388196257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2130119832833306, 0.2130119832833308, 0.2859544991774566], 
reward next is 0.7140, 
noisyNet noise sample is [array([-0.90101236], dtype=float32), -1.2147052]. 
=============================================
[2019-03-27 05:50:52,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8897228e-15 1.0000000e+00 1.2951978e-22 1.0518565e-14 1.5891259e-23], sum to 1.0000
[2019-03-27 05:50:52,855] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3322
[2019-03-27 05:50:52,860] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 79.0, 1.0, 2.0, 0.5495153097932858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767887.2769922784, 767887.276992279, 191717.473560603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5772600.0000, 
sim time next is 5773200.0000, 
raw observation next is [28.6, 80.0, 1.0, 2.0, 0.5487683742202893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766843.1398199901, 766843.1398199908, 191589.5144488959], 
processed observation next is [0.0, 0.8260869565217391, 0.5545023696682465, 0.8, 1.0, 1.0, 0.4563474388196257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2130119832833306, 0.2130119832833308, 0.2859544991774566], 
reward next is 0.7140, 
noisyNet noise sample is [array([-2.3094647], dtype=float32), -0.5368624]. 
=============================================
[2019-03-27 05:50:57,206] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3862907e-05 7.9666322e-01 8.1155189e-09 2.0331299e-01 1.1450670e-09], sum to 1.0000
[2019-03-27 05:50:57,213] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6991
[2019-03-27 05:50:57,221] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2518164.362370619 W.
[2019-03-27 05:50:57,227] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.05, 66.0, 1.0, 2.0, 0.9003106308009736, 1.0, 1.0, 0.9003106308009736, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2518164.362370619, 2518164.362370619, 471663.1918530928], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5830200.0000, 
sim time next is 5830800.0000, 
raw observation next is [32.1, 65.66666666666666, 1.0, 2.0, 0.5794242561700493, 1.0, 2.0, 0.5794242561700493, 1.0, 1.0, 1.006268314437758, 6.9112, 6.9112, 170.5573041426782, 2430885.334879234, 2430885.334879234, 474413.6901851029], 
processed observation next is [1.0, 0.4782608695652174, 0.7203791469194314, 0.6566666666666666, 1.0, 1.0, 0.493282236349457, 1.0, 1.0, 0.493282236349457, 1.0, 0.5, 1.0076442858997048, 0.0, 0.0, 0.8375144448122397, 0.6752459263553428, 0.6752459263553428, 0.7080801346046313], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7105013], dtype=float32), -0.4340865]. 
=============================================
[2019-03-27 05:50:58,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.0503260e-15 1.0000000e+00 9.2464233e-20 1.0412641e-12 1.5193785e-21], sum to 1.0000
[2019-03-27 05:50:58,741] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3262
[2019-03-27 05:50:58,753] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 93.66666666666667, 1.0, 2.0, 0.525196342282297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733892.4519447049, 733892.4519447049, 187638.0130278393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5881800.0000, 
sim time next is 5882400.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.5250008546552274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733619.1895011489, 733619.1895011495, 187605.9281109089], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.94, 1.0, 1.0, 0.4277118730785872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20378310819476358, 0.20378310819476375, 0.2800088479267297], 
reward next is 0.7200, 
noisyNet noise sample is [array([1.6050372], dtype=float32), -0.271302]. 
=============================================
[2019-03-27 05:51:01,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3913016e-04 8.8424969e-01 6.6885342e-08 1.1561108e-01 7.2770097e-09], sum to 1.0000
[2019-03-27 05:51:01,048] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9411
[2019-03-27 05:51:01,056] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.3, 72.0, 1.0, 2.0, 0.6047473436802219, 1.0, 2.0, 0.6047473436802219, 1.0, 1.0, 1.03, 6.933959911558315, 6.9112, 170.5573041426782, 2537232.314817866, 2520928.464354636, 489706.5708744852], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5918400.0000, 
sim time next is 5919000.0000, 
raw observation next is [31.16666666666667, 72.16666666666667, 1.0, 2.0, 0.9016990080360494, 1.0, 2.0, 0.9016990080360494, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2522051.565717897, 2522051.565717897, 472423.1199504047], 
processed observation next is [1.0, 0.5217391304347826, 0.6761453396524489, 0.7216666666666667, 1.0, 1.0, 0.8815650699229511, 1.0, 1.0, 0.8815650699229511, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7005698793660825, 0.7005698793660825, 0.7051091342543353], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21605317], dtype=float32), 0.8426344]. 
=============================================
[2019-03-27 05:51:01,070] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[ 8.620645]
 [ 9.329154]
 [ 8.363645]
 [ 9.38061 ]
 [11.406575]], R is [[9.75296688]
 [9.65543747]
 [9.87190437]
 [9.94485569]
 [9.84540749]].
[2019-03-27 05:51:01,130] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9920883e-07 9.9706405e-01 8.0613190e-11 2.9357299e-03 3.6732241e-12], sum to 1.0000
[2019-03-27 05:51:01,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1239
[2019-03-27 05:51:01,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2172335.489841669 W.
[2019-03-27 05:51:01,147] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.83333333333333, 74.66666666666666, 1.0, 2.0, 0.7767770830520487, 1.0, 1.0, 0.7767770830520487, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2172335.489841669, 2172335.489841669, 408725.0482074147], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5925000.0000, 
sim time next is 5925600.0000, 
raw observation next is [29.7, 75.0, 1.0, 2.0, 0.5497774141999207, 1.0, 2.0, 0.5497774141999207, 1.0, 1.0, 0.9547815542961029, 6.911200000000001, 6.9112, 170.5573041426782, 2306391.791093857, 2306391.791093856, 451274.4493900876], 
processed observation next is [1.0, 0.6086956521739131, 0.6066350710900474, 0.75, 1.0, 1.0, 0.4575631496384586, 1.0, 1.0, 0.4575631496384586, 1.0, 0.5, 0.9448555540196377, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6406643864149603, 0.64066438641496, 0.6735439543135636], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.263704], dtype=float32), 2.569782]. 
=============================================
[2019-03-27 05:51:01,246] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6961133e-05 6.8970656e-01 1.0431303e-08 3.1022635e-01 1.0788406e-08], sum to 1.0000
[2019-03-27 05:51:01,255] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2568
[2019-03-27 05:51:01,262] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2485179.508011102 W.
[2019-03-27 05:51:01,267] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.3, 72.0, 1.0, 2.0, 0.5923529235934463, 1.0, 2.0, 0.5923529235934463, 1.0, 2.0, 1.028721134176547, 6.9112, 6.9112, 170.5573041426782, 2485179.508011102, 2485179.508011102, 484883.0758707085], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5918400.0000, 
sim time next is 5919000.0000, 
raw observation next is [31.16666666666667, 72.16666666666667, 1.0, 2.0, 0.8985765143946066, 1.0, 2.0, 0.8985765143946066, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2513309.169309859, 2513309.169309859, 470719.4053438019], 
processed observation next is [1.0, 0.5217391304347826, 0.6761453396524489, 0.7216666666666667, 1.0, 1.0, 0.8778030293910923, 1.0, 1.0, 0.8778030293910923, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6981414359194054, 0.6981414359194054, 0.7025662766325401], 
reward next is 0.2974, 
noisyNet noise sample is [array([-0.03367994], dtype=float32), -0.43866372]. 
=============================================
[2019-03-27 05:51:01,281] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[ 9.345676]
 [10.007559]
 [11.797031]
 [10.13912 ]
 [ 9.189688]], R is [[11.00107288]
 [11.16735649]
 [11.34074879]
 [11.52737331]
 [11.41209984]].
[2019-03-27 05:51:03,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4326141e-15 1.0000000e+00 4.3706847e-20 1.6643384e-12 2.8701039e-22], sum to 1.0000
[2019-03-27 05:51:03,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4439
[2019-03-27 05:51:03,867] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 86.5, 1.0, 2.0, 0.5343484840969185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746685.8550658324, 746685.855065833, 189152.646901865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6125400.0000, 
sim time next is 6126000.0000, 
raw observation next is [27.26666666666667, 86.66666666666667, 1.0, 2.0, 0.5376333720016498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751277.702518975, 751277.702518975, 189702.2476683482], 
processed observation next is [1.0, 0.9130434782608695, 0.4913112164297, 0.8666666666666667, 1.0, 1.0, 0.44293177349596363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20868825069971528, 0.20868825069971528, 0.28313768308708687], 
reward next is 0.7169, 
noisyNet noise sample is [array([0.93086046], dtype=float32), -2.3185806]. 
=============================================
[2019-03-27 05:51:03,884] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[67.23406 ]
 [67.26102 ]
 [66.903145]
 [67.3233  ]
 [67.68461 ]], R is [[67.10590363]
 [67.15252686]
 [67.19967651]
 [67.24690247]
 [67.29395294]].
[2019-03-27 05:51:05,832] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.7973916e-15 1.0000000e+00 2.7794057e-20 8.1715412e-13 1.3315744e-22], sum to 1.0000
[2019-03-27 05:51:05,838] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1825
[2019-03-27 05:51:05,843] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 88.0, 1.0, 2.0, 0.7225304708573849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1009771.888934351, 1009771.888934351, 225784.9448147022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6161400.0000, 
sim time next is 6162000.0000, 
raw observation next is [27.66666666666666, 87.66666666666667, 1.0, 2.0, 0.8856103043301344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1237816.822793957, 1237816.822793957, 266011.4167593589], 
processed observation next is [1.0, 0.30434782608695654, 0.5102685624012636, 0.8766666666666667, 1.0, 1.0, 0.8621810895543788, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34383800633165473, 0.34383800633165473, 0.397031965312476], 
reward next is 0.6030, 
noisyNet noise sample is [array([-0.6318352], dtype=float32), -0.8900609]. 
=============================================
[2019-03-27 05:51:05,853] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[62.80587 ]
 [62.663544]
 [62.663567]
 [63.24301 ]
 [62.862877]], R is [[62.56544113]
 [62.60279465]
 [62.64391327]
 [62.6950264 ]
 [62.74223709]].
[2019-03-27 05:51:07,743] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8421353e-08 9.9999619e-01 1.3445042e-13 3.8370854e-06 1.4271106e-15], sum to 1.0000
[2019-03-27 05:51:07,751] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9876
[2019-03-27 05:51:07,756] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 71.0, 1.0, 2.0, 0.51840249138015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724395.7064655919, 724395.7064655919, 186531.699575753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6025800.0000, 
sim time next is 6026400.0000, 
raw observation next is [30.0, 72.0, 1.0, 2.0, 0.5288687236323367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739025.9044827132, 739025.9044827132, 188244.2116367532], 
processed observation next is [1.0, 0.782608695652174, 0.6208530805687204, 0.72, 1.0, 1.0, 0.4323719561835382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20528497346742033, 0.20528497346742033, 0.2809615099056018], 
reward next is 0.7190, 
noisyNet noise sample is [array([-1.7382144], dtype=float32), 0.4557616]. 
=============================================
[2019-03-27 05:51:18,148] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7797182e-14 1.0000000e+00 2.1964850e-21 4.0447914e-12 2.0487310e-22], sum to 1.0000
[2019-03-27 05:51:18,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9876
[2019-03-27 05:51:18,163] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 87.0, 1.0, 2.0, 0.5257972362066528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734732.412153903, 734732.4121539037, 187736.7214196131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6213600.0000, 
sim time next is 6214200.0000, 
raw observation next is [26.96666666666667, 87.16666666666667, 1.0, 2.0, 0.5248792185859644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733449.1605173522, 733449.1605173516, 187586.0279355092], 
processed observation next is [1.0, 0.9565217391304348, 0.47709320695102697, 0.8716666666666667, 1.0, 1.0, 0.42756532359754745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2037358779214867, 0.20373587792148656, 0.27997914617240177], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.8687515], dtype=float32), -0.28505325]. 
=============================================
[2019-03-27 05:51:22,342] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9879410e-15 1.0000000e+00 8.6167687e-21 9.0154589e-14 2.9774523e-23], sum to 1.0000
[2019-03-27 05:51:22,351] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2731
[2019-03-27 05:51:22,355] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 71.33333333333334, 1.0, 2.0, 0.5134306512682089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717445.9021108026, 717445.9021108032, 185727.3601446861], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6286800.0000, 
sim time next is 6287400.0000, 
raw observation next is [29.1, 72.5, 1.0, 2.0, 0.5160567709256376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721116.7743134424, 721116.7743134424, 186150.3065310237], 
processed observation next is [0.0, 0.782608695652174, 0.5781990521327015, 0.725, 1.0, 1.0, 0.41693586858510556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20031021508706734, 0.20031021508706734, 0.27783627840451297], 
reward next is 0.7222, 
noisyNet noise sample is [array([1.8904791], dtype=float32), -0.76080817]. 
=============================================
[2019-03-27 05:51:24,300] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2263846e-16 1.0000000e+00 8.3342137e-22 1.2655328e-14 4.8323795e-25], sum to 1.0000
[2019-03-27 05:51:24,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5121
[2019-03-27 05:51:24,323] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 85.0, 1.0, 2.0, 0.5327649415462958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744472.274954185, 744472.2749541856, 188888.6182001234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6302400.0000, 
sim time next is 6303000.0000, 
raw observation next is [27.33333333333333, 85.0, 1.0, 2.0, 0.5317478955359676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743050.5834638949, 743050.5834638949, 188719.5245679506], 
processed observation next is [0.0, 0.9565217391304348, 0.494470774091627, 0.85, 1.0, 1.0, 0.4358408379951416, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2064029398510819, 0.2064029398510819, 0.28167093219097106], 
reward next is 0.7183, 
noisyNet noise sample is [array([-0.2984172], dtype=float32), -0.76917595]. 
=============================================
[2019-03-27 05:51:24,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.01223 ]
 [76.968315]
 [76.908264]
 [76.84774 ]
 [76.78559 ]], R is [[76.99094391]
 [76.93911743]
 [76.88776398]
 [76.83686066]
 [76.78623199]].
[2019-03-27 05:51:24,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0083277e-14 1.0000000e+00 1.0907882e-19 2.1397384e-14 1.0296053e-21], sum to 1.0000
[2019-03-27 05:51:24,483] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2578
[2019-03-27 05:51:24,489] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 87.33333333333334, 1.0, 2.0, 0.5312311069662969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742328.1843169428, 742328.1843169421, 188633.784903191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6319200.0000, 
sim time next is 6319800.0000, 
raw observation next is [26.95, 87.5, 1.0, 2.0, 0.5303874794034517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741148.9103894912, 741148.9103894919, 188493.8891668532], 
processed observation next is [0.0, 0.13043478260869565, 0.476303317535545, 0.875, 1.0, 1.0, 0.43420178241379714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20587469733041425, 0.20587469733041444, 0.28133416293560176], 
reward next is 0.7187, 
noisyNet noise sample is [array([0.5963736], dtype=float32), -0.6180826]. 
=============================================
[2019-03-27 05:51:31,276] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.5705099e-05 9.5010614e-01 2.2440299e-08 4.9858138e-02 2.7878531e-09], sum to 1.0000
[2019-03-27 05:51:31,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1503
[2019-03-27 05:51:31,289] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2232036.379095336 W.
[2019-03-27 05:51:31,296] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.7, 69.66666666666666, 1.0, 2.0, 0.5320690377744147, 1.0, 2.0, 0.5320690377744147, 1.0, 2.0, 0.9143779073983521, 6.911199999999999, 6.9112, 170.5573041426782, 2232036.379095336, 2232036.379095336, 436155.4648929795], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6432600.0000, 
sim time next is 6433200.0000, 
raw observation next is [29.8, 69.0, 1.0, 2.0, 0.5241711380903419, 1.0, 2.0, 0.5241711380903419, 1.0, 2.0, 0.9004614186289378, 6.9112, 6.9112, 170.5573041426782, 2198873.426431967, 2198873.426431967, 430379.9239305064], 
processed observation next is [1.0, 0.4782608695652174, 0.6113744075829385, 0.69, 1.0, 1.0, 0.42671221456667696, 1.0, 1.0, 0.42671221456667696, 1.0, 1.0, 0.8786114861328509, 0.0, 0.0, 0.8375144448122397, 0.6107981740088797, 0.6107981740088797, 0.6423580954186663], 
reward next is 0.3576, 
noisyNet noise sample is [array([-2.534197], dtype=float32), -0.6921695]. 
=============================================
[2019-03-27 05:51:39,455] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 05:51:39,458] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:51:39,459] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:51:39,460] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:51:39,462] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:51:39,462] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:51:39,463] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:51:39,464] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:51:39,465] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:51:39,467] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:51:39,465] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:51:39,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run25
[2019-03-27 05:51:39,485] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run25
[2019-03-27 05:51:39,523] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run25
[2019-03-27 05:51:39,542] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run25
[2019-03-27 05:51:39,543] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run25
[2019-03-27 05:51:48,972] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00476464], dtype=float32), 0.03645867]
[2019-03-27 05:51:48,975] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.03333333333333, 83.33333333333334, 1.0, 2.0, 0.3124614583422478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497383.9207178384, 497383.9207178384, 166884.3432401373]
[2019-03-27 05:51:48,976] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:51:48,978] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8288717e-14 1.0000000e+00 2.0116684e-20 1.7910719e-13 1.6003872e-22], sampled 0.07910946644558059
[2019-03-27 05:51:51,055] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00476464], dtype=float32), 0.03645867]
[2019-03-27 05:51:51,058] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.96666666666667, 72.33333333333334, 1.0, 2.0, 0.3647459612797441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562950.9992646246, 562950.9992646246, 171769.1324014423]
[2019-03-27 05:51:51,058] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:51:51,061] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.9715477e-14 1.0000000e+00 1.5941138e-19 5.3092058e-13 1.7368271e-21], sampled 0.850584658169472
[2019-03-27 05:52:56,843] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00476464], dtype=float32), 0.03645867]
[2019-03-27 05:52:56,844] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.8, 73.0, 1.0, 2.0, 0.6186967544142697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864600.0922715691, 864600.0922715691, 204280.8723759376]
[2019-03-27 05:52:56,845] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:52:56,849] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5474359e-11 1.0000000e+00 1.5702821e-16 9.3933039e-09 1.3832010e-18], sampled 0.8704686414229329
[2019-03-27 05:53:21,046] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00476464], dtype=float32), 0.03645867]
[2019-03-27 05:53:21,046] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.56666666666667, 70.5, 1.0, 2.0, 0.5385112144212029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752504.8157832186, 752504.8157832192, 189848.8487382196]
[2019-03-27 05:53:21,050] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:53:21,053] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2559184e-14 1.0000000e+00 2.0403021e-20 2.2822505e-13 1.4894504e-22], sampled 0.040731775390373315
[2019-03-27 05:53:23,464] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00476464], dtype=float32), 0.03645867]
[2019-03-27 05:53:23,464] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.91666666666667, 68.66666666666667, 1.0, 2.0, 0.720884456803472, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976587514597, 6.9112, 168.9123160369624, 1904353.94599533, 1837116.427802014, 389255.8016988889]
[2019-03-27 05:53:23,468] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:53:23,476] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6539784e-08 9.9998009e-01 2.3815620e-12 1.9921667e-05 5.3743028e-14], sampled 0.2678175987013426
[2019-03-27 05:53:23,477] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1904353.94599533 W.
[2019-03-27 05:53:32,664] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7386 2842482880.9472 1130.0000
[2019-03-27 05:53:33,580] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.7336 2927490999.0485 1338.0000
[2019-03-27 05:53:33,684] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8889 2779256332.3854 933.0000
[2019-03-27 05:53:33,833] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.8045 3007699122.3419 1766.0000
[2019-03-27 05:53:33,873] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.7062 3164036501.6753 1776.0000
[2019-03-27 05:53:34,889] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 600000, evaluation results [600000.0, 7884.706178877507, 3164036501.6752567, 1776.0, 8252.733593483104, 2927490999.0485487, 1338.0, 8659.888853383243, 2779256332.3854055, 933.0, 7996.804512416601, 3007699122.3418503, 1766.0, 8496.73859717199, 2842482880.947204, 1130.0]
[2019-03-27 05:53:35,497] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5500642e-13 1.0000000e+00 3.5629394e-19 1.2829111e-12 9.6925615e-22], sum to 1.0000
[2019-03-27 05:53:35,508] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5131
[2019-03-27 05:53:35,513] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 89.0, 1.0, 2.0, 0.5195887127387313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726053.8530634192, 726053.8530634185, 186722.0346897692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6570000.0000, 
sim time next is 6570600.0000, 
raw observation next is [26.45, 89.16666666666667, 1.0, 2.0, 0.519268848784936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725606.734466834, 725606.7344668333, 186670.0259647582], 
processed observation next is [1.0, 0.043478260869565216, 0.45260663507109006, 0.8916666666666667, 1.0, 1.0, 0.4208058419095614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2015574262407872, 0.201557426240787, 0.27861197905187796], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.03794397], dtype=float32), -1.2894849]. 
=============================================
[2019-03-27 05:53:37,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3596799e-14 1.0000000e+00 2.7658795e-19 7.5242113e-14 1.9789302e-22], sum to 1.0000
[2019-03-27 05:53:37,700] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8638
[2019-03-27 05:53:37,705] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 78.66666666666667, 1.0, 2.0, 0.3776137079235777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570824.3978629658, 570824.3978629658, 172123.2898209765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6853200.0000, 
sim time next is 6853800.0000, 
raw observation next is [24.8, 78.33333333333333, 1.0, 2.0, 0.3801581559053236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573762.7834650266, 573762.7834650266, 172355.1561369583], 
processed observation next is [0.0, 0.30434782608695654, 0.3744075829383887, 0.7833333333333333, 1.0, 1.0, 0.2532025974762935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15937855096250741, 0.15937855096250741, 0.2572465016969527], 
reward next is 0.7428, 
noisyNet noise sample is [array([0.5875439], dtype=float32), -1.2315218]. 
=============================================
[2019-03-27 05:53:38,268] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5690255e-15 1.0000000e+00 2.3901174e-21 1.2766986e-12 2.5750310e-23], sum to 1.0000
[2019-03-27 05:53:38,276] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1417
[2019-03-27 05:53:38,281] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333334, 86.33333333333334, 1.0, 2.0, 0.5116595717994673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714970.2389922425, 714970.2389922425, 185443.1247821274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6643200.0000, 
sim time next is 6643800.0000, 
raw observation next is [26.7, 86.5, 1.0, 2.0, 0.5123973425287498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716001.5143104362, 716001.5143104362, 185561.2818871424], 
processed observation next is [1.0, 0.9130434782608695, 0.46445497630331756, 0.865, 1.0, 1.0, 0.41252691870933705, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1988893095306767, 0.1988893095306767, 0.27695713714498865], 
reward next is 0.7230, 
noisyNet noise sample is [array([-1.7337793], dtype=float32), -2.3932483]. 
=============================================
[2019-03-27 05:53:38,638] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4265515e-13 1.0000000e+00 1.5096747e-18 2.1273382e-11 4.2704455e-21], sum to 1.0000
[2019-03-27 05:53:38,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0080
[2019-03-27 05:53:38,650] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 59.5, 1.0, 2.0, 0.3197452436669642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504924.4537710003, 504924.453771001, 167382.0851305817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6813000.0000, 
sim time next is 6813600.0000, 
raw observation next is [26.03333333333333, 60.33333333333333, 1.0, 2.0, 0.3230952826349026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509817.3087269706, 509817.3087269706, 167745.9518597321], 
processed observation next is [1.0, 0.8695652173913043, 0.4328593996840442, 0.6033333333333333, 1.0, 1.0, 0.1844521477528947, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14161591909082516, 0.14161591909082516, 0.25036709232795834], 
reward next is 0.7496, 
noisyNet noise sample is [array([0.8933749], dtype=float32), 0.35385197]. 
=============================================
[2019-03-27 05:53:41,530] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7513801e-08 9.9999881e-01 3.0868461e-12 1.2400886e-06 4.5967754e-15], sum to 1.0000
[2019-03-27 05:53:41,536] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1113
[2019-03-27 05:53:41,547] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1914980.065368223 W.
[2019-03-27 05:53:41,552] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.2, 78.0, 1.0, 2.0, 0.7284778007675782, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.980107970856989, 6.9112, 168.9125462398009, 1914980.065368223, 1866094.4979508, 391864.8365506223], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6688800.0000, 
sim time next is 6689400.0000, 
raw observation next is [28.36666666666667, 77.0, 1.0, 2.0, 0.6099979831775487, 1.0, 1.0, 0.6099979831775487, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1705549.660535335, 1705549.660535335, 337970.4434052265], 
processed observation next is [1.0, 0.43478260869565216, 0.543443917851501, 0.77, 1.0, 1.0, 0.530118052021143, 1.0, 0.5, 0.530118052021143, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.47376379459314866, 0.47376379459314866, 0.5044334976197411], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.378587], dtype=float32), 0.21587667]. 
=============================================
[2019-03-27 05:53:41,746] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1587235e-06 9.9690670e-01 1.1923059e-08 3.0921702e-03 7.0974761e-11], sum to 1.0000
[2019-03-27 05:53:41,757] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6917
[2019-03-27 05:53:41,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2094906.114493115 W.
[2019-03-27 05:53:41,773] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.76666666666667, 65.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.814357413442604, 6.9112, 168.9080341069787, 2094906.114493115, 1454193.833079432, 311348.0189322581], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6698400.0000, 
sim time next is 6699000.0000, 
raw observation next is [29.83333333333333, 64.66666666666667, 1.0, 2.0, 0.6447495045856074, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.967274786559999, 6.9112, 168.9118878312429, 1797816.618001409, 1758035.486159558, 374404.0109132798], 
processed observation next is [1.0, 0.5217391304347826, 0.6129541864139019, 0.6466666666666667, 1.0, 1.0, 0.5719873549224186, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.005607478655999909, 0.0, 0.8294346974482456, 0.4993935050003914, 0.4883431905998772, 0.5588119565869848], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.84574777], dtype=float32), 1.1717381]. 
=============================================
[2019-03-27 05:53:41,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[22.19602 ]
 [18.161829]
 [18.864328]
 [17.73166 ]
 [17.519466]], R is [[22.77837944]
 [22.55059624]
 [22.32509041]
 [22.10183907]
 [21.88287735]].
[2019-03-27 05:53:53,761] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4020253e-14 1.0000000e+00 1.5811638e-21 1.0879278e-14 1.3882153e-21], sum to 1.0000
[2019-03-27 05:53:53,769] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3001
[2019-03-27 05:53:53,775] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.23333333333333, 70.66666666666667, 1.0, 2.0, 0.3902538720148053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584690.2530060142, 584690.2530060142, 173205.4094153247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6902400.0000, 
sim time next is 6903000.0000, 
raw observation next is [26.15, 71.5, 1.0, 2.0, 0.3928187320567826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587529.9346267583, 587529.9346267583, 173433.5319510599], 
processed observation next is [0.0, 0.9130434782608695, 0.43838862559241704, 0.715, 1.0, 1.0, 0.2684563036828706, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16320275961854397, 0.16320275961854397, 0.25885601783740286], 
reward next is 0.7411, 
noisyNet noise sample is [array([0.16395468], dtype=float32), -1.7742094]. 
=============================================
[2019-03-27 05:53:53,792] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[81.07423 ]
 [80.98793 ]
 [80.897255]
 [80.793976]
 [80.72025 ]], R is [[81.08669281]
 [81.0173111 ]
 [80.94886017]
 [80.88137817]
 [80.81495667]].
[2019-03-27 05:54:02,033] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.9636349e-05 8.5091192e-01 2.9291645e-08 1.4906840e-01 6.6401634e-10], sum to 1.0000
[2019-03-27 05:54:02,038] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1535
[2019-03-27 05:54:02,045] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1885475.189296861 W.
[2019-03-27 05:54:02,048] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.05, 66.0, 1.0, 2.0, 0.4495284271163023, 1.0, 2.0, 0.4495284271163023, 1.0, 1.0, 0.7577692159454887, 6.9112, 6.9112, 170.5573041426782, 1885475.189296861, 1885475.189296861, 378246.4628365557], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7057800.0000, 
sim time next is 7058400.0000, 
raw observation next is [28.83333333333333, 67.66666666666666, 1.0, 2.0, 0.4458544157142644, 1.0, 2.0, 0.4458544157142644, 1.0, 2.0, 0.7526527601169964, 6.9112, 6.9112, 170.5573041426782, 1870051.687174727, 1870051.687174727, 376216.1602446036], 
processed observation next is [1.0, 0.6956521739130435, 0.5655608214849919, 0.6766666666666665, 1.0, 1.0, 0.3323547177280294, 1.0, 1.0, 0.3323547177280294, 1.0, 1.0, 0.6983570245329224, 0.0, 0.0, 0.8375144448122397, 0.5194588019929797, 0.5194588019929797, 0.5615166570814979], 
reward next is 0.4385, 
noisyNet noise sample is [array([-1.06127], dtype=float32), 1.280626]. 
=============================================
[2019-03-27 05:54:05,633] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.57319950e-08 9.99971032e-01 1.52266411e-11 2.90224616e-05
 1.05492465e-13], sum to 1.0000
[2019-03-27 05:54:05,642] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4633
[2019-03-27 05:54:05,645] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 83.16666666666667, 1.0, 2.0, 0.5868321658514047, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9839117225790065, 6.911200000000001, 6.9112, 168.912956510431, 1640740.856834042, 1640740.856834042, 351456.0554707569], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7139400.0000, 
sim time next is 7140000.0000, 
raw observation next is [26.33333333333334, 83.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.936410452597335, 6.9112, 168.9072548346165, 2181548.19734758, 1454253.168008504, 311347.0762606848], 
processed observation next is [1.0, 0.6521739130434783, 0.44707740916271754, 0.8333333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.10252104525973352, 0.0, 0.8294119473138151, 0.6059856103743277, 0.40395921333569557, 0.46469712874729074], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9796755], dtype=float32), -0.62416613]. 
=============================================
[2019-03-27 05:54:05,660] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[31.163462]
 [32.367775]
 [35.891857]
 [39.7953  ]
 [44.085697]], R is [[34.57183456]
 [34.70155334]
 [34.80134583]
 [34.89873505]
 [34.99378586]].
[2019-03-27 05:54:05,960] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0350332e-08 9.9994540e-01 8.7430246e-10 5.4611090e-05 2.3913760e-11], sum to 1.0000
[2019-03-27 05:54:05,969] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3357
[2019-03-27 05:54:05,976] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.48333333333333, 67.66666666666667, 1.0, 2.0, 0.3895259614990286, 1.0, 1.0, 0.3895259614990286, 1.0, 2.0, 0.6537033585108781, 6.9112, 6.9112, 170.5573041426782, 1633612.591642379, 1633612.591642379, 343988.6347634977], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7125000.0000, 
sim time next is 7125600.0000, 
raw observation next is [28.36666666666667, 68.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 8.388848112843158, 6.9112, 168.9045541991439, 2502684.937706968, 1454442.655502452, 310706.9516465974], 
processed observation next is [1.0, 0.4782608695652174, 0.543443917851501, 0.6833333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.14776481128431582, 0.0, 0.8293986859572882, 0.6951902604741578, 0.40401184875068114, 0.4637417188755185], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.43395248], dtype=float32), 0.8534714]. 
=============================================
[2019-03-27 05:54:16,002] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2863286e-13 1.0000000e+00 2.4664419e-19 3.4836690e-13 2.7498856e-21], sum to 1.0000
[2019-03-27 05:54:16,012] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1712
[2019-03-27 05:54:16,018] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 65.0, 1.0, 2.0, 0.7596995043752693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1155855.917024349, 1155855.917024349, 246575.1284569858], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7299000.0000, 
sim time next is 7299600.0000, 
raw observation next is [26.73333333333333, 64.33333333333334, 1.0, 2.0, 0.7743874539833375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1179373.118487598, 1179373.118487598, 250502.7218039786], 
processed observation next is [1.0, 0.4782608695652174, 0.4660347551342811, 0.6433333333333334, 1.0, 1.0, 0.7281776554016115, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32760364402433273, 0.32760364402433273, 0.3738846594089233], 
reward next is 0.6261, 
noisyNet noise sample is [array([-0.5943863], dtype=float32), -0.4074513]. 
=============================================
[2019-03-27 05:54:17,972] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7650215e-14 1.0000000e+00 2.9870160e-18 2.4887984e-12 1.2395226e-21], sum to 1.0000
[2019-03-27 05:54:17,980] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0113
[2019-03-27 05:54:17,983] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 63.0, 1.0, 2.0, 0.8965846925747636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1345635.751955853, 1345635.751955853, 282170.7008800929], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7317000.0000, 
sim time next is 7317600.0000, 
raw observation next is [27.46666666666667, 63.33333333333333, 1.0, 2.0, 0.9239121924335195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1385454.020382266, 1385454.020382266, 290211.8001243837], 
processed observation next is [1.0, 0.6956521739130435, 0.500789889415482, 0.6333333333333333, 1.0, 1.0, 0.9083279426909874, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.38484833899507387, 0.38484833899507387, 0.4331519404841548], 
reward next is 0.5668, 
noisyNet noise sample is [array([1.4965565], dtype=float32), 0.9964024]. 
=============================================
[2019-03-27 05:54:19,042] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0154117e-13 1.0000000e+00 4.1182075e-20 3.0793900e-14 9.2257583e-24], sum to 1.0000
[2019-03-27 05:54:19,049] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5092
[2019-03-27 05:54:19,054] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.23333333333333, 81.0, 1.0, 2.0, 0.3772160127202847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 572707.964964087, 572707.9649640864, 172363.0587321109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7357200.0000, 
sim time next is 7357800.0000, 
raw observation next is [24.16666666666667, 82.0, 1.0, 2.0, 0.3771318744214461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571243.9689907632, 571243.9689907638, 172194.9392325905], 
processed observation next is [1.0, 0.13043478260869565, 0.34439178515007923, 0.82, 1.0, 1.0, 0.24955647520656155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.158678880275212, 0.15867888027521215, 0.257007371988941], 
reward next is 0.7430, 
noisyNet noise sample is [array([-0.25728598], dtype=float32), 2.263109]. 
=============================================
[2019-03-27 05:54:19,984] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.3770373e-14 1.0000000e+00 7.2899500e-21 4.5257499e-14 7.5569876e-22], sum to 1.0000
[2019-03-27 05:54:19,995] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4780
[2019-03-27 05:54:20,000] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.68333333333333, 88.0, 1.0, 2.0, 0.4133061746971389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619006.4355631499, 619006.4355631499, 176363.4572957827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7361400.0000, 
sim time next is 7362000.0000, 
raw observation next is [23.6, 89.0, 1.0, 2.0, 0.417661102547572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 624482.1407145852, 624482.1407145846, 176857.0954445016], 
processed observation next is [1.0, 0.21739130434782608, 0.3175355450236968, 0.89, 1.0, 1.0, 0.29838687053924334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.173467261309607, 0.17346726130960685, 0.26396581409627107], 
reward next is 0.7360, 
noisyNet noise sample is [array([0.729852], dtype=float32), 2.0155702]. 
=============================================
[2019-03-27 05:54:20,016] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6553106e-13 1.0000000e+00 6.6977066e-20 1.1247110e-13 2.4001928e-23], sum to 1.0000
[2019-03-27 05:54:20,018] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.551  ]
 [72.53116]
 [72.60437]
 [72.73479]
 [72.88412]], R is [[72.74531555]
 [72.75463104]
 [72.76251221]
 [72.76869202]
 [72.77516937]].
[2019-03-27 05:54:20,025] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8859
[2019-03-27 05:54:20,030] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.91666666666667, 72.83333333333333, 1.0, 2.0, 0.5686544204305177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 876473.0143512196, 876473.0143512203, 204890.7498148936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7351800.0000, 
sim time next is 7352400.0000, 
raw observation next is [24.83333333333334, 73.66666666666667, 1.0, 2.0, 0.4606569003450017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709042.0398696957, 709042.0398696951, 185532.1371341849], 
processed observation next is [1.0, 0.08695652173913043, 0.3759873617693526, 0.7366666666666667, 1.0, 1.0, 0.350189036560243, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19695612218602657, 0.1969561221860264, 0.2769136375137088], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.03616812], dtype=float32), -2.8616924]. 
=============================================
[2019-03-27 05:54:23,382] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7511232e-17 1.0000000e+00 1.0658681e-22 1.6887655e-16 2.4116547e-24], sum to 1.0000
[2019-03-27 05:54:23,394] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8506
[2019-03-27 05:54:23,400] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.36666666666667, 91.33333333333334, 1.0, 2.0, 0.3131527610906916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494711.8894483713, 494711.8894483713, 166621.4210507116], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7438800.0000, 
sim time next is 7439400.0000, 
raw observation next is [21.35, 91.5, 1.0, 2.0, 0.3133050122001033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 494907.9830848176, 494907.983084817, 166635.0146898113], 
processed observation next is [0.0, 0.08695652173913043, 0.2109004739336494, 0.915, 1.0, 1.0, 0.17265664120494376, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13747443974578266, 0.1374744397457825, 0.24870897714897208], 
reward next is 0.7513, 
noisyNet noise sample is [array([-0.03260848], dtype=float32), 0.5392483]. 
=============================================
[2019-03-27 05:54:27,340] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2821860e-16 1.0000000e+00 2.2611032e-23 7.2709136e-17 4.6203657e-26], sum to 1.0000
[2019-03-27 05:54:27,354] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3259
[2019-03-27 05:54:27,362] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 87.5, 1.0, 2.0, 0.4043904469570637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597081.3529848715, 597081.3529848715, 174081.5188522395], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7506600.0000, 
sim time next is 7507200.0000, 
raw observation next is [24.1, 88.0, 1.0, 2.0, 0.4038394038054958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595894.2537914377, 595894.2537914377, 173960.5580025301], 
processed observation next is [0.0, 0.9130434782608695, 0.3412322274881518, 0.88, 1.0, 1.0, 0.2817342214524046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1655261816087327, 0.1655261816087327, 0.2596426238843733], 
reward next is 0.7404, 
noisyNet noise sample is [array([1.7470053], dtype=float32), 0.96549475]. 
=============================================
[2019-03-27 05:54:28,644] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 05:54:28,646] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:54:28,647] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:54:28,648] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:54:28,649] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:54:28,648] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:54:28,649] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:54:28,650] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:54:28,651] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:54:28,652] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:54:28,654] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:54:28,677] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run26
[2019-03-27 05:54:28,699] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run26
[2019-03-27 05:54:28,717] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run26
[2019-03-27 05:54:28,717] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run26
[2019-03-27 05:54:28,718] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run26
[2019-03-27 05:54:30,338] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00485928], dtype=float32), 0.03734594]
[2019-03-27 05:54:30,340] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.90055817, 68.42460062, 1.0, 2.0, 0.7158635090919426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1065644.431653947, 1065644.431653946, 232849.7089429383]
[2019-03-27 05:54:30,341] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:54:30,343] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.4187437e-15 1.0000000e+00 1.3875859e-20 1.5207089e-14 4.2188862e-23], sampled 0.4129474428005032
[2019-03-27 05:54:50,577] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00485928], dtype=float32), 0.03734594]
[2019-03-27 05:54:50,579] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.2, 95.33333333333334, 1.0, 2.0, 0.366633294487637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558481.7451203557, 558481.7451203557, 171183.0095961729]
[2019-03-27 05:54:50,581] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:54:50,582] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7365129e-16 1.0000000e+00 5.3292292e-22 9.4692905e-16 1.0545791e-24], sampled 0.6862847354565665
[2019-03-27 05:55:09,469] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00485928], dtype=float32), 0.03734594]
[2019-03-27 05:55:09,470] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.4, 73.0, 1.0, 2.0, 0.5724287479168738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799918.3461369403, 799918.3461369403, 195721.415756616]
[2019-03-27 05:55:09,470] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:55:09,473] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.2030885e-16 1.0000000e+00 6.4220028e-22 2.5106550e-15 1.1633608e-24], sampled 0.7467588396920104
[2019-03-27 05:55:21,782] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00485928], dtype=float32), 0.03734594]
[2019-03-27 05:55:21,783] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.3, 52.0, 1.0, 2.0, 0.6370276148273246, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997787927936605, 6.9112, 168.9123599699173, 1781201.530346182, 1719773.291084939, 371599.2305989832]
[2019-03-27 05:55:21,784] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:55:21,788] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4643418e-08 9.9999321e-01 4.6735823e-12 6.7887686e-06 4.5396988e-14], sampled 0.7955806497357136
[2019-03-27 05:55:21,790] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1781201.530346182 W.
[2019-03-27 05:55:23,889] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00485928], dtype=float32), 0.03734594]
[2019-03-27 05:55:23,890] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.46666666666667, 64.0, 1.0, 2.0, 0.949092938258631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104307, 1326601.885043646, 1326601.885043647, 283799.5772733256]
[2019-03-27 05:55:23,890] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:55:23,891] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.1684073e-14 1.0000000e+00 2.4904087e-19 5.3819725e-13 8.7709751e-22], sampled 0.14288984828963336
[2019-03-27 05:55:27,132] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00485928], dtype=float32), 0.03734594]
[2019-03-27 05:55:27,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.30401569, 80.20563199, 1.0, 2.0, 0.956079278886825, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991008717571, 6.9112, 168.9123159581947, 2233528.523604251, 2166280.774583358, 450085.4265533318]
[2019-03-27 05:55:27,138] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:55:27,142] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1039008e-07 9.9991190e-01 4.0119835e-11 8.7947185e-05 1.9583254e-12], sampled 0.24267208798513784
[2019-03-27 05:55:27,143] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2233528.523604251 W.
[2019-03-27 05:55:44,098] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00485928], dtype=float32), 0.03734594]
[2019-03-27 05:55:44,099] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.98161635166667, 70.97975310166666, 1.0, 2.0, 0.609546910091163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 851808.4807545642, 851808.4807545636, 202538.113002827]
[2019-03-27 05:55:44,104] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:55:44,108] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1870810e-15 1.0000000e+00 1.1078631e-21 3.5869622e-15 2.0896306e-24], sampled 0.6772998907902441
[2019-03-27 05:55:58,149] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00485928], dtype=float32), 0.03734594]
[2019-03-27 05:55:58,150] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.76098505833333, 60.530971635, 1.0, 2.0, 0.5933635835726637, 0.0, 2.0, 0.0, 1.0, 1.0, 1.012448127152387, 6.911200000000001, 6.9112, 168.9124356731488, 1659016.511468141, 1659016.51146814, 359479.8342600444]
[2019-03-27 05:55:58,153] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:55:58,155] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.6089336e-13 1.0000000e+00 1.0319151e-18 7.2015493e-12 2.2123953e-21], sampled 0.0824409055519999
[2019-03-27 05:55:58,156] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1659016.511468141 W.
[2019-03-27 05:56:16,299] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00485928], dtype=float32), 0.03734594]
[2019-03-27 05:56:16,299] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.13383861666667, 91.79203401, 1.0, 2.0, 0.3601496558632313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 555394.0334475139, 555394.0334475145, 171115.5002338059]
[2019-03-27 05:56:16,300] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:56:16,303] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.2401253e-15 1.0000000e+00 1.4629911e-20 7.7338047e-15 5.5540471e-23], sampled 0.9749710107235824
[2019-03-27 05:56:23,436] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.7173 3164023725.6911 1776.0000
[2019-03-27 05:56:23,448] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.6550 2779351946.6178 933.0000
[2019-03-27 05:56:23,609] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6748 2842558682.6638 1131.0000
[2019-03-27 05:56:23,686] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2161 2927390796.4298 1338.0000
[2019-03-27 05:56:23,705] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5359 3007679224.9830 1766.0000
[2019-03-27 05:56:24,721] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 625000, evaluation results [625000.0, 7881.717250507676, 3164023725.6911316, 1776.0, 8254.216109707997, 2927390796.4297576, 1338.0, 8657.654983724233, 2779351946.617828, 933.0, 7997.535892921484, 3007679224.9830427, 1766.0, 8496.674815541954, 2842558682.6637573, 1131.0]
[2019-03-27 05:56:26,520] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.37022603e-15 1.00000000e+00 1.32623766e-20 1.59853201e-15
 3.17372815e-23], sum to 1.0000
[2019-03-27 05:56:26,530] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5191
[2019-03-27 05:56:26,538] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 65.0, 1.0, 2.0, 0.4609718946588133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 646762.4357191233, 646762.4357191239, 178049.3615695616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7561200.0000, 
sim time next is 7561800.0000, 
raw observation next is [29.0, 64.5, 1.0, 2.0, 0.4564778505367695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 642895.3599905617, 642895.3599905611, 177712.4757013884], 
processed observation next is [0.0, 0.5217391304347826, 0.5734597156398105, 0.645, 1.0, 1.0, 0.3451540367912885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1785820444418227, 0.17858204444182255, 0.2652425010468484], 
reward next is 0.7348, 
noisyNet noise sample is [array([0.5727616], dtype=float32), -1.2129731]. 
=============================================
[2019-03-27 05:56:30,445] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4760142e-16 1.0000000e+00 1.3914752e-22 3.4465056e-17 2.1330397e-25], sum to 1.0000
[2019-03-27 05:56:30,457] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0638
[2019-03-27 05:56:30,461] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.26666666666667, 91.0, 1.0, 2.0, 0.4819794068826063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673483.3545946846, 673483.3545946846, 180820.6963287197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7597200.0000, 
sim time next is 7597800.0000, 
raw observation next is [25.2, 91.5, 1.0, 2.0, 0.4819867045926162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673493.5551222712, 673493.5551222712, 180821.801901838], 
processed observation next is [0.0, 0.9565217391304348, 0.3933649289099526, 0.915, 1.0, 1.0, 0.3758875958947184, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18708154308951977, 0.18708154308951977, 0.26988328642065373], 
reward next is 0.7301, 
noisyNet noise sample is [array([0.5208256], dtype=float32), -0.075277045]. 
=============================================
[2019-03-27 05:56:32,603] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7814164e-12 1.0000000e+00 3.2263988e-17 1.8229063e-10 4.5028435e-19], sum to 1.0000
[2019-03-27 05:56:32,614] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5818
[2019-03-27 05:56:32,619] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 80.66666666666667, 1.0, 2.0, 0.5100794505965144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712761.5074086621, 712761.5074086614, 185190.4575802646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7669200.0000, 
sim time next is 7669800.0000, 
raw observation next is [27.4, 81.5, 1.0, 2.0, 0.5092657754887085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711624.1344809077, 711624.1344809083, 185060.6776000005], 
processed observation next is [1.0, 0.782608695652174, 0.4976303317535545, 0.815, 1.0, 1.0, 0.40875394637193796, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19767337068914104, 0.19767337068914118, 0.2762099665671649], 
reward next is 0.7238, 
noisyNet noise sample is [array([-2.4307985], dtype=float32), 2.1134439]. 
=============================================
[2019-03-27 05:56:36,795] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.3643215e-07 9.9982661e-01 1.8472154e-09 1.7256713e-04 5.1969752e-12], sum to 1.0000
[2019-03-27 05:56:36,810] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9237
[2019-03-27 05:56:36,818] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2075828.258265712 W.
[2019-03-27 05:56:36,825] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.9, 71.0, 1.0, 2.0, 0.7423017266109353, 1.0, 2.0, 0.7423017266109353, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2075828.258265712, 2075828.258265711, 392757.293668338], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7725600.0000, 
sim time next is 7726200.0000, 
raw observation next is [30.1, 70.0, 1.0, 2.0, 0.476653753177723, 1.0, 2.0, 0.476653753177723, 1.0, 1.0, 0.822969318494487, 6.9112, 6.9112, 170.5573041426782, 1999354.196299893, 1999354.196299893, 398551.9800410497], 
processed observation next is [1.0, 0.43478260869565216, 0.6255924170616115, 0.7, 1.0, 1.0, 0.3694623532261723, 1.0, 1.0, 0.3694623532261723, 1.0, 0.5, 0.7841089249932767, 0.0, 0.0, 0.8375144448122397, 0.5553761656388592, 0.5553761656388592, 0.5948537015538056], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5913612], dtype=float32), -0.0050791306]. 
=============================================
[2019-03-27 05:56:38,021] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:38,022] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:38,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-03-27 05:56:38,074] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3970074e-05 7.9239976e-01 1.2807087e-07 2.0758615e-01 4.0188708e-10], sum to 1.0000
[2019-03-27 05:56:38,083] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1254
[2019-03-27 05:56:38,093] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2208019.929255093 W.
[2019-03-27 05:56:38,099] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.7, 61.66666666666667, 1.0, 2.0, 0.7895238776580027, 1.0, 2.0, 0.7895238776580027, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2208019.929255093, 2208019.929255092, 414794.1687265788], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7748400.0000, 
sim time next is 7749000.0000, 
raw observation next is [30.6, 62.0, 1.0, 2.0, 0.5143961468329438, 1.0, 2.0, 0.5143961468329438, 1.0, 1.0, 0.8770427624560607, 6.9112, 6.9112, 170.5573041426782, 2157826.491609579, 2157826.491609579, 422173.3595630797], 
processed observation next is [1.0, 0.6956521739130435, 0.6492890995260664, 0.62, 1.0, 1.0, 0.41493511666619726, 1.0, 1.0, 0.41493511666619726, 1.0, 0.5, 0.8500521493366593, 0.0, 0.0, 0.8375144448122397, 0.5993962476693275, 0.5993962476693275, 0.6301094918851936], 
reward next is 0.3699, 
noisyNet noise sample is [array([-0.05535004], dtype=float32), 1.6016283]. 
=============================================
[2019-03-27 05:56:38,115] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[17.243952]
 [15.360331]
 [13.323243]
 [12.06963 ]
 [14.072517]], R is [[15.88623047]
 [16.10827255]
 [16.28874207]
 [16.47529984]
 [16.62976837]].
[2019-03-27 05:56:38,773] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9122975e-14 1.0000000e+00 2.5609005e-19 1.8656422e-12 6.9751372e-23], sum to 1.0000
[2019-03-27 05:56:38,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6667
[2019-03-27 05:56:38,783] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 85.66666666666667, 1.0, 2.0, 0.5195943645146639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726061.7533430048, 726061.7533430048, 186723.2850285625], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7762800.0000, 
sim time next is 7763400.0000, 
raw observation next is [27.05, 86.0, 1.0, 2.0, 0.5204531958061286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727262.2629279441, 727262.2629279447, 186862.8848569513], 
processed observation next is [1.0, 0.8695652173913043, 0.4810426540284361, 0.86, 1.0, 1.0, 0.4222327660314802, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20201729525776224, 0.2020172952577624, 0.27889982814470343], 
reward next is 0.7211, 
noisyNet noise sample is [array([0.03908943], dtype=float32), 0.0870993]. 
=============================================
[2019-03-27 05:56:39,638] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7054170e-15 1.0000000e+00 1.1404831e-20 1.5260589e-13 2.7911116e-22], sum to 1.0000
[2019-03-27 05:56:39,642] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0269
[2019-03-27 05:56:39,647] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666666, 86.0, 1.0, 2.0, 0.6004791451257872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839131.7693257811, 839131.7693257811, 200828.6099888635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7800000.0000, 
sim time next is 7800600.0000, 
raw observation next is [26.78333333333333, 85.5, 1.0, 2.0, 0.6063062478881444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 847278.026195525, 847278.0261955243, 201919.7704292652], 
processed observation next is [1.0, 0.2608695652173913, 0.46840442338072663, 0.855, 1.0, 1.0, 0.5256701781784872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23535500727653472, 0.23535500727653452, 0.30137279168547043], 
reward next is 0.6986, 
noisyNet noise sample is [array([0.36393902], dtype=float32), 0.23774487]. 
=============================================
[2019-03-27 05:56:40,562] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:40,564] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:40,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-03-27 05:56:44,063] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.7365672e-14 1.0000000e+00 1.6784869e-19 6.3602804e-12 2.2436110e-22], sum to 1.0000
[2019-03-27 05:56:44,072] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1932
[2019-03-27 05:56:44,077] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333334, 90.66666666666667, 1.0, 2.0, 0.5084106517669695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710428.8250831377, 710428.8250831377, 184924.6590287534], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7865400.0000, 
sim time next is 7866000.0000, 
raw observation next is [26.1, 91.0, 1.0, 2.0, 0.508843291642284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711033.5777661896, 711033.5777661896, 184993.5532540771], 
processed observation next is [1.0, 0.043478260869565216, 0.4360189573459717, 0.91, 1.0, 1.0, 0.4082449296894988, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19750932715727487, 0.19750932715727487, 0.27610978097623445], 
reward next is 0.7239, 
noisyNet noise sample is [array([0.92741454], dtype=float32), 0.7594994]. 
=============================================
[2019-03-27 05:56:44,096] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[66.98072 ]
 [67.30076 ]
 [67.722466]
 [68.171104]
 [68.55039 ]], R is [[66.8500824 ]
 [66.90557098]
 [66.96060944]
 [67.01519012]
 [67.06931305]].
[2019-03-27 05:56:45,030] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.77491236e-12 1.00000000e+00 1.43825124e-19 1.44420855e-11
 2.29008143e-21], sum to 1.0000
[2019-03-27 05:56:45,036] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9679
[2019-03-27 05:56:45,042] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 87.33333333333333, 1.0, 2.0, 0.6027447994734746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842299.1339300844, 842299.1339300844, 201251.8063687376], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7883400.0000, 
sim time next is 7884000.0000, 
raw observation next is [26.6, 87.0, 1.0, 2.0, 0.5981795680841518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 835916.9907559324, 835916.9907559331, 200400.9850089962], 
processed observation next is [1.0, 0.2608695652173913, 0.4597156398104266, 0.87, 1.0, 1.0, 0.5158789976917492, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23219916409887012, 0.23219916409887031, 0.29910594777462124], 
reward next is 0.7009, 
noisyNet noise sample is [array([0.8534795], dtype=float32), -1.0770068]. 
=============================================
[2019-03-27 05:56:45,061] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[63.50659 ]
 [63.637302]
 [63.691097]
 [63.41574 ]
 [63.332386]], R is [[63.74816895]
 [63.81031036]
 [63.87086105]
 [63.93188095]
 [63.99599075]].
[2019-03-27 05:56:48,593] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:48,593] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:48,618] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-03-27 05:56:48,784] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:48,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:48,791] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-03-27 05:56:48,812] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:48,813] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:48,826] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-03-27 05:56:48,847] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:48,848] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:48,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-03-27 05:56:49,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,185] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,189] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-03-27 05:56:49,332] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,338] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-03-27 05:56:49,360] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,361] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,368] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-03-27 05:56:49,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,467] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,467] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,468] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-03-27 05:56:49,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,489] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-03-27 05:56:49,511] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,512] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,514] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-03-27 05:56:49,512] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-03-27 05:56:49,513] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-03-27 05:56:49,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,666] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-03-27 05:56:49,804] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 05:56:49,805] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:56:49,806] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-03-27 05:56:53,596] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3485288e-13 1.0000000e+00 9.8159565e-20 6.3650224e-13 1.2357342e-24], sum to 1.0000
[2019-03-27 05:56:53,607] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0430
[2019-03-27 05:56:53,610] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 62.66666666666667, 1.0, 2.0, 0.8106494063434205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1215766.281177766, 1215766.281177767, 257815.2429750311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 48000.0000, 
sim time next is 48600.0000, 
raw observation next is [27.7, 62.5, 1.0, 2.0, 0.883599305018736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1322282.517607203, 1322282.517607203, 277843.0599871189], 
processed observation next is [1.0, 0.5652173913043478, 0.5118483412322274, 0.625, 1.0, 1.0, 0.8597581988177542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36730069933533416, 0.36730069933533416, 0.4146911343091326], 
reward next is 0.5853, 
noisyNet noise sample is [array([-1.1959381], dtype=float32), 0.2673496]. 
=============================================
[2019-03-27 05:57:01,821] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6989806e-16 1.0000000e+00 2.4022941e-20 7.8034757e-15 7.4737301e-23], sum to 1.0000
[2019-03-27 05:57:01,828] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5809
[2019-03-27 05:57:01,832] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.76666666666667, 87.0, 1.0, 2.0, 0.2281675200590061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 378658.161109756, 378658.1611097553, 158610.1881972825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 519600.0000, 
sim time next is 520200.0000, 
raw observation next is [18.75, 87.0, 1.0, 2.0, 0.2275384810893793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 377668.3390990801, 377668.3390990808, 158545.4468832665], 
processed observation next is [1.0, 0.0, 0.08767772511848347, 0.87, 1.0, 1.0, 0.06932347119202324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1049078719719667, 0.1049078719719669, 0.23663499534815896], 
reward next is 0.7634, 
noisyNet noise sample is [array([-0.11307857], dtype=float32), -1.0903765]. 
=============================================
[2019-03-27 05:57:09,830] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6707087e-16 1.0000000e+00 2.3056216e-22 1.4579076e-16 2.5898359e-24], sum to 1.0000
[2019-03-27 05:57:09,838] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3320
[2019-03-27 05:57:09,842] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.78333333333333, 83.5, 1.0, 2.0, 0.2943319659023842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 470804.6572689256, 470804.6572689256, 164999.0932222761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 327000.0000, 
sim time next is 327600.0000, 
raw observation next is [21.7, 84.0, 1.0, 2.0, 0.2923484757194712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467795.4244397502, 467795.4244397502, 164790.8867092818], 
processed observation next is [0.0, 0.8260869565217391, 0.2274881516587678, 0.84, 1.0, 1.0, 0.14740780207165205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12994317345548617, 0.12994317345548617, 0.24595654732728625], 
reward next is 0.7540, 
noisyNet noise sample is [array([-1.4618429], dtype=float32), 1.1991904]. 
=============================================
[2019-03-27 05:57:12,171] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1368292e-14 1.0000000e+00 9.8213925e-19 3.1316980e-14 9.4079400e-23], sum to 1.0000
[2019-03-27 05:57:12,181] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3216
[2019-03-27 05:57:12,186] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 76.5, 1.0, 2.0, 0.522385142679708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 838717.0915180474, 838717.0915180481, 199160.1414957579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 400200.0000, 
sim time next is 400800.0000, 
raw observation next is [22.5, 77.0, 1.0, 2.0, 0.4975196734632705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 798472.6572228409, 798472.6572228416, 194600.2014099483], 
processed observation next is [1.0, 0.6521739130434783, 0.2654028436018958, 0.77, 1.0, 1.0, 0.3946020162208078, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22179796033967802, 0.22179796033967822, 0.290448061805893], 
reward next is 0.7096, 
noisyNet noise sample is [array([-1.3265189], dtype=float32), 0.7591391]. 
=============================================
[2019-03-27 05:57:18,358] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 05:57:18,360] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 05:57:18,360] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 05:57:18,360] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:57:18,362] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 05:57:18,363] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:57:18,363] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:57:18,364] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 05:57:18,365] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:57:18,366] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 05:57:18,367] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 05:57:18,389] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run27
[2019-03-27 05:57:18,412] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run27
[2019-03-27 05:57:18,412] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run27
[2019-03-27 05:57:18,413] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run27
[2019-03-27 05:57:18,429] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run27
[2019-03-27 05:57:19,848] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:57:19,849] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.96666666666667, 61.00000000000001, 1.0, 2.0, 0.7262282614554364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014942.202797113, 1014942.202797113, 226604.5786343692]
[2019-03-27 05:57:19,850] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:57:19,854] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3719782e-15 1.0000000e+00 2.6890141e-21 5.1984932e-15 1.9729358e-24], sampled 0.8560524039943338
[2019-03-27 05:57:38,403] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:57:38,403] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.33893361333334, 92.88258376666667, 1.0, 2.0, 0.4486288327825482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 641770.8969422838, 641770.8969422838, 177856.131472686]
[2019-03-27 05:57:38,404] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:57:38,408] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.3631097e-16 1.0000000e+00 2.0499125e-21 1.4346978e-15 1.9062556e-24], sampled 0.297013719507894
[2019-03-27 05:57:39,487] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:57:39,488] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.57228061666667, 97.43745203333334, 1.0, 2.0, 0.3011224322244062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479319.0337215477, 479319.0337215477, 165570.5748043607]
[2019-03-27 05:57:39,489] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:57:39,490] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.5467017e-16 1.0000000e+00 2.1741770e-21 1.5467166e-15 2.8318844e-24], sampled 0.6764460357826205
[2019-03-27 05:57:42,348] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:57:42,349] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.45, 93.0, 1.0, 2.0, 0.423968690961478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625454.8084614415, 625454.8084614415, 176744.0211345631]
[2019-03-27 05:57:42,349] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:57:42,352] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.7416046e-16 1.0000000e+00 1.6291290e-21 1.4690065e-15 1.6089247e-24], sampled 0.590106959827559
[2019-03-27 05:57:58,233] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:57:58,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.75167867166667, 98.55451075500001, 1.0, 2.0, 0.4360208350861943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 636006.9953580395, 636006.9953580395, 177593.5864035739]
[2019-03-27 05:57:58,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:57:58,239] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.2636741e-16 1.0000000e+00 2.2691156e-21 1.3634738e-15 2.6338732e-24], sampled 0.8100955035646961
[2019-03-27 05:57:59,586] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:57:59,589] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.88333333333333, 88.83333333333334, 1.0, 2.0, 0.7270681940851242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1016116.614429378, 1016116.614429378, 226796.1251972008]
[2019-03-27 05:57:59,590] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 05:57:59,592] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0538561e-15 1.0000000e+00 2.4731097e-21 3.4968332e-15 2.0937717e-24], sampled 0.44187632955601974
[2019-03-27 05:58:23,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:58:23,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.56196993, 74.38446220666667, 1.0, 2.0, 0.5511212484565101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770132.213384959, 770132.2133849584, 191992.8982392338]
[2019-03-27 05:58:23,575] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 05:58:23,577] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.4066679e-16 1.0000000e+00 1.2530338e-21 1.1177402e-15 1.0391804e-24], sampled 0.5843703188975472
[2019-03-27 05:58:37,760] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:58:37,762] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.2, 73.0, 1.0, 2.0, 0.567590074248983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793154.2047882704, 793154.2047882704, 194862.7414086782]
[2019-03-27 05:58:37,762] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 05:58:37,765] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1236179e-16 1.0000000e+00 1.5195690e-22 8.0899720e-16 6.1774280e-26], sampled 0.057439359498044684
[2019-03-27 05:58:54,330] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:58:54,331] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.72741037333333, 87.27216001166667, 1.0, 2.0, 0.5212124013504928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728323.5126894945, 728323.512689494, 186986.0963288923]
[2019-03-27 05:58:54,332] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 05:58:54,334] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0441482e-15 1.0000000e+00 2.3944978e-21 2.6015020e-15 2.1747544e-24], sampled 0.7678268700393706
[2019-03-27 05:59:08,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00524874], dtype=float32), 0.038454294]
[2019-03-27 05:59:08,649] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 64.0, 1.0, 2.0, 0.4519709058075205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 638945.6299543233, 638945.6299543233, 177370.9684343898]
[2019-03-27 05:59:08,650] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 05:59:08,654] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.8260768e-16 1.0000000e+00 1.4171622e-21 1.0006983e-15 1.4630104e-24], sampled 0.5599760693177728
[2019-03-27 05:59:13,102] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0142 3007608811.4011 1766.0000
[2019-03-27 05:59:13,260] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8615 2842433766.9939 1131.0000
[2019-03-27 05:59:13,352] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.8887 2927319438.9013 1338.0000
[2019-03-27 05:59:13,389] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5202 2779263176.1304 933.0000
[2019-03-27 05:59:13,479] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4187 3164067725.7596 1778.0000
[2019-03-27 05:59:14,497] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 650000, evaluation results [650000.0, 7883.418721718924, 3164067725.7596025, 1778.0, 8252.888698701774, 2927319438.9012923, 1338.0, 8660.520172588904, 2779263176.1304064, 933.0, 7999.014205657292, 3007608811.4011106, 1766.0, 8496.861463514279, 2842433766.993932, 1131.0]
[2019-03-27 05:59:17,528] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.9283339e-15 1.0000000e+00 3.4537259e-20 6.8475118e-15 5.7980538e-23], sum to 1.0000
[2019-03-27 05:59:17,537] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7546
[2019-03-27 05:59:17,540] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.16666666666667, 89.33333333333334, 1.0, 2.0, 0.219604316795121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 365366.1159042977, 365366.1159042984, 157679.6430990646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 530400.0000, 
sim time next is 531000.0000, 
raw observation next is [18.1, 89.5, 1.0, 2.0, 0.2208344335891245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367540.3790585317, 367540.3790585317, 157758.9833277278], 
processed observation next is [1.0, 0.13043478260869565, 0.05687203791469207, 0.895, 1.0, 1.0, 0.061246305529065646, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10209454973848103, 0.10209454973848103, 0.23546116914586238], 
reward next is 0.7645, 
noisyNet noise sample is [array([0.48554263], dtype=float32), 1.2675875]. 
=============================================
[2019-03-27 05:59:17,551] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.542046]
 [74.642044]
 [74.549   ]
 [74.557945]
 [74.50561 ]], R is [[74.43850708]
 [74.45877838]
 [74.47651672]
 [74.49633026]
 [74.51596832]].
[2019-03-27 05:59:22,656] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7592224e-13 1.0000000e+00 2.8089522e-20 2.1573650e-13 1.8071161e-22], sum to 1.0000
[2019-03-27 05:59:22,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2278
[2019-03-27 05:59:22,673] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 96.0, 1.0, 2.0, 0.5330327924032255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824628.4409084235, 824628.4409084235, 198432.8941335611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1003200.0000, 
sim time next is 1003800.0000, 
raw observation next is [21.6, 96.0, 1.0, 2.0, 0.5202789958299762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804925.5168352509, 804925.5168352509, 196103.687232592], 
processed observation next is [1.0, 0.6086956521739131, 0.22274881516587688, 0.96, 1.0, 1.0, 0.42202288654213993, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22359042134312523, 0.22359042134312523, 0.29269207049640594], 
reward next is 0.7073, 
noisyNet noise sample is [array([-0.7618097], dtype=float32), 0.14233881]. 
=============================================
[2019-03-27 05:59:23,672] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6874240e-14 1.0000000e+00 2.6016643e-20 4.6922952e-14 3.0724943e-23], sum to 1.0000
[2019-03-27 05:59:23,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1093
[2019-03-27 05:59:23,684] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 80.0, 1.0, 2.0, 0.2208335492003436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 366683.8036039399, 366683.8036039399, 157923.2939653766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 630000.0000, 
sim time next is 630600.0000, 
raw observation next is [19.83333333333334, 79.0, 1.0, 2.0, 0.2216664275007999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367760.7904005787, 367760.7904005787, 158041.4913121994], 
processed observation next is [1.0, 0.30434782608695654, 0.13902053712480286, 0.79, 1.0, 1.0, 0.06224870783228902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10215577511127186, 0.10215577511127186, 0.23588282285402895], 
reward next is 0.7641, 
noisyNet noise sample is [array([-0.62610584], dtype=float32), 0.19735357]. 
=============================================
[2019-03-27 05:59:29,453] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6087056e-14 1.0000000e+00 6.3769660e-21 2.5473627e-15 1.0672311e-22], sum to 1.0000
[2019-03-27 05:59:29,462] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6075
[2019-03-27 05:59:29,467] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.85, 95.0, 1.0, 2.0, 0.3083842137190738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488194.5129736612, 488194.5129736612, 166163.0152901783], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1065000.0000, 
sim time next is 1065600.0000, 
raw observation next is [20.9, 95.0, 1.0, 2.0, 0.3114368452079352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492449.6841941559, 492449.6841941559, 166463.6412705227], 
processed observation next is [1.0, 0.34782608695652173, 0.1895734597156398, 0.95, 1.0, 1.0, 0.1704058375999219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1367915789428211, 0.1367915789428211, 0.24845319592615328], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.19242518], dtype=float32), 0.646431]. 
=============================================
[2019-03-27 05:59:34,002] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4175479e-16 1.0000000e+00 2.6990829e-23 1.6494250e-16 2.8723433e-26], sum to 1.0000
[2019-03-27 05:59:34,017] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5285
[2019-03-27 05:59:34,023] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 79.66666666666667, 1.0, 2.0, 0.2991246697559024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476844.6019803797, 476844.6019803797, 165404.4113641931], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 847200.0000, 
sim time next is 847800.0000, 
raw observation next is [22.4, 80.0, 1.0, 2.0, 0.2984134135397092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475912.278578721, 475912.278578721, 165341.1375158966], 
processed observation next is [0.0, 0.8260869565217391, 0.2606635071090047, 0.8, 1.0, 1.0, 0.15471495607193878, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13219785516075583, 0.13219785516075583, 0.2467778171879054], 
reward next is 0.7532, 
noisyNet noise sample is [array([-1.174297], dtype=float32), -0.52142996]. 
=============================================
[2019-03-27 05:59:37,353] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5712336e-15 1.0000000e+00 7.2627484e-21 5.3197015e-15 6.6757275e-25], sum to 1.0000
[2019-03-27 05:59:37,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2985
[2019-03-27 05:59:37,366] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 82.83333333333334, 1.0, 2.0, 0.3540346151204448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546202.1469404835, 546202.1469404835, 170352.7373304945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1203000.0000, 
sim time next is 1203600.0000, 
raw observation next is [23.3, 83.66666666666667, 1.0, 2.0, 0.3541673993488564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546192.685384067, 546192.6853840663, 170345.9873198644], 
processed observation next is [1.0, 0.9565217391304348, 0.3033175355450238, 0.8366666666666667, 1.0, 1.0, 0.22188843295042937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15172019038446305, 0.15172019038446286, 0.2542477422684543], 
reward next is 0.7458, 
noisyNet noise sample is [array([0.48961887], dtype=float32), -0.7244458]. 
=============================================
[2019-03-27 05:59:38,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9339656e-15 1.0000000e+00 3.1981840e-21 2.5942874e-15 5.2918850e-25], sum to 1.0000
[2019-03-27 05:59:38,789] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7294
[2019-03-27 05:59:38,797] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2926940999104102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 467221.1734289101, 467221.1734289107, 164736.5207204623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 894000.0000, 
sim time next is 894600.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2930747842748531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 467827.9844454972, 467827.9844454965, 164778.8066060361], 
processed observation next is [0.0, 0.34782608695652173, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14828287262030496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.129952217901527, 0.1299522179015268, 0.24593851732244193], 
reward next is 0.7541, 
noisyNet noise sample is [array([-1.3067064], dtype=float32), 0.2761356]. 
=============================================
[2019-03-27 05:59:41,917] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.31819608e-15 1.00000000e+00 7.83761450e-20 1.16763034e-14
 7.84477757e-24], sum to 1.0000
[2019-03-27 05:59:41,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9878
[2019-03-27 05:59:41,926] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.65, 94.0, 1.0, 2.0, 0.8230026852319834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1240633.399539506, 1240633.399539506, 261960.4021930824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1337400.0000, 
sim time next is 1338000.0000, 
raw observation next is [22.56666666666667, 93.66666666666667, 1.0, 2.0, 0.7483828928246812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1132725.695171514, 1132725.695171513, 242998.8884753696], 
processed observation next is [1.0, 0.4782608695652174, 0.26856240126382325, 0.9366666666666668, 1.0, 1.0, 0.696846858824917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31464602643653167, 0.31464602643653133, 0.36268490817219345], 
reward next is 0.6373, 
noisyNet noise sample is [array([0.34689122], dtype=float32), 0.54536784]. 
=============================================
[2019-03-27 05:59:41,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.15914 ]
 [72.1068  ]
 [72.043526]
 [72.00728 ]
 [71.964554]], R is [[72.23548126]
 [72.12214661]
 [72.01153564]
 [71.89707947]
 [71.80017853]].
[2019-03-27 05:59:46,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4159278e-16 1.0000000e+00 6.1573252e-22 2.9643294e-15 2.9598300e-24], sum to 1.0000
[2019-03-27 05:59:46,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1981
[2019-03-27 05:59:46,624] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.91666666666667, 84.83333333333333, 1.0, 2.0, 0.481559489056565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765140.1450929537, 765140.1450929537, 191220.0249324001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1072200.0000, 
sim time next is 1072800.0000, 
raw observation next is [22.0, 84.0, 1.0, 2.0, 0.5164311119203919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821012.0469414337, 821012.0469414337, 197449.7681642453], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.84, 1.0, 1.0, 0.41738688183179745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22805890192817602, 0.22805890192817602, 0.294701146513799], 
reward next is 0.7053, 
noisyNet noise sample is [array([0.6378077], dtype=float32), 0.046194926]. 
=============================================
[2019-03-27 05:59:50,349] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2231382e-15 1.0000000e+00 3.1840770e-20 5.6408235e-15 6.1224940e-23], sum to 1.0000
[2019-03-27 05:59:50,363] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5994
[2019-03-27 05:59:50,369] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 81.33333333333334, 1.0, 2.0, 0.3105481130154208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492629.4037627461, 492629.4037627461, 166507.4942497036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1113600.0000, 
sim time next is 1114200.0000, 
raw observation next is [22.35, 82.0, 1.0, 2.0, 0.3103949106922713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492663.1602369188, 492663.1602369188, 166514.7629283872], 
processed observation next is [1.0, 0.9130434782608695, 0.25829383886255936, 0.82, 1.0, 1.0, 0.16915049480996544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13685087784358854, 0.13685087784358854, 0.2485294969080406], 
reward next is 0.7515, 
noisyNet noise sample is [array([-1.1245661], dtype=float32), 0.41888794]. 
=============================================
[2019-03-27 05:59:51,652] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.9780966e-14 1.0000000e+00 3.2347880e-19 1.8997980e-13 7.9229786e-22], sum to 1.0000
[2019-03-27 05:59:51,664] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8276
[2019-03-27 05:59:51,669] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 69.0, 1.0, 2.0, 0.7513651351288666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1152749.972506847, 1152749.972506846, 245571.9352812504], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1162800.0000, 
sim time next is 1163400.0000, 
raw observation next is [25.8, 68.5, 1.0, 2.0, 0.7205659639855024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1104329.640723859, 1104329.640723859, 237746.4048036471], 
processed observation next is [1.0, 0.4782608695652174, 0.42180094786729866, 0.685, 1.0, 1.0, 0.6633324867295209, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3067582335344053, 0.3067582335344053, 0.3548453803039509], 
reward next is 0.6452, 
noisyNet noise sample is [array([0.21474186], dtype=float32), 0.4917339]. 
=============================================
[2019-03-27 06:00:01,129] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4715087e-14 1.0000000e+00 1.0383113e-19 5.8926232e-14 3.1507723e-23], sum to 1.0000
[2019-03-27 06:00:01,139] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9818
[2019-03-27 06:00:01,147] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333334, 94.33333333333334, 1.0, 2.0, 0.4359369424408381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 638819.8099186216, 638819.8099186221, 177944.6119180975], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1322400.0000, 
sim time next is 1323000.0000, 
raw observation next is [23.35, 94.5, 1.0, 2.0, 0.4359191512092802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 640328.4553322832, 640328.4553322826, 178130.3918815759], 
processed observation next is [1.0, 0.30434782608695654, 0.3056872037914693, 0.945, 1.0, 1.0, 0.3203845195292533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17786901537007865, 0.1778690153700785, 0.2658662565396655], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.52532214], dtype=float32), 1.4604988]. 
=============================================
[2019-03-27 06:00:01,161] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[72.85093 ]
 [72.74911 ]
 [72.735146]
 [72.52447 ]
 [72.563286]], R is [[73.00177002]
 [73.00616455]
 [72.98886108]
 [72.98733521]
 [72.97224426]].
[2019-03-27 06:00:04,396] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6270442e-16 1.0000000e+00 1.4009083e-21 2.6191669e-15 4.2313588e-24], sum to 1.0000
[2019-03-27 06:00:04,405] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6803
[2019-03-27 06:00:04,410] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 95.33333333333334, 1.0, 2.0, 0.3171240389280713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501519.2736703553, 501519.2736703559, 167139.9895251932], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1376400.0000, 
sim time next is 1377000.0000, 
raw observation next is [20.8, 95.5, 1.0, 2.0, 0.3160292879083856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499949.0761886003, 499949.0761886003, 167025.4563251972], 
processed observation next is [1.0, 0.9565217391304348, 0.1848341232227489, 0.955, 1.0, 1.0, 0.17593890109444046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1388747433857223, 0.1388747433857223, 0.24929172585850332], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.9247646], dtype=float32), 0.46571892]. 
=============================================
[2019-03-27 06:00:04,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.23137 ]
 [75.172165]
 [75.109665]
 [75.00624 ]
 [74.870804]], R is [[75.29156494]
 [75.28918457]
 [75.28668976]
 [75.2841568 ]
 [75.28168488]].
[2019-03-27 06:00:08,274] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 06:00:08,275] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:00:08,276] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:00:08,276] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:00:08,277] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:00:08,279] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:00:08,279] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:00:08,277] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:00:08,280] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:00:08,282] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:00:08,281] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:00:08,305] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run28
[2019-03-27 06:00:08,306] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run28
[2019-03-27 06:00:08,345] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run28
[2019-03-27 06:00:08,346] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run28
[2019-03-27 06:00:08,388] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run28
[2019-03-27 06:00:16,642] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00496163], dtype=float32), 0.03792302]
[2019-03-27 06:00:16,644] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.99102785, 65.48239886, 1.0, 2.0, 0.3227297047731492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521494.5866658249, 521494.5866658243, 168690.8552755994]
[2019-03-27 06:00:16,645] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:00:16,650] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1059090e-15 1.0000000e+00 8.6257142e-21 2.4168323e-15 1.3815469e-23], sampled 0.7104955036580436
[2019-03-27 06:01:47,080] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00496163], dtype=float32), 0.03792302]
[2019-03-27 06:01:47,081] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.65, 54.0, 1.0, 2.0, 0.9901011126904239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104286, 1383958.735243169, 1383958.735243168, 295925.1799389761]
[2019-03-27 06:01:47,082] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:01:47,084] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6528922e-12 1.0000000e+00 5.3709995e-17 4.7048129e-11 6.3027234e-20], sampled 0.23460649477543583
[2019-03-27 06:01:52,106] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00496163], dtype=float32), 0.03792302]
[2019-03-27 06:01:52,107] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.96666666666667, 67.66666666666667, 1.0, 2.0, 0.6784033999726581, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005974176683111, 6.9112, 168.9118857240709, 1844907.943409148, 1777672.306822888, 380049.2705957252]
[2019-03-27 06:01:52,109] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:01:52,113] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.1935019e-14 1.0000000e+00 1.8809350e-19 3.1767967e-13 1.1947863e-22], sampled 0.27118149834078853
[2019-03-27 06:01:52,114] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1844907.943409148 W.
[2019-03-27 06:02:02,716] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.3428 2842529354.8862 1131.0000
[2019-03-27 06:02:03,068] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2623 3007616512.7150 1766.0000
[2019-03-27 06:02:03,174] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.0972 3164075515.2915 1778.0000
[2019-03-27 06:02:03,175] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 06:02:03,257] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2377 2779196147.7214 933.0000
[2019-03-27 06:02:04,274] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 675000, evaluation results [675000.0, 7884.097213959008, 3164075515.2915134, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8659.23765033799, 2779196147.721415, 933.0, 7998.262342612011, 3007616512.7150273, 1766.0, 8495.342760625537, 2842529354.886171, 1131.0]
[2019-03-27 06:02:05,583] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2189925e-15 1.0000000e+00 7.4933217e-21 2.2971140e-15 1.5501886e-24], sum to 1.0000
[2019-03-27 06:02:05,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7814
[2019-03-27 06:02:05,597] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333333, 82.33333333333334, 1.0, 2.0, 0.4220432983190036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 613200.330282592, 613200.3302825913, 175305.1974712079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1428000.0000, 
sim time next is 1428600.0000, 
raw observation next is [25.66666666666667, 80.66666666666667, 1.0, 2.0, 0.4230837763388082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612587.0450986268, 612587.0450986268, 175183.2670321294], 
processed observation next is [0.0, 0.5217391304347826, 0.4154818325434442, 0.8066666666666668, 1.0, 1.0, 0.30492021245639545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17016306808295187, 0.17016306808295187, 0.2614675627345215], 
reward next is 0.7385, 
noisyNet noise sample is [array([1.0267224], dtype=float32), 0.13846368]. 
=============================================
[2019-03-27 06:02:06,637] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.3262198e-15 1.0000000e+00 2.3649755e-22 7.4330093e-17 2.5895692e-24], sum to 1.0000
[2019-03-27 06:02:06,647] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9708
[2019-03-27 06:02:06,654] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 93.33333333333334, 1.0, 2.0, 0.3817276113525269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575046.238524547, 575046.2385245475, 172435.9016830741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1456800.0000, 
sim time next is 1457400.0000, 
raw observation next is [22.75, 93.66666666666667, 1.0, 2.0, 0.3812725536995554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 574517.8006273448, 574517.800627344, 172393.7785899106], 
processed observation next is [0.0, 0.8695652173913043, 0.27725118483412325, 0.9366666666666668, 1.0, 1.0, 0.2545452454211511, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1595882779520402, 0.15958827795204, 0.2573041471491203], 
reward next is 0.7427, 
noisyNet noise sample is [array([-1.1693302], dtype=float32), -0.39872807]. 
=============================================
[2019-03-27 06:02:11,814] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.65815332e-14 1.00000000e+00 2.20712053e-19 1.11580734e-13
 2.14464497e-22], sum to 1.0000
[2019-03-27 06:02:11,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2627
[2019-03-27 06:02:11,832] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 88.0, 1.0, 2.0, 0.3212464011813297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 502674.9943207598, 502674.9943207592, 167100.9012685133], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1576800.0000, 
sim time next is 1577400.0000, 
raw observation next is [22.3, 87.66666666666667, 1.0, 2.0, 0.3527663989668288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551096.573161713, 551096.573161713, 170923.8904187363], 
processed observation next is [1.0, 0.2608695652173913, 0.25592417061611383, 0.8766666666666667, 1.0, 1.0, 0.22020048068292628, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15308238143380914, 0.15308238143380914, 0.2551102842070691], 
reward next is 0.7449, 
noisyNet noise sample is [array([-1.4628975], dtype=float32), -0.35476297]. 
=============================================
[2019-03-27 06:02:12,269] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6697432e-13 1.0000000e+00 1.1661012e-18 1.7254460e-13 1.5988137e-21], sum to 1.0000
[2019-03-27 06:02:12,281] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6515
[2019-03-27 06:02:12,285] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 85.33333333333334, 1.0, 2.0, 0.3917076470776573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602260.0155573708, 602260.0155573714, 175180.5040984568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1582800.0000, 
sim time next is 1583400.0000, 
raw observation next is [23.3, 85.16666666666667, 1.0, 2.0, 0.3847513131915307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590258.5987439593, 590258.5987439593, 174078.2045986252], 
processed observation next is [1.0, 0.30434782608695654, 0.3033175355450238, 0.8516666666666667, 1.0, 1.0, 0.2587365219175069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16396072187332203, 0.16396072187332203, 0.25981821581884357], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.15162398], dtype=float32), -0.44462785]. 
=============================================
[2019-03-27 06:02:13,645] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7926408e-10 1.0000000e+00 1.7254591e-13 2.4754151e-08 4.3887182e-17], sum to 1.0000
[2019-03-27 06:02:13,658] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7743
[2019-03-27 06:02:13,665] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1803951.634127669 W.
[2019-03-27 06:02:13,669] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.46666666666667, 74.33333333333333, 1.0, 2.0, 0.6451623198236648, 1.0, 1.0, 0.6451623198236648, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1803951.634127669, 1803951.634127669, 351519.6712537094], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1950000.0000, 
sim time next is 1950600.0000, 
raw observation next is [27.53333333333333, 74.16666666666667, 1.0, 2.0, 0.4254555548112532, 1.0, 2.0, 0.4254555548112532, 1.0, 1.0, 0.7159383804099917, 6.9112, 6.9112, 170.5573041426782, 1784421.294743571, 1784421.294743571, 363892.8313056144], 
processed observation next is [1.0, 0.5652173913043478, 0.5039494470774091, 0.7416666666666667, 1.0, 1.0, 0.3077777768810279, 1.0, 1.0, 0.3077777768810279, 1.0, 0.5, 0.6535833907438924, 0.0, 0.0, 0.8375144448122397, 0.49567258187321417, 0.49567258187321417, 0.5431236288143498], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.48677537], dtype=float32), 1.2974175]. 
=============================================
[2019-03-27 06:02:14,419] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6410548e-17 1.0000000e+00 1.8855227e-22 6.7181915e-16 2.4373846e-24], sum to 1.0000
[2019-03-27 06:02:14,428] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8732
[2019-03-27 06:02:14,432] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 95.66666666666666, 1.0, 2.0, 0.4117705017689461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 607493.277684888, 607493.2776848886, 175036.1080805592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1621200.0000, 
sim time next is 1621800.0000, 
raw observation next is [23.1, 95.5, 1.0, 2.0, 0.4116340556942512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607754.4664720419, 607754.4664720419, 175073.8917299772], 
processed observation next is [1.0, 0.782608695652174, 0.2938388625592418, 0.955, 1.0, 1.0, 0.29112536830632674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16882068513112275, 0.16882068513112275, 0.26130431601489135], 
reward next is 0.7387, 
noisyNet noise sample is [array([-0.19820523], dtype=float32), 0.117613316]. 
=============================================
[2019-03-27 06:02:17,660] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5290120e-14 1.0000000e+00 7.0406739e-19 3.0757406e-13 8.1189419e-23], sum to 1.0000
[2019-03-27 06:02:17,672] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3330
[2019-03-27 06:02:17,680] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 89.0, 1.0, 2.0, 0.9251684943578017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1293140.94533404, 1293140.94533404, 276951.4914677984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1680000.0000, 
sim time next is 1680600.0000, 
raw observation next is [25.78333333333333, 88.5, 1.0, 2.0, 0.9213328693092347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287776.504167697, 1287776.504167696, 275870.1029602659], 
processed observation next is [1.0, 0.43478260869565216, 0.4210110584518167, 0.885, 1.0, 1.0, 0.9052203244689575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35771569560213806, 0.3577156956021378, 0.411746422328755], 
reward next is 0.5883, 
noisyNet noise sample is [array([-1.15239], dtype=float32), -0.10218026]. 
=============================================
[2019-03-27 06:02:17,972] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6331388e-14 1.0000000e+00 6.2670008e-20 1.6908629e-13 1.9111722e-22], sum to 1.0000
[2019-03-27 06:02:17,979] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3900
[2019-03-27 06:02:17,985] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.41666666666667, 92.5, 1.0, 2.0, 0.407016700142621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 602061.6783329732, 602061.6783329732, 174575.3975316858], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1968600.0000, 
sim time next is 1969200.0000, 
raw observation next is [23.3, 93.0, 1.0, 2.0, 0.4057690004841984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601360.3309604963, 601360.3309604963, 174543.9804971265], 
processed observation next is [1.0, 0.8260869565217391, 0.3033175355450238, 0.93, 1.0, 1.0, 0.28405903672794985, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16704453637791564, 0.16704453637791564, 0.26051340372705445], 
reward next is 0.7395, 
noisyNet noise sample is [array([-1.044813], dtype=float32), 0.33150262]. 
=============================================
[2019-03-27 06:02:18,084] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5576255e-15 1.0000000e+00 8.6118396e-21 4.9587488e-13 7.6200316e-23], sum to 1.0000
[2019-03-27 06:02:18,092] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9505
[2019-03-27 06:02:18,098] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 87.0, 1.0, 2.0, 0.9152412999523736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1279256.992655835, 1279256.992655834, 274161.9407981627], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1683000.0000, 
sim time next is 1683600.0000, 
raw observation next is [26.3, 86.66666666666666, 1.0, 2.0, 0.9821274446072353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1372805.972537936, 1372805.972537937, 293525.112580921], 
processed observation next is [1.0, 0.4782608695652174, 0.4454976303317536, 0.8666666666666666, 1.0, 1.0, 0.9784668007316089, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38133499237164886, 0.3813349923716492, 0.4380971829565985], 
reward next is 0.5619, 
noisyNet noise sample is [array([-1.0072414], dtype=float32), -1.66694]. 
=============================================
[2019-03-27 06:02:31,215] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6726135e-14 1.0000000e+00 7.0745624e-21 2.7326099e-14 8.8580536e-24], sum to 1.0000
[2019-03-27 06:02:31,222] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2465
[2019-03-27 06:02:31,226] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 90.0, 1.0, 2.0, 0.453565014078821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646514.4292675435, 646514.4292675435, 178281.3387856896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1893600.0000, 
sim time next is 1894200.0000, 
raw observation next is [24.65, 90.33333333333333, 1.0, 2.0, 0.4527578666226864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645650.7652913006, 645650.7652913006, 178200.1073501263], 
processed observation next is [1.0, 0.9565217391304348, 0.3672985781990521, 0.9033333333333333, 1.0, 1.0, 0.340672128461068, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17934743480313906, 0.17934743480313906, 0.2659703094778004], 
reward next is 0.7340, 
noisyNet noise sample is [array([-1.0403582], dtype=float32), -0.05314828]. 
=============================================
[2019-03-27 06:02:46,883] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7137687e-14 1.0000000e+00 1.4048760e-18 9.6698332e-14 3.1338006e-22], sum to 1.0000
[2019-03-27 06:02:46,885] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5061148e-13 1.0000000e+00 1.8270062e-18 1.9848401e-13 6.1866143e-22], sum to 1.0000
[2019-03-27 06:02:46,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8008
[2019-03-27 06:02:46,892] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7438
[2019-03-27 06:02:46,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1773492.994362928 W.
[2019-03-27 06:02:46,901] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.55, 94.00000000000001, 1.0, 2.0, 0.5093806351527911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711784.6877669791, 711784.6877669791, 185078.8189309198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2164200.0000, 
sim time next is 2164800.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5079720506626362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709815.7400262054, 709815.7400262054, 184854.4702663285], 
processed observation next is [1.0, 0.043478260869565216, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4071952417622122, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1971710388961682, 0.1971710388961682, 0.275902194427356], 
reward next is 0.7241, 
noisyNet noise sample is [array([-0.7661189], dtype=float32), 1.858747]. 
=============================================
[2019-03-27 06:02:46,906] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.38333333333333, 95.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.359698216321023, 6.9112, 169.6265963146485, 1773492.994362928, 1453968.645634479, 311482.866855632], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2513400.0000, 
sim time next is 2514000.0000, 
raw observation next is [26.36666666666667, 95.33333333333334, 1.0, 2.0, 0.5665335931043616, 1.0, 1.0, 0.5665335931043616, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1583933.752718461, 1583933.752718461, 322221.2379259699], 
processed observation next is [1.0, 0.08695652173913043, 0.4486571879936811, 0.9533333333333335, 1.0, 1.0, 0.4777513169932067, 1.0, 0.5, 0.4777513169932067, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.43998159797735026, 0.43998159797735026, 0.4809272207850297], 
reward next is 0.5191, 
noisyNet noise sample is [array([-0.9675948], dtype=float32), 0.21945536]. 
=============================================
[2019-03-27 06:02:46,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[65.89908 ]
 [66.51403 ]
 [66.461784]
 [66.56725 ]
 [66.71177 ]], R is [[61.69508362]
 [61.07813263]
 [61.18250656]
 [61.28574753]
 [61.38779449]].
[2019-03-27 06:02:50,857] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3522918e-13 1.0000000e+00 1.4742525e-19 2.2047880e-13 6.6952184e-23], sum to 1.0000
[2019-03-27 06:02:50,863] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7922
[2019-03-27 06:02:50,868] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.18333333333334, 86.66666666666666, 1.0, 2.0, 0.5332053691326285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745087.9331896056, 745087.9331896056, 188962.0568143222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2580600.0000, 
sim time next is 2581200.0000, 
raw observation next is [27.1, 87.0, 1.0, 2.0, 0.5330832358044094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744917.2072691167, 744917.2072691167, 188941.636561287], 
processed observation next is [1.0, 0.9130434782608695, 0.4834123222748816, 0.87, 1.0, 1.0, 0.4374496816920595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20692144646364352, 0.20692144646364352, 0.2820024426287866], 
reward next is 0.7180, 
noisyNet noise sample is [array([-1.5319719], dtype=float32), 0.11025486]. 
=============================================
[2019-03-27 06:02:53,879] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4051766e-06 9.8807728e-01 1.1483887e-08 1.1921288e-02 4.2668154e-11], sum to 1.0000
[2019-03-27 06:02:53,886] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0059
[2019-03-27 06:02:53,895] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2334138.854563231 W.
[2019-03-27 06:02:53,900] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.08333333333334, 62.16666666666667, 1.0, 2.0, 1.027960788203797, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.995422377055226, 6.9112, 168.9123883675122, 2334138.854563231, 2274388.802541204, 472429.8637485568], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2286600.0000, 
sim time next is 2287200.0000, 
raw observation next is [32.06666666666667, 62.33333333333334, 1.0, 2.0, 0.8262976773515979, 1.0, 1.0, 0.8262976773515979, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2310959.077979595, 2310959.077979595, 432876.3634731765], 
processed observation next is [1.0, 0.4782608695652174, 0.7187993680884678, 0.6233333333333334, 1.0, 1.0, 0.7907200931946963, 1.0, 0.5, 0.7907200931946963, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6419330772165541, 0.6419330772165541, 0.6460841245868306], 
reward next is 0.3539, 
noisyNet noise sample is [array([-1.7833902], dtype=float32), -0.9375055]. 
=============================================
[2019-03-27 06:02:58,065] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 06:02:58,071] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:02:58,072] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:02:58,073] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:02:58,073] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:02:58,075] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:02:58,074] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:02:58,076] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:02:58,077] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:02:58,078] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:02:58,080] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:02:58,102] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run29
[2019-03-27 06:02:58,103] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run29
[2019-03-27 06:02:58,120] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run29
[2019-03-27 06:02:58,158] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run29
[2019-03-27 06:02:58,180] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run29
[2019-03-27 06:03:27,988] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00499049], dtype=float32), 0.03667433]
[2019-03-27 06:03:27,992] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.11538804, 81.94765611, 1.0, 2.0, 0.7937177902128207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1117614.738653237, 1117614.738653237, 243539.5987095135]
[2019-03-27 06:03:27,994] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:03:27,998] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9213482e-14 1.0000000e+00 1.6417014e-19 1.4599189e-13 1.1721062e-22], sampled 0.28004165635920664
[2019-03-27 06:03:37,930] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00499049], dtype=float32), 0.03667433]
[2019-03-27 06:03:37,932] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.4, 86.0, 1.0, 2.0, 0.5103460920830768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713134.2249969062, 713134.2249969055, 185232.2607116708]
[2019-03-27 06:03:37,933] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:03:37,936] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7489094e-14 1.0000000e+00 3.4821967e-19 6.5088065e-14 4.4125446e-22], sampled 0.8139826569092927
[2019-03-27 06:04:53,056] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.7736 3164171702.3214 1776.0000
[2019-03-27 06:04:53,309] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5886 2927456187.7328 1338.0000
[2019-03-27 06:04:53,359] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5239 3007689202.3210 1766.0000
[2019-03-27 06:04:53,498] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.0113 2779355431.2860 933.0000
[2019-03-27 06:04:53,556] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 06:04:54,575] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 700000, evaluation results [700000.0, 7881.773559774429, 3164171702.3214016, 1776.0, 8253.588552262909, 2927456187.7328076, 1338.0, 8659.011314617308, 2779355431.2860456, 933.0, 7997.5238745961915, 3007689202.3209686, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 06:04:58,839] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.5704105e-14 1.0000000e+00 3.0271990e-18 1.7208948e-12 5.1353608e-23], sum to 1.0000
[2019-03-27 06:04:58,845] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9167
[2019-03-27 06:04:58,850] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 80.0, 1.0, 2.0, 0.5516432354518738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 770861.8983163009, 770861.8983163016, 192083.0174811172], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2422800.0000, 
sim time next is 2423400.0000, 
raw observation next is [28.76666666666667, 80.0, 1.0, 2.0, 0.5502540305683199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768919.9317527176, 768919.9317527176, 191844.3924685367], 
processed observation next is [1.0, 0.043478260869565216, 0.5624012638230649, 0.8, 1.0, 1.0, 0.45813738622689143, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21358886993131043, 0.21358886993131043, 0.28633491413214435], 
reward next is 0.7137, 
noisyNet noise sample is [array([-0.2599007], dtype=float32), 0.13557158]. 
=============================================
[2019-03-27 06:05:01,929] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2575262e-05 7.6868224e-01 7.9979316e-07 2.3130430e-01 9.2377900e-10], sum to 1.0000
[2019-03-27 06:05:01,933] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4508
[2019-03-27 06:05:01,937] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1947241.243769539 W.
[2019-03-27 06:05:01,951] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.93333333333333, 83.33333333333334, 1.0, 2.0, 0.6963616570622011, 1.0, 2.0, 0.6963616570622011, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1947241.243769539, 1947241.243769539, 372569.9509454613], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2478000.0000, 
sim time next is 2478600.0000, 
raw observation next is [28.05, 83.0, 1.0, 2.0, 0.461771795367849, 1.0, 2.0, 0.461771795367849, 1.0, 1.0, 0.7978814148001867, 6.9112, 6.9112, 170.5573041426782, 1936874.459556204, 1936874.459556204, 389046.9527000308], 
processed observation next is [1.0, 0.6956521739130435, 0.528436018957346, 0.83, 1.0, 1.0, 0.35153228357572175, 1.0, 1.0, 0.35153228357572175, 1.0, 0.5, 0.7535139204880326, 0.0, 0.0, 0.8375144448122397, 0.5380206832100567, 0.5380206832100567, 0.5806670935821355], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.20670205], dtype=float32), 1.3939391]. 
=============================================
[2019-03-27 06:05:03,342] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5299776e-15 1.0000000e+00 7.4462271e-19 8.5426363e-14 2.5266142e-22], sum to 1.0000
[2019-03-27 06:05:03,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0068
[2019-03-27 06:05:03,360] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.46666666666667, 95.0, 1.0, 2.0, 0.5472357266830534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764700.6633828653, 764700.6633828653, 191327.8186946378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2510400.0000, 
sim time next is 2511000.0000, 
raw observation next is [26.45, 95.0, 1.0, 2.0, 0.5464744912966144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763636.5397815894, 763636.53978159, 191197.9855881855], 
processed observation next is [1.0, 0.043478260869565216, 0.45260663507109006, 0.95, 1.0, 1.0, 0.4535837244537523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2121212610504415, 0.21212126105044166, 0.2853701277435604], 
reward next is 0.7146, 
noisyNet noise sample is [array([1.1956749], dtype=float32), 1.3613681]. 
=============================================
[2019-03-27 06:05:03,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.10124 ]
 [64.36385 ]
 [64.6081  ]
 [64.800575]
 [65.133766]], R is [[63.97035217]
 [64.04508972]
 [64.11899567]
 [64.19200134]
 [64.26403809]].
[2019-03-27 06:05:09,979] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4248477e-15 1.0000000e+00 1.2643522e-19 6.4726658e-15 3.5830070e-24], sum to 1.0000
[2019-03-27 06:05:09,986] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0327
[2019-03-27 06:05:09,993] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4745087563260036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663170.6458928146, 663170.6458928146, 179714.9691434588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2622600.0000, 
sim time next is 2623200.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4749715298666664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663817.185979863, 663817.185979863, 179783.9465901326], 
processed observation next is [0.0, 0.34782608695652173, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3674355781526102, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1843936627721842, 0.1843936627721842, 0.26833424864198896], 
reward next is 0.7317, 
noisyNet noise sample is [array([0.7501934], dtype=float32), 0.12397026]. 
=============================================
[2019-03-27 06:05:14,338] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.0878867e-14 1.0000000e+00 4.0278501e-18 3.6549417e-14 4.4004564e-23], sum to 1.0000
[2019-03-27 06:05:14,347] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4915
[2019-03-27 06:05:14,353] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4758230458241832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664878.2063941116, 664878.206394111, 179894.6935287791], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2700600.0000, 
sim time next is 2701200.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4759033026671583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664990.3861802926, 664990.3861802932, 179906.6897810711], 
processed observation next is [0.0, 0.2608695652173913, 0.3364928909952607, 1.0, 1.0, 1.0, 0.36855819598452805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18471955171674795, 0.18471955171674811, 0.26851744743443445], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.9121148], dtype=float32), -0.9744257]. 
=============================================
[2019-03-27 06:05:28,556] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9018998e-14 1.0000000e+00 1.6267654e-18 3.9490783e-14 7.6157307e-23], sum to 1.0000
[2019-03-27 06:05:28,564] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0425
[2019-03-27 06:05:28,569] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333334, 98.0, 1.0, 2.0, 0.4639121058157368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736267.3941554653, 736267.3941554659, 188173.8603917266], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2983800.0000, 
sim time next is 2984400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.5159348702410322, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820788.1294222798, 820788.1294222791, 197403.8990551119], 
processed observation next is [1.0, 0.5652173913043478, 0.1469194312796209, 1.0, 1.0, 1.0, 0.4167890002904003, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22799670261729993, 0.22799670261729973, 0.2946326851568834], 
reward next is 0.7054, 
noisyNet noise sample is [array([-1.1423244], dtype=float32), -0.6927682]. 
=============================================
[2019-03-27 06:05:29,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0142460e-15 1.0000000e+00 2.2529114e-19 2.0785735e-15 1.0473912e-23], sum to 1.0000
[2019-03-27 06:05:29,161] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4247
[2019-03-27 06:05:29,175] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 95.0, 1.0, 2.0, 0.3258918714676126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 514351.9279243697, 514351.9279243691, 168095.8772717536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2959800.0000, 
sim time next is 2960400.0000, 
raw observation next is [21.0, 96.0, 1.0, 2.0, 0.3134351607399103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493290.6173517926, 493290.6173517926, 166472.5207403959], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 0.96, 1.0, 1.0, 0.1728134466745907, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13702517148660906, 0.13702517148660906, 0.24846644886626254], 
reward next is 0.7515, 
noisyNet noise sample is [array([-0.5851069], dtype=float32), 0.9117738]. 
=============================================
[2019-03-27 06:05:29,405] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.7993388e-15 1.0000000e+00 7.4163159e-20 7.3744911e-13 3.3093487e-21], sum to 1.0000
[2019-03-27 06:05:29,413] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4821
[2019-03-27 06:05:29,418] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333334, 98.0, 1.0, 2.0, 0.4639121058157368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736267.3941554653, 736267.3941554659, 188173.8603917266], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2983800.0000, 
sim time next is 2984400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.5159348702410322, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820788.1294222798, 820788.1294222791, 197403.8990551119], 
processed observation next is [1.0, 0.5652173913043478, 0.1469194312796209, 1.0, 1.0, 1.0, 0.4167890002904003, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22799670261729993, 0.22799670261729973, 0.2946326851568834], 
reward next is 0.7054, 
noisyNet noise sample is [array([-0.9204461], dtype=float32), -0.86118823]. 
=============================================
[2019-03-27 06:05:43,398] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2541537e-15 1.0000000e+00 1.8649871e-20 1.3521769e-15 1.6587893e-23], sum to 1.0000
[2019-03-27 06:05:43,408] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9270
[2019-03-27 06:05:43,415] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5348318217707659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747361.497230726, 747361.497230726, 189234.8607281175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3232800.0000, 
sim time next is 3233400.0000, 
raw observation next is [29.16666666666667, 77.50000000000001, 1.0, 2.0, 0.5387596731589183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752852.1300982146, 752852.1300982139, 189892.6394897953], 
processed observation next is [0.0, 0.43478260869565216, 0.581358609794629, 0.7750000000000001, 1.0, 1.0, 0.4442887628420702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091255916939485, 0.20912559169394832, 0.2834218499847691], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.6399091], dtype=float32), -0.23788096]. 
=============================================
[2019-03-27 06:05:45,328] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.4626500e-15 1.0000000e+00 2.5644589e-19 2.2785696e-14 1.1543558e-24], sum to 1.0000
[2019-03-27 06:05:45,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1807
[2019-03-27 06:05:45,341] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 74.5, 1.0, 2.0, 0.5357670944871656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748668.886054139, 748668.886054139, 189390.4782181815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [29.66666666666667, 73.0, 1.0, 2.0, 0.5341717959314043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746438.8684780421, 746438.8684780427, 189123.865751679], 
processed observation next is [0.0, 0.43478260869565216, 0.6050552922590839, 0.73, 1.0, 1.0, 0.4387611999173545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20734413013278946, 0.20734413013278963, 0.2822744264950433], 
reward next is 0.7177, 
noisyNet noise sample is [array([-2.2181349], dtype=float32), 1.0635053]. 
=============================================
[2019-03-27 06:05:45,344] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.6130855e-16 1.0000000e+00 2.2653435e-22 2.9065046e-16 1.1992914e-25], sum to 1.0000
[2019-03-27 06:05:45,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3462
[2019-03-27 06:05:45,357] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.83333333333334, 64.33333333333333, 1.0, 2.0, 0.5874568731785758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820926.9322557616, 820926.9322557609, 198432.8783152514], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3255000.0000, 
sim time next is 3255600.0000, 
raw observation next is [32.66666666666667, 65.66666666666667, 1.0, 2.0, 0.5863683956701757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819405.2798309708, 819405.2798309708, 198234.6409096399], 
processed observation next is [0.0, 0.6956521739130435, 0.7472353870458138, 0.6566666666666667, 1.0, 1.0, 0.5016486694821394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22761257773082522, 0.22761257773082522, 0.29587259837259683], 
reward next is 0.7041, 
noisyNet noise sample is [array([0.42086568], dtype=float32), 1.9825481]. 
=============================================
[2019-03-27 06:05:45,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4352211e-15 1.0000000e+00 6.7822655e-21 2.4513232e-14 4.8571653e-24], sum to 1.0000
[2019-03-27 06:05:45,749] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5626
[2019-03-27 06:05:45,755] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.58333333333334, 80.66666666666666, 1.0, 2.0, 0.6873813027547047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 960626.9795823063, 960626.9795823058, 218141.3270338594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3556200.0000, 
sim time next is 3556800.0000, 
raw observation next is [26.5, 81.0, 1.0, 2.0, 0.6625600249346988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 925923.7032223347, 925923.7032223353, 212967.6112693294], 
processed observation next is [1.0, 0.17391304347826086, 0.4549763033175356, 0.81, 1.0, 1.0, 0.5934458131743359, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25720102867287076, 0.2572010286728709, 0.31786210637213347], 
reward next is 0.6821, 
noisyNet noise sample is [array([0.87342393], dtype=float32), 1.858973]. 
=============================================
[2019-03-27 06:05:48,340] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 06:05:48,344] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:05:48,346] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:05:48,347] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:05:48,347] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:05:48,349] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:05:48,350] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:05:48,348] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:05:48,352] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:05:48,354] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:05:48,353] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:05:48,376] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run30
[2019-03-27 06:05:48,376] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run30
[2019-03-27 06:05:48,377] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run30
[2019-03-27 06:05:48,377] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run30
[2019-03-27 06:05:48,445] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run30
[2019-03-27 06:05:52,504] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00502365], dtype=float32), 0.037425146]
[2019-03-27 06:05:52,507] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.279192765, 76.13109562833333, 1.0, 2.0, 0.3718432137577146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 560365.108056499, 560365.1080564997, 171153.7240430829]
[2019-03-27 06:05:52,508] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:05:52,510] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.3039348e-15 1.0000000e+00 5.1195643e-20 7.0920934e-15 4.3775690e-23], sampled 0.3069111037204755
[2019-03-27 06:05:53,493] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00502365], dtype=float32), 0.037425146]
[2019-03-27 06:05:53,493] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.223790115, 86.79889415, 1.0, 2.0, 0.252997094061003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 414262.4194142815, 414262.4194142821, 161201.0285039467]
[2019-03-27 06:05:53,495] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:05:53,498] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.1655406e-15 1.0000000e+00 9.1335060e-20 7.8842640e-15 1.2051910e-22], sampled 0.23407746848683653
[2019-03-27 06:06:47,415] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00502365], dtype=float32), 0.037425146]
[2019-03-27 06:06:47,417] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.33333333333333, 70.33333333333333, 1.0, 2.0, 0.6583944996374056, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.992969214381653, 6.9112, 168.9124016884308, 1816909.571679419, 1758899.867260818, 376384.8675446677]
[2019-03-27 06:06:47,418] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:06:47,421] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3248051e-11 1.0000000e+00 2.2213831e-15 3.8137690e-10 2.5796781e-18], sampled 0.4257025129520261
[2019-03-27 06:06:47,422] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1816909.571679419 W.
[2019-03-27 06:06:48,526] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00502365], dtype=float32), 0.037425146]
[2019-03-27 06:06:48,527] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 89.83333333333334, 1.0, 2.0, 0.7336170904601088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1025273.464897259, 1025273.464897259, 228269.2253772327]
[2019-03-27 06:06:48,528] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:06:48,529] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.9811012e-15 1.0000000e+00 1.1149402e-19 3.5000980e-14 3.0232214e-23], sampled 0.7375166317428831
[2019-03-27 06:07:05,283] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00502365], dtype=float32), 0.037425146]
[2019-03-27 06:07:05,284] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 67.0, 1.0, 2.0, 0.8792275902561141, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005986476991691, 6.9112, 168.9123159670406, 2125961.279734075, 2058716.745659778, 428348.7404083157]
[2019-03-27 06:07:05,284] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:07:05,286] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.2963532e-07 9.9802858e-01 3.9660830e-09 1.9708709e-03 4.7978188e-12], sampled 0.7721517832285538
[2019-03-27 06:07:05,288] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2125961.279734075 W.
[2019-03-27 06:07:06,653] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00502365], dtype=float32), 0.037425146]
[2019-03-27 06:07:06,654] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.9, 74.33333333333333, 1.0, 2.0, 0.5669159234226239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 792211.7905544989, 792211.7905544996, 194743.5652176811]
[2019-03-27 06:07:06,656] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:07:06,658] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0504845e-15 1.0000000e+00 1.0944555e-20 7.9979824e-15 2.4612099e-24], sampled 0.4174641782748949
[2019-03-27 06:07:43,086] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6842 2927263131.0635 1338.0000
[2019-03-27 06:07:43,555] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.8294 2842580827.1141 1129.0000
[2019-03-27 06:07:43,561] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.1741 3007774312.2770 1765.0000
[2019-03-27 06:07:43,585] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 06:07:43,644] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1085 3164104309.7194 1772.0000
[2019-03-27 06:07:44,659] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 725000, evaluation results [725000.0, 7884.108523542139, 3164104309.7193594, 1772.0, 8253.684238692544, 2927263131.0635424, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7997.1741155837, 3007774312.277008, 1765.0, 8494.829369639077, 2842580827.1141343, 1129.0]
[2019-03-27 06:07:48,562] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8602729e-16 1.0000000e+00 3.2367245e-20 1.0604703e-14 7.9995895e-25], sum to 1.0000
[2019-03-27 06:07:48,569] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7307
[2019-03-27 06:07:48,578] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5373605047599435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750896.2687323533, 750896.2687323539, 189656.6609110289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3365400.0000, 
sim time next is 3366000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5371847715771958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750650.6161097618, 750650.6161097625, 189627.1977992605], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 1.0, 1.0, 0.44239129105686237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2085140600304894, 0.20851406003048958, 0.28302566835710524], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.785862], dtype=float32), 0.21718846]. 
=============================================
[2019-03-27 06:07:48,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.11771 ]
 [78.11623 ]
 [78.10321 ]
 [78.070724]
 [78.03245 ]], R is [[78.09754944]
 [78.03350067]
 [77.96999359]
 [77.90699768]
 [77.84468079]].
[2019-03-27 06:07:51,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6691514e-12 1.0000000e+00 2.0137620e-15 7.4302009e-10 5.9699822e-21], sum to 1.0000
[2019-03-27 06:07:51,614] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2026
[2019-03-27 06:07:51,620] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 72.0, 1.0, 2.0, 0.538849562144199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752977.7837121148, 752977.7837121154, 189907.4777205666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3436200.0000, 
sim time next is 3436800.0000, 
raw observation next is [29.66666666666666, 72.66666666666666, 1.0, 2.0, 0.5334975255055133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745496.3285065243, 745496.3285065236, 189011.3577720589], 
processed observation next is [1.0, 0.782608695652174, 0.6050552922590835, 0.7266666666666666, 1.0, 1.0, 0.43794882591025697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20708231347403452, 0.20708231347403433, 0.28210650413740135], 
reward next is 0.7179, 
noisyNet noise sample is [array([-0.9345297], dtype=float32), 2.530196]. 
=============================================
[2019-03-27 06:08:09,410] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8130917e-13 1.0000000e+00 1.2669606e-17 1.0882455e-10 1.0600909e-20], sum to 1.0000
[2019-03-27 06:08:09,413] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7333
[2019-03-27 06:08:09,420] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 75.66666666666667, 1.0, 2.0, 0.6117656248956994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876283.5734749376, 876283.5734749376, 205718.1153941139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3730800.0000, 
sim time next is 3731400.0000, 
raw observation next is [26.5, 76.5, 1.0, 2.0, 0.6064170940601058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869819.2734582076, 869819.2734582076, 204832.4250598399], 
processed observation next is [1.0, 0.17391304347826086, 0.4549763033175356, 0.765, 1.0, 1.0, 0.52580372778326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2416164648495021, 0.2416164648495021, 0.3057200374027461], 
reward next is 0.6943, 
noisyNet noise sample is [array([-0.18538518], dtype=float32), 0.4101056]. 
=============================================
[2019-03-27 06:08:23,245] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7763659e-15 1.0000000e+00 2.0421853e-21 9.1770850e-14 2.1135115e-25], sum to 1.0000
[2019-03-27 06:08:23,257] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4035
[2019-03-27 06:08:23,262] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6210960742048425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867954.4007017325, 867954.4007017325, 204742.1950792598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3973200.0000, 
sim time next is 3973800.0000, 
raw observation next is [30.16666666666666, 83.16666666666666, 1.0, 2.0, 0.6205568352776938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867200.5304170754, 867200.5304170754, 204638.3574451312], 
processed observation next is [0.0, 1.0, 0.6287519747235385, 0.8316666666666666, 1.0, 1.0, 0.5428395605755346, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2408890362269654, 0.2408890362269654, 0.30543038424646446], 
reward next is 0.6946, 
noisyNet noise sample is [array([2.4201837], dtype=float32), 1.3161187]. 
=============================================
[2019-03-27 06:08:26,125] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6930728e-15 1.0000000e+00 2.4805859e-19 1.4018663e-12 2.4339388e-23], sum to 1.0000
[2019-03-27 06:08:26,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6398
[2019-03-27 06:08:26,140] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6182377847733076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863958.4425649181, 863958.4425649188, 204193.0585556088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4326000.0000, 
sim time next is 4326600.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6179569735819913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863565.8625013917, 863565.8625013924, 204139.2569754155], 
processed observation next is [1.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5397071970867365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2398794062503866, 0.23987940625038678, 0.30468545817226195], 
reward next is 0.6953, 
noisyNet noise sample is [array([1.2434154], dtype=float32), 0.5248422]. 
=============================================
[2019-03-27 06:08:28,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.9162861e-13 1.0000000e+00 2.7885218e-17 3.6526268e-11 4.9338315e-21], sum to 1.0000
[2019-03-27 06:08:28,309] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2154
[2019-03-27 06:08:28,313] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 87.0, 1.0, 2.0, 0.8924119824337585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1247329.107210734, 1247329.107210734, 267859.2786935231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4071600.0000, 
sim time next is 4072200.0000, 
raw observation next is [27.36666666666667, 87.16666666666667, 1.0, 2.0, 0.9615982584703593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1344092.35665427, 1344092.35665427, 287443.9991670955], 
processed observation next is [1.0, 0.13043478260869565, 0.49605055292259104, 0.8716666666666667, 1.0, 1.0, 0.9537328415305534, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3733589879595195, 0.3733589879595195, 0.42902089427924706], 
reward next is 0.5710, 
noisyNet noise sample is [array([-0.44800207], dtype=float32), -1.2716897]. 
=============================================
[2019-03-27 06:08:30,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3558445e-15 1.0000000e+00 1.1565720e-18 3.1629907e-11 1.1356618e-24], sum to 1.0000
[2019-03-27 06:08:30,344] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8774
[2019-03-27 06:08:30,348] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 78.33333333333334, 1.0, 2.0, 0.6175177135275672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862951.7678225458, 862951.7678225458, 204055.3345715984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4129800.0000, 
sim time next is 4130400.0000, 
raw observation next is [31.0, 77.66666666666667, 1.0, 2.0, 0.6134229029188494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857227.1546310324, 857227.1546310324, 203273.6847184631], 
processed observation next is [1.0, 0.8260869565217391, 0.6682464454976303, 0.7766666666666667, 1.0, 1.0, 0.5342444613480113, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23811865406417565, 0.23811865406417565, 0.3033935592812882], 
reward next is 0.6966, 
noisyNet noise sample is [array([-0.59983164], dtype=float32), 0.53458697]. 
=============================================
[2019-03-27 06:08:34,689] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.4183323e-14 1.0000000e+00 1.4277253e-16 5.1250983e-11 5.2758487e-22], sum to 1.0000
[2019-03-27 06:08:34,704] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5065
[2019-03-27 06:08:34,708] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.7562996446112641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1056989.458223951, 1056989.45822395, 233469.5705925064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4164600.0000, 
sim time next is 4165200.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.8130366652437334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1136326.4258456, 1136326.4258456, 247138.7285865187], 
processed observation next is [1.0, 0.21739130434782608, 0.5260663507109005, 0.89, 1.0, 1.0, 0.7747429701731727, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31564622940155557, 0.31564622940155557, 0.3688637740097294], 
reward next is 0.6311, 
noisyNet noise sample is [array([0.20430723], dtype=float32), 0.85691035]. 
=============================================
[2019-03-27 06:08:38,342] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 06:08:38,344] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:08:38,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:08:38,346] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:08:38,347] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:08:38,348] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:08:38,348] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:08:38,349] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:08:38,348] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:08:38,350] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:08:38,352] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:08:38,373] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run31
[2019-03-27 06:08:38,391] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run31
[2019-03-27 06:08:38,391] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run31
[2019-03-27 06:08:38,392] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run31
[2019-03-27 06:08:38,410] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run31
[2019-03-27 06:09:11,378] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00721403], dtype=float32), 0.037183363]
[2019-03-27 06:09:11,379] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.01666666666667, 65.83333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.311604781549409, 6.9112, 168.9102262268932, 2567996.051000183, 2283939.709618053, 475212.9682125036]
[2019-03-27 06:09:11,380] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:09:11,385] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.4015084e-08 9.9832088e-01 2.9447761e-10 1.6789662e-03 3.3946519e-13], sampled 0.7140575935522092
[2019-03-27 06:09:11,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2567996.051000183 W.
[2019-03-27 06:10:33,438] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9268 2927399763.0331 1338.0000
[2019-03-27 06:10:33,643] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.9417 2842597005.2612 1130.0000
[2019-03-27 06:10:33,690] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.4468 3163938350.2854 1773.0000
[2019-03-27 06:10:33,780] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2178 3007719775.7786 1766.0000
[2019-03-27 06:10:33,813] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.5064 2779204297.6294 933.0000
[2019-03-27 06:10:34,830] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 750000, evaluation results [750000.0, 7884.446771606816, 3163938350.2854357, 1773.0, 8252.926780629994, 2927399763.0331407, 1338.0, 8658.50640803287, 2779204297.6293545, 933.0, 7998.217808978373, 3007719775.7785764, 1766.0, 8495.941739145319, 2842597005.2612405, 1130.0]
[2019-03-27 06:10:41,147] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.0014416e-10 9.9998593e-01 1.2587374e-12 1.4094094e-05 9.6568456e-15], sum to 1.0000
[2019-03-27 06:10:41,159] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2870
[2019-03-27 06:10:41,166] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.5086911005916703, 1.0, 1.0, 0.5086911005916703, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1422108.472805729, 1422108.472805729, 302988.9624724696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4336800.0000, 
sim time next is 4337400.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.9722339185491939, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565099263, 1358968.069008637, 1358968.069008637, 290585.4720559362], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.89, 1.0, 1.0, 0.9665468898183059, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451498243, 0.37749113028017695, 0.37749113028017695, 0.4337096597849794], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4637333], dtype=float32), -0.6044706]. 
=============================================
[2019-03-27 06:10:41,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0414356e-12 9.9999988e-01 2.1745951e-14 7.6384531e-08 4.4383315e-18], sum to 1.0000
[2019-03-27 06:10:41,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8423
[2019-03-27 06:10:41,849] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2075380.082245389 W.
[2019-03-27 06:10:41,855] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.5, 84.0, 1.0, 2.0, 0.4947610780552695, 1.0, 2.0, 0.4947610780552695, 1.0, 1.0, 0.8592363725932302, 6.9112, 6.9112, 170.5573041426782, 2075380.082245389, 2075380.082245389, 411544.0297252427], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4347000.0000, 
sim time next is 4347600.0000, 
raw observation next is [30.66666666666666, 84.0, 1.0, 2.0, 0.6569826986010169, 1.0, 2.0, 0.6569826986010169, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1837031.170822581, 1837031.170822581, 356271.7800086886], 
processed observation next is [1.0, 0.30434782608695654, 0.6524486571879934, 0.84, 1.0, 1.0, 0.5867261428927913, 1.0, 1.0, 0.5867261428927913, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5102864363396058, 0.5102864363396058, 0.5317489253861024], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7271125], dtype=float32), 0.26583728]. 
=============================================
[2019-03-27 06:10:43,483] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.0365438e-16 1.0000000e+00 2.9757309e-20 1.4863023e-12 3.0013412e-24], sum to 1.0000
[2019-03-27 06:10:43,496] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3786
[2019-03-27 06:10:43,500] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5168451136438941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722218.7469001882, 722218.7469001882, 186277.7166453443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4736400.0000, 
sim time next is 4737000.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5165380532790991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721789.5272171806, 721789.5272171812, 186228.0238231255], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.417515726842288, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20049709089366127, 0.20049709089366144, 0.27795227436287384], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.03844025], dtype=float32), 0.45010865]. 
=============================================
[2019-03-27 06:10:43,519] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.43065 ]
 [73.329285]
 [72.92172 ]
 [72.27515 ]
 [71.97843 ]], R is [[73.05149841]
 [73.04296112]
 [73.0346756 ]
 [73.02655029]
 [73.01837158]].
[2019-03-27 06:10:48,618] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1397689e-16 1.0000000e+00 1.7212415e-21 1.6099643e-14 4.8860417e-25], sum to 1.0000
[2019-03-27 06:10:48,626] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9567
[2019-03-27 06:10:48,632] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 73.66666666666667, 1.0, 2.0, 0.6069456993342348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 848171.9795230635, 848171.9795230641, 202046.9249382585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4468800.0000, 
sim time next is 4469400.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5976065777655433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835115.9593134685, 835115.9593134685, 200301.1169543692], 
processed observation next is [0.0, 0.7391304347826086, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5151886479102931, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23197665536485237, 0.23197665536485237, 0.29895689097667044], 
reward next is 0.7010, 
noisyNet noise sample is [array([-0.5522433], dtype=float32), -0.022928668]. 
=============================================
[2019-03-27 06:10:50,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1619430e-14 1.0000000e+00 1.3822772e-19 5.0886219e-14 9.7409925e-25], sum to 1.0000
[2019-03-27 06:10:50,870] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2852
[2019-03-27 06:10:50,882] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 87.33333333333334, 1.0, 2.0, 0.5031500513758868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703075.4800365015, 703075.4800365015, 184090.356017219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4504800.0000, 
sim time next is 4505400.0000, 
raw observation next is [26.0, 86.5, 1.0, 2.0, 0.4995824983773147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 698088.7306454465, 698088.730645447, 183529.8919037986], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.865, 1.0, 1.0, 0.3970873474425478, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19391353629040178, 0.19391353629040195, 0.2739252117967143], 
reward next is 0.7261, 
noisyNet noise sample is [array([-0.6246769], dtype=float32), -1.3176931]. 
=============================================
[2019-03-27 06:11:11,476] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1890601e-06 8.1115371e-01 1.6150932e-07 1.8884392e-01 4.3142323e-11], sum to 1.0000
[2019-03-27 06:11:11,486] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8300
[2019-03-27 06:11:11,490] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7834560232199953, 1.0, 2.0, 0.7834560232199953, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2191032.923453474, 2191032.923453475, 411895.567975822], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8207617043436033, 1.0, 2.0, 0.8207617043436033, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2295462.052102628, 2295462.052102628, 430102.6257620162], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.7840502461971124, 1.0, 1.0, 0.7840502461971124, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6376283478062856, 0.6376283478062856, 0.6419442175552481], 
reward next is 0.3581, 
noisyNet noise sample is [array([0.03990684], dtype=float32), -0.057825163]. 
=============================================
[2019-03-27 06:11:20,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8849973e-15 1.0000000e+00 6.2107628e-20 1.4667346e-13 1.3387136e-24], sum to 1.0000
[2019-03-27 06:11:20,989] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1879
[2019-03-27 06:11:20,995] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 62.33333333333333, 1.0, 2.0, 0.5208317382904694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727791.405560802, 727791.4055608013, 186924.6560493465], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5068200.0000, 
sim time next is 5068800.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.5206810162924177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727580.7199703257, 727580.7199703263, 186900.042869508], 
processed observation next is [0.0, 0.6956521739130435, 0.6682464454976303, 0.63, 1.0, 1.0, 0.42250724854508154, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2021057555473127, 0.20210575554731286, 0.27895528786493734], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.29596266], dtype=float32), -1.710593]. 
=============================================
[2019-03-27 06:11:22,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2067635e-16 1.0000000e+00 1.9776945e-20 1.0772736e-13 1.9281814e-24], sum to 1.0000
[2019-03-27 06:11:22,152] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5750
[2019-03-27 06:11:22,155] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 62.33333333333333, 1.0, 2.0, 0.5346691542317358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747134.109469819, 747134.109469819, 189205.5921577475], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5062200.0000, 
sim time next is 5062800.0000, 
raw observation next is [31.33333333333334, 61.66666666666667, 1.0, 2.0, 0.5224315252581876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730027.658095248, 730027.6580952486, 187185.5399929009], 
processed observation next is [0.0, 0.6086956521739131, 0.6840442338072673, 0.6166666666666667, 1.0, 1.0, 0.42461629549179225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20278546058201335, 0.2027854605820135, 0.279381402974479], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.9042548], dtype=float32), -0.85497975]. 
=============================================
[2019-03-27 06:11:25,988] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6064107e-16 1.0000000e+00 2.2672911e-20 4.2191951e-14 5.1881875e-24], sum to 1.0000
[2019-03-27 06:11:25,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7030
[2019-03-27 06:11:25,999] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4835689770511917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675829.8095865631, 675829.8095865624, 181077.0919183193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5112000.0000, 
sim time next is 5112600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4830505893968829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675104.7403194243, 675104.740319425, 180998.4027749045], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3771693848155216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18752909453317343, 0.18752909453317362, 0.2701468698132903], 
reward next is 0.7299, 
noisyNet noise sample is [array([-1.3753568], dtype=float32), 0.23743205]. 
=============================================
[2019-03-27 06:11:28,241] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.0706108e-17 1.0000000e+00 2.8145495e-21 2.8738610e-15 1.4968871e-25], sum to 1.0000
[2019-03-27 06:11:28,252] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9709
[2019-03-27 06:11:28,258] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.55082529293436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769718.4977741354, 769718.4977741349, 191942.8620324664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5154600.0000, 
sim time next is 5155200.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5516185973084917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770827.45667694, 770827.4566769394, 192079.1430813605], 
processed observation next is [0.0, 0.6956521739130435, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45978144254035147, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21411873796581665, 0.21411873796581649, 0.2866852881811351], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.7892077], dtype=float32), 0.30183274]. 
=============================================
[2019-03-27 06:11:28,621] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 06:11:28,623] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:11:28,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:11:28,625] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:11:28,627] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:11:28,628] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:11:28,629] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:11:28,628] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:11:28,629] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:11:28,633] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:11:28,633] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:11:28,651] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run32
[2019-03-27 06:11:28,671] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run32
[2019-03-27 06:11:28,672] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run32
[2019-03-27 06:11:28,705] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run32
[2019-03-27 06:11:28,722] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run32
[2019-03-27 06:11:45,024] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00863469], dtype=float32), 0.038465593]
[2019-03-27 06:11:45,025] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.75, 95.0, 1.0, 2.0, 0.3074585171128207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487850.8660610176, 487850.8660610176, 166159.7398412576]
[2019-03-27 06:11:45,027] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:11:45,032] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.3747074e-15 1.0000000e+00 8.7047201e-20 9.6709580e-14 3.0119864e-23], sampled 0.43490386551101434
[2019-03-27 06:12:22,768] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00863469], dtype=float32), 0.038465593]
[2019-03-27 06:12:22,770] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.321397015, 66.22666405666666, 1.0, 2.0, 0.8494971386041771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1187313.257406838, 1187313.257406837, 256420.0612687067]
[2019-03-27 06:12:22,770] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:12:22,772] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.8815273e-15 1.0000000e+00 1.6761066e-19 9.1052914e-13 2.7948925e-23], sampled 0.43200375954992876
[2019-03-27 06:12:38,101] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00863469], dtype=float32), 0.038465593]
[2019-03-27 06:12:38,103] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.98333333333333, 80.33333333333333, 1.0, 2.0, 0.9050197838332417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1264961.597909072, 1264961.597909071, 271326.696569313]
[2019-03-27 06:12:38,104] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:12:38,106] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5840057e-15 1.0000000e+00 3.9434062e-20 5.3073729e-13 3.8567544e-24], sampled 0.819669517549276
[2019-03-27 06:12:59,133] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00863469], dtype=float32), 0.038465593]
[2019-03-27 06:12:59,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.380453945, 73.4354641, 1.0, 2.0, 0.4876150315514592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688450.9486468445, 688450.9486468445, 182589.0992701593]
[2019-03-27 06:12:59,136] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:12:59,138] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0538480e-15 1.0000000e+00 1.7831032e-20 1.2615713e-13 1.7794844e-24], sampled 0.8775083077769678
[2019-03-27 06:13:23,185] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.8346 3007664847.7666 1766.0000
[2019-03-27 06:13:23,440] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00863469], dtype=float32), 0.038465593]
[2019-03-27 06:13:23,441] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.30697563, 64.18923397, 1.0, 2.0, 0.4253883983636464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 633353.6751199226, 633353.6751199232, 177646.5103511314]
[2019-03-27 06:13:23,441] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:13:23,442] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0159531e-15 1.0000000e+00 1.4098460e-20 1.8666949e-13 1.1290184e-24], sampled 0.5660819004874729
[2019-03-27 06:13:23,683] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5193 2779288834.7305 933.0000
[2019-03-27 06:13:23,773] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.3786 2927520831.5885 1338.0000
[2019-03-27 06:13:23,787] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1110 2842752515.0984 1130.0000
[2019-03-27 06:13:23,843] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.7331 3164261484.6424 1776.0000
[2019-03-27 06:13:24,860] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 775000, evaluation results [775000.0, 7881.7331289299955, 3164261484.64235, 1776.0, 8253.378550137528, 2927520831.588529, 1338.0, 8660.519269005526, 2779288834.7304883, 933.0, 7996.834631788349, 3007664847.766646, 1766.0, 8496.11098891998, 2842752515.09836, 1130.0]
[2019-03-27 06:13:33,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8891468e-07 3.2501817e-01 2.5701092e-09 6.7498165e-01 3.6163276e-13], sum to 1.0000
[2019-03-27 06:13:33,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5171
[2019-03-27 06:13:33,202] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.15000000000001, 52.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.408047138820618, 6.9112, 170.5573041426782, 3265656.127458729, 2909744.311472089, 550970.7342926734], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5319000.0000, 
sim time next is 5319600.0000, 
raw observation next is [36.13333333333333, 52.66666666666666, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.604780485131712, 6.9112, 170.5573041426782, 3406748.401909456, 2909908.487359306, 549810.1503462374], 
processed observation next is [1.0, 0.5652173913043478, 0.9115323854660348, 0.5266666666666666, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.06935804851317115, 0.0, 0.8375144448122397, 0.9463190005304045, 0.8083079131553628, 0.8206121646958766], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2212766], dtype=float32), -1.790984]. 
=============================================
[2019-03-27 06:13:36,190] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2155518e-15 1.0000000e+00 9.7007784e-21 2.1975625e-13 5.5776302e-25], sum to 1.0000
[2019-03-27 06:13:36,197] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4046
[2019-03-27 06:13:36,202] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.43333333333334, 60.00000000000001, 1.0, 2.0, 0.5469084769276154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764243.2039768497, 764243.2039768504, 191272.2893454655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5667600.0000, 
sim time next is 5668200.0000, 
raw observation next is [32.4, 60.0, 1.0, 2.0, 0.5446811968666186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761129.7150157875, 761129.7150157881, 190893.1658854079], 
processed observation next is [0.0, 0.6086956521739131, 0.7345971563981042, 0.6, 1.0, 1.0, 0.4514231287549622, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21142492083771874, 0.2114249208377189, 0.2849151729632954], 
reward next is 0.7151, 
noisyNet noise sample is [array([-0.09810379], dtype=float32), -1.0038635]. 
=============================================
[2019-03-27 06:13:40,756] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.1461084e-15 1.0000000e+00 3.9813301e-20 1.4431932e-10 4.4072602e-23], sum to 1.0000
[2019-03-27 06:13:40,764] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4875
[2019-03-27 06:13:40,769] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 85.66666666666667, 1.0, 2.0, 0.5906428527188955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825380.8296423217, 825380.8296423217, 199015.6882775268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5443800.0000, 
sim time next is 5444400.0000, 
raw observation next is [28.9, 86.33333333333334, 1.0, 2.0, 0.5921734180530719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827520.5176142306, 827520.5176142306, 199296.8405240625], 
processed observation next is [1.0, 0.0, 0.5687203791469194, 0.8633333333333334, 1.0, 1.0, 0.5086426723530987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2298668104483974, 0.2298668104483974, 0.2974579709314366], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.77512974], dtype=float32), 0.29163703]. 
=============================================
[2019-03-27 06:13:46,553] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3208641e-11 1.0000000e+00 7.3148447e-16 8.6362489e-10 6.4062030e-20], sum to 1.0000
[2019-03-27 06:13:46,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2041
[2019-03-27 06:13:46,567] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 95.0, 1.0, 2.0, 0.9105049435848319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1272632.894257895, 1272632.894257895, 272843.5281920508], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5540400.0000, 
sim time next is 5541000.0000, 
raw observation next is [26.05, 95.0, 1.0, 2.0, 0.9738793817596663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1361269.537394623, 1361269.537394624, 291067.9874401698], 
processed observation next is [1.0, 0.13043478260869565, 0.43364928909952616, 0.95, 1.0, 1.0, 0.9685293756140557, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37813042705406197, 0.3781304270540622, 0.4344298320002534], 
reward next is 0.5656, 
noisyNet noise sample is [array([-0.83945984], dtype=float32), 0.016324634]. 
=============================================
[2019-03-27 06:13:46,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[57.002583]
 [56.03376 ]
 [55.424175]
 [55.27106 ]
 [59.401836]], R is [[57.61289597]
 [57.62953568]
 [57.64281845]
 [57.06639099]
 [56.49572754]].
[2019-03-27 06:13:55,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.8060028e-16 1.0000000e+00 5.0851692e-21 4.6422125e-14 1.5820603e-24], sum to 1.0000
[2019-03-27 06:13:55,480] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2482
[2019-03-27 06:13:55,490] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.15, 62.0, 1.0, 2.0, 0.5198103373176135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726363.6488525588, 726363.6488525594, 186758.3632864149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5740200.0000, 
sim time next is 5740800.0000, 
raw observation next is [31.3, 61.33333333333334, 1.0, 2.0, 0.5196873384969101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726191.7161211448, 726191.7161211448, 186738.4154674287], 
processed observation next is [0.0, 0.43478260869565216, 0.6824644549763034, 0.6133333333333334, 1.0, 1.0, 0.42131004638181935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20171992114476245, 0.20171992114476245, 0.2787140529364608], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.78628105], dtype=float32), 0.030812765]. 
=============================================
[2019-03-27 06:14:03,448] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.47501381e-10 9.99884725e-01 3.14607586e-14 1.15299685e-04
 5.80931326e-17], sum to 1.0000
[2019-03-27 06:14:03,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0331
[2019-03-27 06:14:03,462] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.4, 68.0, 1.0, 2.0, 0.5140030562933241, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718246.026678661, 718246.0266786617, 185823.2611544366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5851800.0000, 
sim time next is 5852400.0000, 
raw observation next is [31.2, 69.0, 1.0, 2.0, 0.5264992737826555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735713.7569650905, 735713.7569650899, 187855.3382799121], 
processed observation next is [1.0, 0.7391304347826086, 0.6777251184834123, 0.69, 1.0, 1.0, 0.4295171973285006, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20436493249030294, 0.20436493249030277, 0.28038110191031657], 
reward next is 0.7196, 
noisyNet noise sample is [array([1.001815], dtype=float32), -0.99134135]. 
=============================================
[2019-03-27 06:14:05,628] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2282054e-13 1.0000000e+00 1.9258864e-18 7.9328821e-11 3.3827621e-23], sum to 1.0000
[2019-03-27 06:14:05,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4369
[2019-03-27 06:14:05,645] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 92.66666666666667, 1.0, 2.0, 0.5262040121936707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735301.0247756034, 735301.0247756029, 187803.544846052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5880000.0000, 
sim time next is 5880600.0000, 
raw observation next is [26.15, 93.0, 1.0, 2.0, 0.5257780160128267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734705.5451753035, 734705.545175304, 187733.5330950924], 
processed observation next is [1.0, 0.043478260869565216, 0.43838862559241704, 0.93, 1.0, 1.0, 0.4286482120636466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20408487365980651, 0.20408487365980668, 0.2801993031270036], 
reward next is 0.7198, 
noisyNet noise sample is [array([0.2899471], dtype=float32), 2.1791909]. 
=============================================
[2019-03-27 06:14:13,676] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4358336e-15 1.0000000e+00 2.6077551e-20 1.6295226e-11 7.9210327e-25], sum to 1.0000
[2019-03-27 06:14:13,684] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1729
[2019-03-27 06:14:13,687] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 86.0, 1.0, 2.0, 0.5387169063691704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752792.347454091, 752792.3474540904, 189884.8297704713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6037200.0000, 
sim time next is 6037800.0000, 
raw observation next is [27.53333333333334, 86.33333333333334, 1.0, 2.0, 0.5393763918031707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753714.2263053434, 753714.2263053434, 189995.6976512825], 
processed observation next is [1.0, 0.9130434782608695, 0.5039494470774094, 0.8633333333333334, 1.0, 1.0, 0.44503179735321763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2093650628625954, 0.2093650628625954, 0.2835756681362425], 
reward next is 0.7164, 
noisyNet noise sample is [array([2.081627], dtype=float32), -0.3236972]. 
=============================================
[2019-03-27 06:14:15,980] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5994861e-13 1.0000000e+00 1.2331480e-18 3.1685474e-10 3.7220197e-22], sum to 1.0000
[2019-03-27 06:14:15,992] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3209
[2019-03-27 06:14:15,997] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.18333333333333, 93.0, 1.0, 2.0, 0.7186882424336557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1004399.64630749, 1004399.64630749, 224930.2274513614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6059400.0000, 
sim time next is 6060000.0000, 
raw observation next is [26.16666666666667, 93.0, 1.0, 2.0, 0.6952603385547894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 971643.1022969646, 971643.1022969646, 219824.931110276], 
processed observation next is [1.0, 0.13043478260869565, 0.4391785150078992, 0.93, 1.0, 1.0, 0.6328437813913125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26990086174915684, 0.26990086174915684, 0.32809691210488956], 
reward next is 0.6719, 
noisyNet noise sample is [array([2.3690639], dtype=float32), 0.6577355]. 
=============================================
[2019-03-27 06:14:16,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[64.861725]
 [64.614006]
 [64.50812 ]
 [64.38483 ]
 [64.2802  ]], R is [[64.81604004]
 [64.83216095]
 [64.83442688]
 [64.83569336]
 [64.82060242]].
[2019-03-27 06:14:18,766] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 06:14:18,767] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:14:18,767] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:14:18,768] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:14:18,769] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:14:18,770] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:14:18,770] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:14:18,771] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:14:18,769] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:14:18,774] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:14:18,775] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:14:18,800] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run33
[2019-03-27 06:14:18,801] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run33
[2019-03-27 06:14:18,819] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run33
[2019-03-27 06:14:18,839] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run33
[2019-03-27 06:14:18,882] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run33
[2019-03-27 06:14:31,102] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01062933], dtype=float32), 0.03843369]
[2019-03-27 06:14:31,103] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.11666666666667, 82.66666666666667, 1.0, 2.0, 0.4319477688662558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 630870.8440993034, 630870.8440993034, 177108.9180947499]
[2019-03-27 06:14:31,106] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:14:31,109] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2900350e-15 1.0000000e+00 1.3454184e-19 3.5658737e-13 2.2953703e-23], sampled 0.1477725637333448
[2019-03-27 06:14:33,304] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01062933], dtype=float32), 0.03843369]
[2019-03-27 06:14:33,306] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.5, 88.33333333333333, 1.0, 2.0, 0.3383860009893634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524331.8892428037, 524331.8892428043, 168640.3182976949]
[2019-03-27 06:14:33,308] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:14:33,312] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8731913e-15 1.0000000e+00 3.6274768e-20 1.7924697e-13 5.1789728e-24], sampled 0.9804782401364072
[2019-03-27 06:15:18,978] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01062933], dtype=float32), 0.03843369]
[2019-03-27 06:15:18,978] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.10201828166667, 95.70313404166666, 1.0, 2.0, 0.9966641265269918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1393138.504973975, 1393138.504973975, 297922.2671704975]
[2019-03-27 06:15:18,979] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:15:18,982] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.1267526e-15 1.0000000e+00 2.3479447e-19 1.6795560e-12 2.4450315e-23], sampled 0.9504762637894211
[2019-03-27 06:15:28,617] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01062933], dtype=float32), 0.03843369]
[2019-03-27 06:15:28,619] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.85245077333334, 70.02689601, 1.0, 2.0, 0.4372953322124287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 623708.6085702629, 623708.6085702629, 175994.9578535746]
[2019-03-27 06:15:28,620] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:15:28,623] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.2411468e-13 1.0000000e+00 2.0682338e-17 7.7983919e-10 9.4058988e-22], sampled 0.20375731542841025
[2019-03-27 06:15:37,985] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01062933], dtype=float32), 0.03843369]
[2019-03-27 06:15:37,990] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.06509782333333, 87.79827581666666, 1.0, 2.0, 0.8389057417670449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1172501.848883661, 1172501.848883661, 253689.9535622432]
[2019-03-27 06:15:37,992] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:15:37,996] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.3638816e-15 1.0000000e+00 9.8635142e-20 1.5411650e-12 6.7085347e-24], sampled 0.34176573583315495
[2019-03-27 06:15:43,723] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01062933], dtype=float32), 0.03843369]
[2019-03-27 06:15:43,725] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.46666666666667, 76.66666666666667, 1.0, 2.0, 0.5988648228074902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 836874.9668616236, 836874.966861623, 200534.1516583224]
[2019-03-27 06:15:43,726] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:15:43,728] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.7257157e-16 1.0000000e+00 1.1421419e-20 3.2415547e-13 6.6640997e-25], sampled 0.4272797239308096
[2019-03-27 06:16:13,628] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9767 2779207024.4606 933.0000
[2019-03-27 06:16:14,080] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.1475 2842325823.7453 1126.0000
[2019-03-27 06:16:14,208] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6749 2927578955.4543 1338.0000
[2019-03-27 06:16:14,219] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9941 3007482371.7761 1762.0000
[2019-03-27 06:16:14,256] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.5705 3164045445.4889 1773.0000
[2019-03-27 06:16:15,268] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 800000, evaluation results [800000.0, 7883.570487957822, 3164045445.488865, 1773.0, 8254.674855885642, 2927578955.4543257, 1338.0, 8659.976662972434, 2779207024.4606214, 933.0, 7998.994070898481, 3007482371.7761426, 1762.0, 8497.147521539913, 2842325823.7452893, 1126.0]
[2019-03-27 06:16:17,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9920732e-07 8.9146501e-01 7.0078415e-10 1.0853484e-01 1.5463385e-12], sum to 1.0000
[2019-03-27 06:16:17,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7191
[2019-03-27 06:16:17,729] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2210819.416516066 W.
[2019-03-27 06:16:17,734] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.88333333333334, 73.5, 1.0, 2.0, 0.7905238610865972, 1.0, 1.0, 0.7905238610865972, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2210819.416516066, 2210819.416516066, 415284.0971136523], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6178200.0000, 
sim time next is 6178800.0000, 
raw observation next is [29.96666666666667, 73.0, 1.0, 2.0, 0.6515578111669662, 1.0, 2.0, 0.6515578111669662, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1821849.401686162, 1821849.401686162, 354078.0085890845], 
processed observation next is [1.0, 0.5217391304347826, 0.6192733017377569, 0.73, 1.0, 1.0, 0.5801901339361039, 1.0, 1.0, 0.5801901339361039, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5060692782461561, 0.5060692782461561, 0.5284746396852007], 
reward next is 0.4715, 
noisyNet noise sample is [array([-0.16084825], dtype=float32), -0.33682683]. 
=============================================
[2019-03-27 06:16:18,127] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.1839541e-07 4.3370560e-01 3.4923758e-08 5.6629348e-01 4.7787490e-12], sum to 1.0000
[2019-03-27 06:16:18,133] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4991493e-12 1.0000000e+00 4.9643694e-17 1.8243760e-08 2.3847258e-21], sum to 1.0000
[2019-03-27 06:16:18,133] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8194
[2019-03-27 06:16:18,134] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7778
[2019-03-27 06:16:18,138] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.2, 70.5, 1.0, 2.0, 0.5075011301951279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709157.4787359153, 709157.4787359147, 184779.8942073489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6459000.0000, 
sim time next is 6459600.0000, 
raw observation next is [29.1, 71.0, 1.0, 2.0, 0.5080060797651659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709863.3065473617, 709863.3065473617, 184860.1133665758], 
processed observation next is [1.0, 0.782608695652174, 0.5781990521327015, 0.71, 1.0, 1.0, 0.40723624068092273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19718425181871158, 0.19718425181871158, 0.2759106169650385], 
reward next is 0.7241, 
noisyNet noise sample is [array([1.6424174], dtype=float32), -0.1858309]. 
=============================================
[2019-03-27 06:16:18,143] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.7, 68.5, 1.0, 2.0, 0.8521206336525884, 1.0, 2.0, 0.8521206336525884, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2383248.642487893, 2383248.642487893, 446040.8440274041], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6185400.0000, 
sim time next is 6186000.0000, 
raw observation next is [30.6, 69.0, 1.0, 2.0, 0.841577885443284, 1.0, 2.0, 0.841577885443284, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2353734.449312922, 2353734.449312923, 440618.4419997997], 
processed observation next is [1.0, 0.6086956521739131, 0.6492890995260664, 0.69, 1.0, 1.0, 0.809129982461788, 1.0, 1.0, 0.809129982461788, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.653815124809145, 0.6538151248091453, 0.6576394656713428], 
reward next is 0.3424, 
noisyNet noise sample is [array([-1.4279959], dtype=float32), -0.052027542]. 
=============================================
[2019-03-27 06:16:18,158] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[28.390535]
 [27.340635]
 [30.860939]
 [30.533669]
 [33.26197 ]], R is [[28.84182549]
 [28.88767433]
 [28.59879875]
 [28.3128109 ]
 [28.02968216]].
[2019-03-27 06:16:19,988] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2236475e-14 1.0000000e+00 6.2344799e-19 1.2803042e-10 1.9665434e-22], sum to 1.0000
[2019-03-27 06:16:20,000] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1271
[2019-03-27 06:16:20,006] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 89.5, 1.0, 2.0, 0.5209585752752517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727968.7037008121, 727968.7037008121, 186945.2724549527], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6222600.0000, 
sim time next is 6223200.0000, 
raw observation next is [26.56666666666666, 89.66666666666667, 1.0, 2.0, 0.5205903980642519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727454.0499943167, 727454.0499943167, 186885.3165544713], 
processed observation next is [0.0, 0.0, 0.4581358609794626, 0.8966666666666667, 1.0, 1.0, 0.42239806995692997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20207056944286575, 0.20207056944286575, 0.27893330829025564], 
reward next is 0.7211, 
noisyNet noise sample is [array([-0.26572186], dtype=float32), -0.5600033]. 
=============================================
[2019-03-27 06:16:21,817] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0349622e-15 1.0000000e+00 3.8920114e-20 8.6278017e-14 1.5339062e-24], sum to 1.0000
[2019-03-27 06:16:21,826] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3979
[2019-03-27 06:16:21,832] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.55, 74.83333333333334, 1.0, 2.0, 0.5418757336280602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757208.0034848315, 757208.0034848315, 190417.7333256967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6256200.0000, 
sim time next is 6256800.0000, 
raw observation next is [29.7, 74.0, 1.0, 2.0, 0.5419591470242147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757324.6055087395, 757324.6055087395, 190431.8515311663], 
processed observation next is [0.0, 0.43478260869565216, 0.6066350710900474, 0.74, 1.0, 1.0, 0.4481435506315839, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21036794597464986, 0.21036794597464986, 0.2842266440763676], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.6034235], dtype=float32), -1.1650014]. 
=============================================
[2019-03-27 06:16:27,483] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0269285e-13 1.0000000e+00 8.4184223e-19 6.4773933e-12 2.5077732e-23], sum to 1.0000
[2019-03-27 06:16:27,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8080
[2019-03-27 06:16:27,503] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 95.0, 1.0, 2.0, 0.6050253766004485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845487.3699859132, 845487.3699859132, 201677.6649269905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6664800.0000, 
sim time next is 6665400.0000, 
raw observation next is [24.85, 95.0, 1.0, 2.0, 0.6174718364561248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862887.6307443506, 862887.6307443506, 204037.042439798], 
processed observation next is [1.0, 0.13043478260869565, 0.37677725118483424, 0.95, 1.0, 1.0, 0.5391226945254516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2396910085400974, 0.2396910085400974, 0.3045328991638776], 
reward next is 0.6955, 
noisyNet noise sample is [array([0.70275235], dtype=float32), -1.0874933]. 
=============================================
[2019-03-27 06:16:28,627] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8690524e-14 1.0000000e+00 2.6453866e-21 4.1245717e-14 2.9268971e-25], sum to 1.0000
[2019-03-27 06:16:28,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0469
[2019-03-27 06:16:28,636] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 77.66666666666667, 1.0, 2.0, 0.5243435756053749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 732700.4122814663, 732700.4122814657, 187497.7452966479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6374400.0000, 
sim time next is 6375000.0000, 
raw observation next is [28.16666666666667, 78.33333333333334, 1.0, 2.0, 0.5261262978023042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735192.3915356952, 735192.3915356958, 187790.3831392881], 
processed observation next is [0.0, 0.782608695652174, 0.5339652448657191, 0.7833333333333334, 1.0, 1.0, 0.42906782867747495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20422010875991534, 0.20422010875991548, 0.280284153939236], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.76222336], dtype=float32), -0.31247082]. 
=============================================
[2019-03-27 06:16:28,654] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.24941 ]
 [75.22474 ]
 [75.17606 ]
 [75.13359 ]
 [75.082214]], R is [[75.23712921]
 [75.20491028]
 [75.17356873]
 [75.14309692]
 [75.11335754]].
[2019-03-27 06:16:29,383] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9584311e-10 9.9984467e-01 1.1305543e-12 1.5531390e-04 5.1549133e-16], sum to 1.0000
[2019-03-27 06:16:29,391] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6467
[2019-03-27 06:16:29,399] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2249113.989536808 W.
[2019-03-27 06:16:29,403] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 62.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.031596775236231, 6.9112, 168.9070445886007, 2249113.989536808, 1454293.775635456, 311347.5226329364], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6700800.0000, 
sim time next is 6701400.0000, 
raw observation next is [30.05, 62.0, 1.0, 2.0, 0.7290630204550836, 1.0, 1.0, 0.7290630204550836, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2038771.317736335, 2038771.317736334, 386803.0497115489], 
processed observation next is [1.0, 0.5652173913043478, 0.6232227488151659, 0.62, 1.0, 1.0, 0.6735699041627513, 1.0, 0.5, 0.6735699041627513, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5663253660378709, 0.5663253660378705, 0.5773179846441028], 
reward next is 0.4227, 
noisyNet noise sample is [array([0.12783045], dtype=float32), 0.09718194]. 
=============================================
[2019-03-27 06:16:31,105] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2565078e-15 1.0000000e+00 1.0007879e-19 1.6103468e-12 1.0984812e-24], sum to 1.0000
[2019-03-27 06:16:31,114] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4757
[2019-03-27 06:16:31,117] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 82.0, 1.0, 2.0, 0.5246813269307252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733172.5377181085, 733172.5377181091, 187552.9600008283], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6382800.0000, 
sim time next is 6383400.0000, 
raw observation next is [27.48333333333333, 82.00000000000001, 1.0, 2.0, 0.5244402931836308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732835.608854199, 732835.608854199, 187513.4122890611], 
processed observation next is [0.0, 0.9130434782608695, 0.5015797788309636, 0.8200000000000002, 1.0, 1.0, 0.4270364978116034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20356544690394415, 0.20356544690394415, 0.27987076461053895], 
reward next is 0.7201, 
noisyNet noise sample is [array([-1.0413551], dtype=float32), 1.1283824]. 
=============================================
[2019-03-27 06:16:31,750] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.2075062e-15 1.0000000e+00 2.3547805e-18 5.9648904e-11 7.6534749e-23], sum to 1.0000
[2019-03-27 06:16:31,757] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9966
[2019-03-27 06:16:31,761] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 80.0, 1.0, 2.0, 0.7369009601951348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029865.093325775, 1029865.093325775, 229010.8452252608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6422400.0000, 
sim time next is 6423000.0000, 
raw observation next is [28.0, 79.5, 1.0, 2.0, 0.8306915036831023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161014.891009867, 1161014.891009867, 251582.8259627409], 
processed observation next is [1.0, 0.34782608695652173, 0.5260663507109005, 0.795, 1.0, 1.0, 0.7960138598591594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3225041363916297, 0.3225041363916297, 0.37549675516827], 
reward next is 0.6245, 
noisyNet noise sample is [array([2.1719933], dtype=float32), 1.2211972]. 
=============================================
[2019-03-27 06:16:31,771] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[65.49424 ]
 [65.39408 ]
 [65.341156]
 [65.27651 ]
 [65.20339 ]], R is [[65.39292908]
 [65.39719391]
 [65.40470886]
 [65.40870667]
 [65.40016937]].
[2019-03-27 06:16:32,140] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1514818e-07 9.6409798e-01 3.0188438e-10 3.5901703e-02 9.9473359e-13], sum to 1.0000
[2019-03-27 06:16:32,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8888
[2019-03-27 06:16:32,151] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2109726.06785802 W.
[2019-03-27 06:16:32,155] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.2, 73.0, 1.0, 2.0, 0.8676283121723236, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98314259685851, 6.9112, 168.9125282151283, 2109726.06785802, 2058687.642986743, 426059.9329739875], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6429600.0000, 
sim time next is 6430200.0000, 
raw observation next is [29.3, 72.33333333333334, 1.0, 2.0, 0.8188993483467506, 1.0, 1.0, 0.8188993483467506, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2290248.74273632, 2290248.742736321, 429170.9487351993], 
processed observation next is [1.0, 0.43478260869565216, 0.5876777251184835, 0.7233333333333334, 1.0, 1.0, 0.7818064437912657, 1.0, 0.5, 0.7818064437912657, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6361802063156444, 0.6361802063156448, 0.6405536548286557], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6397958], dtype=float32), -2.0578153]. 
=============================================
[2019-03-27 06:16:38,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.09463706e-07 8.80579233e-01 8.83559836e-10 1.19420663e-01
 1.19750152e-12], sum to 1.0000
[2019-03-27 06:16:38,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1124
[2019-03-27 06:16:38,694] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1924438.496223534 W.
[2019-03-27 06:16:38,699] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.45, 61.5, 1.0, 2.0, 0.6882143816198573, 1.0, 1.0, 0.6882143816198573, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1924438.496223534, 1924438.496223534, 369110.886809706], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6517800.0000, 
sim time next is 6518400.0000, 
raw observation next is [30.66666666666667, 60.0, 1.0, 2.0, 0.7712674915081664, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.968130218431442, 6.9112, 168.9126173931066, 1974861.92576938, 1934473.750512512, 402112.8360532122], 
processed observation next is [1.0, 0.43478260869565216, 0.6524486571879939, 0.6, 1.0, 1.0, 0.7244186644676703, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.005693021843144219, 0.0, 0.8294382799309912, 0.5485727571581611, 0.5373538195868088, 0.6001684120197197], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46267366], dtype=float32), -0.6446154]. 
=============================================
[2019-03-27 06:16:39,269] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2676749e-14 1.0000000e+00 2.1111650e-19 1.6741380e-12 1.0817938e-23], sum to 1.0000
[2019-03-27 06:16:39,276] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6857
[2019-03-27 06:16:39,280] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.93333333333333, 50.66666666666667, 1.0, 2.0, 0.3428301808668725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529361.4641478624, 529361.4641478618, 168987.8760007616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6888000.0000, 
sim time next is 6888600.0000, 
raw observation next is [28.8, 51.5, 1.0, 2.0, 0.3473424583430342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535688.6631004182, 535688.6631004182, 169481.8156276213], 
processed observation next is [0.0, 0.7391304347826086, 0.5639810426540285, 0.515, 1.0, 1.0, 0.21366561246148696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14880240641678283, 0.14880240641678283, 0.25295793377256914], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.9188438], dtype=float32), -0.3069414]. 
=============================================
[2019-03-27 06:16:39,584] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.0638730e-09 9.4687670e-01 1.3661462e-09 5.3123299e-02 1.6125950e-14], sum to 1.0000
[2019-03-27 06:16:39,595] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1768
[2019-03-27 06:16:39,601] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2074934.739041447 W.
[2019-03-27 06:16:39,607] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.6, 61.0, 1.0, 2.0, 0.7419825195036076, 1.0, 2.0, 0.7419825195036076, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2074934.739041447, 2074934.739041447, 392605.0187532002], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6537600.0000, 
sim time next is 6538200.0000, 
raw observation next is [30.48333333333333, 61.66666666666667, 1.0, 2.0, 0.7560230996789009, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.970098103185552, 6.9112, 168.9126050880591, 1953527.835338806, 1911743.580754775, 398466.4775806904], 
processed observation next is [1.0, 0.6956521739130435, 0.6437598736176934, 0.6166666666666667, 1.0, 1.0, 0.706051927323977, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0058898103185551776, 0.0, 0.8294382195075748, 0.5426466209274461, 0.531039883542993, 0.5947260859413289], 
reward next is 0.1108, 
noisyNet noise sample is [array([1.1854793], dtype=float32), 0.36767834]. 
=============================================
[2019-03-27 06:16:43,315] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0976886e-07 7.4227279e-01 2.0832583e-10 2.5772709e-01 5.4243160e-14], sum to 1.0000
[2019-03-27 06:16:43,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3153
[2019-03-27 06:16:43,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2382153.943762465 W.
[2019-03-27 06:16:43,335] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333334, 61.00000000000001, 1.0, 2.0, 0.8517296013880585, 1.0, 2.0, 0.8517296013880585, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2382153.943762465, 2382153.943762465, 445833.225108309], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6618000.0000, 
sim time next is 6618600.0000, 
raw observation next is [31.25, 61.5, 1.0, 2.0, 0.8258025239599235, 1.0, 2.0, 0.8258025239599235, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2309572.971465984, 2309572.971465984, 432621.4313918042], 
processed observation next is [1.0, 0.6086956521739131, 0.6800947867298578, 0.615, 1.0, 1.0, 0.7901235228432812, 1.0, 1.0, 0.7901235228432812, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.64154804762944, 0.64154804762944, 0.6457036289429913], 
reward next is 0.3543, 
noisyNet noise sample is [array([1.1394814], dtype=float32), 0.58030605]. 
=============================================
[2019-03-27 06:16:46,085] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1973962e-14 1.0000000e+00 8.5931105e-19 1.8264064e-11 2.6080297e-23], sum to 1.0000
[2019-03-27 06:16:46,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9168
[2019-03-27 06:16:46,094] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 95.0, 1.0, 2.0, 0.6050253766004485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845487.3699859132, 845487.3699859132, 201677.6649269905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6664800.0000, 
sim time next is 6665400.0000, 
raw observation next is [24.85, 95.0, 1.0, 2.0, 0.6174718364561248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862887.6307443506, 862887.6307443506, 204037.042439798], 
processed observation next is [1.0, 0.13043478260869565, 0.37677725118483424, 0.95, 1.0, 1.0, 0.5391226945254516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2396910085400974, 0.2396910085400974, 0.3045328991638776], 
reward next is 0.6955, 
noisyNet noise sample is [array([0.16748472], dtype=float32), 0.5143228]. 
=============================================
[2019-03-27 06:16:47,562] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7768140e-08 9.6947539e-01 1.1620278e-09 3.0524593e-02 3.2859562e-14], sum to 1.0000
[2019-03-27 06:16:47,570] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1633
[2019-03-27 06:16:47,580] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2094904.326369781 W.
[2019-03-27 06:16:47,586] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.76666666666667, 65.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.81435489451818, 6.9112, 168.9080341229995, 2094904.326369781, 1454193.831854932, 311348.0193135719], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6698400.0000, 
sim time next is 6699000.0000, 
raw observation next is [29.83333333333333, 64.66666666666667, 1.0, 2.0, 0.6327759331806876, 1.0, 1.0, 0.6327759331806876, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1769289.210236836, 1769289.210236836, 346665.0311969622], 
processed observation next is [1.0, 0.5217391304347826, 0.6129541864139019, 0.6466666666666667, 1.0, 1.0, 0.5575613652779369, 1.0, 0.5, 0.5575613652779369, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4914692250657878, 0.4914692250657878, 0.5174104943238242], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.20689902], dtype=float32), -0.9275283]. 
=============================================
[2019-03-27 06:16:47,599] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[37.840683]
 [32.931747]
 [31.01677 ]
 [28.670507]
 [28.746702]], R is [[38.51832199]
 [38.13314056]
 [38.25845337]
 [38.35048294]
 [37.96697998]].
[2019-03-27 06:16:53,479] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3253162e-15 1.0000000e+00 1.5777965e-20 6.3560636e-11 1.1412697e-22], sum to 1.0000
[2019-03-27 06:16:53,489] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6541
[2019-03-27 06:16:53,494] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.63333333333333, 70.33333333333334, 1.0, 2.0, 0.3308280593349421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 518461.7341679638, 518461.7341679644, 168336.370314148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6820800.0000, 
sim time next is 6821400.0000, 
raw observation next is [24.51666666666667, 71.16666666666666, 1.0, 2.0, 0.3309378349317248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518444.9361335237, 518444.9361335237, 168330.5553564526], 
processed observation next is [1.0, 0.9565217391304348, 0.36097946287519767, 0.7116666666666666, 1.0, 1.0, 0.1939010059418371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14401248225931215, 0.14401248225931215, 0.251239634860377], 
reward next is 0.7488, 
noisyNet noise sample is [array([-0.3372499], dtype=float32), -0.8316694]. 
=============================================
[2019-03-27 06:17:03,312] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4492850e-16 1.0000000e+00 4.3111342e-22 8.1713343e-14 1.5794304e-25], sum to 1.0000
[2019-03-27 06:17:03,321] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0871
[2019-03-27 06:17:03,328] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 62.0, 1.0, 2.0, 0.399429641364331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594224.82345362, 594224.82345362, 173953.8014515109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6987600.0000, 
sim time next is 6988200.0000, 
raw observation next is [27.88333333333334, 63.16666666666667, 1.0, 2.0, 0.399229534664357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591997.600673984, 591997.6006739847, 173689.046321372], 
processed observation next is [0.0, 0.9130434782608695, 0.520537124802528, 0.6316666666666667, 1.0, 1.0, 0.2761801622462133, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16444377796499557, 0.16444377796499576, 0.25923738256921197], 
reward next is 0.7408, 
noisyNet noise sample is [array([-0.08120576], dtype=float32), 1.2408165]. 
=============================================
[2019-03-27 06:17:04,599] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0222539e-14 1.0000000e+00 5.1734521e-21 1.8719442e-12 1.4645914e-23], sum to 1.0000
[2019-03-27 06:17:04,606] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9232
[2019-03-27 06:17:04,613] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 76.33333333333334, 1.0, 2.0, 0.4428623873691883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634940.0951931007, 634940.0951931007, 177204.2987184869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6995400.0000, 
sim time next is 6996000.0000, 
raw observation next is [26.5, 76.66666666666667, 1.0, 2.0, 0.4449989567746867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 637915.7614895086, 637915.7614895086, 177500.9528324651], 
processed observation next is [0.0, 1.0, 0.4549763033175356, 0.7666666666666667, 1.0, 1.0, 0.3313240443068514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1771988226359746, 0.1771988226359746, 0.264926795272336], 
reward next is 0.7351, 
noisyNet noise sample is [array([-0.2056183], dtype=float32), 0.42226508]. 
=============================================
[2019-03-27 06:17:04,632] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.33154 ]
 [79.25356 ]
 [79.174324]
 [79.13264 ]
 [79.08703 ]], R is [[79.33591461]
 [79.27807617]
 [79.22016907]
 [79.16320801]
 [79.10723114]].
[2019-03-27 06:17:06,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1736371e-11 9.9999893e-01 2.1576084e-14 1.0636630e-06 1.1227281e-19], sum to 1.0000
[2019-03-27 06:17:06,205] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5665
[2019-03-27 06:17:06,211] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 60.66666666666667, 1.0, 2.0, 0.9441274718808699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104151, 1425410.982888581, 1425410.982888581, 297737.5913253675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7312800.0000, 
sim time next is 7313400.0000, 
raw observation next is [27.7, 61.0, 1.0, 2.0, 0.9084325580752467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1370361.680236845, 1370361.680236845, 286622.5058487903], 
processed observation next is [1.0, 0.6521739130434783, 0.5118483412322274, 0.61, 1.0, 1.0, 0.8896777808135502, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3806560222880125, 0.3806560222880125, 0.42779478484894073], 
reward next is 0.5722, 
noisyNet noise sample is [array([0.49350345], dtype=float32), 1.3995056]. 
=============================================
[2019-03-27 06:17:07,797] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 06:17:07,798] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:17:07,799] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:17:07,800] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:17:07,800] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:17:07,800] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:17:07,801] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:17:07,802] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:17:07,803] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:17:07,800] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:17:07,808] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:17:07,829] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run34
[2019-03-27 06:17:07,850] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run34
[2019-03-27 06:17:07,851] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run34
[2019-03-27 06:17:07,871] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run34
[2019-03-27 06:17:07,890] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run34
[2019-03-27 06:17:22,098] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:17:22,099] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.0, 60.0, 1.0, 2.0, 0.3483059861418489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538076.0946659121, 538076.0946659115, 169703.151464087]
[2019-03-27 06:17:22,102] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:17:22,107] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1750852e-15 1.0000000e+00 5.7075844e-20 4.2713345e-13 6.0505150e-24], sampled 0.64571696491039
[2019-03-27 06:17:31,933] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:17:31,934] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.05, 87.0, 1.0, 2.0, 0.554453839366105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823290.938042089, 823290.938042089, 198619.9720864316]
[2019-03-27 06:17:31,936] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:17:31,938] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.8863090e-15 1.0000000e+00 1.1438080e-19 1.0849366e-12 1.0517795e-23], sampled 0.34738784838335235
[2019-03-27 06:17:37,019] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:17:37,021] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.3, 94.0, 1.0, 2.0, 0.5181519517929876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724045.4927653032, 724045.4927653025, 186487.7613265147]
[2019-03-27 06:17:37,023] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:17:37,027] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4975858e-15 1.0000000e+00 2.5691871e-20 9.7202281e-13 9.2859894e-25], sampled 0.39235495714004476
[2019-03-27 06:17:42,591] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:17:42,592] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.53333333333333, 93.50000000000001, 1.0, 2.0, 0.3785914467894154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575319.7690804836, 575319.7690804836, 172608.2004566886]
[2019-03-27 06:17:42,593] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:17:42,595] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.9259667e-15 1.0000000e+00 1.1660340e-19 6.9208257e-13 1.3651180e-23], sampled 0.19301812993276934
[2019-03-27 06:17:43,480] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:17:43,480] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.65, 89.0, 1.0, 2.0, 0.3667905480371782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 564760.6667230598, 564760.6667230593, 171890.5024599229]
[2019-03-27 06:17:43,481] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:17:43,486] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5299510e-15 1.0000000e+00 2.5187162e-20 4.9530866e-13 1.4953868e-24], sampled 0.47861141442660937
[2019-03-27 06:17:48,356] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:17:48,358] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.645770655, 96.816562045, 1.0, 2.0, 0.3162720210564853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501674.1084359332, 501674.1084359332, 167178.3497423315]
[2019-03-27 06:17:48,362] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:17:48,363] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.8343276e-15 1.0000000e+00 3.2455152e-20 2.3497887e-13 2.8761949e-24], sampled 0.7940583182242394
[2019-03-27 06:17:54,975] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:17:54,976] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.45018518, 83.92171908, 1.0, 2.0, 0.6872990920092928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 977714.3621799648, 977714.3621799654, 220419.5049341765]
[2019-03-27 06:17:54,977] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:17:54,979] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.2115197e-15 1.0000000e+00 5.8618671e-20 3.0341623e-12 2.5840926e-24], sampled 0.18918810614252202
[2019-03-27 06:17:56,246] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:17:56,248] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.2, 56.5, 1.0, 2.0, 0.583481846235965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815369.9980524246, 815369.9980524246, 197708.9048558182]
[2019-03-27 06:17:56,249] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:17:56,252] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2444299e-15 1.0000000e+00 1.4268838e-20 6.2352239e-13 6.0444450e-25], sampled 0.4553316545402334
[2019-03-27 06:18:16,812] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:18:16,816] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.9, 49.0, 1.0, 2.0, 0.5766825799732593, 0.0, 2.0, 0.0, 1.0, 2.0, 1.001506929570106, 6.911200000000001, 6.9112, 168.9129427099781, 1612341.784197265, 1612341.784197264, 352906.3117634608]
[2019-03-27 06:18:16,817] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:18:16,818] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2875111e-08 9.9897528e-01 2.9894229e-11 1.0247452e-03 4.2586030e-15], sampled 0.7909856755734487
[2019-03-27 06:18:37,853] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01174663], dtype=float32), 0.03941399]
[2019-03-27 06:18:37,854] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.76666666666667, 56.0, 1.0, 2.0, 0.9782820609673336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1367427.479618228, 1367427.479618227, 292383.1008891077]
[2019-03-27 06:18:37,858] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:18:37,861] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7384540e-15 1.0000000e+00 5.3039842e-20 6.8318675e-12 1.5788590e-24], sampled 0.9061839740238012
[2019-03-27 06:19:02,619] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.8893 2927412044.1685 1337.0000
[2019-03-27 06:19:02,744] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1007 2779436630.8531 932.0000
[2019-03-27 06:19:02,960] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7887.1220 3164182477.8537 1768.0000
[2019-03-27 06:19:02,980] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7994.3651 3007768042.8082 1763.0000
[2019-03-27 06:19:03,020] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3980 2842444226.7593 1129.0000
[2019-03-27 06:19:04,035] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 825000, evaluation results [825000.0, 7887.121953281884, 3164182477.853712, 1768.0, 8253.889277055267, 2927412044.168471, 1337.0, 8659.100749494497, 2779436630.8531294, 932.0, 7994.365131166385, 3007768042.808219, 1763.0, 8497.397951694853, 2842444226.759257, 1129.0]
[2019-03-27 06:19:06,592] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.4373801e-17 1.0000000e+00 1.4719534e-20 1.2335853e-11 4.7151668e-24], sum to 1.0000
[2019-03-27 06:19:06,602] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2402
[2019-03-27 06:19:06,607] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.36666666666667, 88.16666666666667, 1.0, 2.0, 0.4745974721251046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664380.7659595414, 664380.7659595414, 179868.5737645611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7081800.0000, 
sim time next is 7082400.0000, 
raw observation next is [25.33333333333333, 88.33333333333334, 1.0, 2.0, 0.4744718242976024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664514.5440947058, 664514.5440947058, 179889.8016697373], 
processed observation next is [1.0, 1.0, 0.3996840442338071, 0.8833333333333334, 1.0, 1.0, 0.3668335232501233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1845873733596405, 0.1845873733596405, 0.2684922412981154], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.6020414], dtype=float32), 1.3462102]. 
=============================================
[2019-03-27 06:19:12,154] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.7139345e-14 1.0000000e+00 8.9976044e-19 3.1002056e-10 2.5278289e-22], sum to 1.0000
[2019-03-27 06:19:12,163] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8359
[2019-03-27 06:19:12,170] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 89.0, 1.0, 2.0, 0.572333966816984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799785.8481008594, 799785.8481008594, 195701.1906973771], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7192800.0000, 
sim time next is 7193400.0000, 
raw observation next is [26.73333333333333, 88.5, 1.0, 2.0, 0.5892476029859566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823430.3128911055, 823430.3128911055, 198755.0397218193], 
processed observation next is [1.0, 0.2608695652173913, 0.4660347551342811, 0.885, 1.0, 1.0, 0.5051175939589838, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22873064246975153, 0.22873064246975153, 0.29664931301764075], 
reward next is 0.7034, 
noisyNet noise sample is [array([-0.66512877], dtype=float32), -0.7777712]. 
=============================================
[2019-03-27 06:19:13,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5721192e-12 9.9999940e-01 1.7470580e-15 5.4277109e-07 6.5099404e-19], sum to 1.0000
[2019-03-27 06:19:13,534] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7657
[2019-03-27 06:19:13,539] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 87.5, 1.0, 2.0, 0.8170358605810989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1207507.395587931, 1207507.395587931, 257209.0716316512], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7227000.0000, 
sim time next is 7227600.0000, 
raw observation next is [24.13333333333333, 87.0, 1.0, 2.0, 0.8822849876322661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1305562.168483747, 1305562.168483746, 275467.7186570702], 
processed observation next is [1.0, 0.6521739130434783, 0.3428120063191152, 0.87, 1.0, 1.0, 0.8581746838942965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36265615791215194, 0.36265615791215167, 0.4111458487418958], 
reward next is 0.5889, 
noisyNet noise sample is [array([0.21339607], dtype=float32), -0.2178251]. 
=============================================
[2019-03-27 06:19:21,575] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.05457245e-14 1.00000000e+00 5.92235469e-19 2.93153132e-12
 7.79954113e-24], sum to 1.0000
[2019-03-27 06:19:21,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0782
[2019-03-27 06:19:21,587] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 92.33333333333333, 1.0, 2.0, 0.4111758867973876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651858.8488127897, 651858.8488127897, 179845.8991403281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7389600.0000, 
sim time next is 7390200.0000, 
raw observation next is [21.03333333333333, 92.16666666666667, 1.0, 2.0, 0.412606165552435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654922.4811184471, 654922.4811184466, 180124.86327734], 
processed observation next is [1.0, 0.5217391304347826, 0.19589257503949445, 0.9216666666666667, 1.0, 1.0, 0.29229658500293376, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18192291142179087, 0.1819229114217907, 0.2688430795184179], 
reward next is 0.7312, 
noisyNet noise sample is [array([-0.48853782], dtype=float32), -1.2420211]. 
=============================================
[2019-03-27 06:19:21,752] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1594707e-15 1.0000000e+00 5.6703011e-20 1.1659224e-12 4.3719930e-24], sum to 1.0000
[2019-03-27 06:19:21,761] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8038
[2019-03-27 06:19:21,765] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 93.0, 1.0, 2.0, 0.7074684945793798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1114112.989064285, 1114112.989064286, 237691.992370086], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7383600.0000, 
sim time next is 7384200.0000, 
raw observation next is [21.28333333333333, 93.0, 1.0, 2.0, 0.6395973424484709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1006985.689436953, 1006985.689436953, 221741.7633299674], 
processed observation next is [1.0, 0.4782608695652174, 0.20774091627172192, 0.93, 1.0, 1.0, 0.5657799306608082, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27971824706582027, 0.27971824706582027, 0.33095785571636926], 
reward next is 0.6690, 
noisyNet noise sample is [array([-0.21422485], dtype=float32), -0.26681888]. 
=============================================
[2019-03-27 06:19:27,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8886594e-08 9.9259853e-01 2.0189131e-10 7.4014654e-03 4.0728453e-13], sum to 1.0000
[2019-03-27 06:19:27,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3800
[2019-03-27 06:19:27,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2072776.336455435 W.
[2019-03-27 06:19:27,940] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.4, 73.0, 1.0, 2.0, 0.4941409569652167, 1.0, 1.0, 0.4941409569652167, 1.0, 2.0, 0.8509439205350298, 6.911199999999999, 6.9112, 170.5573041426782, 2072776.336455435, 2072776.336455435, 409829.1088610134], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7814400.0000, 
sim time next is 7815000.0000, 
raw observation next is [29.5, 72.5, 1.0, 2.0, 0.493654759310594, 1.0, 2.0, 0.493654759310594, 1.0, 2.0, 0.8512538699786582, 6.9112, 6.9112, 170.5573041426782, 2070734.908786044, 2070734.908786044, 409705.4160051387], 
processed observation next is [1.0, 0.43478260869565216, 0.5971563981042655, 0.725, 1.0, 1.0, 0.3899454931452939, 1.0, 1.0, 0.3899454931452939, 1.0, 1.0, 0.8186022804617781, 0.0, 0.0, 0.8375144448122397, 0.5752041413294566, 0.5752041413294566, 0.6115006209031921], 
reward next is 0.3885, 
noisyNet noise sample is [array([-0.54330844], dtype=float32), -0.17578691]. 
=============================================
[2019-03-27 06:19:27,955] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[34.866005]
 [38.00004 ]
 [39.652534]
 [40.23832 ]
 [41.749382]], R is [[32.13479233]
 [31.81344414]
 [31.49530983]
 [31.62289619]
 [31.34392738]].
[2019-03-27 06:19:36,935] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:36,935] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:36,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-03-27 06:19:38,226] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2113963e-14 1.0000000e+00 3.6283507e-19 3.0737614e-11 1.4792301e-22], sum to 1.0000
[2019-03-27 06:19:38,232] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6108
[2019-03-27 06:19:38,239] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 94.0, 1.0, 2.0, 0.4779205471834551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667810.0132973973, 667810.0132973973, 180208.7063805271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7695600.0000, 
sim time next is 7696200.0000, 
raw observation next is [24.65, 94.5, 1.0, 2.0, 0.47821504555122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668221.6524615595, 668221.6524615588, 180252.9526438478], 
processed observation next is [1.0, 0.043478260869565216, 0.3672985781990521, 0.945, 1.0, 1.0, 0.3713434283749639, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18561712568376654, 0.18561712568376634, 0.26903425767738476], 
reward next is 0.7310, 
noisyNet noise sample is [array([-0.6757474], dtype=float32), 0.87165684]. 
=============================================
[2019-03-27 06:19:38,981] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:38,984] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:39,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-03-27 06:19:42,548] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6017033e-08 9.9632227e-01 2.4299934e-11 3.6777821e-03 1.3540461e-15], sum to 1.0000
[2019-03-27 06:19:42,557] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5844
[2019-03-27 06:19:42,565] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.75, 61.33333333333334, 1.0, 2.0, 0.9978171375057148, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1394751.244082637, 1394751.244082637, 298261.6789903523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7735800.0000, 
sim time next is 7736400.0000, 
raw observation next is [31.8, 61.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.783885193426347, 6.9112, 168.9085003918233, 2073275.597261593, 1454179.018451264, 311350.7647756461], 
processed observation next is [1.0, 0.5652173913043478, 0.7061611374407584, 0.61, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.08726851934263466, 0.0, 0.829418063569919, 0.5759098881282203, 0.4039386162364622, 0.46470263399350165], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.097284], dtype=float32), 0.4319633]. 
=============================================
[2019-03-27 06:19:44,930] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6575500e-15 1.0000000e+00 5.1714055e-19 2.2527217e-12 5.3684578e-25], sum to 1.0000
[2019-03-27 06:19:44,940] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8609
[2019-03-27 06:19:44,946] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 90.0, 1.0, 2.0, 0.5192745789501413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725614.7443187244, 725614.7443187244, 186671.1438542013], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7774200.0000, 
sim time next is 7774800.0000, 
raw observation next is [26.4, 89.66666666666666, 1.0, 2.0, 0.5182406489843199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724169.4770371129, 724169.4770371135, 186503.341211195], 
processed observation next is [1.0, 1.0, 0.45023696682464454, 0.8966666666666666, 1.0, 1.0, 0.41956704696906005, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2011581880658647, 0.20115818806586486, 0.27836319583760444], 
reward next is 0.7216, 
noisyNet noise sample is [array([-0.01885144], dtype=float32), 0.09568828]. 
=============================================
[2019-03-27 06:19:47,868] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4657688e-14 1.0000000e+00 3.6329508e-18 4.4237560e-13 1.3762343e-23], sum to 1.0000
[2019-03-27 06:19:47,875] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4947
[2019-03-27 06:19:47,878] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2866967877716635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461796.446713019, 461796.446713019, 164395.6499334722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 184800.0000, 
sim time next is 185400.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.2863924508741391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461306.5985648271, 461306.5985648271, 164362.226335434], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.14023186852305916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1281407218235631, 0.1281407218235631, 0.24531675572452838], 
reward next is 0.7547, 
noisyNet noise sample is [array([-1.3024569], dtype=float32), 1.1845614]. 
=============================================
[2019-03-27 06:19:48,158] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6207637e-15 1.0000000e+00 8.9227507e-19 3.1323420e-11 1.2440926e-23], sum to 1.0000
[2019-03-27 06:19:48,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8063
[2019-03-27 06:19:48,174] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.85, 88.0, 1.0, 2.0, 0.522364377997228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729933.796608091, 729933.7966080903, 187174.5724984668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7853400.0000, 
sim time next is 7854000.0000, 
raw observation next is [26.8, 88.0, 1.0, 2.0, 0.5205528639608469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727401.5832481319, 727401.5832481312, 186879.2071351952], 
processed observation next is [1.0, 0.9130434782608695, 0.4691943127962086, 0.88, 1.0, 1.0, 0.42235284814559865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20205599534670332, 0.20205599534670313, 0.27892418975402267], 
reward next is 0.7211, 
noisyNet noise sample is [array([-1.5269223], dtype=float32), 0.70188266]. 
=============================================
[2019-03-27 06:19:48,188] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[70.07297 ]
 [70.18175 ]
 [70.13156 ]
 [70.492546]
 [70.834366]], R is [[70.63097382]
 [70.64530182]
 [70.65898895]
 [70.67196655]
 [70.6841507 ]].
[2019-03-27 06:19:53,946] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:53,947] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:53,983] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-03-27 06:19:54,438] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:54,439] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:54,448] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-03-27 06:19:54,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:54,491] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:54,495] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-03-27 06:19:54,689] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:54,689] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:54,695] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-03-27 06:19:54,717] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:54,717] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:54,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-03-27 06:19:54,739] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:54,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:54,752] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-03-27 06:19:55,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:55,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:55,148] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-03-27 06:19:55,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:55,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:55,187] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-03-27 06:19:55,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:55,285] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:55,286] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-03-27 06:19:55,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:55,338] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:55,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-03-27 06:19:55,359] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:55,360] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:55,365] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-03-27 06:19:55,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:55,386] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:55,386] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:55,387] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:55,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-03-27 06:19:55,414] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:19:55,414] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:55,416] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-03-27 06:19:55,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-03-27 06:19:57,043] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 06:19:57,047] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:19:57,049] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:57,049] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:19:57,050] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:19:57,050] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:57,051] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:57,052] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:19:57,050] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:19:57,055] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:57,055] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:19:57,069] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run35
[2019-03-27 06:19:57,088] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run35
[2019-03-27 06:19:57,114] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run35
[2019-03-27 06:19:57,115] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run35
[2019-03-27 06:19:57,146] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run35
[2019-03-27 06:20:32,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01355891], dtype=float32), 0.040771708]
[2019-03-27 06:20:32,390] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.52174258, 56.41314824666667, 1.0, 2.0, 0.6394887762182947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893668.2050551514, 893668.2050551514, 208325.7143822373]
[2019-03-27 06:20:32,391] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:20:32,394] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.1515208e-16 1.0000000e+00 2.2899952e-20 8.7479259e-13 1.7505725e-25], sampled 0.25484208847377465
[2019-03-27 06:20:33,328] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01355891], dtype=float32), 0.040771708]
[2019-03-27 06:20:33,329] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.5, 94.0, 1.0, 2.0, 0.3827968489730174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580759.6817606973, 580759.6817606973, 173064.4348713016]
[2019-03-27 06:20:33,331] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:20:33,333] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4128483e-15 1.0000000e+00 2.8200965e-20 1.5524786e-13 1.4373426e-24], sampled 0.40472431249135155
[2019-03-27 06:20:43,459] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01355891], dtype=float32), 0.040771708]
[2019-03-27 06:20:43,460] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.90000000000001, 52.0, 1.0, 2.0, 1.019186404516154, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956430947, 1424641.301186023, 1424641.301186022, 304831.255998679]
[2019-03-27 06:20:43,461] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:20:43,465] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0221462e-14 1.0000000e+00 2.8961299e-19 2.7069203e-11 4.9960592e-24], sampled 0.21565789778645816
[2019-03-27 06:20:59,975] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01355891], dtype=float32), 0.040771708]
[2019-03-27 06:20:59,977] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.40000000000001, 51.33333333333334, 1.0, 2.0, 0.5998192812116578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565094672, 838209.2872254337, 838209.2872254337, 200713.3080930683]
[2019-03-27 06:20:59,978] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:20:59,980] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4413496e-12 1.0000000e+00 2.8336466e-16 2.2579915e-08 5.3320583e-21], sampled 0.8034902259390778
[2019-03-27 06:21:22,039] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01355891], dtype=float32), 0.040771708]
[2019-03-27 06:21:22,076] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.4, 74.0, 1.0, 2.0, 1.01572850827885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1419804.544941714, 1419804.544941715, 303760.1796217784]
[2019-03-27 06:21:22,077] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:21:22,080] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3032914e-15 1.0000000e+00 6.6075911e-20 3.7583495e-12 9.4565766e-25], sampled 0.3150332721342308
[2019-03-27 06:21:28,738] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01355891], dtype=float32), 0.040771708]
[2019-03-27 06:21:28,740] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.75, 78.0, 1.0, 2.0, 0.5832211156839181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815005.5078226008, 815005.5078226008, 197660.9864024999]
[2019-03-27 06:21:28,742] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:21:28,744] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7271033e-16 1.0000000e+00 5.0884872e-21 1.2235268e-13 9.5363053e-26], sampled 0.8765065466474616
[2019-03-27 06:21:51,684] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0749 2842351803.8184 1128.0000
[2019-03-27 06:21:51,864] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.5628 3163867620.0006 1769.0000
[2019-03-27 06:21:51,939] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.9942 2779323274.0091 934.0000
[2019-03-27 06:21:52,008] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.3644 3007796590.6228 1766.0000
[2019-03-27 06:21:52,031] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.3570 2927563409.4727 1338.0000
[2019-03-27 06:21:53,045] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 850000, evaluation results [850000.0, 7884.562776576825, 3163867620.000579, 1769.0, 8251.35697523072, 2927563409.472671, 1338.0, 8657.994208704247, 2779323274.0090537, 934.0, 7997.364353810858, 3007796590.6227736, 1766.0, 8496.07489098766, 2842351803.8183684, 1128.0]
[2019-03-27 06:21:59,281] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.1476273e-14 1.0000000e+00 1.3275499e-18 3.2644033e-11 1.2550238e-21], sum to 1.0000
[2019-03-27 06:21:59,288] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4712
[2019-03-27 06:21:59,295] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.93333333333333, 91.66666666666666, 1.0, 2.0, 0.6899812295964433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1041901.524199216, 1041901.524199216, 228628.4966481672], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 117600.0000, 
sim time next is 118200.0000, 
raw observation next is [22.91666666666666, 91.83333333333333, 1.0, 2.0, 0.6901428479089019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1041999.03035852, 1041999.03035852, 228648.5417650107], 
processed observation next is [1.0, 0.34782608695652173, 0.2851500789889413, 0.9183333333333333, 1.0, 1.0, 0.6266781300107251, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28944417509958886, 0.28944417509958886, 0.34126648024628464], 
reward next is 0.6587, 
noisyNet noise sample is [array([0.62661004], dtype=float32), -0.97578394]. 
=============================================
[2019-03-27 06:22:00,304] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0505166e-15 1.0000000e+00 1.0658011e-18 8.0296802e-12 2.6849906e-25], sum to 1.0000
[2019-03-27 06:22:00,315] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3394
[2019-03-27 06:22:00,320] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 96.0, 1.0, 2.0, 0.8019058968446152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1194224.090016205, 1194224.090016205, 254402.1387896959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 135000.0000, 
sim time next is 135600.0000, 
raw observation next is [22.8, 96.0, 1.0, 2.0, 0.8174763486541843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217378.522407393, 1217378.522407393, 258538.744758705], 
processed observation next is [1.0, 0.5652173913043478, 0.2796208530805688, 0.96, 1.0, 1.0, 0.7800919863303425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3381607006687203, 0.3381607006687203, 0.3858787235204552], 
reward next is 0.6141, 
noisyNet noise sample is [array([-0.4973171], dtype=float32), 1.6892604]. 
=============================================
[2019-03-27 06:22:01,736] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4517835e-16 1.0000000e+00 9.4375322e-21 8.9305587e-13 7.9753354e-25], sum to 1.0000
[2019-03-27 06:22:01,744] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4337
[2019-03-27 06:22:01,752] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.88333333333333, 96.0, 1.0, 2.0, 0.2862261540114023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461138.5560566716, 461138.5560566716, 164350.7811383822], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 187800.0000, 
sim time next is 188400.0000, 
raw observation next is [19.86666666666667, 96.0, 1.0, 2.0, 0.2856944411826112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460424.7653101006, 460424.7653101006, 164302.1342669122], 
processed observation next is [0.0, 0.17391304347826086, 0.14060031595576644, 0.96, 1.0, 1.0, 0.13939089299109786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12789576814169462, 0.12789576814169462, 0.24522706607001823], 
reward next is 0.7548, 
noisyNet noise sample is [array([-1.0717871], dtype=float32), -0.19198479]. 
=============================================
[2019-03-27 06:22:09,264] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0235269e-15 1.0000000e+00 1.9907743e-20 6.4209753e-14 4.7071334e-24], sum to 1.0000
[2019-03-27 06:22:09,271] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0537
[2019-03-27 06:22:09,275] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.23333333333333, 77.0, 1.0, 2.0, 0.3069112413170522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 485394.033710158, 485394.0337101585, 165948.6277054654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 299400.0000, 
sim time next is 300000.0000, 
raw observation next is [23.26666666666667, 77.0, 1.0, 2.0, 0.3076862754026411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486261.7797807696, 486261.7797807696, 166003.8724069634], 
processed observation next is [0.0, 0.4782608695652174, 0.3017377567140602, 0.77, 1.0, 1.0, 0.16588707879836276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13507271660576933, 0.13507271660576933, 0.24776697374173642], 
reward next is 0.7522, 
noisyNet noise sample is [array([0.15018912], dtype=float32), -0.97169095]. 
=============================================
[2019-03-27 06:22:09,292] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.78223 ]
 [77.727425]
 [77.66188 ]
 [77.63696 ]
 [77.60172 ]], R is [[77.79812622]
 [77.77246094]
 [77.74716949]
 [77.72225189]
 [77.69759369]].
[2019-03-27 06:22:18,720] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3801759e-15 1.0000000e+00 3.3624252e-21 4.7147728e-12 8.1108543e-25], sum to 1.0000
[2019-03-27 06:22:18,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4621
[2019-03-27 06:22:18,733] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 56.33333333333333, 1.0, 2.0, 0.4835530432168294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790551.8589840526, 790551.8589840526, 192854.9999312984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 479400.0000, 
sim time next is 480000.0000, 
raw observation next is [24.8, 55.66666666666667, 1.0, 2.0, 0.5498549963960074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898547.6938167696, 898547.6938167696, 205195.2833264635], 
processed observation next is [1.0, 0.5652173913043478, 0.3744075829383887, 0.5566666666666668, 1.0, 1.0, 0.4576566221638644, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24959658161576934, 0.24959658161576934, 0.3062616169051694], 
reward next is 0.6937, 
noisyNet noise sample is [array([-0.37391144], dtype=float32), -0.91759163]. 
=============================================
[2019-03-27 06:22:18,752] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.06776 ]
 [75.21218 ]
 [75.21937 ]
 [75.143684]
 [75.066505]], R is [[74.87566376]
 [74.83906555]
 [74.83050537]
 [74.8327713 ]
 [74.8349762 ]].
[2019-03-27 06:22:22,476] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.09321545e-14 1.00000000e+00 1.95228604e-18 1.69252470e-13
 1.03966858e-23], sum to 1.0000
[2019-03-27 06:22:22,486] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1849
[2019-03-27 06:22:22,492] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.35, 88.66666666666667, 1.0, 2.0, 0.2194411765333577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 364841.5771777953, 364841.5771777959, 157718.0398155709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 528600.0000, 
sim time next is 529200.0000, 
raw observation next is [18.3, 89.0, 1.0, 2.0, 0.2196828848417186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 365275.653738632, 365275.6537386326, 157732.9339472855], 
processed observation next is [1.0, 0.13043478260869565, 0.06635071090047404, 0.89, 1.0, 1.0, 0.05985889739966096, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10146545937184222, 0.1014654593718424, 0.23542228947356045], 
reward next is 0.7646, 
noisyNet noise sample is [array([0.9109871], dtype=float32), 0.7521718]. 
=============================================
[2019-03-27 06:22:23,003] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.36730726e-15 1.00000000e+00 1.48843035e-19 6.07292943e-14
 1.03516505e-23], sum to 1.0000
[2019-03-27 06:22:23,015] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7443
[2019-03-27 06:22:23,023] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.7, 89.0, 1.0, 2.0, 0.210511395566066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 351343.47900249, 351343.47900249, 156540.9589865816], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 608400.0000, 
sim time next is 609000.0000, 
raw observation next is [17.61666666666667, 89.33333333333334, 1.0, 2.0, 0.2095822888778458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 349862.5436621279, 349862.5436621279, 156430.2283826522], 
processed observation next is [1.0, 0.043478260869565216, 0.03396524486571906, 0.8933333333333334, 1.0, 1.0, 0.04768950467210336, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09718403990614664, 0.09718403990614664, 0.23347795280992864], 
reward next is 0.7665, 
noisyNet noise sample is [array([-1.0301152], dtype=float32), 2.6962447]. 
=============================================
[2019-03-27 06:22:23,041] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.40041 ]
 [74.585884]
 [74.905   ]
 [75.37439 ]
 [75.88783 ]], R is [[74.20710754]
 [74.23139191]
 [74.2552948 ]
 [74.27879333]
 [74.30187988]].
[2019-03-27 06:22:25,269] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6407216e-17 1.0000000e+00 8.1297398e-20 2.4432070e-13 9.3267752e-26], sum to 1.0000
[2019-03-27 06:22:25,271] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8437
[2019-03-27 06:22:25,275] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.53333333333333, 63.33333333333333, 1.0, 2.0, 0.6178825194587746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008599.170384955, 1008599.170384955, 219393.6204799811], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 578400.0000, 
sim time next is 579000.0000, 
raw observation next is [23.51666666666667, 63.66666666666666, 1.0, 2.0, 0.6213822625029956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1013779.672211657, 1013779.672211657, 220151.5694562062], 
processed observation next is [1.0, 0.6956521739130435, 0.31358609794628767, 0.6366666666666666, 1.0, 1.0, 0.5438340512084283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2816054645032381, 0.2816054645032381, 0.32858443202418836], 
reward next is 0.6714, 
noisyNet noise sample is [array([0.02893443], dtype=float32), -1.6866018]. 
=============================================
[2019-03-27 06:22:25,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.607285]
 [75.439064]
 [75.24886 ]
 [75.022156]
 [74.77464 ]], R is [[75.6252594 ]
 [75.54155731]
 [75.45987701]
 [75.37795258]
 [75.29839325]].
[2019-03-27 06:22:25,645] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7104990e-15 1.0000000e+00 3.8617068e-21 1.2748882e-12 3.2015604e-24], sum to 1.0000
[2019-03-27 06:22:25,653] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1596
[2019-03-27 06:22:25,658] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.0, 1.0, 2.0, 0.3497808518227349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 537152.0915332764, 537152.091533277, 169531.2477243921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1047600.0000, 
sim time next is 1048200.0000, 
raw observation next is [21.56666666666667, 96.83333333333334, 1.0, 2.0, 0.3545090253303371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546747.7960604882, 546747.7960604876, 170392.7842107117], 
processed observation next is [1.0, 0.13043478260869565, 0.22116903633491333, 0.9683333333333334, 1.0, 1.0, 0.22230003051847846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15187438779458007, 0.1518743877945799, 0.25431758837419655], 
reward next is 0.7457, 
noisyNet noise sample is [array([-1.5688587], dtype=float32), -0.7391357]. 
=============================================
[2019-03-27 06:22:30,444] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5120055e-15 1.0000000e+00 1.2431748e-19 2.2896290e-12 4.2998954e-24], sum to 1.0000
[2019-03-27 06:22:30,452] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8697
[2019-03-27 06:22:30,457] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 54.0, 1.0, 2.0, 0.6459675601153325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1060564.243799926, 1060564.243799927, 225845.9538199728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 664200.0000, 
sim time next is 664800.0000, 
raw observation next is [24.7, 54.0, 1.0, 2.0, 0.6503903047182602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1067798.144919577, 1067798.144919577, 226868.7953992902], 
processed observation next is [1.0, 0.6956521739130435, 0.3696682464454976, 0.54, 1.0, 1.0, 0.5787834996605544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29661059581099364, 0.29661059581099364, 0.33861014238700027], 
reward next is 0.6614, 
noisyNet noise sample is [array([-1.3330158], dtype=float32), 0.2010653]. 
=============================================
[2019-03-27 06:22:33,969] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2347685e-16 1.0000000e+00 2.7058586e-21 2.7545240e-15 5.0313051e-25], sum to 1.0000
[2019-03-27 06:22:33,978] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1764
[2019-03-27 06:22:33,986] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 84.0, 1.0, 2.0, 0.2782670354413174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 449709.3666166418, 449709.3666166418, 163578.5477792853], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 802800.0000, 
sim time next is 803400.0000, 
raw observation next is [21.35, 83.33333333333333, 1.0, 2.0, 0.2795554455489975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 451282.3836246138, 451282.3836246132, 163685.1376142359], 
processed observation next is [0.0, 0.30434782608695654, 0.2109004739336494, 0.8333333333333333, 1.0, 1.0, 0.13199451270963558, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12535621767350383, 0.12535621767350366, 0.24430617554363565], 
reward next is 0.7557, 
noisyNet noise sample is [array([0.16725433], dtype=float32), 2.031339]. 
=============================================
[2019-03-27 06:22:35,440] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.0848772e-17 1.0000000e+00 1.6333658e-22 2.0426135e-14 1.5192472e-26], sum to 1.0000
[2019-03-27 06:22:35,450] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4469
[2019-03-27 06:22:35,457] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 92.0, 1.0, 2.0, 0.2599698969719779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425513.1070046628, 425513.1070046628, 161899.20165011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 784800.0000, 
sim time next is 785400.0000, 
raw observation next is [19.4, 92.0, 1.0, 2.0, 0.2598266316685817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425273.1627359597, 425273.1627359597, 161884.5354224691], 
processed observation next is [0.0, 0.08695652173913043, 0.11848341232227487, 0.92, 1.0, 1.0, 0.10822485743202616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11813143409332214, 0.11813143409332214, 0.2416187095857748], 
reward next is 0.7584, 
noisyNet noise sample is [array([1.0840096], dtype=float32), 0.073447615]. 
=============================================
[2019-03-27 06:22:39,912] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6276450e-17 1.0000000e+00 1.7808181e-21 1.0521208e-15 4.6214915e-27], sum to 1.0000
[2019-03-27 06:22:39,920] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2260
[2019-03-27 06:22:39,924] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 89.0, 1.0, 2.0, 0.3087097295269451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489279.8158706811, 489279.8158706804, 166253.6816097139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 862200.0000, 
sim time next is 862800.0000, 
raw observation next is [21.46666666666667, 89.0, 1.0, 2.0, 0.3079426918569733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488430.3244859311, 488430.3244859317, 166198.4926851338], 
processed observation next is [0.0, 1.0, 0.21642969984202226, 0.89, 1.0, 1.0, 0.16619601428551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13567509013498086, 0.13567509013498102, 0.2480574517688564], 
reward next is 0.7519, 
noisyNet noise sample is [array([2.0644379], dtype=float32), -0.028230684]. 
=============================================
[2019-03-27 06:22:42,942] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4624703e-15 1.0000000e+00 2.3181205e-20 1.6153472e-14 5.3716713e-26], sum to 1.0000
[2019-03-27 06:22:42,951] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4296
[2019-03-27 06:22:42,956] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.91666666666667, 88.16666666666667, 1.0, 2.0, 0.2863654259169712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 460412.6558931374, 460412.6558931368, 164299.2700965568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 877800.0000, 
sim time next is 878400.0000, 
raw observation next is [20.9, 88.0, 1.0, 2.0, 0.2858749732194508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459951.6349758428, 459951.6349758428, 164268.9991846066], 
processed observation next is [0.0, 0.17391304347826086, 0.1895734597156398, 0.88, 1.0, 1.0, 0.13960840146921785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1277643430488452, 0.1277643430488452, 0.24517761072329342], 
reward next is 0.7548, 
noisyNet noise sample is [array([-0.06755992], dtype=float32), 0.020552203]. 
=============================================
[2019-03-27 06:22:46,727] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 06:22:46,729] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:22:46,730] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:22:46,730] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:22:46,731] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:22:46,732] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:22:46,732] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:22:46,734] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:22:46,735] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:22:46,737] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:22:46,732] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:22:46,765] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run36
[2019-03-27 06:22:46,766] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run36
[2019-03-27 06:22:46,814] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run36
[2019-03-27 06:22:46,815] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run36
[2019-03-27 06:22:46,815] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run36
[2019-03-27 06:22:49,744] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01376176], dtype=float32), 0.040986713]
[2019-03-27 06:22:49,746] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.58083553, 97.61578004, 1.0, 2.0, 0.4558305228004364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684570.2971634181, 684570.2971634181, 182870.2277337285]
[2019-03-27 06:22:49,746] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:22:49,751] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.1563634e-16 1.0000000e+00 7.8989012e-21 7.3049246e-14 2.7739014e-25], sampled 0.4245323785878292
[2019-03-27 06:23:27,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01376176], dtype=float32), 0.040986713]
[2019-03-27 06:23:27,431] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.8, 65.66666666666667, 1.0, 2.0, 0.5738851188177142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 801954.263438614, 801954.2634386134, 195981.1469397593]
[2019-03-27 06:23:27,432] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:23:27,434] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7115359e-16 1.0000000e+00 1.5552256e-21 5.1517875e-14 2.5583332e-26], sampled 0.2049843989605249
[2019-03-27 06:23:40,523] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01376176], dtype=float32), 0.040986713]
[2019-03-27 06:23:40,525] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.58333333333333, 75.0, 1.0, 2.0, 0.9220044660948483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1288715.786263851, 1288715.786263851, 276063.3882229266]
[2019-03-27 06:23:40,526] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:23:40,528] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.6190365e-15 1.0000000e+00 2.3130074e-19 2.5573855e-12 5.2689649e-24], sampled 0.6787356973757451
[2019-03-27 06:23:54,270] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01376176], dtype=float32), 0.040986713]
[2019-03-27 06:23:54,272] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.520078295, 61.98557398, 1.0, 2.0, 0.5436450099097887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759681.2443989402, 759681.2443989395, 190716.16867837]
[2019-03-27 06:23:54,274] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:23:54,278] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1323741e-16 1.0000000e+00 2.0585088e-21 5.9632095e-14 3.9868651e-26], sampled 0.24528409296830633
[2019-03-27 06:24:28,730] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01376176], dtype=float32), 0.040986713]
[2019-03-27 06:24:28,731] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.05464007666667, 75.11728067333334, 1.0, 2.0, 0.4113297729909969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607652.8607747807, 607652.8607747807, 175074.5962233204]
[2019-03-27 06:24:28,733] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:24:28,738] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.70370823e-16 1.00000000e+00 1.29887535e-20 5.07032059e-14
 9.21657257e-25], sampled 0.3833820684978029
[2019-03-27 06:24:42,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9447 2927342593.4681 1338.0000
[2019-03-27 06:24:42,380] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.0079 3164209482.5844 1777.0000
[2019-03-27 06:24:42,460] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4791 3007718786.5045 1766.0000
[2019-03-27 06:24:42,491] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1321 2842497763.3803 1131.0000
[2019-03-27 06:24:42,525] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1365 2779297768.5367 933.0000
[2019-03-27 06:24:43,541] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 875000, evaluation results [875000.0, 7881.007932179015, 3164209482.5844307, 1777.0, 8252.944688759258, 2927342593.4681134, 1338.0, 8659.1365090549, 2779297768.5367293, 933.0, 7997.479055512932, 3007718786.504511, 1766.0, 8496.132109434688, 2842497763.3802733, 1131.0]
[2019-03-27 06:24:54,688] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5696670e-15 1.0000000e+00 1.5887412e-19 4.1249355e-13 8.2063417e-24], sum to 1.0000
[2019-03-27 06:24:54,695] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6257
[2019-03-27 06:24:54,702] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 93.5, 1.0, 2.0, 0.2755818247337372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445192.6284017785, 445192.6284017785, 163280.6944681586], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1143000.0000, 
sim time next is 1143600.0000, 
raw observation next is [20.2, 92.66666666666666, 1.0, 2.0, 0.2773614965207933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447636.3536289852, 447636.3536289852, 163443.0537337247], 
processed observation next is [1.0, 0.21739130434782608, 0.15639810426540288, 0.9266666666666665, 1.0, 1.0, 0.12935120062746178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.124343431563607, 0.124343431563607, 0.24394485631899207], 
reward next is 0.7561, 
noisyNet noise sample is [array([-0.3532406], dtype=float32), -0.9107717]. 
=============================================
[2019-03-27 06:24:58,441] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.7892118e-16 1.0000000e+00 1.2913615e-19 5.0164641e-13 6.0756331e-24], sum to 1.0000
[2019-03-27 06:24:58,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6710
[2019-03-27 06:24:58,451] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.56666666666667, 98.33333333333334, 1.0, 2.0, 0.4781503930941028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683549.6838074777, 683549.6838074777, 182215.226587118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1665600.0000, 
sim time next is 1666200.0000, 
raw observation next is [23.58333333333333, 98.16666666666667, 1.0, 2.0, 0.4773386055273855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682501.2686907761, 682501.2686907767, 182104.4983785113], 
processed observation next is [1.0, 0.2608695652173913, 0.31674565560821466, 0.9816666666666667, 1.0, 1.0, 0.3702874765390186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1895836857474378, 0.18958368574743797, 0.2717977587738975], 
reward next is 0.7282, 
noisyNet noise sample is [array([-0.40083197], dtype=float32), 1.1721958]. 
=============================================
[2019-03-27 06:25:03,228] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1528599e-16 1.0000000e+00 1.7071114e-20 1.7070142e-12 3.9305275e-24], sum to 1.0000
[2019-03-27 06:25:03,236] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7005
[2019-03-27 06:25:03,242] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.31666666666667, 94.0, 1.0, 2.0, 0.4592063636277687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650618.5539348436, 650618.5539348436, 178607.5266730238], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1295400.0000, 
sim time next is 1296000.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.4583264292921354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 649845.9318804906, 649845.9318804899, 178539.1992395844], 
processed observation next is [1.0, 0.0, 0.3507109004739337, 0.94, 1.0, 1.0, 0.34738124011100646, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18051275885569185, 0.18051275885569165, 0.2664764167754991], 
reward next is 0.7335, 
noisyNet noise sample is [array([-0.958101], dtype=float32), 0.5727171]. 
=============================================
[2019-03-27 06:25:03,254] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.261185]
 [75.254845]
 [75.24046 ]
 [75.212524]
 [75.18602 ]], R is [[74.10945892]
 [74.10178375]
 [74.09407043]
 [74.08629608]
 [74.07842255]].
[2019-03-27 06:25:04,759] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3537356e-15 1.0000000e+00 1.8211176e-20 1.0132241e-12 9.3632528e-24], sum to 1.0000
[2019-03-27 06:25:04,767] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6433
[2019-03-27 06:25:04,773] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.18333333333334, 92.66666666666666, 1.0, 2.0, 0.4364102202341828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626270.748492715, 626270.748492715, 176357.1436132321], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1317000.0000, 
sim time next is 1317600.0000, 
raw observation next is [24.1, 93.0, 1.0, 2.0, 0.4359762272019624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 626681.1483727191, 626681.1483727198, 176426.6340832223], 
processed observation next is [1.0, 0.2608695652173913, 0.3412322274881518, 0.93, 1.0, 1.0, 0.3204532857854968, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17407809677019975, 0.17407809677019995, 0.2633233344525706], 
reward next is 0.7367, 
noisyNet noise sample is [array([-0.9470633], dtype=float32), -0.38661778]. 
=============================================
[2019-03-27 06:25:13,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4570539e-13 1.0000000e+00 1.7374998e-17 3.5614744e-10 2.1381660e-23], sum to 1.0000
[2019-03-27 06:25:13,623] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0195
[2019-03-27 06:25:13,628] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 91.5, 1.0, 2.0, 0.4119402022171317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607025.5068812162, 607025.5068812162, 174971.4271775751], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1967400.0000, 
sim time next is 1968000.0000, 
raw observation next is [23.53333333333333, 92.0, 1.0, 2.0, 0.4073195822037484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601361.8269960075, 601361.8269960075, 174476.4283478703], 
processed observation next is [1.0, 0.782608695652174, 0.3143759873617693, 0.92, 1.0, 1.0, 0.2859272074743957, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1670449519433354, 0.1670449519433354, 0.260412579623687], 
reward next is 0.7396, 
noisyNet noise sample is [array([-0.39328003], dtype=float32), -1.1157453]. 
=============================================
[2019-03-27 06:25:13,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.76348 ]
 [73.57753 ]
 [72.474915]
 [71.03399 ]
 [69.764595]], R is [[74.77941132]
 [74.77046204]
 [74.76074219]
 [74.75065613]
 [74.74103546]].
[2019-03-27 06:25:16,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1378207e-14 1.0000000e+00 5.4756110e-19 3.5714588e-11 6.8163104e-24], sum to 1.0000
[2019-03-27 06:25:16,087] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1310
[2019-03-27 06:25:16,091] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.85, 95.5, 1.0, 2.0, 0.3992332777631029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595434.7233784811, 595434.7233784818, 174110.2442602914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1974600.0000, 
sim time next is 1975200.0000, 
raw observation next is [22.9, 95.66666666666667, 1.0, 2.0, 0.4016734466723614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 597582.7548619639, 597582.7548619632, 174263.4205400444], 
processed observation next is [1.0, 0.8695652173913043, 0.2843601895734597, 0.9566666666666667, 1.0, 1.0, 0.2791246345450137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16599520968387887, 0.16599520968387868, 0.2600946575224543], 
reward next is 0.7399, 
noisyNet noise sample is [array([0.6069055], dtype=float32), 0.19431585]. 
=============================================
[2019-03-27 06:25:25,493] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0576702e-07 9.2467499e-01 7.7884060e-11 7.5324848e-02 1.2441920e-14], sum to 1.0000
[2019-03-27 06:25:25,502] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9721
[2019-03-27 06:25:25,508] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.78333333333333, 74.66666666666667, 1.0, 2.0, 0.3383608533670294, 1.0, 1.0, 0.3383608533670294, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 945719.3216288825, 945719.3216288825, 257714.2436596856], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1703400.0000, 
sim time next is 1704000.0000, 
raw observation next is [28.66666666666667, 75.33333333333334, 1.0, 2.0, 0.2420241310209511, 1.0, 2.0, 0.2420241310209511, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 676373.0683321272, 676373.0683321272, 239660.4622887032], 
processed observation next is [1.0, 0.7391304347826086, 0.5576619273301741, 0.7533333333333334, 1.0, 1.0, 0.08677606147102541, 1.0, 1.0, 0.08677606147102541, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18788140787003532, 0.18788140787003532, 0.3577021825204525], 
reward next is 0.6423, 
noisyNet noise sample is [array([1.9905252], dtype=float32), -0.32367128]. 
=============================================
[2019-03-27 06:25:25,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[42.481747]
 [39.898643]
 [40.41763 ]
 [38.728493]
 [41.098804]], R is [[46.02877426]
 [45.56848526]
 [45.1128006 ]
 [44.6616745 ]
 [44.21505737]].
[2019-03-27 06:25:37,202] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 06:25:37,204] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:25:37,205] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:25:37,205] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:25:37,206] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:25:37,207] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:25:37,208] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:25:37,208] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:25:37,209] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:25:37,210] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:25:37,213] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:25:37,224] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run37
[2019-03-27 06:25:37,225] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run37
[2019-03-27 06:25:37,263] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run37
[2019-03-27 06:25:37,279] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run37
[2019-03-27 06:25:37,302] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run37
[2019-03-27 06:26:47,474] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01364778], dtype=float32), 0.040687516]
[2019-03-27 06:26:47,475] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.3, 65.33333333333334, 1.0, 2.0, 0.9288466230768315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1298285.140221929, 1298285.140221928, 277999.371416439]
[2019-03-27 06:26:47,476] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:26:47,480] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8564953e-15 1.0000000e+00 5.7905254e-20 8.3335845e-13 9.2448483e-25], sampled 0.07657080254183368
[2019-03-27 06:27:31,321] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.3508 2842782731.7192 1131.0000
[2019-03-27 06:27:31,405] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.7793 3007825347.1524 1765.0000
[2019-03-27 06:27:31,581] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.2148 3164115325.0973 1772.0000
[2019-03-27 06:27:31,585] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.6913 2927494213.2100 1338.0000
[2019-03-27 06:27:31,725] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8687 2779234941.0559 933.0000
[2019-03-27 06:27:32,740] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 900000, evaluation results [900000.0, 7884.21477793177, 3164115325.0972614, 1772.0, 8252.691278872726, 2927494213.2100177, 1338.0, 8659.868701540174, 2779234941.055889, 933.0, 7995.779314824483, 3007825347.152394, 1765.0, 8494.350768626628, 2842782731.7191825, 1131.0]
[2019-03-27 06:27:34,036] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.1958143e-14 1.0000000e+00 2.1026295e-19 3.0591744e-11 5.1752232e-23], sum to 1.0000
[2019-03-27 06:27:34,044] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3415
[2019-03-27 06:27:34,051] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.23333333333333, 95.66666666666667, 1.0, 2.0, 0.4659344309157669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656519.0869793519, 656519.0869793525, 179136.0009251675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1906800.0000, 
sim time next is 1907400.0000, 
raw observation next is [24.21666666666667, 95.83333333333333, 1.0, 2.0, 0.466590250540021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657309.3593871044, 657309.3593871044, 179215.8329127621], 
processed observation next is [1.0, 0.043478260869565216, 0.34676145339652464, 0.9583333333333333, 1.0, 1.0, 0.35733765125303735, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18258593316308455, 0.18258593316308455, 0.26748631778024196], 
reward next is 0.7325, 
noisyNet noise sample is [array([-0.686633], dtype=float32), 0.6063622]. 
=============================================
[2019-03-27 06:27:41,091] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9439292e-12 9.9999297e-01 1.5405816e-15 6.9748362e-06 1.0678972e-19], sum to 1.0000
[2019-03-27 06:27:41,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5721
[2019-03-27 06:27:41,109] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1997079.348321315 W.
[2019-03-27 06:27:41,115] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.33333333333333, 89.0, 1.0, 2.0, 0.7141678884960919, 1.0, 2.0, 0.7141678884960919, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1997079.348321315, 1997079.348321315, 380242.2140945058], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2468400.0000, 
sim time next is 2469000.0000, 
raw observation next is [26.36666666666667, 89.0, 1.0, 2.0, 0.7151008901369159, 1.0, 2.0, 0.7151008901369159, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1999690.803291218, 1999690.803291218, 380649.8213341391], 
processed observation next is [1.0, 0.5652173913043478, 0.4486571879936811, 0.89, 1.0, 1.0, 0.6567480604059228, 1.0, 1.0, 0.6567480604059228, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5554696675808939, 0.5554696675808939, 0.5681340616927449], 
reward next is 0.4319, 
noisyNet noise sample is [array([-1.0196755], dtype=float32), 1.3507758]. 
=============================================
[2019-03-27 06:27:41,137] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[53.16813 ]
 [53.98651 ]
 [55.30368 ]
 [52.677467]
 [48.44525 ]], R is [[52.2836647 ]
 [52.19330597]
 [52.05924606]
 [51.53865433]
 [51.02326965]].
[2019-03-27 06:27:43,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2968333e-16 1.0000000e+00 4.1885208e-20 3.9875629e-12 2.7920657e-25], sum to 1.0000
[2019-03-27 06:27:43,965] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2684
[2019-03-27 06:27:43,972] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 93.0, 1.0, 2.0, 0.5228074776523001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730553.1815580953, 730553.1815580946, 187246.6412671545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2157000.0000, 
sim time next is 2157600.0000, 
raw observation next is [26.0, 93.0, 1.0, 2.0, 0.5209833541982788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728003.3407383447, 728003.3407383454, 186948.9830338373], 
processed observation next is [0.0, 1.0, 0.4312796208530806, 0.93, 1.0, 1.0, 0.42287151108226356, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20222315020509576, 0.20222315020509596, 0.27902833288632434], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.22733247], dtype=float32), 0.6373658]. 
=============================================
[2019-03-27 06:27:44,712] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0653643e-14 1.0000000e+00 2.1101494e-20 5.7424572e-14 1.2400598e-24], sum to 1.0000
[2019-03-27 06:27:44,719] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5012
[2019-03-27 06:27:44,723] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 94.5, 1.0, 2.0, 0.4782479117612639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668267.5916708254, 668267.5916708247, 180258.1185356054], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2095800.0000, 
sim time next is 2096400.0000, 
raw observation next is [24.9, 94.0, 1.0, 2.0, 0.4797412250532267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670354.8935108938, 670354.8935108938, 180482.9102815486], 
processed observation next is [0.0, 0.2608695652173913, 0.3791469194312796, 0.94, 1.0, 1.0, 0.37318219885930926, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18620969264191495, 0.18620969264191495, 0.2693774780321621], 
reward next is 0.7306, 
noisyNet noise sample is [array([0.87506837], dtype=float32), -2.3665762]. 
=============================================
[2019-03-27 06:27:46,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8807395e-15 1.0000000e+00 2.9415564e-20 4.5719266e-13 7.4303144e-25], sum to 1.0000
[2019-03-27 06:27:46,749] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8459
[2019-03-27 06:27:46,753] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 89.33333333333334, 1.0, 2.0, 0.5373715115108973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750911.6547711656, 750911.654771165, 189658.679064698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2149800.0000, 
sim time next is 2150400.0000, 
raw observation next is [26.93333333333334, 89.66666666666667, 1.0, 2.0, 0.5391069384648353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753337.5638576915, 753337.5638576915, 189950.0373977612], 
processed observation next is [0.0, 0.9130434782608695, 0.4755134281200636, 0.8966666666666667, 1.0, 1.0, 0.4447071547769099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2092604344049143, 0.2092604344049143, 0.2835075185041212], 
reward next is 0.7165, 
noisyNet noise sample is [array([-1.507975], dtype=float32), -0.9758928]. 
=============================================
[2019-03-27 06:27:46,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6389743e-15 1.0000000e+00 6.4462619e-19 1.2295679e-12 5.3832054e-24], sum to 1.0000
[2019-03-27 06:27:46,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4275
[2019-03-27 06:27:46,802] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333333, 92.33333333333334, 1.0, 2.0, 0.4297736264683027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 626709.2718968348, 626709.2718968341, 176675.7676320548], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2611200.0000, 
sim time next is 2611800.0000, 
raw observation next is [23.8, 92.5, 1.0, 2.0, 0.4289859438025845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 625845.1367824614, 625845.1367824607, 176599.1175567735], 
processed observation next is [0.0, 0.21739130434782608, 0.3270142180094788, 0.925, 1.0, 1.0, 0.3120312575934752, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17384587132846152, 0.17384587132846133, 0.26358077247279627], 
reward next is 0.7364, 
noisyNet noise sample is [array([1.6136458], dtype=float32), 1.5944484]. 
=============================================
[2019-03-27 06:27:49,278] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9272060e-14 1.0000000e+00 4.7434693e-18 1.4192191e-12 2.2321365e-23], sum to 1.0000
[2019-03-27 06:27:49,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9887
[2019-03-27 06:27:49,295] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 94.5, 1.0, 2.0, 0.562948585415229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786665.7550940972, 786665.7550940979, 194041.5687225549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2179800.0000, 
sim time next is 2180400.0000, 
raw observation next is [25.36666666666667, 93.66666666666666, 1.0, 2.0, 0.5759891990431278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804895.6455888246, 804895.6455888246, 196352.5197220974], 
processed observation next is [1.0, 0.21739130434782608, 0.40126382306477115, 0.9366666666666665, 1.0, 1.0, 0.4891436133049733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2235821237746735, 0.2235821237746735, 0.2930634622717872], 
reward next is 0.7069, 
noisyNet noise sample is [array([0.6943186], dtype=float32), 0.0029210187]. 
=============================================
[2019-03-27 06:28:01,281] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7505394e-10 9.9792647e-01 5.8683288e-13 2.0735089e-03 4.9914863e-16], sum to 1.0000
[2019-03-27 06:28:01,291] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9516
[2019-03-27 06:28:01,297] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2535434.456433351 W.
[2019-03-27 06:28:01,300] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.6, 63.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 8.434988546475331, 6.9112, 168.9042893508144, 2535434.456433351, 1454461.885867381, 310616.2616306461], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2379600.0000, 
sim time next is 2380200.0000, 
raw observation next is [32.63333333333333, 63.0, 1.0, 2.0, 0.4546091703243873, 1.0, 1.0, 0.4546091703243873, 1.0, 1.0, 0.789505787303481, 6.9112, 6.9112, 170.5573041426782, 1906804.518227998, 1906804.518227998, 385177.2971482481], 
processed observation next is [1.0, 0.5652173913043478, 0.7456556082148499, 0.63, 1.0, 1.0, 0.34290261484865936, 1.0, 0.5, 0.34290261484865936, 1.0, 0.5, 0.7432997406140012, 0.0, 0.0, 0.8375144448122397, 0.5296679217299994, 0.5296679217299994, 0.5748914882809674], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.64161617], dtype=float32), -0.46577442]. 
=============================================
[2019-03-27 06:28:26,344] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 06:28:26,345] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:28:26,346] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:28:26,348] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:28:26,352] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:28:26,352] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:28:26,353] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:28:26,354] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:28:26,355] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:28:26,355] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:28:26,356] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:28:26,381] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run38
[2019-03-27 06:28:26,401] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run38
[2019-03-27 06:28:26,422] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run38
[2019-03-27 06:28:26,450] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run38
[2019-03-27 06:28:26,472] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run38
[2019-03-27 06:28:36,605] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01465859], dtype=float32), 0.04057558]
[2019-03-27 06:28:36,606] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.2, 54.33333333333334, 1.0, 2.0, 0.4914550032374267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773958.4716010699, 773958.4716010693, 192344.4790392769]
[2019-03-27 06:28:36,607] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:28:36,610] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1938876e-15 1.0000000e+00 1.6761003e-19 3.9020748e-13 1.0724298e-23], sampled 0.15734656524272772
[2019-03-27 06:28:49,486] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01465859], dtype=float32), 0.04057558]
[2019-03-27 06:28:49,487] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.36429146, 50.35448053, 1.0, 2.0, 0.3053404048799062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 488172.4422044204, 488172.4422044198, 166233.4260197252]
[2019-03-27 06:28:49,490] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:28:49,492] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3483807e-15 1.0000000e+00 3.3821218e-20 1.5117128e-13 1.3136571e-24], sampled 0.49678136180695986
[2019-03-27 06:29:28,643] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01465859], dtype=float32), 0.04057558]
[2019-03-27 06:29:28,644] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.640617743256142, 6.9112, 169.6696770914696, 2686948.683060544, 1454543.732831527, 310321.7702259737]
[2019-03-27 06:29:28,645] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:29:28,650] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.2129656e-15 1.0000000e+00 1.1411669e-19 1.4160555e-12 2.8647849e-24], sampled 0.232461746927505
[2019-03-27 06:29:28,650] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2686948.683060544 W.
[2019-03-27 06:29:52,067] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01465859], dtype=float32), 0.04057558]
[2019-03-27 06:29:52,068] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.55, 63.0, 1.0, 2.0, 0.6768959492254499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 945966.9948064346, 945966.994806434, 215947.7440325813]
[2019-03-27 06:29:52,070] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:29:52,074] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.8026783e-15 1.0000000e+00 5.3569364e-19 6.6623679e-11 6.6792680e-25], sampled 0.6980966167153249
[2019-03-27 06:29:55,565] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01465859], dtype=float32), 0.04057558]
[2019-03-27 06:29:55,566] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.84525818333334, 94.19190277333333, 1.0, 2.0, 0.5426613321388324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758306.1774338896, 758306.1774338896, 190548.5031109912]
[2019-03-27 06:29:55,568] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:29:55,569] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6396185e-15 1.0000000e+00 7.7668085e-20 1.0244718e-12 2.3011907e-24], sampled 0.521526695364074
[2019-03-27 06:30:06,728] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01465859], dtype=float32), 0.04057558]
[2019-03-27 06:30:06,730] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.46666666666667, 44.33333333333334, 1.0, 2.0, 0.55094422179292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769884.7481595799, 769884.7481595805, 191963.2363247429]
[2019-03-27 06:30:06,731] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:30:06,733] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1165641e-15 1.0000000e+00 2.6677332e-20 3.0510541e-13 5.8744776e-25], sampled 0.659903320431984
[2019-03-27 06:30:21,233] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.7501 2927548216.5439 1338.0000
[2019-03-27 06:30:21,400] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.5162 2842256060.4007 1126.0000
[2019-03-27 06:30:21,491] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.5816 3007814638.5370 1766.0000
[2019-03-27 06:30:21,771] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 06:30:22,031] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.4733 3163841021.4623 1769.0000
[2019-03-27 06:30:23,050] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 925000, evaluation results [925000.0, 7886.4733351406485, 3163841021.462344, 1769.0, 8252.75007418401, 2927548216.5439415, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7996.581638457047, 3007814638.536975, 1766.0, 8499.516154102183, 2842256060.4007277, 1126.0]
[2019-03-27 06:30:24,761] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.3078479e-15 1.0000000e+00 1.4036754e-19 5.6462573e-12 2.0138219e-24], sum to 1.0000
[2019-03-27 06:30:24,772] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4376
[2019-03-27 06:30:24,778] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.3512009365583582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 540994.8831060061, 540994.8831060055, 169897.9269692278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2851800.0000, 
sim time next is 2852400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3505705274872588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 540041.7948673894, 540041.79486739, 169819.9699637635], 
processed observation next is [1.0, 0.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2175548523942877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15001160968538593, 0.1500116096853861, 0.2534626417369604], 
reward next is 0.7465, 
noisyNet noise sample is [array([0.374521], dtype=float32), 0.98558164]. 
=============================================
[2019-03-27 06:30:36,779] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4139385e-16 1.0000000e+00 2.8860593e-20 7.8580632e-12 1.7438616e-24], sum to 1.0000
[2019-03-27 06:30:36,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9018
[2019-03-27 06:30:36,792] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7547629212847533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1054840.697331774, 1054840.697331774, 233106.3449193097], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3552600.0000, 
sim time next is 3553200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7268589847660603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1015824.093397069, 1015824.093397069, 226746.1007951709], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.670914439477181, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28217335927696363, 0.28217335927696363, 0.3384270161121954], 
reward next is 0.6616, 
noisyNet noise sample is [array([1.7863287], dtype=float32), -0.87999874]. 
=============================================
[2019-03-27 06:30:55,405] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8990080e-08 2.3004580e-01 5.6367444e-10 7.6995420e-01 1.0334878e-14], sum to 1.0000
[2019-03-27 06:30:55,413] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8768
[2019-03-27 06:30:55,417] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.7342229982238726, 1.0, 2.0, 0.7342229982238726, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2053214.645421428, 2053214.645421428, 389132.7439369059], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3416400.0000, 
sim time next is 3417000.0000, 
raw observation next is [33.16666666666666, 66.33333333333334, 1.0, 2.0, 0.6606507283953234, 1.0, 2.0, 0.6606507283953234, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1847296.423968045, 1847296.423968045, 357748.864424184], 
processed observation next is [1.0, 0.5652173913043478, 0.7709320695102682, 0.6633333333333334, 1.0, 1.0, 0.59114545589798, 1.0, 1.0, 0.59114545589798, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5131378955466792, 0.5131378955466792, 0.5339535289913194], 
reward next is 0.4660, 
noisyNet noise sample is [array([-0.22403486], dtype=float32), 0.83587253]. 
=============================================
[2019-03-27 06:30:55,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[38.83929 ]
 [37.0405  ]
 [35.616024]
 [34.4314  ]
 [37.202633]], R is [[41.43743515]
 [41.44226456]
 [41.41096497]
 [40.99685669]
 [40.87001801]].
[2019-03-27 06:30:57,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8609578e-08 8.8832201e-03 1.8235595e-10 9.9111676e-01 3.3494564e-13], sum to 1.0000
[2019-03-27 06:30:57,173] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1863
[2019-03-27 06:30:57,178] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.66666666666666, 66.66666666666667, 1.0, 2.0, 0.5775927261410128, 1.0, 1.0, 0.5775927261410128, 1.0, 2.0, 1.003087552473557, 6.9112, 6.9112, 170.5573041426782, 2423193.982100603, 2423193.982100603, 472948.9560262106], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3498000.0000, 
sim time next is 3498600.0000, 
raw observation next is [31.83333333333333, 66.83333333333333, 1.0, 2.0, 0.8655457947868253, 1.0, 2.0, 0.8655457947868253, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2420833.096674958, 2420833.096674958, 453045.4553123562], 
processed observation next is [1.0, 0.4782608695652174, 0.7077409162717218, 0.6683333333333333, 1.0, 1.0, 0.8380069816708738, 1.0, 1.0, 0.8380069816708738, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.672453637965266, 0.672453637965266, 0.67618724673486], 
reward next is 0.3238, 
noisyNet noise sample is [array([0.6211984], dtype=float32), 0.80964917]. 
=============================================
[2019-03-27 06:30:57,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4412308e-14 1.0000000e+00 1.1104706e-19 7.6316273e-11 2.1627243e-25], sum to 1.0000
[2019-03-27 06:30:57,783] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7099
[2019-03-27 06:30:57,787] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5157695029439897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720715.2215249649, 720715.2215249649, 186104.0597786985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3440400.0000, 
sim time next is 3441000.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.516028016286732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721076.5801110404, 721076.5801110397, 186145.6822240083], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.41690122444184574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20029905003084456, 0.20029905003084436, 0.2778293764537437], 
reward next is 0.7222, 
noisyNet noise sample is [array([-1.9226396], dtype=float32), -0.026576368]. 
=============================================
[2019-03-27 06:30:57,799] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.92707 ]
 [76.407715]
 [76.54113 ]
 [76.69163 ]
 [76.40702 ]], R is [[75.35506439]
 [75.32374573]
 [75.29252625]
 [75.26134491]
 [75.2301178 ]].
[2019-03-27 06:30:57,956] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0318286e-09 1.2968437e-01 3.7534784e-11 8.7031567e-01 1.5566216e-15], sum to 1.0000
[2019-03-27 06:30:57,966] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1758
[2019-03-27 06:30:57,973] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.5, 63.0, 1.0, 2.0, 0.8954235669549364, 1.0, 2.0, 0.8954235669549364, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2504481.570045247, 2504481.570045247, 469002.036766772], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3515400.0000, 
sim time next is 3516000.0000, 
raw observation next is [32.33333333333334, 64.33333333333333, 1.0, 2.0, 0.8826176247540928, 1.0, 2.0, 0.8826176247540928, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2468628.227431213, 2468628.227431213, 462099.0907455505], 
processed observation next is [1.0, 0.6956521739130435, 0.7314375987361774, 0.6433333333333333, 1.0, 1.0, 0.8585754515109552, 1.0, 1.0, 0.8585754515109552, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6857300631753369, 0.6857300631753369, 0.6897001354411202], 
reward next is 0.3103, 
noisyNet noise sample is [array([0.15439582], dtype=float32), -1.1881338]. 
=============================================
[2019-03-27 06:30:57,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[40.473763]
 [39.770813]
 [39.4524  ]
 [42.300377]
 [41.07905 ]], R is [[41.32957077]
 [41.21627045]
 [40.80410767]
 [40.67361832]
 [40.57471085]].
[2019-03-27 06:31:03,825] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3206996e-08 1.1480723e-02 9.5242168e-11 9.8851925e-01 1.7744907e-14], sum to 1.0000
[2019-03-27 06:31:03,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7311
[2019-03-27 06:31:03,838] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 1.037078361710513, 1.0, 2.0, 1.037078361710513, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2901147.193166377, 2901147.193166377, 551797.9811329399], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4024200.0000, 
sim time next is 4024800.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.011951729407145, 6.9112, 170.5573041426782, 2981586.384356617, 2909413.822263957, 553121.1893748404], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.6, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.010075172940714517, 0.0, 0.8375144448122397, 0.8282184400990603, 0.8081705061844325, 0.825554013992299], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.14920966], dtype=float32), -1.6885409]. 
=============================================
[2019-03-27 06:31:03,970] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2386243e-13 1.0000000e+00 1.1580996e-18 1.7822246e-10 1.4407092e-22], sum to 1.0000
[2019-03-27 06:31:03,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7740
[2019-03-27 06:31:03,988] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 77.33333333333333, 1.0, 2.0, 0.4867438890485498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680143.0305494744, 680143.0305494744, 181544.9979083273], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3548400.0000, 
sim time next is 3549000.0000, 
raw observation next is [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.485625832047042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678580.234230278, 678580.2342302772, 181374.384125974], 
processed observation next is [1.0, 0.043478260869565216, 0.4865718799368086, 0.7816666666666667, 1.0, 1.0, 0.38027208680366503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18849450950841054, 0.18849450950841035, 0.2707080360089164], 
reward next is 0.7293, 
noisyNet noise sample is [array([2.1577327], dtype=float32), -0.86512387]. 
=============================================
[2019-03-27 06:31:03,996] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.83448 ]
 [64.785095]
 [64.81018 ]
 [64.860596]
 [64.92842 ]], R is [[65.00328064]
 [65.08228302]
 [65.16017914]
 [65.23691559]
 [65.31242371]].
[2019-03-27 06:31:16,057] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0356315e-09 9.9510890e-01 9.5103318e-12 4.8910743e-03 4.2490547e-14], sum to 1.0000
[2019-03-27 06:31:16,066] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7926
[2019-03-27 06:31:16,073] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1906192.659135151 W.
[2019-03-27 06:31:16,083] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 66.5, 1.0, 2.0, 0.6816951361700987, 1.0, 1.0, 0.6816951361700987, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1906192.659135151, 1906192.659135151, 366380.9933242563], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3749400.0000, 
sim time next is 3750000.0000, 
raw observation next is [30.33333333333333, 65.33333333333334, 1.0, 2.0, 0.4515583822607385, 1.0, 2.0, 0.4515583822607385, 1.0, 1.0, 0.7721969295607479, 6.9112, 6.9112, 170.5573041426782, 1893997.037466506, 1893997.037466506, 381315.891413391], 
processed observation next is [1.0, 0.391304347826087, 0.6366508688783569, 0.6533333333333334, 1.0, 1.0, 0.339226966579203, 1.0, 1.0, 0.339226966579203, 1.0, 0.5, 0.7221913775131071, 0.0, 0.0, 0.8375144448122397, 0.5261102881851406, 0.5261102881851406, 0.5691281961393896], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4189616], dtype=float32), 1.5372583]. 
=============================================
[2019-03-27 06:31:16,099] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[36.42333 ]
 [37.89963 ]
 [41.106674]
 [43.09709 ]
 [46.42601 ]], R is [[31.94079781]
 [31.62138939]
 [31.30517578]
 [30.99212456]
 [30.68220329]].
[2019-03-27 06:31:16,788] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 06:31:16,790] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:31:16,791] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:31:16,792] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:31:16,793] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:31:16,792] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:31:16,796] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:31:16,794] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:31:16,798] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:31:16,795] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:31:16,800] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:31:16,816] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run39
[2019-03-27 06:31:16,835] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run39
[2019-03-27 06:31:16,852] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run39
[2019-03-27 06:31:16,874] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run39
[2019-03-27 06:31:16,894] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run39
[2019-03-27 06:32:07,255] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01669974], dtype=float32), 0.04010754]
[2019-03-27 06:32:07,256] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.33333333333334, 65.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.65947544102357, 6.9112, 168.9090664806431, 2815007.609424041, 2284167.483326573, 474429.5539470987]
[2019-03-27 06:32:07,258] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:32:07,262] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3918592e-08 9.2453039e-01 9.0970974e-11 7.5469591e-02 4.1377555e-15], sampled 0.7017961715212405
[2019-03-27 06:32:07,263] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2815007.609424041 W.
[2019-03-27 06:32:36,026] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01669974], dtype=float32), 0.04010754]
[2019-03-27 06:32:36,028] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.86666666666667, 70.33333333333334, 1.0, 2.0, 0.6017915714257089, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129565092009, 840966.5284560139, 840966.5284560146, 201077.6521664786]
[2019-03-27 06:32:36,029] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:32:36,030] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.5447783e-10 9.9994349e-01 1.3863703e-12 5.6448760e-05 4.1923985e-17], sampled 0.2316895890309345
[2019-03-27 06:32:58,823] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01669974], dtype=float32), 0.04010754]
[2019-03-27 06:32:58,824] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.75, 70.83333333333334, 1.0, 2.0, 0.5129373658261419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 716756.3737050421, 716756.3737050428, 185646.9306249483]
[2019-03-27 06:32:58,825] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:32:58,827] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.6029930e-15 1.0000000e+00 1.1562979e-19 2.7877366e-12 4.7806217e-24], sampled 0.743310644590866
[2019-03-27 06:33:02,204] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01669974], dtype=float32), 0.04010754]
[2019-03-27 06:33:02,205] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.89587900833333, 76.08840533833333, 1.0, 2.0, 0.7339927918036339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1025798.783440922, 1025798.783440923, 228351.1439700803]
[2019-03-27 06:33:02,205] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:33:02,207] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4478083e-14 1.0000000e+00 5.2137349e-19 1.1851446e-11 2.9299272e-23], sampled 0.01020632239625241
[2019-03-27 06:33:12,303] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7892.4191 3163336560.4310 1747.0000
[2019-03-27 06:33:12,467] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.3668 3006883109.0451 1746.0000
[2019-03-27 06:33:12,494] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.7451 2927368008.2729 1332.0000
[2019-03-27 06:33:12,498] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8656.5986 2779344938.4289 934.0000
[2019-03-27 06:33:12,559] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.6939 2842341859.4139 1125.0000
[2019-03-27 06:33:13,575] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 950000, evaluation results [950000.0, 7892.419099014654, 3163336560.4309664, 1747.0, 8254.745084711865, 2927368008.2728868, 1332.0, 8656.598613030048, 2779344938.4288797, 934.0, 8001.3668442284115, 3006883109.0450773, 1746.0, 8498.693851613234, 2842341859.413935, 1125.0]
[2019-03-27 06:33:20,622] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.30760915e-14 1.00000000e+00 1.23785170e-18 4.46563706e-12
 1.24951049e-22], sum to 1.0000
[2019-03-27 06:33:20,624] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2945
[2019-03-27 06:33:20,630] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 90.5, 1.0, 2.0, 0.5781903139495733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 807972.6853141182, 807972.6853141176, 196752.6146647437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3893400.0000, 
sim time next is 3894000.0000, 
raw observation next is [27.66666666666666, 91.0, 1.0, 2.0, 0.577627009638801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807185.2154711109, 807185.2154711109, 196651.3604342198], 
processed observation next is [0.0, 0.043478260869565216, 0.5102685624012636, 0.91, 1.0, 1.0, 0.4911168790828927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22421811540864192, 0.22421811540864192, 0.2935094931854027], 
reward next is 0.7065, 
noisyNet noise sample is [array([-0.26104075], dtype=float32), -0.72415584]. 
=============================================
[2019-03-27 06:33:20,662] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.73689 ]
 [70.71713 ]
 [70.734314]
 [70.75528 ]
 [70.767006]], R is [[70.79347229]
 [70.79187775]
 [70.78994751]
 [70.78768158]
 [70.78580475]].
[2019-03-27 06:33:35,338] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7920228e-14 1.0000000e+00 1.0997472e-17 4.4283721e-10 5.7167494e-23], sum to 1.0000
[2019-03-27 06:33:35,343] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0013
[2019-03-27 06:33:35,349] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 86.5, 1.0, 2.0, 0.792343887860219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1107390.448011893, 1107390.448011894, 242044.6475833661], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4170600.0000, 
sim time next is 4171200.0000, 
raw observation next is [29.33333333333334, 85.66666666666667, 1.0, 2.0, 0.8205747403747187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1146867.573878952, 1146867.573878952, 249028.7923550595], 
processed observation next is [1.0, 0.2608695652173913, 0.5892575039494474, 0.8566666666666667, 1.0, 1.0, 0.7838249884032755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3185743260774867, 0.3185743260774867, 0.37168476470904405], 
reward next is 0.6283, 
noisyNet noise sample is [array([-0.2957042], dtype=float32), 2.0352144]. 
=============================================
[2019-03-27 06:33:37,983] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.0223760e-09 2.5697385e-03 9.5626965e-11 9.9743026e-01 3.0563570e-15], sum to 1.0000
[2019-03-27 06:33:37,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3200
[2019-03-27 06:33:38,003] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.344899801206498, 6.9112, 170.5573041426782, 3220368.427722022, 2909691.61833784, 551330.1618536807], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4204800.0000, 
sim time next is 4205400.0000, 
raw observation next is [36.0, 52.5, 1.0, 2.0, 1.012414594405251, 1.0, 2.0, 1.012414594405251, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2832073.982576526, 2832073.982576526, 536537.3265550699], 
processed observation next is [1.0, 0.6956521739130435, 0.9052132701421801, 0.525, 1.0, 1.0, 1.014957342656929, 1.0, 1.0, 1.014957342656929, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7866872173823682, 0.7866872173823682, 0.8008019799329401], 
reward next is 0.1992, 
noisyNet noise sample is [array([-1.3700475], dtype=float32), 0.25958386]. 
=============================================
[2019-03-27 06:33:43,258] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0531516e-15 1.0000000e+00 1.0324978e-18 6.9648634e-11 5.5879858e-23], sum to 1.0000
[2019-03-27 06:33:43,266] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4159
[2019-03-27 06:33:43,271] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.8244836235566202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1152333.747422435, 1152333.747422435, 250005.8600467185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4761000.0000, 
sim time next is 4761600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7835879753519386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1095146.756755355, 1095146.756755355, 239918.6676532144], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7392626209059501, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30420743243204307, 0.30420743243204307, 0.358087563661514], 
reward next is 0.6419, 
noisyNet noise sample is [array([1.0796695], dtype=float32), 1.724071]. 
=============================================
[2019-03-27 06:33:43,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7778517e-08 2.6999651e-03 6.0248606e-10 9.9730009e-01 3.1919827e-14], sum to 1.0000
[2019-03-27 06:33:43,516] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8234
[2019-03-27 06:33:43,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3378658.055605156 W.
[2019-03-27 06:33:43,531] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 51.83333333333334, 1.0, 2.0, 0.9688791902153022, 1.0, 2.0, 0.8050296346219139, 1.0, 2.0, 1.03, 7.005118942614393, 6.9112, 170.5573041426782, 3378658.055605156, 3311380.09663214, 619721.9486441902], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4284600.0000, 
sim time next is 4285200.0000, 
raw observation next is [38.0, 51.66666666666667, 1.0, 2.0, 0.9808689406533735, 1.0, 2.0, 0.8110245098409493, 1.0, 2.0, 1.03, 7.005119888869157, 6.9112, 170.5573041426782, 3403852.45896842, 3336573.822154626, 624737.9874072333], 
processed observation next is [1.0, 0.6086956521739131, 1.0, 0.5166666666666667, 1.0, 1.0, 0.9769505309076789, 1.0, 1.0, 0.7723186865553606, 1.0, 1.0, 1.0365853658536586, 0.00939198888691566, 0.0, 0.8375144448122397, 0.9455145719356722, 0.9268260617096182, 0.9324447573242288], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.65662974], dtype=float32), 0.4103284]. 
=============================================
[2019-03-27 06:33:43,642] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3894672e-11 9.9994457e-01 1.9196581e-15 5.5390221e-05 3.2262750e-21], sum to 1.0000
[2019-03-27 06:33:43,654] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7706
[2019-03-27 06:33:43,659] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.0, 50.0, 1.0, 2.0, 0.5650363426059064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 789584.276548383, 789584.2765483835, 194414.7933850085], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4298400.0000, 
sim time next is 4299000.0000, 
raw observation next is [36.0, 50.0, 1.0, 2.0, 0.5737720627958658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801796.2179062049, 801796.2179062054, 195962.5317660337], 
processed observation next is [1.0, 0.782608695652174, 0.9052132701421801, 0.5, 1.0, 1.0, 0.4864723648142961, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22272117164061248, 0.22272117164061261, 0.29248139069557266], 
reward next is 0.7075, 
noisyNet noise sample is [array([0.1615696], dtype=float32), -0.58504474]. 
=============================================
[2019-03-27 06:33:43,674] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[65.56973 ]
 [61.93515 ]
 [56.17575 ]
 [50.65627 ]
 [45.327133]], R is [[69.55084229]
 [69.56516266]
 [69.58140564]
 [69.59853363]
 [69.53627014]].
[2019-03-27 06:33:44,652] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0460466e-14 1.0000000e+00 1.8295736e-17 2.9226238e-10 6.1403166e-23], sum to 1.0000
[2019-03-27 06:33:44,657] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5356
[2019-03-27 06:33:44,662] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5163970000001702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721592.3580844634, 721592.3580844641, 186205.1566826971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4740600.0000, 
sim time next is 4741200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5156445368819844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720540.5398331, 720540.5398331007, 186083.7356781453], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4164392010626318, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2001501499536389, 0.20015014995363908, 0.27773691892260494], 
reward next is 0.7223, 
noisyNet noise sample is [array([2.3279026], dtype=float32), 1.0027689]. 
=============================================
[2019-03-27 06:33:51,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0542633e-15 1.0000000e+00 2.2730055e-21 1.2296147e-12 1.4432765e-24], sum to 1.0000
[2019-03-27 06:33:51,267] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8899
[2019-03-27 06:33:51,272] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 67.66666666666667, 1.0, 2.0, 0.5570139373287976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778369.6244875496, 778369.6244875503, 193009.9096949909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4459200.0000, 
sim time next is 4459800.0000, 
raw observation next is [30.33333333333333, 66.83333333333333, 1.0, 2.0, 0.5417347371058826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757010.9071168598, 757010.9071168598, 190392.2489773461], 
processed observation next is [0.0, 0.6086956521739131, 0.6366508688783569, 0.6683333333333333, 1.0, 1.0, 0.4478731772360031, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21028080753246106, 0.21028080753246106, 0.28416753578708376], 
reward next is 0.7158, 
noisyNet noise sample is [array([-1.1597227], dtype=float32), -0.5195398]. 
=============================================
[2019-03-27 06:33:56,577] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.5569082e-17 1.0000000e+00 5.4573202e-20 6.4681306e-13 2.0547634e-24], sum to 1.0000
[2019-03-27 06:33:56,585] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6155
[2019-03-27 06:33:56,591] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 75.66666666666667, 1.0, 2.0, 0.5004519591862049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699304.0666094151, 699304.0666094158, 183666.8470487834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4526400.0000, 
sim time next is 4527000.0000, 
raw observation next is [28.0, 76.5, 1.0, 2.0, 0.5034308039703232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703467.9188803635, 703467.9188803635, 184135.4065179983], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.765, 1.0, 1.0, 0.4017238602052087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1954077552445454, 0.1954077552445454, 0.27482896495223624], 
reward next is 0.7252, 
noisyNet noise sample is [array([1.4060664], dtype=float32), -0.603647]. 
=============================================
[2019-03-27 06:33:56,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.18381]
 [72.18249]
 [72.17741]
 [72.15293]
 [72.16526]], R is [[72.19456482]
 [72.19848633]
 [72.20290375]
 [72.20742035]
 [72.21099854]].
[2019-03-27 06:33:56,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.1947973e-16 1.0000000e+00 3.1730913e-20 7.4144433e-14 3.9730058e-25], sum to 1.0000
[2019-03-27 06:33:56,842] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3786
[2019-03-27 06:33:56,846] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 52.5, 1.0, 2.0, 0.5339334248901505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746105.6575136452, 746105.657513646, 189084.4687387619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4546200.0000, 
sim time next is 4546800.0000, 
raw observation next is [34.0, 53.0, 1.0, 2.0, 0.5366912403143218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749960.7225235285, 749960.7225235285, 189545.519842363], 
processed observation next is [0.0, 0.6521739130434783, 0.8104265402843602, 0.53, 1.0, 1.0, 0.44179667507749615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20832242292320235, 0.20832242292320235, 0.28290376095875075], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.5906508], dtype=float32), 0.7077059]. 
=============================================
[2019-03-27 06:33:59,199] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.0166670e-16 1.0000000e+00 5.2491920e-21 1.7682549e-13 2.0022160e-25], sum to 1.0000
[2019-03-27 06:33:59,210] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5046
[2019-03-27 06:33:59,214] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 62.66666666666667, 1.0, 2.0, 0.5290955894582405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739343.0305724948, 739343.0305724948, 188280.6058050734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4555200.0000, 
sim time next is 4555800.0000, 
raw observation next is [31.0, 64.5, 1.0, 2.0, 0.5295354357464656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739957.8732625517, 739957.8732625517, 188353.3893941975], 
processed observation next is [0.0, 0.7391304347826086, 0.6682464454976303, 0.645, 1.0, 1.0, 0.4331752237909223, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20554385368404215, 0.20554385368404215, 0.2811244617823843], 
reward next is 0.7189, 
noisyNet noise sample is [array([-0.527572], dtype=float32), 1.845043]. 
=============================================
[2019-03-27 06:33:59,222] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1076039e-16 1.0000000e+00 2.6512373e-20 4.4946075e-11 2.1565465e-24], sum to 1.0000
[2019-03-27 06:33:59,233] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7271
[2019-03-27 06:33:59,239] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5498011070409204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768286.7916807865, 768286.791680786, 191766.3772246501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4575600.0000, 
sim time next is 4576200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5507766209849794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769650.4593724336, 769650.4593724343, 191933.6903364013], 
processed observation next is [0.0, 1.0, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45876701323491487, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21379179427012043, 0.21379179427012063, 0.2864681945319423], 
reward next is 0.7135, 
noisyNet noise sample is [array([-1.127076], dtype=float32), 1.9075024]. 
=============================================
[2019-03-27 06:33:59,434] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.6828613e-11 9.9999809e-01 1.0471489e-13 1.8494989e-06 1.2685832e-17], sum to 1.0000
[2019-03-27 06:33:59,441] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2065
[2019-03-27 06:33:59,446] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5734695303502823, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9838335240287258, 6.9112, 6.9112, 168.912956510431, 1603351.661364582, 1603351.661364582, 348315.1687964216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4590000.0000, 
sim time next is 4590600.0000, 
raw observation next is [27.83333333333334, 85.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.95724700743606, 6.9112, 168.9071654307853, 2196339.103897034, 1454262.797176837, 311351.6017179039], 
processed observation next is [1.0, 0.13043478260869565, 0.5181674565560824, 0.8566666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.10460470074360603, 0.0, 0.8294115083000746, 0.610094195526954, 0.4039618881046769, 0.4647038831610506], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1628835], dtype=float32), 1.7908823]. 
=============================================
[2019-03-27 06:34:00,176] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0556301e-15 1.0000000e+00 1.8244971e-20 1.4232555e-12 4.8251772e-25], sum to 1.0000
[2019-03-27 06:34:00,186] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6110
[2019-03-27 06:34:00,192] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.548969785781907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767124.691844724, 767124.691844724, 191624.0174137361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4578000.0000, 
sim time next is 4578600.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5487935177849461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766878.2878554862, 766878.2878554862, 191593.8586549387], 
processed observation next is [0.0, 1.0, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4563777322710194, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21302174662652393, 0.21302174662652393, 0.2859609830670727], 
reward next is 0.7140, 
noisyNet noise sample is [array([-0.24888389], dtype=float32), -2.93394]. 
=============================================
[2019-03-27 06:34:02,615] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5871351e-09 9.6854770e-01 1.9693012e-12 3.1452339e-02 4.3965547e-16], sum to 1.0000
[2019-03-27 06:34:02,625] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0607
[2019-03-27 06:34:02,633] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3797343.97843117 W.
[2019-03-27 06:34:02,637] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 9.043008113872272, 6.9112, 168.9009014415744, 3797343.97843117, 2285073.867470515, 470280.3664994396], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4619400.0000, 
sim time next is 4620000.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.9132226683529263, 1.0, 1.0, 0.777201373690726, 1.0, 2.0, 1.03, 7.005114550591511, 6.9112, 170.5573041426782, 3261712.080789285, 3194437.268000916, 597139.5484235459], 
processed observation next is [1.0, 0.4782608695652174, 0.7156398104265403, 0.71, 1.0, 1.0, 0.8954489980155739, 1.0, 0.5, 0.7315679201093084, 1.0, 1.0, 1.0365853658536586, 0.009391455059151088, 0.0, 0.8375144448122397, 0.9060311335525791, 0.88734368555581, 0.891253057348576], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.2610123], dtype=float32), 0.95919585]. 
=============================================
[2019-03-27 06:34:02,655] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[44.937916]
 [44.664455]
 [42.0712  ]
 [40.227825]
 [37.999973]], R is [[41.52701569]
 [41.11174774]
 [41.02365494]
 [40.88830948]
 [40.75637436]].
[2019-03-27 06:34:04,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.90708216e-17 1.00000000e+00 1.27083595e-20 5.06453646e-12
 3.53513579e-24], sum to 1.0000
[2019-03-27 06:34:04,186] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8369
[2019-03-27 06:34:04,189] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 72.66666666666666, 1.0, 2.0, 0.5276515775416417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737324.5093424097, 737324.509342409, 188041.9838990092], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5078400.0000, 
sim time next is 5079000.0000, 
raw observation next is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.5264006788904537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735575.9358176247, 735575.9358176247, 187835.9249750646], 
processed observation next is [0.0, 0.782608695652174, 0.581358609794629, 0.7333333333333334, 1.0, 1.0, 0.42939840830175147, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20432664883822907, 0.20432664883822907, 0.2803521268284546], 
reward next is 0.7196, 
noisyNet noise sample is [array([0.12211925], dtype=float32), -0.6138214]. 
=============================================
[2019-03-27 06:34:04,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.13115]
 [73.10756]
 [73.06849]
 [73.04649]
 [73.02187]], R is [[73.13961792]
 [73.12756348]
 [73.11528015]
 [73.1028595 ]
 [73.09092712]].
[2019-03-27 06:34:04,411] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2124706e-15 1.0000000e+00 1.6732184e-20 1.9643220e-13 4.3280361e-24], sum to 1.0000
[2019-03-27 06:34:04,420] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0368
[2019-03-27 06:34:04,425] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.5237423621435907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731860.0070143661, 731860.0070143661, 187399.706347494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5126400.0000, 
sim time next is 5127000.0000, 
raw observation next is [29.16666666666667, 72.66666666666667, 1.0, 2.0, 0.5234998566177778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731521.0212447378, 731521.0212447371, 187359.871395574], 
processed observation next is [0.0, 0.34782608695652173, 0.581358609794629, 0.7266666666666667, 1.0, 1.0, 0.42590344170816596, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20320028367909382, 0.20320028367909362, 0.2796415990978717], 
reward next is 0.7204, 
noisyNet noise sample is [array([-0.7228051], dtype=float32), 0.6414834]. 
=============================================
[2019-03-27 06:34:04,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[70.703156]
 [70.69283 ]
 [70.72292 ]
 [70.74113 ]
 [70.74973 ]], R is [[70.71166992]
 [70.72485352]
 [70.7383194 ]
 [70.75209808]
 [70.76625824]].
[2019-03-27 06:34:06,034] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2967038e-07 1.3013482e-01 2.3113307e-09 8.6986494e-01 3.9398806e-13], sum to 1.0000
[2019-03-27 06:34:06,044] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0231
[2019-03-27 06:34:06,053] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2514423.433161768 W.
[2019-03-27 06:34:06,056] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.83333333333334, 70.83333333333333, 1.0, 2.0, 0.5993163291000064, 1.0, 2.0, 0.5993163291000064, 1.0, 1.0, 1.03, 6.92335685920897, 6.9112, 170.5573041426782, 2514423.433161768, 2505714.980400967, 487730.2001997823], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4704600.0000, 
sim time next is 4705200.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.575407942423341, 1.0, 2.0, 0.575407942423341, 1.0, 2.0, 0.9992933056749774, 6.911199999999999, 6.9112, 170.5573041426782, 2414019.232766488, 2414019.232766488, 471207.9698763674], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.48844330412450726, 1.0, 1.0, 0.48844330412450726, 1.0, 1.0, 0.9991381776524113, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6705608979906912, 0.6705608979906912, 0.703295477427414], 
reward next is 0.2967, 
noisyNet noise sample is [array([1.46817], dtype=float32), 0.3261032]. 
=============================================
[2019-03-27 06:34:07,125] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 06:34:07,129] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:34:07,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:34:07,131] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:34:07,132] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:34:07,133] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:34:07,134] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:34:07,135] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:34:07,133] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:34:07,137] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:34:07,138] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:34:07,154] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run40
[2019-03-27 06:34:07,176] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run40
[2019-03-27 06:34:07,177] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run40
[2019-03-27 06:34:07,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run40
[2019-03-27 06:34:07,222] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run40
[2019-03-27 06:34:46,712] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01967901], dtype=float32), 0.040374834]
[2019-03-27 06:34:46,713] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.4972636705872457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694847.4702376963, 694847.4702376963, 183168.6027589912]
[2019-03-27 06:34:46,714] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:34:46,716] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0368078e-15 1.0000000e+00 5.0772827e-20 1.1202818e-12 2.2679785e-24], sampled 0.8445704860881165
[2019-03-27 06:34:51,709] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01967901], dtype=float32), 0.040374834]
[2019-03-27 06:34:51,711] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.1, 90.0, 1.0, 2.0, 0.5183757408309665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724358.3135318041, 724358.3135318048, 186524.6274331459]
[2019-03-27 06:34:51,713] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:34:51,716] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8978155e-15 1.0000000e+00 9.3329606e-20 7.9669344e-12 2.3606645e-24], sampled 0.5253308121427387
[2019-03-27 06:35:06,888] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01967901], dtype=float32), 0.040374834]
[2019-03-27 06:35:06,891] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.89671588166667, 64.889301605, 1.0, 2.0, 0.5903993799682756, 0.0, 2.0, 0.0, 1.0, 2.0, 1.020588247610468, 6.911199999999999, 6.9112, 168.9128864017186, 1650722.289727091, 1650722.289727092, 360537.6077022108]
[2019-03-27 06:35:06,892] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:35:06,895] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.6541447e-09 9.9929845e-01 8.8806592e-12 7.0153800e-04 6.9300228e-16], sampled 0.8299554809623418
[2019-03-27 06:35:09,342] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01967901], dtype=float32), 0.040374834]
[2019-03-27 06:35:09,343] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.5, 57.33333333333333, 1.0, 2.0, 0.5380066837759849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751799.545241202, 751799.545241202, 189764.923392691]
[2019-03-27 06:35:09,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:35:09,349] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1021446e-15 1.0000000e+00 4.2688840e-20 4.5673708e-12 1.1429633e-24], sampled 0.13087463518217757
[2019-03-27 06:35:09,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01967901], dtype=float32), 0.040374834]
[2019-03-27 06:35:09,543] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.36666666666667, 64.66666666666667, 1.0, 2.0, 0.9006100019831147, 1.0, 2.0, 0.9006100019831147, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2518968.688036515, 2518968.688036515, 472305.6409439725]
[2019-03-27 06:35:09,544] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:35:09,546] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6235691e-09 9.9993598e-01 1.6879652e-12 6.3999250e-05 3.6734967e-16], sampled 0.02869610714330284
[2019-03-27 06:35:09,547] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2518968.688036515 W.
[2019-03-27 06:35:28,557] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01967901], dtype=float32), 0.040374834]
[2019-03-27 06:35:28,557] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.7, 95.0, 1.0, 2.0, 0.680513616185878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 951024.9742693322, 951024.9742693322, 216694.2386626063]
[2019-03-27 06:35:28,559] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:35:28,562] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.41743872e-15 1.00000000e+00 2.15660138e-19 7.56743973e-12
 1.09761475e-23], sampled 0.8784979452689818
[2019-03-27 06:35:35,279] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01967901], dtype=float32), 0.040374834]
[2019-03-27 06:35:35,280] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.9, 81.66666666666666, 1.0, 2.0, 0.8839465227839559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1235490.003618527, 1235490.003618528, 265562.0930880198]
[2019-03-27 06:35:35,281] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:35:35,283] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.9477562e-15 1.0000000e+00 1.5713583e-19 9.2417358e-12 4.7546508e-24], sampled 0.5432847060849869
[2019-03-27 06:35:44,729] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01967901], dtype=float32), 0.040374834]
[2019-03-27 06:35:44,730] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.3, 91.33333333333333, 1.0, 2.0, 0.501973089265814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701430.3119564517, 701430.3119564517, 183904.7902760261]
[2019-03-27 06:35:44,733] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:35:44,735] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0362484e-14 1.0000000e+00 3.5225585e-19 1.4232535e-11 1.6487335e-23], sampled 0.8002610488498868
[2019-03-27 06:35:45,949] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01967901], dtype=float32), 0.040374834]
[2019-03-27 06:35:45,950] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.11978778666667, 63.87075599333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.937926947447855, 6.9112, 168.9125084199044, 1472728.879690206, 1453767.913599745, 311347.916175983]
[2019-03-27 06:35:45,950] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:35:45,952] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.4161123e-14 1.0000000e+00 3.5804765e-18 3.1892353e-10 2.0032427e-22], sampled 0.17287912235839575
[2019-03-27 06:36:02,736] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2194 2779483784.2180 933.0000
[2019-03-27 06:36:02,784] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.8968 2927326531.0849 1336.0000
[2019-03-27 06:36:02,812] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.4064 3163985901.0226 1763.0000
[2019-03-27 06:36:02,866] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1212 3007638581.4836 1766.0000
[2019-03-27 06:36:02,929] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.2673 2842320545.2084 1126.0000
[2019-03-27 06:36:03,945] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 975000, evaluation results [975000.0, 7885.406408907808, 3163985901.0225563, 1763.0, 8252.896789407403, 2927326531.0848923, 1336.0, 8659.219407755714, 2779483784.217976, 933.0, 7998.121210679458, 3007638581.4836097, 1766.0, 8495.267321436892, 2842320545.2084208, 1126.0]
[2019-03-27 06:36:11,340] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0433047e-14 1.0000000e+00 3.8648941e-19 1.0474681e-10 3.0969198e-25], sum to 1.0000
[2019-03-27 06:36:11,351] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8778
[2019-03-27 06:36:11,355] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 71.33333333333334, 1.0, 2.0, 0.4976041737908742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695323.4254254154, 695323.4254254154, 183221.5086271375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [28.5, 72.0, 1.0, 2.0, 0.4954519769395319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692315.0894553369, 692315.0894553375, 182886.5316965814], 
processed observation next is [1.0, 0.8260869565217391, 0.5497630331753555, 0.72, 1.0, 1.0, 0.39211081558979743, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19230974707092693, 0.19230974707092707, 0.27296497268146475], 
reward next is 0.7270, 
noisyNet noise sample is [array([1.0193766], dtype=float32), 0.6614163]. 
=============================================
[2019-03-27 06:36:18,172] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3107561e-13 1.0000000e+00 2.3832744e-18 4.8092108e-09 4.2093073e-22], sum to 1.0000
[2019-03-27 06:36:18,180] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9168
[2019-03-27 06:36:18,185] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5940333328374745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830120.6328028388, 830120.6328028388, 199632.858922759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4942200.0000, 
sim time next is 4942800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5994737717057516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 837726.269262195, 837726.2692621943, 200640.8238202819], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.5174382791635561, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23270174146172082, 0.23270174146172062, 0.29946391614967444], 
reward next is 0.7005, 
noisyNet noise sample is [array([0.6825313], dtype=float32), -0.44317263]. 
=============================================
[2019-03-27 06:36:19,384] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8742409e-07 4.3627268e-01 5.1654085e-09 5.6372714e-01 7.3150654e-13], sum to 1.0000
[2019-03-27 06:36:19,392] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9490
[2019-03-27 06:36:19,397] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 69.33333333333333, 1.0, 2.0, 0.4924771984681712, 1.0, 1.0, 0.4924771984681712, 1.0, 2.0, 0.8470354873803471, 6.9112, 6.9112, 170.5573041426782, 2065790.628563377, 2065790.628563377, 408515.9887404732], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4960200.0000, 
sim time next is 4960800.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.6489761076081814, 1.0, 2.0, 0.6489761076081814, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1814624.47433577, 1814624.474335769, 353044.7757906563], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.7, 1.0, 1.0, 0.5770796477207004, 1.0, 1.0, 0.5770796477207004, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5040623539821583, 0.504062353982158, 0.526932501180084], 
reward next is 0.4731, 
noisyNet noise sample is [array([-0.5586654], dtype=float32), 0.21413733]. 
=============================================
[2019-03-27 06:36:42,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.6529667e-14 1.0000000e+00 8.4320864e-18 4.5678652e-09 1.8907230e-22], sum to 1.0000
[2019-03-27 06:36:42,012] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6799
[2019-03-27 06:36:42,021] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.68333333333334, 84.83333333333334, 1.0, 2.0, 0.9271896082850597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1295967.652439482, 1295967.652439482, 277531.9163570391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5379000.0000, 
sim time next is 5379600.0000, 
raw observation next is [29.86666666666667, 83.66666666666667, 1.0, 2.0, 1.00421400093909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1403698.707146698, 1403698.707146698, 300218.0830243566], 
processed observation next is [1.0, 0.2608695652173913, 0.6145339652448659, 0.8366666666666667, 1.0, 1.0, 1.0050771095651687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3899163075407494, 0.3899163075407494, 0.4480866910811292], 
reward next is 0.5519, 
noisyNet noise sample is [array([-0.87763816], dtype=float32), 1.2928245]. 
=============================================
[2019-03-27 06:36:50,022] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1540817e-14 1.0000000e+00 3.7897499e-19 3.2234332e-10 1.1549258e-21], sum to 1.0000
[2019-03-27 06:36:50,033] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0431
[2019-03-27 06:36:50,040] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2011510.098922495 W.
[2019-03-27 06:36:50,045] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.26666666666667, 95.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.693277045166149, 6.9112, 169.6884640040826, 2011510.098922495, 1454130.398308694, 311494.5481175783], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5537400.0000, 
sim time next is 5538000.0000, 
raw observation next is [26.23333333333333, 95.0, 1.0, 2.0, 0.4039465503127977, 1.0, 1.0, 0.4039465503127977, 1.0, 1.0, 0.6961135284389518, 6.911199999999999, 6.9112, 170.5573041426782, 1694138.149786136, 1694138.149786137, 354318.8463266812], 
processed observation next is [1.0, 0.08695652173913043, 0.44233807266982617, 0.95, 1.0, 1.0, 0.2818633136298767, 1.0, 0.5, 0.2818633136298767, 1.0, 0.5, 0.6294067419987216, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4705939304961489, 0.47059393049614917, 0.5288340989950465], 
reward next is 0.4712, 
noisyNet noise sample is [array([1.22917], dtype=float32), -0.071429275]. 
=============================================
[2019-03-27 06:36:50,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[63.60494 ]
 [64.38327 ]
 [64.114655]
 [64.03296 ]
 [63.978737]], R is [[56.52284622]
 [55.95761871]
 [56.11336136]
 [56.26752853]
 [56.42026138]].
[2019-03-27 06:36:53,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9788666e-08 2.3707950e-01 7.9933044e-10 7.6292056e-01 9.0412669e-15], sum to 1.0000
[2019-03-27 06:36:53,634] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2344
[2019-03-27 06:36:53,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2628770.142424372 W.
[2019-03-27 06:36:53,648] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.3, 69.0, 1.0, 2.0, 0.9398135781751712, 1.0, 2.0, 0.9398135781751712, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2628770.142424372, 2628770.142424372, 493684.4168900937], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6003000.0000, 
sim time next is 6003600.0000, 
raw observation next is [32.53333333333333, 68.33333333333333, 1.0, 2.0, 0.9071058008915466, 1.0, 2.0, 0.9071058008915466, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2537189.716045716, 2537189.716045716, 475390.9934792155], 
processed observation next is [1.0, 0.4782608695652174, 0.7409162717219588, 0.6833333333333332, 1.0, 1.0, 0.8880792781825863, 1.0, 1.0, 0.8880792781825863, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7047749211238099, 0.7047749211238099, 0.709538796237635], 
reward next is 0.2905, 
noisyNet noise sample is [array([-1.2418458], dtype=float32), -1.1532927]. 
=============================================
[2019-03-27 06:36:57,599] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 06:36:57,605] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:36:57,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:36:57,607] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:36:57,609] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:36:57,610] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:36:57,611] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:36:57,611] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:36:57,610] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:36:57,614] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:36:57,616] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:36:57,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run41
[2019-03-27 06:36:58,001] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run41
[2019-03-27 06:36:58,106] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run41
[2019-03-27 06:36:58,650] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run41
[2019-03-27 06:36:58,650] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run41
[2019-03-27 06:37:24,649] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02141787], dtype=float32), 0.040901035]
[2019-03-27 06:37:24,651] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.60049713, 97.37981236833335, 1.0, 2.0, 0.3952026799930243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628392.8592396224, 628392.8592396224, 177684.9665986985]
[2019-03-27 06:37:24,653] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:37:24,657] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5962676e-15 1.0000000e+00 7.7217569e-20 5.1251321e-13 4.9404158e-24], sampled 0.205328314197473
[2019-03-27 06:37:30,181] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02141787], dtype=float32), 0.040901035]
[2019-03-27 06:37:30,181] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.34014752333334, 83.66200757, 1.0, 2.0, 0.4467785935186172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657944.4918672508, 657944.4918672508, 179929.168057427]
[2019-03-27 06:37:30,185] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:37:30,189] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7512685e-15 1.0000000e+00 5.8749411e-20 2.4587476e-12 1.8935939e-24], sampled 0.20620065612374583
[2019-03-27 06:38:05,401] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02141787], dtype=float32), 0.040901035]
[2019-03-27 06:38:05,403] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.16666666666666, 74.16666666666667, 1.0, 2.0, 0.9127263192643151, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.001447476360211, 6.9112, 168.9123469045359, 2172849.978089194, 2108825.543345491, 437777.9380527213]
[2019-03-27 06:38:05,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:38:05,413] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7092921e-09 9.9967706e-01 3.6839182e-12 3.2290129e-04 5.5860521e-16], sampled 0.9200338696451982
[2019-03-27 06:38:05,414] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2172849.978089194 W.
[2019-03-27 06:38:19,603] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02141787], dtype=float32), 0.040901035]
[2019-03-27 06:38:19,604] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.1, 51.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.28198761749572, 6.9112, 168.905420820586, 2426838.648395193, 1454398.118962389, 310936.9189217404]
[2019-03-27 06:38:19,605] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:38:19,611] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.0159478e-10 9.9995148e-01 2.0092197e-13 4.8481608e-05 2.8535953e-18], sampled 0.17335606765705402
[2019-03-27 06:38:19,614] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2426838.648395193 W.
[2019-03-27 06:38:20,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02141787], dtype=float32), 0.040901035]
[2019-03-27 06:38:20,148] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.250090135, 53.708613955, 1.0, 2.0, 0.9394055924632426, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005482038544359, 6.9112, 168.9123960390102, 2210192.046497508, 2143305.345354835, 445219.1860439288]
[2019-03-27 06:38:20,149] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:38:20,152] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0174935e-09 9.9725729e-01 2.1343602e-12 2.7427373e-03 5.6981084e-17], sampled 0.6824950127611374
[2019-03-27 06:38:20,153] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2210192.046497508 W.
[2019-03-27 06:38:44,726] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02141787], dtype=float32), 0.040901035]
[2019-03-27 06:38:44,726] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.06666666666667, 55.0, 1.0, 2.0, 0.9301100138657986, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00598961818377, 6.9112, 168.9123159546058, 2197182.155205915, 2129935.392675247, 442556.7162510977]
[2019-03-27 06:38:44,727] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:38:44,729] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9023805e-09 9.9961698e-01 2.1075038e-12 3.8302643e-04 1.8906825e-16], sampled 0.48225609705586603
[2019-03-27 06:38:44,732] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2197182.155205915 W.
[2019-03-27 06:38:52,111] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.7473 3164021066.9114 1768.0000
[2019-03-27 06:38:53,095] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1344 2779202222.6088 933.0000
[2019-03-27 06:38:53,237] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.3761 2927548961.9401 1338.0000
[2019-03-27 06:38:53,582] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.1582 3007946251.6583 1766.0000
[2019-03-27 06:38:53,682] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7384 2842412250.9401 1130.0000
[2019-03-27 06:38:54,698] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1000000, evaluation results [1000000.0, 7885.747340385645, 3164021066.9113636, 1768.0, 8253.376063485865, 2927548961.9400806, 1338.0, 8659.13438650126, 2779202222.6087985, 933.0, 7997.158155021997, 3007946251.6582756, 1766.0, 8496.738389433609, 2842412250.940102, 1130.0]
[2019-03-27 06:38:54,737] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.9911676e-14 1.0000000e+00 4.3752531e-21 3.4042569e-12 1.4915519e-25], sum to 1.0000
[2019-03-27 06:38:54,744] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5180
[2019-03-27 06:38:54,749] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 92.16666666666667, 1.0, 2.0, 0.5096127999432747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712109.2127325597, 712109.2127325597, 185115.6167547054], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5626200.0000, 
sim time next is 5626800.0000, 
raw observation next is [25.7, 92.0, 1.0, 2.0, 0.508023626854121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709887.8341999968, 709887.8341999968, 184862.5178650199], 
processed observation next is [0.0, 0.13043478260869565, 0.4170616113744076, 0.92, 1.0, 1.0, 0.4072573817519529, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19719106505555467, 0.19719106505555467, 0.2759142057686864], 
reward next is 0.7241, 
noisyNet noise sample is [array([1.179367], dtype=float32), -0.30462146]. 
=============================================
[2019-03-27 06:39:03,586] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2009843e-14 1.0000000e+00 1.1946429e-18 2.3053277e-10 1.3221591e-23], sum to 1.0000
[2019-03-27 06:39:03,595] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8419
[2019-03-27 06:39:03,600] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.91666666666667, 83.83333333333334, 1.0, 2.0, 0.8759053997930168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1224244.465030682, 1224244.465030681, 263396.2920625035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5813400.0000, 
sim time next is 5814000.0000, 
raw observation next is [28.1, 83.0, 1.0, 2.0, 0.8977171774923044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1254748.588977918, 1254748.588977918, 269311.1353555602], 
processed observation next is [1.0, 0.30434782608695654, 0.5308056872037916, 0.83, 1.0, 1.0, 0.8767676837256679, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34854127471608837, 0.34854127471608837, 0.4019569184411347], 
reward next is 0.5980, 
noisyNet noise sample is [array([-2.601834], dtype=float32), 0.49606395]. 
=============================================
[2019-03-27 06:39:03,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[66.88907 ]
 [67.328415]
 [67.94802 ]
 [68.88573 ]
 [69.77523 ]], R is [[66.75461578]
 [66.69393921]
 [66.62626648]
 [66.53340912]
 [66.47327423]].
[2019-03-27 06:39:09,558] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2543140e-14 1.0000000e+00 1.8159504e-19 9.7067687e-11 2.4602350e-23], sum to 1.0000
[2019-03-27 06:39:09,566] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2172
[2019-03-27 06:39:09,573] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.76666666666667, 95.0, 1.0, 2.0, 0.8652177586961459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1209297.945986567, 1209297.945986568, 260548.0217877897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5888400.0000, 
sim time next is 5889000.0000, 
raw observation next is [25.73333333333333, 95.0, 1.0, 2.0, 0.8228677927758413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1150074.169842294, 1150074.169842294, 249600.277331638], 
processed observation next is [1.0, 0.13043478260869565, 0.41864139020537117, 0.95, 1.0, 1.0, 0.7865877021395679, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.319465047178415, 0.319465047178415, 0.3725377273606537], 
reward next is 0.6275, 
noisyNet noise sample is [array([1.742755], dtype=float32), -0.45644382]. 
=============================================
[2019-03-27 06:39:09,589] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[65.63538]
 [65.39017]
 [65.38947]
 [65.67724]
 [65.75063]], R is [[65.64863586]
 [65.60327148]
 [65.51802063]
 [65.43236542]
 [65.352211  ]].
[2019-03-27 06:39:18,483] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.9536385e-08 3.5740453e-01 1.1593467e-09 6.4259541e-01 9.1135333e-15], sum to 1.0000
[2019-03-27 06:39:18,495] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9508
[2019-03-27 06:39:18,501] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.5601715826533851, 1.0, 2.0, 0.5601715826533851, 1.0, 1.0, 0.9675050804136472, 6.911199999999999, 6.9112, 170.5573041426782, 2350037.755456648, 2350037.755456649, 458157.7465762747], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6088200.0000, 
sim time next is 6088800.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.8723836658836579, 1.0, 2.0, 0.8723836658836579, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2439976.515804995, 2439976.515804995, 456643.6521001104], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.65, 1.0, 1.0, 0.8462453805827204, 1.0, 1.0, 0.8462453805827204, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6777712543902763, 0.6777712543902763, 0.6815576897016573], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.340287], dtype=float32), -1.0852196]. 
=============================================
[2019-03-27 06:39:19,523] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1524870e-16 1.0000000e+00 5.3190793e-20 5.1066339e-12 5.5109665e-25], sum to 1.0000
[2019-03-27 06:39:19,531] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7756
[2019-03-27 06:39:19,537] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 88.33333333333333, 1.0, 2.0, 0.5388578802352892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752989.4113689464, 752989.4113689458, 189908.2768996689], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6133200.0000, 
sim time next is 6133800.0000, 
raw observation next is [27.13333333333333, 88.66666666666667, 1.0, 2.0, 0.5391978051154577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753464.5841841894, 753464.58418419, 189965.4832137343], 
processed observation next is [1.0, 1.0, 0.484992101105845, 0.8866666666666667, 1.0, 1.0, 0.4448166326692261, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20929571782894152, 0.20929571782894169, 0.28353057196079745], 
reward next is 0.7165, 
noisyNet noise sample is [array([-2.5922701], dtype=float32), -0.3908249]. 
=============================================
[2019-03-27 06:39:20,173] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.90920952e-14 1.00000000e+00 1.61029809e-19 1.10503294e-10
 5.81040588e-24], sum to 1.0000
[2019-03-27 06:39:20,181] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0106
[2019-03-27 06:39:20,185] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 82.33333333333334, 1.0, 2.0, 0.7013784734207615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 980197.2892790865, 980197.2892790865, 221144.1413560637], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6075600.0000, 
sim time next is 6076200.0000, 
raw observation next is [28.46666666666667, 81.66666666666666, 1.0, 2.0, 0.7067526511077058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 987711.3551835127, 987711.3551835127, 222310.6945217957], 
processed observation next is [1.0, 0.30434782608695654, 0.5481832543443919, 0.8166666666666665, 1.0, 1.0, 0.6466899410936214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2743642653287535, 0.2743642653287535, 0.3318070067489488], 
reward next is 0.6682, 
noisyNet noise sample is [array([-0.573347], dtype=float32), 0.38520858]. 
=============================================
[2019-03-27 06:39:25,002] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2239439e-08 3.9428294e-01 8.3074103e-11 6.0571700e-01 5.9807616e-15], sum to 1.0000
[2019-03-27 06:39:25,016] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3447
[2019-03-27 06:39:25,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2079687.257681677 W.
[2019-03-27 06:39:25,030] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.13333333333333, 78.0, 1.0, 2.0, 0.7436803397116204, 1.0, 1.0, 0.7436803397116204, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2079687.257681677, 2079687.257681678, 393385.2347294278], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6172800.0000, 
sim time next is 6173400.0000, 
raw observation next is [29.21666666666667, 77.5, 1.0, 2.0, 0.7525100302069122, 1.0, 2.0, 0.7525100302069122, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2104403.564899022, 2104403.564899022, 397414.3620492552], 
processed observation next is [1.0, 0.43478260869565216, 0.5837282780410744, 0.775, 1.0, 1.0, 0.7018193135023039, 1.0, 1.0, 0.7018193135023039, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5845565458052839, 0.5845565458052839, 0.5931557642526197], 
reward next is 0.4068, 
noisyNet noise sample is [array([1.756731], dtype=float32), -0.09427144]. 
=============================================
[2019-03-27 06:39:25,183] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3153926e-10 9.9992907e-01 2.6707149e-13 7.0875023e-05 5.4223506e-17], sum to 1.0000
[2019-03-27 06:39:25,197] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5698
[2019-03-27 06:39:25,204] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1864648.739633579 W.
[2019-03-27 06:39:25,209] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.6925107537062335, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.980317212817086, 6.9112, 168.9125446375471, 1864648.739633579, 1815614.729593786, 383947.0070046876], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6598800.0000, 
sim time next is 6599400.0000, 
raw observation next is [28.18333333333334, 78.16666666666667, 1.0, 2.0, 0.4824995747364519, 1.0, 1.0, 0.4824995747364519, 1.0, 2.0, 0.8261622042997676, 6.911199999999999, 6.9112, 170.5573041426782, 2023898.025238406, 2023898.025238406, 401196.559131908], 
processed observation next is [1.0, 0.391304347826087, 0.534755134281201, 0.7816666666666667, 1.0, 1.0, 0.3765055117306649, 1.0, 0.5, 0.3765055117306649, 1.0, 1.0, 0.7880026881704483, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5621938958995572, 0.5621938958995572, 0.5988008345252358], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4100065], dtype=float32), 1.962235]. 
=============================================
[2019-03-27 06:39:31,816] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6461249e-16 1.0000000e+00 4.7293128e-22 6.7816690e-14 3.1262614e-25], sum to 1.0000
[2019-03-27 06:39:31,826] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4708
[2019-03-27 06:39:31,832] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 67.0, 1.0, 2.0, 0.510293733637623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713061.0371501195, 713061.0371501202, 185224.5668484073], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6284400.0000, 
sim time next is 6285000.0000, 
raw observation next is [29.65, 68.0, 1.0, 2.0, 0.5110203740385633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714076.7523697888, 714076.7523697882, 185340.7400284199], 
processed observation next is [0.0, 0.7391304347826086, 0.6042654028436019, 0.68, 1.0, 1.0, 0.41086792052838944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19835465343605246, 0.1983546534360523, 0.2766279701916715], 
reward next is 0.7234, 
noisyNet noise sample is [array([-0.26648912], dtype=float32), 0.50654525]. 
=============================================
[2019-03-27 06:39:31,852] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.94926]
 [74.93144]
 [74.90513]
 [74.88374]
 [74.85233]], R is [[74.93915558]
 [74.91331482]
 [74.88827515]
 [74.86423492]
 [74.84092712]].
[2019-03-27 06:39:34,593] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.4308890e-14 1.0000000e+00 8.5084612e-20 3.2672857e-12 2.2424024e-24], sum to 1.0000
[2019-03-27 06:39:34,603] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2577
[2019-03-27 06:39:34,609] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5363770076742971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749521.4657144422, 749521.4657144416, 189492.4173407074], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6361200.0000, 
sim time next is 6361800.0000, 
raw observation next is [31.0, 65.5, 1.0, 2.0, 0.5559283677469988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776852.0980397784, 776852.0980397784, 192821.5390709247], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.655, 1.0, 1.0, 0.4649739370445769, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.215792249455494, 0.215792249455494, 0.28779334189690253], 
reward next is 0.7122, 
noisyNet noise sample is [array([0.13912404], dtype=float32), -1.3549216]. 
=============================================
[2019-03-27 06:39:41,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.7932452e-09 6.7739445e-01 2.0479700e-10 3.2260561e-01 5.1394264e-16], sum to 1.0000
[2019-03-27 06:39:41,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5394
[2019-03-27 06:39:41,080] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2322378.817987974 W.
[2019-03-27 06:39:41,083] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.96666666666667, 68.33333333333333, 1.0, 2.0, 1.019559079484178, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.982454399821192, 6.9112, 168.9125326556918, 2322378.817987974, 2271828.621214007, 470227.5735275518], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6442800.0000, 
sim time next is 6443400.0000, 
raw observation next is [29.98333333333333, 68.16666666666667, 1.0, 2.0, 0.8229022984612836, 1.0, 1.0, 0.8229022984612836, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2301454.26272878, 2301454.262728781, 431168.4258234184], 
processed observation next is [1.0, 0.5652173913043478, 0.6200631911532385, 0.6816666666666668, 1.0, 1.0, 0.7866292752545585, 1.0, 0.5, 0.7866292752545585, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6392928507579945, 0.6392928507579947, 0.6435349639155499], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.18429632], dtype=float32), 0.25993934]. 
=============================================
[2019-03-27 06:39:48,334] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 06:39:48,335] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:39:48,337] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:39:48,338] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:39:48,339] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:39:48,337] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:39:48,339] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:39:48,340] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:39:48,341] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:39:48,341] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:39:48,343] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:39:48,359] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run42
[2019-03-27 06:39:48,377] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run42
[2019-03-27 06:39:48,400] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run42
[2019-03-27 06:39:48,418] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run42
[2019-03-27 06:39:48,444] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run42
[2019-03-27 06:40:55,230] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02331715], dtype=float32), 0.0416713]
[2019-03-27 06:40:55,230] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.6, 58.5, 1.0, 2.0, 0.5144898980693919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718926.548896467, 718926.5488964664, 185897.3947566175]
[2019-03-27 06:40:55,232] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:40:55,235] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.6924199e-15 1.0000000e+00 6.5122760e-20 1.0460599e-11 1.3465241e-24], sampled 0.9782254110981913
[2019-03-27 06:41:05,544] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02331715], dtype=float32), 0.0416713]
[2019-03-27 06:41:05,544] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.68256891666667, 96.58195191833335, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 13.86113538034514, 6.9112, 177.4920196563318, 6637515.575673353, 1456571.693691837, 290089.5345515636]
[2019-03-27 06:41:05,546] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:41:05,548] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6352366e-14 1.0000000e+00 6.6286749e-19 5.5170007e-11 1.9829947e-23], sampled 0.8337986772708827
[2019-03-27 06:41:05,549] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 6637515.575673353 W.
[2019-03-27 06:41:32,059] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02331715], dtype=float32), 0.0416713]
[2019-03-27 06:41:32,137] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.85498644666666, 73.346679525, 1.0, 2.0, 0.7262659687923427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1083451.100859356, 1083451.100859355, 235611.7436768358]
[2019-03-27 06:41:32,138] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:41:32,140] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.6655202e-15 1.0000000e+00 1.6738576e-19 6.9369259e-12 4.4200709e-24], sampled 0.12012509480681799
[2019-03-27 06:41:34,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02331715], dtype=float32), 0.0416713]
[2019-03-27 06:41:34,388] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.76666666666667, 76.66666666666667, 1.0, 2.0, 0.3716483787252557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565372.9723228022, 565372.9723228022, 171755.0186960867]
[2019-03-27 06:41:34,391] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:41:34,393] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3916235e-15 1.0000000e+00 3.0697106e-20 1.9188084e-12 6.2361732e-25], sampled 0.0633812141967448
[2019-03-27 06:41:41,962] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02331715], dtype=float32), 0.0416713]
[2019-03-27 06:41:41,963] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.5, 64.0, 1.0, 2.0, 0.478835925910563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753261.6483868965, 753261.6483868965, 190102.667932079]
[2019-03-27 06:41:41,964] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:41:41,966] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6609471e-15 1.0000000e+00 9.7667658e-20 7.0961093e-12 2.1167643e-24], sampled 0.7320992779274142
[2019-03-27 06:41:42,162] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02331715], dtype=float32), 0.0416713]
[2019-03-27 06:41:42,163] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.6, 71.66666666666667, 1.0, 2.0, 0.3405133489283557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531502.9342495002, 531502.9342495002, 169317.3449859368]
[2019-03-27 06:41:42,163] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:41:42,167] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.9015506e-15 1.0000000e+00 4.4348581e-20 1.7094840e-12 1.2275076e-24], sampled 0.05361429263357731
[2019-03-27 06:41:42,232] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.9010 3007634098.4593 1766.0000
[2019-03-27 06:41:42,428] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.3735 2927553971.9876 1337.0000
[2019-03-27 06:41:42,522] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.9069 3164026665.2290 1762.0000
[2019-03-27 06:41:42,571] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.7385 2779270754.0765 931.0000
[2019-03-27 06:41:42,608] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.0008 2842584577.2673 1127.0000
[2019-03-27 06:41:43,623] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1025000, evaluation results [1025000.0, 7886.90690797358, 3164026665.2289753, 1762.0, 8255.373491334356, 2927553971.98756, 1337.0, 8657.738495062507, 2779270754.076537, 931.0, 7996.9009642832125, 3007634098.4593177, 1766.0, 8497.000804045996, 2842584577.267259, 1127.0]
[2019-03-27 06:41:45,002] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0680573e-10 9.9926096e-01 7.8158935e-14 7.3896948e-04 2.8800266e-18], sum to 1.0000
[2019-03-27 06:41:45,009] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0083
[2019-03-27 06:41:45,015] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.08333333333334, 65.0, 1.0, 2.0, 0.5863255419341937, 0.0, 2.0, 0.0, 1.0, 1.0, 1.002780247359499, 6.9112, 6.9112, 168.9125709094943, 1639323.281457269, 1639323.281457269, 355593.8722656369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6613800.0000, 
sim time next is 6614400.0000, 
raw observation next is [31.16666666666667, 64.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 9.849632771856326, 6.9112, 168.8958560934186, 3539466.026578162, 1455051.728560511, 306796.8887939987], 
processed observation next is [1.0, 0.5652173913043478, 0.6761453396524489, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.29384327718563263, 0.0, 0.8293559742761669, 0.9831850073828228, 0.4041810357112531, 0.4579058041701473], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.52583784], dtype=float32), 0.42842218]. 
=============================================
[2019-03-27 06:41:48,218] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6664174e-15 1.0000000e+00 1.5348583e-19 5.5011837e-12 2.9340305e-24], sum to 1.0000
[2019-03-27 06:41:48,224] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1534
[2019-03-27 06:41:48,230] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 95.0, 1.0, 2.0, 0.5907893339468563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825585.6061579635, 825585.6061579635, 199035.3279170854], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6663600.0000, 
sim time next is 6664200.0000, 
raw observation next is [24.88333333333333, 95.0, 1.0, 2.0, 0.6139510287585741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857965.4815870208, 857965.4815870208, 203365.0439199614], 
processed observation next is [1.0, 0.13043478260869565, 0.3783570300157976, 0.95, 1.0, 1.0, 0.5348807575404507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23832374488528354, 0.23832374488528354, 0.30352991629844983], 
reward next is 0.6965, 
noisyNet noise sample is [array([0.18559322], dtype=float32), -0.68311214]. 
=============================================
[2019-03-27 06:41:51,237] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4473912e-14 1.0000000e+00 8.0558731e-18 2.5352223e-10 2.9498574e-22], sum to 1.0000
[2019-03-27 06:41:51,247] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5575
[2019-03-27 06:41:51,263] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 79.0, 1.0, 2.0, 0.3309207002968157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 518690.2853947809, 518690.2853947815, 168356.120219637], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6742800.0000, 
sim time next is 6743400.0000, 
raw observation next is [23.15, 79.66666666666667, 1.0, 2.0, 0.3288233166778067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516118.5951951673, 516118.5951951667, 168172.7069545603], 
processed observation next is [1.0, 0.043478260869565216, 0.2962085308056872, 0.7966666666666667, 1.0, 1.0, 0.19135339358771888, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14336627644310201, 0.14336627644310188, 0.251004040230687], 
reward next is 0.7490, 
noisyNet noise sample is [array([-0.42521146], dtype=float32), 1.0245045]. 
=============================================
[2019-03-27 06:42:02,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0146888e-14 1.0000000e+00 5.0528431e-19 2.3830932e-10 2.2010971e-22], sum to 1.0000
[2019-03-27 06:42:02,986] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5230
[2019-03-27 06:42:02,990] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 82.33333333333334, 1.0, 2.0, 0.2874782655347496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462706.1998459483, 462706.1998459483, 164457.4963176636], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7415400.0000, 
sim time next is 7416000.0000, 
raw observation next is [21.7, 82.0, 1.0, 2.0, 0.2876649825195954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462816.0945530452, 462816.0945530459, 164464.6261958088], 
processed observation next is [1.0, 0.8695652173913043, 0.2274881516587678, 0.82, 1.0, 1.0, 0.1417650391802354, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12856002626473478, 0.12856002626473498, 0.2454695913370281], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.05901648], dtype=float32), -0.5670161]. 
=============================================
[2019-03-27 06:42:03,016] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.558044]
 [71.628845]
 [71.723045]
 [71.83231 ]
 [71.96414 ]], R is [[71.58701324]
 [71.62568665]
 [71.66396332]
 [71.70185852]
 [71.73941803]].
[2019-03-27 06:42:05,053] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.5627955e-17 1.0000000e+00 8.1981207e-22 1.2392486e-13 2.1252344e-25], sum to 1.0000
[2019-03-27 06:42:05,063] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8064
[2019-03-27 06:42:05,068] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.51666666666667, 57.83333333333334, 1.0, 2.0, 0.4310396285639245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 625009.0755310854, 625009.075531086, 176413.4769398913], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6977400.0000, 
sim time next is 6978000.0000, 
raw observation next is [29.43333333333334, 57.66666666666667, 1.0, 2.0, 0.4269511401382279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 621593.6425464626, 621593.6425464626, 176150.6303184638], 
processed observation next is [0.0, 0.782608695652174, 0.5939968404423385, 0.5766666666666667, 1.0, 1.0, 0.30957968691352755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17266490070735072, 0.17266490070735072, 0.2629113885350206], 
reward next is 0.7371, 
noisyNet noise sample is [array([1.1901553], dtype=float32), -0.41826847]. 
=============================================
[2019-03-27 06:42:05,086] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.813835]
 [77.79254 ]
 [77.754906]
 [77.75105 ]
 [77.73812 ]], R is [[77.7779541 ]
 [77.73686981]
 [77.69500732]
 [77.65283203]
 [77.61045074]].
[2019-03-27 06:42:13,570] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4562310e-12 1.0000000e+00 1.9899031e-18 5.3505538e-08 2.4455189e-21], sum to 1.0000
[2019-03-27 06:42:13,579] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0066
[2019-03-27 06:42:13,585] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1734552.224321608 W.
[2019-03-27 06:42:13,588] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.7, 71.33333333333334, 1.0, 2.0, 0.6166416644805994, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.911694863106829, 6.9112, 168.9121673675936, 1734552.224321608, 1734201.153041923, 369882.0315017942], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7118400.0000, 
sim time next is 7119000.0000, 
raw observation next is [27.8, 71.0, 1.0, 2.0, 0.5990558705190031, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9986542588652546, 6.911199999999999, 6.9112, 168.9129559133916, 1679364.148991898, 1679364.148991898, 357937.7259576876], 
processed observation next is [1.0, 0.391304347826087, 0.5165876777251186, 0.71, 1.0, 1.0, 0.5169347837578351, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9983588522747006, -8.881784197001253e-17, 0.0, 0.8294399422205659, 0.4664900413866383, 0.4664900413866383, 0.5342354118771456], 
reward next is 0.4658, 
noisyNet noise sample is [array([0.28569221], dtype=float32), -0.8864259]. 
=============================================
[2019-03-27 06:42:13,605] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[62.406147]
 [67.168274]
 [67.6977  ]
 [67.87577 ]
 [68.20337 ]], R is [[58.63702774]
 [58.49612045]
 [57.91115952]
 [57.33205032]
 [57.12718582]].
[2019-03-27 06:42:14,774] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4374811e-13 9.9999964e-01 1.4978995e-15 3.8994924e-07 9.6033458e-22], sum to 1.0000
[2019-03-27 06:42:14,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3814
[2019-03-27 06:42:14,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1886032.092856401 W.
[2019-03-27 06:42:14,796] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.48333333333333, 81.33333333333333, 1.0, 2.0, 0.6744916280464089, 1.0, 1.0, 0.6744916280464089, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1886032.092856401, 1886032.092856401, 363383.6688204555], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7135800.0000, 
sim time next is 7136400.0000, 
raw observation next is [26.46666666666667, 81.66666666666667, 1.0, 2.0, 0.6641031150085537, 1.0, 2.0, 0.6641031150085537, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1856958.273539043, 1856958.273539043, 359123.9071199273], 
processed observation next is [1.0, 0.6086956521739131, 0.45339652448657203, 0.8166666666666668, 1.0, 1.0, 0.595304957841631, 1.0, 1.0, 0.595304957841631, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5158217426497341, 0.5158217426497341, 0.5360058315222795], 
reward next is 0.4640, 
noisyNet noise sample is [array([1.3196843], dtype=float32), 0.2821354]. 
=============================================
[2019-03-27 06:42:16,136] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8904856e-14 1.0000000e+00 1.3705428e-18 5.0406029e-10 2.5019063e-23], sum to 1.0000
[2019-03-27 06:42:16,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9659
[2019-03-27 06:42:16,153] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 86.0, 1.0, 2.0, 0.4736779639699703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661879.9016697747, 661879.9016697753, 179574.5086698887], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7172400.0000, 
sim time next is 7173000.0000, 
raw observation next is [25.75, 86.0, 1.0, 2.0, 0.4740798169992675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662441.5942000849, 662441.5942000843, 179634.3761610241], 
processed observation next is [1.0, 0.0, 0.41943127962085314, 0.86, 1.0, 1.0, 0.3663612253003222, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18401155394446803, 0.18401155394446786, 0.26811100919555836], 
reward next is 0.7319, 
noisyNet noise sample is [array([-0.32437336], dtype=float32), -1.3360261]. 
=============================================
[2019-03-27 06:42:16,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.08292 ]
 [72.522606]
 [73.51529 ]
 [74.65115 ]
 [74.644684]], R is [[71.66759491]
 [71.68289948]
 [71.69808197]
 [71.71329498]
 [71.72857666]].
[2019-03-27 06:42:18,547] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9675549e-13 1.0000000e+00 2.5662596e-19 9.9708427e-12 2.0509652e-24], sum to 1.0000
[2019-03-27 06:42:18,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9581
[2019-03-27 06:42:18,560] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 84.0, 1.0, 2.0, 0.6548218342147747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 915104.9686728433, 915104.9686728439, 211397.2321037971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7200000.0000, 
sim time next is 7200600.0000, 
raw observation next is [28.33333333333333, 84.0, 1.0, 2.0, 0.7503280934906575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1048639.615524592, 1048639.615524593, 232085.2367678844], 
processed observation next is [1.0, 0.34782608695652173, 0.541864139020537, 0.84, 1.0, 1.0, 0.6991904740851296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29128878209016446, 0.29128878209016473, 0.34639587577296177], 
reward next is 0.6536, 
noisyNet noise sample is [array([-0.13050283], dtype=float32), -1.9163859]. 
=============================================
[2019-03-27 06:42:22,327] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.6838046e-14 1.0000000e+00 9.1825599e-19 2.1087700e-12 1.7716209e-24], sum to 1.0000
[2019-03-27 06:42:22,340] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9243
[2019-03-27 06:42:22,345] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 90.66666666666667, 1.0, 2.0, 0.3311335112969416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522813.9779869437, 522813.9779869437, 168755.0481316826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7274400.0000, 
sim time next is 7275000.0000, 
raw observation next is [21.43333333333333, 90.83333333333334, 1.0, 2.0, 0.3283024194652454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518500.3955674687, 518500.3955674687, 168422.2432845208], 
processed observation next is [1.0, 0.17391304347826086, 0.21484992101105835, 0.9083333333333334, 1.0, 1.0, 0.190725806584633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1440278876576302, 0.1440278876576302, 0.2513764825142102], 
reward next is 0.7486, 
noisyNet noise sample is [array([-0.73397034], dtype=float32), -1.0229373]. 
=============================================
[2019-03-27 06:42:22,363] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.192665]
 [71.290306]
 [71.423416]
 [71.53106 ]
 [71.67266 ]], R is [[71.21769714]
 [71.25364685]
 [71.28839874]
 [71.32403564]
 [71.353508  ]].
[2019-03-27 06:42:22,610] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1161212e-15 1.0000000e+00 1.2986679e-19 5.3105403e-10 5.3659366e-26], sum to 1.0000
[2019-03-27 06:42:22,618] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7299
[2019-03-27 06:42:22,621] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 90.33333333333334, 1.0, 2.0, 0.527664243299526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737342.2142358475, 737342.2142358475, 188043.839472033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7771200.0000, 
sim time next is 7771800.0000, 
raw observation next is [26.45, 90.66666666666667, 1.0, 2.0, 0.5268233517355035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736166.7703692763, 736166.7703692763, 187905.3444407823], 
processed observation next is [1.0, 0.9565217391304348, 0.45260663507109006, 0.9066666666666667, 1.0, 1.0, 0.4299076526933777, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2044907695470212, 0.2044907695470212, 0.28045573797131684], 
reward next is 0.7195, 
noisyNet noise sample is [array([1.6109555], dtype=float32), 0.5482641]. 
=============================================
[2019-03-27 06:42:22,871] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.8568482e-14 1.0000000e+00 1.6820716e-17 4.5712845e-08 2.4177471e-21], sum to 1.0000
[2019-03-27 06:42:22,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9702
[2019-03-27 06:42:22,883] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 60.66666666666667, 1.0, 2.0, 0.9443274650202662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510418, 1425410.27929131, 1425410.27929131, 297759.4441999813], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7312800.0000, 
sim time next is 7313400.0000, 
raw observation next is [27.7, 61.0, 1.0, 2.0, 0.9084434543839838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1370361.680012752, 1370361.680012752, 286623.6138967029], 
processed observation next is [1.0, 0.6521739130434783, 0.5118483412322274, 0.61, 1.0, 1.0, 0.889690908896366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.38065602222576445, 0.38065602222576445, 0.42779643865179534], 
reward next is 0.5722, 
noisyNet noise sample is [array([-1.9138683], dtype=float32), 0.606257]. 
=============================================
[2019-03-27 06:42:34,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:42:34,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:42:34,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-03-27 06:42:35,623] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 06:42:35,624] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:42:35,625] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:42:35,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:42:35,625] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:42:35,628] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:42:35,629] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:42:35,630] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:42:35,630] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:42:35,633] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:42:35,631] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:42:35,648] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run43
[2019-03-27 06:42:35,649] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run43
[2019-03-27 06:42:35,677] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run43
[2019-03-27 06:42:35,694] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run43
[2019-03-27 06:42:35,709] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run43
[2019-03-27 06:42:53,708] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02453917], dtype=float32), 0.04243278]
[2019-03-27 06:42:53,709] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.05, 65.5, 1.0, 2.0, 0.3182385469338928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512492.6237734109, 512492.6237734109, 168030.1332676496]
[2019-03-27 06:42:53,711] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:42:53,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1044444e-15 1.0000000e+00 1.9919895e-20 2.5262401e-13 7.0644619e-25], sampled 0.1616276462443048
[2019-03-27 06:43:01,334] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02453917], dtype=float32), 0.04243278]
[2019-03-27 06:43:01,339] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.43333333333334, 94.0, 1.0, 2.0, 0.503261366934298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703231.0780435816, 703231.0780435816, 184108.6900725193]
[2019-03-27 06:43:01,341] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:43:01,346] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3128343e-16 1.0000000e+00 3.4149328e-21 3.9402477e-13 2.6509865e-26], sampled 0.6753331449441663
[2019-03-27 06:43:15,570] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02453917], dtype=float32), 0.04243278]
[2019-03-27 06:43:15,571] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.16666666666667, 100.0, 1.0, 2.0, 0.4427102211633037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636735.7042807669, 636735.7042807676, 177437.2248689152]
[2019-03-27 06:43:15,572] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:43:15,575] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.4848849e-16 1.0000000e+00 6.9208901e-21 1.2558527e-13 1.4709112e-25], sampled 0.8050597051752622
[2019-03-27 06:43:23,753] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02453917], dtype=float32), 0.04243278]
[2019-03-27 06:43:23,754] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.83333333333334, 1.0, 2.0, 0.5266959904319644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735988.7379402305, 735988.7379402305, 187884.2469297457]
[2019-03-27 06:43:23,756] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:43:23,760] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7012706e-16 1.0000000e+00 1.9298990e-21 2.4828089e-13 1.4689311e-26], sampled 0.9927585622908026
[2019-03-27 06:43:25,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02453917], dtype=float32), 0.04243278]
[2019-03-27 06:43:25,368] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.65, 57.33333333333333, 1.0, 2.0, 0.5182058851510473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724120.8828410414, 724120.8828410414, 186496.9682183577]
[2019-03-27 06:43:25,369] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:43:25,372] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7273272e-16 1.0000000e+00 3.1911402e-21 2.9332412e-13 3.2670443e-26], sampled 0.46233607702748525
[2019-03-27 06:43:45,540] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02453917], dtype=float32), 0.04243278]
[2019-03-27 06:43:45,541] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.96666666666667, 85.0, 1.0, 2.0, 0.63627293965341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 889172.2795410267, 889172.2795410273, 207697.7209916482]
[2019-03-27 06:43:45,543] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:43:45,546] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.2969129e-16 1.0000000e+00 1.4947055e-20 4.4505697e-12 1.1319114e-25], sampled 0.5457071485106371
[2019-03-27 06:44:01,454] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02453917], dtype=float32), 0.04243278]
[2019-03-27 06:44:01,457] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.76666666666667, 58.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.529600649838208, 6.9112, 168.9037457291336, 3432823.95447367, 2284737.440711492, 472012.7517431136]
[2019-03-27 06:44:01,459] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:44:01,463] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.8002291e-09 9.7596890e-01 6.6925484e-12 2.4031108e-02 9.8638853e-17], sampled 0.8783728268368208
[2019-03-27 06:44:01,465] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3432823.95447367 W.
[2019-03-27 06:44:28,285] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02453917], dtype=float32), 0.04243278]
[2019-03-27 06:44:28,287] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.821874115, 62.818544485, 1.0, 2.0, 0.8960715650995911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1252447.14286005, 1252447.14286005, 268867.7985555878]
[2019-03-27 06:44:28,290] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:44:28,295] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.5004073e-16 1.0000000e+00 1.7354968e-20 1.4513073e-12 2.2333352e-25], sampled 0.04676768446835888
[2019-03-27 06:44:31,005] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.4339 3164030528.5839 1765.0000
[2019-03-27 06:44:31,177] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5815 3007786016.0708 1764.0000
[2019-03-27 06:44:31,200] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7883 2779362921.8578 932.0000
[2019-03-27 06:44:31,391] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4580 2842529864.5585 1131.0000
[2019-03-27 06:44:31,452] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.7965 2927424629.7961 1338.0000
[2019-03-27 06:44:32,467] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1050000, evaluation results [1050000.0, 7889.4338862482155, 3164030528.5839043, 1765.0, 8252.796533646178, 2927424629.7961226, 1338.0, 8660.788344965533, 2779362921.8578463, 932.0, 7997.581519761262, 3007786016.070808, 1764.0, 8497.45799412183, 2842529864.5584927, 1131.0]
[2019-03-27 06:44:34,143] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:34,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:34,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-03-27 06:44:39,051] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6456975e-15 1.0000000e+00 8.3733955e-18 3.9653694e-09 2.6757708e-23], sum to 1.0000
[2019-03-27 06:44:39,058] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4024
[2019-03-27 06:44:39,064] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 86.5, 1.0, 2.0, 0.5077434834523744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709496.244525124, 709496.244525124, 184818.1776674127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7673400.0000, 
sim time next is 7674000.0000, 
raw observation next is [26.4, 87.33333333333333, 1.0, 2.0, 0.50606117955779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707144.6917831568, 707144.6917831575, 184551.185786542], 
processed observation next is [1.0, 0.8260869565217391, 0.45023696682464454, 0.8733333333333333, 1.0, 1.0, 0.404892987419024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1964290810508769, 0.19642908105087709, 0.27544953102468955], 
reward next is 0.7246, 
noisyNet noise sample is [array([-0.69450635], dtype=float32), 0.9432069]. 
=============================================
[2019-03-27 06:44:39,081] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.57458 ]
 [75.080414]
 [75.28472 ]
 [75.37428 ]
 [74.54283 ]], R is [[74.3014679 ]
 [74.28260803]
 [74.2634964 ]
 [74.24435425]
 [74.22556305]].
[2019-03-27 06:44:40,766] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1054017: loss 0.0035
[2019-03-27 06:44:40,768] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1054018: learning rate 0.0000
[2019-03-27 06:44:42,246] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0829719e-14 1.0000000e+00 3.0304657e-20 9.1008165e-12 1.3634884e-24], sum to 1.0000
[2019-03-27 06:44:42,259] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6951
[2019-03-27 06:44:42,264] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666667, 87.50000000000001, 1.0, 2.0, 0.5925288807123426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 828017.4451896423, 828017.4451896416, 199356.4914869946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7715400.0000, 
sim time next is 7716000.0000, 
raw observation next is [26.73333333333333, 87.0, 1.0, 2.0, 0.6034012002840791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843216.7781832678, 843216.7781832678, 201374.9563591422], 
processed observation next is [1.0, 0.30434782608695654, 0.4660347551342811, 0.87, 1.0, 1.0, 0.5221701208241917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2342268828286855, 0.2342268828286855, 0.30055963635692867], 
reward next is 0.6994, 
noisyNet noise sample is [array([0.80377626], dtype=float32), -0.46846196]. 
=============================================
[2019-03-27 06:44:42,283] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[69.55018]
 [69.52083]
 [69.37148]
 [69.46327]
 [69.6188 ]], R is [[69.55813599]
 [69.56501007]
 [69.5721283 ]
 [69.57645416]
 [69.58365631]].
[2019-03-27 06:44:43,929] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1055499: loss 0.0044
[2019-03-27 06:44:43,931] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1055499: learning rate 0.0000
[2019-03-27 06:44:47,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.2877927e-16 1.0000000e+00 3.2453171e-20 4.6488224e-14 2.4559859e-24], sum to 1.0000
[2019-03-27 06:44:47,746] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4851
[2019-03-27 06:44:47,754] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.85, 88.66666666666667, 1.0, 2.0, 0.2812510604852813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 452555.221064153, 452555.221064153, 163769.1885237974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 283800.0000, 
sim time next is 284400.0000, 
raw observation next is [21.0, 88.0, 1.0, 2.0, 0.282696241791992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 454294.7908719257, 454294.7908719263, 163883.2565483385], 
processed observation next is [0.0, 0.30434782608695654, 0.19431279620853087, 0.88, 1.0, 1.0, 0.13577860456866503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1261929974644238, 0.12619299746442397, 0.24460187544528136], 
reward next is 0.7554, 
noisyNet noise sample is [array([1.9282191], dtype=float32), 1.0444334]. 
=============================================
[2019-03-27 06:44:52,761] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.0435220e-16 1.0000000e+00 5.2856243e-20 1.7929683e-10 2.0824459e-23], sum to 1.0000
[2019-03-27 06:44:52,767] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6504
[2019-03-27 06:44:52,771] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 89.5, 1.0, 2.0, 0.5937695231609816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 829751.8329669524, 829751.8329669529, 199584.9481805256], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7878600.0000, 
sim time next is 7879200.0000, 
raw observation next is [26.26666666666667, 89.33333333333333, 1.0, 2.0, 0.693426089341136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 969078.5251910627, 969078.5251910621, 219431.0477504276], 
processed observation next is [1.0, 0.17391304347826086, 0.44391785150079005, 0.8933333333333333, 1.0, 1.0, 0.6306338425796819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26918847921973965, 0.2691884792197395, 0.32750902649317554], 
reward next is 0.6725, 
noisyNet noise sample is [array([0.09510702], dtype=float32), -0.56140393]. 
=============================================
[2019-03-27 06:44:54,575] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:54,575] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:54,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-03-27 06:44:55,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:55,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:55,827] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-03-27 06:44:55,843] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:55,847] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:55,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-03-27 06:44:55,973] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1061250: loss 0.1431
[2019-03-27 06:44:55,974] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1061251: learning rate 0.0000
[2019-03-27 06:44:56,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:56,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:56,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-03-27 06:44:56,400] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:56,401] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:56,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:56,404] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:56,432] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-03-27 06:44:56,447] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:56,448] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:56,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-03-27 06:44:56,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-03-27 06:44:56,535] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1061569: loss 0.1180
[2019-03-27 06:44:56,536] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1061569: learning rate 0.0000
[2019-03-27 06:44:56,721] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:56,721] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:56,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-03-27 06:44:56,808] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:56,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:56,815] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-03-27 06:44:56,836] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:56,837] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:56,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-03-27 06:44:57,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:57,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:57,039] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:57,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:57,045] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-03-27 06:44:57,076] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:57,076] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:57,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-03-27 06:44:57,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-03-27 06:44:57,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 06:44:57,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:44:57,104] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-03-27 06:44:58,329] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1062254: loss 0.0200
[2019-03-27 06:44:58,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1062255: learning rate 0.0000
[2019-03-27 06:45:01,913] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.7858851e-15 1.0000000e+00 2.3600390e-20 3.6882585e-12 2.3941197e-24], sum to 1.0000
[2019-03-27 06:45:01,922] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7372
[2019-03-27 06:45:01,925] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.45, 89.5, 1.0, 2.0, 0.3884612252746842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 600477.9889225508, 600477.9889225514, 175081.0517185793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 95400.0000, 
sim time next is 96000.0000, 
raw observation next is [22.46666666666667, 89.66666666666667, 1.0, 2.0, 0.3878659190502755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598953.3325073369, 598953.3325073362, 174934.9719188603], 
processed observation next is [1.0, 0.08695652173913043, 0.2638230647709322, 0.8966666666666667, 1.0, 1.0, 0.2624890590967175, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16637592569648246, 0.16637592569648227, 0.2610969730132243], 
reward next is 0.7389, 
noisyNet noise sample is [array([1.9333966], dtype=float32), 1.3028858]. 
=============================================
[2019-03-27 06:45:01,945] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.5734  ]
 [73.29624 ]
 [73.08207 ]
 [73.04852 ]
 [72.801636]], R is [[73.68907166]
 [73.69086456]
 [73.68133545]
 [73.64875793]
 [73.65875244]].
[2019-03-27 06:45:04,656] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1065312: loss 0.0384
[2019-03-27 06:45:04,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1065313: learning rate 0.0000
[2019-03-27 06:45:04,713] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1065341: loss 0.0423
[2019-03-27 06:45:04,716] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1065341: learning rate 0.0000
[2019-03-27 06:45:05,304] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1065612: loss 0.0402
[2019-03-27 06:45:05,307] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1065612: learning rate 0.0000
[2019-03-27 06:45:05,955] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1065915: loss 0.0426
[2019-03-27 06:45:05,960] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1065916: learning rate 0.0000
[2019-03-27 06:45:06,009] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1065940: loss 0.0442
[2019-03-27 06:45:06,011] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1065941: learning rate 0.0000
[2019-03-27 06:45:06,121] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1065995: loss 0.0424
[2019-03-27 06:45:06,126] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1065995: learning rate 0.0000
[2019-03-27 06:45:06,506] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1066179: loss 0.0476
[2019-03-27 06:45:06,508] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1066179: learning rate 0.0000
[2019-03-27 06:45:06,554] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1066202: loss 0.0513
[2019-03-27 06:45:06,557] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1066202: learning rate 0.0000
[2019-03-27 06:45:06,758] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1066296: loss 0.0434
[2019-03-27 06:45:06,761] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1066297: learning rate 0.0000
[2019-03-27 06:45:07,067] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1066439: loss 0.0463
[2019-03-27 06:45:07,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1066440: learning rate 0.0000
[2019-03-27 06:45:07,147] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1066476: loss 0.0571
[2019-03-27 06:45:07,148] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1066477: learning rate 0.0000
[2019-03-27 06:45:07,224] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1066515: loss 0.0610
[2019-03-27 06:45:07,228] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1066516: learning rate 0.0000
[2019-03-27 06:45:07,276] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1066539: loss 0.2227
[2019-03-27 06:45:07,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1066541: learning rate 0.0000
[2019-03-27 06:45:07,507] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1066647: loss 0.0562
[2019-03-27 06:45:07,511] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1066647: learning rate 0.0000
[2019-03-27 06:45:09,787] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1061663e-17 1.0000000e+00 2.6338129e-24 1.9270946e-15 3.9208945e-26], sum to 1.0000
[2019-03-27 06:45:09,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0358
[2019-03-27 06:45:09,802] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333333, 88.66666666666666, 1.0, 2.0, 0.3008194031433185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479127.6619617537, 479127.6619617537, 165560.8652732381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 240000.0000, 
sim time next is 240600.0000, 
raw observation next is [21.31666666666667, 88.83333333333334, 1.0, 2.0, 0.3006929564937792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478882.1058054178, 478882.1058054178, 165542.645426857], 
processed observation next is [0.0, 0.782608695652174, 0.20932069510268583, 0.8883333333333334, 1.0, 1.0, 0.15746139336599901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1330228071681716, 0.1330228071681716, 0.24707857526396565], 
reward next is 0.7529, 
noisyNet noise sample is [array([0.85926145], dtype=float32), 1.1556388]. 
=============================================
[2019-03-27 06:45:10,471] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1068214: loss 0.1890
[2019-03-27 06:45:10,475] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1068214: learning rate 0.0000
[2019-03-27 06:45:13,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4749388e-15 1.0000000e+00 3.1053756e-19 2.4386014e-14 7.1389609e-25], sum to 1.0000
[2019-03-27 06:45:13,028] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7299
[2019-03-27 06:45:13,032] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 89.66666666666667, 1.0, 2.0, 0.266010431261114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 433403.7071040743, 433403.7071040743, 162465.6624087682], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 798000.0000, 
sim time next is 798600.0000, 
raw observation next is [20.15, 88.83333333333334, 1.0, 2.0, 0.2669562477402215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 434605.4772025349, 434605.4772025342, 162550.7975552343], 
processed observation next is [0.0, 0.21739130434782608, 0.15402843601895733, 0.8883333333333334, 1.0, 1.0, 0.11681475631351984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1207237436673708, 0.12072374366737061, 0.2426131306794542], 
reward next is 0.7574, 
noisyNet noise sample is [array([0.65299517], dtype=float32), -0.4237426]. 
=============================================
[2019-03-27 06:45:14,857] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1070195: loss 0.0371
[2019-03-27 06:45:14,862] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1070195: learning rate 0.0000
[2019-03-27 06:45:15,874] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5417675e-16 1.0000000e+00 1.1940625e-21 5.2206544e-14 6.5972450e-26], sum to 1.0000
[2019-03-27 06:45:15,887] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6538
[2019-03-27 06:45:15,892] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 86.0, 1.0, 2.0, 0.2830604192682807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 455026.1301090805, 455026.1301090805, 163933.381175378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 333000.0000, 
sim time next is 333600.0000, 
raw observation next is [21.16666666666667, 86.0, 1.0, 2.0, 0.2820151670907188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 453632.9606308299, 453632.9606308305, 163840.9071685976], 
processed observation next is [0.0, 0.8695652173913043, 0.2022116903633494, 0.86, 1.0, 1.0, 0.13495803263942022, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12600915573078608, 0.12600915573078625, 0.2445386674158173], 
reward next is 0.7555, 
noisyNet noise sample is [array([-0.57065105], dtype=float32), 0.0822804]. 
=============================================
[2019-03-27 06:45:17,077] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6388989e-18 1.0000000e+00 8.5861136e-23 3.3196873e-16 2.0152788e-27], sum to 1.0000
[2019-03-27 06:45:17,087] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4829
[2019-03-27 06:45:17,095] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 86.0, 1.0, 2.0, 0.284091025076415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456391.554335087, 456391.554335087, 164024.0112794014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 332400.0000, 
sim time next is 333000.0000, 
raw observation next is [21.2, 86.0, 1.0, 2.0, 0.2830604192682807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 455026.1301090805, 455026.1301090805, 163933.381175378], 
processed observation next is [0.0, 0.8695652173913043, 0.20379146919431282, 0.86, 1.0, 1.0, 0.13621737261238637, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12639614725252238, 0.12639614725252238, 0.2446766883214597], 
reward next is 0.7553, 
noisyNet noise sample is [array([0.33569697], dtype=float32), -1.7555702]. 
=============================================
[2019-03-27 06:45:17,111] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[79.143456]
 [79.125565]
 [79.09213 ]
 [79.04182 ]
 [79.03331 ]], R is [[79.1255722 ]
 [79.08950806]
 [79.05367279]
 [79.01806641]
 [78.98270416]].
[2019-03-27 06:45:21,678] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1073278: loss 0.0272
[2019-03-27 06:45:21,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1073279: learning rate 0.0000
[2019-03-27 06:45:21,832] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1073348: loss 0.0299
[2019-03-27 06:45:21,834] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1073348: learning rate 0.0000
[2019-03-27 06:45:22,487] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1073645: loss 0.0231
[2019-03-27 06:45:22,491] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1073648: learning rate 0.0000
[2019-03-27 06:45:23,090] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1073920: loss 0.0258
[2019-03-27 06:45:23,091] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1073920: learning rate 0.0000
[2019-03-27 06:45:23,107] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1073928: loss 0.0236
[2019-03-27 06:45:23,109] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1073928: learning rate 0.0000
[2019-03-27 06:45:23,360] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1074040: loss 0.0201
[2019-03-27 06:45:23,362] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1074041: learning rate 0.0000
[2019-03-27 06:45:23,668] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1074181: loss 0.0190
[2019-03-27 06:45:23,670] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1074182: learning rate 0.0000
[2019-03-27 06:45:23,675] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1074184: loss 0.0220
[2019-03-27 06:45:23,680] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1074184: learning rate 0.0000
[2019-03-27 06:45:23,957] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1074312: loss 0.0214
[2019-03-27 06:45:23,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1074315: learning rate 0.0000
[2019-03-27 06:45:24,195] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1074418: loss 0.0194
[2019-03-27 06:45:24,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1074418: learning rate 0.0000
[2019-03-27 06:45:24,277] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1074456: loss 0.0162
[2019-03-27 06:45:24,279] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1074456: learning rate 0.0000
[2019-03-27 06:45:24,341] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1074484: loss 0.0163
[2019-03-27 06:45:24,345] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1074485: learning rate 0.0000
[2019-03-27 06:45:24,493] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1074555: loss 0.0078
[2019-03-27 06:45:24,496] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1074555: learning rate 0.0000
[2019-03-27 06:45:24,744] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1074667: loss 0.0174
[2019-03-27 06:45:24,748] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1074668: learning rate 0.0000
[2019-03-27 06:45:24,761] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9710933e-16 1.0000000e+00 9.0575510e-21 3.8192594e-13 2.1110941e-25], sum to 1.0000
[2019-03-27 06:45:24,770] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4857
[2019-03-27 06:45:24,776] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 75.83333333333333, 1.0, 2.0, 0.4052513659093974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666519.4414477205, 666519.4414477205, 180246.454953312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 463800.0000, 
sim time next is 464400.0000, 
raw observation next is [21.2, 75.0, 1.0, 2.0, 0.4228065751600374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695236.5461233738, 695236.5461233745, 182903.5054878025], 
processed observation next is [1.0, 0.391304347826087, 0.20379146919431282, 0.75, 1.0, 1.0, 0.30458623513257516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19312126281204828, 0.19312126281204847, 0.27299030669821267], 
reward next is 0.7270, 
noisyNet noise sample is [array([-0.5922859], dtype=float32), 0.2157995]. 
=============================================
[2019-03-27 06:45:25,485] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 06:45:25,487] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:45:25,489] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:45:25,489] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:45:25,490] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:45:25,491] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:45:25,493] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:45:25,493] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:45:25,495] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:45:25,496] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:45:25,497] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:45:25,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run44
[2019-03-27 06:45:25,523] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run44
[2019-03-27 06:45:25,541] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run44
[2019-03-27 06:45:25,543] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run44
[2019-03-27 06:45:25,598] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run44
[2019-03-27 06:45:31,039] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02630001], dtype=float32), 0.043617327]
[2019-03-27 06:45:31,043] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.88322035833334, 68.22093151166668, 1.0, 2.0, 0.2433293921366372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401616.6690712473, 401616.6690712473, 160213.8697680776]
[2019-03-27 06:45:31,044] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:45:31,047] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5984091e-16 1.0000000e+00 1.3717176e-21 1.7587015e-14 3.6510523e-26], sampled 0.27971982638837567
[2019-03-27 06:45:42,061] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02630001], dtype=float32), 0.043617327]
[2019-03-27 06:45:42,063] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.11666666666667, 91.83333333333334, 1.0, 2.0, 0.3158801108525273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 501123.9745420394, 501123.9745420388, 167138.1022639651]
[2019-03-27 06:45:42,067] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:45:42,068] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1315853e-16 1.0000000e+00 2.0699738e-21 2.4875335e-14 6.5331821e-26], sampled 0.7272802314505447
[2019-03-27 06:45:58,261] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02630001], dtype=float32), 0.043617327]
[2019-03-27 06:45:58,263] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.27555859333334, 77.98902876166667, 1.0, 2.0, 0.7755407595187651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1083894.184738693, 1083894.184738693, 237992.3432052993]
[2019-03-27 06:45:58,268] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:45:58,273] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9748866e-16 1.0000000e+00 2.8555478e-21 1.8414316e-13 2.3507771e-26], sampled 0.7938172660912148
[2019-03-27 06:46:10,279] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02630001], dtype=float32), 0.043617327]
[2019-03-27 06:46:10,281] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 1.0, 2.0, 0.3037599603951716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483719.6979696548, 483719.6979696554, 165889.0435059302]
[2019-03-27 06:46:10,283] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:46:10,286] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.1163663e-17 1.0000000e+00 7.4015837e-22 2.6866296e-14 9.5897429e-27], sampled 0.8114266341715568
[2019-03-27 06:47:20,969] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.7307 3007743086.5330 1765.0000
[2019-03-27 06:47:21,093] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3438 2842502610.3471 1130.0000
[2019-03-27 06:47:21,136] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.1084 2927456598.3536 1338.0000
[2019-03-27 06:47:21,296] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.5527 3164222776.4071 1775.0000
[2019-03-27 06:47:21,299] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4205 2779406097.1540 933.0000
[2019-03-27 06:47:22,317] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1075000, evaluation results [1075000.0, 7882.552650674568, 3164222776.407089, 1775.0, 8252.108356809289, 2927456598.3535867, 1338.0, 8660.420501940682, 2779406097.1539907, 933.0, 7996.730735873059, 3007743086.5330386, 1765.0, 8497.343800161021, 2842502610.347142, 1130.0]
[2019-03-27 06:47:24,800] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1076177: loss 0.0158
[2019-03-27 06:47:24,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1076177: learning rate 0.0000
[2019-03-27 06:47:25,071] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3400833e-16 1.0000000e+00 8.8603898e-23 1.8837484e-13 4.5311378e-26], sum to 1.0000
[2019-03-27 06:47:25,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5361
[2019-03-27 06:47:25,085] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.45, 95.0, 1.0, 2.0, 0.2951410373204548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471407.6856369834, 471407.6856369834, 165033.1494310229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1060200.0000, 
sim time next is 1060800.0000, 
raw observation next is [20.5, 95.0, 1.0, 2.0, 0.2967616354059585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473492.4332114068, 473492.4332114068, 165173.0611870271], 
processed observation next is [1.0, 0.2608695652173913, 0.1706161137440759, 0.95, 1.0, 1.0, 0.15272486193488977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13152567589205744, 0.13152567589205744, 0.24652695699556285], 
reward next is 0.7535, 
noisyNet noise sample is [array([0.62863964], dtype=float32), -1.2968712]. 
=============================================
[2019-03-27 06:47:28,948] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.7769622e-14 1.0000000e+00 9.7698633e-19 2.9044617e-11 6.6253178e-25], sum to 1.0000
[2019-03-27 06:47:28,956] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8637
[2019-03-27 06:47:28,960] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.7, 90.33333333333334, 1.0, 2.0, 0.290466962418617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466581.8356410136, 466581.835641013, 164721.0190601599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1128000.0000, 
sim time next is 1128600.0000, 
raw observation next is [20.65, 90.5, 1.0, 2.0, 0.288849489992387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464252.1078560731, 464252.1078560736, 164561.6028122241], 
processed observation next is [1.0, 0.043478260869565216, 0.1777251184834123, 0.905, 1.0, 1.0, 0.14319215661733375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1289589188489092, 0.12895891884890934, 0.24561433255555834], 
reward next is 0.7544, 
noisyNet noise sample is [array([0.327292], dtype=float32), 2.2318127]. 
=============================================
[2019-03-27 06:47:29,123] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1078202: loss 0.0905
[2019-03-27 06:47:29,125] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1078202: learning rate 0.0000
[2019-03-27 06:47:31,813] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.60670441e-16 1.00000000e+00 1.32921575e-20 1.59043254e-12
 1.31415266e-25], sum to 1.0000
[2019-03-27 06:47:31,823] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8565
[2019-03-27 06:47:31,830] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 58.66666666666667, 1.0, 2.0, 0.5545895935895162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 908492.3237519702, 908492.3237519709, 206181.3662040037], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 645000.0000, 
sim time next is 645600.0000, 
raw observation next is [24.06666666666667, 58.33333333333334, 1.0, 2.0, 0.5886806297422197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 964628.6644423761, 964628.6644423761, 213178.2764068074], 
processed observation next is [1.0, 0.4782608695652174, 0.3396524486571882, 0.5833333333333335, 1.0, 1.0, 0.5044344936653249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2679524067895489, 0.2679524067895489, 0.3181765319504588], 
reward next is 0.6818, 
noisyNet noise sample is [array([-0.6129258], dtype=float32), 1.4765483]. 
=============================================
[2019-03-27 06:47:32,834] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3909410e-15 1.0000000e+00 2.4502279e-19 9.1479792e-12 1.3889176e-24], sum to 1.0000
[2019-03-27 06:47:32,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8058
[2019-03-27 06:47:32,850] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.21666666666667, 79.16666666666667, 1.0, 2.0, 0.2448214115971884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 404476.7633929561, 404476.7633929567, 160337.0956815377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 679800.0000, 
sim time next is 680400.0000, 
raw observation next is [20.1, 80.0, 1.0, 2.0, 0.2428836359520094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401307.3486973058, 401307.3486973058, 160148.350047122], 
processed observation next is [1.0, 0.9130434782608695, 0.15165876777251197, 0.8, 1.0, 1.0, 0.08781160958073421, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11147426352702938, 0.11147426352702938, 0.23902738813003283], 
reward next is 0.7610, 
noisyNet noise sample is [array([0.920149], dtype=float32), -1.5983279]. 
=============================================
[2019-03-27 06:47:33,698] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3394728e-14 1.0000000e+00 2.9132144e-20 4.0948594e-12 5.5369486e-25], sum to 1.0000
[2019-03-27 06:47:33,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8156
[2019-03-27 06:47:33,713] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.8, 82.0, 1.0, 2.0, 0.2401950543834583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397035.174288188, 397035.174288188, 159880.6168841991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 682200.0000, 
sim time next is 682800.0000, 
raw observation next is [19.7, 82.66666666666667, 1.0, 2.0, 0.2393789772093673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395749.9689925067, 395749.9689925067, 159798.8369622887], 
processed observation next is [1.0, 0.9130434782608695, 0.1327014218009479, 0.8266666666666667, 1.0, 1.0, 0.08358912916791239, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10993054694236297, 0.10993054694236297, 0.23850572680938614], 
reward next is 0.7615, 
noisyNet noise sample is [array([0.8742195], dtype=float32), 1.2205493]. 
=============================================
[2019-03-27 06:47:33,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4244945e-15 1.0000000e+00 2.7356693e-19 1.0822188e-12 1.6796612e-24], sum to 1.0000
[2019-03-27 06:47:33,948] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1572
[2019-03-27 06:47:33,952] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.03333333333333, 86.66666666666667, 1.0, 2.0, 0.2377911770333683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 393784.5974520952, 393784.5974520946, 159595.9677909474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 686400.0000, 
sim time next is 687000.0000, 
raw observation next is [18.91666666666667, 87.33333333333334, 1.0, 2.0, 0.2361683106435371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 391235.6798511617, 391235.6798511617, 159430.0451352234], 
processed observation next is [1.0, 0.9565217391304348, 0.09557661927330202, 0.8733333333333334, 1.0, 1.0, 0.07972085619703265, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10867657773643381, 0.10867657773643381, 0.23795529124660209], 
reward next is 0.7620, 
noisyNet noise sample is [array([-1.7416488], dtype=float32), -1.3383561]. 
=============================================
[2019-03-27 06:47:33,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.44592 ]
 [77.43561 ]
 [77.42291 ]
 [77.41617 ]
 [77.395676]], R is [[77.43930817]
 [77.42671204]
 [77.41394043]
 [77.40107727]
 [77.38831329]].
[2019-03-27 06:47:34,764] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2494631e-14 1.0000000e+00 1.7952801e-19 2.6148295e-11 1.9711830e-24], sum to 1.0000
[2019-03-27 06:47:34,773] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5169
[2019-03-27 06:47:34,779] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 49.33333333333334, 1.0, 2.0, 0.6256760036857524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1024025.362968306, 1024025.362968305, 221193.4819674156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 740400.0000, 
sim time next is 741000.0000, 
raw observation next is [25.93333333333333, 48.66666666666666, 1.0, 2.0, 0.6237115175825323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1021657.733317938, 1021657.733317938, 220771.0262853622], 
processed observation next is [1.0, 0.5652173913043478, 0.42812006319115314, 0.4866666666666666, 1.0, 1.0, 0.5466403826295569, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28379381481053834, 0.28379381481053834, 0.3295089944557645], 
reward next is 0.6705, 
noisyNet noise sample is [array([-1.7242787], dtype=float32), -0.42073792]. 
=============================================
[2019-03-27 06:47:34,794] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.35163]
 [74.54069]
 [74.70529]
 [74.94256]
 [75.17451]], R is [[74.141922  ]
 [74.07036591]
 [73.9985733 ]
 [73.93490601]
 [73.89407349]].
[2019-03-27 06:47:35,585] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1081243: loss 0.0873
[2019-03-27 06:47:35,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1081243: learning rate 0.0000
[2019-03-27 06:47:35,688] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1081292: loss 0.0947
[2019-03-27 06:47:35,689] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1081293: learning rate 0.0000
[2019-03-27 06:47:36,552] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1081691: loss 0.0750
[2019-03-27 06:47:36,556] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1081692: learning rate 0.0000
[2019-03-27 06:47:37,038] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1081916: loss 0.0782
[2019-03-27 06:47:37,041] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1081918: learning rate 0.0000
[2019-03-27 06:47:37,163] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1081976: loss 0.0856
[2019-03-27 06:47:37,166] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1081976: learning rate 0.0000
[2019-03-27 06:47:37,204] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1081993: loss 0.0783
[2019-03-27 06:47:37,206] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1081994: learning rate 0.0000
[2019-03-27 06:47:37,591] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1082175: loss 0.0759
[2019-03-27 06:47:37,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1082175: learning rate 0.0000
[2019-03-27 06:47:37,697] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1082221: loss 0.0773
[2019-03-27 06:47:37,699] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1082222: learning rate 0.0000
[2019-03-27 06:47:37,982] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1082355: loss 0.0835
[2019-03-27 06:47:37,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1082355: learning rate 0.0000
[2019-03-27 06:47:38,007] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1082366: loss 0.0758
[2019-03-27 06:47:38,009] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1082366: learning rate 0.0000
[2019-03-27 06:47:38,138] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1082427: loss 0.0767
[2019-03-27 06:47:38,140] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1082427: learning rate 0.0000
[2019-03-27 06:47:38,175] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1082443: loss 0.0824
[2019-03-27 06:47:38,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1082444: learning rate 0.0000
[2019-03-27 06:47:38,593] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1082638: loss 0.0143
[2019-03-27 06:47:38,594] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1082638: learning rate 0.0000
[2019-03-27 06:47:38,649] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8249798e-16 1.0000000e+00 2.2685788e-21 5.1283587e-13 1.4859693e-25], sum to 1.0000
[2019-03-27 06:47:38,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1507
[2019-03-27 06:47:38,663] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 82.50000000000001, 1.0, 2.0, 0.2573163386722432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 422445.1565407084, 422445.1565407078, 161640.7203193426], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 767400.0000, 
sim time next is 768000.0000, 
raw observation next is [20.26666666666667, 83.0, 1.0, 2.0, 0.2571927438817255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 422256.5592356881, 422256.5592356881, 161628.2295664809], 
processed observation next is [1.0, 0.9130434782608695, 0.15955766192733034, 0.83, 1.0, 1.0, 0.10505149865268135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11729348867658003, 0.11729348867658003, 0.24123616353206107], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.82530046], dtype=float32), 0.9383537]. 
=============================================
[2019-03-27 06:47:38,674] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1082676: loss 0.0780
[2019-03-27 06:47:38,675] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1082676: learning rate 0.0000
[2019-03-27 06:47:38,677] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.280235]
 [77.11807 ]
 [76.95513 ]
 [76.81993 ]
 [76.707596]], R is [[77.44557953]
 [77.42987061]
 [77.41439819]
 [77.39923096]
 [77.3842926 ]].
[2019-03-27 06:47:42,084] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1084249: loss 0.0216
[2019-03-27 06:47:42,087] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1084249: learning rate 0.0000
[2019-03-27 06:47:42,514] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.7174992e-17 1.0000000e+00 2.2509567e-23 2.3836156e-14 3.0487738e-28], sum to 1.0000
[2019-03-27 06:47:42,522] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7147
[2019-03-27 06:47:42,528] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 79.33333333333334, 1.0, 2.0, 0.2996558232071326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 477486.3531816378, 477486.3531816385, 165447.0151375067], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 846600.0000, 
sim time next is 847200.0000, 
raw observation next is [22.46666666666667, 79.66666666666667, 1.0, 2.0, 0.2991246697559024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476844.6019803797, 476844.6019803797, 165404.4113641931], 
processed observation next is [0.0, 0.8260869565217391, 0.2638230647709322, 0.7966666666666667, 1.0, 1.0, 0.15557189127217155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13245683388343882, 0.13245683388343882, 0.2468722557674524], 
reward next is 0.7531, 
noisyNet noise sample is [array([-1.7355638], dtype=float32), 1.2023592]. 
=============================================
[2019-03-27 06:47:42,569] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1791333e-15 1.0000000e+00 9.6688631e-20 3.0576239e-14 2.8632526e-26], sum to 1.0000
[2019-03-27 06:47:42,577] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0977
[2019-03-27 06:47:42,582] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 76.33333333333334, 1.0, 2.0, 0.3101469496316649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490613.1999019503, 490613.1999019509, 166332.5888356419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 841200.0000, 
sim time next is 841800.0000, 
raw observation next is [23.15, 77.16666666666666, 1.0, 2.0, 0.3103404121180225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491231.2380670637, 491231.2380670637, 166384.4380141593], 
processed observation next is [0.0, 0.7391304347826086, 0.2962085308056872, 0.7716666666666666, 1.0, 1.0, 0.16908483387713552, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13645312168529547, 0.13645312168529547, 0.24833498211068553], 
reward next is 0.7517, 
noisyNet noise sample is [array([-0.11153115], dtype=float32), 2.719334]. 
=============================================
[2019-03-27 06:47:46,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7901394e-16 1.0000000e+00 7.6139374e-21 4.0016083e-14 4.6009854e-25], sum to 1.0000
[2019-03-27 06:47:46,323] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1086224: loss 0.0019
[2019-03-27 06:47:46,326] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0847
[2019-03-27 06:47:46,327] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1086227: learning rate 0.0000
[2019-03-27 06:47:46,332] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.294035003737254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 469359.9485909388, 469359.9485909381, 164885.8053358046], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 900000.0000, 
sim time next is 900600.0000, 
raw observation next is [22.5, 79.00000000000001, 1.0, 2.0, 0.2939398844746918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 469208.0941894522, 469208.0941894515, 164875.1835354764], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 0.7900000000000001, 1.0, 1.0, 0.149325162017701, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13033558171929227, 0.13033558171929208, 0.24608236348578566], 
reward next is 0.7539, 
noisyNet noise sample is [array([-1.5835646], dtype=float32), 0.15087056]. 
=============================================
[2019-03-27 06:47:47,209] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0354773e-15 1.0000000e+00 9.4172180e-20 3.9076758e-13 4.6429656e-25], sum to 1.0000
[2019-03-27 06:47:47,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8903
[2019-03-27 06:47:47,222] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 71.5, 1.0, 2.0, 0.31413214871874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494627.0220118128, 494627.0220118128, 166577.6945062599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 922200.0000, 
sim time next is 922800.0000, 
raw observation next is [24.2, 72.0, 1.0, 2.0, 0.3149666127718496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495637.203822925, 495637.2038229257, 166645.467085181], 
processed observation next is [0.0, 0.6956521739130435, 0.3459715639810427, 0.72, 1.0, 1.0, 0.17465856960463808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1376770010619236, 0.1376770010619238, 0.24872457773907614], 
reward next is 0.7513, 
noisyNet noise sample is [array([1.9825467], dtype=float32), -1.0711831]. 
=============================================
[2019-03-27 06:47:47,531] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6395571e-16 1.0000000e+00 1.1193114e-21 3.5069693e-13 1.5733989e-25], sum to 1.0000
[2019-03-27 06:47:47,541] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0990
[2019-03-27 06:47:47,546] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 51.0, 1.0, 2.0, 0.3501266927854602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536672.4447150303, 536672.4447150296, 169459.796027057], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1512600.0000, 
sim time next is 1513200.0000, 
raw observation next is [29.23333333333333, 51.00000000000001, 1.0, 2.0, 0.3512963936820009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537565.9046053564, 537565.9046053557, 169504.0934773705], 
processed observation next is [0.0, 0.5217391304347826, 0.5845181674565559, 0.5100000000000001, 1.0, 1.0, 0.21842938997831432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1493238623903768, 0.1493238623903766, 0.25299118429458284], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.9931917], dtype=float32), -0.33397242]. 
=============================================
[2019-03-27 06:47:50,353] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2274045e-16 1.0000000e+00 8.3284427e-19 1.4073115e-10 4.8033415e-25], sum to 1.0000
[2019-03-27 06:47:50,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8901
[2019-03-27 06:47:50,363] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 95.0, 1.0, 2.0, 0.4102773023912795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 634948.3089447392, 634948.3089447386, 178232.2694986775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 997200.0000, 
sim time next is 997800.0000, 
raw observation next is [21.68333333333333, 95.16666666666667, 1.0, 2.0, 0.3863456947753357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 598072.0404958345, 598072.0404958345, 174882.9244018733], 
processed observation next is [1.0, 0.5652173913043478, 0.22669826224328585, 0.9516666666666667, 1.0, 1.0, 0.2606574635847418, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16613112235995403, 0.16613112235995403, 0.26101929015204967], 
reward next is 0.7390, 
noisyNet noise sample is [array([-0.4347325], dtype=float32), 0.4346973]. 
=============================================
[2019-03-27 06:47:50,863] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.4142970e-16 1.0000000e+00 2.6080137e-20 3.1343350e-13 2.8173260e-25], sum to 1.0000
[2019-03-27 06:47:50,870] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4585
[2019-03-27 06:47:50,875] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.66666666666667, 1.0, 2.0, 0.3269795633378633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 507537.7101245375, 507537.7101245369, 167350.9904420424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 970800.0000, 
sim time next is 971400.0000, 
raw observation next is [21.9, 92.83333333333333, 1.0, 2.0, 0.3275763960988012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508178.4018330448, 508178.4018330454, 167391.3225322385], 
processed observation next is [1.0, 0.21739130434782608, 0.23696682464454974, 0.9283333333333332, 1.0, 1.0, 0.18985107963710984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14116066717584577, 0.14116066717584594, 0.24983779482423657], 
reward next is 0.7502, 
noisyNet noise sample is [array([-0.35620397], dtype=float32), 0.92074364]. 
=============================================
[2019-03-27 06:47:51,601] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6589147e-17 1.0000000e+00 1.1169234e-20 1.8522529e-13 2.2120268e-26], sum to 1.0000
[2019-03-27 06:47:51,609] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2646
[2019-03-27 06:47:51,619] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 97.83333333333334, 1.0, 2.0, 0.3713452241191626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563523.7318817053, 563523.731881706, 171552.6863757075], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1033800.0000, 
sim time next is 1034400.0000, 
raw observation next is [22.06666666666667, 97.66666666666667, 1.0, 2.0, 0.3720308147603605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 564311.300097655, 564311.3000976543, 171613.2578318562], 
processed observation next is [1.0, 1.0, 0.2448657187993683, 0.9766666666666667, 1.0, 1.0, 0.2434106201932054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15675313891601528, 0.15675313891601508, 0.25613919079381525], 
reward next is 0.7439, 
noisyNet noise sample is [array([0.746483], dtype=float32), -0.28397968]. 
=============================================
[2019-03-27 06:47:52,506] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1089140: loss 0.0015
[2019-03-27 06:47:52,510] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1089141: learning rate 0.0000
[2019-03-27 06:47:52,874] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1089312: loss 0.0024
[2019-03-27 06:47:52,878] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1089313: learning rate 0.0000
[2019-03-27 06:47:53,610] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1089657: loss 0.0013
[2019-03-27 06:47:53,613] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1089658: learning rate 0.0000
[2019-03-27 06:47:54,013] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1089851: loss 0.0014
[2019-03-27 06:47:54,017] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1089852: learning rate 0.0000
[2019-03-27 06:47:54,275] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1089973: loss 0.0011
[2019-03-27 06:47:54,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1089973: learning rate 0.0000
[2019-03-27 06:47:54,450] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1090054: loss 0.0016
[2019-03-27 06:47:54,453] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1090055: learning rate 0.0000
[2019-03-27 06:47:54,690] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1090165: loss 0.0011
[2019-03-27 06:47:54,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1090165: learning rate 0.0000
[2019-03-27 06:47:54,930] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1090280: loss 0.0013
[2019-03-27 06:47:54,934] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1090282: learning rate 0.0000
[2019-03-27 06:47:55,144] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1090378: loss 0.0014
[2019-03-27 06:47:55,149] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1090379: learning rate 0.0000
[2019-03-27 06:47:55,189] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1090396: loss 0.0017
[2019-03-27 06:47:55,191] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1090396: learning rate 0.0000
[2019-03-27 06:47:55,193] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1090398: loss 0.0011
[2019-03-27 06:47:55,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1090398: learning rate 0.0000
[2019-03-27 06:47:55,264] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1090437: loss 0.0011
[2019-03-27 06:47:55,266] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1090437: learning rate 0.0000
[2019-03-27 06:47:55,624] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1090603: loss 0.0016
[2019-03-27 06:47:55,627] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1090604: learning rate 0.0000
[2019-03-27 06:47:55,694] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1090634: loss 0.0307
[2019-03-27 06:47:55,697] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1090635: learning rate 0.0000
[2019-03-27 06:47:59,135] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1092252: loss 0.0386
[2019-03-27 06:47:59,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1092253: learning rate 0.0000
[2019-03-27 06:48:03,279] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1094328: loss 0.0704
[2019-03-27 06:48:03,281] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1094329: learning rate 0.0000
[2019-03-27 06:48:05,925] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0913911e-09 9.8133653e-01 9.2643400e-11 1.8663511e-02 1.5318154e-15], sum to 1.0000
[2019-03-27 06:48:05,932] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7262
[2019-03-27 06:48:05,940] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1840690.567280397 W.
[2019-03-27 06:48:05,946] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.96666666666667, 75.16666666666667, 1.0, 2.0, 0.658290295834458, 1.0, 2.0, 0.658290295834458, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1840690.567280397, 1840690.567280397, 356770.8135532322], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1267800.0000, 
sim time next is 1268400.0000, 
raw observation next is [27.93333333333333, 75.33333333333334, 1.0, 2.0, 0.4620624323189023, 1.0, 2.0, 0.4620624323189023, 1.0, 1.0, 0.7838916557157015, 6.911199999999999, 6.9112, 170.5573041426782, 1938094.621666146, 1938094.621666146, 386787.8733313697], 
processed observation next is [1.0, 0.6956521739130435, 0.522906793048973, 0.7533333333333334, 1.0, 1.0, 0.3518824485769907, 1.0, 1.0, 0.3518824485769907, 1.0, 0.5, 0.7364532386776846, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.538359617129485, 0.538359617129485, 0.5772953333304026], 
reward next is 0.4227, 
noisyNet noise sample is [array([-0.19016398], dtype=float32), -2.7681286]. 
=============================================
[2019-03-27 06:48:07,857] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.99756639e-16 1.00000000e+00 2.46490743e-20 3.62573280e-13
 1.30931455e-24], sum to 1.0000
[2019-03-27 06:48:07,863] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4717
[2019-03-27 06:48:07,867] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 94.0, 1.0, 2.0, 0.4570678034078495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649487.8429542875, 649487.8429542875, 178537.2880311009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1301400.0000, 
sim time next is 1302000.0000, 
raw observation next is [24.23333333333333, 94.0, 1.0, 2.0, 0.4560888869921874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648564.8309758569, 648564.8309758569, 178453.6880327009], 
processed observation next is [1.0, 0.043478260869565216, 0.3475513428120062, 0.94, 1.0, 1.0, 0.3446854060146836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18015689749329358, 0.18015689749329358, 0.2663487881085088], 
reward next is 0.7337, 
noisyNet noise sample is [array([1.5775295], dtype=float32), 1.9087756]. 
=============================================
[2019-03-27 06:48:07,889] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.59673]
 [72.58712]
 [72.56626]
 [72.54714]
 [72.52506]], R is [[72.65807343]
 [72.6650238 ]
 [72.67177582]
 [72.6783371 ]
 [72.68473816]].
[2019-03-27 06:48:09,715] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1097221: loss 0.1034
[2019-03-27 06:48:09,722] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1097222: learning rate 0.0000
[2019-03-27 06:48:09,819] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1097265: loss 0.0897
[2019-03-27 06:48:09,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1097266: learning rate 0.0000
[2019-03-27 06:48:10,665] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1097648: loss 0.1261
[2019-03-27 06:48:10,670] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1097649: learning rate 0.0000
[2019-03-27 06:48:11,152] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1097871: loss 0.1248
[2019-03-27 06:48:11,158] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1097871: learning rate 0.0000
[2019-03-27 06:48:11,457] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1098009: loss 0.1179
[2019-03-27 06:48:11,460] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1098009: learning rate 0.0000
[2019-03-27 06:48:11,465] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1098010: loss 0.1357
[2019-03-27 06:48:11,469] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1098011: learning rate 0.0000
[2019-03-27 06:48:11,594] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0913359e-14 1.0000000e+00 8.9762079e-20 4.2628095e-11 1.0041468e-24], sum to 1.0000
[2019-03-27 06:48:11,603] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1026
[2019-03-27 06:48:11,609] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.81666666666667, 92.16666666666667, 1.0, 2.0, 0.3646134778447146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581773.2967046147, 581773.2967046147, 173611.6546320042], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1357800.0000, 
sim time next is 1358400.0000, 
raw observation next is [20.83333333333334, 92.33333333333334, 1.0, 2.0, 0.3012415671543864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480381.7677212527, 480381.7677212527, 165658.9165345882], 
processed observation next is [1.0, 0.7391304347826086, 0.1864139020537128, 0.9233333333333335, 1.0, 1.0, 0.15812237006552576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13343937992257018, 0.13343937992257018, 0.24725211423072865], 
reward next is 0.7527, 
noisyNet noise sample is [array([1.1701584], dtype=float32), 0.40319538]. 
=============================================
[2019-03-27 06:48:11,729] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1098128: loss 0.1352
[2019-03-27 06:48:11,733] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1098128: learning rate 0.0000
[2019-03-27 06:48:11,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4073678e-15 1.0000000e+00 3.0392544e-19 6.3241745e-11 6.6389074e-24], sum to 1.0000
[2019-03-27 06:48:11,798] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7777
[2019-03-27 06:48:11,803] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.15, 94.0, 1.0, 2.0, 0.3223590488058941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508003.9912409307, 508003.9912409307, 167594.3001819901], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1366200.0000, 
sim time next is 1366800.0000, 
raw observation next is [21.16666666666667, 94.0, 1.0, 2.0, 0.3231308022277213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509013.5723045908, 509013.5723045908, 167666.8986250857], 
processed observation next is [1.0, 0.8260869565217391, 0.2022116903633494, 0.94, 1.0, 1.0, 0.18449494244303768, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14139265897349743, 0.14139265897349743, 0.2502491024255011], 
reward next is 0.7498, 
noisyNet noise sample is [array([0.01886493], dtype=float32), -1.5647831]. 
=============================================
[2019-03-27 06:48:11,924] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1098212: loss 0.1217
[2019-03-27 06:48:11,926] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1098212: learning rate 0.0000
[2019-03-27 06:48:12,315] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1098388: loss 0.1176
[2019-03-27 06:48:12,317] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1098389: loss 0.1222
[2019-03-27 06:48:12,318] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1098389: learning rate 0.0000
[2019-03-27 06:48:12,321] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1098389: learning rate 0.0000
[2019-03-27 06:48:12,372] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1098415: loss 0.1283
[2019-03-27 06:48:12,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1098416: learning rate 0.0000
[2019-03-27 06:48:12,428] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1098439: loss 0.1401
[2019-03-27 06:48:12,431] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1098439: learning rate 0.0000
[2019-03-27 06:48:12,749] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1098583: loss 0.1100
[2019-03-27 06:48:12,751] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1098585: learning rate 0.0000
[2019-03-27 06:48:13,119] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1098748: loss -126.1025
[2019-03-27 06:48:13,122] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1098749: learning rate 0.0000
[2019-03-27 06:48:15,887] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 06:48:15,889] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:48:15,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:48:15,891] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:48:15,892] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:48:15,892] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:48:15,894] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:48:15,894] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:48:15,893] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:48:15,895] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:48:15,899] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:48:15,918] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run45
[2019-03-27 06:48:15,939] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run45
[2019-03-27 06:48:15,939] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run45
[2019-03-27 06:48:15,973] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run45
[2019-03-27 06:48:15,993] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run45
[2019-03-27 06:48:27,171] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:48:27,173] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.55, 74.0, 1.0, 2.0, 0.3075638274208875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 487778.3974083118, 487778.3974083111, 166150.0258549135]
[2019-03-27 06:48:27,174] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:48:27,179] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.8241316e-16 1.0000000e+00 6.3304302e-21 4.4679755e-14 3.8215605e-25], sampled 0.2951346800993492
[2019-03-27 06:48:47,991] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:48:47,993] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.0, 90.16666666666667, 1.0, 2.0, 0.4230932003191427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619985.4479629924, 619985.4479629924, 176106.4766743232]
[2019-03-27 06:48:47,994] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:48:47,997] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.10583917e-16 1.00000000e+00 1.18857271e-21 4.15340234e-14
 1.98223535e-26], sampled 0.8095795191670468
[2019-03-27 06:48:52,990] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:48:52,991] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.16666666666667, 91.66666666666667, 1.0, 2.0, 0.5531454007898564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772961.776055016, 772961.776055016, 192341.8934636629]
[2019-03-27 06:48:52,993] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:48:52,996] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1571999e-16 1.0000000e+00 1.5033394e-21 7.1759189e-13 3.0434650e-27], sampled 0.6308686717923361
[2019-03-27 06:48:55,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:48:55,345] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.16666666666667, 87.33333333333334, 1.0, 2.0, 0.4936624669961582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689813.7229251548, 689813.7229251548, 182609.2640349944]
[2019-03-27 06:48:55,346] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:48:55,350] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2197083e-16 1.0000000e+00 1.3158960e-21 5.0656324e-14 2.1200754e-26], sampled 0.8658897622190274
[2019-03-27 06:48:59,169] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:48:59,170] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.5, 91.5, 1.0, 2.0, 0.691377519829407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1058189.701077414, 1058189.701077414, 230590.0933971494]
[2019-03-27 06:48:59,170] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:48:59,174] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.6815273e-16 1.0000000e+00 9.4766028e-21 7.4942092e-13 1.1190145e-25], sampled 0.1272521980831416
[2019-03-27 06:49:07,809] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:49:07,811] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.66293999, 78.66640305666667, 1.0, 2.0, 0.5173882108808446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722977.9075767404, 722977.9075767404, 186364.4704765549]
[2019-03-27 06:49:07,812] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:49:07,815] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.1615340e-16 1.0000000e+00 6.2679781e-21 3.8534968e-13 1.1922394e-25], sampled 0.21446780481159766
[2019-03-27 06:49:21,955] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:49:21,957] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.53601158333334, 61.39770114333334, 1.0, 2.0, 0.4946483293100857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691191.7547716302, 691191.7547716295, 182761.983343732]
[2019-03-27 06:49:21,958] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:49:21,962] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.249683e-17 1.000000e+00 9.378276e-22 9.405471e-14 8.676212e-27], sampled 0.8038657160074356
[2019-03-27 06:49:28,811] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:49:28,811] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.002494265, 88.695529365, 1.0, 2.0, 0.5078058642063167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709583.4414946137, 709583.441494613, 184827.5447168539]
[2019-03-27 06:49:28,812] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:49:28,815] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8215924e-16 1.0000000e+00 5.7031631e-21 3.3601691e-13 1.1231371e-25], sampled 0.8239317512174378
[2019-03-27 06:49:30,663] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:49:30,665] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.17033841, 67.49979331, 1.0, 2.0, 0.6031233107446364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 842828.2902376532, 842828.2902376525, 201328.5722983457]
[2019-03-27 06:49:30,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:49:30,670] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2867264e-16 1.0000000e+00 3.2328850e-21 7.6025876e-14 6.9298281e-26], sampled 0.09068340333611746
[2019-03-27 06:49:32,440] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:49:32,442] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.56666666666667, 88.33333333333334, 1.0, 2.0, 0.6274377529083274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 876820.2754362323, 876820.2754362329, 205969.2580120971]
[2019-03-27 06:49:32,444] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:49:32,446] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.4244696e-17 1.0000000e+00 5.8111432e-22 9.1211042e-14 4.6794297e-27], sampled 0.4890137508103145
[2019-03-27 06:49:37,251] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:49:37,251] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.530485065, 61.26623488999999, 1.0, 2.0, 0.6494026232160358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 907528.4563270484, 907528.4563270484, 210309.984951941]
[2019-03-27 06:49:37,252] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:49:37,255] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.0260544e-16 1.0000000e+00 1.0129785e-20 6.9211594e-12 2.3100539e-26], sampled 0.1234186979365618
[2019-03-27 06:49:42,909] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:49:42,911] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.374980035, 95.96673571, 1.0, 2.0, 0.614548545141941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858800.8180657976, 858800.8180657976, 203488.4732585797]
[2019-03-27 06:49:42,915] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:49:42,917] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.8865729e-17 1.0000000e+00 8.4177355e-22 1.5062558e-13 6.4584196e-27], sampled 0.6269305863221751
[2019-03-27 06:49:52,873] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:49:52,874] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.8973921, 88.20332348, 1.0, 2.0, 0.496348519205243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693568.2732042106, 693568.2732042106, 183025.6452322286]
[2019-03-27 06:49:52,878] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:49:52,882] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2195827e-16 1.0000000e+00 1.4848107e-21 1.0212775e-13 1.9210432e-26], sampled 0.3099432236103077
[2019-03-27 06:50:07,251] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02630792], dtype=float32), 0.043177254]
[2019-03-27 06:50:07,253] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.91666666666667, 58.33333333333334, 1.0, 2.0, 0.8065508937279751, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.964990440917981, 6.9112, 168.9125912024308, 2024241.684338502, 1986080.976757516, 410785.1854874616]
[2019-03-27 06:50:07,254] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:50:07,257] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7490427e-12 9.9999976e-01 4.0018323e-16 2.3012824e-07 9.2647759e-21], sampled 0.8471304063722574
[2019-03-27 06:50:07,258] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2024241.684338502 W.
[2019-03-27 06:50:11,246] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.2182 2842346753.3935 1130.0000
[2019-03-27 06:50:11,430] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7887.6388 3163990784.8941 1773.0000
[2019-03-27 06:50:11,431] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7647 3007775956.8125 1766.0000
[2019-03-27 06:50:11,690] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.6548 2779490254.0246 933.0000
[2019-03-27 06:50:11,702] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6262 2927437294.0986 1337.0000
[2019-03-27 06:50:12,718] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1100000, evaluation results [1100000.0, 7887.638826493281, 3163990784.894108, 1773.0, 8254.62623016878, 2927437294.0986147, 1337.0, 8657.654820088315, 2779490254.0245733, 933.0, 7998.764734894051, 3007775956.812482, 1766.0, 8498.218214586843, 2842346753.3934665, 1130.0]
[2019-03-27 06:50:12,923] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7282472e-15 1.0000000e+00 9.0648746e-22 8.7008288e-14 1.3483731e-26], sum to 1.0000
[2019-03-27 06:50:12,932] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9367
[2019-03-27 06:50:12,941] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 95.33333333333334, 1.0, 2.0, 0.366633294487637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558481.7451203557, 558481.7451203557, 171183.0095961729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1462800.0000, 
sim time next is 1463400.0000, 
raw observation next is [22.15, 95.5, 1.0, 2.0, 0.3652517941314636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556892.7480991762, 556892.7480991769, 171062.7908768603], 
processed observation next is [0.0, 0.9565217391304348, 0.24881516587677724, 0.955, 1.0, 1.0, 0.23524312545959467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15469243002754895, 0.15469243002754915, 0.25531759832367207], 
reward next is 0.7447, 
noisyNet noise sample is [array([0.2193301], dtype=float32), -0.70326173]. 
=============================================
[2019-03-27 06:50:13,414] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1100330: loss -77.6942
[2019-03-27 06:50:13,419] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1100331: learning rate 0.0000
[2019-03-27 06:50:13,559] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.3465705e-15 1.0000000e+00 2.7587625e-22 2.4455464e-14 4.5973868e-26], sum to 1.0000
[2019-03-27 06:50:13,568] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1165
[2019-03-27 06:50:13,574] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.85, 72.5, 1.0, 2.0, 0.4235511067033637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614299.4433071408, 614299.4433071408, 175378.5844271042], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1445400.0000, 
sim time next is 1446000.0000, 
raw observation next is [26.6, 73.66666666666667, 1.0, 2.0, 0.4220774919393711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613484.5792079386, 613484.5792079386, 175339.262532732], 
processed observation next is [0.0, 0.7391304347826086, 0.4597156398104266, 0.7366666666666667, 1.0, 1.0, 0.3037078216137002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17041238311331627, 0.17041238311331627, 0.2617003918398985], 
reward next is 0.7383, 
noisyNet noise sample is [array([-1.0508137], dtype=float32), 0.7294113]. 
=============================================
[2019-03-27 06:50:13,590] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[79.276474]
 [79.20833 ]
 [79.14431 ]
 [79.069   ]
 [78.979034]], R is [[79.27870941]
 [79.22416687]
 [79.17002106]
 [79.11611938]
 [79.06243134]].
[2019-03-27 06:50:13,620] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8207795e-18 1.0000000e+00 1.8104013e-21 3.9362625e-15 2.4025800e-26], sum to 1.0000
[2019-03-27 06:50:13,629] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3911
[2019-03-27 06:50:13,635] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 69.66666666666667, 1.0, 2.0, 0.4425385865042282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 629922.9750266817, 629922.9750266817, 176577.7746709021], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1438800.0000, 
sim time next is 1439400.0000, 
raw observation next is [27.93333333333334, 69.83333333333333, 1.0, 2.0, 0.4476488768239278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634772.7633866058, 634772.7633866058, 176999.4665341146], 
processed observation next is [0.0, 0.6521739130434783, 0.5229067930489735, 0.6983333333333333, 1.0, 1.0, 0.33451671906497327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1763257676073905, 0.1763257676073905, 0.26417830825987254], 
reward next is 0.7358, 
noisyNet noise sample is [array([2.2287102], dtype=float32), 0.6436465]. 
=============================================
[2019-03-27 06:50:15,889] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1173995e-17 1.0000000e+00 3.9714815e-22 8.6462196e-14 1.1207831e-25], sum to 1.0000
[2019-03-27 06:50:15,898] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8913
[2019-03-27 06:50:15,909] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.26666666666667, 98.83333333333333, 1.0, 2.0, 0.3089550858985022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490307.2586242497, 490307.2586242491, 166340.6201551674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1486200.0000, 
sim time next is 1486800.0000, 
raw observation next is [20.2, 99.0, 1.0, 2.0, 0.3072791896070122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488180.2791504993, 488180.2791505, 166194.4007160553], 
processed observation next is [0.0, 0.21739130434782608, 0.15639810426540288, 0.99, 1.0, 1.0, 0.16539661398435207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13560563309736093, 0.13560563309736112, 0.24805134435232135], 
reward next is 0.7519, 
noisyNet noise sample is [array([-0.02195418], dtype=float32), -0.6286972]. 
=============================================
[2019-03-27 06:50:15,955] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.9012367e-17 1.0000000e+00 3.8217410e-22 8.4926789e-14 1.0643713e-25], sum to 1.0000
[2019-03-27 06:50:15,964] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1018
[2019-03-27 06:50:15,969] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.41666666666666, 98.33333333333333, 1.0, 2.0, 0.3094574603397627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490732.9703280488, 490732.9703280488, 166365.2974024624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1487400.0000, 
sim time next is 1488000.0000, 
raw observation next is [20.63333333333333, 97.66666666666669, 1.0, 2.0, 0.3132529939445962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495157.7733525257, 495157.7733525263, 166660.6729363645], 
processed observation next is [0.0, 0.21739130434782608, 0.17693522906793036, 0.9766666666666669, 1.0, 1.0, 0.17259396860794723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13754382593125714, 0.1375438259312573, 0.24874727303935001], 
reward next is 0.7513, 
noisyNet noise sample is [array([-0.02195418], dtype=float32), -0.6286972]. 
=============================================
[2019-03-27 06:50:15,985] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.81888]
 [74.797  ]
 [74.75198]
 [74.74675]
 [74.78398]], R is [[74.82191467]
 [74.82539368]
 [74.8290863 ]
 [74.83252716]
 [74.83575439]].
[2019-03-27 06:50:17,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2316665e-17 1.0000000e+00 2.9102916e-21 1.2975663e-15 1.7474559e-26], sum to 1.0000
[2019-03-27 06:50:17,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8110
[2019-03-27 06:50:17,212] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 66.5, 1.0, 2.0, 0.3633739608108957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 552737.9662072669, 552737.9662072662, 170668.3423413], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1503000.0000, 
sim time next is 1503600.0000, 
raw observation next is [26.66666666666667, 64.66666666666667, 1.0, 2.0, 0.3609115328024661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549785.6662363309, 549785.6662363316, 170443.7135454415], 
processed observation next is [0.0, 0.391304347826087, 0.4628751974723541, 0.6466666666666667, 1.0, 1.0, 0.2300138949427302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.152718240621203, 0.1527182406212032, 0.2543936023066291], 
reward next is 0.7456, 
noisyNet noise sample is [array([1.0168363], dtype=float32), 0.8367639]. 
=============================================
[2019-03-27 06:50:17,591] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1102272: loss 0.0319
[2019-03-27 06:50:17,593] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1102274: learning rate 0.0000
[2019-03-27 06:50:18,283] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9962415e-17 1.0000000e+00 4.7896649e-21 2.1603699e-14 1.4940120e-25], sum to 1.0000
[2019-03-27 06:50:18,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3079
[2019-03-27 06:50:18,301] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.35, 84.5, 1.0, 2.0, 0.3581996044590471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549699.2639673849, 549699.2639673844, 170561.4200263185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1542600.0000, 
sim time next is 1543200.0000, 
raw observation next is [23.26666666666667, 85.0, 1.0, 2.0, 0.3581687164955957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549893.545817771, 549893.5458177704, 170584.802836403], 
processed observation next is [0.0, 0.8695652173913043, 0.3017377567140602, 0.85, 1.0, 1.0, 0.2267092969826454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15274820717160306, 0.1527482071716029, 0.2546041833379149], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.38098675], dtype=float32), 1.3817006]. 
=============================================
[2019-03-27 06:50:19,799] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.0518748e-16 1.0000000e+00 1.1565533e-20 1.3491449e-11 7.8398117e-24], sum to 1.0000
[2019-03-27 06:50:19,809] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9287
[2019-03-27 06:50:19,818] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 85.33333333333334, 1.0, 2.0, 0.3917076470776573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602260.0155573708, 602260.0155573714, 175180.5040984568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1582800.0000, 
sim time next is 1583400.0000, 
raw observation next is [23.3, 85.16666666666667, 1.0, 2.0, 0.3847513131915307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590258.5987439593, 590258.5987439593, 174078.2045986252], 
processed observation next is [1.0, 0.30434782608695654, 0.3033175355450238, 0.8516666666666667, 1.0, 1.0, 0.2587365219175069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16396072187332203, 0.16396072187332203, 0.25981821581884357], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.9974806], dtype=float32), -0.27598822]. 
=============================================
[2019-03-27 06:50:23,925] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1105241: loss 0.0130
[2019-03-27 06:50:23,927] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1105241: learning rate 0.0000
[2019-03-27 06:50:23,987] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1105272: loss 0.0106
[2019-03-27 06:50:23,993] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1105273: learning rate 0.0000
[2019-03-27 06:50:24,703] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1105610: loss 0.0131
[2019-03-27 06:50:24,706] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1105610: learning rate 0.0000
[2019-03-27 06:50:25,070] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1105779: loss 0.0140
[2019-03-27 06:50:25,073] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1105779: learning rate 0.0000
[2019-03-27 06:50:25,553] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1106005: loss 0.0207
[2019-03-27 06:50:25,555] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1106006: learning rate 0.0000
[2019-03-27 06:50:25,572] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1106016: loss 0.0152
[2019-03-27 06:50:25,576] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1106016: learning rate 0.0000
[2019-03-27 06:50:25,842] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1106142: loss 0.0177
[2019-03-27 06:50:25,846] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1106142: learning rate 0.0000
[2019-03-27 06:50:25,976] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1106200: loss 0.0197
[2019-03-27 06:50:25,979] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1106201: learning rate 0.0000
[2019-03-27 06:50:26,280] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1106346: loss 0.0186
[2019-03-27 06:50:26,282] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1106347: learning rate 0.0000
[2019-03-27 06:50:26,302] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1106354: loss 0.0292
[2019-03-27 06:50:26,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1106354: learning rate 0.0000
[2019-03-27 06:50:26,335] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1106369: loss 0.0241
[2019-03-27 06:50:26,339] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1106370: learning rate 0.0000
[2019-03-27 06:50:26,408] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1106404: loss 0.0223
[2019-03-27 06:50:26,411] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1106404: learning rate 0.0000
[2019-03-27 06:50:26,654] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1106521: loss 0.0222
[2019-03-27 06:50:26,655] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1106521: learning rate 0.0000
[2019-03-27 06:50:26,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.4358408e-13 9.9999976e-01 3.2293042e-17 2.5623862e-07 4.0035017e-21], sum to 1.0000
[2019-03-27 06:50:26,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4302
[2019-03-27 06:50:26,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1954935.087934467 W.
[2019-03-27 06:50:26,745] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.5, 83.0, 1.0, 2.0, 0.7570286625704306, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.979187057094947, 6.9112, 168.9117337614022, 1954935.087934467, 1906703.078848047, 398452.4845551426], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1690200.0000, 
sim time next is 1690800.0000, 
raw observation next is [27.6, 82.66666666666667, 1.0, 2.0, 0.4166403398621438, 1.0, 1.0, 0.4166403398621438, 1.0, 2.0, 0.7139559152614232, 6.9112, 6.9112, 170.5573041426782, 1747418.879518675, 1747418.879518675, 360873.500998114], 
processed observation next is [1.0, 0.5652173913043478, 0.5071090047393366, 0.8266666666666667, 1.0, 1.0, 0.29715703597848653, 1.0, 0.5, 0.29715703597848653, 1.0, 1.0, 0.6511657503188089, 0.0, 0.0, 0.8375144448122397, 0.4853941331996319, 0.4853941331996319, 0.5386171656688269], 
reward next is 0.4614, 
noisyNet noise sample is [array([-1.0142717], dtype=float32), 1.8587275]. 
=============================================
[2019-03-27 06:50:27,312] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1106830: loss 0.1189
[2019-03-27 06:50:27,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1106830: learning rate 0.0000
[2019-03-27 06:50:28,679] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5949477e-09 9.8436099e-01 1.2894516e-11 1.5639054e-02 2.0331500e-16], sum to 1.0000
[2019-03-27 06:50:28,690] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5431
[2019-03-27 06:50:28,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1950645.453744221 W.
[2019-03-27 06:50:28,706] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.45, 77.33333333333334, 1.0, 2.0, 0.465051963050491, 1.0, 2.0, 0.465051963050491, 1.0, 1.0, 0.7980930051882361, 6.9112, 6.9112, 170.5573041426782, 1950645.453744221, 1950645.453744221, 390223.1171582406], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1698600.0000, 
sim time next is 1699200.0000, 
raw observation next is [28.5, 77.0, 1.0, 2.0, 0.4546812869790741, 1.0, 2.0, 0.4546812869790741, 1.0, 2.0, 0.7806216184134048, 6.9112, 6.9112, 170.5573041426782, 1907107.272237523, 1907107.272237523, 383754.2929353936], 
processed observation next is [1.0, 0.6956521739130435, 0.5497630331753555, 0.77, 1.0, 1.0, 0.34298950238442666, 1.0, 1.0, 0.34298950238442666, 1.0, 1.0, 0.7324653883090302, 0.0, 0.0, 0.8375144448122397, 0.5297520200659787, 0.5297520200659787, 0.5727676013961098], 
reward next is 0.4272, 
noisyNet noise sample is [array([-0.8652195], dtype=float32), 0.6907702]. 
=============================================
[2019-03-27 06:50:30,767] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1108399: loss 0.1006
[2019-03-27 06:50:30,771] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1108400: learning rate 0.0000
[2019-03-27 06:50:33,164] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3770526e-09 8.4168398e-01 1.1959914e-11 1.5831599e-01 3.3962269e-15], sum to 1.0000
[2019-03-27 06:50:33,174] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0133
[2019-03-27 06:50:33,186] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2082941.820226332 W.
[2019-03-27 06:50:33,191] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.85, 64.5, 1.0, 2.0, 0.7448430161414825, 1.0, 2.0, 0.7448430161414825, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2082941.820226332, 2082941.820226332, 393914.7442963764], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2295000.0000, 
sim time next is 2295600.0000, 
raw observation next is [31.83333333333334, 64.66666666666666, 1.0, 2.0, 0.5393666190402353, 1.0, 2.0, 0.5393666190402353, 1.0, 1.0, 0.9367014460062993, 6.911199999999999, 6.9112, 170.5573041426782, 2262677.537766359, 2262677.53776636, 443436.2497187288], 
processed observation next is [1.0, 0.5652173913043478, 0.7077409162717223, 0.6466666666666666, 1.0, 1.0, 0.4450200229400425, 1.0, 1.0, 0.4450200229400425, 1.0, 0.5, 0.9228066414710966, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.628521538268433, 0.6285215382684334, 0.6618451488339236], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3720444], dtype=float32), 0.24939202]. 
=============================================
[2019-03-27 06:50:33,393] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.0438994e-15 1.0000000e+00 9.4918025e-20 2.6196819e-11 8.7526839e-24], sum to 1.0000
[2019-03-27 06:50:33,404] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8912
[2019-03-27 06:50:33,410] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.06666666666667, 86.5, 1.0, 2.0, 0.3165906930151058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499050.6682210597, 499050.6682210597, 166920.8893998603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1792200.0000, 
sim time next is 1792800.0000, 
raw observation next is [22.0, 87.0, 1.0, 2.0, 0.3202477053859197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504834.306482217, 504834.306482217, 167356.9322906791], 
processed observation next is [1.0, 0.782608695652174, 0.2417061611374408, 0.87, 1.0, 1.0, 0.18102133179026467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14023175180061584, 0.14023175180061584, 0.2497864661054912], 
reward next is 0.7502, 
noisyNet noise sample is [array([1.7861181], dtype=float32), -0.7584372]. 
=============================================
[2019-03-27 06:50:34,954] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1110359: loss -151.8733
[2019-03-27 06:50:34,956] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1110359: learning rate 0.0000
[2019-03-27 06:50:40,958] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1113189: loss 98.7206
[2019-03-27 06:50:40,960] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1113189: learning rate 0.0000
[2019-03-27 06:50:41,158] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1113280: loss -148.0092
[2019-03-27 06:50:41,159] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1113280: learning rate 0.0000
[2019-03-27 06:50:41,920] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1113632: loss -181.1148
[2019-03-27 06:50:41,922] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1113632: learning rate 0.0000
[2019-03-27 06:50:42,340] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1113826: loss -158.8784
[2019-03-27 06:50:42,342] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1113826: learning rate 0.0000
[2019-03-27 06:50:42,694] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1113993: loss -131.3872
[2019-03-27 06:50:42,696] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1113994: learning rate 0.0000
[2019-03-27 06:50:42,805] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1114052: loss -41.0654
[2019-03-27 06:50:42,807] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1114052: learning rate 0.0000
[2019-03-27 06:50:42,955] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1114124: loss 6.2029
[2019-03-27 06:50:42,957] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1114125: learning rate 0.0000
[2019-03-27 06:50:43,005] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1114142: loss -30.3418
[2019-03-27 06:50:43,007] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1114142: learning rate 0.0000
[2019-03-27 06:50:43,263] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1114283: loss -117.2596
[2019-03-27 06:50:43,267] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1114283: learning rate 0.0000
[2019-03-27 06:50:43,338] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1114323: loss -290.3133
[2019-03-27 06:50:43,339] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1114323: learning rate 0.0000
[2019-03-27 06:50:43,409] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1114367: loss -143.4994
[2019-03-27 06:50:43,410] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1114368: learning rate 0.0000
[2019-03-27 06:50:43,441] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1114383: loss -208.4467
[2019-03-27 06:50:43,443] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1114384: learning rate 0.0000
[2019-03-27 06:50:43,610] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1114484: loss -75.7186
[2019-03-27 06:50:43,614] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1114486: learning rate 0.0000
[2019-03-27 06:50:44,684] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1115132: loss -126.9403
[2019-03-27 06:50:44,689] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1115134: learning rate 0.0000
[2019-03-27 06:50:48,212] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1116759: loss -554.0314
[2019-03-27 06:50:48,215] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1116759: learning rate 0.0000
[2019-03-27 06:50:49,456] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.4136820e-17 1.0000000e+00 6.6008867e-21 7.9695256e-14 4.5192442e-26], sum to 1.0000
[2019-03-27 06:50:49,466] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1157
[2019-03-27 06:50:49,472] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 92.66666666666667, 1.0, 2.0, 0.4732750241149288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661416.11784795, 661416.11784795, 179527.3883171952], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2067600.0000, 
sim time next is 2068200.0000, 
raw observation next is [24.75, 93.0, 1.0, 2.0, 0.472992022023516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661204.8724338688, 661204.8724338694, 179509.1613234807], 
processed observation next is [0.0, 0.9565217391304348, 0.3720379146919432, 0.93, 1.0, 1.0, 0.36505062894399515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1836680201205191, 0.18366802012051928, 0.2679241213783294], 
reward next is 0.7321, 
noisyNet noise sample is [array([-0.92956007], dtype=float32), -1.4036692]. 
=============================================
[2019-03-27 06:50:51,133] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1118303: loss 0.1222
[2019-03-27 06:50:51,135] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1118304: learning rate 0.0000
[2019-03-27 06:50:54,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3095419e-16 1.0000000e+00 2.6472151e-20 3.9491934e-13 3.9878864e-25], sum to 1.0000
[2019-03-27 06:50:54,196] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2401
[2019-03-27 06:50:54,199] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3989862227200194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594915.4175127856, 594915.4175127856, 174057.6605508872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2664000.0000, 
sim time next is 2664600.0000, 
raw observation next is [23.0, 94.00000000000001, 1.0, 2.0, 0.3977482839684885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 593474.40523567, 593474.40523567, 173936.5160771536], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.9400000000000002, 1.0, 1.0, 0.2743955228536006, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16485400145435278, 0.16485400145435278, 0.25960674041366205], 
reward next is 0.7404, 
noisyNet noise sample is [array([1.597925], dtype=float32), -2.5323093]. 
=============================================
[2019-03-27 06:50:57,319] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1121107: loss 0.0635
[2019-03-27 06:50:57,321] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1121108: learning rate 0.0000
[2019-03-27 06:50:57,451] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1121169: loss 0.0670
[2019-03-27 06:50:57,456] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1121169: learning rate 0.0000
[2019-03-27 06:50:58,375] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1121582: loss 0.0640
[2019-03-27 06:50:58,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1121583: learning rate 0.0000
[2019-03-27 06:50:58,752] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1121750: loss 0.0655
[2019-03-27 06:50:58,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1121750: learning rate 0.0000
[2019-03-27 06:50:59,348] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1122019: loss 0.0680
[2019-03-27 06:50:59,350] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1122019: learning rate 0.0000
[2019-03-27 06:50:59,403] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1122044: loss 0.0635
[2019-03-27 06:50:59,406] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1122045: learning rate 0.0000
[2019-03-27 06:50:59,538] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1122102: loss 0.0646
[2019-03-27 06:50:59,540] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1122103: learning rate 0.0000
[2019-03-27 06:50:59,569] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1122118: loss 0.0633
[2019-03-27 06:50:59,570] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1122118: learning rate 0.0000
[2019-03-27 06:50:59,807] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1122224: loss 0.0642
[2019-03-27 06:50:59,810] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1122224: learning rate 0.0000
[2019-03-27 06:50:59,827] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1122229: loss 0.0662
[2019-03-27 06:50:59,831] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1122232: learning rate 0.0000
[2019-03-27 06:51:00,103] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1122358: loss 0.0692
[2019-03-27 06:51:00,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1122358: learning rate 0.0000
[2019-03-27 06:51:00,139] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1122371: loss 0.0721
[2019-03-27 06:51:00,141] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1122371: learning rate 0.0000
[2019-03-27 06:51:00,326] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1122455: loss 0.0575
[2019-03-27 06:51:00,328] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1122455: learning rate 0.0000
[2019-03-27 06:51:00,677] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1297917e-15 1.0000000e+00 6.5448794e-19 4.4004353e-10 3.4583217e-23], sum to 1.0000
[2019-03-27 06:51:00,687] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6183
[2019-03-27 06:51:00,693] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 79.66666666666667, 1.0, 2.0, 0.6622450349849871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 925483.3148388439, 925483.3148388439, 212904.4483568755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2270400.0000, 
sim time next is 2271000.0000, 
raw observation next is [27.45, 78.83333333333333, 1.0, 2.0, 0.6725429737793114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 939880.9882351824, 939880.988235183, 215027.8458054866], 
processed observation next is [1.0, 0.2608695652173913, 0.5, 0.7883333333333333, 1.0, 1.0, 0.6054734623847124, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26107805228755065, 0.2610780522875508, 0.32093708329177106], 
reward next is 0.6791, 
noisyNet noise sample is [array([-1.1629639], dtype=float32), 0.112270825]. 
=============================================
[2019-03-27 06:51:00,704] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[65.88522 ]
 [66.10737 ]
 [66.156105]
 [66.26787 ]
 [66.42116 ]], R is [[65.719841  ]
 [65.74488068]
 [65.77789307]
 [65.81328583]
 [65.83569336]].
[2019-03-27 06:51:01,446] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1122953: loss 0.0751
[2019-03-27 06:51:01,451] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1122953: learning rate 0.0000
[2019-03-27 06:51:02,958] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9592860e-10 9.9066705e-01 6.7234778e-14 9.3329325e-03 6.5922639e-18], sum to 1.0000
[2019-03-27 06:51:02,967] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8066
[2019-03-27 06:51:02,975] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2641514.594515148 W.
[2019-03-27 06:51:02,982] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.36666666666667, 64.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.415142459733333, 6.9112, 168.9096343128637, 2641514.594515148, 2284007.500844362, 474982.5798503779], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2307000.0000, 
sim time next is 2307600.0000, 
raw observation next is [32.4, 64.0, 1.0, 2.0, 0.5849655171653664, 1.0, 1.0, 0.5849655171653664, 1.0, 2.0, 1.015891652263609, 6.911199999999999, 6.9112, 170.5573041426782, 2454155.671969077, 2454155.671969078, 478872.5458754809], 
processed observation next is [1.0, 0.7391304347826086, 0.7345971563981042, 0.64, 1.0, 1.0, 0.49995845441610404, 1.0, 0.5, 0.49995845441610404, 1.0, 1.0, 1.0193800637361086, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6817099088802991, 0.6817099088802995, 0.7147351430977327], 
reward next is 0.2853, 
noisyNet noise sample is [array([-1.4462547], dtype=float32), 0.8492001]. 
=============================================
[2019-03-27 06:51:04,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.26362965e-08 6.69518292e-01 2.93218477e-10 3.30481768e-01
 5.81853984e-16], sum to 1.0000
[2019-03-27 06:51:04,717] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2557
[2019-03-27 06:51:04,725] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2604722.754711577 W.
[2019-03-27 06:51:04,729] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.4, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.363326680065642, 6.9112, 168.9103010725829, 2604722.754711577, 2283973.56936967, 475099.2794742752], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2376000.0000, 
sim time next is 2376600.0000, 
raw observation next is [32.43333333333334, 63.83333333333334, 1.0, 2.0, 0.7727997833461502, 1.0, 1.0, 0.7727997833461502, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2161201.34882602, 2161201.34882602, 406851.3850285819], 
processed observation next is [1.0, 0.5217391304347826, 0.7361769352290681, 0.6383333333333334, 1.0, 1.0, 0.7262647992122291, 1.0, 0.5, 0.7262647992122291, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6003337080072277, 0.6003337080072277, 0.6072408731769879], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1897272], dtype=float32), -1.4677376]. 
=============================================
[2019-03-27 06:51:05,150] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1124633: loss 0.0787
[2019-03-27 06:51:05,153] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1124633: learning rate 0.0000
[2019-03-27 06:51:05,923] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.6159136e-12 9.9999917e-01 3.0033096e-16 8.8806820e-07 7.2012220e-21], sum to 1.0000
[2019-03-27 06:51:05,934] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7653
[2019-03-27 06:51:05,942] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1921181.005778967 W.
[2019-03-27 06:51:05,951] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.1, 70.0, 1.0, 2.0, 0.6870504870222949, 1.0, 2.0, 0.6870504870222949, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1921181.005778967, 1921181.005778967, 368629.0260074215], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2364000.0000, 
sim time next is 2364600.0000, 
raw observation next is [30.25, 69.5, 1.0, 2.0, 0.7713201676578914, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.989554583509987, 6.9112, 168.9124903123288, 1974935.646661619, 1919348.361783779, 401490.6301107613], 
processed observation next is [1.0, 0.34782608695652173, 0.6327014218009479, 0.695, 1.0, 1.0, 0.7244821297083028, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007835458350998702, 0.0, 0.8294376559061944, 0.548593235183783, 0.5331523227177164, 0.5992397464339722], 
reward next is 0.0090, 
noisyNet noise sample is [array([-1.1281798], dtype=float32), -0.40346107]. 
=============================================
[2019-03-27 06:51:05,957] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 06:51:05,958] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:51:05,959] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:51:05,960] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:51:05,961] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:51:05,962] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:51:05,962] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:51:05,962] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:51:05,967] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:51:05,967] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:51:05,970] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:51:05,987] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run46
[2019-03-27 06:51:06,005] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run46
[2019-03-27 06:51:06,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run46
[2019-03-27 06:51:06,046] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run46
[2019-03-27 06:51:06,047] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run46
[2019-03-27 06:52:05,133] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02738577], dtype=float32), 0.04231682]
[2019-03-27 06:52:05,135] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.66666666666667, 80.66666666666667, 1.0, 2.0, 0.9234342399273381, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005989195338664, 6.9112, 168.9123931355559, 2187838.459859124, 2120591.966581326, 440627.6657766824]
[2019-03-27 06:52:05,136] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:52:05,141] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7991945e-11 9.9999952e-01 4.3197240e-15 5.0585516e-07 6.5013548e-19], sampled 0.479793052987736
[2019-03-27 06:52:05,142] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2187838.459859124 W.
[2019-03-27 06:52:26,713] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02738577], dtype=float32), 0.04231682]
[2019-03-27 06:52:26,716] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.82359084, 68.66695018, 1.0, 2.0, 0.6466695521766505, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005971953771668, 6.9112, 168.9123160671426, 1800503.247965747, 1733269.017084668, 373512.4039153365]
[2019-03-27 06:52:26,721] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:52:26,723] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4518118e-12 9.9999988e-01 3.7470466e-16 6.8017030e-08 3.3992858e-20], sampled 0.8780725824898732
[2019-03-27 06:52:26,723] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1800503.247965747 W.
[2019-03-27 06:52:32,655] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02738577], dtype=float32), 0.04231682]
[2019-03-27 06:52:32,656] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.41666666666666, 82.83333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.386758016428325, 6.9112, 168.9102938965813, 1791357.906817571, 1453986.000674479, 311354.5132557623]
[2019-03-27 06:52:32,658] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:52:32,662] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.2292695e-15 1.0000000e+00 2.1897777e-19 4.9585929e-11 5.7684448e-24], sampled 0.6218916142894684
[2019-03-27 06:52:32,663] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1791357.906817571 W.
[2019-03-27 06:52:36,024] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02738577], dtype=float32), 0.04231682]
[2019-03-27 06:52:36,024] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.26292495, 72.099597655, 1.0, 2.0, 0.7028167464759293, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005975459473369, 6.9112, 168.9123158079941, 1879070.429083504, 1811833.711249568, 385274.5680450254]
[2019-03-27 06:52:36,025] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:52:36,028] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9468068e-11 9.9999547e-01 5.9913275e-15 4.5843831e-06 2.2070600e-19], sampled 0.23442509498107733
[2019-03-27 06:52:36,029] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1879070.429083504 W.
[2019-03-27 06:52:45,083] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02738577], dtype=float32), 0.04231682]
[2019-03-27 06:52:45,084] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.9, 70.5, 1.0, 2.0, 0.8707432385939515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1217025.227339289, 1217025.22733929, 262019.0676478792]
[2019-03-27 06:52:45,086] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:52:45,087] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.2046576e-15 1.0000000e+00 3.1795996e-19 3.5754528e-11 9.5936002e-24], sampled 0.629141235377376
[2019-03-27 06:52:54,691] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02738577], dtype=float32), 0.04231682]
[2019-03-27 06:52:54,692] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.43333333333333, 89.16666666666667, 1.0, 2.0, 0.537329516017936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869358.8287553352, 869358.8287553352, 202392.1847829527]
[2019-03-27 06:52:54,693] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:52:54,695] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.7364580e-15 1.0000000e+00 1.4413050e-19 6.9831502e-12 4.4082837e-24], sampled 0.5686891922212057
[2019-03-27 06:53:01,474] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.8259 2927409401.1429 1336.0000
[2019-03-27 06:53:01,640] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.3031 3006813695.8749 1744.0000
[2019-03-27 06:53:01,825] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.1979 2779166948.3119 930.0000
[2019-03-27 06:53:01,904] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7893.1979 3161940540.9682 1728.0000
[2019-03-27 06:53:01,939] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6714 2842183963.5380 1123.0000
[2019-03-27 06:53:02,956] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1125000, evaluation results [1125000.0, 7893.19786167206, 3161940540.9682455, 1728.0, 8252.825896696128, 2927409401.1429424, 1336.0, 8660.197853068235, 2779166948.311893, 930.0, 7996.303134720785, 3006813695.8749495, 1744.0, 8496.671390855221, 2842183963.537955, 1123.0]
[2019-03-27 06:53:06,149] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1126491: loss -458.2124
[2019-03-27 06:53:06,153] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1126491: learning rate 0.0000
[2019-03-27 06:53:06,779] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0387720e-16 1.0000000e+00 3.9296081e-19 4.8980178e-12 4.8505946e-23], sum to 1.0000
[2019-03-27 06:53:06,788] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0498
[2019-03-27 06:53:06,795] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 80.0, 1.0, 2.0, 0.5435166482092623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759501.8095895732, 759501.8095895732, 190695.2003592339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2426400.0000, 
sim time next is 2427000.0000, 
raw observation next is [28.55, 80.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.828641230438564, 6.9112, 168.9081154799867, 2105046.506800886, 1454200.775753411, 311351.2622795781], 
processed observation next is [1.0, 0.08695652173913043, 0.552132701421801, 0.8016666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.09174412304385644, 0.0, 0.8294161734765878, 0.5847351407780238, 0.4039446599315031, 0.4647033765366837], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9812672], dtype=float32), 0.468945]. 
=============================================
[2019-03-27 06:53:06,806] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.661194]
 [67.45353 ]
 [67.40136 ]
 [67.351814]
 [67.32    ]], R is [[66.02303314]
 [66.07817841]
 [66.13246918]
 [66.18589783]
 [66.2384491 ]].
[2019-03-27 06:53:06,920] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.712210e-10 9.951003e-01 9.693356e-14 4.899702e-03 4.183292e-19], sum to 1.0000
[2019-03-27 06:53:06,931] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6020
[2019-03-27 06:53:06,936] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.96666666666667, 77.0, 1.0, 2.0, 0.5029648967626221, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702816.669106411, 702816.6691064105, 184064.5716060475], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2568000.0000, 
sim time next is 2568600.0000, 
raw observation next is [28.9, 77.5, 1.0, 2.0, 0.4997080477953588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698264.2240624734, 698264.2240624728, 183553.2336430737], 
processed observation next is [1.0, 0.7391304347826086, 0.5687203791469194, 0.775, 1.0, 1.0, 0.3972386118016371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19396228446179817, 0.193962284461798, 0.27396005021354286], 
reward next is 0.7260, 
noisyNet noise sample is [array([-1.6253995], dtype=float32), 1.2109436]. 
=============================================
[2019-03-27 06:53:12,081] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1129231: loss -215.4674
[2019-03-27 06:53:12,083] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1129231: learning rate 0.0000
[2019-03-27 06:53:12,185] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1129279: loss -111.0578
[2019-03-27 06:53:12,187] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1129281: learning rate 0.0000
[2019-03-27 06:53:12,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6715646e-14 1.0000000e+00 7.6420876e-18 5.4215238e-10 8.1705801e-23], sum to 1.0000
[2019-03-27 06:53:12,776] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4861
[2019-03-27 06:53:12,785] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 95.33333333333333, 1.0, 2.0, 0.7050951351341217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 985393.8444012653, 985393.8444012653, 221949.5710385862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2528400.0000, 
sim time next is 2529000.0000, 
raw observation next is [26.35, 95.0, 1.0, 2.0, 0.7298930664276032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1020066.420432293, 1020066.420432294, 227430.54580256], 
processed observation next is [1.0, 0.2608695652173913, 0.4478672985781992, 0.95, 1.0, 1.0, 0.6745699595513291, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2833517834534147, 0.283351783453415, 0.33944857582471644], 
reward next is 0.6606, 
noisyNet noise sample is [array([0.7597729], dtype=float32), 0.07725465]. 
=============================================
[2019-03-27 06:53:12,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[68.97648 ]
 [69.13224 ]
 [69.493866]
 [69.7118  ]
 [70.014656]], R is [[68.88241577]
 [68.86232758]
 [68.84029388]
 [68.82906342]
 [68.79749298]].
[2019-03-27 06:53:13,160] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1129731: loss -409.6995
[2019-03-27 06:53:13,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1129731: learning rate 0.0000
[2019-03-27 06:53:13,381] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1129833: loss -199.1227
[2019-03-27 06:53:13,384] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1129833: learning rate 0.0000
[2019-03-27 06:53:13,888] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1130066: loss -360.2460
[2019-03-27 06:53:13,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1130067: learning rate 0.0000
[2019-03-27 06:53:13,916] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1130077: loss -147.1228
[2019-03-27 06:53:13,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1130077: learning rate 0.0000
[2019-03-27 06:53:13,934] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1130083: loss -106.5482
[2019-03-27 06:53:13,937] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1130083: learning rate 0.0000
[2019-03-27 06:53:14,239] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1130228: loss -562.1302
[2019-03-27 06:53:14,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1130228: learning rate 0.0000
[2019-03-27 06:53:14,452] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1130327: loss -73.7548
[2019-03-27 06:53:14,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1130328: learning rate 0.0000
[2019-03-27 06:53:14,557] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1130378: loss -57.4921
[2019-03-27 06:53:14,560] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1130378: learning rate 0.0000
[2019-03-27 06:53:14,574] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1130387: loss -531.3406
[2019-03-27 06:53:14,576] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1130387: learning rate 0.0000
[2019-03-27 06:53:14,599] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1130396: loss -151.1431
[2019-03-27 06:53:14,600] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1130397: learning rate 0.0000
[2019-03-27 06:53:14,963] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1130567: loss -203.0773
[2019-03-27 06:53:14,964] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1130568: learning rate 0.0000
[2019-03-27 06:53:15,120] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1130643: loss 0.1931
[2019-03-27 06:53:15,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1130646: learning rate 0.0000
[2019-03-27 06:53:18,893] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1132406: loss 0.1530
[2019-03-27 06:53:18,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1132407: learning rate 0.0000
[2019-03-27 06:53:18,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6649102e-17 1.0000000e+00 1.8781011e-21 5.2825969e-14 1.6280873e-26], sum to 1.0000
[2019-03-27 06:53:18,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9477
[2019-03-27 06:53:18,946] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 82.33333333333334, 1.0, 2.0, 0.5016134653787779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700927.6270045955, 700927.6270045955, 183849.2239025121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2643600.0000, 
sim time next is 2644200.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.4980252465494651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695912.0009869435, 695912.0009869428, 183287.144818409], 
processed observation next is [0.0, 0.6086956521739131, 0.4786729857819906, 0.815, 1.0, 1.0, 0.3952111404210423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19330888916303987, 0.19330888916303968, 0.27356290271404327], 
reward next is 0.7264, 
noisyNet noise sample is [array([1.1930225], dtype=float32), 0.306019]. 
=============================================
[2019-03-27 06:53:20,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.1418561e-16 1.0000000e+00 2.0409503e-18 3.1739550e-10 5.7310454e-25], sum to 1.0000
[2019-03-27 06:53:20,524] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7701
[2019-03-27 06:53:20,527] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.00000000000001, 1.0, 2.0, 0.4893401349047127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683772.0167078648, 683772.0167078648, 181942.5830266677], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3183000.0000, 
sim time next is 3183600.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4872632814982477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680869.0270612424, 680869.0270612424, 181624.5002603318], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.94, 1.0, 1.0, 0.3822449174677683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18913028529478956, 0.18913028529478956, 0.27108134367213704], 
reward next is 0.7289, 
noisyNet noise sample is [array([-0.78335905], dtype=float32), 0.54300666]. 
=============================================
[2019-03-27 06:53:22,815] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1134248: loss 0.0172
[2019-03-27 06:53:22,817] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1134248: learning rate 0.0000
[2019-03-27 06:53:26,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.7829463e-15 1.0000000e+00 7.9951292e-19 2.9722971e-10 1.7129136e-24], sum to 1.0000
[2019-03-27 06:53:26,791] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8568
[2019-03-27 06:53:26,797] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 84.0, 1.0, 2.0, 0.7553082718076954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1107020.553110888, 1107020.553110888, 240178.0434369872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2814600.0000, 
sim time next is 2815200.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.6960582949264096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1019228.509947903, 1019228.509947904, 226111.2174311034], 
processed observation next is [1.0, 0.6086956521739131, 0.38388625592417064, 0.83, 1.0, 1.0, 0.633805174610132, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28311903054108417, 0.28311903054108445, 0.33747942900164685], 
reward next is 0.6625, 
noisyNet noise sample is [array([-0.10429623], dtype=float32), -0.3980909]. 
=============================================
[2019-03-27 06:53:29,065] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1137169: loss 0.0245
[2019-03-27 06:53:29,067] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1137169: learning rate 0.0000
[2019-03-27 06:53:29,126] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1137198: loss 0.0237
[2019-03-27 06:53:29,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1137199: learning rate 0.0000
[2019-03-27 06:53:30,237] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1137718: loss 0.0202
[2019-03-27 06:53:30,239] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1137718: learning rate 0.0000
[2019-03-27 06:53:30,428] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1137804: loss 0.0139
[2019-03-27 06:53:30,431] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1137804: learning rate 0.0000
[2019-03-27 06:53:30,896] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1138028: loss 0.0161
[2019-03-27 06:53:30,899] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1138029: learning rate 0.0000
[2019-03-27 06:53:30,966] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1138058: loss 0.0166
[2019-03-27 06:53:30,969] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1138058: learning rate 0.0000
[2019-03-27 06:53:31,019] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1138084: loss 0.0144
[2019-03-27 06:53:31,021] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1138085: learning rate 0.0000
[2019-03-27 06:53:31,232] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1138186: loss 0.0133
[2019-03-27 06:53:31,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1138186: learning rate 0.0000
[2019-03-27 06:53:31,647] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1138374: loss 0.0123
[2019-03-27 06:53:31,651] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1138376: learning rate 0.0000
[2019-03-27 06:53:31,682] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1138397: loss 0.0125
[2019-03-27 06:53:31,690] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1138399: learning rate 0.0000
[2019-03-27 06:53:31,693] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1138400: loss 0.0132
[2019-03-27 06:53:31,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1138400: learning rate 0.0000
[2019-03-27 06:53:31,717] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1138413: loss 0.0133
[2019-03-27 06:53:31,718] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1138413: learning rate 0.0000
[2019-03-27 06:53:32,132] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1138602: loss 0.0132
[2019-03-27 06:53:32,134] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1138602: learning rate 0.0000
[2019-03-27 06:53:32,416] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1138743: loss 1.8850
[2019-03-27 06:53:32,420] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1138744: learning rate 0.0000
[2019-03-27 06:53:34,145] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2673341e-15 1.0000000e+00 1.0654505e-20 8.7539559e-12 1.0156932e-23], sum to 1.0000
[2019-03-27 06:53:34,157] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9679
[2019-03-27 06:53:34,161] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.6698743651496316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1031683.842355097, 1031683.842355097, 226323.6684308749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2912400.0000, 
sim time next is 2913000.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.4163884446079314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 641823.9082245174, 641823.9082245168, 178850.6932124772], 
processed observation next is [1.0, 0.7391304347826086, 0.23380726698262277, 0.95, 1.0, 1.0, 0.2968535477203993, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17828441895125482, 0.17828441895125466, 0.26694133315295104], 
reward next is 0.7331, 
noisyNet noise sample is [array([-1.12842], dtype=float32), -0.54790866]. 
=============================================
[2019-03-27 06:53:34,173] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.71175 ]
 [72.73214 ]
 [72.54542 ]
 [72.29804 ]
 [72.213394]], R is [[73.11512756]
 [73.04618073]
 [73.0056076 ]
 [72.95783997]
 [72.88028717]].
[2019-03-27 06:53:36,165] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1140506: loss 2.0309
[2019-03-27 06:53:36,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1140507: learning rate 0.0000
[2019-03-27 06:53:39,263] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1514151e-14 1.0000000e+00 1.9468476e-20 3.2834662e-12 4.1830351e-24], sum to 1.0000
[2019-03-27 06:53:39,271] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0977
[2019-03-27 06:53:39,279] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5609949487648926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 886911.2213742542, 886911.2213742542, 205547.0908245246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2989800.0000, 
sim time next is 2990400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5570100737043119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 880588.1501533274, 880588.1501533274, 204759.978187945], 
processed observation next is [1.0, 0.6086956521739131, 0.19431279620853087, 0.94, 1.0, 1.0, 0.46627719723411065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2446078194870354, 0.2446078194870354, 0.3056119077432015], 
reward next is 0.6944, 
noisyNet noise sample is [array([-0.9787958], dtype=float32), -0.34199458]. 
=============================================
[2019-03-27 06:53:40,004] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1142273: loss 0.1226
[2019-03-27 06:53:40,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1142274: learning rate 0.0000
[2019-03-27 06:53:40,469] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.07586743e-16 1.00000000e+00 2.99142437e-21 2.86416613e-10
 1.92900989e-27], sum to 1.0000
[2019-03-27 06:53:40,478] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7054
[2019-03-27 06:53:40,482] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 77.0, 1.0, 2.0, 0.5552167309940984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775857.2964768948, 775857.2964768953, 192699.7982324186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3526200.0000, 
sim time next is 3526800.0000, 
raw observation next is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.5547762969872104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775241.6114318797, 775241.611431879, 192623.499144762], 
processed observation next is [1.0, 0.8260869565217391, 0.5892575039494474, 0.7766666666666667, 1.0, 1.0, 0.4635858999845909, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21534489206441101, 0.21534489206441082, 0.2874977599175552], 
reward next is 0.7125, 
noisyNet noise sample is [array([-0.9781257], dtype=float32), 1.4997915]. 
=============================================
[2019-03-27 06:53:43,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8420245e-15 1.0000000e+00 3.5841780e-19 5.0534615e-11 3.1392055e-24], sum to 1.0000
[2019-03-27 06:53:43,385] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9349
[2019-03-27 06:53:43,395] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333333, 95.0, 1.0, 2.0, 0.8422351893929869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1212111.288152422, 1212111.288152422, 259527.3534516696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3081000.0000, 
sim time next is 3081600.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.8214801701034867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1180295.802366947, 1180295.802366947, 253793.5278438263], 
processed observation next is [1.0, 0.6956521739130435, 0.3364928909952607, 0.94, 1.0, 1.0, 0.7849158675945623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32785994510192973, 0.32785994510192973, 0.37879631021466614], 
reward next is 0.6212, 
noisyNet noise sample is [array([0.7441665], dtype=float32), 0.28911817]. 
=============================================
[2019-03-27 06:53:45,865] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1145156: loss 0.1240
[2019-03-27 06:53:45,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1145157: learning rate 0.0000
[2019-03-27 06:53:45,900] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1145174: loss 0.1247
[2019-03-27 06:53:45,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1145174: learning rate 0.0000
[2019-03-27 06:53:47,037] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1145685: loss 0.1414
[2019-03-27 06:53:47,040] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1145685: learning rate 0.0000
[2019-03-27 06:53:47,219] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1145766: loss 0.1455
[2019-03-27 06:53:47,221] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1145767: learning rate 0.0000
[2019-03-27 06:53:47,541] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1196758e-16 1.0000000e+00 5.7096121e-19 3.2211699e-11 8.9010288e-26], sum to 1.0000
[2019-03-27 06:53:47,551] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1560
[2019-03-27 06:53:47,556] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6238818424582347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 871848.9869346417, 871848.9869346417, 205269.7875840174], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3650400.0000, 
sim time next is 3651000.0000, 
raw observation next is [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.6891340640324746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 963077.6037385333, 963077.6037385333, 218513.9988579413], 
processed observation next is [1.0, 0.2608695652173913, 0.4865718799368086, 0.7816666666666667, 1.0, 1.0, 0.6254627277499694, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26752155659403704, 0.26752155659403704, 0.32614029680289747], 
reward next is 0.6739, 
noisyNet noise sample is [array([0.77421546], dtype=float32), -0.58651763]. 
=============================================
[2019-03-27 06:53:47,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.89739 ]
 [71.04328 ]
 [71.315994]
 [71.526054]
 [71.52275 ]], R is [[70.52883911]
 [70.5171814 ]
 [70.50669098]
 [70.49819183]
 [70.49299622]].
[2019-03-27 06:53:47,688] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1145974: loss 0.1353
[2019-03-27 06:53:47,690] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1145974: learning rate 0.0000
[2019-03-27 06:53:47,787] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1146023: loss 0.1443
[2019-03-27 06:53:47,788] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1146023: loss 0.1424
[2019-03-27 06:53:47,790] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1146024: learning rate 0.0000
[2019-03-27 06:53:47,791] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1146025: learning rate 0.0000
[2019-03-27 06:53:47,975] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1146107: loss 0.1465
[2019-03-27 06:53:47,982] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1146109: learning rate 0.0000
[2019-03-27 06:53:48,394] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1146291: loss 0.1502
[2019-03-27 06:53:48,397] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1146292: learning rate 0.0000
[2019-03-27 06:53:48,458] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1146323: loss 0.1527
[2019-03-27 06:53:48,463] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1146325: learning rate 0.0000
[2019-03-27 06:53:48,500] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1146343: loss 0.1496
[2019-03-27 06:53:48,503] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1146343: learning rate 0.0000
[2019-03-27 06:53:48,519] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1146351: loss 0.1498
[2019-03-27 06:53:48,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1146353: learning rate 0.0000
[2019-03-27 06:53:48,897] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1146521: loss 0.1523
[2019-03-27 06:53:48,900] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1146521: learning rate 0.0000
[2019-03-27 06:53:49,507] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.1192032e-15 1.0000000e+00 5.8763079e-20 1.2867684e-12 1.2941495e-24], sum to 1.0000
[2019-03-27 06:53:49,519] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5871
[2019-03-27 06:53:49,524] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4564397554793406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646267.9345350269, 646267.9345350263, 178147.4588664022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3216000.0000, 
sim time next is 3216600.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4566762021398829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646602.4204941794, 646602.4204941794, 178181.9071402132], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3453930146263649, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17961178347060539, 0.17961178347060539, 0.26594314498539284], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.7596174], dtype=float32), 0.038086303]. 
=============================================
[2019-03-27 06:53:49,840] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1146947: loss 17.2899
[2019-03-27 06:53:49,842] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1146947: learning rate 0.0000
[2019-03-27 06:53:54,068] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1148849: loss 39.6892
[2019-03-27 06:53:54,070] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1148849: learning rate 0.0000
[2019-03-27 06:53:56,658] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 06:53:56,663] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:53:56,664] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:53:56,664] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:53:56,665] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:53:56,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:53:56,666] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:53:56,667] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:53:56,668] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:53:56,669] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:53:56,671] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:53:56,688] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run47
[2019-03-27 06:53:56,710] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run47
[2019-03-27 06:53:56,726] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run47
[2019-03-27 06:53:56,727] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run47
[2019-03-27 06:53:56,776] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run47
[2019-03-27 06:54:00,270] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02849117], dtype=float32), 0.04290264]
[2019-03-27 06:54:00,270] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.93333333333333, 95.33333333333333, 1.0, 2.0, 0.2837356703576741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 457444.3041101692, 457444.3041101685, 164099.8781361738]
[2019-03-27 06:54:00,272] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:54:00,275] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.7462406e-16 1.0000000e+00 1.8495209e-20 1.0469931e-13 1.6164670e-24], sampled 0.37382624943037157
[2019-03-27 06:54:04,858] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02849117], dtype=float32), 0.04290264]
[2019-03-27 06:54:04,860] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.236658955, 46.65524139999999, 1.0, 2.0, 0.2910110408899097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483259.7872953769, 483259.7872953775, 164982.312273893]
[2019-03-27 06:54:04,862] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:54:04,865] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1633624e-15 1.0000000e+00 5.7249196e-20 5.2212347e-13 4.1487256e-24], sampled 0.049927642217970525
[2019-03-27 06:54:56,430] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02849117], dtype=float32), 0.04290264]
[2019-03-27 06:54:56,430] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.46414497333333, 71.83772711, 1.0, 2.0, 0.5563119272080493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777388.2786470061, 777388.2786470054, 192889.5956517464]
[2019-03-27 06:54:56,432] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:54:56,435] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.4647424e-16 1.0000000e+00 5.9083807e-21 4.2758605e-12 5.9234179e-26], sampled 0.14264177646162224
[2019-03-27 06:55:06,149] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.02849117], dtype=float32), 0.04290264]
[2019-03-27 06:55:06,151] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.38103824666667, 80.27954042333333, 1.0, 2.0, 0.4854968554500014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684551.940865591, 684551.940865591, 182145.8699489474]
[2019-03-27 06:55:06,151] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:55:06,153] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.5214717e-16 1.0000000e+00 1.5983693e-20 4.0329359e-12 2.4361750e-25], sampled 0.03959262383378892
[2019-03-27 06:55:27,357] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02849117], dtype=float32), 0.04290264]
[2019-03-27 06:55:27,358] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.03333333333333, 90.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.704956406250805, 6.9112, 168.9085538185866, 2017244.089977478, 1454140.653988418, 311348.5577100932]
[2019-03-27 06:55:27,361] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 06:55:27,363] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0770117e-11 9.9999988e-01 3.3441939e-15 1.5880948e-07 2.7306333e-19], sampled 0.024806854738242512
[2019-03-27 06:55:27,364] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2017244.089977478 W.
[2019-03-27 06:55:40,243] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02849117], dtype=float32), 0.04290264]
[2019-03-27 06:55:40,245] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.77982208, 74.14975835999999, 1.0, 2.0, 0.3304376833724401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 528699.8144053443, 528699.8144053443, 169279.0491575624]
[2019-03-27 06:55:40,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:55:40,248] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2637858e-15 1.0000000e+00 3.1786645e-20 4.7745976e-13 1.6283622e-24], sampled 0.09269878858632408
[2019-03-27 06:55:49,193] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.4001 2842001219.2881 1123.0000
[2019-03-27 06:55:49,229] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7899.4886 3162805044.4733 1730.0000
[2019-03-27 06:55:49,262] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5606 2927361377.0479 1336.0000
[2019-03-27 06:55:49,316] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.9954 2779091696.9687 932.0000
[2019-03-27 06:55:49,367] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8004.4047 3007019830.3633 1742.0000
[2019-03-27 06:55:50,389] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1150000, evaluation results [1150000.0, 7899.4886268202035, 3162805044.473332, 1730.0, 8253.560582055938, 2927361377.047865, 1336.0, 8660.995443258584, 2779091696.968666, 932.0, 8004.404738131482, 3007019830.363258, 1742.0, 8500.400054269368, 2842001219.2880664, 1123.0]
[2019-03-27 06:55:51,103] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1150339: loss 1.5965
[2019-03-27 06:55:51,104] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1150339: learning rate 0.0000
[2019-03-27 06:55:54,408] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.9204529e-16 1.0000000e+00 1.6589120e-20 4.4606658e-12 1.4335315e-24], sum to 1.0000
[2019-03-27 06:55:54,422] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4814
[2019-03-27 06:55:54,427] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 81.5, 1.0, 2.0, 0.5371186532427136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750558.1911082439, 750558.1911082446, 189615.812435779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3357000.0000, 
sim time next is 3357600.0000, 
raw observation next is [28.0, 80.66666666666667, 1.0, 2.0, 0.532547906146615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744168.8889322068, 744168.8889322075, 188852.4513456892], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8066666666666668, 1.0, 1.0, 0.4368047062007409, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20671358025894632, 0.2067135802589465, 0.28186933036670025], 
reward next is 0.7181, 
noisyNet noise sample is [array([-2.0435014], dtype=float32), 0.15666795]. 
=============================================
[2019-03-27 06:55:57,099] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1153140: loss 1.3622
[2019-03-27 06:55:57,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1153141: learning rate 0.0000
[2019-03-27 06:55:57,238] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1153207: loss 1.3148
[2019-03-27 06:55:57,243] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1153209: learning rate 0.0000
[2019-03-27 06:55:58,339] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1153725: loss 1.3753
[2019-03-27 06:55:58,341] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1153725: learning rate 0.0000
[2019-03-27 06:55:58,403] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1153757: loss 1.4122
[2019-03-27 06:55:58,405] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1153758: learning rate 0.0000
[2019-03-27 06:55:58,834] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1153938: loss 1.4148
[2019-03-27 06:55:58,836] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1153939: learning rate 0.0000
[2019-03-27 06:55:58,989] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1154010: loss 1.3942
[2019-03-27 06:55:58,992] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1154010: learning rate 0.0000
[2019-03-27 06:55:58,993] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1154010: loss 1.3981
[2019-03-27 06:55:58,996] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1154010: learning rate 0.0000
[2019-03-27 06:55:59,029] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1154029: loss 1.4020
[2019-03-27 06:55:59,036] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1154031: learning rate 0.0000
[2019-03-27 06:55:59,516] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2896943e-08 6.3084900e-01 2.3753441e-10 3.6915094e-01 7.4716005e-15], sum to 1.0000
[2019-03-27 06:55:59,526] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7492
[2019-03-27 06:55:59,532] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.83333333333334, 66.0, 1.0, 2.0, 0.8054952427443751, 1.0, 2.0, 0.8054952427443751, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2252727.154685278, 2252727.154685278, 422551.9293770266], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3495000.0000, 
sim time next is 3495600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8181476334803165, 1.0, 2.0, 0.8181476334803165, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2288144.466922634, 2288144.466922634, 428800.1839111891], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.780900763229297, 1.0, 1.0, 0.780900763229297, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6355956852562872, 0.6355956852562872, 0.6400002744943121], 
reward next is 0.3600, 
noisyNet noise sample is [array([0.552964], dtype=float32), 0.56058806]. 
=============================================
[2019-03-27 06:55:59,571] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1154282: loss 1.3823
[2019-03-27 06:55:59,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1154284: learning rate 0.0000
[2019-03-27 06:55:59,584] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1154287: loss 1.3674
[2019-03-27 06:55:59,586] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1154287: learning rate 0.0000
[2019-03-27 06:55:59,619] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1154303: loss 1.3666
[2019-03-27 06:55:59,623] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1154304: learning rate 0.0000
[2019-03-27 06:55:59,740] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1154360: loss 1.4065
[2019-03-27 06:55:59,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1154361: learning rate 0.0000
[2019-03-27 06:56:00,028] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1154492: loss 1.3384
[2019-03-27 06:56:00,032] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1154493: learning rate 0.0000
[2019-03-27 06:56:00,725] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.8810192e-14 9.9999976e-01 5.8074308e-18 2.1930339e-07 4.7107990e-23], sum to 1.0000
[2019-03-27 06:56:00,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8731
[2019-03-27 06:56:00,737] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5455810744206206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762387.6423113113, 762387.6423113113, 191045.8341374866], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4048200.0000, 
sim time next is 4048800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5445282443353512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760915.9047814367, 760915.9047814367, 190866.8922560311], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45123884859680863, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21136552910595463, 0.21136552910595463, 0.2848759585910912], 
reward next is 0.7151, 
noisyNet noise sample is [array([0.27563083], dtype=float32), -0.3840968]. 
=============================================
[2019-03-27 06:56:01,115] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1155000: loss 0.0789
[2019-03-27 06:56:01,116] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1155000: learning rate 0.0000
[2019-03-27 06:56:04,950] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1156802: loss 0.0250
[2019-03-27 06:56:04,952] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1156802: learning rate 0.0000
[2019-03-27 06:56:08,433] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1158436: loss 18.9461
[2019-03-27 06:56:08,436] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1158438: learning rate 0.0000
[2019-03-27 06:56:08,968] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6159451e-14 1.0000000e+00 5.4015059e-19 1.1800728e-09 3.8223770e-25], sum to 1.0000
[2019-03-27 06:56:08,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8618
[2019-03-27 06:56:08,981] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.5239995308192782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732219.4897511872, 732219.4897511872, 187441.802578741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3615000.0000, 
sim time next is 3615600.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.5234008158471993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731382.5773714219, 731382.5773714213, 187343.8294250153], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.74, 1.0, 1.0, 0.4257841154785534, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20316182704761718, 0.203161827047617, 0.2796176558582318], 
reward next is 0.7204, 
noisyNet noise sample is [array([-0.61458176], dtype=float32), -1.461614]. 
=============================================
[2019-03-27 06:56:09,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9087453e-11 2.3072448e-02 6.5927522e-14 9.7692752e-01 9.7446161e-20], sum to 1.0000
[2019-03-27 06:56:09,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3999
[2019-03-27 06:56:09,950] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 50.0, 1.0, 2.0, 0.9836026564267562, 1.0, 2.0, 0.9836026564267562, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2751388.264747042, 2751388.264747043, 519156.880435914], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4208400.0000, 
sim time next is 4209000.0000, 
raw observation next is [35.83333333333334, 51.0, 1.0, 2.0, 0.4264874579818426, 1.0, 2.0, 0.4264874579818426, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1192170.304761771, 1192170.304761771, 279016.6664306874], 
processed observation next is [1.0, 0.7391304347826086, 0.8973143759873622, 0.51, 1.0, 1.0, 0.3090210337130634, 1.0, 1.0, 0.3090210337130634, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.33115841798938084, 0.33115841798938084, 0.4164427857174439], 
reward next is 0.5836, 
noisyNet noise sample is [array([-1.416141], dtype=float32), 0.33692417]. 
=============================================
[2019-03-27 06:56:09,959] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[51.365284]
 [50.367214]
 [49.151882]
 [47.34938 ]
 [45.02505 ]], R is [[55.75519943]
 [55.42278671]
 [55.11341858]
 [54.79634094]
 [54.45494843]].
[2019-03-27 06:56:11,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4339134e-11 9.9990129e-01 1.3325974e-13 9.8678000e-05 1.1321022e-17], sum to 1.0000
[2019-03-27 06:56:11,870] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6509
[2019-03-27 06:56:11,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1694841.758147642 W.
[2019-03-27 06:56:11,881] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 76.33333333333334, 1.0, 2.0, 0.4041141845704221, 1.0, 2.0, 0.4041141845704221, 1.0, 1.0, 0.7018126959958071, 6.911199999999999, 6.9112, 170.5573041426782, 1694841.758147642, 1694841.758147642, 355177.8904259392], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4243200.0000, 
sim time next is 4243800.0000, 
raw observation next is [30.0, 75.66666666666666, 1.0, 2.0, 0.5693087991499329, 1.0, 2.0, 0.5693087991499329, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1591698.529239972, 1591698.529239972, 323198.5026892696], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.7566666666666666, 1.0, 1.0, 0.4810949387348589, 1.0, 1.0, 0.4810949387348589, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.44213848034443665, 0.44213848034443665, 0.4823858249093576], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4682509], dtype=float32), 0.1275555]. 
=============================================
[2019-03-27 06:56:14,210] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1161126: loss 37.9664
[2019-03-27 06:56:14,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1161126: learning rate 0.0000
[2019-03-27 06:56:14,390] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1161208: loss 16.5123
[2019-03-27 06:56:14,391] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1161208: learning rate 0.0000
[2019-03-27 06:56:15,465] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1161711: loss -102.0789
[2019-03-27 06:56:15,468] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1161712: learning rate 0.0000
[2019-03-27 06:56:15,594] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1161771: loss 37.1032
[2019-03-27 06:56:15,598] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1161772: learning rate 0.0000
[2019-03-27 06:56:15,912] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1161920: loss 61.8709
[2019-03-27 06:56:15,913] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1161920: learning rate 0.0000
[2019-03-27 06:56:16,030] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1161977: loss -43.9249
[2019-03-27 06:56:16,036] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1161977: learning rate 0.0000
[2019-03-27 06:56:16,079] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1161997: loss 104.9536
[2019-03-27 06:56:16,082] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1161997: learning rate 0.0000
[2019-03-27 06:56:16,191] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1162047: loss -40.1142
[2019-03-27 06:56:16,199] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1162047: learning rate 0.0000
[2019-03-27 06:56:16,777] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1162326: loss 22.8146
[2019-03-27 06:56:16,779] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1162326: learning rate 0.0000
[2019-03-27 06:56:16,856] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1162364: loss 76.0642
[2019-03-27 06:56:16,858] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1162364: learning rate 0.0000
[2019-03-27 06:56:16,896] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1162384: loss 33.4238
[2019-03-27 06:56:16,900] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1162386: learning rate 0.0000
[2019-03-27 06:56:16,953] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1162406: loss 17.1202
[2019-03-27 06:56:16,955] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1162406: learning rate 0.0000
[2019-03-27 06:56:17,030] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1162442: loss 64.3594
[2019-03-27 06:56:17,032] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1162442: learning rate 0.0000
[2019-03-27 06:56:18,465] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1163114: loss 119.7769
[2019-03-27 06:56:18,466] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1163114: learning rate 0.0000
[2019-03-27 06:56:20,908] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3725419e-15 1.0000000e+00 7.8208596e-20 7.3510850e-12 3.7340045e-24], sum to 1.0000
[2019-03-27 06:56:20,915] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5737
[2019-03-27 06:56:20,921] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 59.33333333333333, 1.0, 2.0, 0.6260128455368718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874828.201770172, 874828.201770172, 205692.8945068055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3852600.0000, 
sim time next is 3853200.0000, 
raw observation next is [35.0, 58.66666666666667, 1.0, 2.0, 0.6194142991590283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865603.2364676001, 865603.2364676001, 204419.2682650468], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5866666666666667, 1.0, 1.0, 0.5414630110349739, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24044534346322224, 0.24044534346322224, 0.3051033854702191], 
reward next is 0.6949, 
noisyNet noise sample is [array([-0.4423374], dtype=float32), -0.05104368]. 
=============================================
[2019-03-27 06:56:22,288] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1164895: loss 111.4425
[2019-03-27 06:56:22,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1164896: learning rate 0.0000
[2019-03-27 06:56:22,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5081620e-16 1.0000000e+00 1.7395721e-21 2.9871407e-11 3.1942532e-24], sum to 1.0000
[2019-03-27 06:56:22,524] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9547
[2019-03-27 06:56:22,531] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 77.5, 1.0, 2.0, 0.5483971457951963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766324.2020067868, 766324.2020067868, 191526.43717965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3880200.0000, 
sim time next is 3880800.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5518252422133616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771116.3254456399, 771116.3254456405, 192114.4235088931], 
processed observation next is [0.0, 0.9565217391304348, 0.5734597156398105, 0.79, 1.0, 1.0, 0.46003041230525493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21419897929045553, 0.2141989792904557, 0.28673794553566134], 
reward next is 0.7133, 
noisyNet noise sample is [array([0.6557235], dtype=float32), 1.3487535]. 
=============================================
[2019-03-27 06:56:23,157] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5613832e-16 1.0000000e+00 2.6815364e-21 5.3307810e-12 3.9639401e-24], sum to 1.0000
[2019-03-27 06:56:23,167] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6470
[2019-03-27 06:56:23,176] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 90.0, 1.0, 2.0, 0.5794924777087481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 809793.0440974713, 809793.0440974719, 196986.9945224427], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3892800.0000, 
sim time next is 3893400.0000, 
raw observation next is [27.75, 90.5, 1.0, 2.0, 0.5781903139495733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 807972.6853141182, 807972.6853141176, 196752.6146647437], 
processed observation next is [0.0, 0.043478260869565216, 0.514218009478673, 0.905, 1.0, 1.0, 0.4917955589753895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2244368570316995, 0.22443685703169933, 0.29366061890260253], 
reward next is 0.7063, 
noisyNet noise sample is [array([1.3606695], dtype=float32), -0.5794338]. 
=============================================
[2019-03-27 06:56:23,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5243620e-16 1.0000000e+00 4.6355977e-20 4.7458188e-12 1.5911147e-24], sum to 1.0000
[2019-03-27 06:56:23,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2420
[2019-03-27 06:56:23,422] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5780382386054245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807760.0919121156, 807760.0919121163, 196726.0384655843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3884400.0000, 
sim time next is 3885000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5821277182919646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813476.9861038653, 813476.9861038653, 197463.5189598995], 
processed observation next is [0.0, 1.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49653941962887305, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2259658294732959, 0.2259658294732959, 0.2947216700894022], 
reward next is 0.7053, 
noisyNet noise sample is [array([-0.01914798], dtype=float32), -0.66810995]. 
=============================================
[2019-03-27 06:56:23,437] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.42585 ]
 [73.394875]
 [73.41529 ]
 [73.42875 ]
 [73.43444 ]], R is [[73.39910126]
 [73.37149048]
 [73.34546661]
 [73.320961  ]
 [73.29777527]].
[2019-03-27 06:56:25,574] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1166434: loss 0.0126
[2019-03-27 06:56:25,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1166434: learning rate 0.0000
[2019-03-27 06:56:29,987] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0249026e-12 9.9999893e-01 4.8364904e-16 1.0210424e-06 2.7427689e-21], sum to 1.0000
[2019-03-27 06:56:29,995] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1532
[2019-03-27 06:56:30,000] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 84.0, 1.0, 2.0, 0.5852251304814248, 0.0, 1.0, 0.0, 1.0, 1.0, 1.016342514738167, 6.911200000000001, 6.9112, 168.912956323002, 1636244.237088901, 1636244.237088901, 358283.5409244243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4001400.0000, 
sim time next is 4002000.0000, 
raw observation next is [29.33333333333334, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.341896745730021, 6.9112, 168.91037755512, 1759510.630162348, 1453964.200443667, 311354.7405808669], 
processed observation next is [1.0, 0.30434782608695654, 0.5892575039494474, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.043069674573002106, 0.0, 0.8294272813010748, 0.48875295282287445, 0.40387894456768525, 0.4647085680311447], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03145523], dtype=float32), 0.9812125]. 
=============================================
[2019-03-27 06:56:30,013] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[59.356064]
 [63.558304]
 [67.49881 ]
 [67.90321 ]
 [67.690994]], R is [[58.77281952]
 [58.18509293]
 [58.08139801]
 [57.50058365]
 [56.92557907]].
[2019-03-27 06:56:30,878] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1169117: loss 0.0798
[2019-03-27 06:56:30,881] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1169118: learning rate 0.0000
[2019-03-27 06:56:31,063] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1169197: loss 0.0284
[2019-03-27 06:56:31,069] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1169198: learning rate 0.0000
[2019-03-27 06:56:31,832] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2739893e-15 1.0000000e+00 1.6753962e-20 6.3381739e-10 1.1367009e-26], sum to 1.0000
[2019-03-27 06:56:31,849] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1360
[2019-03-27 06:56:31,856] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 79.0, 1.0, 2.0, 0.5587371736765008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780778.5548874289, 780778.5548874289, 193310.5622956924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4042200.0000, 
sim time next is 4042800.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5542029551291414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 774440.13432818, 774440.1343281806, 192524.1516699628], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.79, 1.0, 1.0, 0.46289512666161614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21512225953560554, 0.2151222595356057, 0.2873494801044221], 
reward next is 0.7127, 
noisyNet noise sample is [array([-0.1495849], dtype=float32), 1.0073936]. 
=============================================
[2019-03-27 06:56:32,018] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1169627: loss 0.0222
[2019-03-27 06:56:32,022] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1169628: learning rate 0.0000
[2019-03-27 06:56:32,227] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1169723: loss 0.0417
[2019-03-27 06:56:32,235] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1169725: learning rate 0.0000
[2019-03-27 06:56:32,613] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1169892: loss 0.0140
[2019-03-27 06:56:32,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1169893: learning rate 0.0000
[2019-03-27 06:56:32,739] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1169948: loss 0.0207
[2019-03-27 06:56:32,741] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1169950: learning rate 0.0000
[2019-03-27 06:56:32,931] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1170035: loss 0.0323
[2019-03-27 06:56:32,934] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1170036: learning rate 0.0000
[2019-03-27 06:56:32,955] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1170048: loss 0.0177
[2019-03-27 06:56:32,958] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1170048: learning rate 0.0000
[2019-03-27 06:56:33,032] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5418657e-17 1.0000000e+00 6.1042666e-20 1.4663250e-11 5.4900861e-26], sum to 1.0000
[2019-03-27 06:56:33,042] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7787
[2019-03-27 06:56:33,047] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5411218965421045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756154.2293244835, 756154.2293244842, 190289.4820549435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4057200.0000, 
sim time next is 4057800.0000, 
raw observation next is [27.16666666666666, 88.16666666666667, 1.0, 2.0, 0.5406849000012719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755543.3608614736, 755543.3608614736, 190215.8729619542], 
processed observation next is [1.0, 1.0, 0.4865718799368086, 0.8816666666666667, 1.0, 1.0, 0.4466083132545444, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20987315579485377, 0.20987315579485377, 0.2839042880029167], 
reward next is 0.7161, 
noisyNet noise sample is [array([-1.8520541], dtype=float32), 0.68437505]. 
=============================================
[2019-03-27 06:56:33,601] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1170331: loss 0.0218
[2019-03-27 06:56:33,603] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1170331: learning rate 0.0000
[2019-03-27 06:56:33,724] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1170385: loss 0.0216
[2019-03-27 06:56:33,725] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1170385: loss 0.0367
[2019-03-27 06:56:33,732] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1170386: learning rate 0.0000
[2019-03-27 06:56:33,734] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1170386: learning rate 0.0000
[2019-03-27 06:56:33,775] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1170408: loss 0.0204
[2019-03-27 06:56:33,778] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1170409: learning rate 0.0000
[2019-03-27 06:56:33,848] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1170439: loss 0.0187
[2019-03-27 06:56:33,852] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1170440: learning rate 0.0000
[2019-03-27 06:56:35,269] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1171085: loss 0.4004
[2019-03-27 06:56:35,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1171086: learning rate 0.0000
[2019-03-27 06:56:39,051] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1172791: loss 0.2938
[2019-03-27 06:56:39,051] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1172791: learning rate 0.0000
[2019-03-27 06:56:42,972] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1174520: loss 40.2997
[2019-03-27 06:56:42,974] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1174520: learning rate 0.0000
[2019-03-27 06:56:43,727] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.9081731e-14 1.0000000e+00 1.8287354e-18 2.1182909e-09 3.4231332e-23], sum to 1.0000
[2019-03-27 06:56:43,739] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1813
[2019-03-27 06:56:43,749] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.5980822105192695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835780.8863707257, 835780.8863707257, 200389.3920534862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4237200.0000, 
sim time next is 4237800.0000, 
raw observation next is [30.83333333333334, 75.66666666666667, 1.0, 2.0, 0.598659206729781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 836587.5184940917, 836587.5184940911, 200496.4403018882], 
processed observation next is [1.0, 0.043478260869565216, 0.6603475513428123, 0.7566666666666667, 1.0, 1.0, 0.5164568755780494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23238542180391436, 0.2323854218039142, 0.29924841836102717], 
reward next is 0.7008, 
noisyNet noise sample is [array([-1.175072], dtype=float32), -0.9199861]. 
=============================================
[2019-03-27 06:56:44,037] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 06:56:44,040] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:56:44,042] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:56:44,042] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:56:44,044] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:56:44,045] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:56:44,045] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:56:44,048] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:56:44,049] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:56:44,047] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:56:44,054] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:56:44,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run48
[2019-03-27 06:56:44,070] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run48
[2019-03-27 06:56:44,110] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run48
[2019-03-27 06:56:44,131] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run48
[2019-03-27 06:56:44,148] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run48
[2019-03-27 06:57:09,101] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:57:09,103] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.26666666666667, 95.0, 1.0, 2.0, 0.4197293278211236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617601.6462112984, 617601.646211299, 175947.8249921991]
[2019-03-27 06:57:09,105] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:57:09,108] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.6622827e-16 1.0000000e+00 2.1934718e-20 1.5600517e-12 1.1379950e-24], sampled 0.5773139105023795
[2019-03-27 06:57:15,844] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:57:15,844] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.55565151, 77.9040713, 1.0, 2.0, 0.457581005472382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664099.4807713788, 664099.4807713783, 180361.466902792]
[2019-03-27 06:57:15,846] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:57:15,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.7486839e-16 1.0000000e+00 1.2444695e-20 1.7466818e-12 5.3554081e-25], sampled 0.5486407783258564
[2019-03-27 06:57:15,859] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:57:15,861] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.28235171, 91.76474275, 1.0, 2.0, 0.496903203811326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694343.610212161, 694343.610212161, 183111.6771406243]
[2019-03-27 06:57:15,862] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:57:15,865] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3626159e-15 1.0000000e+00 3.0231222e-20 4.5772929e-13 3.2725609e-24], sampled 0.3057841402273199
[2019-03-27 06:57:48,088] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:57:48,088] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.5, 46.5, 1.0, 2.0, 0.5153352019011183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720108.1412962289, 720108.1412962289, 186033.6281030191]
[2019-03-27 06:57:48,089] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:57:48,091] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.8728271e-15 1.0000000e+00 1.7054135e-19 1.8446721e-10 2.3392132e-24], sampled 0.16682757640835566
[2019-03-27 06:57:52,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:57:52,388] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.66666666666667, 64.33333333333333, 1.0, 2.0, 0.9032727035658686, 1.0, 1.0, 0.9032727035658686, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2526457.641154543, 2526457.641154543, 473290.3103657695]
[2019-03-27 06:57:52,390] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:57:52,392] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0728266e-09 9.3296725e-01 1.9888360e-12 6.7032784e-02 1.2705668e-17], sampled 0.052576070185763824
[2019-03-27 06:57:52,393] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2526457.641154543 W.
[2019-03-27 06:57:58,079] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:57:58,081] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.08333333333334, 84.0, 1.0, 2.0, 0.4821069802189829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673661.6729216995, 673661.6729216989, 180839.6229727611]
[2019-03-27 06:57:58,082] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:57:58,084] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.9660907e-15 1.0000000e+00 8.8574972e-20 1.0586207e-11 5.6660514e-24], sampled 0.34331012783159154
[2019-03-27 06:58:06,070] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:58:06,071] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.21554208666667, 83.39906733666666, 1.0, 2.0, 0.5499675290844052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768519.4323769404, 768519.4323769398, 191795.239006131]
[2019-03-27 06:58:06,073] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:58:06,077] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6954471e-15 1.0000000e+00 3.0659424e-20 9.2156491e-12 1.2285008e-24], sampled 0.05525688188981526
[2019-03-27 06:58:10,622] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:58:10,623] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.2, 63.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.455857899718744, 6.9112, 168.9090222097461, 2670424.224866585, 2284034.165089296, 474935.2350672577]
[2019-03-27 06:58:10,624] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:58:10,627] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.65344571e-09 8.54716301e-01 1.32994536e-11 1.45283669e-01
 1.77451708e-16], sampled 0.9295111659073539
[2019-03-27 06:58:18,348] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:58:18,350] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.07842797833333, 80.341921995, 1.0, 2.0, 0.542838247778129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 758553.4847628123, 758553.4847628116, 190579.0708715663]
[2019-03-27 06:58:18,352] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:58:18,354] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4529168e-15 1.0000000e+00 4.9025777e-20 6.4228683e-12 2.5500538e-24], sampled 0.42516695181441544
[2019-03-27 06:58:27,294] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:58:27,295] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.64405870333334, 79.57616429000001, 1.0, 2.0, 0.4459057857444824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 648000.4898214351, 648000.4898214344, 178735.1333256334]
[2019-03-27 06:58:27,296] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 06:58:27,298] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8222670e-15 1.0000000e+00 5.2095774e-20 5.3579775e-12 4.1388459e-24], sampled 0.22118036239909689
[2019-03-27 06:58:29,181] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03207463], dtype=float32), 0.042392183]
[2019-03-27 06:58:29,184] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.43333333333333, 82.33333333333333, 1.0, 2.0, 0.7022321880173541, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.96075267673421, 6.9112, 168.9126215100817, 1878252.419165092, 1843098.11453902, 386548.999560528]
[2019-03-27 06:58:29,184] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 06:58:29,188] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.96325638e-11 9.99988794e-01 1.34669565e-14 1.11731242e-05
 4.44029519e-19], sampled 0.7093913605969677
[2019-03-27 06:58:29,190] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1878252.419165092 W.
[2019-03-27 06:58:39,801] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9541 2779268993.2573 934.0000
[2019-03-27 06:58:39,823] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4452 2927243129.1758 1336.0000
[2019-03-27 06:58:40,100] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.8225 2842413331.9026 1126.0000
[2019-03-27 06:58:40,160] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7891.2217 3163347419.6995 1754.0000
[2019-03-27 06:58:40,177] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.7041 3007008668.1255 1754.0000
[2019-03-27 06:58:41,196] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1175000, evaluation results [1175000.0, 7891.221689124081, 3163347419.699471, 1754.0, 8254.445167844204, 2927243129.175761, 1336.0, 8659.954077092008, 2779268993.2573085, 934.0, 8000.704070349734, 3007008668.1255007, 1754.0, 8499.822545604096, 2842413331.9026117, 1126.0]
[2019-03-27 06:58:43,560] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0367857e-09 1.3525543e-02 1.7607122e-11 9.8647445e-01 2.1779775e-16], sum to 1.0000
[2019-03-27 06:58:43,566] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9588
[2019-03-27 06:58:43,570] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 60.0, 1.0, 2.0, 0.8753094950998275, 1.0, 2.0, 0.7582447870641765, 1.0, 1.0, 1.03, 7.005111559220119, 6.9112, 170.5573041426782, 3182054.857515066, 3114782.187567707, 582422.2355298423], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4273200.0000, 
sim time next is 4273800.0000, 
raw observation next is [36.16666666666666, 59.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.850315185584618, 6.9112, 170.5573041426782, 3582839.822670004, 2910113.414472398, 548286.5406752978], 
processed observation next is [1.0, 0.4782608695652174, 0.9131121642969979, 0.595, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.0939115185584618, 0.0, 0.8375144448122397, 0.9952332840750011, 0.8083648373534439, 0.8183381204108923], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.17091659], dtype=float32), -0.9711268]. 
=============================================
[2019-03-27 06:58:45,777] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1177149: loss 1.8130
[2019-03-27 06:58:45,781] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1177150: learning rate 0.0000
[2019-03-27 06:58:45,943] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1177228: loss -0.4910
[2019-03-27 06:58:45,947] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1177229: learning rate 0.0000
[2019-03-27 06:58:46,707] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1177581: loss 56.5236
[2019-03-27 06:58:46,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1177583: learning rate 0.0000
[2019-03-27 06:58:47,077] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1177752: loss 161.9673
[2019-03-27 06:58:47,079] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1177754: learning rate 0.0000
[2019-03-27 06:58:47,496] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1177950: loss 146.2949
[2019-03-27 06:58:47,499] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1177950: learning rate 0.0000
[2019-03-27 06:58:47,619] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1177989: loss 149.8841
[2019-03-27 06:58:47,621] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1177989: learning rate 0.0000
[2019-03-27 06:58:47,732] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1178043: loss 167.5015
[2019-03-27 06:58:47,736] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1178043: learning rate 0.0000
[2019-03-27 06:58:47,742] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1178044: loss 98.4130
[2019-03-27 06:58:47,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1178044: learning rate 0.0000
[2019-03-27 06:58:48,289] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1178304: loss 109.6755
[2019-03-27 06:58:48,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1178304: learning rate 0.0000
[2019-03-27 06:58:48,349] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1178329: loss 140.7888
[2019-03-27 06:58:48,351] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1178329: learning rate 0.0000
[2019-03-27 06:58:48,473] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1178391: loss 95.3865
[2019-03-27 06:58:48,474] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1178391: learning rate 0.0000
[2019-03-27 06:58:48,506] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1178405: loss 155.9818
[2019-03-27 06:58:48,509] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1178406: learning rate 0.0000
[2019-03-27 06:58:48,769] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1178527: loss 115.4299
[2019-03-27 06:58:48,773] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1178529: learning rate 0.0000
[2019-03-27 06:58:49,737] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1178976: loss -94.0145
[2019-03-27 06:58:49,740] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1178978: learning rate 0.0000
[2019-03-27 06:58:52,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9685136e-16 1.0000000e+00 5.2634110e-20 2.0460927e-13 1.2511541e-25], sum to 1.0000
[2019-03-27 06:58:52,761] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8126
[2019-03-27 06:58:52,764] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6415212096858719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 896509.6744091983, 896509.6744091978, 208736.5282130332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4443000.0000, 
sim time next is 4443600.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.6357747105557006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 888475.7281487472, 888475.7281487467, 207600.5647921237], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.75, 1.0, 1.0, 0.5611743500671091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.246798813374652, 0.24679881337465184, 0.3098515892419757], 
reward next is 0.6901, 
noisyNet noise sample is [array([1.0664632], dtype=float32), -0.021907076]. 
=============================================
[2019-03-27 06:58:53,741] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1180858: loss -137.8011
[2019-03-27 06:58:53,745] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1180858: learning rate 0.0000
[2019-03-27 06:58:56,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4027052e-16 1.0000000e+00 3.5852018e-20 5.6665884e-14 7.6601855e-25], sum to 1.0000
[2019-03-27 06:58:56,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4317
[2019-03-27 06:58:56,558] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 51.5, 1.0, 2.0, 0.5282437871637812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738152.332860504, 738152.332860504, 188140.407343605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4545000.0000, 
sim time next is 4545600.0000, 
raw observation next is [34.0, 52.0, 1.0, 2.0, 0.5316923754061694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 742972.973956, 742972.9739560005, 188711.4153926059], 
processed observation next is [0.0, 0.6086956521739131, 0.8104265402843602, 0.52, 1.0, 1.0, 0.4357739462724933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20638138165444442, 0.20638138165444458, 0.2816588289441879], 
reward next is 0.7183, 
noisyNet noise sample is [array([1.1736201], dtype=float32), 0.53255814]. 
=============================================
[2019-03-27 06:58:57,180] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1182466: loss 0.1785
[2019-03-27 06:58:57,183] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1182466: learning rate 0.0000
[2019-03-27 06:59:01,568] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2694741e-12 9.9939346e-01 1.8884938e-16 6.0648564e-04 1.6278614e-21], sum to 1.0000
[2019-03-27 06:59:01,577] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1216
[2019-03-27 06:59:01,583] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 70.0, 1.0, 2.0, 0.5202999934834526, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956421987, 727048.1103418493, 727048.1103418493, 186840.5513477334], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4728000.0000, 
sim time next is 4728600.0000, 
raw observation next is [30.5, 70.0, 1.0, 2.0, 0.5069622544597705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104089, 708404.228822898, 708404.228822898, 184697.329598326], 
processed observation next is [1.0, 0.7391304347826086, 0.6445497630331753, 0.7, 1.0, 1.0, 0.40597861983104877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451521941, 0.196778952450805, 0.196778952450805, 0.27566765611690447], 
reward next is 0.7243, 
noisyNet noise sample is [array([0.23701692], dtype=float32), 1.3742731]. 
=============================================
[2019-03-27 06:59:02,876] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1185138: loss 0.7083
[2019-03-27 06:59:02,879] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1185138: learning rate 0.0000
[2019-03-27 06:59:03,001] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1185201: loss 0.4011
[2019-03-27 06:59:03,005] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1185201: learning rate 0.0000
[2019-03-27 06:59:03,862] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1185601: loss 0.2562
[2019-03-27 06:59:03,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1185602: learning rate 0.0000
[2019-03-27 06:59:04,179] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1185749: loss 0.5991
[2019-03-27 06:59:04,181] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1185750: learning rate 0.0000
[2019-03-27 06:59:04,371] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1185838: loss 1.0398
[2019-03-27 06:59:04,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1185838: learning rate 0.0000
[2019-03-27 06:59:04,556] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1185925: loss 0.3155
[2019-03-27 06:59:04,558] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1185925: learning rate 0.0000
[2019-03-27 06:59:04,683] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1185986: loss 0.1608
[2019-03-27 06:59:04,685] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1185986: learning rate 0.0000
[2019-03-27 06:59:04,736] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1186008: loss 0.5427
[2019-03-27 06:59:04,738] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1186008: learning rate 0.0000
[2019-03-27 06:59:05,438] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1186339: loss 0.3798
[2019-03-27 06:59:05,440] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1186339: learning rate 0.0000
[2019-03-27 06:59:05,451] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1186344: loss 0.4681
[2019-03-27 06:59:05,452] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1186344: learning rate 0.0000
[2019-03-27 06:59:05,491] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1186364: loss 0.3111
[2019-03-27 06:59:05,493] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1186364: learning rate 0.0000
[2019-03-27 06:59:05,621] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1186425: loss 0.2611
[2019-03-27 06:59:05,623] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1186425: learning rate 0.0000
[2019-03-27 06:59:05,876] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1186556: loss 0.1665
[2019-03-27 06:59:05,877] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1186556: learning rate 0.0000
[2019-03-27 06:59:06,793] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1186989: loss 13.4676
[2019-03-27 06:59:06,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1186989: learning rate 0.0000
[2019-03-27 06:59:10,932] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1188897: loss 10.5588
[2019-03-27 06:59:10,933] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1188897: learning rate 0.0000
[2019-03-27 06:59:14,451] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1190540: loss -94.2893
[2019-03-27 06:59:14,453] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1190540: learning rate 0.0000
[2019-03-27 06:59:15,955] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.9153898e-15 1.0000000e+00 7.6476790e-20 1.6257787e-11 9.7379136e-24], sum to 1.0000
[2019-03-27 06:59:15,966] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8391
[2019-03-27 06:59:15,971] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4914228084718356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686683.1496406853, 686683.1496406853, 182262.9594596369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4830600.0000, 
sim time next is 4831200.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4912224075202763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 686403.0315551979, 686403.0315551986, 182232.0875610797], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.74, 1.0, 1.0, 0.38701494881961, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19066750876533275, 0.19066750876533295, 0.2719881903896712], 
reward next is 0.7280, 
noisyNet noise sample is [array([-0.5229156], dtype=float32), -0.52390236]. 
=============================================
[2019-03-27 06:59:19,527] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4908246e-08 1.0519414e-02 2.2323698e-11 9.8948061e-01 1.9970758e-15], sum to 1.0000
[2019-03-27 06:59:19,541] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1132
[2019-03-27 06:59:19,547] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.7, 68.0, 1.0, 2.0, 0.9930360629744842, 1.0, 2.0, 0.9930360629744842, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2777805.253673135, 2777805.253673135, 524803.6758470422], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5479200.0000, 
sim time next is 5479800.0000, 
raw observation next is [33.91666666666667, 67.5, 1.0, 2.0, 0.9924943376295464, 1.0, 2.0, 0.9924943376295464, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2776288.209433879, 2776288.209433879, 524479.4167761178], 
processed observation next is [1.0, 0.43478260869565216, 0.8064770932069513, 0.675, 1.0, 1.0, 0.9909570332886102, 1.0, 1.0, 0.9909570332886102, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7711911692871886, 0.7711911692871886, 0.7828050996658474], 
reward next is 0.2172, 
noisyNet noise sample is [array([1.2091478], dtype=float32), -0.24791119]. 
=============================================
[2019-03-27 06:59:19,831] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1193062: loss -69.8920
[2019-03-27 06:59:19,833] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1193062: learning rate 0.0000
[2019-03-27 06:59:20,098] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1193179: loss -71.6254
[2019-03-27 06:59:20,101] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1193179: learning rate 0.0000
[2019-03-27 06:59:20,860] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1193558: loss -90.7526
[2019-03-27 06:59:20,862] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1193558: learning rate 0.0000
[2019-03-27 06:59:21,214] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1193766: loss -123.3707
[2019-03-27 06:59:21,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1193766: learning rate 0.0000
[2019-03-27 06:59:21,353] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1193853: loss -135.8175
[2019-03-27 06:59:21,356] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1193853: learning rate 0.0000
[2019-03-27 06:59:21,380] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1193867: loss -100.6589
[2019-03-27 06:59:21,382] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1193867: learning rate 0.0000
[2019-03-27 06:59:21,520] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1193956: loss -135.5252
[2019-03-27 06:59:21,521] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1193956: loss -122.6445
[2019-03-27 06:59:21,522] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1193956: learning rate 0.0000
[2019-03-27 06:59:21,526] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1193957: learning rate 0.0000
[2019-03-27 06:59:22,033] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1194285: loss -98.7938
[2019-03-27 06:59:22,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1194286: learning rate 0.0000
[2019-03-27 06:59:22,096] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1194318: loss -92.6546
[2019-03-27 06:59:22,098] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1194319: learning rate 0.0000
[2019-03-27 06:59:22,129] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1194329: loss -97.5325
[2019-03-27 06:59:22,131] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1194329: learning rate 0.0000
[2019-03-27 06:59:22,246] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1194385: loss -122.9103
[2019-03-27 06:59:22,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1194387: learning rate 0.0000
[2019-03-27 06:59:22,479] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1194487: loss -100.4836
[2019-03-27 06:59:22,483] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1194487: learning rate 0.0000
[2019-03-27 06:59:23,443] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1194926: loss 0.2174
[2019-03-27 06:59:23,446] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1194927: learning rate 0.0000
[2019-03-27 06:59:27,378] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8466260e-16 1.0000000e+00 5.5567865e-21 5.9256914e-14 1.0820781e-24], sum to 1.0000
[2019-03-27 06:59:27,393] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8579
[2019-03-27 06:59:27,404] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 69.0, 1.0, 2.0, 0.5190845397182875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725349.0999853517, 725349.0999853524, 186640.1265021333], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5043000.0000, 
sim time next is 5043600.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.512187673146058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715708.4328341713, 715708.4328341719, 185527.516551328], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.41227430499525053, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19880789800949203, 0.1988078980094922, 0.2769067411213851], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.3239904], dtype=float32), -1.1989162]. 
=============================================
[2019-03-27 06:59:27,820] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1196890: loss 0.3427
[2019-03-27 06:59:27,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1196890: learning rate 0.0000
[2019-03-27 06:59:29,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3016829e-16 1.0000000e+00 2.5820849e-22 2.2967210e-14 5.2470139e-27], sum to 1.0000
[2019-03-27 06:59:29,657] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2987
[2019-03-27 06:59:29,663] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5304522940026406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741239.512132653, 741239.5121326536, 188505.219979638], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5075400.0000, 
sim time next is 5076000.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5308752258061428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741830.7118224282, 741830.7118224282, 188575.3002265263], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4347894286820997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20606408661734116, 0.20606408661734116, 0.28145567197989], 
reward next is 0.7185, 
noisyNet noise sample is [array([-0.0711621], dtype=float32), 2.8096728]. 
=============================================
[2019-03-27 06:59:29,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.10519]
 [75.09018]
 [75.07337]
 [75.02797]
 [74.97638]], R is [[75.12047577]
 [75.08792114]
 [75.0557785 ]
 [75.02416992]
 [74.99233246]].
[2019-03-27 06:59:31,753] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1198537: loss 10.4854
[2019-03-27 06:59:31,756] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1198539: learning rate 0.0000
[2019-03-27 06:59:34,992] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 06:59:34,994] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 06:59:34,995] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 06:59:34,995] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:59:34,996] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:59:34,997] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 06:59:34,998] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 06:59:34,996] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 06:59:35,002] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:59:35,003] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:59:35,006] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 06:59:35,025] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run49
[2019-03-27 06:59:35,025] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run49
[2019-03-27 06:59:35,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run49
[2019-03-27 06:59:35,086] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run49
[2019-03-27 06:59:35,087] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run49
[2019-03-27 06:59:55,569] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 06:59:55,571] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.9, 35.5, 1.0, 2.0, 0.3188724708847616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531320.641849202, 531320.641849202, 167980.4037239832]
[2019-03-27 06:59:55,572] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 06:59:55,574] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2859273e-15 1.0000000e+00 2.4566750e-20 3.4993053e-13 2.4274376e-24], sampled 0.9102872024196657
[2019-03-27 06:59:56,851] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 06:59:56,853] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.12316365666666, 93.09716788666667, 1.0, 2.0, 0.5115911691941325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714874.6240832133, 714874.6240832133, 185430.6307849064]
[2019-03-27 06:59:56,854] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 06:59:56,858] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4589771e-16 1.0000000e+00 3.6504349e-21 3.5976682e-13 1.1003764e-25], sampled 0.41457991662557414
[2019-03-27 07:00:05,510] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:00:05,511] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.66666666666667, 81.0, 1.0, 2.0, 0.4918703611540917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687308.7337657951, 687308.7337657951, 182331.4521110208]
[2019-03-27 07:00:05,512] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:00:05,514] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.3155569e-16 1.0000000e+00 7.6988406e-21 2.2440493e-13 3.6479497e-25], sampled 0.6538707721313595
[2019-03-27 07:00:08,867] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:00:08,868] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.2, 92.33333333333334, 1.0, 2.0, 0.398931958346485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595596.5777865292, 595596.5777865292, 174143.003533506]
[2019-03-27 07:00:08,869] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:00:08,871] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0687089e-16 1.0000000e+00 1.1487311e-20 1.5397715e-13 9.8495772e-25], sampled 0.8379659051046416
[2019-03-27 07:00:09,845] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:00:09,845] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.3, 82.0, 1.0, 2.0, 0.670485267407717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 937004.0683421745, 937004.0683421738, 214601.8070888437]
[2019-03-27 07:00:09,847] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:00:09,851] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.3215167e-16 1.0000000e+00 1.3538148e-20 1.1641181e-12 5.8195640e-25], sampled 0.04458555298143496
[2019-03-27 07:00:27,326] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:00:27,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.56666666666667, 74.66666666666666, 1.0, 2.0, 0.5875664574910295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 821080.1273056795, 821080.1273056801, 198452.091987018]
[2019-03-27 07:00:27,328] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:00:27,331] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.3518591e-16 1.0000000e+00 9.5372390e-21 1.0949408e-11 6.0976846e-26], sampled 0.3482043084726931
[2019-03-27 07:00:59,752] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:00:59,753] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.03333333333334, 50.0, 1.0, 2.0, 0.5866023687675757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819732.365625616, 819732.365625616, 198276.7557149807]
[2019-03-27 07:00:59,755] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:00:59,759] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.7231791e-16 1.0000000e+00 6.7645146e-21 9.2114261e-13 1.9159944e-25], sampled 0.2966279894081155
[2019-03-27 07:01:03,202] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:01:03,204] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.8, 94.66666666666666, 1.0, 2.0, 0.7736997397295282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1081319.869184322, 1081319.869184322, 237559.1751878699]
[2019-03-27 07:01:03,206] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:01:03,211] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1376008e-15 1.0000000e+00 2.5219753e-20 3.0228407e-12 8.5094465e-25], sampled 0.6943273101132893
[2019-03-27 07:01:11,355] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:01:11,355] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.66666666666667, 87.50000000000001, 1.0, 2.0, 0.658464865065084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 920198.2623011803, 920198.262301181, 212134.1025921805]
[2019-03-27 07:01:11,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:01:11,357] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2398762e-15 1.0000000e+00 2.7485971e-20 2.3028732e-12 1.0749605e-24], sampled 0.5532920105166538
[2019-03-27 07:01:23,063] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:01:23,063] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.30957972166667, 92.72189882333333, 1.0, 2.0, 0.2417458219260336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 403219.5091872543, 403219.5091872549, 159442.8745462383]
[2019-03-27 07:01:23,064] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:01:23,066] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.16514731e-16 1.00000000e+00 1.08941957e-20 1.00227525e-13
 1.15147956e-24], sampled 0.5218460105363785
[2019-03-27 07:01:25,980] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:01:25,982] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.23333333333333, 94.16666666666667, 1.0, 2.0, 0.4579213494839708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650251.0205941919, 650251.0205941924, 178605.0607037616]
[2019-03-27 07:01:25,983] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:01:25,987] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0527271e-15 1.0000000e+00 2.2968541e-20 1.4106264e-12 1.1653906e-24], sampled 0.8975071214971826
[2019-03-27 07:01:27,958] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03423326], dtype=float32), 0.04323251]
[2019-03-27 07:01:27,960] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.05951264666667, 63.15014204666667, 1.0, 2.0, 0.9864596067622327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1378865.349490307, 1378865.349490307, 294826.8255316258]
[2019-03-27 07:01:27,961] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:01:27,962] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7530206e-15 1.0000000e+00 4.2171063e-20 2.9391008e-12 2.0629078e-24], sampled 0.058266811398258955
[2019-03-27 07:01:30,404] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2105 2779289845.7918 933.0000
[2019-03-27 07:01:30,628] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.2688 3163942605.5112 1764.0000
[2019-03-27 07:01:30,809] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.2834 2842668680.8634 1130.0000
[2019-03-27 07:01:30,815] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.2170 3007616733.1249 1766.0000
[2019-03-27 07:01:30,851] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.4049 2927308528.4335 1335.0000
[2019-03-27 07:01:31,865] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1200000, evaluation results [1200000.0, 7886.268755276644, 3163942605.5112386, 1764.0, 8255.404856624207, 2927308528.433536, 1335.0, 8659.210468780357, 2779289845.791773, 933.0, 7995.216969144183, 3007616733.1248527, 1766.0, 8497.28340638559, 2842668680.863438, 1130.0]
[2019-03-27 07:01:31,951] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4286041e-17 1.0000000e+00 5.7307915e-22 7.5108664e-13 6.6700185e-27], sum to 1.0000
[2019-03-27 07:01:31,953] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5422
[2019-03-27 07:01:31,959] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.2, 60.0, 1.0, 2.0, 0.5372583710751685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750753.4988334499, 750753.4988334499, 189640.0579212312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5671800.0000, 
sim time next is 5672400.0000, 
raw observation next is [32.16666666666666, 60.0, 1.0, 2.0, 0.5366002143213054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749833.4798758344, 749833.479875835, 189529.6988751857], 
processed observation next is [0.0, 0.6521739130434783, 0.7235387045813582, 0.6, 1.0, 1.0, 0.4416870052063921, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20828707774328734, 0.2082870777432875, 0.28288014757490404], 
reward next is 0.7171, 
noisyNet noise sample is [array([1.6148748], dtype=float32), 1.4634362]. 
=============================================
[2019-03-27 07:01:34,214] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1201113: loss 10.6580
[2019-03-27 07:01:34,219] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1201113: learning rate 0.0000
[2019-03-27 07:01:34,531] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1201259: loss 11.0041
[2019-03-27 07:01:34,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1201259: learning rate 0.0000
[2019-03-27 07:01:35,168] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1201556: loss 11.8297
[2019-03-27 07:01:35,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1201557: learning rate 0.0000
[2019-03-27 07:01:35,601] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1201752: loss 11.5843
[2019-03-27 07:01:35,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1201752: learning rate 0.0000
[2019-03-27 07:01:35,797] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1201844: loss 9.8436
[2019-03-27 07:01:35,800] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1201845: learning rate 0.0000
[2019-03-27 07:01:35,866] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1201875: loss 10.5141
[2019-03-27 07:01:35,870] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1201876: learning rate 0.0000
[2019-03-27 07:01:35,994] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1201936: loss 8.9333
[2019-03-27 07:01:35,997] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1201939: learning rate 0.0000
[2019-03-27 07:01:36,176] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1202018: loss 11.0135
[2019-03-27 07:01:36,179] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1202019: learning rate 0.0000
[2019-03-27 07:01:36,676] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9209809e-14 1.0000000e+00 3.0848421e-20 3.7501044e-10 2.0393253e-24], sum to 1.0000
[2019-03-27 07:01:36,686] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8467
[2019-03-27 07:01:36,689] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 86.33333333333334, 1.0, 2.0, 0.5755005646675427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804212.5623390183, 804212.5623390183, 196270.7866972732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5275200.0000, 
sim time next is 5275800.0000, 
raw observation next is [28.6, 86.5, 1.0, 2.0, 0.576573844478018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 805712.9474522794, 805712.9474522788, 196463.1186468823], 
processed observation next is [1.0, 0.043478260869565216, 0.5545023696682465, 0.865, 1.0, 1.0, 0.48984800539520235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22380915207007762, 0.22380915207007746, 0.2932285352938542], 
reward next is 0.7068, 
noisyNet noise sample is [array([0.17920233], dtype=float32), 0.384516]. 
=============================================
[2019-03-27 07:01:36,837] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1202323: loss 12.5845
[2019-03-27 07:01:36,842] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1202324: learning rate 0.0000
[2019-03-27 07:01:36,891] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1202348: loss 11.1378
[2019-03-27 07:01:36,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1202348: learning rate 0.0000
[2019-03-27 07:01:36,919] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1202360: loss 10.8944
[2019-03-27 07:01:36,921] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1202360: learning rate 0.0000
[2019-03-27 07:01:37,187] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1202484: loss 10.3892
[2019-03-27 07:01:37,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1202484: learning rate 0.0000
[2019-03-27 07:01:37,381] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1202567: loss 7.5416
[2019-03-27 07:01:37,385] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1202569: learning rate 0.0000
[2019-03-27 07:01:38,313] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1203001: loss 20.2046
[2019-03-27 07:01:38,315] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1203001: learning rate 0.0000
[2019-03-27 07:01:42,311] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1204874: loss 31.5622
[2019-03-27 07:01:42,312] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1204874: learning rate 0.0000
[2019-03-27 07:01:45,887] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1206563: loss 0.1194
[2019-03-27 07:01:45,890] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1206563: learning rate 0.0000
[2019-03-27 07:01:46,097] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6723229e-09 6.9849133e-01 7.5758514e-12 3.0150864e-01 2.8479318e-16], sum to 1.0000
[2019-03-27 07:01:46,105] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5469
[2019-03-27 07:01:46,112] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2149288.743923871 W.
[2019-03-27 07:01:46,120] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.23333333333333, 73.66666666666667, 1.0, 2.0, 0.5123629051057708, 1.0, 1.0, 0.5123629051057708, 1.0, 2.0, 0.8898049251667951, 6.911199999999999, 6.9112, 170.5573041426782, 2149288.743923871, 2149288.743923872, 423800.458653467], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5923200.0000, 
sim time next is 5923800.0000, 
raw observation next is [30.1, 74.0, 1.0, 2.0, 0.7818217451653471, 1.0, 2.0, 0.7818217451653471, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2186457.795349259, 2186457.795349259, 411120.3532944139], 
processed observation next is [1.0, 0.5652173913043478, 0.6255924170616115, 0.74, 1.0, 1.0, 0.7371346327293338, 1.0, 1.0, 0.7371346327293338, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6073493875970164, 0.6073493875970164, 0.6136124676036028], 
reward next is 0.3864, 
noisyNet noise sample is [array([-1.2702621], dtype=float32), 0.86437845]. 
=============================================
[2019-03-27 07:01:51,336] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1209115: loss 0.2026
[2019-03-27 07:01:51,337] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1209115: learning rate 0.0000
[2019-03-27 07:01:51,613] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1209238: loss 0.1246
[2019-03-27 07:01:51,616] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1209238: learning rate 0.0000
[2019-03-27 07:01:52,328] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1209576: loss 0.0315
[2019-03-27 07:01:52,332] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1209578: learning rate 0.0000
[2019-03-27 07:01:52,690] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1209726: loss 0.2182
[2019-03-27 07:01:52,693] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1209727: learning rate 0.0000
[2019-03-27 07:01:52,948] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1209850: loss 0.1324
[2019-03-27 07:01:52,949] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1209850: learning rate 0.0000
[2019-03-27 07:01:52,959] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1209855: loss 0.0371
[2019-03-27 07:01:52,961] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1209857: learning rate 0.0000
[2019-03-27 07:01:52,980] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1209861: loss 0.2220
[2019-03-27 07:01:52,984] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1209862: learning rate 0.0000
[2019-03-27 07:01:53,217] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1209975: loss 0.1996
[2019-03-27 07:01:53,220] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1209977: learning rate 0.0000
[2019-03-27 07:01:53,788] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1210241: loss 0.1321
[2019-03-27 07:01:53,792] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1210242: learning rate 0.0000
[2019-03-27 07:01:53,948] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1210315: loss 0.1691
[2019-03-27 07:01:53,950] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1210315: learning rate 0.0000
[2019-03-27 07:01:54,142] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1210405: loss 0.1626
[2019-03-27 07:01:54,143] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1210405: learning rate 0.0000
[2019-03-27 07:01:54,187] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1210420: loss 0.0754
[2019-03-27 07:01:54,189] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1210421: learning rate 0.0000
[2019-03-27 07:01:54,272] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1210466: loss 0.3180
[2019-03-27 07:01:54,273] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1210466: learning rate 0.0000
[2019-03-27 07:01:55,574] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1211072: loss 0.7235
[2019-03-27 07:01:55,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1211072: learning rate 0.0000
[2019-03-27 07:01:59,545] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1212925: loss 0.9912
[2019-03-27 07:01:59,550] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1212927: learning rate 0.0000
[2019-03-27 07:02:00,325] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.01818734e-16 1.00000000e+00 2.28449600e-22 2.01816171e-14
 1.09585167e-26], sum to 1.0000
[2019-03-27 07:02:00,331] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9606
[2019-03-27 07:02:00,337] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.2, 74.0, 1.0, 2.0, 0.5315173219168408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742728.2732834636, 742728.2732834629, 188681.5681455161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5684400.0000, 
sim time next is 5685000.0000, 
raw observation next is [28.98333333333333, 75.5, 1.0, 2.0, 0.5346733109947537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747139.920075564, 747139.9200755634, 189206.8853414786], 
processed observation next is [0.0, 0.8260869565217391, 0.5726698262243285, 0.755, 1.0, 1.0, 0.43936543493343816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20753886668765664, 0.2075388666876565, 0.28239833633056505], 
reward next is 0.7176, 
noisyNet noise sample is [array([-0.53522426], dtype=float32), -1.7186606]. 
=============================================
[2019-03-27 07:02:00,347] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.614105]
 [75.54154 ]
 [75.5169  ]
 [75.493866]
 [75.44771 ]], R is [[75.61530304]
 [75.57753754]
 [75.53996277]
 [75.50260162]
 [75.46566772]].
[2019-03-27 07:02:02,997] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1214547: loss 10.7889
[2019-03-27 07:02:03,001] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1214550: learning rate 0.0000
[2019-03-27 07:02:07,947] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8520622e-15 1.0000000e+00 1.9647103e-18 2.6353444e-10 2.3150682e-23], sum to 1.0000
[2019-03-27 07:02:07,955] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7068
[2019-03-27 07:02:07,963] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.28333333333333, 82.16666666666667, 1.0, 2.0, 0.8528473451741722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1191998.355556287, 1191998.355556287, 257297.132162842], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5814600.0000, 
sim time next is 5815200.0000, 
raw observation next is [28.46666666666667, 81.33333333333334, 1.0, 2.0, 0.971369365972964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1357758.842865163, 1357758.842865164, 290324.7584659207], 
processed observation next is [1.0, 0.30434782608695654, 0.5481832543443919, 0.8133333333333335, 1.0, 1.0, 0.9655052602083903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3771552341292119, 0.3771552341292122, 0.4333205350237622], 
reward next is 0.5667, 
noisyNet noise sample is [array([-1.2664802], dtype=float32), 0.21435632]. 
=============================================
[2019-03-27 07:02:08,470] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1217107: loss 17.8347
[2019-03-27 07:02:08,472] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1217107: learning rate 0.0000
[2019-03-27 07:02:08,860] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1217291: loss 31.7133
[2019-03-27 07:02:08,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1217292: learning rate 0.0000
[2019-03-27 07:02:09,473] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1217577: loss 16.9268
[2019-03-27 07:02:09,477] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1217578: learning rate 0.0000
[2019-03-27 07:02:09,980] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1217811: loss 11.1422
[2019-03-27 07:02:09,987] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1217811: learning rate 0.0000
[2019-03-27 07:02:10,009] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1217822: loss 10.6697
[2019-03-27 07:02:10,010] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1217822: learning rate 0.0000
[2019-03-27 07:02:10,061] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1217849: loss 9.6958
[2019-03-27 07:02:10,062] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1217849: learning rate 0.0000
[2019-03-27 07:02:10,142] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1217884: loss 13.8709
[2019-03-27 07:02:10,144] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1217885: learning rate 0.0000
[2019-03-27 07:02:10,430] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1218021: loss 8.6293
[2019-03-27 07:02:10,436] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1218023: learning rate 0.0000
[2019-03-27 07:02:10,995] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1218281: loss 16.3732
[2019-03-27 07:02:10,997] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1218281: learning rate 0.0000
[2019-03-27 07:02:11,217] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1218382: loss 28.9405
[2019-03-27 07:02:11,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1218382: learning rate 0.0000
[2019-03-27 07:02:11,332] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1218432: loss 8.9113
[2019-03-27 07:02:11,336] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1218433: learning rate 0.0000
[2019-03-27 07:02:11,379] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1218455: loss 17.0484
[2019-03-27 07:02:11,384] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1218455: learning rate 0.0000
[2019-03-27 07:02:11,578] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1218555: loss 7.3940
[2019-03-27 07:02:11,583] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1218555: learning rate 0.0000
[2019-03-27 07:02:12,648] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1219123: loss 629.1588
[2019-03-27 07:02:12,649] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1219123: learning rate 0.0000
[2019-03-27 07:02:16,227] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1220887: loss 203.4137
[2019-03-27 07:02:16,228] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1220888: learning rate 0.0000
[2019-03-27 07:02:20,014] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1222592: loss 0.7970
[2019-03-27 07:02:20,017] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1222592: learning rate 0.0000
[2019-03-27 07:02:21,364] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0601877e-13 1.0000000e+00 4.6655106e-19 1.3036150e-10 7.0405255e-23], sum to 1.0000
[2019-03-27 07:02:21,371] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4261
[2019-03-27 07:02:21,377] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.23333333333333, 92.33333333333333, 1.0, 2.0, 0.7088847551475516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 990692.4344048374, 990692.4344048367, 222774.0295728766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6066600.0000, 
sim time next is 6067200.0000, 
raw observation next is [26.36666666666667, 91.66666666666667, 1.0, 2.0, 0.6765301907666971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 945455.6171859433, 945455.6171859433, 215860.3521127785], 
processed observation next is [1.0, 0.21739130434782608, 0.4486571879936811, 0.9166666666666667, 1.0, 1.0, 0.610277338273129, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2626265603294287, 0.2626265603294287, 0.32217963001907235], 
reward next is 0.6778, 
noisyNet noise sample is [array([1.6077515], dtype=float32), -0.007743403]. 
=============================================
[2019-03-27 07:02:22,847] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3645020e-07 8.4808874e-01 2.1779707e-10 1.5191117e-01 1.2360925e-13], sum to 1.0000
[2019-03-27 07:02:22,858] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9959
[2019-03-27 07:02:22,863] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.06666666666667, 71.0, 1.0, 2.0, 0.8651546931352693, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.990495355789641, 6.9112, 168.9124173809205, 2106263.840854833, 2050009.165874098, 425154.8318584002], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6083400.0000, 
sim time next is 6084000.0000, 
raw observation next is [30.2, 70.0, 1.0, 2.0, 0.7701026645164776, 1.0, 1.0, 0.7701026645164776, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2153651.040401952, 2153651.040401952, 405576.9485357777], 
processed observation next is [1.0, 0.43478260869565216, 0.6303317535545023, 0.7, 1.0, 1.0, 0.7230152584535874, 1.0, 0.5, 0.7230152584535874, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5982364001116532, 0.5982364001116532, 0.6053387291578771], 
reward next is 0.3947, 
noisyNet noise sample is [array([-0.13685513], dtype=float32), -0.88108104]. 
=============================================
[2019-03-27 07:02:22,877] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[38.675667]
 [37.874775]
 [38.230312]
 [41.280445]
 [42.179256]], R is [[39.09958267]
 [38.70858765]
 [38.32150269]
 [38.34668732]
 [37.96324539]].
[2019-03-27 07:02:25,677] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 07:02:25,680] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:02:25,680] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:02:25,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:02:25,681] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:02:25,681] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:02:25,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:02:25,683] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:02:25,685] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:02:25,683] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:02:25,688] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:02:25,703] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run50
[2019-03-27 07:02:25,723] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run50
[2019-03-27 07:02:25,739] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run50
[2019-03-27 07:02:25,764] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run50
[2019-03-27 07:02:25,782] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run50
[2019-03-27 07:02:42,411] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0359263], dtype=float32), 0.04271658]
[2019-03-27 07:02:42,411] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.982380105, 74.447467895, 1.0, 2.0, 0.4164578730647884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616467.8139323456, 616467.8139323463, 175939.5947247815]
[2019-03-27 07:02:42,415] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:02:42,418] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7861507e-15 1.0000000e+00 5.3257794e-20 5.8348714e-13 6.8013244e-24], sampled 0.5199079447549888
[2019-03-27 07:03:55,579] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0359263], dtype=float32), 0.04271658]
[2019-03-27 07:03:55,580] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.26130374, 98.05234565, 1.0, 2.0, 0.5229005397949587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730683.2681204583, 730683.2681204583, 187259.1475053535]
[2019-03-27 07:03:55,581] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:03:55,583] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0781058e-15 1.0000000e+00 4.8714818e-20 4.8273998e-12 3.0486056e-24], sampled 0.3748437437486809
[2019-03-27 07:04:10,564] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0359263], dtype=float32), 0.04271658]
[2019-03-27 07:04:10,565] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.4, 81.0, 1.0, 2.0, 0.5857716199720548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818571.0097849716, 818571.0097849716, 198124.6373703475]
[2019-03-27 07:04:10,567] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:04:10,569] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3842746e-15 1.0000000e+00 2.7985775e-20 1.1015643e-11 8.6906301e-25], sampled 0.9234194070950209
[2019-03-27 07:04:16,884] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0359263], dtype=float32), 0.04271658]
[2019-03-27 07:04:16,886] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.13500829333334, 85.00833478666668, 1.0, 2.0, 0.4387676628019968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634474.8976054955, 634474.8976054955, 177301.227061616]
[2019-03-27 07:04:16,888] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:04:16,892] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3161461e-15 1.0000000e+00 3.0870552e-20 4.6009258e-13 3.6419866e-24], sampled 0.9697900331944178
[2019-03-27 07:04:20,599] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0359263], dtype=float32), 0.04271658]
[2019-03-27 07:04:20,600] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.1, 61.0, 1.0, 2.0, 0.2995045996463072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480240.1397591506, 480240.1397591506, 165674.2809422359]
[2019-03-27 07:04:20,602] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:04:20,604] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6413040e-15 1.0000000e+00 4.0087312e-20 1.4107959e-12 3.4344586e-24], sampled 0.40166505676867903
[2019-03-27 07:04:20,886] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.7308 2842237889.3237 1120.0000
[2019-03-27 07:04:21,181] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.2503 2779308046.5428 932.0000
[2019-03-27 07:04:21,338] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.4291 2927362462.0098 1328.0000
[2019-03-27 07:04:21,384] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7899.4921 3162214190.9479 1722.0000
[2019-03-27 07:04:21,436] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.4752 3006979362.2082 1749.0000
[2019-03-27 07:04:22,454] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1225000, evaluation results [1225000.0, 7899.492053176955, 3162214190.947946, 1722.0, 8253.429053559543, 2927362462.0097923, 1328.0, 8658.250316433003, 2779308046.542806, 932.0, 8000.475219144516, 3006979362.2082314, 1749.0, 8500.730837433484, 2842237889.3237042, 1120.0]
[2019-03-27 07:04:22,644] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1225090: loss 0.7073
[2019-03-27 07:04:22,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1225091: learning rate 0.0000
[2019-03-27 07:04:22,859] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1225190: loss 0.8268
[2019-03-27 07:04:22,862] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1225191: learning rate 0.0000
[2019-03-27 07:04:23,634] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1225554: loss 0.7245
[2019-03-27 07:04:23,636] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1225554: learning rate 0.0000
[2019-03-27 07:04:24,181] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1225805: loss 0.8613
[2019-03-27 07:04:24,185] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1225805: learning rate 0.0000
[2019-03-27 07:04:24,185] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1225806: loss 0.7782
[2019-03-27 07:04:24,192] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1225806: learning rate 0.0000
[2019-03-27 07:04:24,216] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1225816: loss 0.5869
[2019-03-27 07:04:24,220] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1225818: learning rate 0.0000
[2019-03-27 07:04:24,232] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1225825: loss 0.7348
[2019-03-27 07:04:24,234] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1225825: learning rate 0.0000
[2019-03-27 07:04:24,505] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1225956: loss 0.8088
[2019-03-27 07:04:24,507] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1225956: learning rate 0.0000
[2019-03-27 07:04:25,148] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1226254: loss 0.8205
[2019-03-27 07:04:25,151] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1226254: learning rate 0.0000
[2019-03-27 07:04:25,424] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1226384: loss 0.6245
[2019-03-27 07:04:25,426] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1226385: learning rate 0.0000
[2019-03-27 07:04:25,432] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1226388: loss 0.9375
[2019-03-27 07:04:25,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1226389: learning rate 0.0000
[2019-03-27 07:04:25,476] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1226407: loss 0.8447
[2019-03-27 07:04:25,479] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1226408: learning rate 0.0000
[2019-03-27 07:04:25,503] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.7694585e-17 1.0000000e+00 1.9352944e-22 2.1406661e-13 2.7535328e-25], sum to 1.0000
[2019-03-27 07:04:25,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4623
[2019-03-27 07:04:25,519] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.51666666666667, 84.83333333333334, 1.0, 2.0, 0.5351049314570406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747743.2686537235, 747743.268653723, 189278.9555447607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6299400.0000, 
sim time next is 6300000.0000, 
raw observation next is [27.5, 85.0, 1.0, 2.0, 0.5348530469362281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747391.1672156662, 747391.1672156662, 189236.9176902414], 
processed observation next is [0.0, 0.9565217391304348, 0.5023696682464456, 0.85, 1.0, 1.0, 0.4395819842605158, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20760865755990726, 0.20760865755990726, 0.28244316073170356], 
reward next is 0.7176, 
noisyNet noise sample is [array([-1.827668], dtype=float32), -2.872164]. 
=============================================
[2019-03-27 07:04:25,528] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[71.091064]
 [71.10455 ]
 [71.10765 ]
 [71.10151 ]
 [71.094376]], R is [[71.12478638]
 [71.13102722]
 [71.1370163 ]
 [71.14266205]
 [71.14817047]].
[2019-03-27 07:04:25,707] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1226512: loss 0.6297
[2019-03-27 07:04:25,710] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1226513: learning rate 0.0000
[2019-03-27 07:04:25,904] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.8064571e-08 2.9943457e-01 6.0917349e-10 7.0056540e-01 5.3529909e-16], sum to 1.0000
[2019-03-27 07:04:25,911] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9062
[2019-03-27 07:04:25,915] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.63333333333333, 69.0, 1.0, 2.0, 0.786890680383214, 1.0, 1.0, 0.786890680383214, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2200648.2398695, 2200648.2398695, 413539.1403518741], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6183600.0000, 
sim time next is 6184200.0000, 
raw observation next is [30.71666666666667, 68.5, 1.0, 2.0, 0.7970291011338234, 1.0, 2.0, 0.7970291011338234, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2229028.796362014, 2229028.796362014, 418426.6161549363], 
processed observation next is [1.0, 0.5652173913043478, 0.6548183254344393, 0.685, 1.0, 1.0, 0.7554567483540041, 1.0, 1.0, 0.7554567483540041, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.619174665656115, 0.619174665656115, 0.6245173375446811], 
reward next is 0.3755, 
noisyNet noise sample is [array([-0.6026906], dtype=float32), -2.8594975]. 
=============================================
[2019-03-27 07:04:26,845] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1227046: loss 0.0027
[2019-03-27 07:04:26,847] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1227046: learning rate 0.0000
[2019-03-27 07:04:27,018] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8260266e-15 1.0000000e+00 6.0516418e-18 4.9408612e-08 1.4023719e-23], sum to 1.0000
[2019-03-27 07:04:27,025] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1561
[2019-03-27 07:04:27,029] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 82.0, 1.0, 2.0, 0.5302770901753397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740994.6017080314, 740994.6017080307, 188476.222844014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6202800.0000, 
sim time next is 6203400.0000, 
raw observation next is [27.91666666666667, 82.50000000000001, 1.0, 2.0, 0.530277748374694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740995.5217784841, 740995.5217784841, 188476.3151380951], 
processed observation next is [1.0, 0.8260869565217391, 0.5221169036334916, 0.8250000000000002, 1.0, 1.0, 0.434069576355053, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20583208938291225, 0.20583208938291225, 0.28130793304193297], 
reward next is 0.7187, 
noisyNet noise sample is [array([0.6763499], dtype=float32), -0.71886706]. 
=============================================
[2019-03-27 07:04:28,847] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3349777e-17 1.0000000e+00 1.1186412e-21 5.2262306e-15 3.1306534e-25], sum to 1.0000
[2019-03-27 07:04:28,859] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2437
[2019-03-27 07:04:28,864] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 87.66666666666667, 1.0, 2.0, 0.535774913296884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748679.8157333675, 748679.8157333675, 189391.5421148086], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6244800.0000, 
sim time next is 6245400.0000, 
raw observation next is [27.41666666666667, 87.33333333333333, 1.0, 2.0, 0.5361158219725406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749156.3618395384, 749156.3618395384, 189448.6578045307], 
processed observation next is [0.0, 0.2608695652173913, 0.4984202211690366, 0.8733333333333333, 1.0, 1.0, 0.44110339996691633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2080989893998718, 0.2080989893998718, 0.2827591907530309], 
reward next is 0.7172, 
noisyNet noise sample is [array([0.36654815], dtype=float32), 1.9811492]. 
=============================================
[2019-03-27 07:04:29,993] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4530568e-14 1.0000000e+00 6.4107288e-19 8.3163848e-10 4.9766978e-23], sum to 1.0000
[2019-03-27 07:04:30,004] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8298
[2019-03-27 07:04:30,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2448410e-17 1.0000000e+00 8.9500378e-23 2.8610371e-14 1.8245123e-26], sum to 1.0000
[2019-03-27 07:04:30,012] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 58.66666666666667, 1.0, 2.0, 0.3170777072927994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501115.3531999768, 501115.3531999774, 167103.2722701032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6812400.0000, 
sim time next is 6813000.0000, 
raw observation next is [26.15, 59.5, 1.0, 2.0, 0.3197452436669642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504924.4537710003, 504924.453771001, 167382.0851305817], 
processed observation next is [1.0, 0.8695652173913043, 0.43838862559241704, 0.595, 1.0, 1.0, 0.18041595622525808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14025679271416674, 0.14025679271416694, 0.2498240076575846], 
reward next is 0.7502, 
noisyNet noise sample is [array([1.7827814], dtype=float32), 0.47764897]. 
=============================================
[2019-03-27 07:04:30,018] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3668
[2019-03-27 07:04:30,023] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 74.0, 1.0, 2.0, 0.5419591470242147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757324.6055087395, 757324.6055087395, 190431.8515311663], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6256800.0000, 
sim time next is 6257400.0000, 
raw observation next is [29.85, 73.16666666666667, 1.0, 2.0, 0.541994185220643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757373.5847509764, 757373.5847509764, 190437.789080957], 
processed observation next is [0.0, 0.43478260869565216, 0.613744075829384, 0.7316666666666667, 1.0, 1.0, 0.4481857653260759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21038155131971567, 0.21038155131971567, 0.2842355060909806], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.3071207], dtype=float32), -0.7592543]. 
=============================================
[2019-03-27 07:04:30,036] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[67.91634]
 [67.68875]
 [67.56537]
 [67.51087]
 [67.44312]], R is [[68.09737396]
 [68.16699219]
 [68.23601532]
 [68.30429077]
 [68.37152863]].
[2019-03-27 07:04:30,835] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1228898: loss 0.0257
[2019-03-27 07:04:30,838] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1228899: learning rate 0.0000
[2019-03-27 07:04:32,799] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.89529517e-18 1.00000000e+00 1.10429359e-21 1.29658975e-14
 2.95846105e-26], sum to 1.0000
[2019-03-27 07:04:32,806] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5743
[2019-03-27 07:04:32,811] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.4, 63.0, 1.0, 2.0, 0.5024094626308534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702040.278220611, 702040.278220611, 183974.5543643206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6282000.0000, 
sim time next is 6282600.0000, 
raw observation next is [30.25, 64.0, 1.0, 2.0, 0.5027214741685746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 702476.4107185948, 702476.4107185954, 184023.7529082939], 
processed observation next is [0.0, 0.7391304347826086, 0.6327014218009479, 0.64, 1.0, 1.0, 0.4008692459862344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19513233631072077, 0.19513233631072094, 0.27466231777357303], 
reward next is 0.7253, 
noisyNet noise sample is [array([0.31923124], dtype=float32), -0.9262904]. 
=============================================
[2019-03-27 07:04:34,234] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1230627: loss 90.7160
[2019-03-27 07:04:34,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1230628: learning rate 0.0000
[2019-03-27 07:04:38,155] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.1301504e-15 1.0000000e+00 1.1886010e-19 1.5446693e-09 1.8375978e-23], sum to 1.0000
[2019-03-27 07:04:38,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7686
[2019-03-27 07:04:38,174] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 85.66666666666666, 1.0, 2.0, 0.8356262942694533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1167915.787792461, 1167915.787792461, 252841.8606759437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6406800.0000, 
sim time next is 6407400.0000, 
raw observation next is [26.73333333333333, 85.83333333333334, 1.0, 2.0, 0.820116806739823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1146227.202344312, 1146227.202344312, 248906.8426001346], 
processed observation next is [1.0, 0.13043478260869565, 0.4660347551342811, 0.8583333333333334, 1.0, 1.0, 0.7832732611323169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31839644509564224, 0.31839644509564224, 0.37150275014945466], 
reward next is 0.6285, 
noisyNet noise sample is [array([0.10323773], dtype=float32), -0.7526525]. 
=============================================
[2019-03-27 07:04:39,400] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1233179: loss 207.6623
[2019-03-27 07:04:39,402] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1233180: learning rate 0.0000
[2019-03-27 07:04:39,541] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1233266: loss 278.2177
[2019-03-27 07:04:39,543] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1233266: learning rate 0.0000
[2019-03-27 07:04:40,064] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5163185e-15 1.0000000e+00 1.4525429e-19 6.8907220e-11 2.9778499e-23], sum to 1.0000
[2019-03-27 07:04:40,072] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4706
[2019-03-27 07:04:40,077] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 84.33333333333334, 1.0, 2.0, 0.5178707242791181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723652.3824592204, 723652.3824592197, 186443.6304068216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6562200.0000, 
sim time next is 6562800.0000, 
raw observation next is [27.2, 85.0, 1.0, 2.0, 0.5191763705832839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725477.46478193, 725477.4647819294, 186655.4129660442], 
processed observation next is [1.0, 1.0, 0.4881516587677725, 0.85, 1.0, 1.0, 0.42069442238949867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20152151799498055, 0.2015215179949804, 0.27859016860603614], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.10872275], dtype=float32), 1.8289175]. 
=============================================
[2019-03-27 07:04:40,257] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1233613: loss 157.8009
[2019-03-27 07:04:40,259] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1233613: learning rate 0.0000
[2019-03-27 07:04:40,750] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1233837: loss 223.4410
[2019-03-27 07:04:40,753] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1233838: learning rate 0.0000
[2019-03-27 07:04:40,833] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1233874: loss 384.1926
[2019-03-27 07:04:40,834] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1233874: learning rate 0.0000
[2019-03-27 07:04:40,874] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1233896: loss 296.1195
[2019-03-27 07:04:40,875] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1233896: learning rate 0.0000
[2019-03-27 07:04:40,905] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1233912: loss 121.3072
[2019-03-27 07:04:40,907] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1233912: learning rate 0.0000
[2019-03-27 07:04:41,140] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1234020: loss 303.6500
[2019-03-27 07:04:41,142] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1234020: learning rate 0.0000
[2019-03-27 07:04:41,686] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1234276: loss 236.8881
[2019-03-27 07:04:41,689] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1234277: learning rate 0.0000
[2019-03-27 07:04:41,883] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1234368: loss 223.3716
[2019-03-27 07:04:41,889] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1234370: learning rate 0.0000
[2019-03-27 07:04:41,959] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1234410: loss 208.5780
[2019-03-27 07:04:41,962] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1234411: loss 99.2864
[2019-03-27 07:04:41,966] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1234411: learning rate 0.0000
[2019-03-27 07:04:41,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1234411: learning rate 0.0000
[2019-03-27 07:04:42,481] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1234637: loss 70.0355
[2019-03-27 07:04:42,483] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1234637: learning rate 0.0000
[2019-03-27 07:04:43,248] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1234996: loss -98.9755
[2019-03-27 07:04:43,251] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1234997: learning rate 0.0000
[2019-03-27 07:04:46,866] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1236790: loss -39.6827
[2019-03-27 07:04:46,869] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1236791: learning rate 0.0000
[2019-03-27 07:04:48,166] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5020892e-12 9.9976403e-01 2.3382260e-15 2.3597848e-04 6.0636081e-20], sum to 1.0000
[2019-03-27 07:04:48,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8760
[2019-03-27 07:04:48,183] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3530904.462239596 W.
[2019-03-27 07:04:48,186] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.16666666666667, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 9.837569227544508, 6.9112, 168.8959288918541, 3530904.462239596, 1455046.696579593, 306836.0562090148], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6614400.0000, 
sim time next is 6615000.0000, 
raw observation next is [31.25, 63.0, 1.0, 2.0, 0.6116776685276774, 1.0, 1.0, 0.6116776685276774, 1.0, 1.0, 1.03, 6.940188954867205, 6.9112, 170.5573041426782, 2566338.52341019, 2545572.55584352, 492553.2560375702], 
processed observation next is [1.0, 0.5652173913043478, 0.6800947867298578, 0.63, 1.0, 1.0, 0.5321417693104546, 1.0, 0.5, 0.5321417693104546, 1.0, 0.5, 1.0365853658536586, 0.0028988954867204876, 0.0, 0.8375144448122397, 0.7128718120583861, 0.7071034877343111, 0.7351541134889107], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46248546], dtype=float32), -0.4548984]. 
=============================================
[2019-03-27 07:04:48,197] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[57.502354]
 [56.65802 ]
 [60.240196]
 [57.613407]
 [53.957603]], R is [[50.89319611]
 [50.3842659 ]
 [50.36175919]
 [49.85814285]
 [49.9143219 ]].
[2019-03-27 07:04:48,233] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.4889861e-15 1.0000000e+00 1.2961000e-18 1.4369397e-09 8.4915550e-23], sum to 1.0000
[2019-03-27 07:04:48,241] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7635
[2019-03-27 07:04:48,247] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 89.16666666666667, 1.0, 2.0, 0.519268848784936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725606.734466834, 725606.7344668333, 186670.0259647582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6570600.0000, 
sim time next is 6571200.0000, 
raw observation next is [26.4, 89.33333333333334, 1.0, 2.0, 0.5185196867119601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724559.5265676586, 724559.526567658, 186548.4288508109], 
processed observation next is [1.0, 0.043478260869565216, 0.45023696682464454, 0.8933333333333334, 1.0, 1.0, 0.4199032370023616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20126653515768295, 0.20126653515768275, 0.2784304908221058], 
reward next is 0.7216, 
noisyNet noise sample is [array([1.1627865], dtype=float32), -1.2282372]. 
=============================================
[2019-03-27 07:04:50,819] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1238675: loss 0.0001
[2019-03-27 07:04:50,823] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1238675: learning rate 0.0000
[2019-03-27 07:04:54,195] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7439594e-09 2.1722625e-01 1.8607737e-11 7.8277373e-01 1.5326425e-16], sum to 1.0000
[2019-03-27 07:04:54,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0340
[2019-03-27 07:04:54,208] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2230127.0565179 W.
[2019-03-27 07:04:54,213] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.56666666666667, 67.33333333333334, 1.0, 2.0, 0.5316143024208767, 1.0, 2.0, 0.5316143024208767, 1.0, 1.0, 0.9060892776325453, 6.911200000000001, 6.9112, 170.5573041426782, 2230127.0565179, 2230127.0565179, 434343.2702358022], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6696600.0000, 
sim time next is 6697200.0000, 
raw observation next is [29.63333333333333, 66.66666666666667, 1.0, 2.0, 0.6461555232251543, 1.0, 2.0, 0.6461555232251543, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1806731.091914278, 1806731.091914278, 351917.9392139991], 
processed observation next is [1.0, 0.5217391304347826, 0.6034755134281199, 0.6666666666666667, 1.0, 1.0, 0.5736813532833185, 1.0, 1.0, 0.5736813532833185, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5018697477539661, 0.5018697477539661, 0.5252506555432822], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07639793], dtype=float32), 0.81455064]. 
=============================================
[2019-03-27 07:04:54,531] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0936206e-13 1.0000000e+00 1.4675024e-19 1.3767343e-10 7.7842341e-24], sum to 1.0000
[2019-03-27 07:04:54,535] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7761
[2019-03-27 07:04:54,540] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 88.0, 1.0, 2.0, 0.5673907862701171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792875.6144799187, 792875.6144799181, 194825.0522412113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7194000.0000, 
sim time next is 7194600.0000, 
raw observation next is [27.0, 87.5, 1.0, 2.0, 0.5667284434632494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791949.7072414268, 791949.7072414268, 194708.3262203799], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.875, 1.0, 1.0, 0.4779860764617462, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21998602978928522, 0.21998602978928522, 0.29060944211997003], 
reward next is 0.7094, 
noisyNet noise sample is [array([1.4678031], dtype=float32), -0.63968754]. 
=============================================
[2019-03-27 07:04:55,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9301591e-15 1.0000000e+00 4.8788463e-20 2.0225289e-13 5.8520696e-24], sum to 1.0000
[2019-03-27 07:04:55,407] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1645
[2019-03-27 07:04:55,414] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 83.0, 1.0, 2.0, 0.3493246992825027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 540260.4933673695, 540260.4933673688, 169899.2951815642], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6845400.0000, 
sim time next is 6846000.0000, 
raw observation next is [23.4, 82.66666666666667, 1.0, 2.0, 0.3506508702489952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541509.2451905177, 541509.2451905177, 169979.5624931546], 
processed observation next is [0.0, 0.21739130434782608, 0.30805687203791465, 0.8266666666666667, 1.0, 1.0, 0.21765165090240388, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15041923477514382, 0.15041923477514382, 0.25370083954202177], 
reward next is 0.7463, 
noisyNet noise sample is [array([-0.6460091], dtype=float32), 0.46735677]. 
=============================================
[2019-03-27 07:04:55,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[71.18179 ]
 [71.16293 ]
 [71.144585]
 [71.11143 ]
 [71.06101 ]], R is [[71.23053741]
 [71.26465607]
 [71.29865265]
 [71.33270264]
 [71.36670685]].
[2019-03-27 07:04:55,881] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1241182: loss 0.0168
[2019-03-27 07:04:55,884] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1241183: learning rate 0.0000
[2019-03-27 07:04:55,921] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1241200: loss 0.0162
[2019-03-27 07:04:55,923] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1241201: learning rate 0.0000
[2019-03-27 07:04:56,653] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1241549: loss 0.0285
[2019-03-27 07:04:56,655] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1241549: learning rate 0.0000
[2019-03-27 07:04:57,145] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1241799: loss 0.0299
[2019-03-27 07:04:57,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1241801: learning rate 0.0000
[2019-03-27 07:04:57,222] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1241845: loss 0.0364
[2019-03-27 07:04:57,224] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1241847: learning rate 0.0000
[2019-03-27 07:04:57,262] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1241868: loss 0.0383
[2019-03-27 07:04:57,263] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1241869: learning rate 0.0000
[2019-03-27 07:04:57,298] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1241886: loss 0.0350
[2019-03-27 07:04:57,301] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1241887: learning rate 0.0000
[2019-03-27 07:04:57,749] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1242125: loss 0.0445
[2019-03-27 07:04:57,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1242125: learning rate 0.0000
[2019-03-27 07:04:58,033] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1242306: loss 0.0486
[2019-03-27 07:04:58,034] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1242306: learning rate 0.0000
[2019-03-27 07:04:58,041] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1242311: loss 0.0464
[2019-03-27 07:04:58,042] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1242311: learning rate 0.0000
[2019-03-27 07:04:58,117] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1242354: loss 0.0482
[2019-03-27 07:04:58,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1242354: learning rate 0.0000
[2019-03-27 07:04:58,177] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1242394: loss 0.0499
[2019-03-27 07:04:58,181] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1242394: learning rate 0.0000
[2019-03-27 07:04:58,603] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1242655: loss 0.0472
[2019-03-27 07:04:58,607] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1242656: learning rate 0.0000
[2019-03-27 07:04:58,989] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1242903: loss 0.0136
[2019-03-27 07:04:58,990] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1242903: learning rate 0.0000
[2019-03-27 07:04:59,078] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6782803e-13 9.9999988e-01 2.5873058e-18 7.7872961e-08 2.1387368e-23], sum to 1.0000
[2019-03-27 07:04:59,084] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6411
[2019-03-27 07:04:59,091] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.21666666666667, 48.66666666666666, 1.0, 2.0, 0.9774103882051074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1514285.958566208, 1514285.958566209, 313365.614815757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6796200.0000, 
sim time next is 6796800.0000, 
raw observation next is [29.2, 49.0, 1.0, 2.0, 0.9046513252383885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1399623.417966495, 1399623.417966495, 289886.4719257845], 
processed observation next is [1.0, 0.6956521739130435, 0.5829383886255924, 0.49, 1.0, 1.0, 0.8851220786004681, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3887842827684708, 0.3887842827684708, 0.43266637600863356], 
reward next is 0.5673, 
noisyNet noise sample is [array([0.5499855], dtype=float32), 0.050380863]. 
=============================================
[2019-03-27 07:05:02,049] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1244819: loss 0.0002
[2019-03-27 07:05:02,051] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1244819: learning rate 0.0000
[2019-03-27 07:05:03,199] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8337111e-16 1.0000000e+00 6.2796329e-21 1.5561756e-13 6.4144000e-26], sum to 1.0000
[2019-03-27 07:05:03,207] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8696
[2019-03-27 07:05:03,212] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.05, 87.66666666666667, 1.0, 2.0, 0.5912425696324287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 841688.2798063053, 841688.2798063053, 201117.7585156127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7015800.0000, 
sim time next is 7016400.0000, 
raw observation next is [25.0, 88.0, 1.0, 2.0, 0.5832156894620328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 830408.377166842, 830408.3771668413, 199641.8364603053], 
processed observation next is [1.0, 0.21739130434782608, 0.38388625592417064, 0.88, 1.0, 1.0, 0.4978502282675094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2306689936574561, 0.2306689936574559, 0.2979728902392616], 
reward next is 0.7020, 
noisyNet noise sample is [array([-0.26035267], dtype=float32), -0.2102876]. 
=============================================
[2019-03-27 07:05:04,305] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7102558e-17 1.0000000e+00 2.2315247e-22 3.3397755e-12 2.7598003e-25], sum to 1.0000
[2019-03-27 07:05:04,315] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4896
[2019-03-27 07:05:04,322] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.65, 90.5, 1.0, 2.0, 0.6322900661969493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1015932.425856937, 1015932.425856936, 221778.3248422539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7399800.0000, 
sim time next is 7400400.0000, 
raw observation next is [20.63333333333333, 90.33333333333334, 1.0, 2.0, 0.6114700420070801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 983143.4715254193, 983143.4715254187, 217231.662735039], 
processed observation next is [1.0, 0.6521739130434783, 0.17693522906793036, 0.9033333333333334, 1.0, 1.0, 0.5318916168760001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2730954087570609, 0.27309540875706073, 0.32422636229110297], 
reward next is 0.6758, 
noisyNet noise sample is [array([-1.3170865], dtype=float32), 0.16322245]. 
=============================================
[2019-03-27 07:05:05,174] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0876563e-15 1.0000000e+00 3.4086187e-22 2.0385200e-13 7.4565469e-26], sum to 1.0000
[2019-03-27 07:05:05,186] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8761
[2019-03-27 07:05:05,191] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.45, 79.33333333333334, 1.0, 2.0, 0.415919574448055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610821.5112986965, 610821.5112986965, 175269.8816157605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6909000.0000, 
sim time next is 6909600.0000, 
raw observation next is [25.4, 79.66666666666667, 1.0, 2.0, 0.4163573938097708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611451.8569809191, 611451.8569809191, 175329.2391634624], 
processed observation next is [0.0, 1.0, 0.4028436018957346, 0.7966666666666667, 1.0, 1.0, 0.2968161371202058, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16984773805025533, 0.16984773805025533, 0.2616854315872573], 
reward next is 0.7383, 
noisyNet noise sample is [array([-1.5142577], dtype=float32), -0.36394656]. 
=============================================
[2019-03-27 07:05:05,889] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1246702: loss -120.5901
[2019-03-27 07:05:05,892] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1246703: learning rate 0.0000
[2019-03-27 07:05:11,409] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1249197: loss -27.3437
[2019-03-27 07:05:11,412] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1249198: learning rate 0.0000
[2019-03-27 07:05:11,691] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1249318: loss 10.0280
[2019-03-27 07:05:11,694] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1249319: learning rate 0.0000
[2019-03-27 07:05:12,288] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1249586: loss -133.9383
[2019-03-27 07:05:12,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1249586: learning rate 0.0000
[2019-03-27 07:05:13,445] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1249885: loss -17.9422
[2019-03-27 07:05:13,449] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1249886: learning rate 0.0000
[2019-03-27 07:05:13,471] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1249896: loss -74.9830
[2019-03-27 07:05:13,474] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1249898: learning rate 0.0000
[2019-03-27 07:05:13,491] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1249901: loss -122.5391
[2019-03-27 07:05:13,495] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1249902: learning rate 0.0000
[2019-03-27 07:05:13,544] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1249924: loss -23.8949
[2019-03-27 07:05:13,547] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1249924: learning rate 0.0000
[2019-03-27 07:05:13,718] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 07:05:13,720] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:05:13,721] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:05:13,721] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:05:13,722] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:05:13,722] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:05:13,723] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:05:13,723] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:05:13,726] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:05:13,724] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:05:13,727] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:05:13,749] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run51
[2019-03-27 07:05:13,770] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run51
[2019-03-27 07:05:13,791] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run51
[2019-03-27 07:05:13,791] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run51
[2019-03-27 07:05:13,809] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run51
[2019-03-27 07:05:56,404] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03712465], dtype=float32), 0.04378706]
[2019-03-27 07:05:56,406] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.77500155, 94.57669213333332, 1.0, 2.0, 0.3999569218238734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585494.7690485585, 585494.7690485585, 172843.2572134064]
[2019-03-27 07:05:56,408] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:05:56,412] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2094509e-15 1.0000000e+00 1.7104489e-20 9.7431709e-13 1.3203893e-24], sampled 0.8708466943903265
[2019-03-27 07:06:03,959] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03712465], dtype=float32), 0.04378706]
[2019-03-27 07:06:03,961] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.9, 62.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.564561033764099, 6.9112, 168.9089837608156, 1917578.318462314, 1454072.415845872, 311351.6302773236]
[2019-03-27 07:06:03,961] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:06:03,964] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7554104e-14 1.0000000e+00 3.6637616e-19 7.1105866e-10 1.0370108e-23], sampled 0.8906250228710479
[2019-03-27 07:06:03,966] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1917578.318462314 W.
[2019-03-27 07:06:04,550] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03712465], dtype=float32), 0.04378706]
[2019-03-27 07:06:04,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.7, 82.0, 1.0, 2.0, 0.503306300560636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703293.8867248441, 703293.8867248441, 184114.8075103252]
[2019-03-27 07:06:04,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:06:04,554] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4549857e-15 1.0000000e+00 3.8003308e-20 9.2967726e-12 1.6883142e-24], sampled 0.14005697129077788
[2019-03-27 07:06:36,957] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03712465], dtype=float32), 0.04378706]
[2019-03-27 07:06:36,958] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.98817746, 77.85931360333333, 1.0, 2.0, 0.5027603615902005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702530.7679144015, 702530.7679144008, 184030.2576662391]
[2019-03-27 07:06:36,958] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:06:36,960] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.2251796e-16 1.0000000e+00 1.0999092e-20 4.9898589e-13 7.0962520e-25], sampled 0.2960256837204628
[2019-03-27 07:06:43,568] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03712465], dtype=float32), 0.04378706]
[2019-03-27 07:06:43,569] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.73333333333333, 66.66666666666666, 1.0, 2.0, 0.7717855920333323, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988046299736272, 6.9112, 168.9124365583938, 1975587.002149595, 1921069.760088004, 401654.6057496255]
[2019-03-27 07:06:43,570] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:06:43,574] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4681771e-09 9.9782097e-01 9.3249746e-13 2.1790827e-03 9.0164136e-17], sampled 0.5045244155819909
[2019-03-27 07:06:43,578] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1975587.002149595 W.
[2019-03-27 07:06:53,519] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03712465], dtype=float32), 0.04378706]
[2019-03-27 07:06:53,519] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.14087105, 57.71504689666666, 1.0, 2.0, 0.6716924070055517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1016282.656047768, 1016282.656047768, 224687.0483593778]
[2019-03-27 07:06:53,523] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:06:53,526] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2406256e-15 1.0000000e+00 3.8344791e-20 3.3170988e-12 2.7861727e-24], sampled 0.0156749575519739
[2019-03-27 07:07:04,575] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03712465], dtype=float32), 0.04378706]
[2019-03-27 07:07:04,576] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.44077375333334, 79.45425177666667, 1.0, 2.0, 0.4573302680099497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 647550.5178713546, 647550.517871354, 178280.3571145728]
[2019-03-27 07:07:04,578] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:07:04,579] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.4012675e-16 1.0000000e+00 1.1289690e-20 2.5207725e-13 8.5989211e-25], sampled 0.8018412441527555
[2019-03-27 07:07:09,494] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.2273 2779043781.1427 928.0000
[2019-03-27 07:07:09,550] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.8414 2927065669.5250 1322.0000
[2019-03-27 07:07:09,653] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8010.1657 3005643119.6678 1720.0000
[2019-03-27 07:07:09,749] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.7295 2841811592.8495 1113.0000
[2019-03-27 07:07:09,791] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7915.5484 3160721522.4785 1677.0000
[2019-03-27 07:07:10,806] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1250000, evaluation results [1250000.0, 7915.548388736281, 3160721522.4784584, 1677.0, 8256.841384525114, 2927065669.5250154, 1322.0, 8660.227316987874, 2779043781.142687, 928.0, 8010.165731114622, 3005643119.667818, 1720.0, 8500.729486348491, 2841811592.8494816, 1113.0]
[2019-03-27 07:07:10,954] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1250072: loss -169.3574
[2019-03-27 07:07:10,954] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1250072: learning rate 0.0000
[2019-03-27 07:07:11,319] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1250246: loss -51.0293
[2019-03-27 07:07:11,327] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1250247: learning rate 0.0000
[2019-03-27 07:07:11,458] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1250307: loss -185.2245
[2019-03-27 07:07:11,461] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1250308: learning rate 0.0000
[2019-03-27 07:07:11,544] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1250356: loss 6.6941
[2019-03-27 07:07:11,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1250358: learning rate 0.0000
[2019-03-27 07:07:11,598] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1250375: loss -52.2294
[2019-03-27 07:07:11,600] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1250376: learning rate 0.0000
[2019-03-27 07:07:12,293] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1250699: loss -197.4962
[2019-03-27 07:07:12,295] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1250699: learning rate 0.0000
[2019-03-27 07:07:12,581] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1873332e-15 1.0000000e+00 3.4132412e-20 1.8433310e-11 1.4817537e-23], sum to 1.0000
[2019-03-27 07:07:12,593] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5459
[2019-03-27 07:07:12,602] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 92.16666666666667, 1.0, 2.0, 0.772374062045634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1084727.429762709, 1084727.429762708, 237972.5326901757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7092600.0000, 
sim time next is 7093200.0000, 
raw observation next is [24.7, 92.33333333333334, 1.0, 2.0, 0.6809797864940342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 957410.1870051591, 957410.1870051591, 217556.9984728628], 
processed observation next is [1.0, 0.08695652173913043, 0.3696682464454976, 0.9233333333333335, 1.0, 1.0, 0.6156382969807641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26594727416809977, 0.26594727416809977, 0.3247119380191982], 
reward next is 0.6753, 
noisyNet noise sample is [array([-0.76276374], dtype=float32), 0.16182558]. 
=============================================
[2019-03-27 07:07:12,700] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1250891: loss -255.9077
[2019-03-27 07:07:12,705] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1250892: learning rate 0.0000
[2019-03-27 07:07:12,845] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6036565e-15 1.0000000e+00 7.3517244e-21 3.4670923e-12 2.7401090e-26], sum to 1.0000
[2019-03-27 07:07:12,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4077
[2019-03-27 07:07:12,856] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333333, 84.66666666666667, 1.0, 2.0, 0.4823736008480358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674034.3476372827, 674034.3476372827, 180880.7620085391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7588200.0000, 
sim time next is 7588800.0000, 
raw observation next is [26.2, 86.0, 1.0, 2.0, 0.4845946690601449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677138.8985563354, 677138.8985563348, 181217.898083836], 
processed observation next is [0.0, 0.8695652173913043, 0.44075829383886256, 0.86, 1.0, 1.0, 0.37902972175921074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18809413848787093, 0.18809413848787077, 0.270474474751994], 
reward next is 0.7295, 
noisyNet noise sample is [array([-0.29711178], dtype=float32), -1.1702158]. 
=============================================
[2019-03-27 07:07:14,107] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.7302023e-12 9.9999988e-01 1.2356873e-16 8.9651401e-08 6.6258395e-20], sum to 1.0000
[2019-03-27 07:07:14,114] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3602
[2019-03-27 07:07:14,121] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1735670.920083932 W.
[2019-03-27 07:07:14,127] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.7, 71.33333333333334, 1.0, 2.0, 0.4138415186835068, 1.0, 1.0, 0.4138415186835068, 1.0, 1.0, 0.6956038530387393, 6.9112, 6.9112, 170.5573041426782, 1735670.920083932, 1735670.920083932, 357230.368878237], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7118400.0000, 
sim time next is 7119000.0000, 
raw observation next is [27.8, 71.0, 1.0, 2.0, 0.4004596691585105, 1.0, 2.0, 0.4004596691585105, 1.0, 2.0, 0.6717821258549418, 6.9112, 6.9112, 170.5573041426782, 1679502.828748956, 1679502.828748956, 349728.0779763494], 
processed observation next is [1.0, 0.391304347826087, 0.5165876777251186, 0.71, 1.0, 1.0, 0.27766225199820543, 1.0, 1.0, 0.27766225199820543, 1.0, 1.0, 0.5997342998230997, 0.0, 0.0, 0.8375144448122397, 0.4665285635413767, 0.4665285635413767, 0.5219822059348498], 
reward next is 0.4780, 
noisyNet noise sample is [array([-2.3540435], dtype=float32), 1.3047415]. 
=============================================
[2019-03-27 07:07:14,138] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[59.698452]
 [66.239296]
 [66.78338 ]
 [66.94344 ]
 [67.26702 ]], R is [[54.84679413]
 [54.76514816]
 [54.21749878]
 [53.67532349]
 [53.50702286]].
[2019-03-27 07:07:16,685] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1252683: loss -259.8948
[2019-03-27 07:07:16,687] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1252683: learning rate 0.0000
[2019-03-27 07:07:20,612] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1254524: loss 0.0035
[2019-03-27 07:07:20,615] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1254525: learning rate 0.0000
[2019-03-27 07:07:26,157] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1257116: loss 0.0022
[2019-03-27 07:07:26,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1257116: learning rate 0.0000
[2019-03-27 07:07:26,558] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1257303: loss 0.0027
[2019-03-27 07:07:26,562] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1257305: learning rate 0.0000
[2019-03-27 07:07:26,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0928088e-17 1.0000000e+00 6.8344652e-22 9.0599376e-13 2.3758499e-25], sum to 1.0000
[2019-03-27 07:07:26,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5270
[2019-03-27 07:07:26,698] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 75.66666666666667, 1.0, 2.0, 0.382145913401736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577501.9245802921, 577501.9245802921, 172709.1732556759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7336200.0000, 
sim time next is 7336800.0000, 
raw observation next is [25.1, 76.0, 1.0, 2.0, 0.3816698610902023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576763.0394685502, 576763.0394685502, 172642.8671837856], 
processed observation next is [1.0, 0.9565217391304348, 0.38862559241706174, 0.76, 1.0, 1.0, 0.2550239290243401, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16021195540793062, 0.16021195540793062, 0.25767592116982924], 
reward next is 0.7423, 
noisyNet noise sample is [array([-0.9422681], dtype=float32), -0.7615795]. 
=============================================
[2019-03-27 07:07:27,033] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1257525: loss 0.0075
[2019-03-27 07:07:27,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1257528: learning rate 0.0000
[2019-03-27 07:07:27,718] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1257847: loss 0.0068
[2019-03-27 07:07:27,719] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1257847: learning rate 0.0000
[2019-03-27 07:07:27,733] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1257852: loss 0.0041
[2019-03-27 07:07:27,736] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1257853: learning rate 0.0000
[2019-03-27 07:07:27,813] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1257887: loss 0.0053
[2019-03-27 07:07:27,816] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1257889: learning rate 0.0000
[2019-03-27 07:07:27,986] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1257972: loss 0.0040
[2019-03-27 07:07:27,988] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1257974: learning rate 0.0000
[2019-03-27 07:07:28,135] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1258041: loss 0.0052
[2019-03-27 07:07:28,137] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1258041: learning rate 0.0000
[2019-03-27 07:07:28,525] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1258226: loss 0.0053
[2019-03-27 07:07:28,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1258226: learning rate 0.0000
[2019-03-27 07:07:28,671] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1258295: loss 0.0027
[2019-03-27 07:07:28,677] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1258296: learning rate 0.0000
[2019-03-27 07:07:28,798] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1258352: loss 0.0074
[2019-03-27 07:07:28,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1258352: learning rate 0.0000
[2019-03-27 07:07:28,851] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1258379: loss 0.0025
[2019-03-27 07:07:28,855] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1258380: learning rate 0.0000
[2019-03-27 07:07:29,630] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1258736: loss 0.0056
[2019-03-27 07:07:29,632] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1258736: learning rate 0.0000
[2019-03-27 07:07:30,847] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:07:30,848] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:07:30,925] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-03-27 07:07:31,865] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0526105e-15 1.0000000e+00 3.5748792e-19 5.5465376e-13 9.5389662e-25], sum to 1.0000
[2019-03-27 07:07:31,870] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3313
[2019-03-27 07:07:31,873] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 92.0, 1.0, 2.0, 0.3159488283097837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 498901.1413231689, 498901.1413231689, 166928.5734579208], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7434000.0000, 
sim time next is 7434600.0000, 
raw observation next is [21.31666666666667, 91.83333333333334, 1.0, 2.0, 0.315471054679055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 498204.7504799939, 498204.7504799945, 166877.7633131374], 
processed observation next is [0.0, 0.043478260869565216, 0.20932069510268583, 0.9183333333333334, 1.0, 1.0, 0.17526633093862048, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13839020846666497, 0.13839020846666514, 0.24907128852707075], 
reward next is 0.7509, 
noisyNet noise sample is [array([-0.85905695], dtype=float32), -0.15702052]. 
=============================================
[2019-03-27 07:07:34,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:07:34,485] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:07:34,556] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-03-27 07:07:36,682] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1262300: loss -382.4473
[2019-03-27 07:07:36,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1262300: learning rate 0.0000
[2019-03-27 07:07:37,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6299317e-17 1.0000000e+00 5.5778302e-22 3.7299358e-15 3.5499027e-26], sum to 1.0000
[2019-03-27 07:07:37,849] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7868
[2019-03-27 07:07:37,856] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 86.0, 1.0, 2.0, 0.4160183779387334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606298.3111822369, 606298.3111822363, 174703.335527657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7548000.0000, 
sim time next is 7548600.0000, 
raw observation next is [24.96666666666667, 85.0, 1.0, 2.0, 0.419905837964151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 609759.0852984514, 609759.0852984514, 174965.8218824987], 
processed observation next is [0.0, 0.34782608695652173, 0.3823064770932071, 0.85, 1.0, 1.0, 0.30109137104114575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16937752369401426, 0.16937752369401426, 0.2611430177350727], 
reward next is 0.7389, 
noisyNet noise sample is [array([-0.64551544], dtype=float32), -0.09534897]. 
=============================================
[2019-03-27 07:07:42,443] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1264999: loss -394.6154
[2019-03-27 07:07:42,446] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1264999: learning rate 0.0000
[2019-03-27 07:07:42,686] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1265109: loss -108.3111
[2019-03-27 07:07:42,687] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1265110: learning rate 0.0000
[2019-03-27 07:07:43,156] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5670001e-14 1.0000000e+00 1.4793177e-18 8.7630192e-10 9.9528072e-24], sum to 1.0000
[2019-03-27 07:07:43,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9954
[2019-03-27 07:07:43,171] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 65.33333333333333, 1.0, 2.0, 1.014968762314313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1418741.847979567, 1418741.847979567, 303516.420380849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7642200.0000, 
sim time next is 7642800.0000, 
raw observation next is [29.6, 64.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.492263709773956, 6.9112, 168.9101325565075, 1866256.975907177, 1454037.273292058, 311347.1315949328], 
processed observation next is [1.0, 0.4782608695652174, 0.6018957345971565, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.05810637097739564, 0.0, 0.8294260782457267, 0.5184047155297714, 0.4038992425811272, 0.4646972113357206], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3644217], dtype=float32), -0.43403956]. 
=============================================
[2019-03-27 07:07:43,384] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1265436: loss -524.3074
[2019-03-27 07:07:43,387] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1265436: learning rate 0.0000
[2019-03-27 07:07:43,958] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1265703: loss -379.9946
[2019-03-27 07:07:43,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1265705: learning rate 0.0000
[2019-03-27 07:07:43,966] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1265706: loss -472.4646
[2019-03-27 07:07:43,969] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1265706: learning rate 0.0000
[2019-03-27 07:07:44,015] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6761087e-09 8.3083767e-01 3.0028816e-11 1.6916233e-01 1.1851953e-15], sum to 1.0000
[2019-03-27 07:07:44,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9316
[2019-03-27 07:07:44,023] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.46666666666667, 62.0, 1.0, 2.0, 0.4957827092576555, 1.0, 2.0, 0.4957827092576555, 1.0, 1.0, 0.8408357516702555, 6.9112, 6.9112, 170.5573041426782, 2079669.690425897, 2079669.690425897, 408574.8437292991], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7656000.0000, 
sim time next is 7656600.0000, 
raw observation next is [30.33333333333333, 63.0, 1.0, 2.0, 0.7383953881723239, 1.0, 2.0, 0.7383953881723239, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2064893.751942997, 2064893.751942998, 390985.6159912484], 
processed observation next is [1.0, 0.6086956521739131, 0.6366508688783569, 0.63, 1.0, 1.0, 0.6848137206895468, 1.0, 1.0, 0.6848137206895468, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5735815977619436, 0.573581597761944, 0.5835606208824603], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00447316], dtype=float32), -0.45345113]. 
=============================================
[2019-03-27 07:07:44,089] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1265765: loss -246.2361
[2019-03-27 07:07:44,092] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1265766: learning rate 0.0000
[2019-03-27 07:07:44,174] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1265800: loss -488.9341
[2019-03-27 07:07:44,179] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1265800: learning rate 0.0000
[2019-03-27 07:07:44,254] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1265842: loss -263.9970
[2019-03-27 07:07:44,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1265842: learning rate 0.0000
[2019-03-27 07:07:44,628] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4474184e-15 1.0000000e+00 9.7346653e-20 6.3750979e-12 3.0370893e-25], sum to 1.0000
[2019-03-27 07:07:44,639] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7806
[2019-03-27 07:07:44,643] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 89.66666666666667, 1.0, 2.0, 0.5998932225081471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 838312.6562606614, 838312.656260662, 200718.202091726], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7791600.0000, 
sim time next is 7792200.0000, 
raw observation next is [25.55, 90.0, 1.0, 2.0, 0.5954824827290106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 832146.5131714323, 832146.5131714329, 199899.7811755465], 
processed observation next is [1.0, 0.17391304347826086, 0.40995260663507116, 0.9, 1.0, 1.0, 0.5126294972638682, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23115180921428674, 0.2311518092142869, 0.2983578823515619], 
reward next is 0.7016, 
noisyNet noise sample is [array([0.23341836], dtype=float32), -1.2776021]. 
=============================================
[2019-03-27 07:07:44,829] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1266107: loss -480.2165
[2019-03-27 07:07:44,831] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1266108: learning rate 0.0000
[2019-03-27 07:07:44,880] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1266131: loss -285.4239
[2019-03-27 07:07:44,885] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1266132: learning rate 0.0000
[2019-03-27 07:07:44,958] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1266165: loss -183.6528
[2019-03-27 07:07:44,961] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1266165: loss -483.4933
[2019-03-27 07:07:44,961] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1266165: learning rate 0.0000
[2019-03-27 07:07:44,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1266166: learning rate 0.0000
[2019-03-27 07:07:45,834] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1266576: loss -170.9811
[2019-03-27 07:07:45,836] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1266578: learning rate 0.0000
[2019-03-27 07:07:46,610] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.5809896e-15 1.0000000e+00 9.7760516e-19 2.5516861e-11 2.7967787e-24], sum to 1.0000
[2019-03-27 07:07:46,617] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8669
[2019-03-27 07:07:46,621] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 89.0, 1.0, 2.0, 0.5906521114172282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825393.7730333562, 825393.7730333562, 199011.0620205457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7713600.0000, 
sim time next is 7714200.0000, 
raw observation next is [26.25, 88.5, 1.0, 2.0, 0.6012335298748798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840186.3914389374, 840186.3914389374, 200969.1536822974], 
processed observation next is [1.0, 0.2608695652173913, 0.4431279620853081, 0.885, 1.0, 1.0, 0.5195584697287707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23338510873303817, 0.23338510873303817, 0.29995396071984687], 
reward next is 0.7000, 
noisyNet noise sample is [array([0.41132212], dtype=float32), 1.8500175]. 
=============================================
[2019-03-27 07:07:47,543] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.5143943e-14 1.0000000e+00 3.9235268e-19 4.2257273e-10 3.3683016e-23], sum to 1.0000
[2019-03-27 07:07:47,554] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3242
[2019-03-27 07:07:47,560] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 87.0, 1.0, 2.0, 0.6034012002840791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843216.7781832678, 843216.7781832678, 201374.9563591422], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7716000.0000, 
sim time next is 7716600.0000, 
raw observation next is [26.9, 86.5, 1.0, 2.0, 0.6148752916851967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859257.6147002022, 859257.6147002022, 203543.1267455017], 
processed observation next is [1.0, 0.30434782608695654, 0.4739336492890995, 0.865, 1.0, 1.0, 0.5359943273315623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23868267075005617, 0.23868267075005617, 0.3037957115604503], 
reward next is 0.6962, 
noisyNet noise sample is [array([1.0865872], dtype=float32), 0.27416793]. 
=============================================
[2019-03-27 07:07:48,669] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4462319e-09 1.0897763e-01 1.3261391e-11 8.9102232e-01 5.1007666e-17], sum to 1.0000
[2019-03-27 07:07:48,676] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0511
[2019-03-27 07:07:48,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2286011.308577651 W.
[2019-03-27 07:07:48,686] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.36666666666667, 63.66666666666667, 1.0, 2.0, 0.5449237330364575, 1.0, 2.0, 0.5449237330364575, 1.0, 1.0, 0.9429738290110352, 6.9112, 6.9112, 170.5573041426782, 2286011.308577651, 2286011.308577651, 446931.1351005548], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7731600.0000, 
sim time next is 7732200.0000, 
raw observation next is [31.43333333333333, 63.33333333333333, 1.0, 2.0, 0.8152441922033491, 1.0, 2.0, 0.8152441922033491, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2280016.895528396, 2280016.895528396, 427357.8131765554], 
processed observation next is [1.0, 0.4782608695652174, 0.6887835703001578, 0.6333333333333333, 1.0, 1.0, 0.7774026412088543, 1.0, 1.0, 0.7774026412088543, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6333380265356656, 0.6333380265356656, 0.6378474823530678], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8162371], dtype=float32), 2.0232816]. 
=============================================
[2019-03-27 07:07:50,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9288304e-15 1.0000000e+00 7.0441628e-20 1.1265226e-10 1.2543716e-23], sum to 1.0000
[2019-03-27 07:07:50,385] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8277
[2019-03-27 07:07:50,390] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 88.66666666666667, 1.0, 2.0, 0.5131699739753436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717081.5199289086, 717081.5199289093, 185685.245727635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7776600.0000, 
sim time next is 7777200.0000, 
raw observation next is [26.4, 88.33333333333334, 1.0, 2.0, 0.5117471010429623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715092.5896172201, 715092.5896172201, 185457.0375287665], 
processed observation next is [1.0, 0.0, 0.45023696682464454, 0.8833333333333334, 1.0, 1.0, 0.41174349523248466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1986368304492278, 0.1986368304492278, 0.27680154855039774], 
reward next is 0.7232, 
noisyNet noise sample is [array([-1.257846], dtype=float32), -0.105433255]. 
=============================================
[2019-03-27 07:07:50,690] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.6052380e-15 1.0000000e+00 5.2165771e-20 7.7663403e-11 1.9101790e-24], sum to 1.0000
[2019-03-27 07:07:50,699] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5975
[2019-03-27 07:07:50,708] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 85.66666666666667, 1.0, 2.0, 0.4970971705667859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694614.7366797907, 694614.7366797907, 183142.50619799], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7782000.0000, 
sim time next is 7782600.0000, 
raw observation next is [26.4, 85.33333333333334, 1.0, 2.0, 0.4953321174589059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692147.5503904558, 692147.5503904558, 182867.9319834963], 
processed observation next is [1.0, 0.043478260869565216, 0.45023696682464454, 0.8533333333333334, 1.0, 1.0, 0.39196640657699505, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19226320844179326, 0.19226320844179326, 0.2729372119156661], 
reward next is 0.7271, 
noisyNet noise sample is [array([1.6922696], dtype=float32), 1.2248425]. 
=============================================
[2019-03-27 07:07:54,401] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:07:54,402] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:07:54,480] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-03-27 07:07:59,722] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:07:59,723] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:07:59,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-03-27 07:07:59,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:07:59,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:00,038] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-03-27 07:08:00,642] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:00,643] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:00,666] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-03-27 07:08:01,019] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,019] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,056] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-03-27 07:08:01,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,093] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,114] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-03-27 07:08:01,133] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-03-27 07:08:01,158] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-03-27 07:08:01,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,271] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-03-27 07:08:01,458] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,459] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,474] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,477] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,478] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-03-27 07:08:01,478] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,477] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,513] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-03-27 07:08:01,546] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-03-27 07:08:01,566] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-03-27 07:08:01,622] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:08:01,622] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:01,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-03-27 07:08:02,283] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2460040e-07 9.9999213e-01 1.9650974e-09 7.6353354e-06 5.9032952e-11], sum to 1.0000
[2019-03-27 07:08:02,284] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9681
[2019-03-27 07:08:02,289] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 85.0, 1.0, 2.0, 0.2698905453920185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 439649.4341560845, 439649.4341560845, 162867.4804296704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7200.0000, 
sim time next is 7800.0000, 
raw observation next is [20.66666666666667, 85.00000000000001, 1.0, 2.0, 0.375413469154481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610838.981024439, 610838.9810244383, 175835.0637733366], 
processed observation next is [1.0, 0.08695652173913043, 0.17851500789889443, 0.8500000000000001, 1.0, 1.0, 0.24748610741503732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16967749472901084, 0.16967749472901064, 0.26244039369154715], 
reward next is 0.7376, 
noisyNet noise sample is [array([0.2877637], dtype=float32), -0.12298866]. 
=============================================
[2019-03-27 07:08:03,331] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 07:08:03,332] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:08:03,335] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:03,336] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:08:03,337] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:08:03,337] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:03,338] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:03,339] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:08:03,338] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:08:03,342] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:03,343] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:08:03,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run52
[2019-03-27 07:08:03,387] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run52
[2019-03-27 07:08:03,418] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run52
[2019-03-27 07:08:03,420] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run52
[2019-03-27 07:08:03,459] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run52
[2019-03-27 07:08:08,917] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:08:08,919] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.4, 80.0, 1.0, 2.0, 0.2965845280916127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473010.2810107688, 473010.2810107688, 165136.1381677796]
[2019-03-27 07:08:08,920] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:08:08,922] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.5468316e-17 1.0000000e+00 6.8503353e-22 1.9834161e-14 3.1079126e-26], sampled 0.5381730186355094
[2019-03-27 07:08:15,588] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:08:15,590] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.688446585, 87.734514495, 1.0, 2.0, 0.2792547050746392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 453137.8554830523, 453137.855483053, 163787.7695667776]
[2019-03-27 07:08:15,591] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:08:15,594] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1179657e-16 1.0000000e+00 8.4112192e-22 3.5826424e-14 3.7700689e-26], sampled 0.020161192913297876
[2019-03-27 07:08:22,327] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:08:22,328] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.585449635, 93.52851254500001, 1.0, 2.0, 0.6923259191712088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 967540.3123030136, 967540.3123030136, 219193.3238990263]
[2019-03-27 07:08:22,329] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:08:22,332] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.1239067e-16 1.0000000e+00 5.5607730e-21 4.0132974e-13 2.7230941e-25], sampled 0.8284139002276937
[2019-03-27 07:08:27,613] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:08:27,614] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.73333333333333, 92.33333333333333, 1.0, 2.0, 0.4984349527847013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729770.0722038952, 729770.0722038952, 187538.4603286207]
[2019-03-27 07:08:27,616] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:08:27,618] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8516425e-16 1.0000000e+00 1.9817264e-21 1.4613174e-13 6.1015938e-26], sampled 0.7272732651516041
[2019-03-27 07:08:33,145] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:08:33,147] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.9, 93.16666666666666, 1.0, 2.0, 0.4070651192346523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610809.207463132, 610809.2074631326, 175625.0624588548]
[2019-03-27 07:08:33,149] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:08:33,151] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.5986124e-17 1.0000000e+00 8.6898141e-22 1.6912630e-13 1.4292383e-26], sampled 0.531835842794607
[2019-03-27 07:09:06,599] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:09:06,599] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.7235118, 56.73429929, 1.0, 2.0, 0.9682466614017089, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129565051069, 1353391.21249599, 1353391.212495989, 289407.5720273331]
[2019-03-27 07:09:06,600] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:09:06,603] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.0026779e-12 9.9999738e-01 1.4681512e-15 2.6502632e-06 4.2490197e-20], sampled 0.005610374415801922
[2019-03-27 07:09:18,999] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:09:19,001] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.14819913, 60.77082846499999, 1.0, 2.0, 0.5306439512552416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741507.42220728, 741507.4222072794, 188535.4469809843]
[2019-03-27 07:09:19,002] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:09:19,004] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1115106e-16 1.0000000e+00 6.8443016e-22 1.1914010e-13 1.6925168e-26], sampled 0.8203934557998168
[2019-03-27 07:09:22,022] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:09:22,023] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.33333333333333, 85.83333333333334, 1.0, 2.0, 0.601075123746598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839964.9412154674, 839964.9412154674, 200946.6987901429]
[2019-03-27 07:09:22,025] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:09:22,027] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.1156323e-16 1.0000000e+00 2.7855713e-21 8.7150675e-13 6.0633556e-26], sampled 0.4114474509257783
[2019-03-27 07:09:27,307] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:09:27,308] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.42157446, 81.99031549333334, 1.0, 2.0, 0.5214971082513352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728721.4883634148, 728721.4883634154, 187032.289390056]
[2019-03-27 07:09:27,309] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:09:27,310] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.6552211e-17 1.0000000e+00 5.4749138e-22 1.5832707e-13 7.9238351e-27], sampled 0.2959960815377215
[2019-03-27 07:09:31,829] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:09:31,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.6, 86.0, 1.0, 2.0, 0.5387169063691704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752792.347454091, 752792.3474540904, 189884.8297704713]
[2019-03-27 07:09:31,832] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:09:31,836] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.0355747e-16 1.0000000e+00 6.3516198e-21 4.7008460e-12 5.7648094e-26], sampled 0.23351701246851775
[2019-03-27 07:09:44,150] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:09:44,151] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.61437369333333, 47.93677376333333, 1.0, 2.0, 0.2941059591441216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472761.1634864817, 472761.1634864817, 165151.9005492515]
[2019-03-27 07:09:44,152] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:09:44,159] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3741774e-16 1.0000000e+00 8.0939290e-22 1.8864369e-14 5.4326903e-26], sampled 0.3336107069913212
[2019-03-27 07:09:54,181] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:09:54,182] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.58021099, 85.87531301333334, 1.0, 2.0, 0.4262140130851096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 624917.9474849766, 624917.9474849766, 176592.3617253189]
[2019-03-27 07:09:54,183] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:09:54,184] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.2839606e-17 1.0000000e+00 4.5992030e-22 5.6598481e-14 1.1267455e-26], sampled 0.630613984262194
[2019-03-27 07:09:55,454] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03973988], dtype=float32), 0.045001745]
[2019-03-27 07:09:55,457] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.03333333333333, 72.5, 1.0, 2.0, 0.5831872229418436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 814958.127218924, 814958.1272189246, 197655.7144073787]
[2019-03-27 07:09:55,458] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:09:55,461] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1354542e-16 1.0000000e+00 7.6124815e-22 6.7242783e-13 9.4570001e-27], sampled 0.35581163146954087
[2019-03-27 07:09:58,979] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.4636 3007184385.9357 1756.0000
[2019-03-27 07:09:59,107] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.4299 2842472405.6962 1128.0000
[2019-03-27 07:09:59,133] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.6013 3163176845.2142 1743.0000
[2019-03-27 07:09:59,134] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8257.3304 2927149578.1086 1328.0000
[2019-03-27 07:09:59,192] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.4326 2779338026.3803 932.0000
[2019-03-27 07:10:00,210] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1275000, evaluation results [1275000.0, 7889.601304635499, 3163176845.214161, 1743.0, 8257.330441261483, 2927149578.1086273, 1328.0, 8659.432563696786, 2779338026.380326, 932.0, 8001.463561072642, 3007184385.935718, 1756.0, 8495.429887704422, 2842472405.6961746, 1128.0]
[2019-03-27 07:10:00,838] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0734247e-15 1.0000000e+00 4.5481896e-21 3.2157951e-13 7.3315691e-26], sum to 1.0000
[2019-03-27 07:10:00,840] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6780
[2019-03-27 07:10:00,846] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 86.0, 1.0, 2.0, 0.3241420795069231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518568.2885860677, 518568.2885860677, 168497.6843657705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 19200.0000, 
sim time next is 19800.0000, 
raw observation next is [21.5, 86.0, 1.0, 2.0, 0.3247484331166959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519190.7837457117, 519190.7837457111, 168544.2843618096], 
processed observation next is [1.0, 0.21739130434782608, 0.21800947867298584, 0.86, 1.0, 1.0, 0.18644389532132033, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1442196621515866, 0.1442196621515864, 0.25155863337583523], 
reward next is 0.7484, 
noisyNet noise sample is [array([0.48736218], dtype=float32), -1.3186616]. 
=============================================
[2019-03-27 07:10:02,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4844703e-14 1.0000000e+00 1.2294415e-19 1.1878825e-08 8.7591920e-25], sum to 1.0000
[2019-03-27 07:10:02,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3561
[2019-03-27 07:10:02,322] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 64.0, 1.0, 2.0, 0.8708015056141055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1315394.22188225, 1315394.221882251, 275750.6015969994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 45000.0000, 
sim time next is 45600.0000, 
raw observation next is [27.2, 63.66666666666667, 1.0, 2.0, 0.6924300270297054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044727.012058795, 1044727.012058795, 229092.3450298621], 
processed observation next is [1.0, 0.5217391304347826, 0.4881516587677725, 0.6366666666666667, 1.0, 1.0, 0.629433767505669, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29020194779410974, 0.29020194779410974, 0.34192887317889864], 
reward next is 0.6581, 
noisyNet noise sample is [array([-1.2991116], dtype=float32), 1.4001373]. 
=============================================
[2019-03-27 07:10:11,851] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.9704769e-17 1.0000000e+00 1.8088550e-21 4.0847982e-15 3.1783394e-26], sum to 1.0000
[2019-03-27 07:10:11,862] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4748
[2019-03-27 07:10:11,867] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.13333333333333, 86.0, 1.0, 2.0, 0.3189720881451341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502668.9189887386, 502668.9189887393, 167189.863854938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 224400.0000, 
sim time next is 225000.0000, 
raw observation next is [22.1, 86.0, 1.0, 2.0, 0.3176992795855395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501055.3737746762, 501055.3737746769, 167077.0104738689], 
processed observation next is [0.0, 0.6086956521739131, 0.24644549763033188, 0.86, 1.0, 1.0, 0.17795093925968614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13918204827074337, 0.13918204827074357, 0.24936867234905805], 
reward next is 0.7506, 
noisyNet noise sample is [array([-0.21340875], dtype=float32), 0.3046078]. 
=============================================
[2019-03-27 07:10:11,888] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[78.566795]
 [78.53696 ]
 [78.49455 ]
 [78.43912 ]
 [78.425514]], R is [[78.56697845]
 [78.53177643]
 [78.49676514]
 [78.46193695]
 [78.42739105]].
[2019-03-27 07:10:16,263] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2750514e-16 1.0000000e+00 2.2773374e-20 4.3279941e-13 9.6784694e-25], sum to 1.0000
[2019-03-27 07:10:16,270] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8775
[2019-03-27 07:10:16,275] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.11666666666667, 81.5, 1.0, 2.0, 0.2935981594479545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 469009.0832647583, 469009.0832647576, 164866.0181386909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 324600.0000, 
sim time next is 325200.0000, 
raw observation next is [22.03333333333333, 82.0, 1.0, 2.0, 0.2927995388173441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467884.1446575139, 467884.1446575139, 164789.5088517389], 
processed observation next is [0.0, 0.782608695652174, 0.2432859399684044, 0.82, 1.0, 1.0, 0.14795125158716158, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12996781796042053, 0.12996781796042053, 0.2459544908234909], 
reward next is 0.7540, 
noisyNet noise sample is [array([-1.0305272], dtype=float32), -0.402382]. 
=============================================
[2019-03-27 07:10:20,677] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4102784e-16 1.0000000e+00 2.4032290e-20 9.7575325e-12 2.5051318e-25], sum to 1.0000
[2019-03-27 07:10:20,685] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6630
[2019-03-27 07:10:20,690] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.3449124497226351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534645.4057453935, 534645.4057453941, 169474.8919889455], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 976800.0000, 
sim time next is 977400.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3399800326892392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527009.304289304, 527009.304289304, 168860.1313533416], 
processed observation next is [1.0, 0.30434782608695654, 0.23696682464454974, 0.93, 1.0, 1.0, 0.2047952201075171, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14639147341369554, 0.14639147341369554, 0.25203004679603225], 
reward next is 0.7480, 
noisyNet noise sample is [array([-0.3479765], dtype=float32), 0.934859]. 
=============================================
[2019-03-27 07:10:24,595] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1212489e-18 1.0000000e+00 5.4057263e-21 9.9022814e-14 8.0242762e-27], sum to 1.0000
[2019-03-27 07:10:24,602] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7275
[2019-03-27 07:10:24,608] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 96.33333333333333, 1.0, 2.0, 0.397315129270448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600258.6993975863, 600258.6993975863, 174760.7903989587], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1045200.0000, 
sim time next is 1045800.0000, 
raw observation next is [22.15, 96.5, 1.0, 2.0, 0.3606971938124785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547255.1984593577, 547255.1984593577, 170157.3581204329], 
processed observation next is [1.0, 0.08695652173913043, 0.24881516587677724, 0.965, 1.0, 1.0, 0.22975565519575722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15201533290537714, 0.15201533290537714, 0.25396620614989984], 
reward next is 0.7460, 
noisyNet noise sample is [array([-0.53761023], dtype=float32), -0.30733386]. 
=============================================
[2019-03-27 07:10:25,078] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2408274e-15 1.0000000e+00 7.8823197e-20 9.4633155e-12 3.1390720e-25], sum to 1.0000
[2019-03-27 07:10:25,086] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8788
[2019-03-27 07:10:25,093] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 81.66666666666667, 1.0, 2.0, 0.2308156578175492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380968.9817660752, 380968.9817660752, 159036.7521455919], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 454800.0000, 
sim time next is 455400.0000, 
raw observation next is [20.05, 81.5, 1.0, 2.0, 0.2323528972158815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383386.1076044721, 383386.1076044721, 159184.1578109189], 
processed observation next is [1.0, 0.2608695652173913, 0.14928909952606645, 0.815, 1.0, 1.0, 0.07512397254925482, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10649614100124224, 0.10649614100124224, 0.23758829524017747], 
reward next is 0.7624, 
noisyNet noise sample is [array([-1.2069001], dtype=float32), -0.44260314]. 
=============================================
[2019-03-27 07:10:48,161] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9553885e-16 1.0000000e+00 5.5149739e-21 1.9640138e-14 1.4139464e-25], sum to 1.0000
[2019-03-27 07:10:48,170] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0432
[2019-03-27 07:10:48,176] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 67.5, 1.0, 2.0, 0.311547813275998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491032.7704024446, 491032.7704024446, 166322.9622889166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 916200.0000, 
sim time next is 916800.0000, 
raw observation next is [24.8, 68.0, 1.0, 2.0, 0.3132407388071468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493350.3158975887, 493350.3158975881, 166485.9569494055], 
processed observation next is [0.0, 0.6086956521739131, 0.3744075829383887, 0.68, 1.0, 1.0, 0.17257920338210458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13704175441599686, 0.1370417544159967, 0.24848650290956042], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.603231], dtype=float32), -1.5609332]. 
=============================================
[2019-03-27 07:10:53,811] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 07:10:53,814] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:10:53,814] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:10:53,815] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:10:53,818] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:10:53,819] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:10:53,819] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:10:53,820] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:10:53,820] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:10:53,822] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:10:53,822] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:10:53,856] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run53
[2019-03-27 07:10:53,857] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run53
[2019-03-27 07:10:53,857] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run53
[2019-03-27 07:10:53,857] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run53
[2019-03-27 07:10:53,941] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run53
[2019-03-27 07:11:24,571] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:11:24,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.014708955, 88.17255373500001, 1.0, 2.0, 0.4232872097703195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625449.5315135666, 625449.5315135666, 176768.9077946348]
[2019-03-27 07:11:24,573] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:11:24,575] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.10805184e-16 1.00000000e+00 1.01898514e-21 3.05537354e-14
 3.42367636e-26], sampled 0.40376003904036173
[2019-03-27 07:11:25,295] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:11:25,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.33209577666667, 88.71340266166666, 1.0, 2.0, 0.3498934285173539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544920.7074019046, 544920.7074019052, 170377.996173106]
[2019-03-27 07:11:25,298] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:11:25,300] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0526863e-16 1.0000000e+00 8.0232864e-22 2.0020362e-14 2.9120945e-26], sampled 0.9817147888719426
[2019-03-27 07:11:33,804] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:11:33,806] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.33333333333333, 100.0, 1.0, 2.0, 0.4503384064312532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 643133.9493691779, 643133.9493691786, 177966.7730345486]
[2019-03-27 07:11:33,807] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:11:33,812] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.7774416e-17 1.0000000e+00 8.4872851e-22 2.5430628e-14 2.9753245e-26], sampled 0.8301878736332315
[2019-03-27 07:11:49,782] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:11:49,783] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.96666666666667, 80.5, 1.0, 2.0, 0.5694684406261039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795780.0299214703, 795780.0299214703, 195195.1282440634]
[2019-03-27 07:11:49,785] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:11:49,787] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.7321442e-17 1.0000000e+00 5.8842874e-22 2.5455731e-13 6.6797706e-27], sampled 0.05981574087206831
[2019-03-27 07:11:51,164] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:11:51,165] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.89773135, 74.17888741, 1.0, 2.0, 0.535036659489213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747647.8334087235, 747647.8334087235, 189266.7380120177]
[2019-03-27 07:11:51,166] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:11:51,171] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.92513494e-17 1.00000000e+00 2.48092287e-22 1.10062865e-13
 3.19922367e-27], sampled 0.025690380844032523
[2019-03-27 07:11:52,404] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:11:52,405] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.53333333333333, 72.0, 1.0, 2.0, 0.5709101492202295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797795.4423448507, 797795.4423448507, 195451.2429383395]
[2019-03-27 07:11:52,407] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:11:52,409] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4133671e-16 1.0000000e+00 1.8081859e-21 4.0487879e-13 4.8967123e-26], sampled 0.862874635113031
[2019-03-27 07:11:58,277] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:11:58,278] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.16666666666667, 59.83333333333333, 1.0, 2.0, 0.6964648996900635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 973327.2769809135, 973327.2769809135, 220089.3975259496]
[2019-03-27 07:11:58,279] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:11:58,280] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9896395e-16 1.0000000e+00 1.2898291e-21 8.9898141e-12 2.6477914e-27], sampled 0.8564521017919587
[2019-03-27 07:12:00,838] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:12:00,839] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 84.83333333333333, 1.0, 2.0, 0.5508464621512057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769748.0901731236, 769748.0901731236, 191946.0132456327]
[2019-03-27 07:12:00,840] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:12:00,842] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7818828e-16 1.0000000e+00 1.4860233e-21 3.2260853e-13 2.4655113e-26], sampled 0.49334853655478905
[2019-03-27 07:12:07,884] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:12:07,885] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 70.0, 1.0, 2.0, 0.5561092231212424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777104.9170712325, 777104.9170712325, 192852.4636751746]
[2019-03-27 07:12:07,886] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:12:07,893] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.4786015e-17 1.0000000e+00 4.8535398e-22 7.2106546e-14 9.5037971e-27], sampled 0.34476390363077813
[2019-03-27 07:12:14,312] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:12:14,313] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.38096975, 93.10074280666666, 1.0, 2.0, 0.8226098681244458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1149713.488484135, 1149713.488484135, 249541.7442182078]
[2019-03-27 07:12:14,314] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:12:14,316] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.6112331e-16 1.0000000e+00 6.3335463e-21 1.2135705e-12 9.5256520e-26], sampled 0.153883083362923
[2019-03-27 07:12:45,918] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03998709], dtype=float32), 0.045316704]
[2019-03-27 07:12:45,919] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.5, 56.5, 1.0, 2.0, 0.9785855128592008, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005992213904282, 6.9112, 168.9123159325646, 2265028.925765844, 2197780.321757465, 456780.2010932156]
[2019-03-27 07:12:45,920] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:12:45,922] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.6742500e-10 9.9832088e-01 3.5144484e-13 1.6791214e-03 9.5738670e-18], sampled 0.2769957510915141
[2019-03-27 07:12:45,925] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2265028.925765844 W.
[2019-03-27 07:12:48,906] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7887.8985 3163681038.4349 1763.0000
[2019-03-27 07:12:49,389] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.2134 2927266906.8667 1338.0000
[2019-03-27 07:12:49,567] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.0229 2842402756.2276 1128.0000
[2019-03-27 07:12:49,688] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.0207 3007802548.1446 1766.0000
[2019-03-27 07:12:49,760] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8895 2779340593.5932 933.0000
[2019-03-27 07:12:50,777] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1300000, evaluation results [1300000.0, 7887.898520956484, 3163681038.4348526, 1763.0, 8252.21335685056, 2927266906.866705, 1338.0, 8659.889468404313, 2779340593.593175, 933.0, 7997.020693217658, 3007802548.144633, 1766.0, 8498.022863885868, 2842402756.2275944, 1128.0]
[2019-03-27 07:12:57,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4477600e-16 1.0000000e+00 1.4503156e-21 1.9969078e-13 2.6342738e-25], sum to 1.0000
[2019-03-27 07:12:57,184] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5068
[2019-03-27 07:12:57,189] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.65, 95.66666666666666, 1.0, 2.0, 0.2741210631108689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 443848.4408780439, 443848.4408780439, 163186.3234409138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1140600.0000, 
sim time next is 1141200.0000, 
raw observation next is [19.6, 96.0, 1.0, 2.0, 0.2735505203460503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 443018.5352876315, 443018.5352876315, 163131.2002154956], 
processed observation next is [1.0, 0.21739130434782608, 0.127962085308057, 0.96, 1.0, 1.0, 0.1247596630675305, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1230607042465643, 0.1230607042465643, 0.24347940330670986], 
reward next is 0.7565, 
noisyNet noise sample is [array([-0.267205], dtype=float32), 0.13866358]. 
=============================================
[2019-03-27 07:13:09,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.9045809e-14 1.0000000e+00 2.3074298e-18 4.4658632e-10 4.5130493e-23], sum to 1.0000
[2019-03-27 07:13:09,566] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6529
[2019-03-27 07:13:09,574] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1686699.43611035 W.
[2019-03-27 07:13:09,578] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.3, 79.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.204829801506316, 6.9112, 168.9109808844479, 1686699.43611035, 1478390.785395132, 315270.6786723315], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1939200.0000, 
sim time next is 1939800.0000, 
raw observation next is [26.35, 79.16666666666667, 1.0, 2.0, 0.3681318060131081, 1.0, 1.0, 0.3681318060131081, 1.0, 1.0, 0.6155100934736064, 6.9112, 6.9112, 170.5573041426782, 1543824.136298463, 1543824.136298463, 332825.992531214], 
processed observation next is [1.0, 0.43478260869565216, 0.4478672985781992, 0.7916666666666667, 1.0, 1.0, 0.23871301929290129, 1.0, 0.5, 0.23871301929290129, 1.0, 0.5, 0.5311098700897638, 0.0, 0.0, 0.8375144448122397, 0.42884003786068414, 0.42884003786068414, 0.49675521273315526], 
reward next is 0.5032, 
noisyNet noise sample is [array([0.45958692], dtype=float32), -2.870423]. 
=============================================
[2019-03-27 07:13:12,209] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3603993e-16 1.0000000e+00 1.2007966e-19 2.1336837e-09 8.7195202e-25], sum to 1.0000
[2019-03-27 07:13:12,217] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6185
[2019-03-27 07:13:12,222] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 89.5, 1.0, 2.0, 0.6325786714045516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 994203.1120139954, 994203.1120139954, 220021.807208456], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1344600.0000, 
sim time next is 1345200.0000, 
raw observation next is [21.66666666666666, 89.0, 1.0, 2.0, 0.6739470771927084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1062920.317937728, 1062920.317937729, 229780.7504728547], 
processed observation next is [1.0, 0.5652173913043478, 0.22590837282780388, 0.89, 1.0, 1.0, 0.6071651532442269, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2952556438715911, 0.29525564387159137, 0.3429563439893354], 
reward next is 0.6570, 
noisyNet noise sample is [array([0.25695756], dtype=float32), -0.4472251]. 
=============================================
[2019-03-27 07:13:16,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1427170e-16 1.0000000e+00 7.7630093e-22 4.5977565e-14 7.9493618e-26], sum to 1.0000
[2019-03-27 07:13:16,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0205
[2019-03-27 07:13:16,743] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 88.5, 1.0, 2.0, 0.3895339520646209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 582382.9778227152, 582382.9778227152, 172958.5707820453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1420200.0000, 
sim time next is 1420800.0000, 
raw observation next is [23.46666666666667, 88.33333333333334, 1.0, 2.0, 0.3838733222462882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576914.3328353864, 576914.3328353864, 172560.3639169473], 
processed observation next is [0.0, 0.43478260869565216, 0.31121642969984215, 0.8833333333333334, 1.0, 1.0, 0.25767870150155203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16025398134316288, 0.16025398134316288, 0.257552781965593], 
reward next is 0.7424, 
noisyNet noise sample is [array([0.27829057], dtype=float32), 0.34131393]. 
=============================================
[2019-03-27 07:13:26,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6177143e-15 1.0000000e+00 5.3480137e-19 7.9058965e-13 2.1825678e-23], sum to 1.0000
[2019-03-27 07:13:26,678] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9913
[2019-03-27 07:13:26,682] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 85.33333333333334, 1.0, 2.0, 0.3917076470776573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602260.0155573708, 602260.0155573714, 175180.5040984568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1582800.0000, 
sim time next is 1583400.0000, 
raw observation next is [23.3, 85.16666666666667, 1.0, 2.0, 0.3847513131915307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590258.5987439593, 590258.5987439593, 174078.2045986252], 
processed observation next is [1.0, 0.30434782608695654, 0.3033175355450238, 0.8516666666666667, 1.0, 1.0, 0.2587365219175069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16396072187332203, 0.16396072187332203, 0.25981821581884357], 
reward next is 0.7402, 
noisyNet noise sample is [array([-0.65478134], dtype=float32), 0.6984547]. 
=============================================
[2019-03-27 07:13:28,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2333157e-15 1.0000000e+00 2.9138481e-20 3.3789957e-12 2.4262896e-24], sum to 1.0000
[2019-03-27 07:13:28,391] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6138
[2019-03-27 07:13:28,402] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.53333333333333, 98.66666666666669, 1.0, 2.0, 0.4825703655118966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689628.8592915381, 689628.8592915375, 182868.4920696368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1664400.0000, 
sim time next is 1665000.0000, 
raw observation next is [23.55, 98.5, 1.0, 2.0, 0.4780335080428666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683266.0980582122, 683266.0980582122, 182182.5091285533], 
processed observation next is [1.0, 0.2608695652173913, 0.3151658767772513, 0.985, 1.0, 1.0, 0.3711247084853814, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1897961383495034, 0.1897961383495034, 0.27191419272918405], 
reward next is 0.7281, 
noisyNet noise sample is [array([-1.3257515], dtype=float32), -0.52972394]. 
=============================================
[2019-03-27 07:13:28,416] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.6922  ]
 [70.673065]
 [70.77946 ]
 [70.6996  ]
 [70.72375 ]], R is [[70.79673004]
 [70.81581879]
 [70.82354736]
 [70.84559631]
 [70.86722565]].
[2019-03-27 07:13:31,628] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0826275e-16 1.0000000e+00 1.1430528e-20 1.3262228e-12 3.2566809e-25], sum to 1.0000
[2019-03-27 07:13:31,630] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9382
[2019-03-27 07:13:31,638] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 98.0, 1.0, 2.0, 0.4648490107739431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664753.0375699161, 664753.0375699154, 180221.710398359], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1666800.0000, 
sim time next is 1667400.0000, 
raw observation next is [23.61666666666667, 98.0, 1.0, 2.0, 0.4484092435607479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 640969.2873152549, 640969.2873152556, 177762.1315016774], 
processed observation next is [1.0, 0.30434782608695654, 0.31832543443917877, 0.98, 1.0, 1.0, 0.3354328235671662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17804702425423746, 0.17804702425423766, 0.26531661418160807], 
reward next is 0.7347, 
noisyNet noise sample is [array([0.03805003], dtype=float32), -0.26383018]. 
=============================================
[2019-03-27 07:13:33,317] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1285905e-09 3.6669070e-01 1.7937809e-12 6.3330925e-01 1.9099431e-16], sum to 1.0000
[2019-03-27 07:13:33,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6031
[2019-03-27 07:13:33,328] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.93333333333333, 63.66666666666667, 1.0, 2.0, 0.7050373766426945, 1.0, 2.0, 0.7050373766426945, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1971523.544275899, 1971523.544275899, 376291.0638598568], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2292000.0000, 
sim time next is 2292600.0000, 
raw observation next is [31.91666666666666, 63.83333333333334, 1.0, 2.0, 0.7316006623534088, 1.0, 2.0, 0.7316006623534088, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2045874.423516992, 2045874.423516992, 387948.283034664], 
processed observation next is [1.0, 0.5217391304347826, 0.7116903633491308, 0.6383333333333334, 1.0, 1.0, 0.6766273040402516, 1.0, 1.0, 0.6766273040402516, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5682984509769422, 0.5682984509769422, 0.5790272881114388], 
reward next is 0.4210, 
noisyNet noise sample is [array([1.2990853], dtype=float32), -0.46035257]. 
=============================================
[2019-03-27 07:13:44,181] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2385807e-14 1.0000000e+00 6.0129796e-19 8.6765226e-09 1.5239917e-23], sum to 1.0000
[2019-03-27 07:13:44,191] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7320
[2019-03-27 07:13:44,197] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 87.0, 1.0, 2.0, 0.4928728000745627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688709.9329947385, 688709.9329947378, 182486.8250618643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1881600.0000, 
sim time next is 1882200.0000, 
raw observation next is [25.98333333333333, 87.0, 1.0, 2.0, 0.4892254776866197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683611.7506314571, 683611.7506314571, 181925.0582542246], 
processed observation next is [1.0, 0.782608695652174, 0.43048973143759867, 0.87, 1.0, 1.0, 0.38460900926098757, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1898921529531825, 0.1898921529531825, 0.27152993769287254], 
reward next is 0.7285, 
noisyNet noise sample is [array([-0.2528886], dtype=float32), -0.5838203]. 
=============================================
[2019-03-27 07:13:44,272] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 07:13:44,273] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:13:44,274] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:13:44,275] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:13:44,275] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:13:44,276] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:13:44,277] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:13:44,278] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:13:44,276] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:13:44,281] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:13:44,281] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:13:44,311] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run54
[2019-03-27 07:13:44,332] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run54
[2019-03-27 07:13:44,333] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run54
[2019-03-27 07:13:44,370] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run54
[2019-03-27 07:13:44,372] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run54
[2019-03-27 07:13:48,470] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0406895], dtype=float32), 0.04491454]
[2019-03-27 07:13:48,472] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.48333333333333, 93.0, 1.0, 2.0, 0.292651773939553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469284.2277431712, 469284.2277431712, 164903.6525903517]
[2019-03-27 07:13:48,474] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:13:48,477] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6220019e-16 1.0000000e+00 1.5193785e-21 1.0644706e-14 1.5685926e-25], sampled 0.38292081417016177
[2019-03-27 07:14:15,495] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0406895], dtype=float32), 0.04491454]
[2019-03-27 07:14:15,497] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.52591276, 88.33356212, 1.0, 2.0, 0.3845151680517699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 579212.2291871327, 579212.2291871321, 172806.6381646923]
[2019-03-27 07:14:15,497] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:14:15,503] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0789689e-16 1.0000000e+00 2.3385923e-21 4.5797533e-14 1.6176999e-25], sampled 0.7805523418820459
[2019-03-27 07:14:17,973] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0406895], dtype=float32), 0.04491454]
[2019-03-27 07:14:17,976] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.76666666666667, 86.5, 1.0, 2.0, 0.4307970910670454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625282.1207388768, 625282.1207388768, 176457.5019747713]
[2019-03-27 07:14:17,978] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:14:17,982] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7955102e-16 1.0000000e+00 3.2132009e-21 6.8692434e-14 2.3433471e-25], sampled 0.37622784194854386
[2019-03-27 07:14:19,393] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0406895], dtype=float32), 0.04491454]
[2019-03-27 07:14:19,393] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.91884397666667, 76.09882610666668, 1.0, 2.0, 0.5130346715551881, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8938267514502097, 6.911200000000001, 6.9112, 168.9129268207165, 1533769.297145901, 1533769.297145901, 322984.4844762294]
[2019-03-27 07:14:19,394] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:14:19,396] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1207317e-11 9.9999869e-01 4.7791602e-15 1.3462919e-06 6.0150594e-19], sampled 0.1886022732316175
[2019-03-27 07:14:31,705] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0406895], dtype=float32), 0.04491454]
[2019-03-27 07:14:31,706] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.938743155, 98.111717955, 1.0, 2.0, 0.4413489659467371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645644.8878687974, 645644.887868798, 178597.7677560685]
[2019-03-27 07:14:31,708] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:14:31,710] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3170041e-16 1.0000000e+00 1.0218150e-21 1.7377420e-13 3.0527222e-26], sampled 0.11855856954353461
[2019-03-27 07:15:38,999] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0406895], dtype=float32), 0.04491454]
[2019-03-27 07:15:39,000] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.35979311, 87.73324866, 1.0, 2.0, 0.7284858556800761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1018098.821910484, 1018098.821910484, 227120.0096320916]
[2019-03-27 07:15:39,000] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:15:39,004] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.1364072e-16 1.0000000e+00 7.6662518e-21 7.6438563e-13 3.8915564e-25], sampled 0.15233698053243072
[2019-03-27 07:15:39,313] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.1390 3007030118.5413 1748.0000
[2019-03-27 07:15:39,446] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.2217 2927256665.5142 1334.0000
[2019-03-27 07:15:39,664] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.9182 2842069754.0588 1122.0000
[2019-03-27 07:15:39,886] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7896.3765 3162568330.1063 1732.0000
[2019-03-27 07:15:39,948] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.4643 2779425862.2467 933.0000
[2019-03-27 07:15:40,965] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1325000, evaluation results [1325000.0, 7896.376471602461, 3162568330.1063395, 1732.0, 8256.221704172687, 2927256665.5142117, 1334.0, 8657.464252102985, 2779425862.2467246, 933.0, 8000.138959836352, 3007030118.54127, 1748.0, 8500.91819096039, 2842069754.0588307, 1122.0]
[2019-03-27 07:15:41,501] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6828235e-15 1.0000000e+00 6.6194489e-20 1.5777376e-11 3.3955855e-24], sum to 1.0000
[2019-03-27 07:15:41,509] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3934
[2019-03-27 07:15:41,514] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 95.66666666666667, 1.0, 2.0, 0.4016734466723614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 597582.7548619639, 597582.7548619632, 174263.4205400444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1975200.0000, 
sim time next is 1975800.0000, 
raw observation next is [22.95, 95.83333333333333, 1.0, 2.0, 0.4047787068330637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600682.9922490107, 600682.9922490107, 174504.5394089108], 
processed observation next is [1.0, 0.8695652173913043, 0.28672985781990523, 0.9583333333333333, 1.0, 1.0, 0.2828659118470647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1668563867358363, 0.1668563867358363, 0.26045453643121014], 
reward next is 0.7395, 
noisyNet noise sample is [array([-0.25742543], dtype=float32), 1.2205321]. 
=============================================
[2019-03-27 07:15:41,530] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.1861125e-16 1.0000000e+00 2.4337476e-20 4.0379236e-12 1.1466355e-25], sum to 1.0000
[2019-03-27 07:15:41,539] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3396
[2019-03-27 07:15:41,543] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 92.0, 1.0, 2.0, 0.4313916223341231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632436.7785811314, 632436.778581132, 177323.0443746817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1920600.0000, 
sim time next is 1921200.0000, 
raw observation next is [23.86666666666667, 91.33333333333334, 1.0, 2.0, 0.4351395411081841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 637203.9112639973, 637203.9112639967, 177774.0840418001], 
processed observation next is [1.0, 0.21739130434782608, 0.33017377567140627, 0.9133333333333334, 1.0, 1.0, 0.31944523025082416, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17700108646222149, 0.17700108646222132, 0.2653344537937315], 
reward next is 0.7347, 
noisyNet noise sample is [array([0.43923992], dtype=float32), 1.0368162]. 
=============================================
[2019-03-27 07:15:48,801] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.3608800e-17 1.0000000e+00 8.9380329e-22 1.3000683e-15 1.6435557e-27], sum to 1.0000
[2019-03-27 07:15:48,811] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3684
[2019-03-27 07:15:48,819] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 84.0, 1.0, 2.0, 0.5168110363721877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722171.1124931996, 722171.1124931996, 186272.0921017974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2035800.0000, 
sim time next is 2036400.0000, 
raw observation next is [27.36666666666667, 83.33333333333334, 1.0, 2.0, 0.5168558437300147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722233.745791788, 722233.7457917886, 186279.3772949345], 
processed observation next is [0.0, 0.5652173913043478, 0.49605055292259104, 0.8333333333333335, 1.0, 1.0, 0.4178986069036321, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20062048494216334, 0.2006204849421635, 0.27802892133572316], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.8768743], dtype=float32), 0.88493603]. 
=============================================
[2019-03-27 07:16:12,002] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7579019e-13 1.0000000e+00 2.3804655e-19 5.2411980e-10 2.5013337e-23], sum to 1.0000
[2019-03-27 07:16:12,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4481
[2019-03-27 07:16:12,015] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 1.011368921892086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1413706.574780671, 1413706.574780671, 302408.7324246837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2437800.0000, 
sim time next is 2438400.0000, 
raw observation next is [27.6, 84.0, 1.0, 2.0, 0.8730031739614281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1220185.721320816, 1220185.721320815, 262619.1292893169], 
processed observation next is [1.0, 0.21739130434782608, 0.5071090047393366, 0.84, 1.0, 1.0, 0.8469917758571424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33894047814467115, 0.3389404781446708, 0.39196884968554757], 
reward next is 0.6080, 
noisyNet noise sample is [array([0.01774624], dtype=float32), 0.45745105]. 
=============================================
[2019-03-27 07:16:17,000] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.8045920e-16 1.0000000e+00 4.2504712e-21 8.0565736e-12 2.4758515e-25], sum to 1.0000
[2019-03-27 07:16:17,008] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6606
[2019-03-27 07:16:17,016] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 94.0, 1.0, 2.0, 0.551534643858489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770710.0982300511, 770710.0982300511, 192064.3922580315], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2503800.0000, 
sim time next is 2504400.0000, 
raw observation next is [26.73333333333333, 94.0, 1.0, 2.0, 0.5508428459820665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769743.0351386375, 769743.035138637, 191945.510242919], 
processed observation next is [1.0, 1.0, 0.4660347551342811, 0.94, 1.0, 1.0, 0.4588468023880318, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21381750976073263, 0.2138175097607325, 0.2864858361834612], 
reward next is 0.7135, 
noisyNet noise sample is [array([2.1743088], dtype=float32), -0.20243852]. 
=============================================
[2019-03-27 07:16:18,438] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8789729e-16 1.0000000e+00 4.4464235e-22 2.1966844e-15 7.6984651e-27], sum to 1.0000
[2019-03-27 07:16:18,443] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6680
[2019-03-27 07:16:18,448] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.08333333333334, 92.0, 1.0, 2.0, 0.4366324199952233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 631501.1169488276, 631501.1169488283, 177008.9383551589], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2603400.0000, 
sim time next is 2604000.0000, 
raw observation next is [24.06666666666667, 92.0, 1.0, 2.0, 0.4356342599288032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 630484.7907043479, 630484.7907043472, 176919.8375672042], 
processed observation next is [0.0, 0.13043478260869565, 0.3396524486571882, 0.92, 1.0, 1.0, 0.32004127702265445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1751346640845411, 0.1751346640845409, 0.26405945905552863], 
reward next is 0.7359, 
noisyNet noise sample is [array([-0.08982444], dtype=float32), 0.36657524]. 
=============================================
[2019-03-27 07:16:18,472] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.16437 ]
 [73.377014]
 [73.63106 ]
 [73.7053  ]
 [73.82027 ]], R is [[72.8447876 ]
 [72.85214996]
 [72.85925293]
 [72.86602783]
 [72.87250519]].
[2019-03-27 07:16:19,035] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0728475e-16 1.0000000e+00 5.0936336e-21 2.2337516e-11 2.2869350e-24], sum to 1.0000
[2019-03-27 07:16:19,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2044
[2019-03-27 07:16:19,046] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 90.0, 1.0, 2.0, 0.5143747832153845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718765.6378186536, 718765.6378186536, 185878.5886857517], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2586600.0000, 
sim time next is 2587200.0000, 
raw observation next is [26.0, 90.33333333333334, 1.0, 2.0, 0.5116815054405028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715000.8983977824, 715000.8983977824, 185446.2512504732], 
processed observation next is [1.0, 0.9565217391304348, 0.4312796208530806, 0.9033333333333334, 1.0, 1.0, 0.41166446438614795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19861136066605065, 0.19861136066605065, 0.27678544962757196], 
reward next is 0.7232, 
noisyNet noise sample is [array([0.66231894], dtype=float32), 0.81276155]. 
=============================================
[2019-03-27 07:16:20,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1128474e-15 1.0000000e+00 3.8207226e-21 7.2362434e-13 8.4908018e-25], sum to 1.0000
[2019-03-27 07:16:20,213] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0258
[2019-03-27 07:16:20,222] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5304637583312806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741255.5376612237, 741255.537661223, 188507.1194313451], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3236400.0000, 
sim time next is 3237000.0000, 
raw observation next is [30.33333333333333, 69.5, 1.0, 2.0, 0.5342622896260973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746565.3666367391, 746565.3666367391, 189139.0601715926], 
processed observation next is [0.0, 0.4782608695652174, 0.6366508688783569, 0.695, 1.0, 1.0, 0.4388702284651775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2073792685102053, 0.2073792685102053, 0.2822971047337203], 
reward next is 0.7177, 
noisyNet noise sample is [array([-1.2729827], dtype=float32), 0.15188335]. 
=============================================
[2019-03-27 07:16:20,242] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.036194]
 [73.96609 ]
 [73.93692 ]
 [73.90494 ]
 [73.8695  ]], R is [[74.05562592]
 [74.03371429]
 [74.0114975 ]
 [73.98911285]
 [73.96655273]].
[2019-03-27 07:16:20,687] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0122887e-16 1.0000000e+00 2.7462073e-20 2.7129562e-12 1.4059379e-25], sum to 1.0000
[2019-03-27 07:16:20,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4489
[2019-03-27 07:16:20,701] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 92.0, 1.0, 2.0, 0.4469728444415622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 641486.3640677447, 641486.3640677447, 177880.2673638058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2599800.0000, 
sim time next is 2600400.0000, 
raw observation next is [24.23333333333333, 92.0, 1.0, 2.0, 0.4449993894872299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639556.9271264095, 639556.9271264095, 177708.6451254884], 
processed observation next is [0.0, 0.08695652173913043, 0.3475513428120062, 0.92, 1.0, 1.0, 0.331324565647265, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17765470197955818, 0.17765470197955818, 0.2652367837693857], 
reward next is 0.7348, 
noisyNet noise sample is [array([1.1525774], dtype=float32), 0.03820352]. 
=============================================
[2019-03-27 07:16:34,357] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 07:16:34,359] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:16:34,360] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:16:34,361] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:16:34,363] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:16:34,361] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:16:34,365] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:16:34,364] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:16:34,365] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:16:34,369] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:16:34,366] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:16:34,392] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run55
[2019-03-27 07:16:34,392] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run55
[2019-03-27 07:16:34,433] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run55
[2019-03-27 07:16:34,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run55
[2019-03-27 07:16:34,455] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run55
[2019-03-27 07:16:36,588] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04233868], dtype=float32), 0.045118347]
[2019-03-27 07:16:36,589] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.57715988166667, 96.21165706166666, 1.0, 2.0, 0.400597496857408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600882.7368285238, 600882.7368285238, 174704.1734652188]
[2019-03-27 07:16:36,590] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:16:36,595] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.9080283e-17 1.0000000e+00 8.1319957e-22 9.4139425e-14 2.4052944e-26], sampled 0.04866086435769279
[2019-03-27 07:16:52,935] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04233868], dtype=float32), 0.045118347]
[2019-03-27 07:16:52,935] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.85422676, 91.30698187666667, 1.0, 2.0, 0.3766334741070064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572386.9311181757, 572386.9311181757, 172351.1949801698]
[2019-03-27 07:16:52,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:16:52,939] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6168587e-16 1.0000000e+00 3.3894307e-21 1.8100566e-13 2.0745785e-25], sampled 0.8125359389951589
[2019-03-27 07:17:30,930] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04233868], dtype=float32), 0.045118347]
[2019-03-27 07:17:30,931] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.13333333333334, 46.33333333333334, 1.0, 2.0, 0.6100073359703333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852452.1589516648, 852452.1589516648, 202624.0853641585]
[2019-03-27 07:17:30,933] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:17:30,936] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7921216e-16 1.0000000e+00 1.1082900e-21 2.8228350e-13 3.5267947e-26], sampled 0.09934511398801149
[2019-03-27 07:17:32,107] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04233868], dtype=float32), 0.045118347]
[2019-03-27 07:17:32,109] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.66666666666666, 70.0, 1.0, 2.0, 0.5378717009732003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751610.8562976753, 751610.8562976747, 189741.4910978748]
[2019-03-27 07:17:32,110] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:17:32,114] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6086862e-16 1.0000000e+00 3.0581964e-21 5.4825149e-13 1.2934776e-25], sampled 0.765511712069158
[2019-03-27 07:17:46,617] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04233868], dtype=float32), 0.045118347]
[2019-03-27 07:17:46,617] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.03773505833333, 62.86501884, 1.0, 2.0, 0.5969245351922842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 882800.3568090918, 882800.356809091, 206312.6031285016]
[2019-03-27 07:17:46,618] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:17:46,621] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.080916e-16 1.000000e+00 4.292871e-21 1.871994e-12 1.223608e-25], sampled 0.3260083546067033
[2019-03-27 07:18:22,408] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04233868], dtype=float32), 0.045118347]
[2019-03-27 07:18:22,486] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.98333333333333, 80.16666666666667, 1.0, 2.0, 0.5688512329958854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 794917.2160037027, 794917.216003702, 195085.7279174213]
[2019-03-27 07:18:22,486] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:18:22,487] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3108092e-16 1.0000000e+00 9.2777181e-22 5.5891446e-13 1.7412408e-26], sampled 0.9038605027880118
[2019-03-27 07:18:29,742] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7900.0781 3162215488.1119 1732.0000
[2019-03-27 07:18:30,017] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.3442 2927340228.9014 1327.0000
[2019-03-27 07:18:30,160] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.8705 2841931780.7951 1121.0000
[2019-03-27 07:18:30,258] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.7920 2779111684.8043 932.0000
[2019-03-27 07:18:30,338] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.2935 3007229411.6355 1755.0000
[2019-03-27 07:18:31,356] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1350000, evaluation results [1350000.0, 7900.078081811109, 3162215488.111872, 1732.0, 8258.344167976886, 2927340228.9014077, 1327.0, 8661.79198888932, 2779111684.804315, 932.0, 8000.293490717236, 3007229411.635463, 1755.0, 8498.870546238582, 2841931780.7950788, 1121.0]
[2019-03-27 07:18:33,652] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2710598e-17 1.0000000e+00 3.9829817e-22 2.0868423e-13 4.3676355e-27], sum to 1.0000
[2019-03-27 07:18:33,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9360
[2019-03-27 07:18:33,665] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3837926002778032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 578065.3288533741, 578065.3288533748, 172702.1588169946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3103200.0000, 
sim time next is 3103800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3841203323757227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578558.9167785364, 578558.9167785364, 172746.2695860706], 
processed observation next is [1.0, 0.9565217391304348, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2579763040671358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1607108102162601, 0.1607108102162601, 0.2578302531135382], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.23651573], dtype=float32), 0.8618523]. 
=============================================
[2019-03-27 07:18:41,279] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6058585e-17 1.0000000e+00 1.7766985e-22 1.3481050e-12 7.1680979e-27], sum to 1.0000
[2019-03-27 07:18:41,287] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4390
[2019-03-27 07:18:41,293] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3081319549091588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490679.3759790361, 490679.3759790355, 166394.001582187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3005400.0000, 
sim time next is 3006000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3081003722989929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490629.5866254861, 490629.5866254867, 166390.3722322918], 
processed observation next is [1.0, 0.8260869565217391, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16638599072167817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13628599628485724, 0.13628599628485744, 0.2483438391526743], 
reward next is 0.7517, 
noisyNet noise sample is [array([0.50741726], dtype=float32), -0.4230406]. 
=============================================
[2019-03-27 07:18:41,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.736595]
 [74.75219 ]
 [74.82331 ]
 [74.846504]
 [74.816444]], R is [[74.76993561]
 [74.77388763]
 [74.7776413 ]
 [74.7811203 ]
 [74.78445435]].
[2019-03-27 07:18:46,611] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.7682265e-15 1.0000000e+00 3.9467020e-20 5.0689650e-11 8.1647143e-25], sum to 1.0000
[2019-03-27 07:18:46,618] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6989
[2019-03-27 07:18:46,623] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 97.0, 1.0, 2.0, 0.3947736472976615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591416.8127952688, 591416.8127952688, 173817.2642209453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3097800.0000, 
sim time next is 3098400.0000, 
raw observation next is [22.33333333333334, 98.0, 1.0, 2.0, 0.3925455952621777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589003.3438173933, 589003.3438173933, 173623.5774825238], 
processed observation next is [1.0, 0.8695652173913043, 0.2575039494470777, 0.98, 1.0, 1.0, 0.268127223207443, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16361203994927592, 0.16361203994927592, 0.2591396678843639], 
reward next is 0.7409, 
noisyNet noise sample is [array([-0.44959942], dtype=float32), 0.68363357]. 
=============================================
[2019-03-27 07:18:46,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9348053e-15 1.0000000e+00 1.4827455e-20 1.6790958e-11 2.1065249e-25], sum to 1.0000
[2019-03-27 07:18:46,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4914
[2019-03-27 07:18:46,813] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3837926002778032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 578065.3288533741, 578065.3288533748, 172702.1588169946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3103200.0000, 
sim time next is 3103800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3841203323757227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578558.9167785364, 578558.9167785364, 172746.2695860706], 
processed observation next is [1.0, 0.9565217391304348, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2579763040671358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1607108102162601, 0.1607108102162601, 0.2578302531135382], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.5589547], dtype=float32), -0.6232895]. 
=============================================
[2019-03-27 07:18:54,036] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1940825e-18 1.0000000e+00 1.2542998e-23 8.6698464e-15 1.6452189e-26], sum to 1.0000
[2019-03-27 07:18:54,052] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8274
[2019-03-27 07:18:54,055] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.16666666666666, 65.16666666666667, 1.0, 2.0, 0.5640217650058872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788165.9753220618, 788165.9753220612, 194235.5952034292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3869400.0000, 
sim time next is 3870000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5719960276763714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 799313.430501277, 799313.4305012776, 195645.613357322], 
processed observation next is [0.0, 0.8260869565217391, 0.7156398104265403, 0.67, 1.0, 1.0, 0.4843325634655076, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22203150847257694, 0.2220315084725771, 0.2920083781452567], 
reward next is 0.7080, 
noisyNet noise sample is [array([-0.9284901], dtype=float32), 0.7432165]. 
=============================================
[2019-03-27 07:18:54,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.632744]
 [76.63062 ]
 [76.62515 ]
 [76.56499 ]
 [76.52564 ]], R is [[76.64826965]
 [76.59188843]
 [76.53784943]
 [76.48584747]
 [76.43585205]].
[2019-03-27 07:18:54,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1916379e-16 1.0000000e+00 4.4234869e-20 1.3732913e-13 4.7301148e-26], sum to 1.0000
[2019-03-27 07:18:54,591] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9532
[2019-03-27 07:18:54,597] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.5817942227358855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 813010.7743443921, 813010.7743443914, 197403.8150358533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3250800.0000, 
sim time next is 3251400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.5912967650527855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826294.9804577488, 826294.9804577488, 199135.6507573434], 
processed observation next is [0.0, 0.6521739130434783, 0.7630331753554502, 0.63, 1.0, 1.0, 0.5075864639190186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22952638346048576, 0.22952638346048576, 0.29721738919006474], 
reward next is 0.7028, 
noisyNet noise sample is [array([-1.5302426], dtype=float32), 1.6628984]. 
=============================================
[2019-03-27 07:18:55,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2947844e-15 1.0000000e+00 6.3749958e-21 4.1267770e-13 3.9129024e-25], sum to 1.0000
[2019-03-27 07:18:55,620] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6585
[2019-03-27 07:18:55,628] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 71.33333333333333, 1.0, 2.0, 0.4935120880577622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689603.5243930613, 689603.5243930606, 182585.9867746901], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3314400.0000, 
sim time next is 3315000.0000, 
raw observation next is [28.83333333333334, 70.66666666666667, 1.0, 2.0, 0.495279537346645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692074.0541447345, 692074.0541447351, 182859.9811153516], 
processed observation next is [0.0, 0.34782608695652173, 0.5655608214849924, 0.7066666666666667, 1.0, 1.0, 0.39190305704415057, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19224279281798182, 0.19224279281798198, 0.27292534494828596], 
reward next is 0.7271, 
noisyNet noise sample is [array([1.1807097], dtype=float32), -0.64595354]. 
=============================================
[2019-03-27 07:18:55,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.4239  ]
 [73.425545]
 [73.42501 ]
 [73.429115]
 [73.42196 ]], R is [[73.41066742]
 [73.4040451 ]
 [73.39781189]
 [73.39202881]
 [73.38690186]].
[2019-03-27 07:19:04,378] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.2696948e-12 5.3820621e-02 1.7554240e-13 9.4617939e-01 1.5334633e-17], sum to 1.0000
[2019-03-27 07:19:04,386] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2172
[2019-03-27 07:19:04,392] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.83333333333334, 63.66666666666666, 1.0, 2.0, 0.8999315569909953, 1.0, 2.0, 0.8999315569909953, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2517103.026857565, 2517103.026857565, 471464.5171236144], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3419400.0000, 
sim time next is 3420000.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.943719211903714, 1.0, 2.0, 0.943719211903714, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2639706.204313108, 2639706.204313108, 495913.998404043], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.63, 1.0, 1.0, 0.9321918215707398, 1.0, 1.0, 0.9321918215707398, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7332517234203079, 0.7332517234203079, 0.740170146871706], 
reward next is 0.2598, 
noisyNet noise sample is [array([1.400372], dtype=float32), 0.4218]. 
=============================================
[2019-03-27 07:19:04,406] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[49.029907]
 [49.032303]
 [51.71714 ]
 [52.353855]
 [55.957188]], R is [[49.28166199]
 [48.78884506]
 [48.30095673]
 [47.81794739]
 [47.62666321]].
[2019-03-27 07:19:07,893] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9023888e-09 7.9379302e-01 2.0732712e-11 2.0620698e-01 3.5998907e-17], sum to 1.0000
[2019-03-27 07:19:07,903] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8506
[2019-03-27 07:19:07,907] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.7717142940305638, 1.0, 2.0, 0.7717142940305638, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2158162.626062531, 2158162.626062531, 406342.3405739905], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3500400.0000, 
sim time next is 3501000.0000, 
raw observation next is [32.5, 65.0, 1.0, 2.0, 0.7474018530282578, 1.0, 2.0, 0.7474018530282578, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2090104.548366769, 2090104.548366769, 395084.3960812973], 
processed observation next is [1.0, 0.5217391304347826, 0.7393364928909952, 0.65, 1.0, 1.0, 0.6956648831665756, 1.0, 1.0, 0.6956648831665756, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5805845967685469, 0.5805845967685469, 0.589678203106414], 
reward next is 0.4103, 
noisyNet noise sample is [array([-0.9564242], dtype=float32), 1.21979]. 
=============================================
[2019-03-27 07:19:07,921] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[50.576473]
 [48.992065]
 [47.55516 ]
 [45.931557]
 [44.90182 ]], R is [[52.20841599]
 [52.07985306]
 [51.88061142]
 [51.72702408]
 [51.53459167]].
[2019-03-27 07:19:08,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7455344e-15 1.0000000e+00 2.4639298e-20 3.6329653e-10 1.3876103e-23], sum to 1.0000
[2019-03-27 07:19:08,702] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7863
[2019-03-27 07:19:08,710] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.23333333333333, 87.83333333333334, 1.0, 2.0, 0.7821619180971322, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1093152.665600299, 1093152.665600299, 239579.602343137], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4074600.0000, 
sim time next is 4075200.0000, 
raw observation next is [27.2, 88.0, 1.0, 2.0, 0.7677713513770087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1073030.189535858, 1073030.189535858, 236153.9268283316], 
processed observation next is [1.0, 0.17391304347826086, 0.4881516587677725, 0.88, 1.0, 1.0, 0.7202064474421791, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29806394153773835, 0.29806394153773835, 0.35246854750497253], 
reward next is 0.6475, 
noisyNet noise sample is [array([0.87271976], dtype=float32), -0.057986915]. 
=============================================
[2019-03-27 07:19:09,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8880690e-10 1.3899484e-01 4.0404472e-13 8.6100519e-01 5.4995834e-17], sum to 1.0000
[2019-03-27 07:19:09,873] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9307
[2019-03-27 07:19:09,877] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 60.33333333333333, 1.0, 2.0, 0.9100467081500011, 1.0, 2.0, 0.9100467081500011, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2545423.860344616, 2545423.860344616, 477002.7119030033], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3512400.0000, 
sim time next is 3513000.0000, 
raw observation next is [33.0, 59.66666666666667, 1.0, 2.0, 0.8971526002260612, 1.0, 2.0, 0.8971526002260612, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2509322.497074763, 2509322.497074763, 469940.3092123429], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.5966666666666667, 1.0, 1.0, 0.876087470151881, 1.0, 1.0, 0.876087470151881, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6970340269652119, 0.6970340269652119, 0.7014034465855864], 
reward next is 0.2986, 
noisyNet noise sample is [array([-0.29205644], dtype=float32), 0.08848126]. 
=============================================
[2019-03-27 07:19:09,902] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[48.77504 ]
 [48.499058]
 [51.714672]
 [51.497974]
 [51.4581  ]], R is [[48.92300415]
 [48.43377304]
 [47.94951248]
 [47.76789093]
 [47.59654617]].
[2019-03-27 07:19:16,994] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.8567764e-15 1.0000000e+00 6.6402887e-20 2.2007832e-10 3.2081272e-23], sum to 1.0000
[2019-03-27 07:19:17,005] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0307
[2019-03-27 07:19:17,013] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 79.0, 1.0, 2.0, 0.5000131816004845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698690.7412866844, 698690.7412866838, 183598.1716212607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3630600.0000, 
sim time next is 3631200.0000, 
raw observation next is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5019401521394582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701384.2721856466, 701384.2721856473, 183900.7382228879], 
processed observation next is [1.0, 0.0, 0.4944707740916275, 0.8066666666666668, 1.0, 1.0, 0.3999278941439255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19482896449601295, 0.19482896449601314, 0.27447871376550437], 
reward next is 0.7255, 
noisyNet noise sample is [array([0.53971094], dtype=float32), 0.12745956]. 
=============================================
[2019-03-27 07:19:17,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6279387e-16 1.0000000e+00 4.8713148e-20 9.8086517e-10 1.5305344e-23], sum to 1.0000
[2019-03-27 07:19:17,552] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6613
[2019-03-27 07:19:17,557] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.7964846343115369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1138471.644683996, 1138471.644683996, 246585.3304880439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3726600.0000, 
sim time next is 3727200.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.6683239317910767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 955188.7983335488, 955188.7983335488, 216955.5882730162], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.74, 1.0, 1.0, 0.6003902792663575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26533022175931914, 0.26533022175931914, 0.3238143108552481], 
reward next is 0.6762, 
noisyNet noise sample is [array([0.23714504], dtype=float32), 1.6826361]. 
=============================================
[2019-03-27 07:19:19,949] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0689054e-15 1.0000000e+00 1.3232258e-20 5.4633915e-09 1.0478183e-24], sum to 1.0000
[2019-03-27 07:19:19,958] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4019
[2019-03-27 07:19:19,966] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6231511112308057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870827.4017313664, 870827.4017313664, 205138.6838726971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4320000.0000, 
sim time next is 4320600.0000, 
raw observation next is [30.83333333333334, 79.83333333333334, 1.0, 2.0, 0.6263210589470625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 875259.0952998261, 875259.0952998255, 205752.1866650157], 
processed observation next is [1.0, 0.0, 0.6603475513428123, 0.7983333333333335, 1.0, 1.0, 0.5497844083699548, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24312752647217392, 0.24312752647217376, 0.30709281591793386], 
reward next is 0.6929, 
noisyNet noise sample is [array([-0.17416829], dtype=float32), 0.17426403]. 
=============================================
[2019-03-27 07:19:20,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.11133365e-14 1.00000000e+00 1.78987885e-18 9.09078501e-09
 8.83307869e-25], sum to 1.0000
[2019-03-27 07:19:20,308] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9388
[2019-03-27 07:19:20,314] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 77.33333333333334, 1.0, 2.0, 0.5458690742630228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762790.2339132758, 762790.2339132765, 191094.7013062788], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3698400.0000, 
sim time next is 3699000.0000, 
raw observation next is [29.0, 76.5, 1.0, 2.0, 0.5413980553911583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756540.2662723038, 756540.2662723038, 190336.320302138], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.765, 1.0, 1.0, 0.4474675366158533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2101500739645288, 0.2101500739645288, 0.28408406015244475], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.27851966], dtype=float32), 0.39072528]. 
=============================================
[2019-03-27 07:19:20,329] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[71.729866]
 [72.3871  ]
 [73.26947 ]
 [74.13663 ]
 [74.45953 ]], R is [[70.97848511]
 [70.98348236]
 [70.98775482]
 [70.99140167]
 [70.99459076]].
[2019-03-27 07:19:23,723] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2477641e-08 1.6376574e-02 1.9112964e-10 9.8362339e-01 2.9357506e-15], sum to 1.0000
[2019-03-27 07:19:23,731] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5763
[2019-03-27 07:19:23,735] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 64.5, 1.0, 2.0, 0.5589447621238094, 1.0, 2.0, 0.5589447621238094, 1.0, 1.0, 0.9644648498652566, 6.9112, 6.9112, 170.5573041426782, 2344886.156263719, 2344886.156263719, 457024.5400415739], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3753000.0000, 
sim time next is 3753600.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.8369497962701871, 1.0, 2.0, 0.8369497962701871, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2340778.434990686, 2340778.434990686, 438256.2838337697], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.65, 1.0, 1.0, 0.803553971409864, 1.0, 1.0, 0.803553971409864, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6502162319418572, 0.6502162319418572, 0.654113856468313], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46263936], dtype=float32), 2.1059697]. 
=============================================
[2019-03-27 07:19:24,611] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 07:19:24,612] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:19:24,613] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:19:24,613] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:19:24,616] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:19:24,616] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:19:24,616] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:19:24,619] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:19:24,617] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:19:24,623] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:19:24,623] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:19:24,641] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run56
[2019-03-27 07:19:24,660] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run56
[2019-03-27 07:19:24,677] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run56
[2019-03-27 07:19:24,699] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run56
[2019-03-27 07:19:24,714] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run56
[2019-03-27 07:19:27,289] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04477241], dtype=float32), 0.04456902]
[2019-03-27 07:19:27,291] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.32443549, 89.22694610666667, 1.0, 2.0, 0.3653483199029631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595082.6172447444, 595082.6172447439, 174488.4740934294]
[2019-03-27 07:19:27,292] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:19:27,295] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1266969e-15 1.0000000e+00 4.1043122e-20 1.3665428e-12 6.2750737e-24], sampled 0.18997016397903543
[2019-03-27 07:19:32,772] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04477241], dtype=float32), 0.04456902]
[2019-03-27 07:19:32,774] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.1, 74.0, 1.0, 2.0, 0.3092063059719858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494550.422632436, 494550.422632436, 166699.2367125796]
[2019-03-27 07:19:32,775] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:19:32,778] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.8347063e-16 1.0000000e+00 5.8508060e-21 4.6707245e-13 4.7121255e-25], sampled 0.22499914123153897
[2019-03-27 07:19:36,368] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04477241], dtype=float32), 0.04456902]
[2019-03-27 07:19:36,368] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.12962185833334, 47.70421348666666, 1.0, 2.0, 0.2886266076206411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478400.6593656272, 478400.6593656266, 164809.6992990009]
[2019-03-27 07:19:36,369] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:19:36,370] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2117738e-15 1.0000000e+00 2.0401542e-20 2.5427731e-13 3.9115763e-24], sampled 0.16864734597568842
[2019-03-27 07:20:05,914] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04477241], dtype=float32), 0.04456902]
[2019-03-27 07:20:05,915] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.51666666666667, 89.5, 1.0, 2.0, 0.5687146820371805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 794726.3272274671, 794726.3272274664, 195061.4106616343]
[2019-03-27 07:20:05,918] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:20:05,922] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.2848964e-15 1.0000000e+00 3.1301955e-20 1.2208006e-11 2.3771020e-24], sampled 0.1523720075934425
[2019-03-27 07:20:51,490] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04477241], dtype=float32), 0.04456902]
[2019-03-27 07:20:51,493] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.65, 95.5, 1.0, 2.0, 0.6471841503213586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 904426.8603967883, 904426.8603967878, 209857.000412857]
[2019-03-27 07:20:51,496] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:20:51,498] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.9329138e-15 1.0000000e+00 4.3219896e-20 1.0531168e-11 3.9248189e-24], sampled 0.4063055427640073
[2019-03-27 07:20:55,416] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04477241], dtype=float32), 0.04456902]
[2019-03-27 07:20:55,417] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.0, 55.0, 1.0, 2.0, 1.027534841303548, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9127497457395, 1436318.826822501, 1436318.826822501, 307425.3418530109]
[2019-03-27 07:20:55,420] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:20:55,423] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.90993488e-11 9.99892235e-01 8.16424988e-15 1.07727436e-04
 6.70974533e-20], sampled 0.47524879678945664
[2019-03-27 07:21:20,096] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8024.8997 3003767765.9686 1669.0000
[2019-03-27 07:21:20,103] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7102 2779002543.0763 925.0000
[2019-03-27 07:21:20,156] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8512.8870 2840648649.1717 1093.0000
[2019-03-27 07:21:20,208] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8265.3102 2926034534.3269 1304.0000
[2019-03-27 07:21:20,277] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7924.8874 3158674968.5194 1631.0000
[2019-03-27 07:21:21,292] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1375000, evaluation results [1375000.0, 7924.887388078854, 3158674968.5193973, 1631.0, 8265.310226285845, 2926034534.326866, 1304.0, 8660.7102367472, 2779002543.076273, 925.0, 8024.899677015328, 3003767765.968645, 1669.0, 8512.886992617097, 2840648649.17172, 1093.0]
[2019-03-27 07:21:24,137] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3381129e-16 1.0000000e+00 8.0812533e-21 2.7456479e-12 2.7488312e-25], sum to 1.0000
[2019-03-27 07:21:24,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3035
[2019-03-27 07:21:24,151] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 69.5, 1.0, 2.0, 0.5714166052600571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798503.4350052854, 798503.4350052854, 195542.0834691583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3834600.0000, 
sim time next is 3835200.0000, 
raw observation next is [31.66666666666667, 69.0, 1.0, 2.0, 0.5746693473168272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803050.5684547573, 803050.5684547573, 196122.4919113029], 
processed observation next is [0.0, 0.391304347826087, 0.6998420221169038, 0.69, 1.0, 1.0, 0.4875534305022014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2230696023485437, 0.2230696023485437, 0.2927201371810491], 
reward next is 0.7073, 
noisyNet noise sample is [array([-0.265019], dtype=float32), 0.28297573]. 
=============================================
[2019-03-27 07:21:29,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.5974405e-17 1.0000000e+00 3.2443375e-21 5.9598979e-11 5.5733126e-25], sum to 1.0000
[2019-03-27 07:21:29,672] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0646
[2019-03-27 07:21:29,676] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5833665816437785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 815208.863036261, 815208.8630362618, 197687.9080803803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4143000.0000, 
sim time next is 4143600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5832576327811173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 815056.5571648519, 815056.5571648526, 197668.1565032419], 
processed observation next is [1.0, 1.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4979007623868883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22640459921245887, 0.22640459921245906, 0.29502709925856996], 
reward next is 0.7050, 
noisyNet noise sample is [array([-0.38984782], dtype=float32), -1.1455028]. 
=============================================
[2019-03-27 07:21:30,598] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9607584e-15 1.0000000e+00 1.2617147e-22 5.2140323e-12 3.3635730e-25], sum to 1.0000
[2019-03-27 07:21:30,606] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1386
[2019-03-27 07:21:30,613] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 73.0, 1.0, 2.0, 0.6058632231405846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846658.677897004, 846658.677897004, 201843.3974250862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3961800.0000, 
sim time next is 3962400.0000, 
raw observation next is [31.33333333333333, 73.66666666666667, 1.0, 2.0, 0.6045686963531289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844848.9323900266, 844848.9323900266, 201600.19291027], 
processed observation next is [0.0, 0.8695652173913043, 0.6840442338072668, 0.7366666666666667, 1.0, 1.0, 0.5235767425941312, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2346802589972296, 0.2346802589972296, 0.30089581031383583], 
reward next is 0.6991, 
noisyNet noise sample is [array([-0.2777294], dtype=float32), 0.30949253]. 
=============================================
[2019-03-27 07:21:33,976] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.5020705e-11 3.2247942e-02 4.5766945e-12 9.6775204e-01 6.6519346e-18], sum to 1.0000
[2019-03-27 07:21:33,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5202
[2019-03-27 07:21:33,985] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.069319666334734, 6.9112, 170.5573041426782, 3022729.232454103, 2909461.683561737, 552826.0180626631], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4621800.0000, 
sim time next is 4622400.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.138381729149775, 6.9112, 170.5573041426782, 3072258.816954618, 2909519.303202736, 552462.1315056359], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.71, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.02271817291497751, 0.0, 0.8375144448122397, 0.8534052269318383, 0.8081998064452045, 0.8245703455307999], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.74765325], dtype=float32), -1.4072909]. 
=============================================
[2019-03-27 07:21:36,747] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.6701064e-16 1.0000000e+00 7.0979483e-20 1.0674377e-09 2.5950464e-25], sum to 1.0000
[2019-03-27 07:21:36,756] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7581
[2019-03-27 07:21:36,761] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5430151983184947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758800.8409369349, 758800.8409369349, 190610.3056436918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4050000.0000, 
sim time next is 4050600.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5435752090157918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759583.6708194437, 759583.6708194437, 190705.1947949817], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4500906132720383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21099546411651213, 0.21099546411651213, 0.2846346190969876], 
reward next is 0.7154, 
noisyNet noise sample is [array([0.6905293], dtype=float32), 1.5019228]. 
=============================================
[2019-03-27 07:21:38,573] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3413018e-14 1.0000000e+00 8.3958210e-17 6.5778483e-10 2.2030869e-23], sum to 1.0000
[2019-03-27 07:21:38,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1481
[2019-03-27 07:21:38,592] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 87.33333333333334, 1.0, 2.0, 0.760833118357168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1063328.523854169, 1063328.52385417, 234525.1928203088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4088400.0000, 
sim time next is 4089000.0000, 
raw observation next is [27.83333333333334, 85.66666666666666, 1.0, 2.0, 0.7594872664644381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1061446.642131792, 1061446.642131793, 234210.5042758792], 
processed observation next is [1.0, 0.30434782608695654, 0.5181674565560824, 0.8566666666666666, 1.0, 1.0, 0.7102256222463109, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2948462894810533, 0.2948462894810536, 0.3495679168296705], 
reward next is 0.6504, 
noisyNet noise sample is [array([-0.5269009], dtype=float32), -0.62315077]. 
=============================================
[2019-03-27 07:21:38,603] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[65.18872]
 [65.04482]
 [64.89102]
 [64.6992 ]
 [64.69614]], R is [[65.35173798]
 [65.34818268]
 [65.34088898]
 [65.33042145]
 [65.29902649]].
[2019-03-27 07:21:45,268] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4622218e-12 9.9998069e-01 9.4414432e-16 1.9355322e-05 1.8498186e-18], sum to 1.0000
[2019-03-27 07:21:45,275] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0144
[2019-03-27 07:21:45,279] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.66666666666666, 1.0, 2.0, 0.5685495654897653, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9861907414439958, 6.911200000000001, 6.9112, 168.9129565104293, 1589585.732118708, 1589585.732118707, 347588.6809246272], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4243800.0000, 
sim time next is 4244400.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.440011452030399, 6.9112, 168.9099495112963, 1829162.24713929, 1454011.8813528, 311352.6768372117], 
processed observation next is [1.0, 0.13043478260869565, 0.6208530805687204, 0.75, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0528811452030399, 0.0, 0.8294251794099412, 0.5081006242053583, 0.40389218926466663, 0.4647054878167339], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2043535], dtype=float32), -0.6758353]. 
=============================================
[2019-03-27 07:21:50,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7932093e-14 1.0000000e+00 1.2604137e-18 3.3711003e-10 5.1015844e-22], sum to 1.0000
[2019-03-27 07:21:50,456] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8797
[2019-03-27 07:21:50,460] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 84.0, 1.0, 2.0, 0.6866836397159045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959651.5431896322, 959651.5431896322, 217996.7302538599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4865400.0000, 
sim time next is 4866000.0000, 
raw observation next is [27.66666666666666, 82.33333333333334, 1.0, 2.0, 0.6701306356375218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 936508.2514518601, 936508.2514518608, 214529.4205903352], 
processed observation next is [1.0, 0.30434782608695654, 0.5102685624012636, 0.8233333333333335, 1.0, 1.0, 0.6025670308885805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26014118095885, 0.2601411809588502, 0.3201931650602018], 
reward next is 0.6798, 
noisyNet noise sample is [array([0.59917927], dtype=float32), 1.0620397]. 
=============================================
[2019-03-27 07:21:50,474] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[64.14404 ]
 [64.05168 ]
 [63.97275 ]
 [63.910168]
 [63.716084]], R is [[64.2268219 ]
 [64.25918579]
 [64.28984833]
 [64.31303406]
 [64.34458923]].
[2019-03-27 07:21:54,520] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8099824e-10 8.2665530e-04 8.8367169e-12 9.9917334e-01 3.2495615e-17], sum to 1.0000
[2019-03-27 07:21:54,532] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4414
[2019-03-27 07:21:54,540] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3386457.680771934 W.
[2019-03-27 07:21:54,545] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.83333333333334, 55.0, 1.0, 2.0, 0.9725909753011139, 1.0, 2.0, 0.8068855271648194, 1.0, 2.0, 1.03, 7.005119235551717, 6.9112, 170.5573041426782, 3386457.680771934, 3319179.511955997, 621268.7409310858], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4362600.0000, 
sim time next is 4363200.0000, 
raw observation next is [37.0, 54.0, 1.0, 2.0, 0.9717108608162636, 1.0, 2.0, 0.8064454699223945, 1.0, 2.0, 1.03, 7.005119166091982, 6.9112, 170.5573041426782, 3384608.279666219, 3317330.160607117, 620901.349591307], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.54, 1.0, 1.0, 0.9659166997786308, 1.0, 1.0, 0.7668017709908368, 1.0, 1.0, 1.0365853658536586, 0.009391916609198159, 0.0, 0.8375144448122397, 0.9401689665739497, 0.9214806001686436, 0.9267184322258313], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0474392], dtype=float32), 0.2555255]. 
=============================================
[2019-03-27 07:21:55,489] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0745642e-14 1.0000000e+00 8.0601104e-19 3.1996739e-09 6.0550148e-25], sum to 1.0000
[2019-03-27 07:21:55,497] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8827
[2019-03-27 07:21:55,501] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 76.33333333333334, 1.0, 2.0, 0.5887426322615759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822724.3800377763, 822724.3800377763, 198668.8642275119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4395000.0000, 
sim time next is 4395600.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6045746050301529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844857.1927000735, 844857.1927000735, 201602.8412600016], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5235838614821119, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23468255352779818, 0.23468255352779818, 0.30089976307462923], 
reward next is 0.6991, 
noisyNet noise sample is [array([0.6762506], dtype=float32), -0.44487855]. 
=============================================
[2019-03-27 07:21:56,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6104925e-14 1.0000000e+00 1.4732974e-18 2.6798260e-08 1.2592378e-23], sum to 1.0000
[2019-03-27 07:21:56,403] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2757
[2019-03-27 07:21:56,407] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6045746050301529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844857.1927000735, 844857.1927000735, 201602.8412600016], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4395600.0000, 
sim time next is 4396200.0000, 
raw observation next is [31.0, 79.00000000000001, 1.0, 2.0, 0.6144071240269208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858603.1092581487, 858603.1092581487, 203461.6278172622], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.7900000000000001, 1.0, 1.0, 0.5354302699119527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2385008636828191, 0.2385008636828191, 0.30367407136904806], 
reward next is 0.6963, 
noisyNet noise sample is [array([-0.98415816], dtype=float32), -0.1810239]. 
=============================================
[2019-03-27 07:21:56,608] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2313317e-17 1.0000000e+00 1.1939941e-21 9.8574935e-13 2.6030065e-26], sum to 1.0000
[2019-03-27 07:21:56,618] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5134
[2019-03-27 07:21:56,625] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 79.83333333333334, 1.0, 2.0, 0.6225070610073073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 869927.0001083423, 869927.0001083417, 205014.3152212159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4399800.0000, 
sim time next is 4400400.0000, 
raw observation next is [30.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6216299483060438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868700.7713449328, 868700.7713449328, 204845.1600819521], 
processed observation next is [1.0, 0.9565217391304348, 0.6524486571879939, 0.8066666666666668, 1.0, 1.0, 0.5441324678386069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2413057698180369, 0.2413057698180369, 0.305739044898436], 
reward next is 0.6943, 
noisyNet noise sample is [array([1.2385375], dtype=float32), 0.76510066]. 
=============================================
[2019-03-27 07:22:12,324] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9515063e-10 9.9046409e-01 3.1729784e-13 9.5358975e-03 1.8100545e-17], sum to 1.0000
[2019-03-27 07:22:12,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1132
[2019-03-27 07:22:12,339] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2226871.187871293 W.
[2019-03-27 07:22:12,344] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.75, 64.0, 1.0, 2.0, 0.9513227095052318, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.996370698086491, 6.9112, 168.9123790702199, 2226871.187871293, 2166448.369859679, 449053.2967057716], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4890600.0000, 
sim time next is 4891200.0000, 
raw observation next is [31.66666666666667, 64.33333333333333, 1.0, 2.0, 1.026889281242994, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.996111786016369, 6.9112, 168.9123824790817, 2332639.039339304, 2272399.900617673, 472065.2561584919], 
processed observation next is [1.0, 0.6086956521739131, 0.6998420221169038, 0.6433333333333333, 1.0, 1.0, 1.0323967243891494, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008491178601636928, 0.0, 0.829437126395585, 0.6479552887053622, 0.6312221946160202, 0.704575009191779], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.02336644], dtype=float32), -1.4623575]. 
=============================================
[2019-03-27 07:22:14,931] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 07:22:14,932] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:22:14,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:22:14,935] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:22:14,936] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:22:14,937] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:22:14,937] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:22:14,938] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:22:14,940] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:22:14,941] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:22:14,941] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:22:14,964] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run57
[2019-03-27 07:22:14,985] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run57
[2019-03-27 07:22:15,006] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run57
[2019-03-27 07:22:15,027] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run57
[2019-03-27 07:22:15,029] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run57
[2019-03-27 07:23:00,862] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04768478], dtype=float32), 0.044674832]
[2019-03-27 07:23:00,863] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.87897001, 90.409521075, 1.0, 2.0, 0.5546509847580839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832353.6405442435, 832353.6405442435, 199684.2402424063]
[2019-03-27 07:23:00,864] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:23:00,866] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6979013e-16 1.0000000e+00 4.8526724e-21 2.9439634e-13 4.9583305e-25], sampled 0.20762929462051039
[2019-03-27 07:23:02,427] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04768478], dtype=float32), 0.044674832]
[2019-03-27 07:23:02,429] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 94.0, 1.0, 2.0, 0.4904074582857703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685263.9058920406, 685263.9058920412, 182106.5468497977]
[2019-03-27 07:23:02,432] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:23:02,435] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.1460214e-16 1.0000000e+00 6.6280363e-21 1.7259538e-12 3.8521651e-25], sampled 0.12982184342966718
[2019-03-27 07:23:17,665] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04768478], dtype=float32), 0.044674832]
[2019-03-27 07:23:17,665] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.6041477358526768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844260.4312314629, 844260.4312314629, 201521.4886482221]
[2019-03-27 07:23:17,667] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:23:17,670] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1630312e-16 1.0000000e+00 1.6621418e-21 1.2801002e-12 8.0719588e-26], sampled 0.43277450691640396
[2019-03-27 07:23:17,801] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04768478], dtype=float32), 0.044674832]
[2019-03-27 07:23:17,803] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.05, 46.5, 1.0, 2.0, 0.9966643674052785, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005993182044262, 6.9112, 168.9123159285791, 2290333.215164462, 2223083.924328452, 462274.8927526411]
[2019-03-27 07:23:17,804] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:23:17,808] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.0465541e-10 9.8828959e-01 7.0146661e-13 1.1710414e-02 1.3322640e-17], sampled 0.24633940461484882
[2019-03-27 07:23:17,809] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2290333.215164462 W.
[2019-03-27 07:23:22,341] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04768478], dtype=float32), 0.044674832]
[2019-03-27 07:23:22,342] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.3, 51.16666666666667, 1.0, 2.0, 0.6408220111901413, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104128, 895532.1502377595, 895532.1502377595, 208593.495660047]
[2019-03-27 07:23:22,343] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:23:22,348] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.1608734e-10 9.9544191e-01 3.1715765e-13 4.5580864e-03 5.5512525e-18], sampled 0.6667568696855688
[2019-03-27 07:23:28,037] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04768478], dtype=float32), 0.044674832]
[2019-03-27 07:23:28,038] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.9, 66.33333333333334, 1.0, 2.0, 0.5461517771726321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763185.4211434185, 763185.4211434192, 191142.7111682151]
[2019-03-27 07:23:28,041] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:23:28,044] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1812117e-15 1.0000000e+00 4.8029321e-20 5.4026304e-11 1.1428238e-24], sampled 0.5519317136297375
[2019-03-27 07:23:52,912] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04768478], dtype=float32), 0.044674832]
[2019-03-27 07:23:52,914] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.91666666666667, 89.5, 1.0, 2.0, 0.4981941303632086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696148.0668973145, 696148.0668973145, 183313.6742048307]
[2019-03-27 07:23:52,915] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:23:52,918] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.253187e-16 1.000000e+00 7.270260e-21 1.467950e-12 5.300007e-25], sampled 0.7612896473869795
[2019-03-27 07:24:02,015] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04768478], dtype=float32), 0.044674832]
[2019-03-27 07:24:02,018] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.20513236, 81.07597712500001, 1.0, 2.0, 0.3293370873767582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515779.7092576185, 515779.709257619, 168119.433628116]
[2019-03-27 07:24:02,019] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:24:02,023] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.6789463e-16 1.0000000e+00 3.3378499e-21 3.5905506e-14 6.5112905e-25], sampled 0.25505204198215314
[2019-03-27 07:24:10,232] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8005.6460 3007077237.7978 1750.0000
[2019-03-27 07:24:10,328] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8257.3835 2927294197.8973 1332.0000
[2019-03-27 07:24:10,675] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.4423 2779255930.2783 934.0000
[2019-03-27 07:24:10,721] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.4067 2842199806.2541 1124.0000
[2019-03-27 07:24:10,755] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7895.2687 3162875031.4036 1740.0000
[2019-03-27 07:24:11,772] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1400000, evaluation results [1400000.0, 7895.2686522900485, 3162875031.403597, 1740.0, 8257.383546850691, 2927294197.897335, 1332.0, 8658.44227604626, 2779255930.2783456, 934.0, 8005.646011265923, 3007077237.797823, 1750.0, 8495.406669609984, 2842199806.2541246, 1124.0]
[2019-03-27 07:24:12,330] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7793165e-09 9.5197582e-01 1.6580736e-12 4.8024155e-02 2.5189091e-15], sum to 1.0000
[2019-03-27 07:24:12,339] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1412
[2019-03-27 07:24:12,348] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2100844.686584969 W.
[2019-03-27 07:24:12,359] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.8612829151407196, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.003850200397178, 6.9112, 168.9124051182332, 2100844.686584969, 2035115.660213119, 423614.6742074292], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4701000.0000, 
sim time next is 4701600.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.7154209825585162, 1.0, 1.0, 0.7154209825585162, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2000586.737742974, 2000586.737742974, 380799.9003600642], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.75, 1.0, 1.0, 0.6571337139259231, 1.0, 0.5, 0.6571337139259231, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5557185382619372, 0.5557185382619372, 0.5683580602389018], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0165905], dtype=float32), -2.5507452]. 
=============================================
[2019-03-27 07:24:19,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8156254e-13 9.9999988e-01 6.9692233e-18 1.5617927e-07 2.8007102e-22], sum to 1.0000
[2019-03-27 07:24:19,642] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3286
[2019-03-27 07:24:19,651] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1711522.787558335 W.
[2019-03-27 07:24:19,654] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.21666666666667, 75.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.274298604456446, 6.9112, 168.9108517438647, 1711522.787558335, 1453931.350466198, 311355.9389236551], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5472600.0000, 
sim time next is 5473200.0000, 
raw observation next is [31.43333333333333, 74.66666666666667, 1.0, 2.0, 0.797206759725118, 1.0, 1.0, 0.797206759725118, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2229526.092305091, 2229526.092305091, 418524.9650235623], 
processed observation next is [1.0, 0.34782608695652173, 0.6887835703001578, 0.7466666666666667, 1.0, 1.0, 0.7556707948495398, 1.0, 0.5, 0.7556707948495398, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6193128034180809, 0.6193128034180809, 0.6246641269008393], 
reward next is 0.3753, 
noisyNet noise sample is [array([-0.34983465], dtype=float32), -0.016082667]. 
=============================================
[2019-03-27 07:24:22,710] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4443662e-14 1.0000000e+00 3.4288381e-19 1.0304215e-08 4.4960524e-24], sum to 1.0000
[2019-03-27 07:24:22,719] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8068
[2019-03-27 07:24:22,725] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 77.33333333333333, 1.0, 2.0, 0.4870278561144585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680539.9540463309, 680539.9540463302, 181588.3701370411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4912800.0000, 
sim time next is 4913400.0000, 
raw observation next is [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.4854182362938947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678290.0615507917, 678290.0615507917, 181342.7604716004], 
processed observation next is [1.0, 0.8695652173913043, 0.4865718799368086, 0.7816666666666667, 1.0, 1.0, 0.3800219714384273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18841390598633104, 0.18841390598633104, 0.27066083652477674], 
reward next is 0.7293, 
noisyNet noise sample is [array([-0.41359654], dtype=float32), -1.1055963]. 
=============================================
[2019-03-27 07:24:29,726] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7914580e-18 1.0000000e+00 4.9961926e-24 1.5489840e-15 6.9756253e-28], sum to 1.0000
[2019-03-27 07:24:29,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9003
[2019-03-27 07:24:29,740] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.516654614022118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721952.4598670278, 721952.4598670271, 186246.2210037227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5097000.0000, 
sim time next is 5097600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5183930867223268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724382.5602774939, 724382.5602774945, 186527.5229780296], 
processed observation next is [0.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41975070689436955, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2012173778548594, 0.20121737785485957, 0.27839928802690983], 
reward next is 0.7216, 
noisyNet noise sample is [array([0.7261798], dtype=float32), 0.26326427]. 
=============================================
[2019-03-27 07:24:33,758] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0579479e-16 1.0000000e+00 5.2865842e-23 5.6921609e-15 4.1446086e-26], sum to 1.0000
[2019-03-27 07:24:33,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3170
[2019-03-27 07:24:33,776] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 87.33333333333334, 1.0, 2.0, 0.4991461055401657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 697478.7394321432, 697478.7394321426, 183461.8952438011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5106000.0000, 
sim time next is 5106600.0000, 
raw observation next is [26.0, 86.5, 1.0, 2.0, 0.4955808352433317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692495.2070658989, 692495.2070658989, 182906.0157560403], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.865, 1.0, 1.0, 0.392266066558231, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19235977974052748, 0.19235977974052748, 0.2729940533672243], 
reward next is 0.7270, 
noisyNet noise sample is [array([1.4828131], dtype=float32), -0.09800436]. 
=============================================
[2019-03-27 07:24:35,167] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3508255e-16 1.0000000e+00 2.5196265e-22 7.2607486e-15 1.4644670e-25], sum to 1.0000
[2019-03-27 07:24:35,177] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7863
[2019-03-27 07:24:35,181] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.553675118974134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773702.2703814851, 773702.2703814845, 192433.2994364631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5144400.0000, 
sim time next is 5145000.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5584796310012539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780418.5328980722, 780418.5328980722, 193265.5700643767], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.63, 1.0, 1.0, 0.46804774819428174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21678292580502004, 0.21678292580502004, 0.28845607472295026], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.44376418], dtype=float32), 0.30920807]. 
=============================================
[2019-03-27 07:24:35,194] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.56204]
 [73.46587]
 [73.41813]
 [73.36895]
 [73.30504]], R is [[73.59423065]
 [73.57107544]
 [73.54719543]
 [73.52262878]
 [73.49733734]].
[2019-03-27 07:24:36,182] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6731497e-15 1.0000000e+00 3.5125716e-21 3.0458096e-13 7.6012980e-25], sum to 1.0000
[2019-03-27 07:24:36,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0572
[2019-03-27 07:24:36,196] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 72.66666666666667, 1.0, 2.0, 0.5234998566177778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731521.0212447378, 731521.0212447371, 187359.871395574], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5127000.0000, 
sim time next is 5127600.0000, 
raw observation next is [29.33333333333334, 71.33333333333334, 1.0, 2.0, 0.5221001399677294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729564.4327851983, 729564.4327851989, 187131.0297538437], 
processed observation next is [0.0, 0.34782608695652173, 0.5892575039494474, 0.7133333333333334, 1.0, 1.0, 0.42421703610569805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2026567868847773, 0.20265678688477748, 0.2793000444087219], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.5228865], dtype=float32), -0.12000384]. 
=============================================
[2019-03-27 07:24:36,458] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7724420e-16 1.0000000e+00 9.9696178e-21 4.6672982e-12 2.6853916e-24], sum to 1.0000
[2019-03-27 07:24:36,465] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7652
[2019-03-27 07:24:36,469] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.7720453625760281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079006.543133964, 1079006.543133964, 237162.2853708779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5199600.0000, 
sim time next is 5200200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.7644274144580308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1068354.383069317, 1068354.383069318, 235364.0281585114], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.89, 1.0, 1.0, 0.7161776077807599, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29676510640814363, 0.2967651064081439, 0.35128959426643497], 
reward next is 0.6487, 
noisyNet noise sample is [array([0.25335437], dtype=float32), 1.4053112]. 
=============================================
[2019-03-27 07:24:44,785] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5405475e-15 1.0000000e+00 3.0181675e-20 2.0636465e-10 2.4959376e-24], sum to 1.0000
[2019-03-27 07:24:44,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7431
[2019-03-27 07:24:44,794] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.8453244220013031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1181477.948575945, 1181477.948575945, 255344.3761271651], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5287800.0000, 
sim time next is 5288400.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.8386196192004669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1172101.727168837, 1172101.727168838, 253615.2669527114], 
processed observation next is [1.0, 0.21739130434782608, 0.5545023696682465, 0.88, 1.0, 1.0, 0.8055658062656228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32558381310245477, 0.325583813102455, 0.3785302491831513], 
reward next is 0.6215, 
noisyNet noise sample is [array([1.8258082], dtype=float32), 1.0116968]. 
=============================================
[2019-03-27 07:24:47,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2538512e-10 9.6013421e-01 5.2409557e-14 3.9865855e-02 7.0073290e-18], sum to 1.0000
[2019-03-27 07:24:47,819] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0571
[2019-03-27 07:24:47,826] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2148485.124281704 W.
[2019-03-27 07:24:47,830] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.23333333333333, 73.66666666666667, 1.0, 2.0, 0.7682572866112225, 1.0, 1.0, 0.7682572866112225, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2148485.124281704, 2148485.124281705, 404716.4154701995], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5923200.0000, 
sim time next is 5923800.0000, 
raw observation next is [30.1, 74.0, 1.0, 2.0, 0.9560708414513952, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.00187532787281, 6.9112, 168.9124170497268, 2233516.713260018, 2169188.720309372, 450232.1456420599], 
processed observation next is [1.0, 0.5652173913043478, 0.6255924170616115, 0.74, 1.0, 1.0, 0.9470733029534882, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009067532787280985, 0.0, 0.8294372961532789, 0.6204213092388938, 0.6025524223081589, 0.6719882770777014], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.39838126], dtype=float32), 0.47046763]. 
=============================================
[2019-03-27 07:24:48,239] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7821513e-10 2.4239348e-02 1.7444237e-13 9.7576058e-01 5.4258643e-19], sum to 1.0000
[2019-03-27 07:24:48,245] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5592
[2019-03-27 07:24:48,251] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3528010.904760936 W.
[2019-03-27 07:24:48,255] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.95, 62.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.773863981339918, 6.9112, 170.5573041426782, 3528010.904760936, 2910049.603999966, 548757.6816147923], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5416200.0000, 
sim time next is 5416800.0000, 
raw observation next is [32.26666666666667, 65.0, 1.0, 2.0, 0.8579084139506654, 1.0, 2.0, 0.7495442464895953, 1.0, 1.0, 1.03, 7.005110186392388, 6.9112, 170.5573041426782, 3145496.131552853, 3078224.44501784, 575843.7201932253], 
processed observation next is [1.0, 0.6956521739130435, 0.7282780410742499, 0.65, 1.0, 1.0, 0.8288053180128498, 1.0, 1.0, 0.6982460801079461, 1.0, 0.5, 1.0365853658536586, 0.009391018639238791, 0.0, 0.8375144448122397, 0.8737489254313481, 0.8550623458382888, 0.8594682390943661], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36931884], dtype=float32), -0.47500798]. 
=============================================
[2019-03-27 07:24:51,110] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.63019856e-10 4.20392631e-03 5.49827870e-13 9.95796084e-01
 1.12545185e-17], sum to 1.0000
[2019-03-27 07:24:51,115] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2384
[2019-03-27 07:24:51,119] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.63333333333333, 54.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.378510474780547, 6.9112, 170.5573041426782, 3244473.166500597, 2909719.664440535, 551151.0036240793], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5409600.0000, 
sim time next is 5410200.0000, 
raw observation next is [37.81666666666666, 54.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 8.283223966590468, 6.9112, 170.5573041426782, 3893311.374966711, 2910474.797331292, 545354.1204245898], 
processed observation next is [1.0, 0.6086956521739131, 0.9913112164296997, 0.54, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.13720239665904677, 0.0, 0.8375144448122397, 1.0814753819351974, 0.8084652214809145, 0.8139613737680446], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34657368], dtype=float32), -0.1974983]. 
=============================================
[2019-03-27 07:24:51,148] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0703701e-14 1.0000000e+00 1.9229930e-19 2.2538952e-08 1.7078580e-23], sum to 1.0000
[2019-03-27 07:24:51,155] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5823
[2019-03-27 07:24:51,160] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.63333333333334, 74.0, 1.0, 2.0, 0.5351201013424776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747764.4741624974, 747764.4741624967, 189282.4901317246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6027600.0000, 
sim time next is 6028200.0000, 
raw observation next is [29.45, 75.0, 1.0, 2.0, 0.533764301461443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745869.2455511234, 745869.2455511227, 189056.2838746075], 
processed observation next is [1.0, 0.782608695652174, 0.5947867298578199, 0.75, 1.0, 1.0, 0.43827024272463017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20718590154197872, 0.20718590154197852, 0.2821735580218022], 
reward next is 0.7178, 
noisyNet noise sample is [array([0.24842311], dtype=float32), 0.674672]. 
=============================================
[2019-03-27 07:24:56,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5825761e-10 7.2476071e-01 5.2209976e-14 2.7523929e-01 6.5770966e-19], sum to 1.0000
[2019-03-27 07:24:56,309] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6318
[2019-03-27 07:24:56,312] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.45, 52.33333333333333, 1.0, 2.0, 0.4142963058549877, 1.0, 2.0, 0.4142963058549877, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1158073.689023341, 1158073.689023341, 275793.5286266304], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5505000.0000, 
sim time next is 5505600.0000, 
raw observation next is [34.2, 53.66666666666667, 1.0, 2.0, 0.5269756644878507, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736379.6813046351, 736379.6813046344, 187933.1508897389], 
processed observation next is [1.0, 0.7391304347826086, 0.8199052132701423, 0.5366666666666667, 1.0, 1.0, 0.4300911620335551, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20454991147350976, 0.20454991147350957, 0.2804972401339387], 
reward next is 0.7195, 
noisyNet noise sample is [array([-0.28650072], dtype=float32), 0.76386446]. 
=============================================
[2019-03-27 07:25:02,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8719541e-15 1.0000000e+00 5.3327906e-21 2.4213394e-12 1.1939604e-24], sum to 1.0000
[2019-03-27 07:25:02,709] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4508
[2019-03-27 07:25:02,714] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 90.0, 1.0, 2.0, 0.5340010684298273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746200.2141612999, 746200.2141613004, 189094.3196509706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5612400.0000, 
sim time next is 5613000.0000, 
raw observation next is [26.55, 90.16666666666667, 1.0, 2.0, 0.5314635425979242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742653.097212558, 742653.0972125573, 188672.2222992554], 
processed observation next is [1.0, 1.0, 0.4573459715639811, 0.9016666666666667, 1.0, 1.0, 0.4354982440938845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20629252700348835, 0.20629252700348816, 0.28160033178993343], 
reward next is 0.7184, 
noisyNet noise sample is [array([0.78856796], dtype=float32), -0.28064886]. 
=============================================
[2019-03-27 07:25:02,730] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[69.17603]
 [69.15778]
 [69.19642]
 [69.22875]
 [69.25865]], R is [[69.20146179]
 [69.22721863]
 [69.25218964]
 [69.27632141]
 [69.29946899]].
[2019-03-27 07:25:03,960] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6532715e-16 1.0000000e+00 4.1489989e-22 5.5006404e-15 6.4029798e-27], sum to 1.0000
[2019-03-27 07:25:03,969] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6778
[2019-03-27 07:25:03,975] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.25, 63.0, 1.0, 2.0, 0.5272085017348208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736705.1537804308, 736705.1537804314, 187969.3024147683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5679000.0000, 
sim time next is 5679600.0000, 
raw observation next is [31.03333333333333, 64.33333333333333, 1.0, 2.0, 0.5284826783616547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738486.2684343568, 738486.2684343574, 188179.487977749], 
processed observation next is [0.0, 0.7391304347826086, 0.669826224328594, 0.6433333333333333, 1.0, 1.0, 0.431906841399584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2051350745650991, 0.20513507456509927, 0.2808649074294761], 
reward next is 0.7191, 
noisyNet noise sample is [array([-2.0044084], dtype=float32), -0.7154827]. 
=============================================
[2019-03-27 07:25:05,106] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 07:25:05,109] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:25:05,110] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:25:05,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:25:05,111] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:25:05,112] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:25:05,113] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:25:05,114] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:25:05,114] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:25:05,111] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:25:05,117] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:25:05,141] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run58
[2019-03-27 07:25:05,161] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run58
[2019-03-27 07:25:05,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run58
[2019-03-27 07:25:05,200] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run58
[2019-03-27 07:25:05,202] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run58
[2019-03-27 07:25:15,829] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04861954], dtype=float32), 0.045266215]
[2019-03-27 07:25:15,831] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [18.03333333333333, 91.33333333333334, 1.0, 2.0, 0.22362272208739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 371807.4158450303, 371807.4158450303, 158085.7699185944]
[2019-03-27 07:25:15,832] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:25:15,834] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1404760e-17 1.0000000e+00 3.5164175e-22 2.4147681e-15 8.1178459e-26], sampled 0.27797900754263316
[2019-03-27 07:25:38,826] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04861954], dtype=float32), 0.045266215]
[2019-03-27 07:25:38,828] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.39269919, 92.71135476666666, 1.0, 2.0, 0.5641795004216588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788386.4771422069, 788386.4771422069, 194262.7524246276]
[2019-03-27 07:25:38,829] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:25:38,833] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0377189e-16 1.0000000e+00 6.1619543e-22 4.3752603e-14 5.8032891e-26], sampled 0.6843044974244982
[2019-03-27 07:25:43,754] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04861954], dtype=float32), 0.045266215]
[2019-03-27 07:25:43,755] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.26666666666667, 67.33333333333334, 1.0, 2.0, 0.9253326130116205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1293370.479548411, 1293370.47954841, 276998.2351168292]
[2019-03-27 07:25:43,756] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:25:43,757] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4422594e-15 1.0000000e+00 1.3846354e-20 2.2213453e-11 5.8665949e-25], sampled 0.8808426556522129
[2019-03-27 07:25:57,966] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04861954], dtype=float32), 0.045266215]
[2019-03-27 07:25:57,967] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.73333333333333, 50.83333333333333, 1.0, 2.0, 0.642784984220877, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005971909014723, 6.9112, 168.9119639014794, 1795067.761824457, 1727833.702872107, 372734.8079931766]
[2019-03-27 07:25:57,970] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:25:57,972] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2186010e-13 1.0000000e+00 4.3662063e-18 2.1979905e-09 3.4620722e-22], sampled 0.06530957386922565
[2019-03-27 07:25:57,974] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1795067.761824457 W.
[2019-03-27 07:26:39,262] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04861954], dtype=float32), 0.045266215]
[2019-03-27 07:26:39,264] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.0, 72.0, 1.0, 2.0, 0.5586033197613212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780591.4388815, 780591.4388815, 193286.362384808]
[2019-03-27 07:26:39,266] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:26:39,269] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5157527e-17 1.0000000e+00 8.8508300e-23 1.0329490e-14 5.1402697e-27], sampled 0.19590123204770749
[2019-03-27 07:26:39,920] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04861954], dtype=float32), 0.045266215]
[2019-03-27 07:26:39,921] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.99746905833334, 72.72562915, 1.0, 2.0, 0.5866575207215412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819809.4660458004, 819809.4660458004, 198279.2242584583]
[2019-03-27 07:26:39,921] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:26:39,925] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4378310e-16 1.0000000e+00 1.0162872e-21 1.0640165e-13 6.7889265e-26], sampled 0.21533556940801846
[2019-03-27 07:27:00,713] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.1977 3163699360.3279 1769.0000
[2019-03-27 07:27:01,108] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6953 2927395964.9296 1338.0000
[2019-03-27 07:27:01,117] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.3706 2842526284.1823 1131.0000
[2019-03-27 07:27:01,220] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779156526.3284 933.0000
[2019-03-27 07:27:01,297] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7087 3007700842.4379 1763.0000
[2019-03-27 07:27:02,317] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1425000, evaluation results [1425000.0, 7886.197688625935, 3163699360.327861, 1769.0, 8254.695277302322, 2927395964.9296117, 1338.0, 8660.716744232455, 2779156526.328446, 933.0, 7998.708717006134, 3007700842.4379272, 1763.0, 8496.370636578167, 2842526284.182315, 1131.0]
[2019-03-27 07:27:05,942] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2490026e-17 1.0000000e+00 2.1770797e-23 2.1934689e-15 1.7779415e-27], sum to 1.0000
[2019-03-27 07:27:05,950] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9674
[2019-03-27 07:27:05,955] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 84.66666666666667, 1.0, 2.0, 0.5372377357674253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750724.653300569, 750724.6533005697, 189635.9136259751], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5691000.0000, 
sim time next is 5691600.0000, 
raw observation next is [27.5, 85.0, 1.0, 2.0, 0.5373561199602739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750890.1393398107, 750890.1393398113, 189655.7173870411], 
processed observation next is [0.0, 0.9130434782608695, 0.5023696682464456, 0.85, 1.0, 1.0, 0.4425977348918962, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20858059426105852, 0.20858059426105868, 0.2830682349060315], 
reward next is 0.7169, 
noisyNet noise sample is [array([1.0025991], dtype=float32), 0.60216206]. 
=============================================
[2019-03-27 07:27:08,317] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2963765e-17 1.0000000e+00 2.3282808e-24 8.5711566e-16 2.3295565e-27], sum to 1.0000
[2019-03-27 07:27:08,325] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6554
[2019-03-27 07:27:08,328] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.73333333333333, 59.16666666666667, 1.0, 2.0, 0.5643201405193947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788583.0810784464, 788583.0810784464, 194286.3640795134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5760600.0000, 
sim time next is 5761200.0000, 
raw observation next is [32.56666666666666, 60.33333333333334, 1.0, 2.0, 0.5521515385808276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771572.4550705901, 771572.4550705901, 192170.8571111246], 
processed observation next is [0.0, 0.6956521739130435, 0.7424960505529224, 0.6033333333333334, 1.0, 1.0, 0.46042354045882833, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21432568196405283, 0.21432568196405283, 0.2868221747927233], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.77743745], dtype=float32), -0.2094392]. 
=============================================
[2019-03-27 07:27:16,386] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.1962933e-14 1.0000000e+00 1.9593588e-18 4.8937135e-08 1.5961134e-22], sum to 1.0000
[2019-03-27 07:27:16,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8253
[2019-03-27 07:27:16,402] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.68333333333334, 80.33333333333334, 1.0, 2.0, 0.5710748388473635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798025.667698816, 798025.667698816, 195481.9636586817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5940600.0000, 
sim time next is 5941200.0000, 
raw observation next is [29.56666666666667, 80.66666666666667, 1.0, 2.0, 0.5706608332825299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797446.9150249014, 797446.9150249014, 195408.2689505969], 
processed observation next is [1.0, 0.782608695652174, 0.6003159557661929, 0.8066666666666668, 1.0, 1.0, 0.4827238955211204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2215130319513615, 0.2215130319513615, 0.2916541327620849], 
reward next is 0.7083, 
noisyNet noise sample is [array([-1.201047], dtype=float32), 1.1174773]. 
=============================================
[2019-03-27 07:27:19,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4787646e-10 2.0985549e-02 3.1553376e-13 9.7901446e-01 5.5909903e-18], sum to 1.0000
[2019-03-27 07:27:19,692] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6057
[2019-03-27 07:27:19,699] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.3, 78.5, 1.0, 2.0, 0.8247006350478893, 1.0, 2.0, 0.8247006350478893, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2306488.406779558, 2306488.406779558, 432082.9185737176], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5934600.0000, 
sim time next is 5935200.0000, 
raw observation next is [30.33333333333334, 78.66666666666667, 1.0, 2.0, 0.8046480198375884, 1.0, 2.0, 0.8046480198375884, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2250355.594621844, 2250355.594621844, 422148.6734432566], 
processed observation next is [1.0, 0.6956521739130435, 0.6366508688783573, 0.7866666666666667, 1.0, 1.0, 0.7646361684790222, 1.0, 1.0, 0.7646361684790222, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6250987762838456, 0.6250987762838456, 0.6300726469302337], 
reward next is 0.3699, 
noisyNet noise sample is [array([-0.23655036], dtype=float32), 1.0272897]. 
=============================================
[2019-03-27 07:27:20,376] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.5191672e-15 1.0000000e+00 1.5715913e-20 7.9359430e-10 8.9213568e-26], sum to 1.0000
[2019-03-27 07:27:20,387] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3233
[2019-03-27 07:27:20,396] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.65, 85.66666666666667, 1.0, 2.0, 0.5382854674684588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752189.2498083162, 752189.2498083162, 189812.3401488796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6036600.0000, 
sim time next is 6037200.0000, 
raw observation next is [27.6, 86.0, 1.0, 2.0, 0.5387169063691704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752792.347454091, 752792.3474540904, 189884.8297704713], 
processed observation next is [1.0, 0.9130434782608695, 0.5071090047393366, 0.86, 1.0, 1.0, 0.4442372365893619, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20910898540391415, 0.20910898540391398, 0.2834101936872706], 
reward next is 0.7166, 
noisyNet noise sample is [array([1.15412], dtype=float32), 0.5631903]. 
=============================================
[2019-03-27 07:27:25,848] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0129863e-15 1.0000000e+00 4.2509252e-21 1.6005855e-11 5.9540298e-24], sum to 1.0000
[2019-03-27 07:27:25,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1078
[2019-03-27 07:27:25,864] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.7133067839727418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 996875.2823695963, 996875.2823695963, 223742.9300283698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6064800.0000, 
sim time next is 6065400.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.6993744570760846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 977395.3280066288, 977395.3280066282, 220709.5056729922], 
processed observation next is [1.0, 0.17391304347826086, 0.4360189573459717, 0.93, 1.0, 1.0, 0.6378005506940778, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2714987022240635, 0.2714987022240634, 0.32941717264625703], 
reward next is 0.6706, 
noisyNet noise sample is [array([1.1791935], dtype=float32), -0.3482437]. 
=============================================
[2019-03-27 07:27:32,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.5719223e-14 1.0000000e+00 2.0967356e-18 4.6409760e-09 8.7289472e-23], sum to 1.0000
[2019-03-27 07:27:32,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8912
[2019-03-27 07:27:32,842] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 53.66666666666666, 1.0, 2.0, 0.3255585290151589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511002.1262415127, 511002.1262415127, 167777.710908791], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6807000.0000, 
sim time next is 6807600.0000, 
raw observation next is [27.4, 54.0, 1.0, 2.0, 0.3236519531712713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508717.8792475551, 508717.8792475551, 167619.3360133275], 
processed observation next is [1.0, 0.8260869565217391, 0.4976303317535545, 0.54, 1.0, 1.0, 0.18512283514611003, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14131052201320976, 0.14131052201320976, 0.2501781134527276], 
reward next is 0.7498, 
noisyNet noise sample is [array([-0.78002197], dtype=float32), 1.3207734]. 
=============================================
[2019-03-27 07:27:35,530] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.6897575e-17 1.0000000e+00 5.5641622e-23 2.8388010e-13 2.0487815e-26], sum to 1.0000
[2019-03-27 07:27:35,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7190
[2019-03-27 07:27:35,547] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.48333333333333, 90.16666666666667, 1.0, 2.0, 0.5199750205467105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726593.849781271, 726593.8497812704, 186785.2032186275], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6225000.0000, 
sim time next is 6225600.0000, 
raw observation next is [26.46666666666667, 90.33333333333334, 1.0, 2.0, 0.5199932660851658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726619.3541415257, 726619.3541415257, 186788.186377479], 
processed observation next is [0.0, 0.043478260869565216, 0.45339652448657203, 0.9033333333333334, 1.0, 1.0, 0.42167863383754917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20183870948375712, 0.20183870948375712, 0.2787883378768343], 
reward next is 0.7212, 
noisyNet noise sample is [array([1.3325777], dtype=float32), -1.2306595]. 
=============================================
[2019-03-27 07:27:37,595] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4866817e-16 1.0000000e+00 2.0166154e-21 4.0660204e-14 4.3529671e-26], sum to 1.0000
[2019-03-27 07:27:37,602] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9011
[2019-03-27 07:27:37,610] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.85, 63.0, 1.0, 2.0, 0.5186639444990421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724761.1756359605, 724761.1756359612, 186572.0322751431], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6269400.0000, 
sim time next is 6270000.0000, 
raw observation next is [30.86666666666667, 62.66666666666666, 1.0, 2.0, 0.5167895115338861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722141.0243194941, 722141.0243194941, 186268.4968835686], 
processed observation next is [0.0, 0.5652173913043478, 0.6619273301737759, 0.6266666666666666, 1.0, 1.0, 0.41781868859504345, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20059472897763725, 0.20059472897763725, 0.278012681915774], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.0852258], dtype=float32), -0.9323119]. 
=============================================
[2019-03-27 07:27:37,628] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.7558  ]
 [74.723694]
 [74.689255]
 [74.643845]
 [74.574066]], R is [[74.75753784]
 [74.73149872]
 [74.70519257]
 [74.67893982]
 [74.65272522]].
[2019-03-27 07:27:44,810] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0651896e-14 1.0000000e+00 1.0412742e-19 2.2473423e-10 5.3358577e-23], sum to 1.0000
[2019-03-27 07:27:44,820] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9903
[2019-03-27 07:27:44,827] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 76.0, 1.0, 2.0, 0.5917320944556285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846554.1892061256, 846554.1892061256, 201742.0874032477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7023000.0000, 
sim time next is 7023600.0000, 
raw observation next is [26.8, 75.0, 1.0, 2.0, 0.6364402611088918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 910828.0310405577, 910828.031040557, 210518.0316832197], 
processed observation next is [1.0, 0.30434782608695654, 0.4691943127962086, 0.75, 1.0, 1.0, 0.5619762182034841, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2530077864001549, 0.2530077864001547, 0.31420601743764137], 
reward next is 0.6858, 
noisyNet noise sample is [array([1.7190319], dtype=float32), -0.647399]. 
=============================================
[2019-03-27 07:27:45,655] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4240380e-13 9.9999976e-01 6.2257868e-17 2.2142152e-07 2.3540310e-21], sum to 1.0000
[2019-03-27 07:27:45,663] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5914
[2019-03-27 07:27:45,671] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2090714.61809028 W.
[2019-03-27 07:27:45,680] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.3, 78.0, 1.0, 2.0, 0.8540453178009665, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.9822060345801, 6.9112, 168.9125341373766, 2090714.61809028, 2040340.619282402, 422483.8113444116], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6424800.0000, 
sim time next is 6425400.0000, 
raw observation next is [28.4, 77.5, 1.0, 2.0, 0.4889293470748991, 1.0, 1.0, 0.4889293470748991, 1.0, 2.0, 0.8389208382151496, 6.9112, 6.9112, 170.5573041426782, 2050894.235611438, 2050894.235611438, 405771.1041263426], 
processed observation next is [1.0, 0.34782608695652173, 0.5450236966824644, 0.775, 1.0, 1.0, 0.38425222539144466, 1.0, 0.5, 0.38425222539144466, 1.0, 1.0, 0.8035619978233532, 0.0, 0.0, 0.8375144448122397, 0.5696928432253995, 0.5696928432253995, 0.6056285136214068], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09662111], dtype=float32), -0.051366076]. 
=============================================
[2019-03-27 07:27:46,422] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2004827e-16 1.0000000e+00 3.8021434e-20 8.6333864e-13 1.2954133e-25], sum to 1.0000
[2019-03-27 07:27:46,427] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7907
[2019-03-27 07:27:46,431] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 82.0, 1.0, 2.0, 0.5186913375641127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724799.4667173998, 724799.4667173992, 186576.5096105392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6471600.0000, 
sim time next is 6472200.0000, 
raw observation next is [27.53333333333333, 82.5, 1.0, 2.0, 0.5202086032077528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726920.3612292683, 726920.3612292683, 186822.9963013954], 
processed observation next is [1.0, 0.9130434782608695, 0.5039494470774091, 0.825, 1.0, 1.0, 0.42193807615391893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2019223225636856, 0.2019223225636856, 0.27884029298715735], 
reward next is 0.7212, 
noisyNet noise sample is [array([-0.13698077], dtype=float32), 0.2190505]. 
=============================================
[2019-03-27 07:27:47,623] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.7431825e-14 9.9999988e-01 3.0781093e-18 8.4849979e-08 2.3235515e-24], sum to 1.0000
[2019-03-27 07:27:47,631] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4034
[2019-03-27 07:27:47,637] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 72.0, 1.0, 2.0, 0.5039603006425784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704208.0551312147, 704208.0551312154, 184219.2377788639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6460800.0000, 
sim time next is 6461400.0000, 
raw observation next is [28.8, 72.5, 1.0, 2.0, 0.5062973892159814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707474.8693109483, 707474.8693109483, 184588.7930917196], 
processed observation next is [1.0, 0.782608695652174, 0.5639810426540285, 0.725, 1.0, 1.0, 0.4051775773686523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19652079703081898, 0.19652079703081898, 0.27550566133092474], 
reward next is 0.7245, 
noisyNet noise sample is [array([-0.6789988], dtype=float32), -1.0309099]. 
=============================================
[2019-03-27 07:27:50,151] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8650633e-08 7.5476074e-01 2.9795780e-12 2.4523927e-01 1.6393546e-16], sum to 1.0000
[2019-03-27 07:27:50,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5186
[2019-03-27 07:27:50,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1955171.693318386 W.
[2019-03-27 07:27:50,180] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.88333333333333, 58.5, 1.0, 2.0, 0.6991951151800707, 1.0, 2.0, 0.6991951151800707, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1955171.693318386, 1955171.693318386, 373769.8320850466], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6519000.0000, 
sim time next is 6519600.0000, 
raw observation next is [31.1, 57.0, 1.0, 2.0, 0.467654763066897, 1.0, 2.0, 0.467654763066897, 1.0, 1.0, 0.7913355339527284, 6.9112, 6.9112, 170.5573041426782, 1961572.808842401, 1961572.808842401, 389946.2401162495], 
processed observation next is [1.0, 0.4782608695652174, 0.6729857819905214, 0.57, 1.0, 1.0, 0.35862019646614096, 1.0, 1.0, 0.35862019646614096, 1.0, 0.5, 0.745531138966742, 0.0, 0.0, 0.8375144448122397, 0.5448813357895559, 0.5448813357895559, 0.5820093136063426], 
reward next is 0.4180, 
noisyNet noise sample is [array([-1.2660427], dtype=float32), -2.0044436]. 
=============================================
[2019-03-27 07:27:54,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.9694498e-14 1.0000000e+00 2.1631764e-19 3.3206979e-10 2.1957456e-23], sum to 1.0000
[2019-03-27 07:27:54,809] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4350
[2019-03-27 07:27:54,813] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.18333333333334, 89.66666666666667, 1.0, 2.0, 0.6752231728762547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 943628.2391805528, 943628.2391805528, 215586.3919657916], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6588600.0000, 
sim time next is 6589200.0000, 
raw observation next is [26.26666666666667, 89.33333333333334, 1.0, 2.0, 0.6995581569466889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 977652.1718730864, 977652.1718730864, 220748.2624320532], 
processed observation next is [1.0, 0.2608695652173913, 0.44391785150079005, 0.8933333333333334, 1.0, 1.0, 0.6380218758393842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27157004774252397, 0.27157004774252397, 0.32947501855530326], 
reward next is 0.6705, 
noisyNet noise sample is [array([-0.8280722], dtype=float32), 0.21549965]. 
=============================================
[2019-03-27 07:27:55,328] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 07:27:55,330] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:27:55,331] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:27:55,331] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:27:55,332] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:27:55,333] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:27:55,334] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:27:55,335] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:27:55,336] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:27:55,339] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:27:55,341] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:27:55,367] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run59
[2019-03-27 07:27:55,388] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run59
[2019-03-27 07:27:55,410] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run59
[2019-03-27 07:27:55,427] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run59
[2019-03-27 07:27:55,428] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run59
[2019-03-27 07:28:24,830] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05088033], dtype=float32), 0.045139387]
[2019-03-27 07:28:24,832] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.63929495, 87.96286843166668, 1.0, 2.0, 0.5029610870173985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702811.3438079226, 702811.343807922, 184059.9740908119]
[2019-03-27 07:28:24,834] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:28:24,837] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4323417e-16 1.0000000e+00 1.4492040e-21 2.9387013e-13 1.5046387e-25], sampled 0.3762599662277688
[2019-03-27 07:28:33,375] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05088033], dtype=float32), 0.045139387]
[2019-03-27 07:28:33,377] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.93333333333334, 56.0, 1.0, 2.0, 0.9861530734540308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1378436.601506692, 1378436.601506691, 294733.2565093982]
[2019-03-27 07:28:33,378] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:28:33,381] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9446921e-14 1.0000000e+00 4.7199836e-19 1.1929747e-09 2.9553636e-23], sampled 0.7954118414309693
[2019-03-27 07:28:41,802] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05088033], dtype=float32), 0.045139387]
[2019-03-27 07:28:41,803] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.66666666666666, 90.66666666666666, 1.0, 2.0, 0.669519341242912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024165.140682468, 1024165.140682468, 225473.9364249042]
[2019-03-27 07:28:41,805] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:28:41,807] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.7523172e-16 1.0000000e+00 4.8644973e-21 1.8939188e-13 1.1698090e-24], sampled 0.4772594265021872
[2019-03-27 07:29:50,861] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7937.7726 3158104248.9241 1613.0000
[2019-03-27 07:29:51,006] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8505.2448 2840861153.6949 1093.0000
[2019-03-27 07:29:51,035] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8024.2984 3004119285.6231 1685.0000
[2019-03-27 07:29:51,142] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.4607 2779132603.3122 932.0000
[2019-03-27 07:29:51,209] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8268.7199 2925938033.8850 1294.0000
[2019-03-27 07:29:52,226] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1450000, evaluation results [1450000.0, 7937.772576575967, 3158104248.9241076, 1613.0, 8268.719852092161, 2925938033.885039, 1294.0, 8658.460699951784, 2779132603.312169, 932.0, 8024.29842560865, 3004119285.6230855, 1685.0, 8505.244799620226, 2840861153.694912, 1093.0]
[2019-03-27 07:29:58,673] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0651390e-09 8.1766063e-01 8.0934977e-13 1.8233934e-01 8.9243709e-17], sum to 1.0000
[2019-03-27 07:29:58,682] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8304
[2019-03-27 07:29:58,688] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 63.0, 1.0, 2.0, 0.6874246780009627, 1.0, 2.0, 0.6874246780009627, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1922228.284701584, 1922228.284701584, 368777.5229579346], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6706800.0000, 
sim time next is 6707400.0000, 
raw observation next is [29.96666666666667, 63.33333333333333, 1.0, 2.0, 0.6433750971769684, 1.0, 2.0, 0.6433750971769684, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1798950.14917687, 1798950.14917687, 350816.0068756667], 
processed observation next is [1.0, 0.6521739130434783, 0.6192733017377569, 0.6333333333333333, 1.0, 1.0, 0.5703314423818896, 1.0, 1.0, 0.5703314423818896, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.49970837477135277, 0.49970837477135277, 0.5236059804114428], 
reward next is 0.4764, 
noisyNet noise sample is [array([-1.5836401], dtype=float32), 2.3022742]. 
=============================================
[2019-03-27 07:30:03,791] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1474003e-14 1.0000000e+00 6.4169857e-21 7.1276957e-10 1.8460069e-24], sum to 1.0000
[2019-03-27 07:30:03,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3031
[2019-03-27 07:30:03,803] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.35, 51.66666666666667, 1.0, 2.0, 0.3243064040085726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504533.2938966963, 504533.2938966957, 167156.2665330617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6803400.0000, 
sim time next is 6804000.0000, 
raw observation next is [28.2, 52.0, 1.0, 2.0, 0.3260153737455134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508090.75159728, 508090.7515972807, 167456.6544811524], 
processed observation next is [1.0, 0.782608695652174, 0.5355450236966824, 0.52, 1.0, 1.0, 0.1879703298138716, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14113631988813333, 0.14113631988813352, 0.24993530519574983], 
reward next is 0.7501, 
noisyNet noise sample is [array([-0.20386139], dtype=float32), -0.07834715]. 
=============================================
[2019-03-27 07:30:03,817] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.47675 ]
 [69.23581 ]
 [68.841385]
 [68.33553 ]
 [67.41381 ]], R is [[69.61050415]
 [69.66490936]
 [69.71943665]
 [69.77404785]
 [69.82710266]].
[2019-03-27 07:30:06,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0718740e-17 1.0000000e+00 2.3899781e-23 9.3447713e-16 5.2542074e-26], sum to 1.0000
[2019-03-27 07:30:06,306] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5129
[2019-03-27 07:30:06,309] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 90.0, 1.0, 2.0, 0.4165771048668537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612831.3522172116, 612831.3522172116, 175490.061568499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6926400.0000, 
sim time next is 6927000.0000, 
raw observation next is [23.85, 90.33333333333333, 1.0, 2.0, 0.416518356706541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613002.4067127772, 613002.4067127772, 175513.5447305596], 
processed observation next is [0.0, 0.17391304347826086, 0.3293838862559243, 0.9033333333333333, 1.0, 1.0, 0.29701006832113375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17027844630910477, 0.17027844630910477, 0.2619605145232233], 
reward next is 0.7380, 
noisyNet noise sample is [array([-0.04222458], dtype=float32), 0.3522998]. 
=============================================
[2019-03-27 07:30:06,318] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[76.89589 ]
 [76.87953 ]
 [76.945114]
 [76.99699 ]
 [77.19759 ]], R is [[76.87240601]
 [76.84175873]
 [76.81127167]
 [76.78095245]
 [76.75076294]].
[2019-03-27 07:30:07,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8674035e-18 1.0000000e+00 1.2811189e-23 3.3959232e-16 1.4196516e-28], sum to 1.0000
[2019-03-27 07:30:07,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5545
[2019-03-27 07:30:07,953] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 63.16666666666666, 1.0, 2.0, 0.3688237051575706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 562188.2769463515, 562188.2769463509, 171512.6995161288], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6897000.0000, 
sim time next is 6897600.0000, 
raw observation next is [26.8, 64.0, 1.0, 2.0, 0.3693110611153672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562659.6548752574, 562659.6548752574, 171545.3113086715], 
processed observation next is [0.0, 0.8695652173913043, 0.4691943127962086, 0.64, 1.0, 1.0, 0.2401338085727316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15629434857646038, 0.15629434857646038, 0.256037778072644], 
reward next is 0.7440, 
noisyNet noise sample is [array([-0.7883824], dtype=float32), -0.70485103]. 
=============================================
[2019-03-27 07:30:14,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4291626e-15 1.0000000e+00 9.9608036e-20 8.9947931e-11 1.8332130e-25], sum to 1.0000
[2019-03-27 07:30:14,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6851
[2019-03-27 07:30:14,173] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 87.0, 1.0, 2.0, 0.4808707903607484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 671933.7636823797, 671933.7636823804, 180652.6696115833], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7077600.0000, 
sim time next is 7078200.0000, 
raw observation next is [25.56666666666667, 87.16666666666667, 1.0, 2.0, 0.4811017808085878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672256.6350135148, 672256.6350135142, 180687.5239666633], 
processed observation next is [1.0, 0.9565217391304348, 0.41074249605055313, 0.8716666666666667, 1.0, 1.0, 0.3748214226609491, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18673795417042077, 0.1867379541704206, 0.2696828715920348], 
reward next is 0.7303, 
noisyNet noise sample is [array([1.5855528], dtype=float32), -0.07449666]. 
=============================================
[2019-03-27 07:30:15,444] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8795524e-08 4.3080762e-01 5.4383133e-11 5.6919235e-01 3.8008777e-16], sum to 1.0000
[2019-03-27 07:30:15,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8943
[2019-03-27 07:30:15,455] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.5, 55.0, 1.0, 2.0, 0.7075081079444405, 1.0, 1.0, 0.7075081079444405, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1978438.926097323, 1978438.926097323, 377339.1163980024], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7053600.0000, 
sim time next is 7054200.0000, 
raw observation next is [30.3, 56.5, 1.0, 2.0, 0.7252472570537879, 1.0, 2.0, 0.7252472570537879, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2028090.714565173, 2028090.714565173, 385103.9760459681], 
processed observation next is [1.0, 0.6521739130434783, 0.6350710900473934, 0.565, 1.0, 1.0, 0.6689725988599854, 1.0, 1.0, 0.6689725988599854, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5633585318236591, 0.5633585318236591, 0.5747820537999524], 
reward next is 0.4252, 
noisyNet noise sample is [array([0.87480897], dtype=float32), -0.35943413]. 
=============================================
[2019-03-27 07:30:26,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8661802e-15 1.0000000e+00 1.5033467e-19 6.5065731e-10 3.4699885e-24], sum to 1.0000
[2019-03-27 07:30:26,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1798
[2019-03-27 07:30:26,799] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 83.0, 1.0, 2.0, 0.3693732107040272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 559374.6693396844, 559374.6693396838, 171157.4053747089], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7234800.0000, 
sim time next is 7235400.0000, 
raw observation next is [23.9, 83.5, 1.0, 2.0, 0.3699750097547244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 560805.8883778467, 560805.8883778473, 171297.4943313054], 
processed observation next is [1.0, 0.7391304347826086, 0.33175355450236965, 0.835, 1.0, 1.0, 0.24093374669243905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15577941343829074, 0.1557794134382909, 0.255667901987023], 
reward next is 0.7443, 
noisyNet noise sample is [array([1.1354021], dtype=float32), 0.9309186]. 
=============================================
[2019-03-27 07:30:27,560] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8016719e-14 1.0000000e+00 2.5595514e-20 6.9227768e-10 2.9303952e-24], sum to 1.0000
[2019-03-27 07:30:27,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9289
[2019-03-27 07:30:27,580] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 86.66666666666667, 1.0, 2.0, 0.370221184261301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 562460.1829974334, 562460.1829974327, 171480.3139426154], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7238400.0000, 
sim time next is 7239000.0000, 
raw observation next is [23.3, 87.33333333333334, 1.0, 2.0, 0.3723478901464754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 565879.6792143573, 565879.6792143566, 171782.4665505339], 
processed observation next is [1.0, 0.782608695652174, 0.3033175355450238, 0.8733333333333334, 1.0, 1.0, 0.24379263873069326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15718879978176592, 0.15718879978176573, 0.25639174112019986], 
reward next is 0.7436, 
noisyNet noise sample is [array([-0.2896059], dtype=float32), -0.3442006]. 
=============================================
[2019-03-27 07:30:27,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.63246 ]
 [70.48003 ]
 [70.52753 ]
 [70.515976]
 [70.43643 ]], R is [[70.75390625]
 [70.79042816]
 [70.82630157]
 [70.86132812]
 [70.89569855]].
[2019-03-27 07:30:30,340] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9562730e-16 1.0000000e+00 4.4356147e-22 9.3999157e-13 3.2338653e-25], sum to 1.0000
[2019-03-27 07:30:30,350] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1389
[2019-03-27 07:30:30,357] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 83.83333333333334, 1.0, 2.0, 0.3973965204707379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625088.3231592949, 625088.3231592949, 177399.5895812583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7287000.0000, 
sim time next is 7287600.0000, 
raw observation next is [22.76666666666667, 82.66666666666667, 1.0, 2.0, 0.557431832983214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 875193.2792397122, 875193.2792397122, 204298.9784368594], 
processed observation next is [1.0, 0.34782608695652173, 0.2780410742496052, 0.8266666666666667, 1.0, 1.0, 0.4667853409436313, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2431092442332534, 0.2431092442332534, 0.304923848413223], 
reward next is 0.6951, 
noisyNet noise sample is [array([-2.4303837], dtype=float32), 0.9666533]. 
=============================================
[2019-03-27 07:30:30,554] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.3251199e-16 1.0000000e+00 4.4820002e-21 3.5570003e-14 1.6155793e-24], sum to 1.0000
[2019-03-27 07:30:30,566] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2932
[2019-03-27 07:30:30,573] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 90.0, 1.0, 2.0, 0.3282137759430772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517660.6663926761, 517660.6663926768, 168344.5824127825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7272000.0000, 
sim time next is 7272600.0000, 
raw observation next is [21.56666666666667, 90.16666666666667, 1.0, 2.0, 0.3597690894197039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 567524.7436648917, 567524.7436648923, 172379.4989227491], 
processed observation next is [1.0, 0.17391304347826086, 0.22116903633491333, 0.9016666666666667, 1.0, 1.0, 0.2286374571321734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15764576212913659, 0.15764576212913675, 0.25728283421305836], 
reward next is 0.7427, 
noisyNet noise sample is [array([-0.34786186], dtype=float32), -0.21996246]. 
=============================================
[2019-03-27 07:30:33,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:30:33,258] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:30:33,312] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-03-27 07:30:35,477] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:30:35,478] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:30:35,533] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9753000e-17 1.0000000e+00 9.2645213e-23 3.2997513e-16 3.5518786e-27], sum to 1.0000
[2019-03-27 07:30:35,543] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9164
[2019-03-27 07:30:35,549] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.36666666666667, 91.33333333333334, 1.0, 2.0, 0.3131527610906916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494711.8894483713, 494711.8894483713, 166621.4210507116], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7438800.0000, 
sim time next is 7439400.0000, 
raw observation next is [21.35, 91.5, 1.0, 2.0, 0.3133050122001033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 494907.9830848176, 494907.983084817, 166635.0146898113], 
processed observation next is [0.0, 0.08695652173913043, 0.2109004739336494, 0.915, 1.0, 1.0, 0.17265664120494376, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13747443974578266, 0.1374744397457825, 0.24870897714897208], 
reward next is 0.7513, 
noisyNet noise sample is [array([0.36285856], dtype=float32), -0.40758333]. 
=============================================
[2019-03-27 07:30:35,554] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run8
[2019-03-27 07:30:38,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2893447e-17 1.0000000e+00 2.2004579e-23 1.1182051e-15 2.6175327e-27], sum to 1.0000
[2019-03-27 07:30:38,185] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0998
[2019-03-27 07:30:38,191] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 93.0, 1.0, 2.0, 0.4059162739980898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 597606.4208987879, 597606.4208987873, 174076.9876780622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7522200.0000, 
sim time next is 7522800.0000, 
raw observation next is [23.5, 93.0, 1.0, 2.0, 0.4046505640864498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595742.8224814701, 595742.8224814708, 173904.6966147587], 
processed observation next is [0.0, 0.043478260869565216, 0.31279620853080575, 0.93, 1.0, 1.0, 0.2827115229957226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1654841173559639, 0.1654841173559641, 0.2595592486787443], 
reward next is 0.7404, 
noisyNet noise sample is [array([-0.6501233], dtype=float32), 0.18621962]. 
=============================================
[2019-03-27 07:30:42,672] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 07:30:42,674] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:30:42,674] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:30:42,675] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:30:42,676] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:30:42,677] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:30:42,677] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:30:42,678] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:30:42,679] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:30:42,679] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:30:42,680] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:30:42,712] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run60
[2019-03-27 07:30:42,733] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run60
[2019-03-27 07:30:42,733] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run60
[2019-03-27 07:30:42,734] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run60
[2019-03-27 07:30:42,735] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run60
[2019-03-27 07:30:48,419] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05275267], dtype=float32), 0.046151154]
[2019-03-27 07:30:48,421] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.98369759333333, 73.95950745, 1.0, 2.0, 0.2836870575181918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 456331.5845306719, 456331.5845306725, 164023.2422930379]
[2019-03-27 07:30:48,422] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:30:48,425] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.5366332e-17 1.0000000e+00 1.8192342e-22 4.7804248e-15 2.8927190e-26], sampled 0.6247827013571088
[2019-03-27 07:32:38,635] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.8925 2841977978.9991 1121.0000
[2019-03-27 07:32:38,759] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7900.5103 3162566376.2621 1728.0000
[2019-03-27 07:32:38,801] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.6423 3006855083.3018 1745.0000
[2019-03-27 07:32:38,926] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.5054 2779248893.7947 932.0000
[2019-03-27 07:32:39,039] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.6459 2927089735.4070 1325.0000
[2019-03-27 07:32:40,060] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1475000, evaluation results [1475000.0, 7900.51025342134, 3162566376.262104, 1728.0, 8259.645871002976, 2927089735.406984, 1325.0, 8659.505384280943, 2779248893.794731, 932.0, 8000.6423290649445, 3006855083.3018436, 1745.0, 8500.892526644444, 2841977978.9991274, 1121.0]
[2019-03-27 07:32:51,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.0181368e-15 1.0000000e+00 6.6983710e-20 4.4330685e-11 8.1787687e-26], sum to 1.0000
[2019-03-27 07:32:51,943] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6289
[2019-03-27 07:32:51,947] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 88.66666666666667, 1.0, 2.0, 0.5131699739753436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717081.5199289086, 717081.5199289093, 185685.245727635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7776600.0000, 
sim time next is 7777200.0000, 
raw observation next is [26.4, 88.33333333333334, 1.0, 2.0, 0.5117471010429623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715092.5896172201, 715092.5896172201, 185457.0375287665], 
processed observation next is [1.0, 0.0, 0.45023696682464454, 0.8833333333333334, 1.0, 1.0, 0.41174349523248466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1986368304492278, 0.1986368304492278, 0.27680154855039774], 
reward next is 0.7232, 
noisyNet noise sample is [array([-1.0808736], dtype=float32), -1.5304704]. 
=============================================
[2019-03-27 07:32:52,509] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:32:52,509] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:32:52,584] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run8
[2019-03-27 07:32:53,778] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7747608e-16 1.0000000e+00 5.3244970e-21 7.4748596e-11 2.4566406e-25], sum to 1.0000
[2019-03-27 07:32:53,788] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8599
[2019-03-27 07:32:53,791] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 90.0, 1.0, 2.0, 0.5192745789501413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725614.7443187244, 725614.7443187244, 186671.1438542013], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7774200.0000, 
sim time next is 7774800.0000, 
raw observation next is [26.4, 89.66666666666666, 1.0, 2.0, 0.5182406489843199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724169.4770371129, 724169.4770371135, 186503.341211195], 
processed observation next is [1.0, 1.0, 0.45023696682464454, 0.8966666666666666, 1.0, 1.0, 0.41956704696906005, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2011581880658647, 0.20115818806586486, 0.27836319583760444], 
reward next is 0.7216, 
noisyNet noise sample is [array([-0.4872116], dtype=float32), -0.24729171]. 
=============================================
[2019-03-27 07:32:55,691] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.8614226e-15 1.0000000e+00 2.0204048e-20 6.8133665e-11 1.3185370e-24], sum to 1.0000
[2019-03-27 07:32:55,700] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0177
[2019-03-27 07:32:55,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.21666666666667, 89.83333333333333, 1.0, 2.0, 0.6508600437936357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 909566.0474464631, 909566.0474464631, 210594.4513989143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7877400.0000, 
sim time next is 7878000.0000, 
raw observation next is [26.23333333333333, 89.66666666666667, 1.0, 2.0, 0.6033241219465292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843109.0231053925, 843109.0231053925, 201360.2444601352], 
processed observation next is [1.0, 0.17391304347826086, 0.44233807266982617, 0.8966666666666667, 1.0, 1.0, 0.522077255357264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23419695086260903, 0.23419695086260903, 0.30053767829870925], 
reward next is 0.6995, 
noisyNet noise sample is [array([0.39289868], dtype=float32), 0.85447156]. 
=============================================
[2019-03-27 07:32:55,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[67.1052  ]
 [66.92211 ]
 [66.912254]
 [67.06925 ]
 [67.16248 ]], R is [[67.21951294]
 [67.23299408]
 [67.21851349]
 [67.19877625]
 [67.17532349]].
[2019-03-27 07:32:56,407] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1265719e-14 1.0000000e+00 4.4452892e-21 1.2321667e-12 1.2200384e-24], sum to 1.0000
[2019-03-27 07:32:56,414] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6047
[2019-03-27 07:32:56,419] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.26666666666667, 80.66666666666667, 1.0, 2.0, 0.24365008911439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401497.8336004322, 401497.8336004322, 160269.8607273031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 458400.0000, 
sim time next is 459000.0000, 
raw observation next is [20.3, 80.5, 1.0, 2.0, 0.2435427851892651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401270.2285228275, 401270.2285228275, 160261.07886859], 
processed observation next is [1.0, 0.30434782608695654, 0.16113744075829392, 0.805, 1.0, 1.0, 0.08860576528827117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11146395236745209, 0.11146395236745209, 0.23919564010237315], 
reward next is 0.7608, 
noisyNet noise sample is [array([0.86943245], dtype=float32), -1.725071]. 
=============================================
[2019-03-27 07:32:56,430] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[68.84591]
 [68.81278]
 [68.83583]
 [68.77102]
 [68.76458]], R is [[68.95069122]
 [69.02197266]
 [69.08956909]
 [69.16022491]
 [69.23019409]].
[2019-03-27 07:32:57,187] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.6627672e-15 1.0000000e+00 3.7389455e-19 1.2865838e-09 1.1779961e-22], sum to 1.0000
[2019-03-27 07:32:57,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0644
[2019-03-27 07:32:57,203] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 90.0, 1.0, 2.0, 0.7379057646237581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1031270.052102532, 1031270.052102532, 229238.3073111918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7876800.0000, 
sim time next is 7877400.0000, 
raw observation next is [26.21666666666667, 89.83333333333333, 1.0, 2.0, 0.6508600437936357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 909566.0474464631, 909566.0474464631, 210594.4513989143], 
processed observation next is [1.0, 0.17391304347826086, 0.44154818325434453, 0.8983333333333333, 1.0, 1.0, 0.5793494503537779, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2526572354017953, 0.2526572354017953, 0.31432007671479745], 
reward next is 0.6857, 
noisyNet noise sample is [array([1.560311], dtype=float32), 0.39380568]. 
=============================================
[2019-03-27 07:33:00,867] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:00,868] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:00,930] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run8
[2019-03-27 07:33:01,371] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0919229e-15 1.0000000e+00 3.2968491e-21 9.6184345e-11 1.1590153e-24], sum to 1.0000
[2019-03-27 07:33:01,377] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7986
[2019-03-27 07:33:01,381] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 78.0, 1.0, 2.0, 0.2397404706644758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 396341.8945255243, 396341.8945255243, 159833.531529172], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 504000.0000, 
sim time next is 504600.0000, 
raw observation next is [20.21666666666667, 78.5, 1.0, 2.0, 0.2401342701356224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397080.5500383901, 397080.5500383901, 159864.9659576104], 
processed observation next is [1.0, 0.8695652173913043, 0.15718799368088482, 0.785, 1.0, 1.0, 0.0844991206453282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1103001527884417, 0.1103001527884417, 0.2386044268024036], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.02422243], dtype=float32), 1.0291224]. 
=============================================
[2019-03-27 07:33:02,305] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:02,306] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:02,375] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run8
[2019-03-27 07:33:02,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:02,914] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:02,974] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run8
[2019-03-27 07:33:03,354] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:03,356] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:03,356] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:03,358] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:03,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run8
[2019-03-27 07:33:03,414] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run8
[2019-03-27 07:33:03,460] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:03,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:03,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run8
[2019-03-27 07:33:03,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:03,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:03,655] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run8
[2019-03-27 07:33:03,689] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:03,689] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:03,709] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run8
[2019-03-27 07:33:03,738] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:03,738] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:03,769] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run8
[2019-03-27 07:33:03,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:03,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:03,832] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run8
[2019-03-27 07:33:03,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:03,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:03,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run8
[2019-03-27 07:33:04,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:04,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:04,143] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run8
[2019-03-27 07:33:04,185] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:33:04,186] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:04,189] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run8
[2019-03-27 07:33:05,359] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8976738e-15 1.0000000e+00 6.5949433e-22 9.9076381e-12 1.5816379e-24], sum to 1.0000
[2019-03-27 07:33:05,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2832
[2019-03-27 07:33:05,371] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 86.0, 1.0, 2.0, 0.2889412922408359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462967.1389990058, 462967.1389990051, 164463.3166264564], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 18000.0000, 
sim time next is 18600.0000, 
raw observation next is [21.43333333333333, 86.0, 1.0, 2.0, 0.3538382837698558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566489.6619203629, 566489.6619203629, 172316.7511963779], 
processed observation next is [1.0, 0.21739130434782608, 0.21484992101105835, 0.86, 1.0, 1.0, 0.22149190815645278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15735823942232302, 0.15735823942232302, 0.2571891808901163], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.22673273], dtype=float32), 0.25329167]. 
=============================================
[2019-03-27 07:33:09,697] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.8782678e-15 1.0000000e+00 7.6707612e-20 6.8229472e-12 4.4692205e-25], sum to 1.0000
[2019-03-27 07:33:09,708] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2979
[2019-03-27 07:33:09,713] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 90.5, 1.0, 2.0, 0.3692542726211967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 567152.388750782, 567152.3887507827, 172060.6127550784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 102600.0000, 
sim time next is 103200.0000, 
raw observation next is [22.56666666666667, 90.66666666666667, 1.0, 2.0, 0.3710589140826002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569293.6442332807, 569293.6442332807, 172229.5895008858], 
processed observation next is [1.0, 0.17391304347826086, 0.26856240126382325, 0.9066666666666667, 1.0, 1.0, 0.24223965552120508, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15813712339813352, 0.15813712339813352, 0.2570590888072922], 
reward next is 0.7429, 
noisyNet noise sample is [array([-0.4401853], dtype=float32), 0.05606923]. 
=============================================
[2019-03-27 07:33:12,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4712572e-13 1.0000000e+00 1.4032745e-18 3.0541244e-08 1.9596529e-23], sum to 1.0000
[2019-03-27 07:33:12,881] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2605
[2019-03-27 07:33:12,886] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.75, 96.0, 1.0, 2.0, 0.9123297797886507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1360691.385182906, 1360691.385182906, 285705.5645954492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 138600.0000, 
sim time next is 139200.0000, 
raw observation next is [22.73333333333333, 96.0, 1.0, 2.0, 0.8486763137904157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1266470.075735952, 1266470.075735952, 267414.8147117951], 
processed observation next is [1.0, 0.6086956521739131, 0.27646129541864134, 0.96, 1.0, 1.0, 0.8176823057715852, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3517972432599867, 0.3517972432599867, 0.3991265891220822], 
reward next is 0.6009, 
noisyNet noise sample is [array([-0.85306525], dtype=float32), -0.20416246]. 
=============================================
[2019-03-27 07:33:16,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.2261031e-18 1.0000000e+00 1.1588651e-24 3.1043279e-15 2.3980464e-27], sum to 1.0000
[2019-03-27 07:33:16,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2289
[2019-03-27 07:33:16,098] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2936542726538034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 468752.1887341646, 468752.1887341653, 164843.3139199838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 897600.0000, 
sim time next is 898200.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2936097048136517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 468681.2068542926, 468681.2068542932, 164838.3568301626], 
processed observation next is [0.0, 0.391304347826087, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14892735519717068, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13018922412619238, 0.13018922412619255, 0.24602739825397404], 
reward next is 0.7540, 
noisyNet noise sample is [array([-0.34352386], dtype=float32), 1.4470941]. 
=============================================
[2019-03-27 07:33:22,364] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.8873245e-18 1.0000000e+00 5.1282179e-22 3.5111739e-16 3.8078359e-26], sum to 1.0000
[2019-03-27 07:33:22,374] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0132
[2019-03-27 07:33:22,381] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.93333333333334, 78.33333333333334, 1.0, 2.0, 0.3050516831218206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483565.4332241096, 483565.4332241096, 165839.4096343256], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 296400.0000, 
sim time next is 297000.0000, 
raw observation next is [23.0, 78.0, 1.0, 2.0, 0.3052345561612678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483650.6060567775, 483650.6060567775, 165841.4262128047], 
processed observation next is [0.0, 0.43478260869565216, 0.28909952606635075, 0.78, 1.0, 1.0, 0.16293320019429858, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13434739057132708, 0.13434739057132708, 0.24752451673552942], 
reward next is 0.7525, 
noisyNet noise sample is [array([-0.9136919], dtype=float32), -0.29210612]. 
=============================================
[2019-03-27 07:33:22,391] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[78.13495 ]
 [78.104744]
 [78.054985]
 [77.99108 ]
 [77.968544]], R is [[78.13585663]
 [78.10697937]
 [78.07855225]
 [78.05051422]
 [78.02277374]].
[2019-03-27 07:33:29,157] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9002707e-15 1.0000000e+00 1.3922784e-20 6.3103478e-13 1.5484797e-25], sum to 1.0000
[2019-03-27 07:33:29,165] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5615
[2019-03-27 07:33:29,172] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 85.0, 1.0, 2.0, 0.2397853028395814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 395649.4862059378, 395649.4862059372, 159880.4258597444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 436800.0000, 
sim time next is 437400.0000, 
raw observation next is [19.6, 85.0, 1.0, 2.0, 0.2393795938962013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 394980.12064865, 394980.1206486494, 159841.9166773104], 
processed observation next is [1.0, 0.043478260869565216, 0.127962085308057, 0.85, 1.0, 1.0, 0.08358987216409795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10971670018018055, 0.10971670018018038, 0.23857002489150805], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.9429795], dtype=float32), -0.6243656]. 
=============================================
[2019-03-27 07:33:32,429] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 07:33:32,431] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:33:32,432] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:32,433] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:33:32,434] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:32,434] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:33:32,435] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:32,436] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:33:32,439] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:32,442] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:33:32,454] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:33:33,374] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run61
[2019-03-27 07:33:33,443] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run61
[2019-03-27 07:33:33,467] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run61
[2019-03-27 07:33:33,470] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run61
[2019-03-27 07:33:33,507] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run61
[2019-03-27 07:34:03,369] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05447191], dtype=float32), 0.04709304]
[2019-03-27 07:34:03,370] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.63333333333333, 94.5, 1.0, 2.0, 0.3913974566059776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590541.7112810689, 590541.7112810689, 173856.4510719394]
[2019-03-27 07:34:03,375] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:34:03,378] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2781826e-17 1.0000000e+00 6.8353744e-23 2.0081084e-15 1.1529332e-26], sampled 0.377082724343895
[2019-03-27 07:34:16,614] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05447191], dtype=float32), 0.04709304]
[2019-03-27 07:34:16,616] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.83333333333333, 95.0, 1.0, 2.0, 0.313909469545086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496768.6923173688, 496768.6923173688, 166791.6523966831]
[2019-03-27 07:34:16,617] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:34:16,620] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2969909e-17 1.0000000e+00 2.2551422e-23 3.3470786e-15 1.5424706e-27], sampled 0.6048567171100918
[2019-03-27 07:34:32,663] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05447191], dtype=float32), 0.04709304]
[2019-03-27 07:34:32,665] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 84.0, 1.0, 2.0, 0.5435760300077559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759584.8184712358, 759584.8184712352, 190705.3339932352]
[2019-03-27 07:34:32,666] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:34:32,668] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0187040e-16 1.0000000e+00 3.8429098e-22 1.2332318e-12 9.7583889e-27], sampled 0.758278409903354
[2019-03-27 07:34:33,091] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05447191], dtype=float32), 0.04709304]
[2019-03-27 07:34:33,092] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.76913427166667, 79.76321531333335, 1.0, 2.0, 0.7814433995840966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1092147.944798519, 1092147.944798518, 239405.3333781121]
[2019-03-27 07:34:33,093] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:34:33,097] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.2930170e-15 1.0000000e+00 1.0708622e-19 2.1744642e-09 9.0538365e-25], sampled 0.1942034449941702
[2019-03-27 07:34:36,637] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05447191], dtype=float32), 0.04709304]
[2019-03-27 07:34:36,639] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.24945192, 75.28095284, 1.0, 2.0, 0.9047176718355876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1264539.079343433, 1264539.079343433, 271239.0709040608]
[2019-03-27 07:34:36,642] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:34:36,646] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4980434e-16 1.0000000e+00 8.8693219e-22 6.9922497e-13 5.4343481e-26], sampled 0.6428274387943038
[2019-03-27 07:34:36,805] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05447191], dtype=float32), 0.04709304]
[2019-03-27 07:34:36,806] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 89.0, 1.0, 2.0, 0.6124893879863407, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.949429851342798, 6.9112, 168.9126834941528, 1712534.498790696, 1685412.970315139, 366908.2980770548]
[2019-03-27 07:34:36,807] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:34:36,812] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.5912380e-13 9.9999988e-01 1.4148962e-17 1.3948484e-07 4.3411685e-22], sampled 0.9532825180013168
[2019-03-27 07:34:36,814] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1712534.498790696 W.
[2019-03-27 07:34:41,766] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05447191], dtype=float32), 0.04709304]
[2019-03-27 07:34:41,768] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.13905374666667, 90.44895176666668, 1.0, 2.0, 0.9579526591892065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1338993.437011968, 1338993.437011968, 286382.7122705539]
[2019-03-27 07:34:41,770] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:34:41,774] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.1355358e-16 1.0000000e+00 3.5204726e-21 2.7143691e-12 2.2382819e-25], sampled 0.21255968781092616
[2019-03-27 07:34:58,524] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05447191], dtype=float32), 0.04709304]
[2019-03-27 07:34:58,528] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.02914139, 58.78765249, 1.0, 2.0, 0.5583869732555141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780289.0055557345, 780289.0055557345, 193249.8274957413]
[2019-03-27 07:34:58,528] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:34:58,530] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.07781910e-17 1.00000000e+00 1.01072324e-22 1.42379428e-14
 8.24954912e-27], sampled 0.504217409301988
[2019-03-27 07:34:59,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05447191], dtype=float32), 0.04709304]
[2019-03-27 07:34:59,759] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.75, 91.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.245747413816964, 6.9112, 168.9109723994707, 1691254.138374216, 1453917.4766517, 311353.2769810186]
[2019-03-27 07:34:59,761] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:34:59,764] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.98843659e-13 9.99999881e-01 1.28593564e-17 6.50711272e-08
 1.49782030e-22], sampled 0.5160588526817431
[2019-03-27 07:34:59,765] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1691254.138374216 W.
[2019-03-27 07:35:25,505] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.4097 3163380770.9718 1750.0000
[2019-03-27 07:35:25,902] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.3935 2927241546.6122 1337.0000
[2019-03-27 07:35:26,113] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.5749 3007389771.3126 1758.0000
[2019-03-27 07:35:26,276] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.8955 2842330982.5294 1125.0000
[2019-03-27 07:35:26,420] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.0492 2779281233.3615 933.0000
[2019-03-27 07:35:27,437] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1500000, evaluation results [1500000.0, 7885.409700782256, 3163380770.9718337, 1750.0, 8251.393455953037, 2927241546.612199, 1337.0, 8659.049180597081, 2779281233.3614902, 933.0, 8002.574926530208, 3007389771.3125772, 1758.0, 8498.895473622388, 2842330982.529404, 1125.0]
[2019-03-27 07:35:30,271] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7688713e-16 1.0000000e+00 2.5036657e-21 4.7385441e-13 3.6511638e-26], sum to 1.0000
[2019-03-27 07:35:30,282] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7155
[2019-03-27 07:35:30,287] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.2, 91.0, 1.0, 2.0, 0.2048236585511596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 342264.3359998221, 342264.3359998221, 155835.6189262077], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 612000.0000, 
sim time next is 612600.0000, 
raw observation next is [17.18333333333333, 91.16666666666667, 1.0, 2.0, 0.245768296161783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 410712.8942951671, 410712.8942951671, 159480.1815492728], 
processed observation next is [1.0, 0.08695652173913043, 0.013428120063191062, 0.9116666666666667, 1.0, 1.0, 0.0912871038093771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11408691508199087, 0.11408691508199087, 0.23803012171533253], 
reward next is 0.7620, 
noisyNet noise sample is [array([-1.1530948], dtype=float32), -2.549158]. 
=============================================
[2019-03-27 07:35:32,839] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2377814e-17 1.0000000e+00 1.6518946e-22 4.3756943e-14 4.0659778e-27], sum to 1.0000
[2019-03-27 07:35:32,991] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7661
[2019-03-27 07:35:32,996] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.06666666666667, 77.66666666666667, 1.0, 2.0, 0.2335263237162743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 387063.5965167027, 387063.5965167021, 159162.1169401951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 594600.0000, 
sim time next is 595200.0000, 
raw observation next is [19.93333333333333, 78.33333333333334, 1.0, 2.0, 0.2323642358406649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 385292.552640969, 385292.5526409696, 159036.8003015741], 
processed observation next is [1.0, 0.9130434782608695, 0.14375987361769343, 0.7833333333333334, 1.0, 1.0, 0.07513763354296975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10702570906693583, 0.107025709066936, 0.23736835865906583], 
reward next is 0.7626, 
noisyNet noise sample is [array([1.3521549], dtype=float32), -0.043496866]. 
=============================================
[2019-03-27 07:35:37,068] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9786913e-15 1.0000000e+00 1.1993213e-21 6.3648921e-12 3.2999755e-25], sum to 1.0000
[2019-03-27 07:35:37,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8296
[2019-03-27 07:35:37,080] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 56.00000000000001, 1.0, 2.0, 0.2417796091616441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 397264.0672467303, 397264.0672467309, 160112.5659771853], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 667200.0000, 
sim time next is 667800.0000, 
raw observation next is [24.05, 57.0, 1.0, 2.0, 0.2372704020837894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 390053.9359032724, 390053.9359032724, 159683.1448555862], 
processed observation next is [1.0, 0.7391304347826086, 0.3388625592417062, 0.57, 1.0, 1.0, 0.08104867720938483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10834831552868679, 0.10834831552868679, 0.238333052023263], 
reward next is 0.7617, 
noisyNet noise sample is [array([-0.15640593], dtype=float32), 1.4952216]. 
=============================================
[2019-03-27 07:35:49,087] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1813715e-14 1.0000000e+00 1.9417299e-20 2.5966507e-10 5.0262434e-24], sum to 1.0000
[2019-03-27 07:35:49,099] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6704
[2019-03-27 07:35:49,110] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 62.0, 1.0, 2.0, 0.8520371013654272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1293168.079774459, 1293168.07977446, 271151.0565299764], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1171800.0000, 
sim time next is 1172400.0000, 
raw observation next is [27.4, 61.66666666666667, 1.0, 2.0, 0.8490237964735008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1287123.343506175, 1287123.343506175, 270109.6205624607], 
processed observation next is [1.0, 0.5652173913043478, 0.4976303317535545, 0.6166666666666667, 1.0, 1.0, 0.8181009596066274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3575342620850486, 0.3575342620850486, 0.4031486874066577], 
reward next is 0.5969, 
noisyNet noise sample is [array([1.1552027], dtype=float32), 0.117435984]. 
=============================================
[2019-03-27 07:35:51,928] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9030459e-17 1.0000000e+00 2.9262303e-23 3.9642132e-16 1.3591553e-27], sum to 1.0000
[2019-03-27 07:35:51,938] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7864
[2019-03-27 07:35:51,944] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 70.0, 1.0, 2.0, 0.3143487674942652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494670.7814844513, 494670.7814844513, 166573.5793522289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 919800.0000, 
sim time next is 920400.0000, 
raw observation next is [24.43333333333333, 70.33333333333333, 1.0, 2.0, 0.3137929996768977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493946.926019905, 493946.926019905, 166523.4966483734], 
processed observation next is [0.0, 0.6521739130434783, 0.3570300157977882, 0.7033333333333333, 1.0, 1.0, 0.17324457792397316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1372074794499736, 0.1372074794499736, 0.24854253231100507], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.19190635], dtype=float32), -0.708395]. 
=============================================
[2019-03-27 07:35:53,972] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5404424e-16 1.0000000e+00 2.2414650e-21 1.2143919e-13 4.1716873e-25], sum to 1.0000
[2019-03-27 07:35:53,984] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2950
[2019-03-27 07:35:53,990] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.33333333333334, 1.0, 2.0, 0.340482990389693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 526970.7987906242, 526970.7987906237, 168833.0523317622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 951600.0000, 
sim time next is 952200.0000, 
raw observation next is [21.8, 94.5, 1.0, 2.0, 0.3401548452683181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 526165.6061189561, 526165.6061189554, 168759.6553157746], 
processed observation next is [1.0, 0.0, 0.23222748815165886, 0.945, 1.0, 1.0, 0.2050058376726724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14615711281082114, 0.14615711281082094, 0.25188008256085764], 
reward next is 0.7481, 
noisyNet noise sample is [array([-0.41294432], dtype=float32), -0.2184242]. 
=============================================
[2019-03-27 07:35:56,073] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.9465149e-17 1.0000000e+00 6.9012417e-23 1.7759622e-12 3.1115561e-25], sum to 1.0000
[2019-03-27 07:35:56,083] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5677
[2019-03-27 07:35:56,088] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.6592111592591493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1015169.468267191, 1015169.46826719, 223890.1482475299], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 989400.0000, 
sim time next is 990000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6562439726076529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1010589.281588501, 1010589.281588501, 223221.2628403766], 
processed observation next is [1.0, 0.4782608695652174, 0.2417061611374408, 0.94, 1.0, 1.0, 0.5858361115754854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2807192448856947, 0.2807192448856947, 0.3331660639408606], 
reward next is 0.6668, 
noisyNet noise sample is [array([-1.0063007], dtype=float32), -0.8663238]. 
=============================================
[2019-03-27 07:35:56,104] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[71.95723 ]
 [72.06809 ]
 [72.216125]
 [72.30035 ]
 [72.253624]], R is [[71.87667084]
 [71.8237381 ]
 [71.77384949]
 [71.73495483]
 [71.70734406]].
[2019-03-27 07:36:00,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2731288e-16 1.0000000e+00 2.4489357e-21 1.0831641e-13 3.5626669e-27], sum to 1.0000
[2019-03-27 07:36:00,040] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6194
[2019-03-27 07:36:00,044] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 90.0, 1.0, 2.0, 0.4775967003707882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756464.660744584, 756464.6607445847, 190345.0809985363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1068600.0000, 
sim time next is 1069200.0000, 
raw observation next is [21.5, 89.0, 1.0, 2.0, 0.5346183266987886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847247.159568022, 847247.159568022, 200622.6061289288], 
processed observation next is [1.0, 0.391304347826087, 0.21800947867298584, 0.89, 1.0, 1.0, 0.4392991887937212, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23534643321333945, 0.23534643321333945, 0.2994367255655654], 
reward next is 0.7006, 
noisyNet noise sample is [array([0.75481784], dtype=float32), 0.4839021]. 
=============================================
[2019-03-27 07:36:13,989] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4475659e-17 1.0000000e+00 1.0475961e-21 1.1764163e-13 4.5298245e-26], sum to 1.0000
[2019-03-27 07:36:13,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0353
[2019-03-27 07:36:14,000] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 92.66666666666667, 1.0, 2.0, 0.3832052040680715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576963.7118294627, 576963.711829462, 172597.2331045948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1455600.0000, 
sim time next is 1456200.0000, 
raw observation next is [22.85, 93.0, 1.0, 2.0, 0.3826004951318505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576206.3941894145, 576206.3941894138, 172534.4013985867], 
processed observation next is [0.0, 0.8695652173913043, 0.28199052132701435, 0.93, 1.0, 1.0, 0.2561451748576512, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1600573317192818, 0.1600573317192816, 0.2575140319381891], 
reward next is 0.7425, 
noisyNet noise sample is [array([2.1020696], dtype=float32), 0.48208573]. 
=============================================
[2019-03-27 07:36:15,227] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9710557e-16 1.0000000e+00 2.0930404e-21 1.9721820e-14 5.9299320e-25], sum to 1.0000
[2019-03-27 07:36:15,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5851
[2019-03-27 07:36:15,244] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 97.83333333333334, 1.0, 2.0, 0.3187335954136062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502412.6620609707, 502412.6620609707, 167173.1726237352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1482600.0000, 
sim time next is 1483200.0000, 
raw observation next is [20.6, 98.0, 1.0, 2.0, 0.3167240878302783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499821.5339507469, 499821.5339507469, 166990.9866928654], 
processed observation next is [0.0, 0.17391304347826086, 0.17535545023696694, 0.98, 1.0, 1.0, 0.1767760094340702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13883931498631857, 0.13883931498631857, 0.24924027864606776], 
reward next is 0.7508, 
noisyNet noise sample is [array([-3.2045863], dtype=float32), 1.2928745]. 
=============================================
[2019-03-27 07:36:20,589] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 07:36:20,594] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:36:20,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:36:20,596] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:36:20,597] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:36:20,599] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:36:20,600] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:36:20,602] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:36:20,602] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:36:20,602] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:36:20,605] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:36:20,610] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.90241636e-16 1.00000000e+00 1.12205224e-20 2.67277980e-13
 1.51009631e-24], sum to 1.0000
[2019-03-27 07:36:20,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5222
[2019-03-27 07:36:20,613] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.33333333333334, 1.0, 2.0, 0.5588145818713958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780886.7647537526, 780886.7647537532, 193324.6083505794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2118000.0000, 
sim time next is 2118600.0000, 
raw observation next is [30.0, 75.5, 1.0, 2.0, 0.5597742409293968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782228.2848523678, 782228.2848523678, 193491.8349713187], 
processed observation next is [0.0, 0.5217391304347826, 0.6208530805687204, 0.755, 1.0, 1.0, 0.46960751919204424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21728563468121329, 0.21728563468121329, 0.28879378353928165], 
reward next is 0.7112, 
noisyNet noise sample is [array([0.0177888], dtype=float32), -0.9001863]. 
=============================================
[2019-03-27 07:36:20,627] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run62
[2019-03-27 07:36:20,646] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run62
[2019-03-27 07:36:20,665] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run62
[2019-03-27 07:36:20,691] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run62
[2019-03-27 07:36:20,692] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run62
[2019-03-27 07:36:23,062] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:36:23,064] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.58333333333334, 90.83333333333334, 1.0, 2.0, 0.3727829568078108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 571299.4506704927, 571299.4506704921, 172387.3284338278]
[2019-03-27 07:36:23,065] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:36:23,070] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1650180e-17 1.0000000e+00 2.1033243e-22 8.0445226e-15 4.2866908e-26], sampled 0.2217174494606443
[2019-03-27 07:36:26,830] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:36:26,832] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.68333333333333, 80.66666666666667, 1.0, 2.0, 0.2803668052568305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 464822.9465904854, 464822.9465904848, 163883.9558475055]
[2019-03-27 07:36:26,834] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:36:26,837] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.7672718e-17 1.0000000e+00 1.1462129e-22 3.9181203e-15 2.4154086e-26], sampled 0.379661884534651
[2019-03-27 07:36:29,385] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:36:29,390] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.18553406333334, 95.03473367333333, 1.0, 2.0, 0.2397020163435092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 396373.2091575573, 396373.209157558, 159823.2895430039]
[2019-03-27 07:36:29,393] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:36:29,396] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.9306267e-17 1.0000000e+00 1.9569721e-22 2.8329131e-15 6.3184961e-26], sampled 0.7441896974005712
[2019-03-27 07:36:49,412] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:36:49,413] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.7, 87.33333333333333, 1.0, 2.0, 0.519794348711968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726341.299330686, 726341.2993306867, 186753.0569712468]
[2019-03-27 07:36:49,414] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:36:49,418] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.6107080e-17 1.0000000e+00 3.0531358e-22 7.1297203e-14 2.3902572e-26], sampled 0.8285523398960173
[2019-03-27 07:36:58,629] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:36:58,631] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.10765590833333, 93.05499270000001, 1.0, 2.0, 0.8535099658104258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1192925.000184227, 1192925.000184228, 257468.6012459154]
[2019-03-27 07:36:58,633] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:36:58,636] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2439602e-16 1.0000000e+00 9.8932356e-22 4.2533758e-13 8.4856165e-26], sampled 0.6779524438203244
[2019-03-27 07:37:02,395] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:37:02,397] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.35, 71.16666666666667, 1.0, 2.0, 0.7604816954270701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1062837.134849528, 1062837.134849528, 234440.6731561451]
[2019-03-27 07:37:02,399] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:37:02,402] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.4644975e-16 1.0000000e+00 2.7895269e-21 2.2400434e-12 1.9877337e-25], sampled 0.8429032590057774
[2019-03-27 07:37:35,983] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:37:35,983] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.47902422, 87.823237645, 1.0, 2.0, 0.3971378652936162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 600037.9539398247, 600037.9539398241, 174742.1768564539]
[2019-03-27 07:37:35,986] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:37:35,987] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.2759576e-17 1.0000000e+00 1.6165858e-22 2.6246793e-14 1.4631316e-26], sampled 0.3079609458740524
[2019-03-27 07:37:46,164] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:37:46,165] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.53333333333333, 63.33333333333333, 1.0, 2.0, 0.8397602578746027, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005984011631991, 6.9112, 168.9123159880581, 2070720.891909642, 2003478.106831368, 417885.396438212]
[2019-03-27 07:37:46,166] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:37:46,172] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2216644e-09 9.0125841e-01 6.4812863e-13 9.8741516e-02 1.6698838e-17], sampled 0.17096848312952762
[2019-03-27 07:37:46,174] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2070720.891909642 W.
[2019-03-27 07:37:49,655] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:37:49,656] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.25898665, 100.0, 1.0, 2.0, 0.5086056797519355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710701.439064116, 710701.439064116, 184954.1535601308]
[2019-03-27 07:37:49,658] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:37:49,661] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3442304e-16 1.0000000e+00 4.5121046e-22 2.0712146e-13 3.3246001e-26], sampled 0.0409618018552963
[2019-03-27 07:37:52,610] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05501323], dtype=float32), 0.0468946]
[2019-03-27 07:37:52,611] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.81576256, 72.86749165, 1.0, 2.0, 0.5609492334473352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783870.8247518877, 783870.824751887, 193695.2432589452]
[2019-03-27 07:37:52,612] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:37:52,614] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.2159822e-17 1.0000000e+00 2.1953949e-22 6.5394183e-14 1.5812623e-26], sampled 0.8892663347495227
[2019-03-27 07:38:16,474] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.9229 2927099939.5444 1330.0000
[2019-03-27 07:38:16,582] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7897.5672 3162934547.4473 1742.0000
[2019-03-27 07:38:16,648] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.9324 3006821445.5302 1749.0000
[2019-03-27 07:38:16,692] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.0134 2842382708.5106 1126.0000
[2019-03-27 07:38:16,797] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.9367 2779302292.2931 934.0000
[2019-03-27 07:38:17,816] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1525000, evaluation results [1525000.0, 7897.567227178985, 3162934547.4472747, 1742.0, 8255.922864770837, 2927099939.544448, 1330.0, 8658.936663031065, 2779302292.293139, 934.0, 8002.932377871643, 3006821445.530223, 1749.0, 8499.013365523142, 2842382708.510587, 1126.0]
[2019-03-27 07:38:17,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.7843566e-15 1.0000000e+00 7.4338205e-20 1.4191448e-10 3.9034335e-26], sum to 1.0000
[2019-03-27 07:38:17,826] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1294
[2019-03-27 07:38:17,830] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 94.0, 1.0, 2.0, 0.4930067856161018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688897.216802289, 688897.216802289, 182507.62037615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1729800.0000, 
sim time next is 1730400.0000, 
raw observation next is [25.1, 94.0, 1.0, 2.0, 0.4911164248082961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686254.890200783, 686254.8902007823, 182215.8789517085], 
processed observation next is [1.0, 0.0, 0.38862559241706174, 0.94, 1.0, 1.0, 0.386887258805176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1906263583891064, 0.19062635838910622, 0.2719639984353858], 
reward next is 0.7280, 
noisyNet noise sample is [array([-1.4628457], dtype=float32), 0.6382392]. 
=============================================
[2019-03-27 07:38:18,395] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3616428e-17 1.0000000e+00 8.9723941e-23 5.6118887e-16 5.3478291e-27], sum to 1.0000
[2019-03-27 07:38:18,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7619
[2019-03-27 07:38:18,412] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 71.0, 1.0, 2.0, 0.4270633068830873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617325.7241782949, 617325.7241782949, 175610.5870608765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1432800.0000, 
sim time next is 1433400.0000, 
raw observation next is [27.26666666666667, 70.66666666666667, 1.0, 2.0, 0.4270259918218965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617269.5286702916, 617269.5286702916, 175605.094778121], 
processed observation next is [0.0, 0.6086956521739131, 0.4913112164297, 0.7066666666666667, 1.0, 1.0, 0.3096698696649355, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1714637579639699, 0.1714637579639699, 0.26209715638525527], 
reward next is 0.7379, 
noisyNet noise sample is [array([-0.26827982], dtype=float32), 1.3280996]. 
=============================================
[2019-03-27 07:38:26,098] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6834910e-11 9.9542254e-01 1.0045323e-14 4.5774174e-03 8.5237111e-19], sum to 1.0000
[2019-03-27 07:38:26,105] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6889
[2019-03-27 07:38:26,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2129801.655791949 W.
[2019-03-27 07:38:26,119] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.96666666666667, 86.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.863514914738495, 6.9112, 168.9076933999592, 2129801.655791949, 1454217.730108746, 311349.6669253307], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1873200.0000, 
sim time next is 1873800.0000, 
raw observation next is [26.95, 87.0, 1.0, 2.0, 0.6864924644158296, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.980059108414249, 6.9112, 168.9117718161232, 1856227.146804934, 1807376.467971631, 382672.581794178], 
processed observation next is [1.0, 0.6956521739130435, 0.476303317535545, 0.87, 1.0, 1.0, 0.6222800776094332, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.006885910841424892, 0.0, 0.829434127760896, 0.5156186518902595, 0.5020490188810086, 0.5711531071554895], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.79553086], dtype=float32), -0.28360984]. 
=============================================
[2019-03-27 07:38:27,249] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1202892e-13 9.9995399e-01 4.2784634e-15 4.6027606e-05 2.4073308e-19], sum to 1.0000
[2019-03-27 07:38:27,258] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7883
[2019-03-27 07:38:27,273] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1920489.685216105 W.
[2019-03-27 07:38:27,279] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.16666666666667, 68.33333333333334, 1.0, 2.0, 0.6868034791600198, 1.0, 2.0, 0.6868034791600198, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1920489.685216105, 1920489.685216104, 368523.6159467798], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2280000.0000, 
sim time next is 2280600.0000, 
raw observation next is [30.35, 67.5, 1.0, 2.0, 0.7150364290252934, 1.0, 2.0, 0.7150364290252934, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1999510.377583543, 1999510.377583543, 380625.0804915424], 
processed observation next is [1.0, 0.391304347826087, 0.637440758293839, 0.675, 1.0, 1.0, 0.6566703964160161, 1.0, 1.0, 0.6566703964160161, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5554195493287619, 0.5554195493287619, 0.5680971350620035], 
reward next is 0.4319, 
noisyNet noise sample is [array([-1.2410402], dtype=float32), -0.3420577]. 
=============================================
[2019-03-27 07:38:39,111] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8486343e-14 1.0000000e+00 7.5439802e-19 4.2282768e-09 1.4516424e-23], sum to 1.0000
[2019-03-27 07:38:39,122] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4116
[2019-03-27 07:38:39,126] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 84.5, 1.0, 2.0, 0.3927133440211353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 618773.9956941775, 618773.9956941782, 176834.4018436307], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1789800.0000, 
sim time next is 1790400.0000, 
raw observation next is [22.26666666666667, 85.0, 1.0, 2.0, 0.3156928407504398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497527.7290635515, 497527.7290635522, 166804.4611558323], 
processed observation next is [1.0, 0.7391304347826086, 0.2543443917851502, 0.85, 1.0, 1.0, 0.17553354307281901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13820214696209765, 0.13820214696209784, 0.24896188232213778], 
reward next is 0.7510, 
noisyNet noise sample is [array([0.3469026], dtype=float32), 0.9710479]. 
=============================================
[2019-03-27 07:38:41,503] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.641836e-16 1.000000e+00 9.903809e-22 2.951686e-11 2.251487e-25], sum to 1.0000
[2019-03-27 07:38:41,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9709
[2019-03-27 07:38:41,519] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.95, 89.0, 1.0, 2.0, 0.9414308579604683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1336882.48503153, 1336882.48503153, 284769.672348431], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1852200.0000, 
sim time next is 1852800.0000, 
raw observation next is [25.06666666666667, 88.66666666666667, 1.0, 2.0, 0.8853274306964721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1253347.294663158, 1253347.294663158, 268275.8140768221], 
processed observation next is [1.0, 0.43478260869565216, 0.38704581358609813, 0.8866666666666667, 1.0, 1.0, 0.8618402779475568, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34815202629532166, 0.34815202629532166, 0.400411662801227], 
reward next is 0.5996, 
noisyNet noise sample is [array([-0.66874284], dtype=float32), 0.9560314]. 
=============================================
[2019-03-27 07:38:53,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5336573e-18 1.0000000e+00 1.5646909e-24 2.9300064e-15 1.2108561e-27], sum to 1.0000
[2019-03-27 07:38:53,154] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6198
[2019-03-27 07:38:53,158] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.33333333333334, 1.0, 2.0, 0.508158368864783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710076.1791484087, 710076.1791484087, 184884.4324152849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2043600.0000, 
sim time next is 2044200.0000, 
raw observation next is [26.95, 84.66666666666667, 1.0, 2.0, 0.5084988919999548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710552.1690202173, 710552.1690202173, 184938.6034889804], 
processed observation next is [0.0, 0.6521739130434783, 0.476303317535545, 0.8466666666666667, 1.0, 1.0, 0.4078299903613913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1973756025056159, 0.1973756025056159, 0.2760277664014633], 
reward next is 0.7240, 
noisyNet noise sample is [array([1.6724885], dtype=float32), -1.4855659]. 
=============================================
[2019-03-27 07:38:55,439] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6452534e-15 1.0000000e+00 3.2625457e-20 3.1743150e-12 1.4696059e-23], sum to 1.0000
[2019-03-27 07:38:55,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1671
[2019-03-27 07:38:55,450] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.55, 94.00000000000001, 1.0, 2.0, 0.5093806351527911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711784.6877669791, 711784.6877669791, 185078.8189309198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2164200.0000, 
sim time next is 2164800.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5079720506626362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709815.7400262054, 709815.7400262054, 184854.4702663285], 
processed observation next is [1.0, 0.043478260869565216, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4071952417622122, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1971710388961682, 0.1971710388961682, 0.275902194427356], 
reward next is 0.7241, 
noisyNet noise sample is [array([1.3351582], dtype=float32), -1.0632454]. 
=============================================
[2019-03-27 07:38:56,419] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.15153560e-15 1.00000000e+00 1.04179820e-20 5.89983229e-10
 1.15514496e-23], sum to 1.0000
[2019-03-27 07:38:56,429] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9656
[2019-03-27 07:38:56,435] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 91.16666666666667, 1.0, 2.0, 0.6528246097881507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 912312.6748109472, 912312.6748109466, 210990.3965070712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2182200.0000, 
sim time next is 2182800.0000, 
raw observation next is [26.2, 90.33333333333334, 1.0, 2.0, 0.6942852340193061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 970279.7476539674, 970279.7476539667, 219615.0888536948], 
processed observation next is [1.0, 0.2608695652173913, 0.44075829383886256, 0.9033333333333334, 1.0, 1.0, 0.6316689566497663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26952215212610203, 0.26952215212610187, 0.3277837147070071], 
reward next is 0.6722, 
noisyNet noise sample is [array([-0.27981174], dtype=float32), -1.1013273]. 
=============================================
[2019-03-27 07:39:00,855] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6473768e-14 1.0000000e+00 2.1395014e-18 2.2841876e-10 6.3392200e-23], sum to 1.0000
[2019-03-27 07:39:00,865] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2574
[2019-03-27 07:39:00,869] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 95.16666666666667, 1.0, 2.0, 0.7088116804154643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 990590.2621577844, 990590.2621577844, 222755.8968294621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2171400.0000, 
sim time next is 2172000.0000, 
raw observation next is [24.93333333333333, 95.33333333333334, 1.0, 2.0, 0.6431525871204777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898790.4477961449, 898790.4477961449, 209050.4975111737], 
processed observation next is [1.0, 0.13043478260869565, 0.38072669826224315, 0.9533333333333335, 1.0, 1.0, 0.5700633579764792, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2496640132767069, 0.2496640132767069, 0.31201566792712493], 
reward next is 0.6880, 
noisyNet noise sample is [array([0.5307844], dtype=float32), 0.5040627]. 
=============================================
[2019-03-27 07:39:00,898] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[65.25933 ]
 [64.78618 ]
 [64.519745]
 [64.569405]
 [64.580414]], R is [[65.60083008]
 [65.61235046]
 [65.59712982]
 [65.57723999]
 [65.54299927]].
[2019-03-27 07:39:01,149] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.52290713e-10 8.46047580e-01 1.22705557e-12 1.53952450e-01
 1.29440514e-17], sum to 1.0000
[2019-03-27 07:39:01,157] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8517
[2019-03-27 07:39:01,161] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.6, 66.66666666666667, 1.0, 2.0, 0.6782688978827719, 1.0, 2.0, 0.6782688978827719, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1896603.547320803, 1896603.547320803, 364964.9347139589], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2207400.0000, 
sim time next is 2208000.0000, 
raw observation next is [31.7, 66.33333333333334, 1.0, 2.0, 0.843980699477347, 1.0, 2.0, 0.843980699477347, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2360461.013351771, 2360461.013351772, 441852.1631166154], 
processed observation next is [1.0, 0.5652173913043478, 0.7014218009478673, 0.6633333333333334, 1.0, 1.0, 0.8120249391293337, 1.0, 1.0, 0.8120249391293337, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6556836148199364, 0.6556836148199366, 0.6594808404725603], 
reward next is 0.3405, 
noisyNet noise sample is [array([-1.3459724], dtype=float32), 1.1663591]. 
=============================================
[2019-03-27 07:39:01,176] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[55.85084 ]
 [55.044895]
 [55.89741 ]
 [54.745235]
 [55.067398]], R is [[55.74443817]
 [55.18699265]
 [54.63512421]
 [54.59166718]
 [54.51994324]].
[2019-03-27 07:39:03,433] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7163745e-15 1.0000000e+00 3.4187536e-20 6.8593464e-10 2.7919819e-24], sum to 1.0000
[2019-03-27 07:39:03,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6954
[2019-03-27 07:39:03,449] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 83.0, 1.0, 2.0, 0.5417498416376717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757032.0214540289, 757032.0214540289, 190396.1012867672], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2237400.0000, 
sim time next is 2238000.0000, 
raw observation next is [28.0, 83.33333333333334, 1.0, 2.0, 0.5399945089657583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754578.278169787, 754578.2781697877, 190099.8050241836], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.8333333333333335, 1.0, 1.0, 0.4457765168262148, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2096050772693853, 0.2096050772693855, 0.28373105227490086], 
reward next is 0.7163, 
noisyNet noise sample is [array([1.5935553], dtype=float32), 0.94060946]. 
=============================================
[2019-03-27 07:39:03,464] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.079216]
 [71.156166]
 [70.91997 ]
 [70.965454]
 [70.77324 ]], R is [[71.12867737]
 [71.13321686]
 [71.13731384]
 [71.1410675 ]
 [71.14447021]].
[2019-03-27 07:39:10,937] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 07:39:10,940] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:39:10,941] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:39:10,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:39:10,946] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:39:10,947] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:39:10,943] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:39:10,948] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:39:10,949] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:39:10,950] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:39:10,949] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:39:10,972] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run63
[2019-03-27 07:39:10,995] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run63
[2019-03-27 07:39:11,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run63
[2019-03-27 07:39:11,038] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run63
[2019-03-27 07:39:11,064] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run63
[2019-03-27 07:39:32,181] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05653593], dtype=float32), 0.046216425]
[2019-03-27 07:39:32,187] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.92796336166667, 97.39362823166667, 1.0, 2.0, 0.3508100064566765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546849.8601088071, 546849.8601088071, 170547.3506811331]
[2019-03-27 07:39:32,188] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:39:32,191] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.7515542e-17 1.0000000e+00 4.1525297e-22 4.9391877e-14 6.1101419e-26], sampled 0.82861336151929
[2019-03-27 07:39:40,963] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05653593], dtype=float32), 0.046216425]
[2019-03-27 07:39:40,963] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.8, 90.33333333333333, 1.0, 2.0, 0.4178129203569713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616570.1281484322, 616570.1281484322, 175898.2729897643]
[2019-03-27 07:39:40,964] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:39:40,967] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5827006e-16 1.0000000e+00 7.1765158e-22 2.2040343e-14 2.0002388e-25], sampled 0.146887903589353
[2019-03-27 07:40:24,072] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05653593], dtype=float32), 0.046216425]
[2019-03-27 07:40:24,075] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.364560765, 95.92785796166667, 1.0, 2.0, 0.9946458524325124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1390315.511243715, 1390315.511243715, 297313.8247431199]
[2019-03-27 07:40:24,077] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:40:24,080] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2675563e-15 1.0000000e+00 1.4354826e-20 1.1253902e-11 2.6043996e-24], sampled 0.8801820107022996
[2019-03-27 07:40:42,995] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05653593], dtype=float32), 0.046216425]
[2019-03-27 07:40:42,999] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.95, 78.0, 1.0, 2.0, 0.5898827939376703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824318.2901059246, 824318.2901059246, 198875.6101827326]
[2019-03-27 07:40:43,000] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:40:43,000] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6697748e-16 1.0000000e+00 1.0149158e-21 4.8273885e-13 1.2848862e-25], sampled 0.15391887564711804
[2019-03-27 07:40:52,623] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05653593], dtype=float32), 0.046216425]
[2019-03-27 07:40:52,624] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.61103383666666, 74.00547613666666, 1.0, 2.0, 0.4125170688228483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 615627.2085893101, 615627.2085893095, 175990.5033308615]
[2019-03-27 07:40:52,625] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:40:52,627] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.5173059e-17 1.0000000e+00 2.9666719e-22 9.3246083e-14 3.6803899e-26], sampled 0.6528732804825303
[2019-03-27 07:40:55,199] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05653593], dtype=float32), 0.046216425]
[2019-03-27 07:40:55,201] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.96666666666667, 91.66666666666667, 1.0, 2.0, 0.6899515086013635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 964220.5162512264, 964220.5162512258, 218695.0385834693]
[2019-03-27 07:40:55,201] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:40:55,209] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1820110e-15 1.0000000e+00 6.7174566e-21 4.5947018e-12 1.0378651e-24], sampled 0.2801521629119912
[2019-03-27 07:40:55,885] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05653593], dtype=float32), 0.046216425]
[2019-03-27 07:40:55,887] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.40852098, 67.70896078333334, 1.0, 2.0, 0.6374567172585404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 964716.5373469549, 964716.5373469549, 217173.0255225656]
[2019-03-27 07:40:55,888] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:40:55,893] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2525358e-15 1.0000000e+00 7.4616877e-21 3.2881163e-12 1.2611842e-24], sampled 0.8952166846065029
[2019-03-27 07:41:06,524] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7923.3480 3159140956.0293 1639.0000
[2019-03-27 07:41:06,649] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.7817 2926431673.3763 1317.0000
[2019-03-27 07:41:06,941] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8019.6476 3004295643.5324 1691.0000
[2019-03-27 07:41:07,073] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.3938 2779001253.0331 927.0000
[2019-03-27 07:41:07,077] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8507.8898 2841285876.7974 1101.0000
[2019-03-27 07:41:08,095] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1550000, evaluation results [1550000.0, 7923.3480365589785, 3159140956.029333, 1639.0, 8259.781707613129, 2926431673.376287, 1317.0, 8662.393828267288, 2779001253.0331264, 927.0, 8019.647609697269, 3004295643.5323834, 1691.0, 8507.889764540925, 2841285876.7974215, 1101.0]
[2019-03-27 07:41:11,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0297765e-15 1.0000000e+00 2.7634418e-20 8.5993523e-09 7.1772986e-25], sum to 1.0000
[2019-03-27 07:41:11,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6741
[2019-03-27 07:41:11,142] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 91.0, 1.0, 2.0, 0.5519825497080537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771336.2255356542, 771336.2255356548, 192141.6382926534], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2489400.0000, 
sim time next is 2490000.0000, 
raw observation next is [27.16666666666667, 91.66666666666667, 1.0, 2.0, 0.5531454007898564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772961.776055016, 772961.776055016, 192341.8934636629], 
processed observation next is [1.0, 0.8260869565217391, 0.4865718799368091, 0.9166666666666667, 1.0, 1.0, 0.4616209648070559, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21471160445972665, 0.21471160445972665, 0.2870774529308402], 
reward next is 0.7129, 
noisyNet noise sample is [array([-1.1761956], dtype=float32), 0.2259888]. 
=============================================
[2019-03-27 07:41:11,153] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.39515]
 [71.49525]
 [72.42786]
 [73.16666]
 [73.84997]], R is [[71.16123962]
 [71.16284943]
 [71.16493225]
 [71.16768646]
 [71.17092133]].
[2019-03-27 07:41:20,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0401617e-17 1.0000000e+00 1.5604723e-22 4.3429178e-14 6.5364445e-27], sum to 1.0000
[2019-03-27 07:41:20,796] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1695
[2019-03-27 07:41:20,800] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.63333333333333, 92.83333333333333, 1.0, 2.0, 0.473072286715056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664660.3296078828, 664660.3296078828, 179952.5281528739], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2596200.0000, 
sim time next is 2596800.0000, 
raw observation next is [24.56666666666667, 92.66666666666667, 1.0, 2.0, 0.4683071221736013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660557.0414491901, 660557.0414491895, 179576.8516253193], 
processed observation next is [0.0, 0.043478260869565216, 0.3633491311216432, 0.9266666666666667, 1.0, 1.0, 0.3594061712934956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18348806706921947, 0.1834880670692193, 0.2680251516795811], 
reward next is 0.7320, 
noisyNet noise sample is [array([-0.3103506], dtype=float32), -0.30639657]. 
=============================================
[2019-03-27 07:41:29,707] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.3989971e-16 1.0000000e+00 1.2317512e-19 6.6616816e-11 2.9786875e-25], sum to 1.0000
[2019-03-27 07:41:29,718] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5385
[2019-03-27 07:41:29,724] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 94.0, 1.0, 2.0, 0.6903514935496612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1039737.312066003, 1039737.312066004, 228390.7154094192], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2899200.0000, 
sim time next is 2899800.0000, 
raw observation next is [22.5, 94.0, 1.0, 2.0, 0.7305418848079579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1106169.369365209, 1106169.369365209, 238633.0476941622], 
processed observation next is [1.0, 0.5652173913043478, 0.2654028436018958, 0.94, 1.0, 1.0, 0.6753516684433227, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3072692692681136, 0.3072692692681136, 0.35616872790173465], 
reward next is 0.6438, 
noisyNet noise sample is [array([-0.12105247], dtype=float32), -1.7427708]. 
=============================================
[2019-03-27 07:41:46,324] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8099088e-16 1.0000000e+00 1.0728816e-20 9.0016834e-13 9.5444261e-25], sum to 1.0000
[2019-03-27 07:41:46,334] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6618
[2019-03-27 07:41:46,344] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3529608422745807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548214.1912496258, 548214.1912496258, 170614.8479203354], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3046200.0000, 
sim time next is 3046800.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3370668014896229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 523520.2192488816, 523520.2192488823, 168611.7350252176], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 1.0, 1.0, 1.0, 0.2012853029995457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14542228312468933, 0.14542228312468952, 0.25165930600778746], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.27880552], dtype=float32), 1.5501945]. 
=============================================
[2019-03-27 07:41:47,167] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.7788348e-16 1.0000000e+00 4.2154639e-21 4.3986645e-10 1.2952699e-25], sum to 1.0000
[2019-03-27 07:41:47,176] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9074
[2019-03-27 07:41:47,181] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4362625591340673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633226.4892287307, 633226.4892287307, 177239.2031660166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3090000.0000, 
sim time next is 3090600.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4329084117847707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628357.4160136356, 628357.4160136356, 176759.1349589133], 
processed observation next is [1.0, 0.782608695652174, 0.28909952606635075, 1.0, 1.0, 1.0, 0.3167571226322538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17454372667045434, 0.17454372667045434, 0.2638196044162885], 
reward next is 0.7362, 
noisyNet noise sample is [array([0.413635], dtype=float32), 0.80631226]. 
=============================================
[2019-03-27 07:41:48,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9044423e-10 2.1152399e-01 2.4785198e-12 7.8847599e-01 4.6991035e-18], sum to 1.0000
[2019-03-27 07:41:48,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6012
[2019-03-27 07:41:48,296] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.678050137178681, 1.0, 2.0, 0.6596151081036031, 1.0, 1.0, 1.03, 7.005096001493559, 6.9112, 170.5573041426782, 2767686.343647218, 2700424.818332241, 514485.5360506875], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3758400.0000, 
sim time next is 3759000.0000, 
raw observation next is [33.16666666666666, 63.66666666666666, 1.0, 2.0, 0.9858492722097798, 1.0, 2.0, 0.9858492722097798, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2757679.560039384, 2757679.560039383, 520496.2697487206], 
processed observation next is [1.0, 0.5217391304347826, 0.7709320695102682, 0.6366666666666666, 1.0, 1.0, 0.9829509303732287, 1.0, 1.0, 0.9829509303732287, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.76602210001094, 0.7660221000109397, 0.776860104102568], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6794433], dtype=float32), 1.2249851]. 
=============================================
[2019-03-27 07:41:48,311] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[50.071102]
 [51.992977]
 [51.841915]
 [50.774204]
 [53.524776]], R is [[50.18009949]
 [49.67829895]
 [49.47779083]
 [49.27369308]
 [48.78095627]].
[2019-03-27 07:41:56,214] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9451471e-17 1.0000000e+00 1.7432315e-22 5.6650753e-14 2.3895825e-26], sum to 1.0000
[2019-03-27 07:41:56,220] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4854
[2019-03-27 07:41:56,225] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.5996724398922827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838004.0048726617, 838004.0048726617, 200685.6172654667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3258600.0000, 
sim time next is 3259200.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.598822191377483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 836815.3687192408, 836815.3687192402, 200527.3486329291], 
processed observation next is [0.0, 0.7391304347826086, 0.6998420221169038, 0.7233333333333334, 1.0, 1.0, 0.5166532426234736, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23244871353312244, 0.23244871353312227, 0.2992945501984016], 
reward next is 0.7007, 
noisyNet noise sample is [array([-1.6314458], dtype=float32), -1.6849841]. 
=============================================
[2019-03-27 07:42:00,997] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 07:42:00,999] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:42:00,999] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:42:01,000] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:42:01,001] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:42:01,001] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:42:01,004] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:42:01,002] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:42:01,005] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:42:01,006] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:42:01,008] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:42:01,029] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run64
[2019-03-27 07:42:01,050] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run64
[2019-03-27 07:42:01,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run64
[2019-03-27 07:42:01,071] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run64
[2019-03-27 07:42:01,071] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run64
[2019-03-27 07:42:15,671] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:42:15,672] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.51364279, 86.34997638, 1.0, 2.0, 0.3694483537772101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561376.9902914955, 561376.9902914955, 171389.7165242081]
[2019-03-27 07:42:15,673] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:42:15,676] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.4280333e-17 1.0000000e+00 1.2547981e-22 1.8400368e-14 2.1094267e-26], sampled 0.8209879786457748
[2019-03-27 07:42:26,469] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:42:26,471] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.63333333333334, 76.0, 1.0, 2.0, 0.7534758288684751, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.980478167961217, 6.9112, 168.9124875096883, 1949963.04573538, 1900814.865477519, 397583.3102241453]
[2019-03-27 07:42:26,472] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:42:26,476] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.9808325e-10 9.9066937e-01 2.8388937e-13 9.3305521e-03 1.1691242e-17], sampled 0.39649190977652404
[2019-03-27 07:42:26,476] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1949963.04573538 W.
[2019-03-27 07:42:29,428] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:42:29,429] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.18972213, 85.74212975, 1.0, 2.0, 0.9166851169746872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1281276.271657097, 1281276.271657096, 274573.1900678535]
[2019-03-27 07:42:29,430] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:42:29,434] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2117646e-15 1.0000000e+00 7.6609025e-21 6.3229292e-12 9.9871447e-25], sampled 0.264111332741397
[2019-03-27 07:42:30,042] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:42:30,044] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.49564018, 99.05521510333334, 1.0, 2.0, 0.4006418325233461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 598643.902932987, 598643.9029329875, 174435.8099342032]
[2019-03-27 07:42:30,047] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:42:30,050] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0851242e-16 1.0000000e+00 6.8497603e-22 1.1655839e-13 1.3649396e-25], sampled 0.5407649999145322
[2019-03-27 07:42:32,945] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:42:32,946] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.97722657, 94.16678106, 1.0, 2.0, 0.4330717227922162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 625498.4585191961, 625498.4585191967, 176394.0318488035]
[2019-03-27 07:42:32,947] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:42:32,949] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2956861e-16 1.0000000e+00 4.4888594e-22 1.9263276e-14 1.1961297e-25], sampled 0.8510981791239018
[2019-03-27 07:42:40,960] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:42:40,962] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.95492472, 77.35403628, 1.0, 2.0, 0.4705224435522997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662894.8361441524, 662894.8361441524, 179806.4088897872]
[2019-03-27 07:42:40,962] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:42:40,966] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3902591e-16 1.0000000e+00 3.9399440e-22 1.1515540e-13 5.8799455e-26], sampled 0.5494152584592543
[2019-03-27 07:43:11,837] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:43:11,843] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.38679189, 95.06606764, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.118887975249283, 6.9112, 168.9118142430503, 2431153.534399222, 2283813.533076769, 475679.262651264]
[2019-03-27 07:43:11,845] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:43:11,848] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.9302146e-11 9.9982613e-01 1.6739858e-14 1.7394191e-04 6.4347011e-19], sampled 0.521878716452613
[2019-03-27 07:43:11,850] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2431153.534399222 W.
[2019-03-27 07:43:12,020] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:43:12,021] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.1, 53.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.054840505997051, 6.9112, 168.9119607471789, 1555727.633379658, 1453824.715434807, 311356.386332965]
[2019-03-27 07:43:12,021] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:43:12,023] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.10437325e-14 1.00000000e+00 5.49408322e-20 1.99743000e-09
 4.09198481e-24], sampled 0.3069552640115053
[2019-03-27 07:43:38,022] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:43:38,024] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.91666666666667, 74.83333333333334, 1.0, 2.0, 0.8156757905608464, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.981759862768937, 6.9112, 168.91247999851, 2037012.517504252, 1986955.063587898, 412617.207484535]
[2019-03-27 07:43:38,025] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:43:38,028] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2225664e-10 9.9972516e-01 4.2487024e-14 2.7477747e-04 1.0088381e-17], sampled 0.4711339861599376
[2019-03-27 07:43:38,028] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2037012.517504252 W.
[2019-03-27 07:43:46,319] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:43:46,322] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.76666666666667, 86.0, 1.0, 2.0, 0.4748253106977512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 663483.6133647504, 663483.6133647497, 179745.5331999008]
[2019-03-27 07:43:46,324] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:43:46,328] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.3359547e-16 1.0000000e+00 2.3415736e-21 1.4861237e-12 3.4366190e-25], sampled 0.49873754134374826
[2019-03-27 07:43:55,750] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05798729], dtype=float32), 0.046551004]
[2019-03-27 07:43:55,751] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 70.0, 1.0, 2.0, 0.6380828330290433, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.998994582679501, 6.9112, 168.9124026842606, 1784154.520188379, 1721870.225795542, 371834.9869546794]
[2019-03-27 07:43:55,752] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:43:55,756] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4720028e-09 9.5360637e-01 1.0929956e-12 4.6393648e-02 3.8219119e-17], sampled 0.5107034322809499
[2019-03-27 07:43:55,757] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1784154.520188379 W.
[2019-03-27 07:43:57,319] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8020.2756 3004514438.7730 1690.0000
[2019-03-27 07:43:57,503] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7916.8239 3159835805.6911 1664.0000
[2019-03-27 07:43:57,618] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8264.3322 2926393618.6049 1307.0000
[2019-03-27 07:43:57,684] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8665.3368 2778882094.5916 921.0000
[2019-03-27 07:43:57,699] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8505.5816 2841063216.9190 1097.0000
[2019-03-27 07:43:58,715] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1575000, evaluation results [1575000.0, 7916.823948405336, 3159835805.691143, 1664.0, 8264.332194319151, 2926393618.604933, 1307.0, 8665.336788079556, 2778882094.591596, 921.0, 8020.27558539998, 3004514438.773037, 1690.0, 8505.581551451922, 2841063216.9190264, 1097.0]
[2019-03-27 07:43:59,593] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.1856113e-17 1.0000000e+00 3.3055115e-22 5.3564829e-13 6.8013244e-24], sum to 1.0000
[2019-03-27 07:43:59,601] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5468
[2019-03-27 07:43:59,606] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5736982499282367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801693.0319605313, 801693.0319605318, 195948.9383480781], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3328200.0000, 
sim time next is 3328800.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.573834901793472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801884.0630500738, 801884.0630500738, 195973.3259548839], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.4865480744499662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22274557306946494, 0.22274557306946494, 0.29249750142519987], 
reward next is 0.7075, 
noisyNet noise sample is [array([0.38733643], dtype=float32), -1.1279918]. 
=============================================
[2019-03-27 07:44:03,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3631839e-15 1.0000000e+00 2.4250004e-19 1.3986774e-12 3.3819327e-25], sum to 1.0000
[2019-03-27 07:44:03,155] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8603
[2019-03-27 07:44:03,160] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5369806270146702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750365.2480657027, 750365.2480657027, 189592.9827728655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3366600.0000, 
sim time next is 3367200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.536715163418273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749994.1639529181, 749994.1639529187, 189548.5090164137], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 1.0, 1.0, 0.44182549809430477, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20833171220914393, 0.2083317122091441, 0.28290822241255775], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.31483293], dtype=float32), 0.20401171]. 
=============================================
[2019-03-27 07:44:03,615] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.8192947e-16 1.0000000e+00 1.1324666e-21 8.9756284e-12 3.5946831e-25], sum to 1.0000
[2019-03-27 07:44:03,624] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4596
[2019-03-27 07:44:03,632] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5176435178990764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723334.7850147382, 723334.7850147382, 186406.6696250263], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3539400.0000, 
sim time next is 3540000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5187864221581748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724932.3796360098, 724932.3796360098, 186591.8366918704], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42022460500984915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20137010545444717, 0.20137010545444717, 0.2784952786445827], 
reward next is 0.7215, 
noisyNet noise sample is [array([0.30023542], dtype=float32), -0.73043907]. 
=============================================
[2019-03-27 07:44:03,662] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.99935 ]
 [70.988335]
 [70.945305]
 [70.959564]
 [70.96281 ]], R is [[71.00373077]
 [71.01547241]
 [71.02729797]
 [71.03889465]
 [71.05025482]].
[2019-03-27 07:44:07,506] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2987944e-12 9.9994457e-01 2.0743848e-17 5.5471311e-05 1.1955498e-22], sum to 1.0000
[2019-03-27 07:44:07,515] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0282
[2019-03-27 07:44:07,521] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.5568726483658775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778172.1153874134, 778172.1153874134, 192988.5860634009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3606000.0000, 
sim time next is 3606600.0000, 
raw observation next is [32.16666666666666, 66.33333333333333, 1.0, 2.0, 0.5573724294602133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778870.7639969952, 778870.7639969952, 193075.2298727081], 
processed observation next is [1.0, 0.7391304347826086, 0.7235387045813582, 0.6633333333333333, 1.0, 1.0, 0.46671377043399187, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21635298999916533, 0.21635298999916533, 0.28817198488463897], 
reward next is 0.7118, 
noisyNet noise sample is [array([1.7807857], dtype=float32), 0.05041349]. 
=============================================
[2019-03-27 07:44:08,996] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.4007044e-11 9.9871862e-01 1.9270423e-15 1.2814588e-03 6.7380105e-21], sum to 1.0000
[2019-03-27 07:44:09,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7571
[2019-03-27 07:44:09,012] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 68.5, 1.0, 2.0, 0.5410660158107942, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756076.1147816492, 756076.1147816486, 190283.0136622214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3519000.0000, 
sim time next is 3519600.0000, 
raw observation next is [31.33333333333333, 69.0, 1.0, 2.0, 0.5461151192979079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763134.1775027339, 763134.1775027339, 191138.8040503989], 
processed observation next is [1.0, 0.7391304347826086, 0.6840442338072668, 0.69, 1.0, 1.0, 0.4531507461420577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21198171597298163, 0.21198171597298163, 0.2852817970901476], 
reward next is 0.7147, 
noisyNet noise sample is [array([0.65139943], dtype=float32), -1.4678999]. 
=============================================
[2019-03-27 07:44:11,514] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0807222e-14 1.0000000e+00 2.5370959e-20 1.3087871e-10 1.7336471e-24], sum to 1.0000
[2019-03-27 07:44:11,522] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4703
[2019-03-27 07:44:11,528] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5168245954510621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722190.065849511, 722190.0658495104, 186274.2302118073], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3538800.0000, 
sim time next is 3539400.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.5176435178990764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723334.7850147382, 723334.7850147382, 186406.6696250263], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.41884761192659803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20092632917076061, 0.20092632917076061, 0.27821890988809894], 
reward next is 0.7218, 
noisyNet noise sample is [array([0.27914095], dtype=float32), -0.013776257]. 
=============================================
[2019-03-27 07:44:17,133] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8714830e-15 1.0000000e+00 1.6192174e-19 2.4115293e-10 1.0999573e-24], sum to 1.0000
[2019-03-27 07:44:17,137] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1593
[2019-03-27 07:44:17,144] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 79.0, 1.0, 2.0, 0.5000131816004845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698690.7412866844, 698690.7412866838, 183598.1716212607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3630600.0000, 
sim time next is 3631200.0000, 
raw observation next is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5019401521394582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701384.2721856466, 701384.2721856473, 183900.7382228879], 
processed observation next is [1.0, 0.0, 0.4944707740916275, 0.8066666666666668, 1.0, 1.0, 0.3999278941439255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19482896449601295, 0.19482896449601314, 0.27447871376550437], 
reward next is 0.7255, 
noisyNet noise sample is [array([-0.8108247], dtype=float32), -1.4841899]. 
=============================================
[2019-03-27 07:44:19,759] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8074876e-10 7.3356336e-01 3.4776934e-13 2.6643670e-01 1.1143077e-17], sum to 1.0000
[2019-03-27 07:44:19,768] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0271
[2019-03-27 07:44:19,776] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2700238.696541065 W.
[2019-03-27 07:44:19,781] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 0.9653367725765261, 1.0, 2.0, 0.9653367725765261, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2700238.696541065, 2700238.696541065, 508387.9920858667], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3684000.0000, 
sim time next is 3684600.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.9606432400802787, 1.0, 2.0, 0.9606432400802787, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2687095.834950826, 2687095.834950826, 505653.8373844618], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.59, 1.0, 1.0, 0.9525822169641912, 1.0, 1.0, 0.9525822169641912, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7464155097085629, 0.7464155097085629, 0.7547072199768086], 
reward next is 0.2453, 
noisyNet noise sample is [array([-0.19573148], dtype=float32), -0.44692716]. 
=============================================
[2019-03-27 07:44:24,121] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0945336e-14 9.9999988e-01 2.1731685e-18 1.7549077e-07 2.1982595e-23], sum to 1.0000
[2019-03-27 07:44:24,126] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9522
[2019-03-27 07:44:24,132] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5365075531008595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749703.9514058627, 749703.9514058627, 189514.2663557047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3783600.0000, 
sim time next is 3784200.0000, 
raw observation next is [30.83333333333334, 67.5, 1.0, 2.0, 0.5378671673391603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751604.5188493406, 751604.5188493399, 189742.479166377], 
processed observation next is [1.0, 0.8260869565217391, 0.6603475513428123, 0.675, 1.0, 1.0, 0.4432134546254943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20877903301370573, 0.20877903301370554, 0.2831977300990701], 
reward next is 0.7168, 
noisyNet noise sample is [array([-1.3196673], dtype=float32), 1.2413676]. 
=============================================
[2019-03-27 07:44:33,215] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0728275e-14 1.0000000e+00 1.0299510e-21 4.6051071e-12 8.2761246e-26], sum to 1.0000
[2019-03-27 07:44:33,228] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8909
[2019-03-27 07:44:33,234] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.579580615603349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809916.2565093081, 809916.2565093081, 197003.6025038642], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3913200.0000, 
sim time next is 3913800.0000, 
raw observation next is [29.16666666666667, 83.16666666666667, 1.0, 2.0, 0.5860418472139938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818948.7772540991, 818948.7772540991, 198174.1133729149], 
processed observation next is [0.0, 0.30434782608695654, 0.581358609794629, 0.8316666666666667, 1.0, 1.0, 0.5012552376072213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.227485771459472, 0.227485771459472, 0.29578225876554465], 
reward next is 0.7042, 
noisyNet noise sample is [array([0.44800243], dtype=float32), 1.531389]. 
=============================================
[2019-03-27 07:44:41,392] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3212764e-16 1.0000000e+00 3.7662855e-20 3.5106287e-10 2.0550848e-24], sum to 1.0000
[2019-03-27 07:44:41,399] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2911
[2019-03-27 07:44:41,407] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5441111608541516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 760332.8697530448, 760332.8697530441, 190796.0943649762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4053600.0000, 
sim time next is 4054200.0000, 
raw observation next is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.543206161525321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759067.7852456463, 759067.785245647, 190642.5440530445], 
processed observation next is [1.0, 0.9565217391304348, 0.5181674565560824, 0.8483333333333333, 1.0, 1.0, 0.44964597774135057, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21085216256823508, 0.21085216256823527, 0.2845411105269321], 
reward next is 0.7155, 
noisyNet noise sample is [array([0.37542522], dtype=float32), -0.94126135]. 
=============================================
[2019-03-27 07:44:46,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.3976853e-14 9.9987960e-01 1.1423160e-16 1.2042138e-04 8.9785332e-24], sum to 1.0000
[2019-03-27 07:44:46,105] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6352
[2019-03-27 07:44:46,110] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.16666666666666, 55.0, 1.0, 2.0, 0.5658835847088228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 790768.6571080822, 790768.6571080829, 194564.895443552], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4211400.0000, 
sim time next is 4212000.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5756656329101973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 804443.3183669555, 804443.3183669561, 196302.1556021057], 
processed observation next is [1.0, 0.782608695652174, 0.8578199052132701, 0.56, 1.0, 1.0, 0.4887537745905992, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2234564773241543, 0.22345647732415447, 0.29298829194344134], 
reward next is 0.7070, 
noisyNet noise sample is [array([0.25512645], dtype=float32), -0.12186399]. 
=============================================
[2019-03-27 07:44:46,126] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.7551 ]
 [70.9182 ]
 [68.67678]
 [64.65834]
 [60.11938]], R is [[73.6993103 ]
 [73.67192078]
 [73.64649963]
 [73.62309265]
 [73.59884644]].
[2019-03-27 07:44:46,667] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9051553e-14 1.0000000e+00 5.1085056e-19 1.1441346e-09 3.7990348e-23], sum to 1.0000
[2019-03-27 07:44:46,675] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9237
[2019-03-27 07:44:46,679] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.8841526440840171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1235778.266542298, 1235778.266542298, 265618.3853730501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4168800.0000, 
sim time next is 4169400.0000, 
raw observation next is [28.33333333333334, 88.16666666666667, 1.0, 2.0, 0.8064759847043237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1127152.139103602, 1127152.139103602, 245509.9468453412], 
processed observation next is [1.0, 0.2608695652173913, 0.5418641390205374, 0.8816666666666667, 1.0, 1.0, 0.7668385357883418, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3130978164176672, 0.3130978164176672, 0.3664327564855839], 
reward next is 0.6336, 
noisyNet noise sample is [array([0.83253324], dtype=float32), -0.63124996]. 
=============================================
[2019-03-27 07:44:51,690] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 07:44:51,692] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:44:51,693] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:44:51,694] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:44:51,695] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:44:51,697] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:44:51,696] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:44:51,700] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:44:51,697] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:44:51,704] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:44:51,706] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:44:51,722] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run65
[2019-03-27 07:44:51,746] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run65
[2019-03-27 07:44:51,768] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run65
[2019-03-27 07:44:51,769] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run65
[2019-03-27 07:44:51,769] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run65
[2019-03-27 07:44:53,513] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06143231], dtype=float32), 0.046054322]
[2019-03-27 07:44:53,514] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.66666666666667, 64.0, 1.0, 2.0, 1.011280045543195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1413582.259378448, 1413582.259378449, 302378.3183694988]
[2019-03-27 07:44:53,516] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:44:53,520] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.3500038e-15 1.0000000e+00 2.7366178e-20 5.9570682e-11 5.9738228e-24], sampled 0.03480967453926409
[2019-03-27 07:45:31,889] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06143231], dtype=float32), 0.046054322]
[2019-03-27 07:45:31,890] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.24333838666667, 96.80170974166666, 1.0, 2.0, 0.3873852603764223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586229.7214969224, 586229.7214969217, 173514.0211873135]
[2019-03-27 07:45:31,891] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:45:31,895] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4349982e-16 1.0000000e+00 4.0625928e-22 3.2525523e-14 1.8885504e-25], sampled 0.15245471379390418
[2019-03-27 07:45:33,550] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06143231], dtype=float32), 0.046054322]
[2019-03-27 07:45:33,551] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.86666666666667, 88.66666666666667, 1.0, 2.0, 0.7181231768540326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1003609.568193591, 1003609.568193591, 224805.3987304602]
[2019-03-27 07:45:33,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:45:33,557] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0973638e-15 1.0000000e+00 4.6472635e-21 2.5540026e-12 1.6062020e-24], sampled 0.12566139371950125
[2019-03-27 07:45:48,247] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06143231], dtype=float32), 0.046054322]
[2019-03-27 07:45:48,248] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.33333333333334, 73.33333333333334, 1.0, 2.0, 0.5643785685439304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 788664.7589306388, 788664.7589306394, 194297.4962826935]
[2019-03-27 07:45:48,250] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:45:48,252] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8813462e-16 1.0000000e+00 9.6109350e-22 1.5309334e-13 3.6485482e-25], sampled 0.4253291439877741
[2019-03-27 07:46:01,282] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06143231], dtype=float32), 0.046054322]
[2019-03-27 07:46:01,284] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.16666666666667, 69.33333333333333, 1.0, 2.0, 0.9476580340803215, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00599055779735, 6.9112, 168.9123159439321, 2221742.102130724, 2154494.67301262, 447616.2508588706]
[2019-03-27 07:46:01,286] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:46:01,288] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1699793e-09 9.4554579e-01 9.5463698e-13 5.4454189e-02 2.6115597e-17], sampled 0.973253013996616
[2019-03-27 07:46:34,388] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06143231], dtype=float32), 0.046054322]
[2019-03-27 07:46:34,390] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.9, 94.33333333333334, 1.0, 2.0, 0.5335401553355503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745555.9192887181, 745555.9192887181, 189017.2227604637]
[2019-03-27 07:46:34,391] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:46:34,396] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.5734889e-16 1.0000000e+00 2.9902151e-21 1.4042159e-12 1.0428895e-24], sampled 0.23902234449361892
[2019-03-27 07:46:39,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06143231], dtype=float32), 0.046054322]
[2019-03-27 07:46:39,370] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.7, 85.0, 1.0, 2.0, 0.5829689552782108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814652.9984120787, 814652.9984120787, 197615.4571152079]
[2019-03-27 07:46:39,371] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:46:39,373] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.7359540e-16 1.0000000e+00 2.8709704e-21 2.0347142e-12 8.7916300e-25], sampled 0.8038375942438319
[2019-03-27 07:46:47,628] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9878 2779131868.1217 933.0000
[2019-03-27 07:46:47,819] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8003.2942 3006774974.0562 1742.0000
[2019-03-27 07:46:47,829] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7896.1922 3162135492.5578 1727.0000
[2019-03-27 07:46:48,080] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.7225 2927082270.0557 1331.0000
[2019-03-27 07:46:48,096] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.0699 2842045645.7688 1122.0000
[2019-03-27 07:46:49,113] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1600000, evaluation results [1600000.0, 7896.192204775494, 3162135492.5578203, 1727.0, 8254.722457177655, 2927082270.055736, 1331.0, 8659.98775396918, 2779131868.121704, 933.0, 8003.294240772717, 3006774974.0561924, 1742.0, 8498.069858019126, 2842045645.7687874, 1122.0]
[2019-03-27 07:46:51,948] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8466282e-15 1.0000000e+00 1.8486039e-20 5.1884295e-11 2.1748124e-24], sum to 1.0000
[2019-03-27 07:46:51,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5905
[2019-03-27 07:46:51,959] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 87.33333333333333, 1.0, 2.0, 0.5657433917194599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790572.6777913194, 790572.6777913194, 194537.339257731], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4581600.0000, 
sim time next is 4582200.0000, 
raw observation next is [28.0, 88.16666666666667, 1.0, 2.0, 0.5706578546402168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 797442.7510791074, 797442.7510791081, 195406.6287452879], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.8816666666666667, 1.0, 1.0, 0.48272030679544187, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22151187529975205, 0.22151187529975225, 0.2916516846944595], 
reward next is 0.7083, 
noisyNet noise sample is [array([0.26331908], dtype=float32), 1.6391041]. 
=============================================
[2019-03-27 07:46:56,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5502369e-11 7.0957020e-02 2.2096884e-14 9.2904299e-01 1.5363770e-18], sum to 1.0000
[2019-03-27 07:46:56,702] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5435
[2019-03-27 07:46:56,712] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.66666666666667, 68.66666666666667, 1.0, 2.0, 0.967298911389938, 1.0, 2.0, 0.967298911389938, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2705733.131962603, 2705733.131962603, 509545.2940474059], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4371600.0000, 
sim time next is 4372200.0000, 
raw observation next is [33.0, 65.5, 1.0, 2.0, 0.9404464185120814, 1.0, 2.0, 0.9404464185120814, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2630542.135445145, 2630542.135445146, 494044.652548458], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.655, 1.0, 1.0, 0.9282486970025078, 1.0, 1.0, 0.9282486970025078, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7307061487347625, 0.7307061487347627, 0.7373800784305343], 
reward next is 0.2626, 
noisyNet noise sample is [array([0.5602624], dtype=float32), 1.2090508]. 
=============================================
[2019-03-27 07:47:02,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3864035e-16 1.0000000e+00 6.7840478e-22 1.1252748e-14 3.1894404e-26], sum to 1.0000
[2019-03-27 07:47:02,376] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3580
[2019-03-27 07:47:02,381] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 73.66666666666667, 1.0, 2.0, 0.6069456993342348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 848171.9795230635, 848171.9795230641, 202046.9249382585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4468800.0000, 
sim time next is 4469400.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5976065777655433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835115.9593134685, 835115.9593134685, 200301.1169543692], 
processed observation next is [0.0, 0.7391304347826086, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5151886479102931, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23197665536485237, 0.23197665536485237, 0.29895689097667044], 
reward next is 0.7010, 
noisyNet noise sample is [array([0.56367534], dtype=float32), -0.08368807]. 
=============================================
[2019-03-27 07:47:06,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5946017e-17 1.0000000e+00 9.0913272e-23 1.8070994e-14 3.1638699e-27], sum to 1.0000
[2019-03-27 07:47:06,129] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4694
[2019-03-27 07:47:06,133] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5233596320074735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731325.008657745, 731325.0086577456, 187336.6337033267], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5176200.0000, 
sim time next is 5176800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.524049294369557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732289.0516479195, 732289.0516479195, 187449.4901735304], 
processed observation next is [0.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42656541490308075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2034136254577554, 0.2034136254577554, 0.2797753584679558], 
reward next is 0.7202, 
noisyNet noise sample is [array([-0.29758713], dtype=float32), -0.32533172]. 
=============================================
[2019-03-27 07:47:06,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1236279e-17 1.0000000e+00 9.9215383e-23 1.4957824e-14 6.4028788e-28], sum to 1.0000
[2019-03-27 07:47:06,782] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4178
[2019-03-27 07:47:06,788] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 59.0, 1.0, 2.0, 0.5268508633309894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736205.227550024, 736205.2275500234, 187910.214314848], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4554000.0000, 
sim time next is 4554600.0000, 
raw observation next is [31.66666666666667, 60.83333333333334, 1.0, 2.0, 0.5312471535549296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742350.6152273774, 742350.6152273774, 188636.7076648846], 
processed observation next is [0.0, 0.7391304347826086, 0.6998420221169038, 0.6083333333333334, 1.0, 1.0, 0.4352375344035296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20620850422982706, 0.20620850422982706, 0.2815473248729621], 
reward next is 0.7185, 
noisyNet noise sample is [array([-1.4022259], dtype=float32), -0.35707888]. 
=============================================
[2019-03-27 07:47:12,584] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0188742e-13 9.9999952e-01 1.0756899e-17 4.6380848e-07 8.6496225e-23], sum to 1.0000
[2019-03-27 07:47:12,593] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3319
[2019-03-27 07:47:12,601] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 70.0, 1.0, 2.0, 0.5263492198758447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735504.0036963735, 735504.0036963728, 187826.8308498702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4649400.0000, 
sim time next is 4650000.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.5123961890151861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715999.9018980737, 715999.9018980744, 185560.360413835], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7, 1.0, 1.0, 0.41252552893395916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19888886163835381, 0.198888861638354, 0.276955761811694], 
reward next is 0.7230, 
noisyNet noise sample is [array([0.6728266], dtype=float32), 1.1576482]. 
=============================================
[2019-03-27 07:47:12,624] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[69.604324]
 [70.13547 ]
 [70.81392 ]
 [71.323265]
 [71.924194]], R is [[69.19404602]
 [69.22176361]
 [69.24580383]
 [69.26583862]
 [69.28411865]].
[2019-03-27 07:47:14,463] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5602282e-14 1.0000000e+00 2.2749583e-20 9.6792867e-09 7.3376481e-24], sum to 1.0000
[2019-03-27 07:47:14,471] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5974
[2019-03-27 07:47:14,480] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 1.016906575026808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1421452.371466654, 1421452.371466655, 304120.8110854432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4694400.0000, 
sim time next is 4695000.0000, 
raw observation next is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 1.012963877539185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1415937.514453931, 1415937.514453931, 302902.2274587305], 
processed observation next is [1.0, 0.34782608695652173, 0.581358609794629, 0.7833333333333334, 1.0, 1.0, 1.015619129565283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.39331597623720305, 0.39331597623720305, 0.4520928768040754], 
reward next is 0.5479, 
noisyNet noise sample is [array([-1.028539], dtype=float32), 2.4243243]. 
=============================================
[2019-03-27 07:47:14,495] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[62.03285 ]
 [62.36281 ]
 [62.409946]
 [62.446472]
 [62.401367]], R is [[61.77249908]
 [61.70086288]
 [61.68171692]
 [61.667202  ]
 [61.66235352]].
[2019-03-27 07:47:20,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6386658e-09 2.2001332e-01 1.8315318e-13 7.7998668e-01 1.2125951e-17], sum to 1.0000
[2019-03-27 07:47:20,971] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9254
[2019-03-27 07:47:20,977] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666666, 65.66666666666667, 1.0, 2.0, 0.822756999143502, 1.0, 2.0, 0.822756999143502, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2301047.522350274, 2301047.522350273, 431100.1382595912], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4804800.0000, 
sim time next is 4805400.0000, 
raw observation next is [31.08333333333334, 65.83333333333333, 1.0, 2.0, 0.8271980781844092, 1.0, 2.0, 0.8271980781844092, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2313479.616929636, 2313479.616929636, 433326.939412588], 
processed observation next is [1.0, 0.6086956521739131, 0.6721958925750398, 0.6583333333333333, 1.0, 1.0, 0.7918049134751918, 1.0, 1.0, 0.7918049134751918, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6426332269248989, 0.6426332269248989, 0.6467566259889374], 
reward next is 0.3532, 
noisyNet noise sample is [array([-1.9961252], dtype=float32), 2.400981]. 
=============================================
[2019-03-27 07:47:22,271] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.0401886e-15 1.0000000e+00 8.0475675e-21 8.2780505e-11 3.0183226e-24], sum to 1.0000
[2019-03-27 07:47:22,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0161
[2019-03-27 07:47:22,282] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4935206361137114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 689615.4727992949, 689615.4727992954, 182586.8451089793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4828200.0000, 
sim time next is 4828800.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4922288899963707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687809.8816305388, 687809.8816305388, 182387.2570110616], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.74, 1.0, 1.0, 0.38822757830888033, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19105830045292746, 0.19105830045292746, 0.272219786583674], 
reward next is 0.7278, 
noisyNet noise sample is [array([2.2882469], dtype=float32), -1.6824478]. 
=============================================
[2019-03-27 07:47:36,528] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9507323e-17 1.0000000e+00 1.0110163e-22 6.3060614e-14 5.2966284e-26], sum to 1.0000
[2019-03-27 07:47:36,535] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2110
[2019-03-27 07:47:36,544] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.482364311720772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674146.5682088706, 674146.5682088699, 180894.5739038551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5111400.0000, 
sim time next is 5112000.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4835689770511917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675829.8095865631, 675829.8095865624, 181077.0919183193], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.84, 1.0, 1.0, 0.37779394825444784, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1877305026629342, 0.187730502662934, 0.27026431629599895], 
reward next is 0.7297, 
noisyNet noise sample is [array([-0.282441], dtype=float32), 0.20831066]. 
=============================================
[2019-03-27 07:47:36,562] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.14914]
 [73.1799 ]
 [73.21096]
 [73.29509]
 [73.34394]], R is [[73.16631317]
 [73.16466522]
 [73.16336823]
 [73.16188812]
 [73.1602478 ]].
[2019-03-27 07:47:38,777] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.05992733e-17 1.00000000e+00 1.26641622e-22 1.00659925e-13
 1.13606363e-26], sum to 1.0000
[2019-03-27 07:47:38,783] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3462
[2019-03-27 07:47:38,786] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.2, 53.0, 1.0, 2.0, 0.5179315487928451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723737.4052203717, 723737.4052203717, 186453.8137669691], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5754000.0000, 
sim time next is 5754600.0000, 
raw observation next is [33.4, 53.0, 1.0, 2.0, 0.5210283406326912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728066.2247017447, 728066.2247017447, 186957.1641838482], 
processed observation next is [0.0, 0.6086956521739131, 0.7819905213270142, 0.53, 1.0, 1.0, 0.422925711605652, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20224061797270687, 0.20224061797270687, 0.27904054355798236], 
reward next is 0.7210, 
noisyNet noise sample is [array([-0.13977082], dtype=float32), -1.2094126]. 
=============================================
[2019-03-27 07:47:41,383] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8677887e-17 1.0000000e+00 2.9111562e-22 2.8463778e-15 3.9895600e-25], sum to 1.0000
[2019-03-27 07:47:41,396] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1102
[2019-03-27 07:47:41,401] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5517852310908158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771060.3939191472, 771060.3939191466, 192107.7926408297], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5148000.0000, 
sim time next is 5148600.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5587069674811876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780736.3292790169, 780736.3292790176, 193305.1201312974], 
processed observation next is [0.0, 0.6086956521739131, 0.7156398104265403, 0.63, 1.0, 1.0, 0.46832164756769595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2168712025775047, 0.2168712025775049, 0.2885151046735782], 
reward next is 0.7115, 
noisyNet noise sample is [array([0.34524235], dtype=float32), 0.16084678]. 
=============================================
[2019-03-27 07:47:42,233] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 07:47:42,235] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:47:42,236] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:47:42,236] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:47:42,239] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:47:42,240] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:47:42,240] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:47:42,244] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:47:42,244] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:47:42,245] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:47:42,247] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:47:42,270] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run66
[2019-03-27 07:47:42,291] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run66
[2019-03-27 07:47:42,314] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run66
[2019-03-27 07:47:42,315] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run66
[2019-03-27 07:47:42,332] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run66
[2019-03-27 07:47:55,755] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06312985], dtype=float32), 0.046593517]
[2019-03-27 07:47:55,755] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.7, 96.0, 1.0, 2.0, 0.3964882551488776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 592925.4866486357, 592925.4866486357, 173925.084588858]
[2019-03-27 07:47:55,757] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:47:55,761] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4245405e-17 1.0000000e+00 4.7574109e-23 3.9624011e-15 1.1684387e-26], sampled 0.516215595345852
[2019-03-27 07:48:11,799] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06312985], dtype=float32), 0.046593517]
[2019-03-27 07:48:11,801] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.41666666666667, 95.0, 1.0, 2.0, 0.3861409291546903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585223.6379533588, 585223.6379533588, 173447.507432081]
[2019-03-27 07:48:11,804] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:48:11,807] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.1493259e-17 1.0000000e+00 8.7668233e-23 1.9976182e-15 5.1245116e-26], sampled 0.07726746363550319
[2019-03-27 07:48:23,288] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06312985], dtype=float32), 0.046593517]
[2019-03-27 07:48:23,290] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.63595799, 92.54591660333332, 1.0, 2.0, 0.4307040826689609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628765.388192679, 628765.388192679, 176894.8329760757]
[2019-03-27 07:48:23,291] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:48:23,295] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.2721604e-17 1.0000000e+00 6.9970965e-23 5.8117624e-15 1.7645640e-26], sampled 0.06791880229506175
[2019-03-27 07:49:16,503] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06312985], dtype=float32), 0.046593517]
[2019-03-27 07:49:16,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.68989493333333, 66.28243662666667, 1.0, 2.0, 0.6303087045088344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 923003.249761559, 923003.249761559, 211937.3243092968]
[2019-03-27 07:49:16,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:49:16,509] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4361104e-16 1.0000000e+00 1.1566629e-21 1.0635868e-12 1.9204434e-25], sampled 0.7710500624003509
[2019-03-27 07:49:26,963] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06312985], dtype=float32), 0.046593517]
[2019-03-27 07:49:26,964] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.66666666666667, 83.5, 1.0, 2.0, 0.7735471197100248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1081106.459433546, 1081106.459433546, 237526.2442949234]
[2019-03-27 07:49:26,966] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:49:26,969] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.3802712e-16 1.0000000e+00 1.8110714e-21 1.4531465e-12 4.1551695e-25], sampled 0.2667237120612165
[2019-03-27 07:49:38,091] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9127 2779180216.1062 933.0000
[2019-03-27 07:49:38,228] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0940 2926979154.1900 1330.0000
[2019-03-27 07:49:38,345] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7896.3850 3162373073.2923 1740.0000
[2019-03-27 07:49:38,410] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.8968 2842176354.8784 1127.0000
[2019-03-27 07:49:38,483] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7569 3007430923.0608 1755.0000
[2019-03-27 07:49:39,500] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1625000, evaluation results [1625000.0, 7896.384955180117, 3162373073.2923293, 1740.0, 8254.09396120621, 2926979154.1900067, 1330.0, 8659.91268860203, 2779180216.1061745, 933.0, 7998.756863457044, 3007430923.060824, 1755.0, 8498.896789761142, 2842176354.878429, 1127.0]
[2019-03-27 07:49:53,244] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.6225992e-15 1.0000000e+00 5.2750114e-19 1.6277133e-09 3.0816601e-24], sum to 1.0000
[2019-03-27 07:49:53,245] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3815
[2019-03-27 07:49:53,249] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.86666666666667, 81.66666666666667, 1.0, 2.0, 0.600158711295077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838683.806446046, 838683.806446046, 200775.5470674199], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5437200.0000, 
sim time next is 5437800.0000, 
raw observation next is [29.8, 81.5, 1.0, 2.0, 0.5962352784512599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833198.906919074, 833198.906919074, 200046.7184252969], 
processed observation next is [1.0, 0.9565217391304348, 0.6113744075829385, 0.815, 1.0, 1.0, 0.5135364800617589, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2314441408108539, 0.2314441408108539, 0.2985771916795476], 
reward next is 0.7014, 
noisyNet noise sample is [array([-0.3121515], dtype=float32), -2.034833]. 
=============================================
[2019-03-27 07:49:59,207] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0112121e-16 1.0000000e+00 1.7028246e-21 2.1323958e-11 9.5445346e-25], sum to 1.0000
[2019-03-27 07:49:59,217] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0460
[2019-03-27 07:49:59,221] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.26666666666667, 84.5, 1.0, 2.0, 0.5625131380973192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786057.034772072, 786057.034772072, 193969.6634648023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5523000.0000, 
sim time next is 5523600.0000, 
raw observation next is [28.13333333333333, 85.0, 1.0, 2.0, 0.5625300381840287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786080.6597273023, 786080.6597273023, 193972.4697935905], 
processed observation next is [1.0, 0.9565217391304348, 0.532385466034755, 0.85, 1.0, 1.0, 0.4729277568482273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21835573881313952, 0.21835573881313952, 0.2895111489456575], 
reward next is 0.7105, 
noisyNet noise sample is [array([-1.5724087], dtype=float32), -0.72829235]. 
=============================================
[2019-03-27 07:50:01,397] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8251703e-09 1.9063012e-01 2.7671635e-12 8.0936986e-01 1.9228997e-15], sum to 1.0000
[2019-03-27 07:50:01,406] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4909
[2019-03-27 07:50:01,410] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.65, 71.5, 1.0, 2.0, 0.5921776529445599, 1.0, 1.0, 0.5921776529445599, 1.0, 2.0, 1.02841674702232, 6.9112, 6.9112, 170.5573041426782, 2484443.439970391, 2484443.439970391, 484738.8943905136], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5563800.0000, 
sim time next is 5564400.0000, 
raw observation next is [30.86666666666667, 70.33333333333333, 1.0, 2.0, 0.8768359334827439, 1.0, 2.0, 0.8768359334827439, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2452441.316837389, 2452441.31683739, 459011.8282038143], 
processed observation next is [1.0, 0.391304347826087, 0.6619273301737759, 0.7033333333333333, 1.0, 1.0, 0.8516095584129445, 1.0, 1.0, 0.8516095584129445, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.681233699121497, 0.6812336991214972, 0.6850922809012153], 
reward next is 0.3149, 
noisyNet noise sample is [array([1.4863119], dtype=float32), 1.5200369]. 
=============================================
[2019-03-27 07:50:03,501] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4111658e-16 1.0000000e+00 8.7984585e-21 6.5557038e-10 5.4204792e-25], sum to 1.0000
[2019-03-27 07:50:03,512] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7293
[2019-03-27 07:50:03,519] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 81.5, 1.0, 2.0, 0.5558250749686544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776707.7043472845, 776707.7043472845, 192805.4338136173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5599800.0000, 
sim time next is 5600400.0000, 
raw observation next is [28.63333333333333, 83.66666666666667, 1.0, 2.0, 0.5594036030144646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781710.165004735, 781710.165004735, 193427.1474426003], 
processed observation next is [1.0, 0.8260869565217391, 0.55608214849921, 0.8366666666666667, 1.0, 1.0, 0.4691609674873067, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21714171250131528, 0.21714171250131528, 0.28869723498895566], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.7067861], dtype=float32), 1.0754739]. 
=============================================
[2019-03-27 07:50:08,065] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.3785008e-18 1.0000000e+00 1.2667918e-24 1.4662345e-16 5.3995242e-28], sum to 1.0000
[2019-03-27 07:50:08,073] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4591
[2019-03-27 07:50:08,080] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 71.66666666666666, 1.0, 2.0, 0.5329130095245321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744679.2539343757, 744679.2539343757, 188913.6345181573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5683200.0000, 
sim time next is 5683800.0000, 
raw observation next is [29.43333333333333, 72.83333333333334, 1.0, 2.0, 0.5322740261263339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743786.0419720585, 743786.0419720579, 188807.3144585471], 
processed observation next is [0.0, 0.782608695652174, 0.5939968404423379, 0.7283333333333334, 1.0, 1.0, 0.4364747302726914, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20660723388112737, 0.2066072338811272, 0.2818019618784285], 
reward next is 0.7182, 
noisyNet noise sample is [array([-0.50509197], dtype=float32), -1.3752106]. 
=============================================
[2019-03-27 07:50:10,191] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1527983e-16 1.0000000e+00 7.9096915e-22 3.9235947e-15 2.3977560e-25], sum to 1.0000
[2019-03-27 07:50:10,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7760
[2019-03-27 07:50:10,208] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.03333333333333, 90.0, 1.0, 2.0, 0.5099661090461131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712603.0760283974, 712603.0760283968, 185172.0625785768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5721600.0000, 
sim time next is 5722200.0000, 
raw observation next is [26.2, 89.0, 1.0, 2.0, 0.5105956776912598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713483.1016054852, 713483.1016054852, 185272.6464170006], 
processed observation next is [0.0, 0.21739130434782608, 0.44075829383886256, 0.89, 1.0, 1.0, 0.4103562381822407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19818975044596812, 0.19818975044596812, 0.27652633793582176], 
reward next is 0.7235, 
noisyNet noise sample is [array([-2.1093414], dtype=float32), -0.7645949]. 
=============================================
[2019-03-27 07:50:24,596] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3661999e-09 6.0578948e-01 2.5836100e-12 3.9421052e-01 5.4712627e-17], sum to 1.0000
[2019-03-27 07:50:24,602] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5954
[2019-03-27 07:50:24,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1804787.687109463 W.
[2019-03-27 07:50:24,621] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 70.5, 1.0, 2.0, 0.6454610728474989, 1.0, 2.0, 0.6454610728474989, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1804787.687109463, 1804787.687109463, 351649.8974589081], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6006600.0000, 
sim time next is 6007200.0000, 
raw observation next is [29.0, 71.66666666666666, 1.0, 2.0, 0.4167036867757603, 1.0, 2.0, 0.4167036867757603, 1.0, 1.0, 0.7122610637695072, 6.9112, 6.9112, 170.5573041426782, 1747684.777586382, 1747684.777586382, 360639.0428666213], 
processed observation next is [1.0, 0.5217391304347826, 0.5734597156398105, 0.7166666666666666, 1.0, 1.0, 0.297233357561157, 1.0, 1.0, 0.297233357561157, 1.0, 0.5, 0.6490988582554964, 0.0, 0.0, 0.8375144448122397, 0.485467993773995, 0.485467993773995, 0.5382672281591362], 
reward next is 0.4617, 
noisyNet noise sample is [array([-1.1828345], dtype=float32), -0.36059114]. 
=============================================
[2019-03-27 07:50:25,099] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.5701449e-14 1.0000000e+00 6.2806661e-20 2.6225327e-10 3.6388084e-23], sum to 1.0000
[2019-03-27 07:50:25,110] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1473
[2019-03-27 07:50:25,115] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.23333333333333, 91.83333333333333, 1.0, 2.0, 0.7524190764490069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1051563.36746616, 1051563.367466161, 232565.8088089804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5971800.0000, 
sim time next is 5972400.0000, 
raw observation next is [26.2, 92.0, 1.0, 2.0, 0.7353930691574068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027756.701699502, 1027756.701699502, 228669.3756156479], 
processed observation next is [1.0, 0.13043478260869565, 0.44075829383886256, 0.92, 1.0, 1.0, 0.6811964688643455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2854879726943061, 0.2854879726943061, 0.34129757554574314], 
reward next is 0.6587, 
noisyNet noise sample is [array([0.79489356], dtype=float32), -1.0999367]. 
=============================================
[2019-03-27 07:50:27,284] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0983034e-11 9.9871290e-01 5.3107096e-14 1.2871124e-03 4.3466340e-19], sum to 1.0000
[2019-03-27 07:50:27,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0187
[2019-03-27 07:50:27,299] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2303325.643117706 W.
[2019-03-27 07:50:27,304] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.1, 65.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.107975323737744, 6.9112, 168.9062787656593, 2303325.643117706, 1454325.604023046, 311272.8291125372], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6095400.0000, 
sim time next is 6096000.0000, 
raw observation next is [31.1, 65.0, 1.0, 2.0, 0.8171653791350685, 1.0, 1.0, 0.8171653791350685, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2285394.847166601, 2285394.847166601, 428311.3707655856], 
processed observation next is [1.0, 0.5652173913043478, 0.6729857819905214, 0.65, 1.0, 1.0, 0.7797173242591187, 1.0, 0.5, 0.7797173242591187, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6348319019907225, 0.6348319019907225, 0.6392707026352025], 
reward next is 0.3607, 
noisyNet noise sample is [array([-0.43464664], dtype=float32), -0.3819855]. 
=============================================
[2019-03-27 07:50:27,328] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[58.540756]
 [57.06061 ]
 [58.183304]
 [58.184616]
 [55.368713]], R is [[55.89398956]
 [55.33504868]
 [54.78170013]
 [54.2338829 ]
 [53.69154358]].
[2019-03-27 07:50:29,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7485277e-15 1.0000000e+00 9.9158405e-20 1.0448328e-09 5.1519802e-23], sum to 1.0000
[2019-03-27 07:50:29,301] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5549
[2019-03-27 07:50:29,304] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.06666666666667, 83.66666666666667, 1.0, 2.0, 0.7124276490214955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995646.0795226158, 995646.0795226158, 223551.506195066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6074400.0000, 
sim time next is 6075000.0000, 
raw observation next is [28.2, 83.0, 1.0, 2.0, 0.7095382681428593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 991606.1691697689, 991606.1691697689, 222918.5250526618], 
processed observation next is [1.0, 0.30434782608695654, 0.5355450236966824, 0.83, 1.0, 1.0, 0.650046106196216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2754461581027136, 0.2754461581027136, 0.3327142164965102], 
reward next is 0.6673, 
noisyNet noise sample is [array([-0.6021964], dtype=float32), 0.77079195]. 
=============================================
[2019-03-27 07:50:29,318] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[63.411327]
 [63.36379 ]
 [63.191444]
 [62.954327]
 [63.06967 ]], R is [[63.54396439]
 [63.57486343]
 [63.61121368]
 [63.64041901]
 [63.64933014]].
[2019-03-27 07:50:29,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5161351e-14 1.0000000e+00 6.9496810e-19 3.1971727e-09 1.0457988e-22], sum to 1.0000
[2019-03-27 07:50:29,957] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7916
[2019-03-27 07:50:29,964] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.61666666666667, 92.0, 1.0, 2.0, 0.7940471286807047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1109772.164176467, 1109772.164176467, 242455.0252983724], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6144600.0000, 
sim time next is 6145200.0000, 
raw observation next is [26.6, 92.0, 1.0, 2.0, 0.7691406544920268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1074944.883617544, 1074944.883617544, 236477.1537974897], 
processed observation next is [1.0, 0.13043478260869565, 0.4597156398104266, 0.92, 1.0, 1.0, 0.7218562102313576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2985958010048734, 0.2985958010048734, 0.3529509758171488], 
reward next is 0.6470, 
noisyNet noise sample is [array([1.0838917], dtype=float32), -0.6263665]. 
=============================================
[2019-03-27 07:50:30,180] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5111010e-14 1.0000000e+00 6.9543187e-20 5.8372285e-10 5.7358411e-23], sum to 1.0000
[2019-03-27 07:50:30,187] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3477
[2019-03-27 07:50:30,191] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.73333333333333, 80.16666666666667, 1.0, 2.0, 0.9318173360033805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1302439.968490294, 1302439.968490294, 278840.8741183142], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6077400.0000, 
sim time next is 6078000.0000, 
raw observation next is [28.86666666666667, 79.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.853983225804193, 6.9112, 168.8962106021904, 3542558.270254852, 1455053.541314377, 306783.9370471115], 
processed observation next is [1.0, 0.34782608695652173, 0.567140600315956, 0.7933333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.29427832258041925, 0.0, 0.8293577150765313, 0.9840439639596811, 0.4041815392539936, 0.45788647320464404], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7483331], dtype=float32), -0.30688322]. 
=============================================
[2019-03-27 07:50:30,198] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[63.104427]
 [63.621975]
 [63.660473]
 [63.65832 ]
 [63.616257]], R is [[59.90960693]
 [59.89433289]
 [59.94762802]
 [60.01634598]
 [60.08611679]].
[2019-03-27 07:50:32,551] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5907456e-16 1.0000000e+00 1.2919916e-20 2.3187381e-10 1.0136683e-24], sum to 1.0000
[2019-03-27 07:50:32,558] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9332
[2019-03-27 07:50:32,563] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 86.0, 1.0, 2.0, 0.5259561978209946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734954.6169261284, 734954.616926129, 187762.9799009111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6120000.0000, 
sim time next is 6120600.0000, 
raw observation next is [27.2, 86.0, 1.0, 2.0, 0.5261270172167318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735193.3971707496, 735193.3971707496, 187791.0509652534], 
processed observation next is [1.0, 0.8695652173913043, 0.4881516587677725, 0.86, 1.0, 1.0, 0.42906869544184556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.204220388102986, 0.204220388102986, 0.28028515069440807], 
reward next is 0.7197, 
noisyNet noise sample is [array([-0.77071595], dtype=float32), -0.057646614]. 
=============================================
[2019-03-27 07:50:32,655] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 07:50:32,657] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:50:32,658] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:50:32,662] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:50:32,665] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:50:32,666] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:50:32,665] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:50:32,661] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:50:32,669] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:50:32,670] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:50:32,668] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:50:32,699] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run67
[2019-03-27 07:50:32,723] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run67
[2019-03-27 07:50:32,723] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run67
[2019-03-27 07:50:32,723] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run67
[2019-03-27 07:50:32,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run67
[2019-03-27 07:50:53,156] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06497453], dtype=float32), 0.04640831]
[2019-03-27 07:50:53,160] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.76666666666667, 60.33333333333334, 1.0, 2.0, 0.2418073485432245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 400373.2727637105, 400373.2727637105, 159983.2110349566]
[2019-03-27 07:50:53,163] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:50:53,168] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9701433e-17 1.0000000e+00 5.2762286e-23 3.9480070e-15 3.2055532e-26], sampled 0.23811121696652493
[2019-03-27 07:50:54,078] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06497453], dtype=float32), 0.04640831]
[2019-03-27 07:50:54,079] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.9, 52.0, 1.0, 2.0, 0.2774204107794405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 451049.9654135235, 451049.9654135229, 163632.139997703]
[2019-03-27 07:50:54,081] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:50:54,084] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.8204685e-17 1.0000000e+00 7.4764677e-23 4.8986991e-16 9.8912157e-26], sampled 0.6507373088366578
[2019-03-27 07:51:13,618] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06497453], dtype=float32), 0.04640831]
[2019-03-27 07:51:13,619] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.45, 83.5, 1.0, 2.0, 0.5672289583428313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 792649.3906920834, 792649.390692084, 194798.9276302756]
[2019-03-27 07:51:13,620] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:51:13,623] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.7588129e-17 1.0000000e+00 8.1088514e-23 8.0548903e-14 1.8695473e-26], sampled 0.8097670990263865
[2019-03-27 07:51:51,498] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06497453], dtype=float32), 0.04640831]
[2019-03-27 07:51:51,499] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.616536085, 63.97160887666666, 1.0, 2.0, 0.5260598806785972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735099.5502160569, 735099.5502160576, 187779.2671084526]
[2019-03-27 07:51:51,500] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:51:51,503] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.3416372e-16 1.0000000e+00 3.1264469e-21 1.5686546e-11 1.4082244e-25], sampled 0.30292593342729157
[2019-03-27 07:52:01,419] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06497453], dtype=float32), 0.04640831]
[2019-03-27 07:52:01,422] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.41209329333333, 84.23690330000001, 1.0, 2.0, 0.5796952150292398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835264.1861712233, 835264.1861712233, 200244.2304126973]
[2019-03-27 07:52:01,423] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:52:01,425] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.4696509e-16 1.0000000e+00 1.5774647e-21 5.4117552e-12 3.4180668e-25], sampled 0.8117867187507996
[2019-03-27 07:52:05,863] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06497453], dtype=float32), 0.04640831]
[2019-03-27 07:52:05,866] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.26291937, 61.50766891166666, 1.0, 2.0, 0.4577558685273935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 656129.7195301864, 656129.7195301857, 179358.1184453683]
[2019-03-27 07:52:05,867] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:52:05,871] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.9061616e-17 1.0000000e+00 7.9693827e-23 1.3899715e-14 3.7907629e-26], sampled 0.1831683572546725
[2019-03-27 07:52:19,366] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06497453], dtype=float32), 0.04640831]
[2019-03-27 07:52:19,368] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 87.0, 1.0, 2.0, 0.3220762306654523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 507762.1145266998, 507762.1145267004, 167580.1688882732]
[2019-03-27 07:52:19,370] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:52:19,374] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.0222368e-17 1.0000000e+00 1.0474677e-22 2.3427959e-15 1.0465434e-25], sampled 0.030228277501043954
[2019-03-27 07:52:25,975] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06497453], dtype=float32), 0.04640831]
[2019-03-27 07:52:25,977] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.23333333333334, 64.33333333333334, 1.0, 2.0, 1.011285974658771, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.989143665166571, 6.9112, 168.9124165523147, 2310798.903333323, 2255503.161412757, 467424.469924499]
[2019-03-27 07:52:25,981] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:52:25,983] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4076996e-09 9.8788095e-01 2.3449141e-13 1.2119028e-02 3.4046595e-17], sampled 0.30230536896457216
[2019-03-27 07:52:25,984] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2310798.903333323 W.
[2019-03-27 07:52:27,296] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06497453], dtype=float32), 0.04640831]
[2019-03-27 07:52:27,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.355052, 69.32982231333334, 1.0, 2.0, 0.4181323564755451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 607603.8813559047, 607603.8813559053, 174773.1987299216]
[2019-03-27 07:52:27,299] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:52:27,302] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0200739e-14 1.0000000e+00 5.7031880e-20 3.2844905e-10 3.2317704e-24], sampled 0.435521350948993
[2019-03-27 07:52:28,605] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.1184 2927236169.8858 1330.0000
[2019-03-27 07:52:28,709] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6340 2841925604.5174 1123.0000
[2019-03-27 07:52:28,793] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.1629 3006714619.4230 1739.0000
[2019-03-27 07:52:28,847] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7903.7295 3162006368.7823 1725.0000
[2019-03-27 07:52:29,038] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.6922 2779308056.7986 933.0000
[2019-03-27 07:52:30,056] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1650000, evaluation results [1650000.0, 7903.729503951962, 3162006368.7822976, 1725.0, 8258.118360766262, 2927236169.8858423, 1330.0, 8657.692244973712, 2779308056.798623, 933.0, 8002.162865303812, 3006714619.42304, 1739.0, 8496.633954496743, 2841925604.517357, 1123.0]
[2019-03-27 07:52:30,907] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2161724e-09 9.6646816e-01 2.4039855e-13 3.3531848e-02 2.3594055e-16], sum to 1.0000
[2019-03-27 07:52:30,917] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6039
[2019-03-27 07:52:30,927] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2164796.600767617 W.
[2019-03-27 07:52:30,931] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.46666666666667, 82.66666666666667, 1.0, 2.0, 0.7740840714461854, 1.0, 2.0, 0.7740840714461854, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2164796.600767617, 2164796.600767618, 407452.2824259059], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6168000.0000, 
sim time next is 6168600.0000, 
raw observation next is [28.55, 82.0, 1.0, 2.0, 0.9277757446785877, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.997487684635835, 6.9112, 168.9124429887931, 2193915.182187542, 2132699.915018463, 442196.4505393308], 
processed observation next is [1.0, 0.391304347826087, 0.552132701421801, 0.82, 1.0, 1.0, 0.9129828249139611, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008628768463583469, 0.0, 0.8294374235259693, 0.6094208839409838, 0.5924166430606842, 0.6599947022975087], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1543978], dtype=float32), 0.70490795]. 
=============================================
[2019-03-27 07:52:32,295] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.4645205e-15 1.0000000e+00 2.8928160e-20 1.3162599e-10 5.1812455e-24], sum to 1.0000
[2019-03-27 07:52:32,306] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4913
[2019-03-27 07:52:32,309] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 88.33333333333334, 1.0, 2.0, 0.7099680150497212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 992207.037051142, 992207.0370511414, 223012.8648631969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6160800.0000, 
sim time next is 6161400.0000, 
raw observation next is [27.6, 88.0, 1.0, 2.0, 0.7225304708573849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1009771.888934351, 1009771.888934351, 225784.9448147022], 
processed observation next is [1.0, 0.30434782608695654, 0.5071090047393366, 0.88, 1.0, 1.0, 0.6656993624787769, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28049219137065307, 0.28049219137065307, 0.3369924549473167], 
reward next is 0.6630, 
noisyNet noise sample is [array([0.21950616], dtype=float32), -0.45593584]. 
=============================================
[2019-03-27 07:52:32,510] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2103660e-14 1.0000000e+00 6.7304391e-20 8.8824413e-11 4.6847775e-24], sum to 1.0000
[2019-03-27 07:52:32,519] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8762
[2019-03-27 07:52:32,523] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 90.66666666666667, 1.0, 2.0, 0.7138826126327332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 997680.4043877538, 997680.4043877531, 223870.475905498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6154800.0000, 
sim time next is 6155400.0000, 
raw observation next is [26.83333333333334, 90.33333333333333, 1.0, 2.0, 0.7111876281845197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 993912.2899439064, 993912.2899439057, 223278.8190458347], 
processed observation next is [1.0, 0.21739130434782608, 0.4707740916271725, 0.9033333333333333, 1.0, 1.0, 0.6520332869693009, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27608674720664067, 0.27608674720664045, 0.3332519687251264], 
reward next is 0.6667, 
noisyNet noise sample is [array([-0.33302206], dtype=float32), -1.1651087]. 
=============================================
[2019-03-27 07:52:35,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.9489242e-11 6.8003213e-01 1.8014893e-13 3.1996781e-01 3.7151391e-17], sum to 1.0000
[2019-03-27 07:52:35,185] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0277
[2019-03-27 07:52:35,192] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2255989.860025298 W.
[2019-03-27 07:52:35,197] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.3, 74.5, 1.0, 2.0, 0.8066608183137334, 1.0, 2.0, 0.8066608183137334, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2255989.860025298, 2255989.860025298, 423123.611511596], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6193800.0000, 
sim time next is 6194400.0000, 
raw observation next is [29.2, 75.0, 1.0, 2.0, 0.8033374886758518, 1.0, 2.0, 0.8033374886758518, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2246687.143462393, 2246687.143462392, 421495.9126918509], 
processed observation next is [1.0, 0.6956521739130435, 0.5829383886255924, 0.75, 1.0, 1.0, 0.7630572152721106, 1.0, 1.0, 0.7630572152721106, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.624079762072887, 0.6240797620728866, 0.6290983771520163], 
reward next is 0.3709, 
noisyNet noise sample is [array([-1.4828085], dtype=float32), 0.92507005]. 
=============================================
[2019-03-27 07:52:39,060] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5752143e-18 1.0000000e+00 1.3126459e-23 1.2476622e-15 5.8585603e-27], sum to 1.0000
[2019-03-27 07:52:39,067] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8859
[2019-03-27 07:52:39,072] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.61666666666667, 62.83333333333333, 1.0, 2.0, 0.5102231700791207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712962.4017829729, 712962.4017829729, 185213.3395992655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6277800.0000, 
sim time next is 6278400.0000, 
raw observation next is [30.6, 63.0, 1.0, 2.0, 0.5102283636509687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712969.6614769972, 712969.6614769972, 185214.2027767767], 
processed observation next is [0.0, 0.6956521739130435, 0.6492890995260664, 0.63, 1.0, 1.0, 0.4099136911457454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19804712818805478, 0.19804712818805478, 0.2764391086220548], 
reward next is 0.7236, 
noisyNet noise sample is [array([2.2678688], dtype=float32), 1.7158539]. 
=============================================
[2019-03-27 07:52:41,819] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6114935e-15 1.0000000e+00 1.1711906e-21 2.0222636e-12 7.7582348e-24], sum to 1.0000
[2019-03-27 07:52:41,829] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8168
[2019-03-27 07:52:41,833] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 83.33333333333334, 1.0, 2.0, 0.9265981529351496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1316476.975238259, 1316476.97523826, 280564.3551477921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7006800.0000, 
sim time next is 7007400.0000, 
raw observation next is [25.65, 83.5, 1.0, 2.0, 0.8336761607015539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1185683.662403752, 1185683.662403752, 255271.9060994001], 
processed observation next is [1.0, 0.08695652173913043, 0.41469194312796204, 0.835, 1.0, 1.0, 0.7996098321705469, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32935657288993114, 0.32935657288993114, 0.3810028449244778], 
reward next is 0.6190, 
noisyNet noise sample is [array([0.76083773], dtype=float32), 1.243861]. 
=============================================
[2019-03-27 07:52:42,691] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7645913e-18 1.0000000e+00 1.1225852e-24 6.1397732e-17 2.2309732e-28], sum to 1.0000
[2019-03-27 07:52:42,698] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9427
[2019-03-27 07:52:42,703] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 81.0, 1.0, 2.0, 0.5230324939998808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730867.7198315278, 730867.7198315272, 187283.4548789769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6332400.0000, 
sim time next is 6333000.0000, 
raw observation next is [27.9, 80.33333333333333, 1.0, 2.0, 0.522397246669101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729979.7419275157, 729979.7419275151, 187179.6792625401], 
processed observation next is [0.0, 0.30434782608695654, 0.5213270142180094, 0.8033333333333332, 1.0, 1.0, 0.4245749959868687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20277215053542103, 0.20277215053542086, 0.2793726556157315], 
reward next is 0.7206, 
noisyNet noise sample is [array([-1.8683089], dtype=float32), 0.3069362]. 
=============================================
[2019-03-27 07:52:42,724] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.464325]
 [73.41974 ]
 [73.427475]
 [73.4305  ]
 [73.42998 ]], R is [[73.47534943]
 [73.4610672 ]
 [73.44674683]
 [73.43249512]
 [73.41849518]].
[2019-03-27 07:52:45,653] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0069514e-16 1.0000000e+00 1.3857564e-19 1.1061490e-09 6.5814157e-24], sum to 1.0000
[2019-03-27 07:52:45,663] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2770
[2019-03-27 07:52:45,666] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.41666666666667, 83.33333333333334, 1.0, 2.0, 0.8167440714093376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1141510.802222194, 1141510.802222194, 248061.3240637312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6418200.0000, 
sim time next is 6418800.0000, 
raw observation next is [27.5, 83.0, 1.0, 2.0, 0.7978300314144624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1115061.981698445, 1115061.981698445, 243377.9230600449], 
processed observation next is [1.0, 0.30434782608695654, 0.5023696682464456, 0.83, 1.0, 1.0, 0.7564217245957379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30973943936067916, 0.30973943936067916, 0.36325063143290287], 
reward next is 0.6367, 
noisyNet noise sample is [array([0.04338055], dtype=float32), -1.6010212]. 
=============================================
[2019-03-27 07:53:07,435] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5410760e-16 1.0000000e+00 2.8139776e-23 3.3249120e-14 2.5623084e-26], sum to 1.0000
[2019-03-27 07:53:07,447] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4605
[2019-03-27 07:53:07,451] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 94.83333333333334, 1.0, 2.0, 0.3180759226206301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501596.6726288902, 501596.6726288902, 167116.5945697944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7427400.0000, 
sim time next is 7428000.0000, 
raw observation next is [21.03333333333333, 94.66666666666667, 1.0, 2.0, 0.3187621316488384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502701.3248446704, 502701.3248446711, 167200.2811768486], 
processed observation next is [1.0, 1.0, 0.19589257503949445, 0.9466666666666668, 1.0, 1.0, 0.1792314839142631, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13963925690129733, 0.13963925690129753, 0.24955265847290836], 
reward next is 0.7504, 
noisyNet noise sample is [array([0.05564415], dtype=float32), 0.8244121]. 
=============================================
[2019-03-27 07:53:07,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.45832 ]
 [75.400505]
 [75.3206  ]
 [75.28708 ]
 [75.246765]], R is [[75.48397064]
 [75.47970581]
 [75.47562408]
 [75.4717865 ]
 [75.46813965]].
[2019-03-27 07:53:12,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.60110896e-16 1.00000000e+00 3.55516638e-23 4.50783470e-16
 1.18952155e-27], sum to 1.0000
[2019-03-27 07:53:12,862] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1656
[2019-03-27 07:53:12,867] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 90.5, 1.0, 2.0, 0.3755939519671112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566659.9541613702, 566659.9541613702, 171722.8529146654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7533000.0000, 
sim time next is 7533600.0000, 
raw observation next is [23.06666666666667, 90.33333333333334, 1.0, 2.0, 0.3731491645966507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563972.4160132934, 563972.4160132934, 171520.1497598521], 
processed observation next is [0.0, 0.17391304347826086, 0.29225908372827825, 0.9033333333333334, 1.0, 1.0, 0.24475802963451893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15665900444813707, 0.15665900444813707, 0.2560002235221673], 
reward next is 0.7440, 
noisyNet noise sample is [array([0.7044014], dtype=float32), 2.0743268]. 
=============================================
[2019-03-27 07:53:13,662] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0564592e-17 1.0000000e+00 4.5782473e-23 3.3611788e-15 1.4694908e-27], sum to 1.0000
[2019-03-27 07:53:13,672] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0026
[2019-03-27 07:53:13,675] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 80.33333333333334, 1.0, 2.0, 0.4170237444848601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612460.5297119211, 612460.5297119211, 175425.7835641209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6910800.0000, 
sim time next is 6911400.0000, 
raw observation next is [25.25, 80.66666666666666, 1.0, 2.0, 0.4172341074361636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612791.8599090766, 612791.8599090766, 175457.8818381976], 
processed observation next is [0.0, 1.0, 0.39573459715639814, 0.8066666666666665, 1.0, 1.0, 0.2978724185977875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1702199610858546, 0.1702199610858546, 0.2618774355793994], 
reward next is 0.7381, 
noisyNet noise sample is [array([-0.2171115], dtype=float32), 0.36438698]. 
=============================================
[2019-03-27 07:53:16,968] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.5459111e-18 1.0000000e+00 3.7411600e-25 4.2914759e-15 5.4578632e-27], sum to 1.0000
[2019-03-27 07:53:16,975] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1542
[2019-03-27 07:53:16,978] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.35, 58.5, 1.0, 2.0, 0.3891835891993862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 584786.8259990885, 584786.8259990878, 173265.0750511373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6985800.0000, 
sim time next is 6986400.0000, 
raw observation next is [28.23333333333333, 59.66666666666666, 1.0, 2.0, 0.3922925721318365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 587481.8435461828, 587481.8435461835, 173451.3244544136], 
processed observation next is [0.0, 0.8695652173913043, 0.537124802527646, 0.5966666666666666, 1.0, 1.0, 0.2678223760624536, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1631894009850508, 0.163189400985051, 0.25888257381255764], 
reward next is 0.7411, 
noisyNet noise sample is [array([0.60209966], dtype=float32), 0.45680356]. 
=============================================
[2019-03-27 07:53:17,318] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3943300e-16 1.0000000e+00 1.0598109e-22 3.8371683e-14 3.9870935e-26], sum to 1.0000
[2019-03-27 07:53:17,330] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8277
[2019-03-27 07:53:17,333] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.46666666666667, 57.33333333333334, 1.0, 2.0, 0.3851901583559446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 580746.5785393581, 580746.5785393575, 172959.3371488834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6985200.0000, 
sim time next is 6985800.0000, 
raw observation next is [28.35, 58.5, 1.0, 2.0, 0.3891835891993862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 584786.8259990885, 584786.8259990878, 173265.0750511373], 
processed observation next is [0.0, 0.8695652173913043, 0.5426540284360191, 0.585, 1.0, 1.0, 0.2640766134932364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1624407849997468, 0.1624407849997466, 0.25860458962856314], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.0169469], dtype=float32), 0.6892505]. 
=============================================
[2019-03-27 07:53:18,542] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8275617e-11 9.9735898e-01 1.8957846e-14 2.6410425e-03 7.2480802e-19], sum to 1.0000
[2019-03-27 07:53:18,549] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3662
[2019-03-27 07:53:18,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1773653.632505264 W.
[2019-03-27 07:53:18,559] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.5, 48.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.276255140465867, 6.9112, 168.9111729727326, 1773653.632505264, 1514673.68620684, 320944.0944097598], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7044000.0000, 
sim time next is 7044600.0000, 
raw observation next is [31.6, 47.66666666666667, 1.0, 2.0, 0.6013576617254158, 1.0, 1.0, 0.6013576617254158, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1709126.947664803, 1709126.947664802, 338108.3709452006], 
processed observation next is [1.0, 0.5217391304347826, 0.6966824644549764, 0.47666666666666674, 1.0, 1.0, 0.5197080261751997, 1.0, 0.5, 0.5197080261751997, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.47475748546244523, 0.474757485462445, 0.5046393596197024], 
reward next is 0.4954, 
noisyNet noise sample is [array([0.77877086], dtype=float32), -0.5155013]. 
=============================================
[2019-03-27 07:53:20,242] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 07:53:20,247] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:53:20,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:53:20,249] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:53:20,250] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:53:20,251] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:53:20,252] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:53:20,252] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:53:20,249] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:53:20,252] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:53:20,255] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:53:20,283] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run68
[2019-03-27 07:53:20,305] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run68
[2019-03-27 07:53:20,324] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run68
[2019-03-27 07:53:20,325] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run68
[2019-03-27 07:53:20,357] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run68
[2019-03-27 07:53:33,356] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06608352], dtype=float32), 0.04690314]
[2019-03-27 07:53:33,357] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.56666666666667, 95.0, 1.0, 2.0, 0.4372197476603219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 635248.2646773752, 635248.2646773745, 177455.7051511552]
[2019-03-27 07:53:33,358] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:53:33,363] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.2930529e-17 1.0000000e+00 8.1450621e-23 8.7704375e-15 5.6514316e-26], sampled 0.5005372780704753
[2019-03-27 07:53:34,708] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06608352], dtype=float32), 0.04690314]
[2019-03-27 07:53:34,711] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.214831675, 68.411557965, 1.0, 2.0, 0.2792702523519455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 452190.332693346, 452190.3326933466, 163736.934640078]
[2019-03-27 07:53:34,712] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:53:34,715] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.1620082e-17 1.0000000e+00 3.7827515e-23 1.3668588e-15 3.6985879e-26], sampled 0.0019552289498905306
[2019-03-27 07:54:42,087] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06608352], dtype=float32), 0.04690314]
[2019-03-27 07:54:42,089] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.16973088333334, 48.69293171, 1.0, 2.0, 0.854905498292698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1259991.68975562, 1259991.68975562, 267042.033646222]
[2019-03-27 07:54:42,090] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:54:42,094] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1259273e-14 1.0000000e+00 1.2419897e-19 1.7083392e-09 1.2291436e-23], sampled 0.8937375495729876
[2019-03-27 07:54:56,557] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06608352], dtype=float32), 0.04690314]
[2019-03-27 07:54:56,558] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.39290954666667, 92.29501877666668, 1.0, 2.0, 0.9083005808379727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1269549.965556043, 1269549.965556044, 272228.3536575868]
[2019-03-27 07:54:56,559] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:54:56,561] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.0844744e-15 1.0000000e+00 1.6877058e-20 3.3534755e-11 7.9171098e-24], sampled 0.16332706180037326
[2019-03-27 07:55:16,230] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8021.8498 3004177154.9129 1685.0000
[2019-03-27 07:55:16,442] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8505.4416 2840817956.5973 1099.0000
[2019-03-27 07:55:16,477] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7926.3920 3158162002.4247 1614.0000
[2019-03-27 07:55:16,503] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8271.5273 2926033480.9706 1299.0000
[2019-03-27 07:55:16,735] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.2664 2778811874.7634 925.0000
[2019-03-27 07:55:17,751] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1675000, evaluation results [1675000.0, 7926.392028945532, 3158162002.4246583, 1614.0, 8271.527345919974, 2926033480.9706492, 1299.0, 8664.266397110534, 2778811874.763395, 925.0, 8021.84976577901, 3004177154.912908, 1685.0, 8505.441597154997, 2840817956.5973363, 1099.0]
[2019-03-27 07:55:19,199] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0008849e-16 1.0000000e+00 1.2233164e-21 9.3678832e-12 4.7325513e-26], sum to 1.0000
[2019-03-27 07:55:19,206] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6851
[2019-03-27 07:55:19,211] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.53333333333333, 87.33333333333334, 1.0, 2.0, 0.4820045854936138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673518.5485279667, 673518.548527966, 180823.9668520683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7078800.0000, 
sim time next is 7079400.0000, 
raw observation next is [25.5, 87.5, 1.0, 2.0, 0.4808562141444735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671913.3894996588, 671913.3894996588, 180650.4006258951], 
processed observation next is [1.0, 0.9565217391304348, 0.40758293838862564, 0.875, 1.0, 1.0, 0.3745255592102091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18664260819434966, 0.18664260819434966, 0.26962746362073897], 
reward next is 0.7304, 
noisyNet noise sample is [array([-1.0496097], dtype=float32), 0.0121548725]. 
=============================================
[2019-03-27 07:55:22,482] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.5487989e-16 1.0000000e+00 2.1599572e-22 2.7528408e-13 2.2104506e-25], sum to 1.0000
[2019-03-27 07:55:22,488] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0203
[2019-03-27 07:55:22,492] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.15, 86.0, 1.0, 2.0, 0.3226473552832425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508417.7875475662, 508417.7875475668, 167624.9678018206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7284600.0000, 
sim time next is 7285200.0000, 
raw observation next is [22.2, 85.66666666666667, 1.0, 2.0, 0.3202498099343761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504565.0852633117, 504565.0852633124, 167330.6141950369], 
processed observation next is [1.0, 0.30434782608695654, 0.2511848341232228, 0.8566666666666667, 1.0, 1.0, 0.18102386739081455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14015696812869768, 0.14015696812869788, 0.24974718536572674], 
reward next is 0.7503, 
noisyNet noise sample is [array([1.3880633], dtype=float32), -0.4205926]. 
=============================================
[2019-03-27 07:55:22,992] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0501006e-17 1.0000000e+00 4.2447007e-22 7.9233668e-12 7.6649164e-26], sum to 1.0000
[2019-03-27 07:55:23,000] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0744
[2019-03-27 07:55:23,005] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 85.83333333333334, 1.0, 2.0, 0.4704582394378091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658808.0729775097, 658808.0729775097, 179281.5992119209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7170600.0000, 
sim time next is 7171200.0000, 
raw observation next is [25.7, 86.0, 1.0, 2.0, 0.4720728632724427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660374.3298207412, 660374.3298207412, 179431.4498755454], 
processed observation next is [1.0, 0.0, 0.4170616113744076, 0.86, 1.0, 1.0, 0.3639432087619791, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18343731383909478, 0.18343731383909478, 0.26780813414260507], 
reward next is 0.7322, 
noisyNet noise sample is [array([-0.9333456], dtype=float32), 0.07984514]. 
=============================================
[2019-03-27 07:55:30,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:55:30,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:55:30,837] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-03-27 07:55:32,002] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0927971e-17 1.0000000e+00 4.2665495e-23 3.3525733e-15 1.8563211e-27], sum to 1.0000
[2019-03-27 07:55:32,010] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5446
[2019-03-27 07:55:32,015] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 88.33333333333334, 1.0, 2.0, 0.4882243640883054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 682212.4106393552, 682212.4106393552, 181771.4638544049], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7593000.0000, 
sim time next is 7593600.0000, 
raw observation next is [25.66666666666667, 88.66666666666667, 1.0, 2.0, 0.4859180628494311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678988.7080143351, 678988.7080143344, 181419.0336984263], 
processed observation next is [0.0, 0.9130434782608695, 0.4154818325434442, 0.8866666666666667, 1.0, 1.0, 0.38062417210774835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18860797444842642, 0.18860797444842622, 0.2707746771618303], 
reward next is 0.7292, 
noisyNet noise sample is [array([-1.0382442], dtype=float32), -2.1926606]. 
=============================================
[2019-03-27 07:55:33,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:55:33,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:55:33,412] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run9
[2019-03-27 07:55:40,481] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2157406e-17 1.0000000e+00 1.5048928e-23 7.1816629e-15 7.8685023e-27], sum to 1.0000
[2019-03-27 07:55:40,493] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2292
[2019-03-27 07:55:40,497] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 86.66666666666666, 1.0, 2.0, 0.4034057216550294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595765.460110381, 595765.4601103804, 173964.466860212], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7505400.0000, 
sim time next is 7506000.0000, 
raw observation next is [24.2, 87.0, 1.0, 2.0, 0.4043482749404768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597252.8594319754, 597252.8594319754, 174104.4724803634], 
processed observation next is [0.0, 0.9130434782608695, 0.3459715639810427, 0.87, 1.0, 1.0, 0.28234731920539374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16590357206443762, 0.16590357206443762, 0.25985742161248265], 
reward next is 0.7401, 
noisyNet noise sample is [array([-0.2172289], dtype=float32), -2.005228]. 
=============================================
[2019-03-27 07:55:40,510] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.60266 ]
 [79.59144 ]
 [79.56939 ]
 [79.54765 ]
 [79.523125]], R is [[79.60495758]
 [79.54925537]
 [79.4942627 ]
 [79.43980408]
 [79.38595581]].
[2019-03-27 07:55:45,047] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9118645e-17 1.0000000e+00 1.7990472e-23 6.2636706e-15 3.9481617e-28], sum to 1.0000
[2019-03-27 07:55:45,055] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2233
[2019-03-27 07:55:45,060] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 74.5, 1.0, 2.0, 0.4594131902218633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645908.7461473537, 645908.7461473542, 177994.9319047655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7583400.0000, 
sim time next is 7584000.0000, 
raw observation next is [27.23333333333333, 75.66666666666667, 1.0, 2.0, 0.4632503953924129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649275.9558403647, 649275.9558403647, 178293.890136891], 
processed observation next is [0.0, 0.782608695652174, 0.4897314375987361, 0.7566666666666667, 1.0, 1.0, 0.35331372938844935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18035443217787908, 0.18035443217787908, 0.26611028378640444], 
reward next is 0.7339, 
noisyNet noise sample is [array([2.347838], dtype=float32), -0.14983243]. 
=============================================
[2019-03-27 07:55:45,082] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.22618 ]
 [77.20827 ]
 [77.186325]
 [77.1463  ]
 [77.08908 ]], R is [[77.20954895]
 [77.17179108]
 [77.13478088]
 [77.0983963 ]
 [77.06250763]].
[2019-03-27 07:55:51,660] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.3267655e-17 1.0000000e+00 3.7950845e-21 1.4611787e-12 1.0448407e-24], sum to 1.0000
[2019-03-27 07:55:51,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3266
[2019-03-27 07:55:51,676] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.26666666666667, 87.33333333333334, 1.0, 2.0, 0.2630778905028809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 428627.6173948369, 428627.6173948369, 162163.38724596], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 354000.0000, 
sim time next is 354600.0000, 
raw observation next is [20.25, 87.5, 1.0, 2.0, 0.2560247998700574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417115.608989569, 417115.608989569, 161447.8500723189], 
processed observation next is [1.0, 0.08695652173913043, 0.1587677725118484, 0.875, 1.0, 1.0, 0.10364433719284022, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11586544694154695, 0.11586544694154695, 0.24096694040644612], 
reward next is 0.7590, 
noisyNet noise sample is [array([0.9007174], dtype=float32), -0.46810386]. 
=============================================
[2019-03-27 07:55:53,733] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:55:53,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:55:53,804] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run9
[2019-03-27 07:55:58,289] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.3930550e-15 1.0000000e+00 1.4716558e-20 7.9370033e-10 2.2694941e-23], sum to 1.0000
[2019-03-27 07:55:58,300] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4078
[2019-03-27 07:55:58,310] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 87.66666666666667, 1.0, 2.0, 0.655604936112846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 916199.8153989692, 916199.8153989692, 211551.6433033448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7788000.0000, 
sim time next is 7788600.0000, 
raw observation next is [25.9, 88.0, 1.0, 2.0, 0.6431744756247157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 898821.0494038481, 898821.0494038475, 209054.8110721068], 
processed observation next is [1.0, 0.13043478260869565, 0.42654028436018954, 0.88, 1.0, 1.0, 0.5700897296683322, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24967251372329113, 0.249672513723291, 0.3120221060777713], 
reward next is 0.6880, 
noisyNet noise sample is [array([-0.37878942], dtype=float32), -2.0426867]. 
=============================================
[2019-03-27 07:55:58,312] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.1298891e-12 9.9997306e-01 1.4592695e-17 2.6961381e-05 1.3299742e-23], sum to 1.0000
[2019-03-27 07:55:58,316] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5547
[2019-03-27 07:55:58,324] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.13333333333333, 66.0, 1.0, 2.0, 0.4748563033345679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 663526.933567307, 663526.9335673077, 179752.5839439685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7926000.0000, 
sim time next is 7926600.0000, 
raw observation next is [29.96666666666667, 67.0, 1.0, 2.0, 0.4842593882564767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676670.2512573008, 676670.2512573001, 181168.4193691385], 
processed observation next is [1.0, 0.7391304347826086, 0.6192733017377569, 0.67, 1.0, 1.0, 0.37862576898370687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18796395868258356, 0.18796395868258337, 0.2704006259240873], 
reward next is 0.7296, 
noisyNet noise sample is [array([0.6505462], dtype=float32), 0.5892202]. 
=============================================
[2019-03-27 07:56:00,541] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.7794377e-16 1.0000000e+00 1.9593848e-22 1.7526254e-12 1.1571025e-24], sum to 1.0000
[2019-03-27 07:56:00,549] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7693
[2019-03-27 07:56:00,558] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.75, 87.0, 1.0, 2.0, 0.2275384810893793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 377668.3390990801, 377668.3390990808, 158545.4468832665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 520200.0000, 
sim time next is 520800.0000, 
raw observation next is [18.73333333333333, 87.0, 1.0, 2.0, 0.2269561658179425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 376755.3101552516, 376755.3101552516, 158484.9593247064], 
processed observation next is [1.0, 0.0, 0.08688783570300151, 0.87, 1.0, 1.0, 0.06862188652764156, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10465425282090322, 0.10465425282090322, 0.23654471541000957], 
reward next is 0.7635, 
noisyNet noise sample is [array([-1.9106392], dtype=float32), -0.6664106]. 
=============================================
[2019-03-27 07:56:00,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:00,727] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:00,801] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run9
[2019-03-27 07:56:04,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:04,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:04,310] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run9
[2019-03-27 07:56:06,196] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:06,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:06,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run9
[2019-03-27 07:56:06,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:06,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:06,679] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:06,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:06,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run9
[2019-03-27 07:56:06,748] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run9
[2019-03-27 07:56:07,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:07,427] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:07,471] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run9
[2019-03-27 07:56:07,544] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:07,545] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:07,586] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run9
[2019-03-27 07:56:07,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:07,630] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:07,645] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:07,645] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:07,674] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run9
[2019-03-27 07:56:07,696] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:07,698] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:07,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run9
[2019-03-27 07:56:07,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:07,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:07,735] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run9
[2019-03-27 07:56:07,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run9
[2019-03-27 07:56:07,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:07,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:07,946] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run9
[2019-03-27 07:56:08,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 07:56:08,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:08,118] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run9
[2019-03-27 07:56:10,379] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 07:56:10,381] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:56:10,383] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:10,384] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:56:10,384] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:56:10,385] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:56:10,386] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:10,387] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:56:10,389] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:10,390] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:10,387] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:56:10,411] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run69
[2019-03-27 07:56:10,432] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run69
[2019-03-27 07:56:10,450] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run69
[2019-03-27 07:56:10,451] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run69
[2019-03-27 07:56:10,499] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run69
[2019-03-27 07:56:19,441] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:56:19,446] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.7, 61.5, 1.0, 2.0, 0.5623010054191893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 919420.9436984098, 919420.9436984098, 207693.937342504]
[2019-03-27 07:56:19,446] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:56:19,449] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6699251e-16 1.0000000e+00 2.3202946e-22 1.8186944e-13 6.8794680e-26], sampled 0.5312445087131367
[2019-03-27 07:56:30,478] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:56:30,480] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.1, 95.0, 1.0, 2.0, 0.440180960308833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651081.9185339112, 651081.9185339105, 179298.5097571632]
[2019-03-27 07:56:30,481] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:56:30,483] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4447032e-17 1.0000000e+00 2.0187301e-23 4.4887897e-15 7.6509200e-27], sampled 0.920362748492533
[2019-03-27 07:56:41,174] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:56:41,176] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.8, 80.5, 1.0, 2.0, 0.4877029821578848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 681483.6323500415, 681483.6323500408, 181691.4877249273]
[2019-03-27 07:56:41,180] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:56:41,183] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3469759e-17 1.0000000e+00 9.1578042e-24 1.9406833e-15 2.8322150e-27], sampled 0.0014550997139821664
[2019-03-27 07:57:13,755] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:57:13,756] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 79.0, 1.0, 2.0, 0.621937577563629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869130.8457266962, 869130.8457266962, 204904.5806662329]
[2019-03-27 07:57:13,757] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:57:13,760] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6184904e-17 1.0000000e+00 7.4490210e-24 5.9104190e-14 9.7182261e-28], sampled 0.33969300984950856
[2019-03-27 07:57:28,870] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:57:28,872] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.29344501833333, 53.63972345333333, 1.0, 2.0, 0.7351495548553303, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977478284185, 6.9112, 168.912316034216, 1924316.604870286, 1857078.454737856, 392453.3763671441]
[2019-03-27 07:57:28,873] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:57:28,876] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.6042468e-11 9.9950659e-01 1.4679905e-15 4.9345824e-04 4.9279582e-20], sampled 0.6946356647148062
[2019-03-27 07:57:28,879] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1924316.604870286 W.
[2019-03-27 07:57:32,378] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:57:32,379] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.2, 60.0, 1.0, 2.0, 0.4871480491061601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680707.9574557989, 680707.9574557989, 181610.6705641095]
[2019-03-27 07:57:32,381] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:57:32,385] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.81555227e-12 9.99987006e-01 3.19945454e-17 1.30388735e-05
 5.73100102e-22], sampled 0.08102427725318972
[2019-03-27 07:57:34,108] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:57:34,109] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.41666666666667, 93.83333333333334, 1.0, 2.0, 0.6243008913524973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 872434.8308916374, 872434.8308916367, 205360.0866578236]
[2019-03-27 07:57:34,110] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 07:57:34,111] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8302579e-17 1.0000000e+00 1.2084591e-23 3.7143899e-14 1.4794463e-27], sampled 0.3391733444353737
[2019-03-27 07:57:48,879] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:57:48,881] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.26666666666667, 67.0, 1.0, 2.0, 0.4522053941760105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643380.8895357632, 643380.8895357632, 177930.5097590841]
[2019-03-27 07:57:48,882] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:57:48,884] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6633257e-16 1.0000000e+00 4.5008102e-22 1.3773849e-11 1.0137330e-26], sampled 0.2834741884144705
[2019-03-27 07:57:51,126] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:57:51,126] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.04007889333333, 69.69617338333333, 1.0, 2.0, 0.4750976544194109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670268.253856771, 670268.253856771, 180610.9474752374]
[2019-03-27 07:57:51,128] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:57:51,133] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4256455e-17 1.0000000e+00 9.4332350e-24 5.3186644e-16 5.7835068e-27], sampled 0.10851584008329884
[2019-03-27 07:57:53,889] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06837174], dtype=float32), 0.048180148]
[2019-03-27 07:57:53,890] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.23333333333333, 85.0, 1.0, 2.0, 0.487506621927842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681209.1636801178, 681209.1636801184, 181661.6944970096]
[2019-03-27 07:57:53,892] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 07:57:53,893] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.1351022e-17 1.0000000e+00 6.7381608e-23 4.8449233e-13 3.4285235e-27], sampled 0.885859080645111
[2019-03-27 07:58:06,342] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.6037 3007273542.8365 1756.0000
[2019-03-27 07:58:06,615] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.5919 2842085562.3041 1125.0000
[2019-03-27 07:58:06,678] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7894.8452 3162235892.4988 1734.0000
[2019-03-27 07:58:06,682] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.1663 2927037208.1675 1331.0000
[2019-03-27 07:58:06,760] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9047 2779147992.5043 931.0000
[2019-03-27 07:58:07,775] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1700000, evaluation results [1700000.0, 7894.845222252146, 3162235892.498819, 1734.0, 8260.166294218427, 2927037208.1674843, 1331.0, 8659.904715726558, 2779147992.504339, 931.0, 7998.603657065941, 3007273542.8364553, 1756.0, 8500.591920540412, 2842085562.304097, 1125.0]
[2019-03-27 07:58:09,910] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8351516e-17 1.0000000e+00 1.8814633e-22 2.5842661e-12 2.0264502e-26], sum to 1.0000
[2019-03-27 07:58:09,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6126
[2019-03-27 07:58:09,926] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333334, 83.66666666666667, 1.0, 2.0, 0.3627457052283415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 557055.4001379946, 557055.400137994, 171193.2174820227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 73200.0000, 
sim time next is 73800.0000, 
raw observation next is [23.35, 84.0, 1.0, 2.0, 0.3609270321389292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 554858.1105003783, 554858.1105003783, 171023.6226664433], 
processed observation next is [1.0, 0.8695652173913043, 0.3056872037914693, 0.84, 1.0, 1.0, 0.23003256884208334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15412725291677176, 0.15412725291677176, 0.2552591383081243], 
reward next is 0.7447, 
noisyNet noise sample is [array([-1.1457201], dtype=float32), -2.2789965]. 
=============================================
[2019-03-27 07:58:10,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6807060e-15 1.0000000e+00 6.3925536e-21 2.5382160e-11 3.0631055e-25], sum to 1.0000
[2019-03-27 07:58:10,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8952
[2019-03-27 07:58:10,825] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333333, 73.0, 1.0, 2.0, 0.5187384732909831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836339.7480974811, 836339.7480974811, 198703.6201582337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 394800.0000, 
sim time next is 395400.0000, 
raw observation next is [22.86666666666666, 73.0, 1.0, 2.0, 0.5312156999476261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855929.6869163697, 855929.6869163697, 201017.8298843545], 
processed observation next is [1.0, 0.5652173913043478, 0.28278041074249577, 0.73, 1.0, 1.0, 0.4351996384911157, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23775824636565826, 0.23775824636565826, 0.3000266117676933], 
reward next is 0.7000, 
noisyNet noise sample is [array([-0.7201104], dtype=float32), -0.31426257]. 
=============================================
[2019-03-27 07:58:10,889] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.4154879e-14 1.0000000e+00 3.8095892e-21 6.2088523e-10 2.2351857e-24], sum to 1.0000
[2019-03-27 07:58:10,899] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0463
[2019-03-27 07:58:10,904] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 75.0, 1.0, 2.0, 0.3830790195477097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 572194.3413916377, 572194.3413916371, 172026.4422149686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 64800.0000, 
sim time next is 65400.0000, 
raw observation next is [25.43333333333334, 75.66666666666667, 1.0, 2.0, 0.3871612906039855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579468.8303373059, 579468.8303373066, 172715.4756395924], 
processed observation next is [1.0, 0.782608695652174, 0.40442338072669864, 0.7566666666666667, 1.0, 1.0, 0.2616401091614283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16096356398258496, 0.16096356398258516, 0.2577842919993916], 
reward next is 0.7422, 
noisyNet noise sample is [array([-2.1119015], dtype=float32), 0.6352869]. 
=============================================
[2019-03-27 07:58:13,911] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.2284877e-16 1.0000000e+00 3.3690725e-23 4.3280457e-12 1.5973535e-25], sum to 1.0000
[2019-03-27 07:58:13,920] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2532
[2019-03-27 07:58:13,924] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.91666666666666, 91.83333333333333, 1.0, 2.0, 0.6901428479089019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1041999.03035852, 1041999.03035852, 228648.5417650107], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 118200.0000, 
sim time next is 118800.0000, 
raw observation next is [22.9, 92.0, 1.0, 2.0, 0.734012973731504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1108080.704503797, 1108080.704503796, 239083.8775810624], 
processed observation next is [1.0, 0.391304347826087, 0.2843601895734597, 0.92, 1.0, 1.0, 0.6795337032909686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30780019569549916, 0.3078001956954989, 0.3568416083299439], 
reward next is 0.6432, 
noisyNet noise sample is [array([0.53879654], dtype=float32), 0.32569718]. 
=============================================
[2019-03-27 07:58:14,261] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7678996e-16 1.0000000e+00 1.0440374e-21 2.1773308e-10 1.9832575e-25], sum to 1.0000
[2019-03-27 07:58:14,272] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3030
[2019-03-27 07:58:14,277] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 96.0, 1.0, 2.0, 0.9100219829494143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1355436.174520329, 1355436.174520329, 284768.7342746886], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 129600.0000, 
sim time next is 130200.0000, 
raw observation next is [22.8, 96.0, 1.0, 2.0, 0.8055072414105805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1199470.901769697, 1199470.901769696, 255338.0040801441], 
processed observation next is [1.0, 0.5217391304347826, 0.2796208530805688, 0.96, 1.0, 1.0, 0.7656713751934705, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3331863616026936, 0.3331863616026933, 0.38110149862708076], 
reward next is 0.6189, 
noisyNet noise sample is [array([1.2979778], dtype=float32), 0.03913319]. 
=============================================
[2019-03-27 07:58:16,921] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9672369e-18 1.0000000e+00 4.9371915e-25 3.4614868e-17 3.8333536e-27], sum to 1.0000
[2019-03-27 07:58:16,929] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5634
[2019-03-27 07:58:16,935] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.86666666666667, 96.0, 1.0, 2.0, 0.2856944411826112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460424.7653101006, 460424.7653101006, 164302.1342669122], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 188400.0000, 
sim time next is 189000.0000, 
raw observation next is [19.85, 96.0, 1.0, 2.0, 0.2850920325991209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 459598.5327173639, 459598.5327173633, 164245.8599323117], 
processed observation next is [0.0, 0.17391304347826086, 0.1398104265402845, 0.96, 1.0, 1.0, 0.13866509951701314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12766625908815665, 0.1276662590881565, 0.24514307452583836], 
reward next is 0.7549, 
noisyNet noise sample is [array([-0.48653972], dtype=float32), -0.5748197]. 
=============================================
[2019-03-27 07:58:16,942] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.38509]
 [77.36755]
 [77.26547]
 [77.21476]
 [77.2187 ]], R is [[77.45972443]
 [77.43990326]
 [77.42020416]
 [77.40068054]
 [77.38140106]].
[2019-03-27 07:58:24,386] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.1181229e-20 1.0000000e+00 3.5000165e-26 6.8449419e-18 5.6640892e-30], sum to 1.0000
[2019-03-27 07:58:24,392] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9813
[2019-03-27 07:58:24,398] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 86.0, 1.0, 2.0, 0.2764787152356477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 447421.9574306399, 447421.9574306393, 163423.2076608312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 339600.0000, 
sim time next is 340200.0000, 
raw observation next is [20.8, 86.0, 1.0, 2.0, 0.2733448314107534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 442603.5072770095, 442603.5072770101, 163104.8118955987], 
processed observation next is [0.0, 0.9565217391304348, 0.1848341232227489, 0.86, 1.0, 1.0, 0.12451184507319685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1229454186880582, 0.12294541868805837, 0.2434400177546249], 
reward next is 0.7566, 
noisyNet noise sample is [array([0.48826396], dtype=float32), 1.0331124]. 
=============================================
[2019-03-27 07:58:30,840] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0546024e-18 1.0000000e+00 3.5696918e-24 8.5561339e-15 8.7903716e-29], sum to 1.0000
[2019-03-27 07:58:30,845] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7060
[2019-03-27 07:58:30,851] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 89.0, 1.0, 2.0, 0.2948111481204048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472284.1767909807, 472284.1767909807, 165109.5834301597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1123200.0000, 
sim time next is 1123800.0000, 
raw observation next is [20.96666666666667, 89.16666666666667, 1.0, 2.0, 0.2938739999370444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470953.5056266566, 470953.5056266561, 165017.8551203497], 
processed observation next is [1.0, 0.0, 0.1927330173775673, 0.8916666666666667, 1.0, 1.0, 0.14924578305667996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13082041822962684, 0.1308204182296267, 0.24629530614977568], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.80096346], dtype=float32), -0.8255126]. 
=============================================
[2019-03-27 07:58:38,351] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9045701e-16 1.0000000e+00 3.4491932e-22 6.8692469e-13 8.7670816e-25], sum to 1.0000
[2019-03-27 07:58:38,363] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0555
[2019-03-27 07:58:38,369] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 55.0, 1.0, 2.0, 0.633009382082244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1039117.682835463, 1039117.682835462, 222885.9367263796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 560400.0000, 
sim time next is 561000.0000, 
raw observation next is [24.78333333333333, 54.5, 1.0, 2.0, 0.6412729187898063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051154.283686693, 1051154.283686693, 224749.5555619333], 
processed observation next is [1.0, 0.4782608695652174, 0.37361769352290675, 0.545, 1.0, 1.0, 0.567798697337116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2919873010240814, 0.2919873010240814, 0.33544709785363175], 
reward next is 0.6646, 
noisyNet noise sample is [array([-1.2042801], dtype=float32), 1.2665468]. 
=============================================
[2019-03-27 07:58:38,381] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.23591 ]
 [71.51146 ]
 [71.77271 ]
 [72.117195]
 [72.58849 ]], R is [[70.95230103]
 [70.91011047]
 [70.87447357]
 [70.84003448]
 [70.8063736 ]].
[2019-03-27 07:58:40,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3955991e-17 1.0000000e+00 1.1713469e-21 7.2951405e-13 3.1517532e-26], sum to 1.0000
[2019-03-27 07:58:40,104] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0040
[2019-03-27 07:58:40,108] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 65.5, 1.0, 2.0, 0.2548724681904998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 416457.5175737196, 416457.5175737189, 161370.4445274106], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 581400.0000, 
sim time next is 582000.0000, 
raw observation next is [22.96666666666667, 66.0, 1.0, 2.0, 0.2559756984143901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 418568.5634198869, 418568.5634198862, 161487.9069930623], 
processed observation next is [1.0, 0.7391304347826086, 0.2875197472353872, 0.66, 1.0, 1.0, 0.10358517881251819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11626904539441303, 0.11626904539441284, 0.24102672685531687], 
reward next is 0.7590, 
noisyNet noise sample is [array([0.6233557], dtype=float32), -0.043971386]. 
=============================================
[2019-03-27 07:58:40,122] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[73.472046]
 [73.07934 ]
 [72.433014]
 [71.59579 ]
 [71.6009  ]], R is [[73.78936005]
 [73.81061554]
 [73.83119202]
 [73.83924103]
 [73.77381897]].
[2019-03-27 07:58:40,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4407502e-18 1.0000000e+00 8.7645488e-23 3.6322217e-15 9.1582445e-27], sum to 1.0000
[2019-03-27 07:58:40,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3505
[2019-03-27 07:58:40,698] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 92.5, 1.0, 2.0, 0.2604571569572372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 425989.4158092547, 425989.4158092541, 161942.9033441317], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 790200.0000, 
sim time next is 790800.0000, 
raw observation next is [19.4, 92.66666666666667, 1.0, 2.0, 0.2605737586324761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 426060.2891390176, 426060.2891390182, 161952.2698383595], 
processed observation next is [0.0, 0.13043478260869565, 0.11848341232227487, 0.9266666666666667, 1.0, 1.0, 0.10912501040057362, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11835008031639377, 0.11835008031639395, 0.24171980572889476], 
reward next is 0.7583, 
noisyNet noise sample is [array([-0.14750953], dtype=float32), -0.9413698]. 
=============================================
[2019-03-27 07:58:41,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5602088e-18 1.0000000e+00 1.1157043e-22 5.2806369e-16 6.5928133e-27], sum to 1.0000
[2019-03-27 07:58:41,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8793
[2019-03-27 07:58:42,003] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.6, 89.0, 1.0, 2.0, 0.2198288230664026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367157.5882223534, 367157.5882223534, 157216.274623464], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 624600.0000, 
sim time next is 625200.0000, 
raw observation next is [17.83333333333333, 88.0, 1.0, 2.0, 0.212748809646652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 355133.0624680157, 355133.0624680163, 156705.7068903017], 
processed observation next is [1.0, 0.21739130434782608, 0.044233807266982464, 0.88, 1.0, 1.0, 0.05150458993572528, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09864807290778214, 0.0986480729077823, 0.2338891147616443], 
reward next is 0.7661, 
noisyNet noise sample is [array([-0.20062253], dtype=float32), 0.8693509]. 
=============================================
[2019-03-27 07:58:43,249] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.1964918e-17 1.0000000e+00 2.5674222e-21 2.6534271e-13 6.4573915e-25], sum to 1.0000
[2019-03-27 07:58:43,261] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1012
[2019-03-27 07:58:43,271] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 56.33333333333334, 1.0, 2.0, 0.3917393684684735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642896.085983045, 642896.085983045, 178270.7484363686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 649200.0000, 
sim time next is 649800.0000, 
raw observation next is [24.35, 56.0, 1.0, 2.0, 0.338113927322048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 555031.0528448228, 555031.0528448233, 170949.8978471664], 
processed observation next is [1.0, 0.5217391304347826, 0.35308056872037924, 0.56, 1.0, 1.0, 0.2025469003880096, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1541752924568952, 0.15417529245689537, 0.25514910126442747], 
reward next is 0.7449, 
noisyNet noise sample is [array([-0.7548015], dtype=float32), 1.1934215]. 
=============================================
[2019-03-27 07:58:54,650] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.5033842e-17 1.0000000e+00 2.5992979e-24 2.9231489e-13 1.7713409e-27], sum to 1.0000
[2019-03-27 07:58:54,660] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0851
[2019-03-27 07:58:54,663] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 97.5, 1.0, 2.0, 0.3724396340667769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 564686.0933807458, 564686.0933807465, 171638.2772160558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1035000.0000, 
sim time next is 1035600.0000, 
raw observation next is [22.13333333333333, 97.33333333333333, 1.0, 2.0, 0.372787209659153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564968.6092827584, 564968.6092827584, 171655.3101044312], 
processed observation next is [1.0, 1.0, 0.24802527646129527, 0.9733333333333333, 1.0, 1.0, 0.24432193934837712, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15693572480076623, 0.15693572480076623, 0.25620195537974805], 
reward next is 0.7438, 
noisyNet noise sample is [array([1.452577], dtype=float32), 3.3581831]. 
=============================================
[2019-03-27 07:58:57,114] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3950861e-19 1.0000000e+00 2.9483582e-24 5.7589040e-16 3.3131262e-28], sum to 1.0000
[2019-03-27 07:58:57,124] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0044
[2019-03-27 07:58:57,129] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.96666666666667, 83.66666666666667, 1.0, 2.0, 0.3323981950555167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 517377.7231322267, 517377.7231322261, 168159.1274346377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 933600.0000, 
sim time next is 934200.0000, 
raw observation next is [22.9, 84.5, 1.0, 2.0, 0.3337932480252307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518922.6041117552, 518922.6041117552, 168262.5010783901], 
processed observation next is [0.0, 0.8260869565217391, 0.2843601895734597, 0.845, 1.0, 1.0, 0.19734126268100083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1441451678088209, 0.1441451678088209, 0.25113806131103], 
reward next is 0.7489, 
noisyNet noise sample is [array([-1.8924593], dtype=float32), 2.0234485]. 
=============================================
[2019-03-27 07:58:57,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8477338e-16 1.0000000e+00 4.3951362e-24 7.5528928e-15 1.9769569e-26], sum to 1.0000
[2019-03-27 07:58:57,875] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2814
[2019-03-27 07:58:57,879] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 91.0, 1.0, 2.0, 0.343914896035247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535283.5384585808, 535283.5384585802, 169584.2315427783], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1218600.0000, 
sim time next is 1219200.0000, 
raw observation next is [21.96666666666667, 91.33333333333333, 1.0, 2.0, 0.3487223212053867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542666.7720986891, 542666.7720986891, 170182.8044519482], 
processed observation next is [1.0, 0.08695652173913043, 0.24012638230647723, 0.9133333333333333, 1.0, 1.0, 0.2153280978378153, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15074077002741365, 0.15074077002741365, 0.25400418574917644], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.36832798], dtype=float32), 0.57876873]. 
=============================================
[2019-03-27 07:58:58,506] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3736477e-18 1.0000000e+00 2.7858751e-24 7.5917700e-16 2.4394447e-28], sum to 1.0000
[2019-03-27 07:58:58,513] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0225
[2019-03-27 07:58:58,520] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 88.33333333333333, 1.0, 2.0, 0.3383860009893634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524331.8892428037, 524331.8892428043, 168640.3182976949], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 938400.0000, 
sim time next is 939000.0000, 
raw observation next is [22.45, 88.66666666666667, 1.0, 2.0, 0.3377354240795242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 523423.4149243106, 523423.41492431, 168570.9782972364], 
processed observation next is [0.0, 0.8695652173913043, 0.26303317535545023, 0.8866666666666667, 1.0, 1.0, 0.20209087238496895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14539539303453072, 0.14539539303453056, 0.2515984750705021], 
reward next is 0.7484, 
noisyNet noise sample is [array([0.09193773], dtype=float32), 0.5931743]. 
=============================================
[2019-03-27 07:58:58,536] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[78.422295]
 [78.406654]
 [78.3919  ]
 [78.37551 ]
 [78.33546 ]], R is [[78.39676666]
 [78.36109924]
 [78.32574463]
 [78.29080963]
 [78.25643158]].
[2019-03-27 07:59:00,412] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.9767698e-17 1.0000000e+00 1.1831707e-22 2.2077451e-14 1.7119516e-26], sum to 1.0000
[2019-03-27 07:59:00,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1499
[2019-03-27 07:59:00,425] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.85, 93.5, 1.0, 2.0, 0.3313024121703679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513390.6211512697, 513390.6211512697, 167778.2939011734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 963000.0000, 
sim time next is 963600.0000, 
raw observation next is [21.86666666666667, 93.33333333333334, 1.0, 2.0, 0.3315436398908417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513817.9847701254, 513817.9847701254, 167813.384624132], 
processed observation next is [1.0, 0.13043478260869565, 0.23538704581358633, 0.9333333333333335, 1.0, 1.0, 0.19463089143474901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14272721799170152, 0.14272721799170152, 0.25046773824497315], 
reward next is 0.7495, 
noisyNet noise sample is [array([-1.9982152], dtype=float32), -0.4061312]. 
=============================================
[2019-03-27 07:59:00,874] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 07:59:00,876] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 07:59:00,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:59:00,879] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 07:59:00,880] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:59:00,881] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 07:59:00,882] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 07:59:00,883] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:59:00,884] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:59:00,883] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 07:59:00,888] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 07:59:00,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run70
[2019-03-27 07:59:00,942] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run70
[2019-03-27 07:59:00,964] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run70
[2019-03-27 07:59:00,983] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run70
[2019-03-27 07:59:01,005] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run70
[2019-03-27 07:59:30,693] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06868772], dtype=float32), 0.0479843]
[2019-03-27 07:59:30,695] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.3, 93.0, 1.0, 2.0, 0.6917409322115347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 966722.4083940213, 966722.4083940213, 219069.9267058321]
[2019-03-27 07:59:30,696] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:59:30,699] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2207465e-16 1.0000000e+00 4.0675859e-22 1.3609867e-12 6.9001522e-26], sampled 0.2674879070531705
[2019-03-27 07:59:34,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06868772], dtype=float32), 0.0479843]
[2019-03-27 07:59:34,037] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.63183696166667, 62.15765555499999, 1.0, 2.0, 0.9244304834195346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1292108.773449169, 1292108.77344917, 276742.1321485959]
[2019-03-27 07:59:34,039] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:59:34,042] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3932701e-15 1.0000000e+00 4.3400289e-21 3.5926935e-11 2.5855012e-25], sampled 0.1278224698795145
[2019-03-27 07:59:44,963] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06868772], dtype=float32), 0.0479843]
[2019-03-27 07:59:44,964] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.83911238333333, 98.04223402666668, 1.0, 2.0, 0.3438241535092937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539180.467449227, 539180.4674492276, 169989.7834901206]
[2019-03-27 07:59:44,965] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:59:44,967] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4319625e-17 1.0000000e+00 1.1625318e-23 3.2733000e-15 3.7216270e-27], sampled 0.7609572550052064
[2019-03-27 07:59:49,909] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06868772], dtype=float32), 0.0479843]
[2019-03-27 07:59:49,910] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.90966741333333, 85.72178126333334, 1.0, 2.0, 0.5125578160507374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716225.828550196, 716225.828550196, 185584.9606931363]
[2019-03-27 07:59:49,912] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 07:59:49,918] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0905549e-17 1.0000000e+00 1.6263890e-23 4.2150284e-14 2.3014857e-27], sampled 0.25137263953092837
[2019-03-27 07:59:50,586] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06868772], dtype=float32), 0.0479843]
[2019-03-27 07:59:50,587] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.91666666666666, 61.66666666666667, 1.0, 2.0, 0.5527342792864365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772387.0686703459, 772387.0686703459, 192270.2608183661]
[2019-03-27 07:59:50,587] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 07:59:50,589] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.5748417e-17 1.0000000e+00 3.3107470e-23 1.1146623e-14 1.1346127e-26], sampled 0.5495784801734261
[2019-03-27 07:59:53,115] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06868772], dtype=float32), 0.0479843]
[2019-03-27 07:59:53,116] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.261799735, 74.48095742, 1.0, 2.0, 0.531440843602582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742621.3671491832, 742621.3671491832, 188669.2253911443]
[2019-03-27 07:59:53,117] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 07:59:53,120] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.7199764e-16 1.0000000e+00 1.0829222e-21 5.7960328e-11 2.4808894e-26], sampled 0.7588566338613105
[2019-03-27 08:00:29,526] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06868772], dtype=float32), 0.0479843]
[2019-03-27 08:00:29,527] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.25447937, 63.72803283, 1.0, 2.0, 0.7567152263065402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1057570.556217786, 1057570.556217787, 233566.0689339004]
[2019-03-27 08:00:29,529] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:00:29,531] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6424869e-16 1.0000000e+00 2.2035924e-22 4.6114685e-13 5.3559773e-26], sampled 0.9787157515428204
[2019-03-27 08:00:36,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06868772], dtype=float32), 0.0479843]
[2019-03-27 08:00:36,787] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.18118399833333, 77.38625629500001, 1.0, 2.0, 0.7367935927910796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029714.967926325, 1029714.967926325, 228993.9196018178]
[2019-03-27 08:00:36,788] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:00:36,791] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3557149e-15 1.0000000e+00 2.7853587e-21 1.4158294e-11 2.9950492e-25], sampled 0.8095746832501726
[2019-03-27 08:00:56,381] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.7102 2779259436.2599 931.0000
[2019-03-27 08:00:57,022] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7899.4136 3162415706.3732 1730.0000
[2019-03-27 08:00:57,199] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.7463 2841983634.5660 1121.0000
[2019-03-27 08:00:57,250] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.3016 2927058800.1765 1329.0000
[2019-03-27 08:00:57,513] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8005.2556 3006843355.7250 1742.0000
[2019-03-27 08:00:58,531] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1725000, evaluation results [1725000.0, 7899.413588815834, 3162415706.373165, 1730.0, 8258.301625565711, 2927058800.176479, 1329.0, 8659.710203581937, 2779259436.259869, 931.0, 8005.255569607466, 3006843355.725012, 1742.0, 8500.74627998965, 2841983634.566042, 1121.0]
[2019-03-27 08:01:00,157] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6519104e-14 1.0000000e+00 1.0356925e-19 9.2708632e-09 9.8931930e-23], sum to 1.0000
[2019-03-27 08:01:00,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5038
[2019-03-27 08:01:00,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2033203.395698269 W.
[2019-03-27 08:01:00,183] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.61666666666667, 85.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.727438285571026, 6.9112, 168.9083712343996, 2033203.395698269, 1454151.582407434, 311348.4293867016], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1685400.0000, 
sim time next is 1686000.0000, 
raw observation next is [26.73333333333333, 85.33333333333334, 1.0, 2.0, 0.4812759974092757, 1.0, 1.0, 0.4812759974092757, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1345418.085254436, 1345418.085254436, 294534.3567681734], 
processed observation next is [1.0, 0.5217391304347826, 0.4660347551342811, 0.8533333333333334, 1.0, 1.0, 0.37503132217985025, 1.0, 0.5, 0.37503132217985025, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.37372724590400996, 0.37372724590400996, 0.4396035175644379], 
reward next is 0.5604, 
noisyNet noise sample is [array([0.20324571], dtype=float32), 0.7249845]. 
=============================================
[2019-03-27 08:01:00,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[64.97845]
 [65.76237]
 [66.23071]
 [66.58881]
 [66.9507 ]], R is [[62.50243759]
 [61.8774147 ]
 [61.25864029]
 [61.19590759]
 [61.14585114]].
[2019-03-27 08:01:07,865] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4857891e-16 1.0000000e+00 7.3763531e-23 6.4205379e-13 1.1257579e-25], sum to 1.0000
[2019-03-27 08:01:07,876] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7222
[2019-03-27 08:01:07,881] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.7, 90.33333333333334, 1.0, 2.0, 0.290466962418617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466581.8356410136, 466581.835641013, 164721.0190601599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1128000.0000, 
sim time next is 1128600.0000, 
raw observation next is [20.65, 90.5, 1.0, 2.0, 0.288849489992387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464252.1078560731, 464252.1078560736, 164561.6028122241], 
processed observation next is [1.0, 0.043478260869565216, 0.1777251184834123, 0.905, 1.0, 1.0, 0.14319215661733375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1289589188489092, 0.12895891884890934, 0.24561433255555834], 
reward next is 0.7544, 
noisyNet noise sample is [array([0.29612577], dtype=float32), -0.51471287]. 
=============================================
[2019-03-27 08:01:11,416] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0998398e-17 1.0000000e+00 4.8453979e-23 4.4813689e-13 6.0008792e-27], sum to 1.0000
[2019-03-27 08:01:11,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9933
[2019-03-27 08:01:11,430] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 73.83333333333334, 1.0, 2.0, 0.3517874959090655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543006.3234376422, 543006.3234376422, 170095.7090169036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1195800.0000, 
sim time next is 1196400.0000, 
raw observation next is [24.6, 74.66666666666667, 1.0, 2.0, 0.3513624789805923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541945.5373656523, 541945.5373656517, 169996.6903790539], 
processed observation next is [1.0, 0.8695652173913043, 0.36492890995260674, 0.7466666666666667, 1.0, 1.0, 0.21850901081999072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15054042704601453, 0.15054042704601436, 0.2537264035508267], 
reward next is 0.7463, 
noisyNet noise sample is [array([1.4015604], dtype=float32), -0.542502]. 
=============================================
[2019-03-27 08:01:15,689] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1511499e-17 1.0000000e+00 2.1316580e-24 7.9798796e-15 4.4674772e-27], sum to 1.0000
[2019-03-27 08:01:15,700] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8455
[2019-03-27 08:01:15,705] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 90.66666666666667, 1.0, 2.0, 0.3877274178202514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581494.2746672592, 581494.2746672592, 172934.328901482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1453800.0000, 
sim time next is 1454400.0000, 
raw observation next is [23.0, 92.0, 1.0, 2.0, 0.3844789086674804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 578128.2771486635, 578128.2771486642, 172678.2718035859], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.92, 1.0, 1.0, 0.25840832369575956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16059118809685097, 0.16059118809685116, 0.2577287638859491], 
reward next is 0.7423, 
noisyNet noise sample is [array([1.3450685], dtype=float32), 1.7943355]. 
=============================================
[2019-03-27 08:01:21,193] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.00694284e-17 1.00000000e+00 6.71299577e-22 3.71799047e-14
 1.29637164e-26], sum to 1.0000
[2019-03-27 08:01:21,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3332
[2019-03-27 08:01:21,209] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 94.0, 1.0, 2.0, 0.3239144034887438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 509831.6226230711, 509831.6226230717, 167720.4217062879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1368000.0000, 
sim time next is 1368600.0000, 
raw observation next is [21.16666666666667, 94.16666666666667, 1.0, 2.0, 0.3237531098649404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 509624.6295686291, 509624.6295686285, 167705.5916745497], 
processed observation next is [1.0, 0.8695652173913043, 0.2022116903633494, 0.9416666666666668, 1.0, 1.0, 0.18524471068065104, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14156239710239696, 0.14156239710239682, 0.2503068532455966], 
reward next is 0.7497, 
noisyNet noise sample is [array([1.2039417], dtype=float32), 1.1555126]. 
=============================================
[2019-03-27 08:01:23,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.2074739e-19 1.0000000e+00 5.1094306e-23 4.6576754e-16 6.2602917e-26], sum to 1.0000
[2019-03-27 08:01:23,865] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1408
[2019-03-27 08:01:23,870] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 92.0, 1.0, 2.0, 0.4897628184162399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684362.8373559648, 684362.8373559642, 182008.0208962008], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2098800.0000, 
sim time next is 2099400.0000, 
raw observation next is [25.63333333333333, 91.5, 1.0, 2.0, 0.4922107507916471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687784.5268342736, 687784.5268342736, 182385.0354891311], 
processed observation next is [0.0, 0.30434782608695654, 0.4139020537124801, 0.915, 1.0, 1.0, 0.38820572384535795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1910512574539649, 0.1910512574539649, 0.2722164708793002], 
reward next is 0.7278, 
noisyNet noise sample is [array([0.45610482], dtype=float32), 1.2311883]. 
=============================================
[2019-03-27 08:01:25,158] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8508427e-18 1.0000000e+00 7.3658009e-25 2.4548063e-16 7.1558274e-28], sum to 1.0000
[2019-03-27 08:01:25,167] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5586
[2019-03-27 08:01:25,173] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333334, 69.83333333333333, 1.0, 2.0, 0.4476488768239278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634772.7633866058, 634772.7633866058, 176999.4665341146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1439400.0000, 
sim time next is 1440000.0000, 
raw observation next is [28.0, 70.0, 1.0, 2.0, 0.4530481645940554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639946.9700882114, 639946.9700882114, 177459.5854733691], 
processed observation next is [0.0, 0.6956521739130435, 0.5260663507109005, 0.7, 1.0, 1.0, 0.34102188505307873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17776304724672537, 0.17776304724672537, 0.26486505294532703], 
reward next is 0.7351, 
noisyNet noise sample is [array([1.258601], dtype=float32), -0.3772675]. 
=============================================
[2019-03-27 08:01:25,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[76.20752 ]
 [76.182144]
 [76.156494]
 [76.13509 ]
 [76.11577 ]], R is [[76.25101471]
 [76.22432709]
 [76.1985321 ]
 [76.17349243]
 [76.1491394 ]].
[2019-03-27 08:01:27,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1621622e-16 1.0000000e+00 2.8606723e-21 1.3366157e-10 3.8888851e-25], sum to 1.0000
[2019-03-27 08:01:27,370] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9899
[2019-03-27 08:01:27,373] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 93.33333333333334, 1.0, 2.0, 0.5371679581402501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851030.6025076078, 851030.6025076078, 201084.4899458381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1784400.0000, 
sim time next is 1785000.0000, 
raw observation next is [21.0, 93.66666666666667, 1.0, 2.0, 0.5309232420199458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840383.3991394218, 840383.3991394218, 199840.6788662375], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.9366666666666668, 1.0, 1.0, 0.43484727954210334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23343983309428382, 0.23343983309428382, 0.2982696699496082], 
reward next is 0.7017, 
noisyNet noise sample is [array([0.06455906], dtype=float32), 1.3218272]. 
=============================================
[2019-03-27 08:01:27,387] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.1072  ]
 [69.90702 ]
 [69.764206]
 [69.49471 ]
 [69.39994 ]], R is [[70.29469299]
 [70.29161835]
 [70.2786026 ]
 [70.26711273]
 [70.24163818]].
[2019-03-27 08:01:27,524] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2862978e-17 1.0000000e+00 5.5280482e-23 4.6177946e-14 7.7283008e-27], sum to 1.0000
[2019-03-27 08:01:27,531] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2824
[2019-03-27 08:01:27,538] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 99.0, 1.0, 2.0, 0.4749539791025801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679709.2318490242, 679709.2318490242, 181816.7391612908], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1662000.0000, 
sim time next is 1662600.0000, 
raw observation next is [23.48333333333333, 99.0, 1.0, 2.0, 0.4689554287411384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 670649.5228793566, 670649.5228793573, 180843.2579851706], 
processed observation next is [1.0, 0.21739130434782608, 0.3120063191153238, 0.99, 1.0, 1.0, 0.3601872635435402, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1862915341331546, 0.1862915341331548, 0.2699153104256278], 
reward next is 0.7301, 
noisyNet noise sample is [array([-1.2602265], dtype=float32), 1.6523277]. 
=============================================
[2019-03-27 08:01:29,136] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.8270306e-17 1.0000000e+00 2.5901337e-23 6.0962973e-16 2.1999667e-27], sum to 1.0000
[2019-03-27 08:01:29,145] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4638
[2019-03-27 08:01:29,151] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.85, 97.0, 1.0, 2.0, 0.317154963496824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499638.4752906411, 499638.4752906417, 166958.1857657255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1488600.0000, 
sim time next is 1489200.0000, 
raw observation next is [21.06666666666667, 96.33333333333334, 1.0, 2.0, 0.3211144020052021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504130.2420193978, 504130.2420193972, 167255.4211590555], 
processed observation next is [0.0, 0.21739130434782608, 0.19747235387045833, 0.9633333333333334, 1.0, 1.0, 0.18206554458458082, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14003617833872162, 0.14003617833872145, 0.24963495695381416], 
reward next is 0.7504, 
noisyNet noise sample is [array([-0.49564913], dtype=float32), -0.28270668]. 
=============================================
[2019-03-27 08:01:31,502] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8863294e-16 1.0000000e+00 4.0714825e-22 4.9882982e-13 1.4252155e-25], sum to 1.0000
[2019-03-27 08:01:31,509] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1999
[2019-03-27 08:01:31,516] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 89.66666666666667, 1.0, 2.0, 0.3063452297009812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483532.9459328792, 483532.9459328799, 165790.6940724863], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1570800.0000, 
sim time next is 1571400.0000, 
raw observation next is [21.6, 89.5, 1.0, 2.0, 0.3064054166652293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 483867.5143219733, 483867.5143219727, 165820.8009257269], 
processed observation next is [1.0, 0.17391304347826086, 0.22274881516587688, 0.895, 1.0, 1.0, 0.16434387550027627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13440764286721482, 0.13440764286721463, 0.2474937327249655], 
reward next is 0.7525, 
noisyNet noise sample is [array([-0.9445201], dtype=float32), 0.4511053]. 
=============================================
[2019-03-27 08:01:33,053] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.0445905e-16 1.0000000e+00 4.2625638e-23 1.2648263e-13 3.7983770e-26], sum to 1.0000
[2019-03-27 08:01:33,061] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0919
[2019-03-27 08:01:33,069] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 88.16666666666667, 1.0, 2.0, 0.3189601329738624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500083.7246922507, 500083.7246922507, 166931.8206902927], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1576200.0000, 
sim time next is 1576800.0000, 
raw observation next is [22.2, 88.0, 1.0, 2.0, 0.3212464011813297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 502674.9943207598, 502674.9943207592, 167100.9012685133], 
processed observation next is [1.0, 0.2608695652173913, 0.2511848341232228, 0.88, 1.0, 1.0, 0.1822245797365418, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13963194286687772, 0.13963194286687755, 0.24940433025151237], 
reward next is 0.7506, 
noisyNet noise sample is [array([0.44515166], dtype=float32), -1.2041609]. 
=============================================
[2019-03-27 08:01:34,773] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9674166e-16 1.0000000e+00 8.8864000e-20 6.7461148e-10 6.0201910e-25], sum to 1.0000
[2019-03-27 08:01:34,774] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3213
[2019-03-27 08:01:34,777] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.01666666666667, 85.00000000000001, 1.0, 2.0, 0.650490053267038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 975997.7207925948, 975997.7207925948, 219010.5520290937], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1602600.0000, 
sim time next is 1603200.0000, 
raw observation next is [24.03333333333333, 85.0, 1.0, 2.0, 0.7575287462398848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1135957.378612038, 1135957.378612038, 244002.6636194027], 
processed observation next is [1.0, 0.5652173913043478, 0.3380726698262243, 0.85, 1.0, 1.0, 0.7078659593251624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31554371628112166, 0.31554371628112166, 0.36418308002895927], 
reward next is 0.6358, 
noisyNet noise sample is [array([0.48224768], dtype=float32), 1.3048671]. 
=============================================
[2019-03-27 08:01:36,389] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5035586e-11 9.9991572e-01 5.7011310e-16 8.4308442e-05 5.2081075e-19], sum to 1.0000
[2019-03-27 08:01:36,399] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7621
[2019-03-27 08:01:36,407] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1776240.090609659 W.
[2019-03-27 08:01:36,410] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.53333333333333, 78.33333333333334, 1.0, 2.0, 0.6352546940992266, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.944039745336311, 6.9112, 168.9127332281176, 1776240.090609659, 1752942.475807918, 372768.7137982527], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1941600.0000, 
sim time next is 1942200.0000, 
raw observation next is [26.6, 78.0, 1.0, 2.0, 0.4225640934560865, 1.0, 1.0, 0.4225640934560865, 1.0, 2.0, 0.7056527361885672, 6.911199999999999, 6.9112, 170.5573041426782, 1772284.064323095, 1772284.064323095, 361380.081721689], 
processed observation next is [1.0, 0.4782608695652174, 0.4597156398104266, 0.78, 1.0, 1.0, 0.30429408850130896, 1.0, 0.5, 0.30429408850130896, 1.0, 1.0, 0.6410399221811794, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4923011289786375, 0.4923011289786375, 0.5393732563010284], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.35232365], dtype=float32), -2.1159978]. 
=============================================
[2019-03-27 08:01:37,798] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1426072e-16 1.0000000e+00 1.6004002e-21 1.2199859e-11 7.8010806e-26], sum to 1.0000
[2019-03-27 08:01:37,807] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0382
[2019-03-27 08:01:37,813] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.13333333333333, 98.33333333333334, 1.0, 2.0, 0.4232027237499308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616241.6862204515, 616241.6862204521, 175636.7641358162], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1642800.0000, 
sim time next is 1643400.0000, 
raw observation next is [23.15, 98.5, 1.0, 2.0, 0.4246064280177466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 617380.0500797402, 617380.0500797395, 175720.4870394862], 
processed observation next is [1.0, 0.0, 0.2962085308056872, 0.985, 1.0, 1.0, 0.30675473255150193, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1714944583554834, 0.17149445835548321, 0.2622693836410242], 
reward next is 0.7377, 
noisyNet noise sample is [array([-0.04996579], dtype=float32), 0.9003817]. 
=============================================
[2019-03-27 08:01:43,777] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.7819791e-16 1.0000000e+00 4.9254260e-21 2.1046855e-10 3.6976605e-24], sum to 1.0000
[2019-03-27 08:01:43,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8060
[2019-03-27 08:01:43,791] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 88.5, 1.0, 2.0, 0.7681212971222121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1111738.9730765, 1111738.973076499, 241469.6755546128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1762200.0000, 
sim time next is 1762800.0000, 
raw observation next is [24.43333333333333, 88.33333333333334, 1.0, 2.0, 0.8651847732561314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1256650.847859527, 1256650.847859528, 267346.7810243415], 
processed observation next is [1.0, 0.391304347826087, 0.3570300157977882, 0.8833333333333334, 1.0, 1.0, 0.8375720159712426, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3490696799609797, 0.34906967996098, 0.39902504630498725], 
reward next is 0.6010, 
noisyNet noise sample is [array([0.41456547], dtype=float32), -0.62225974]. 
=============================================
[2019-03-27 08:01:48,394] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.4037525e-16 1.0000000e+00 2.9831968e-21 9.7237982e-11 1.3127153e-24], sum to 1.0000
[2019-03-27 08:01:48,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6978
[2019-03-27 08:01:48,405] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 90.66666666666667, 1.0, 2.0, 0.8773852283162079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1264676.635659685, 1264676.635659684, 269362.8476898289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1849200.0000, 
sim time next is 1849800.0000, 
raw observation next is [24.48333333333333, 90.33333333333333, 1.0, 2.0, 0.8876414164202816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1275697.956631741, 1275697.956631741, 271685.1948351348], 
processed observation next is [1.0, 0.391304347826087, 0.3593996840442337, 0.9033333333333333, 1.0, 1.0, 0.8646282125545561, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35436054350881696, 0.35436054350881696, 0.40550029079870864], 
reward next is 0.5945, 
noisyNet noise sample is [array([-1.4461787], dtype=float32), -0.6168133]. 
=============================================
[2019-03-27 08:01:50,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4567906e-16 1.0000000e+00 6.7467292e-22 1.9524074e-12 6.0968274e-25], sum to 1.0000
[2019-03-27 08:01:50,190] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8613
[2019-03-27 08:01:50,197] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333334, 85.33333333333334, 1.0, 2.0, 0.7321125345210029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1023169.74562647, 1023169.745626471, 227929.24923643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2186400.0000, 
sim time next is 2187000.0000, 
raw observation next is [27.65, 84.5, 1.0, 2.0, 0.7342333500924266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1026135.140452633, 1026135.140452633, 228408.1018651827], 
processed observation next is [1.0, 0.30434782608695654, 0.509478672985782, 0.845, 1.0, 1.0, 0.6797992169788273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2850375390146203, 0.2850375390146203, 0.34090761472415326], 
reward next is 0.6591, 
noisyNet noise sample is [array([0.4073149], dtype=float32), -0.9617262]. 
=============================================
[2019-03-27 08:01:50,206] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[67.82958]
 [67.82696]
 [68.03624]
 [68.00339]
 [67.8413 ]], R is [[67.80137634]
 [67.78316498]
 [67.75179291]
 [67.74611664]
 [67.7565918 ]].
[2019-03-27 08:01:50,438] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3837485e-15 9.9999952e-01 6.5459998e-19 4.9964621e-07 1.2285102e-23], sum to 1.0000
[2019-03-27 08:01:50,447] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4785
[2019-03-27 08:01:50,452] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.48333333333333, 87.16666666666666, 1.0, 2.0, 0.4949544570009081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691619.6584044911, 691619.6584044911, 182810.1071010977], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1878600.0000, 
sim time next is 1879200.0000, 
raw observation next is [26.4, 87.0, 1.0, 2.0, 0.4977559886810815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695535.6322628313, 695535.6322628313, 183245.6155251919], 
processed observation next is [1.0, 0.782608695652174, 0.45023696682464454, 0.87, 1.0, 1.0, 0.39488673335070057, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19320434229523092, 0.19320434229523092, 0.2735009186943163], 
reward next is 0.7265, 
noisyNet noise sample is [array([1.5145432], dtype=float32), 0.152552]. 
=============================================
[2019-03-27 08:01:51,513] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 08:01:51,515] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:01:51,517] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:01:51,518] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:01:51,519] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:01:51,520] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:01:51,521] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:01:51,522] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:01:51,523] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:01:51,524] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:01:51,524] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:01:51,554] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run71
[2019-03-27 08:01:51,554] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run71
[2019-03-27 08:01:51,594] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run71
[2019-03-27 08:01:51,614] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run71
[2019-03-27 08:01:51,615] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run71
[2019-03-27 08:03:09,581] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06980299], dtype=float32), 0.04781026]
[2019-03-27 08:03:09,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.51666666666667, 77.83333333333333, 1.0, 2.0, 0.7366163204705434, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.000920060075142, 6.9112, 168.9123494042893, 1926369.225914999, 1862718.956143857, 392981.1494171622]
[2019-03-27 08:03:09,587] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:03:09,591] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5947356e-11 9.9999106e-01 7.8676354e-16 8.8897241e-06 4.5826194e-19], sampled 0.41670282677945547
[2019-03-27 08:03:09,592] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1926369.225914999 W.
[2019-03-27 08:03:19,631] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06980299], dtype=float32), 0.04781026]
[2019-03-27 08:03:19,632] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.6, 91.33333333333334, 1.0, 2.0, 0.5132568378207801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717202.9406846645, 717202.9406846651, 185698.1869493474]
[2019-03-27 08:03:19,633] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:03:19,635] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7974894e-16 1.0000000e+00 3.1502210e-22 3.2227519e-13 9.8286664e-26], sampled 0.2326904974665719
[2019-03-27 08:03:21,844] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06980299], dtype=float32), 0.04781026]
[2019-03-27 08:03:21,846] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.58726755, 91.034553655, 1.0, 2.0, 0.6000363752479875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838512.7820824216, 838512.7820824216, 200750.4400426229]
[2019-03-27 08:03:21,847] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:03:21,851] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.81435888e-17 1.00000000e+00 8.40056905e-23 6.12881057e-13
 1.16068664e-26], sampled 0.1563073197516609
[2019-03-27 08:03:28,198] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06980299], dtype=float32), 0.04781026]
[2019-03-27 08:03:28,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.86666666666667, 92.16666666666667, 1.0, 2.0, 0.6030354978630474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842705.5283739825, 842705.5283739825, 201306.1579086976]
[2019-03-27 08:03:28,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:03:28,201] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8650763e-16 1.0000000e+00 3.2787119e-22 2.4214674e-13 1.4586796e-25], sampled 0.866012104735268
[2019-03-27 08:03:48,142] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7922.4279 3159577026.0044 1665.0000
[2019-03-27 08:03:48,177] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8501.7419 2841384559.2876 1112.0000
[2019-03-27 08:03:48,192] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8263.4670 2926590825.3951 1311.0000
[2019-03-27 08:03:48,221] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8021.2093 3004595979.6525 1699.0000
[2019-03-27 08:03:48,268] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.3361 2779185179.6143 931.0000
[2019-03-27 08:03:49,285] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1750000, evaluation results [1750000.0, 7922.42786011502, 3159577026.0044456, 1665.0, 8263.466961615626, 2926590825.3951426, 1311.0, 8658.336109875556, 2779185179.614323, 931.0, 8021.209299338481, 3004595979.652461, 1699.0, 8501.741917714728, 2841384559.287555, 1112.0]
[2019-03-27 08:03:53,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0549575e-11 9.9905556e-01 7.2624155e-16 9.4439270e-04 3.1045959e-20], sum to 1.0000
[2019-03-27 08:03:53,375] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5664
[2019-03-27 08:03:53,381] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 88.0, 1.0, 2.0, 0.3724327110767321, 1.0, 1.0, 0.3724327110767321, 1.0, 1.0, 0.6206604669388291, 6.9112, 6.9112, 170.5573041426782, 1565270.957344361, 1565270.957344361, 334958.7502160473], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1962000.0000, 
sim time next is 1962600.0000, 
raw observation next is [24.5, 88.33333333333334, 1.0, 2.0, 0.5446478465501478, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564423074, 780641.8357903783, 780641.8357903783, 193380.576328688], 
processed observation next is [1.0, 0.7391304347826086, 0.3601895734597157, 0.8833333333333334, 1.0, 1.0, 0.4513829476507804, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399448177846, 0.21684495438621618, 0.21684495438621618, 0.28862772586371344], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.520429], dtype=float32), -1.4085568]. 
=============================================
[2019-03-27 08:03:54,019] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3273727e-17 1.0000000e+00 9.8604893e-23 3.2757515e-13 1.8792655e-25], sum to 1.0000
[2019-03-27 08:03:54,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9693
[2019-03-27 08:03:54,030] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 96.66666666666667, 1.0, 2.0, 0.4589800935241213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650967.111097416, 650967.1110974167, 178659.9968710756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1987800.0000, 
sim time next is 1988400.0000, 
raw observation next is [24.03333333333333, 96.33333333333334, 1.0, 2.0, 0.4605817607332746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652516.0638483983, 652516.0638483976, 178803.1379552874], 
processed observation next is [0.0, 0.0, 0.3380726698262243, 0.9633333333333334, 1.0, 1.0, 0.3500985069075597, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18125446218011063, 0.18125446218011043, 0.2668703551571454], 
reward next is 0.7331, 
noisyNet noise sample is [array([-1.3906331], dtype=float32), -0.3115356]. 
=============================================
[2019-03-27 08:03:55,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1352593e-16 1.0000000e+00 3.9969807e-24 1.8235320e-14 3.7794553e-27], sum to 1.0000
[2019-03-27 08:03:55,616] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9286
[2019-03-27 08:03:55,621] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 73.0, 1.0, 2.0, 0.5711457773894059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798124.835167758, 798124.835167758, 195494.2186136233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2134800.0000, 
sim time next is 2135400.0000, 
raw observation next is [30.61666666666667, 73.66666666666667, 1.0, 2.0, 0.5722373320912588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 799650.7588290756, 799650.7588290762, 195688.1862268141], 
processed observation next is [0.0, 0.7391304347826086, 0.6500789889415484, 0.7366666666666667, 1.0, 1.0, 0.48462329167621543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22212521078585434, 0.2221252107858545, 0.2920719197415136], 
reward next is 0.7079, 
noisyNet noise sample is [array([-0.12674087], dtype=float32), -0.0069144526]. 
=============================================
[2019-03-27 08:04:01,344] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.29570153e-15 1.00000000e+00 7.12284095e-22 1.69626384e-13
 1.09629495e-26], sum to 1.0000
[2019-03-27 08:04:01,352] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7699
[2019-03-27 08:04:01,360] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 73.5, 1.0, 2.0, 0.5695801170196924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795936.1459319879, 795936.1459319879, 195216.2238433336], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2133000.0000, 
sim time next is 2133600.0000, 
raw observation next is [30.7, 73.33333333333333, 1.0, 2.0, 0.570174314702858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796766.7943068421, 796766.7943068421, 195321.6356351433], 
processed observation next is [0.0, 0.6956521739130435, 0.6540284360189573, 0.7333333333333333, 1.0, 1.0, 0.4821377285576602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22132410952967838, 0.22132410952967838, 0.291524829306184], 
reward next is 0.7085, 
noisyNet noise sample is [array([0.06061254], dtype=float32), -1.059514]. 
=============================================
[2019-03-27 08:04:05,695] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2560087e-14 1.0000000e+00 4.7995436e-20 4.5805599e-09 4.4192615e-24], sum to 1.0000
[2019-03-27 08:04:05,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8005
[2019-03-27 08:04:05,708] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.08333333333333, 92.33333333333333, 1.0, 2.0, 0.55378696632678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773858.6221313174, 773858.6221313174, 192452.5599444059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2490600.0000, 
sim time next is 2491200.0000, 
raw observation next is [27.0, 93.0, 1.0, 2.0, 0.5543193037997713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774602.7785973914, 774602.7785973914, 192544.4804045955], 
processed observation next is [1.0, 0.8695652173913043, 0.4786729857819906, 0.93, 1.0, 1.0, 0.463035305782857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2151674384992754, 0.2151674384992754, 0.2873798214993963], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.84225655], dtype=float32), -0.85639876]. 
=============================================
[2019-03-27 08:04:13,042] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7682991e-14 1.0000000e+00 1.4132606e-19 1.1675710e-09 1.7028561e-22], sum to 1.0000
[2019-03-27 08:04:13,049] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7816
[2019-03-27 08:04:13,054] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 82.0, 1.0, 2.0, 0.6908237736366518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 965440.0775156941, 965440.0775156947, 218875.538307512], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2346000.0000, 
sim time next is 2346600.0000, 
raw observation next is [27.35, 82.0, 1.0, 2.0, 0.6725323260219531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 939866.1013753957, 939866.1013753957, 215026.5012050357], 
processed observation next is [1.0, 0.13043478260869565, 0.4952606635071091, 0.82, 1.0, 1.0, 0.6054606337613893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.261073917048721, 0.261073917048721, 0.3209350764254264], 
reward next is 0.6791, 
noisyNet noise sample is [array([-0.06971566], dtype=float32), 1.3764155]. 
=============================================
[2019-03-27 08:04:14,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9549784e-13 1.0000000e+00 2.8916605e-18 2.4871852e-08 2.6944358e-21], sum to 1.0000
[2019-03-27 08:04:14,999] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4922
[2019-03-27 08:04:15,005] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 72.0, 1.0, 2.0, 0.855149821951676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1195218.267409612, 1195218.267409612, 257897.9041275255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2361600.0000, 
sim time next is 2362200.0000, 
raw observation next is [29.65, 71.5, 1.0, 2.0, 0.8092741896984313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1131065.067225003, 1131065.067225003, 246199.7353304236], 
processed observation next is [1.0, 0.34782608695652173, 0.6042654028436019, 0.715, 1.0, 1.0, 0.7702098671065438, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31418474089583415, 0.31418474089583415, 0.3674622915379457], 
reward next is 0.6325, 
noisyNet noise sample is [array([0.07323981], dtype=float32), 0.46315214]. 
=============================================
[2019-03-27 08:04:18,968] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0811998e-15 1.0000000e+00 2.0647069e-20 1.9425642e-08 1.0058029e-24], sum to 1.0000
[2019-03-27 08:04:18,975] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4571
[2019-03-27 08:04:18,983] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666666, 72.33333333333333, 1.0, 2.0, 0.5792358332091772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 809434.2678781697, 809434.2678781691, 196942.0413811374], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2403600.0000, 
sim time next is 2404200.0000, 
raw observation next is [31.03333333333333, 73.16666666666667, 1.0, 2.0, 0.5830151288377925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814717.5470553037, 814717.547055303, 197624.7943381713], 
processed observation next is [1.0, 0.8260869565217391, 0.669826224328594, 0.7316666666666667, 1.0, 1.0, 0.49760858896119575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22631042973758436, 0.22631042973758417, 0.2949623796092109], 
reward next is 0.7050, 
noisyNet noise sample is [array([0.05676734], dtype=float32), -0.6045268]. 
=============================================
[2019-03-27 08:04:21,100] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.4155600e-18 1.0000000e+00 2.4911367e-22 5.1176638e-15 2.0268301e-25], sum to 1.0000
[2019-03-27 08:04:21,107] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1568
[2019-03-27 08:04:21,113] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5061442769512821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707260.8465983135, 707260.8465983128, 184564.6073105778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2642400.0000, 
sim time next is 2643000.0000, 
raw observation next is [27.0, 83.16666666666667, 1.0, 2.0, 0.5042421916762652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704602.0857174951, 704602.0857174951, 184263.4967873398], 
processed observation next is [0.0, 0.6086956521739131, 0.4786729857819906, 0.8316666666666667, 1.0, 1.0, 0.4027014357545363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19572280158819308, 0.19572280158819308, 0.27502014445871614], 
reward next is 0.7250, 
noisyNet noise sample is [array([0.4216875], dtype=float32), 0.024762174]. 
=============================================
[2019-03-27 08:04:21,126] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.93843]
 [72.86823]
 [72.85455]
 [72.83712]
 [72.81535]], R is [[72.97121429]
 [72.96603394]
 [72.96091461]
 [72.95586395]
 [72.95090485]].
[2019-03-27 08:04:37,115] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.9335846e-18 1.0000000e+00 1.2163069e-24 2.5681745e-15 3.2021295e-27], sum to 1.0000
[2019-03-27 08:04:37,124] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2368
[2019-03-27 08:04:37,131] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.34953729368586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538453.7168682074, 538453.7168682081, 169689.5798246755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2763600.0000, 
sim time next is 2764200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3498789882288919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538979.1598952005, 538979.1598952012, 169732.6897413997], 
processed observation next is [0.0, 1.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21672167256493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14971643330422235, 0.14971643330422255, 0.25333237274835774], 
reward next is 0.7467, 
noisyNet noise sample is [array([-1.325052], dtype=float32), -1.2452133]. 
=============================================
[2019-03-27 08:04:37,489] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.8286746e-17 1.0000000e+00 3.6984664e-23 1.0548777e-15 6.0929848e-27], sum to 1.0000
[2019-03-27 08:04:37,497] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2843
[2019-03-27 08:04:37,504] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 97.0, 1.0, 2.0, 0.3879939931931745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 581264.5881379509, 581264.5881379503, 172894.327902857], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2748600.0000, 
sim time next is 2749200.0000, 
raw observation next is [22.33333333333334, 98.0, 1.0, 2.0, 0.385555475527079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578518.6681064686, 578518.6681064686, 172675.5154239802], 
processed observation next is [0.0, 0.8260869565217391, 0.2575039494470777, 0.98, 1.0, 1.0, 0.25970539220129996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16069963002957463, 0.16069963002957463, 0.2577246498865376], 
reward next is 0.7423, 
noisyNet noise sample is [array([0.41951552], dtype=float32), 0.24119447]. 
=============================================
[2019-03-27 08:04:39,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5227486e-17 1.0000000e+00 2.7277455e-22 4.0267807e-15 2.6273994e-25], sum to 1.0000
[2019-03-27 08:04:39,292] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8657
[2019-03-27 08:04:39,302] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 99.00000000000001, 1.0, 2.0, 0.358306491563518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555958.1000476147, 555958.1000476147, 171247.8310646035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2779800.0000, 
sim time next is 2780400.0000, 
raw observation next is [21.33333333333334, 98.0, 1.0, 2.0, 0.3415665411043997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529250.0020647112, 529250.0020647112, 169033.531024559], 
processed observation next is [1.0, 0.17391304347826086, 0.2101105845181678, 0.98, 1.0, 1.0, 0.20670667602939724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14701388946241978, 0.14701388946241978, 0.25228885227546116], 
reward next is 0.7477, 
noisyNet noise sample is [array([-0.5268968], dtype=float32), -0.1571375]. 
=============================================
[2019-03-27 08:04:42,449] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 08:04:42,451] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:04:42,452] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:04:42,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:04:42,454] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:04:42,452] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:04:42,454] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:04:42,456] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:04:42,455] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:04:42,458] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:04:42,457] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:04:42,486] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run72
[2019-03-27 08:04:42,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run72
[2019-03-27 08:04:42,506] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run72
[2019-03-27 08:04:42,541] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run72
[2019-03-27 08:04:42,542] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run72
[2019-03-27 08:04:47,306] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07058687], dtype=float32), 0.047997337]
[2019-03-27 08:04:47,306] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.9, 76.0, 1.0, 2.0, 0.2515914140894691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 414572.043834683, 414572.043834683, 161046.0788522614]
[2019-03-27 08:04:47,308] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:04:47,311] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4009772e-17 1.0000000e+00 2.4859995e-23 1.3687476e-15 2.2195241e-26], sampled 0.7947822680669375
[2019-03-27 08:04:51,982] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07058687], dtype=float32), 0.047997337]
[2019-03-27 08:04:51,985] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.3, 54.0, 1.0, 2.0, 0.4886941664189728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 784516.1618437277, 784516.1618437277, 193054.236606703]
[2019-03-27 08:04:51,987] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:04:51,990] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.1672703e-17 1.0000000e+00 1.0485391e-22 3.0390302e-14 4.0883607e-26], sampled 0.5087180181304624
[2019-03-27 08:05:03,332] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07058687], dtype=float32), 0.047997337]
[2019-03-27 08:05:03,334] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.91666666666667, 56.5, 1.0, 2.0, 0.2647107584974214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 436124.7283092735, 436124.7283092735, 162377.9989214032]
[2019-03-27 08:05:03,335] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:05:03,339] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3168327e-17 1.0000000e+00 8.2023359e-24 1.0472437e-15 4.1365039e-27], sampled 0.4208252960049964
[2019-03-27 08:05:16,746] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07058687], dtype=float32), 0.047997337]
[2019-03-27 08:05:16,748] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.32009602166666, 74.87112722833334, 1.0, 2.0, 0.7932890584949699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1108712.120806692, 1108712.120806692, 242270.5837818031]
[2019-03-27 08:05:16,748] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:05:16,751] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7335484e-16 1.0000000e+00 2.3695266e-22 1.0807228e-13 9.8864252e-26], sampled 0.2899766651025796
[2019-03-27 08:05:35,581] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07058687], dtype=float32), 0.047997337]
[2019-03-27 08:05:35,582] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.62273971166667, 68.95498058, 1.0, 2.0, 0.7084962288716619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 990149.2015626555, 990149.2015626549, 222686.3133121179]
[2019-03-27 08:05:35,583] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:05:35,585] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.1338135e-16 1.0000000e+00 6.4952252e-22 9.7521318e-13 2.3667316e-25], sampled 0.7135708249134698
[2019-03-27 08:05:39,196] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07058687], dtype=float32), 0.047997337]
[2019-03-27 08:05:39,197] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [36.82894699333334, 53.513122415, 1.0, 2.0, 0.6626079304816949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 925990.680125758, 925990.6801257575, 212989.392681061]
[2019-03-27 08:05:39,198] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:05:39,201] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.3927136e-17 1.0000000e+00 7.2275026e-23 2.0996262e-13 1.2401531e-26], sampled 0.8769009231374326
[2019-03-27 08:05:57,325] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07058687], dtype=float32), 0.047997337]
[2019-03-27 08:05:57,326] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.3, 77.0, 1.0, 2.0, 0.6022686598575858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 841633.4942027768, 841633.4942027761, 201168.3555480023]
[2019-03-27 08:05:57,329] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:05:57,332] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4396890e-17 1.0000000e+00 2.9535716e-23 7.7578162e-14 4.4607168e-27], sampled 0.24404134462404314
[2019-03-27 08:06:28,357] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07058687], dtype=float32), 0.047997337]
[2019-03-27 08:06:28,358] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.15000000000001, 62.33333333333334, 1.0, 2.0, 0.6303070530791153, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984543229530489, 6.9112, 168.9124385303002, 1762394.471920136, 1710362.420171469, 370296.9606479909]
[2019-03-27 08:06:28,359] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:06:28,362] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6152915e-12 9.9999881e-01 2.7695532e-17 1.2239358e-06 6.8012486e-21], sampled 0.599813977011715
[2019-03-27 08:06:28,363] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1762394.471920136 W.
[2019-03-27 08:06:32,837] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07058687], dtype=float32), 0.047997337]
[2019-03-27 08:06:32,838] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.08333333333334, 74.33333333333334, 1.0, 2.0, 0.464618845318547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660553.2776833214, 660553.2776833208, 179695.6071273446]
[2019-03-27 08:06:32,840] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:06:32,844] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6505234e-17 1.0000000e+00 2.2265831e-23 1.7664731e-15 1.5883962e-26], sampled 0.25456257787418113
[2019-03-27 08:06:38,673] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2599 2779170259.6100 931.0000
[2019-03-27 08:06:38,689] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8010.6793 3005786620.9545 1722.0000
[2019-03-27 08:06:38,717] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7907.8657 3161283747.4255 1695.0000
[2019-03-27 08:06:38,797] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.0330 2926785005.6813 1324.0000
[2019-03-27 08:06:38,875] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8504.1695 2841190251.6053 1106.0000
[2019-03-27 08:06:39,893] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1775000, evaluation results [1775000.0, 7907.86574874523, 3161283747.4254932, 1695.0, 8258.032971913311, 2926785005.681253, 1324.0, 8659.259940031501, 2779170259.6100073, 931.0, 8010.679314537143, 3005786620.954531, 1722.0, 8504.169493977313, 2841190251.605268, 1106.0]
[2019-03-27 08:06:42,507] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2467603e-17 1.0000000e+00 4.2398297e-22 2.7594396e-12 6.9099772e-26], sum to 1.0000
[2019-03-27 08:06:42,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6399
[2019-03-27 08:06:42,521] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 92.66666666666666, 1.0, 2.0, 0.5411510038919033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832656.5604470108, 832656.5604470108, 199474.2511208097], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2886000.0000, 
sim time next is 2886600.0000, 
raw observation next is [22.25, 92.33333333333333, 1.0, 2.0, 0.5338712714333719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821272.4421623924, 821272.4421623924, 198104.5331663125], 
processed observation next is [1.0, 0.391304347826087, 0.2535545023696683, 0.9233333333333333, 1.0, 1.0, 0.4383991222088818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2281312339339979, 0.2281312339339979, 0.2956784077109142], 
reward next is 0.7043, 
noisyNet noise sample is [array([-0.3123411], dtype=float32), -0.13101535]. 
=============================================
[2019-03-27 08:06:44,543] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.3172528e-16 1.0000000e+00 1.3600812e-23 1.8033154e-14 1.5556548e-26], sum to 1.0000
[2019-03-27 08:06:44,554] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5755
[2019-03-27 08:06:44,560] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3084721257190241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 491020.2512022325, 491020.2512022319, 166416.1234467001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2923200.0000, 
sim time next is 2923800.0000, 
raw observation next is [20.08333333333334, 99.5, 1.0, 2.0, 0.3084631333891708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490992.1554524192, 490992.1554524192, 166413.9369656206], 
processed observation next is [1.0, 0.8695652173913043, 0.15086887835703036, 0.995, 1.0, 1.0, 0.1668230522761094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13638670984789422, 0.13638670984789422, 0.24837901039644866], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.9267883], dtype=float32), -0.7598062]. 
=============================================
[2019-03-27 08:06:45,467] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0940838e-16 1.0000000e+00 6.1498839e-22 5.7974167e-14 9.9701246e-26], sum to 1.0000
[2019-03-27 08:06:45,474] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7248
[2019-03-27 08:06:45,478] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.00000000000001, 1.0, 2.0, 0.3333950727343722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527203.1610932067, 527203.1610932067, 169111.7180486134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2956200.0000, 
sim time next is 2956800.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3112955111629772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 492250.5029396613, 492250.5029396607, 166449.4174983875], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 0.94, 1.0, 1.0, 0.17023555561804482, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1367362508165726, 0.13673625081657242, 0.24843196641550375], 
reward next is 0.7516, 
noisyNet noise sample is [array([-0.11062018], dtype=float32), -0.6982978]. 
=============================================
[2019-03-27 08:06:47,710] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5026012e-17 1.0000000e+00 1.0653194e-22 1.3328414e-12 8.9275921e-25], sum to 1.0000
[2019-03-27 08:06:47,719] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2018
[2019-03-27 08:06:47,725] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333334, 98.0, 1.0, 2.0, 0.4701993980493784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747117.9361224582, 747117.9361224582, 189293.4138336418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2985600.0000, 
sim time next is 2986200.0000, 
raw observation next is [20.5, 97.0, 1.0, 2.0, 0.4848472576099283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769491.9380906202, 769491.9380906209, 191715.1566591969], 
processed observation next is [1.0, 0.5652173913043478, 0.1706161137440759, 0.97, 1.0, 1.0, 0.3793340453131666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21374776058072784, 0.21374776058072803, 0.286142024864473], 
reward next is 0.7139, 
noisyNet noise sample is [array([0.72921145], dtype=float32), -0.5184097]. 
=============================================
[2019-03-27 08:06:49,373] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.10746085e-15 1.00000000e+00 3.08003124e-21 1.10400064e-10
 1.08354029e-24], sum to 1.0000
[2019-03-27 08:06:49,382] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6598
[2019-03-27 08:06:49,392] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333333, 75.66666666666666, 1.0, 2.0, 0.7779722481120304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1087510.913913973, 1087510.913913973, 238602.1432485454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3739200.0000, 
sim time next is 3739800.0000, 
raw observation next is [27.66666666666667, 74.83333333333334, 1.0, 2.0, 0.7895435816668988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1103474.668945929, 1103474.668945929, 241356.8499119864], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012641, 0.7483333333333334, 1.0, 1.0, 0.7464380502010828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3065207413738692, 0.3065207413738692, 0.36023410434624836], 
reward next is 0.6398, 
noisyNet noise sample is [array([-0.08328041], dtype=float32), 2.054891]. 
=============================================
[2019-03-27 08:06:50,251] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.1667727e-17 1.0000000e+00 4.5600356e-22 1.6549927e-13 3.9435446e-26], sum to 1.0000
[2019-03-27 08:06:50,259] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3129
[2019-03-27 08:06:50,265] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3014658689513921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480068.0505538484, 480068.0505538478, 165626.7477637717], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3030000.0000, 
sim time next is 3030600.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3011493778918823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479564.4341337842, 479564.4341337842, 165590.718614072], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15801129866491842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13321234281494004, 0.13321234281494004, 0.2471503262896597], 
reward next is 0.7528, 
noisyNet noise sample is [array([0.31513822], dtype=float32), -1.3272712]. 
=============================================
[2019-03-27 08:06:59,318] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3928196e-18 1.0000000e+00 1.2182724e-22 8.7672979e-14 5.2188676e-27], sum to 1.0000
[2019-03-27 08:06:59,325] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7607
[2019-03-27 08:06:59,329] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 53.0, 1.0, 2.0, 0.5730462446089943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800781.5682079748, 800781.5682079748, 195832.5180236813], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3862800.0000, 
sim time next is 3863400.0000, 
raw observation next is [34.66666666666667, 53.5, 1.0, 2.0, 0.5896154980495452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 823944.6186688639, 823944.6186688646, 198826.1769806215], 
processed observation next is [0.0, 0.7391304347826086, 0.8420221169036337, 0.535, 1.0, 1.0, 0.5055608410235484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22887350518579552, 0.22887350518579572, 0.29675548803077834], 
reward next is 0.7032, 
noisyNet noise sample is [array([0.9689691], dtype=float32), 0.06116157]. 
=============================================
[2019-03-27 08:07:07,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.09397356e-16 1.00000000e+00 5.37055545e-22 1.14028821e-11
 1.83496827e-26], sum to 1.0000
[2019-03-27 08:07:07,488] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7725
[2019-03-27 08:07:07,493] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5163185270364803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721482.6658881874, 721482.665888188, 186192.4860737829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3443400.0000, 
sim time next is 3444000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5163119925786053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721473.5317996818, 721473.5317996818, 186191.4310685439], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41724336455253647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2004093143888005, 0.2004093143888005, 0.27789765831125957], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.66129816], dtype=float32), -0.09421356]. 
=============================================
[2019-03-27 08:07:07,502] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.47304 ]
 [72.54165 ]
 [72.452446]
 [72.34786 ]
 [72.33306 ]], R is [[72.4484024 ]
 [72.44602203]
 [72.44360352]
 [72.44113159]
 [72.43873596]].
[2019-03-27 08:07:14,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0755509e-16 1.0000000e+00 3.0825012e-20 5.1867774e-11 1.8563816e-23], sum to 1.0000
[2019-03-27 08:07:14,847] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7407
[2019-03-27 08:07:14,851] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 84.0, 1.0, 2.0, 0.7802758105508892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090515.280487867, 1090515.280487866, 239123.9517923286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3465000.0000, 
sim time next is 3465600.0000, 
raw observation next is [26.66666666666667, 82.33333333333334, 1.0, 2.0, 0.7623205926511752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1065408.437642088, 1065408.437642088, 234869.0653913397], 
processed observation next is [1.0, 0.08695652173913043, 0.4628751974723541, 0.8233333333333335, 1.0, 1.0, 0.713639268254428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2959467882339133, 0.2959467882339133, 0.3505508438676712], 
reward next is 0.6494, 
noisyNet noise sample is [array([-0.9303321], dtype=float32), 1.7008444]. 
=============================================
[2019-03-27 08:07:16,925] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8861279e-13 1.0000000e+00 5.3144402e-18 2.1061656e-08 2.2408494e-21], sum to 1.0000
[2019-03-27 08:07:16,934] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4759
[2019-03-27 08:07:16,946] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.6895956485255197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 963722.9692951301, 963722.9692951301, 218612.0964300838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3474600.0000, 
sim time next is 3475200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6215309164514117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868562.3220550605, 868562.3220550605, 204816.1581560592], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5440131523510984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24126731168196125, 0.24126731168196125, 0.3056957584418794], 
reward next is 0.6943, 
noisyNet noise sample is [array([1.9857565], dtype=float32), 1.3782036]. 
=============================================
[2019-03-27 08:07:20,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1415834e-14 1.0000000e+00 7.0836355e-21 1.2198296e-08 3.8219724e-23], sum to 1.0000
[2019-03-27 08:07:20,038] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6847
[2019-03-27 08:07:20,044] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.58333333333334, 80.66666666666666, 1.0, 2.0, 0.6873813027547047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 960626.9795823063, 960626.9795823058, 218141.3270338594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3556200.0000, 
sim time next is 3556800.0000, 
raw observation next is [26.5, 81.0, 1.0, 2.0, 0.6625600249346988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 925923.7032223347, 925923.7032223353, 212967.6112693294], 
processed observation next is [1.0, 0.17391304347826086, 0.4549763033175356, 0.81, 1.0, 1.0, 0.5934458131743359, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25720102867287076, 0.2572010286728709, 0.31786210637213347], 
reward next is 0.6821, 
noisyNet noise sample is [array([-0.51397145], dtype=float32), -1.7554606]. 
=============================================
[2019-03-27 08:07:31,412] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5977055e-15 1.0000000e+00 1.4889293e-20 3.9759888e-12 3.7847992e-24], sum to 1.0000
[2019-03-27 08:07:31,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5653
[2019-03-27 08:07:31,425] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.6258347803429243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 894527.8765292667, 894527.8765292667, 208245.7381931925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3726000.0000, 
sim time next is 3726600.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.7964846343115369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1138471.644683996, 1138471.644683996, 246585.3304880439], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.74, 1.0, 1.0, 0.7548007642307674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3162421235233322, 0.3162421235233322, 0.368037806698573], 
reward next is 0.6320, 
noisyNet noise sample is [array([-1.0987012], dtype=float32), -0.12760997]. 
=============================================
[2019-03-27 08:07:33,016] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 08:07:33,020] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:07:33,020] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:07:33,021] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:07:33,022] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:07:33,022] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:07:33,023] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:07:33,023] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:07:33,024] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:07:33,024] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:07:33,024] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:07:33,042] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run73
[2019-03-27 08:07:33,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run73
[2019-03-27 08:07:33,086] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run73
[2019-03-27 08:07:33,110] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run73
[2019-03-27 08:07:33,133] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run73
[2019-03-27 08:07:44,239] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:07:44,240] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.5, 58.66666666666667, 1.0, 2.0, 0.2489671835385688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 410596.8143992401, 410596.8143992407, 160775.5189588711]
[2019-03-27 08:07:44,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:07:44,246] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.26887595e-17 1.00000000e+00 1.15150208e-23 8.58414688e-16
 6.94485639e-27], sampled 0.16548547650766343
[2019-03-27 08:07:49,604] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:07:49,604] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 84.0, 1.0, 2.0, 0.5164311119203919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821012.0469414337, 821012.0469414337, 197449.7681642453]
[2019-03-27 08:07:49,604] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:07:49,608] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.45384710e-17 1.00000000e+00 1.09790846e-22 4.50682260e-15
 1.03338856e-25], sampled 0.6989702468104672
[2019-03-27 08:07:50,003] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:07:50,003] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.98109357666667, 73.80573751, 1.0, 2.0, 0.4398263616498964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 654539.4061753934, 654539.4061753941, 179723.4428733692]
[2019-03-27 08:07:50,004] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:07:50,008] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.0589043e-17 1.0000000e+00 7.0664633e-23 5.7949378e-15 4.9204179e-26], sampled 0.35500690865309537
[2019-03-27 08:07:59,720] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:07:59,723] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.24489493, 91.77121524, 1.0, 2.0, 0.6308292174011314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 911178.7171600376, 911178.7171600376, 210460.5218829951]
[2019-03-27 08:07:59,723] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:07:59,725] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.9224915e-16 1.0000000e+00 2.4715066e-21 1.6396294e-12 1.3210846e-24], sampled 0.767380299523519
[2019-03-27 08:07:59,851] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:07:59,853] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.00678012333334, 85.20054796666666, 1.0, 2.0, 0.5889740555092904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 823047.9022466664, 823047.9022466657, 198704.4363819308]
[2019-03-27 08:07:59,854] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:07:59,856] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0773993e-16 1.0000000e+00 4.5686891e-22 1.0912993e-13 2.3183180e-25], sampled 0.3058706186803394
[2019-03-27 08:08:01,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:08:01,077] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.2, 94.0, 1.0, 2.0, 0.3726846438316697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 570560.0493229558, 570560.0493229564, 172307.3993872528]
[2019-03-27 08:08:01,079] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:08:01,081] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.8722621e-17 1.0000000e+00 5.1467942e-23 1.6478913e-15 5.3257633e-26], sampled 0.39980961685752425
[2019-03-27 08:08:03,460] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:08:03,461] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.43930313666667, 98.70483759999999, 1.0, 2.0, 0.468140590337933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669543.9603860106, 669543.9603860112, 180727.4953107543]
[2019-03-27 08:08:03,462] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:08:03,463] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.8513314e-17 1.0000000e+00 7.7150029e-23 6.4565870e-15 5.7748458e-26], sampled 0.6760975896654147
[2019-03-27 08:08:09,197] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:08:09,199] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.55, 92.5, 1.0, 2.0, 0.3731910472357249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 568890.0621730059, 568890.0621730053, 172094.8365469422]
[2019-03-27 08:08:09,200] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:08:09,204] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.7765640e-17 1.0000000e+00 9.5570776e-23 9.6706954e-15 8.0060267e-26], sampled 0.8972116112602146
[2019-03-27 08:08:25,104] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:08:25,106] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.23333333333333, 85.0, 1.0, 2.0, 0.6677580011761612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 933191.0356027924, 933191.0356027924, 214036.6866403613]
[2019-03-27 08:08:25,107] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:08:25,110] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.1671240e-16 1.0000000e+00 6.7439762e-22 1.7883001e-13 4.3140229e-25], sampled 0.294220390572159
[2019-03-27 08:08:47,961] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:08:47,961] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.62594567333333, 80.72025701666666, 1.0, 2.0, 0.5057995190432399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706778.9387729893, 706778.9387729886, 184510.2859581395]
[2019-03-27 08:08:47,963] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:08:47,966] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.29167856e-17 1.00000000e+00 2.75464927e-23 1.21195016e-14
 1.23448912e-26], sampled 0.9119349561117028
[2019-03-27 08:08:56,344] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:08:56,345] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.73333333333333, 91.33333333333334, 1.0, 2.0, 0.6209408161322828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867737.345754395, 867737.345754395, 204711.7069329934]
[2019-03-27 08:08:56,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:08:56,351] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5837213e-16 1.0000000e+00 2.8937513e-22 7.5615062e-13 6.1204297e-26], sampled 0.14506238982569508
[2019-03-27 08:09:11,049] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07202384], dtype=float32), 0.047319166]
[2019-03-27 08:09:11,050] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.53333333333333, 94.83333333333333, 1.0, 2.0, 0.477320109831466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666970.744183424, 666970.744183424, 180118.5082997673]
[2019-03-27 08:09:11,051] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:09:11,052] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.5617744e-17 1.0000000e+00 1.4295575e-22 1.4805147e-14 1.2042885e-25], sampled 0.34857579672393413
[2019-03-27 08:09:28,219] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8501.5851 2841709574.2835 1114.0000
[2019-03-27 08:09:29,397] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.1653 2779218981.6139 933.0000
[2019-03-27 08:09:29,443] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8009.0201 3006046127.7445 1721.0000
[2019-03-27 08:09:29,565] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7915.5702 3161083383.4303 1685.0000
[2019-03-27 08:09:29,680] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8261.1117 2926781385.7040 1317.0000
[2019-03-27 08:09:30,696] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1800000, evaluation results [1800000.0, 7915.570182186232, 3161083383.4302926, 1685.0, 8261.111715515526, 2926781385.703963, 1317.0, 8660.16533764393, 2779218981.6139445, 933.0, 8009.020057868383, 3006046127.744453, 1721.0, 8501.585138295013, 2841709574.283492, 1114.0]
[2019-03-27 08:09:31,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3882060e-16 1.0000000e+00 1.5105711e-21 2.3001340e-12 6.3340372e-26], sum to 1.0000
[2019-03-27 08:09:31,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9668
[2019-03-27 08:09:31,803] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5177257671430998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723449.7560257459, 723449.7560257465, 186419.9822884864], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3806400.0000, 
sim time next is 3807000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5175745588374312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723238.3915178634, 723238.391517864, 186395.5096457264], 
processed observation next is [0.0, 0.043478260869565216, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4187645287197966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2008995531994065, 0.20089955319940667, 0.2782022532025767], 
reward next is 0.7218, 
noisyNet noise sample is [array([0.14740841], dtype=float32), -0.78944427]. 
=============================================
[2019-03-27 08:09:31,832] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.93303 ]
 [71.84147 ]
 [71.46124 ]
 [71.107445]
 [71.00952 ]], R is [[71.96082306]
 [71.96298218]
 [71.96516418]
 [71.96736145]
 [71.96948242]].
[2019-03-27 08:09:44,336] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.5350163e-11 3.6142880e-01 2.3694344e-13 6.3857114e-01 4.0877789e-18], sum to 1.0000
[2019-03-27 08:09:44,343] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9480
[2019-03-27 08:09:44,346] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.5, 61.5, 1.0, 2.0, 0.9130255417989233, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005988545041504, 6.9112, 168.912393157207, 2173268.813314461, 2106022.78136969, 437688.4368587321], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4019400.0000, 
sim time next is 4020000.0000, 
raw observation next is [33.66666666666666, 61.0, 1.0, 2.0, 0.8737782394839632, 1.0, 1.0, 0.8737782394839632, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2443880.823558873, 2443880.823558873, 457394.1009804247], 
processed observation next is [1.0, 0.5217391304347826, 0.7946287519747232, 0.61, 1.0, 1.0, 0.8479255897397147, 1.0, 0.5, 0.8479255897397147, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6788557843219092, 0.6788557843219092, 0.6826777626573503], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.75018144], dtype=float32), -0.2992212]. 
=============================================
[2019-03-27 08:09:44,369] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[51.596546]
 [51.176033]
 [49.149548]
 [48.55579 ]
 [48.05338 ]], R is [[51.48607635]
 [50.97121429]
 [50.82369995]
 [50.52182388]
 [50.2351532 ]].
[2019-03-27 08:10:01,792] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1853072e-15 1.0000000e+00 2.1743574e-23 2.5414375e-11 2.4526142e-25], sum to 1.0000
[2019-03-27 08:10:01,803] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6041
[2019-03-27 08:10:01,814] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.624745264154756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873056.0788470854, 873056.0788470854, 205446.8751922994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4318800.0000, 
sim time next is 4319400.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.624147732083174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872220.7092239243, 872220.7092239243, 205331.2684898376], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5471659422688843, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.242283530339979, 0.242283530339979, 0.3064645798355785], 
reward next is 0.6935, 
noisyNet noise sample is [array([1.102155], dtype=float32), -0.8435323]. 
=============================================
[2019-03-27 08:10:04,269] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3870766e-12 1.6421573e-03 6.8251184e-14 9.9835783e-01 2.9743497e-19], sum to 1.0000
[2019-03-27 08:10:04,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6183
[2019-03-27 08:10:04,282] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.83333333333334, 71.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.357909779379615, 6.9112, 170.5573041426782, 3229698.860286216, 2909702.474324554, 551261.8454931508], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4369800.0000, 
sim time next is 4370400.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.477669468583869, 6.9112, 170.5573041426782, 3315587.532713015, 2909802.409861654, 550570.6473200466], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.75, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.056646946858386914, 0.0, 0.8375144448122397, 0.9209965368647264, 0.8082784471837928, 0.8217472348060396], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7184607], dtype=float32), 1.5348499]. 
=============================================
[2019-03-27 08:10:04,703] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7606642e-11 4.5893557e-02 3.5760403e-14 9.5410645e-01 2.9336516e-19], sum to 1.0000
[2019-03-27 08:10:04,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2738
[2019-03-27 08:10:04,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3437538.05604217 W.
[2019-03-27 08:10:04,724] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.66666666666666, 56.83333333333333, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.64771232691984, 6.9112, 170.5573041426782, 3437538.05604217, 2909944.316863515, 549551.9519683208], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4377000.0000, 
sim time next is 4377600.0000, 
raw observation next is [36.0, 57.0, 1.0, 2.0, 1.0250403145866, 1.0, 2.0, 0.8331101968075625, 1.0, 1.0, 1.03, 7.00512337529045, 6.9112, 170.5573041426782, 3496675.409147953, 3429394.274868769, 643681.2627548765], 
processed observation next is [1.0, 0.6956521739130435, 0.9052132701421801, 0.57, 1.0, 1.0, 1.0301690537187953, 1.0, 1.0, 0.7989279479609186, 1.0, 0.5, 1.0365853658536586, 0.009392337529044958, 0.0, 0.8375144448122397, 0.9712987247633204, 0.9526095207968803, 0.9607183026192186], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.111473], dtype=float32), -1.044591]. 
=============================================
[2019-03-27 08:10:07,069] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4796674e-17 1.0000000e+00 9.3841136e-21 2.0179646e-11 1.8061555e-25], sum to 1.0000
[2019-03-27 08:10:07,079] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9661
[2019-03-27 08:10:07,085] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6186703196537894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864563.1358813802, 864563.1358813802, 204275.9750784005], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4409400.0000, 
sim time next is 4410000.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.617992481989888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863615.5040085978, 863615.5040085978, 204146.0588657027], 
processed observation next is [0.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5397499783010699, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23989319555794383, 0.23989319555794383, 0.3046956102473175], 
reward next is 0.6953, 
noisyNet noise sample is [array([0.17368259], dtype=float32), -0.67089105]. 
=============================================
[2019-03-27 08:10:07,103] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.28262 ]
 [70.209076]
 [69.91877 ]
 [69.59468 ]
 [69.42792 ]], R is [[70.61951447]
 [70.60843658]
 [70.59752655]
 [70.58712769]
 [70.57704926]].
[2019-03-27 08:10:10,760] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3990820e-18 1.0000000e+00 1.1680572e-23 1.3294373e-15 6.9313545e-28], sum to 1.0000
[2019-03-27 08:10:10,767] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0855
[2019-03-27 08:10:10,772] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5162255930472541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721352.7595897908, 721352.7595897908, 186176.7387330674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4490400.0000, 
sim time next is 4491000.0000, 
raw observation next is [26.5, 86.5, 1.0, 2.0, 0.5142657200147461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718613.1859502372, 718613.1859502365, 185860.829198575], 
processed observation next is [0.0, 1.0, 0.4549763033175356, 0.865, 1.0, 1.0, 0.4147779759213808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1996147738750659, 0.1996147738750657, 0.2774042226844403], 
reward next is 0.7226, 
noisyNet noise sample is [array([0.29922926], dtype=float32), 0.6804607]. 
=============================================
[2019-03-27 08:10:10,784] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.308235]
 [75.27881 ]
 [75.2289  ]
 [75.150734]
 [75.133446]], R is [[75.30941772]
 [75.27845001]
 [75.24728394]
 [75.21572113]
 [75.18315887]].
[2019-03-27 08:10:10,885] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.05634668e-17 1.00000000e+00 3.55193994e-23 1.19512385e-14
 5.19110959e-27], sum to 1.0000
[2019-03-27 08:10:10,893] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8429
[2019-03-27 08:10:10,897] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 70.16666666666667, 1.0, 2.0, 0.5812358386290245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 812230.1789829684, 812230.1789829691, 197303.0774052033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4463400.0000, 
sim time next is 4464000.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.5955079195960712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832182.0733987939, 832182.0733987939, 199913.1216876702], 
processed observation next is [0.0, 0.6956521739130435, 0.7156398104265403, 0.71, 1.0, 1.0, 0.5126601440916521, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23116168705522053, 0.23116168705522053, 0.2983777935636869], 
reward next is 0.7016, 
noisyNet noise sample is [array([0.74397975], dtype=float32), 0.81757045]. 
=============================================
[2019-03-27 08:10:10,908] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.82108 ]
 [73.882774]
 [73.92122 ]
 [73.93339 ]
 [73.918785]], R is [[73.78774261]
 [73.75538635]
 [73.72708893]
 [73.70265198]
 [73.68167114]].
[2019-03-27 08:10:15,383] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4212031e-18 1.0000000e+00 1.6702742e-25 2.4379432e-16 9.3390481e-28], sum to 1.0000
[2019-03-27 08:10:15,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0084
[2019-03-27 08:10:15,399] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.529300894799023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739630.018258668, 739630.0182586674, 188313.6533800974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4563600.0000, 
sim time next is 4564200.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5270184166459418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736439.4425721312, 736439.4425721312, 187937.0538408328], 
processed observation next is [0.0, 0.8260869565217391, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.43014267065776124, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.204566511825592, 0.204566511825592, 0.2805030654340788], 
reward next is 0.7195, 
noisyNet noise sample is [array([3.3892152], dtype=float32), -1.0936852]. 
=============================================
[2019-03-27 08:10:22,543] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2868323e-15 1.0000000e+00 2.2689089e-20 5.3609422e-12 1.5101663e-23], sum to 1.0000
[2019-03-27 08:10:22,552] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1003
[2019-03-27 08:10:22,557] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7798236279340122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1089882.984695318, 1089882.984695318, 239018.6264797775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4681800.0000, 
sim time next is 4682400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7674757770387476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1072616.888923591, 1072616.888923591, 236084.0565103992], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.7198503337816236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29794913581210863, 0.29794913581210863, 0.35236426344835703], 
reward next is 0.6476, 
noisyNet noise sample is [array([-1.6853213], dtype=float32), 0.3968208]. 
=============================================
[2019-03-27 08:10:23,739] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2592474e-10 1.8667008e-01 2.9565668e-12 8.1332999e-01 1.9168519e-17], sum to 1.0000
[2019-03-27 08:10:23,747] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9338
[2019-03-27 08:10:23,752] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7871960280479375, 1.0, 2.0, 0.7871960280479375, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2201503.064273644, 2201503.064273643, 413683.989042822], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4709400.0000, 
sim time next is 4710000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5972852855578793, 1.0, 2.0, 0.5972852855578793, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1669977.36036106, 1669977.36036106, 333252.3150783342], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.5148015488649148, 1.0, 1.0, 0.5148015488649148, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4638826001002944, 0.4638826001002944, 0.49739151504228984], 
reward next is 0.5026, 
noisyNet noise sample is [array([0.06687944], dtype=float32), 0.01928955]. 
=============================================
[2019-03-27 08:10:23,767] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[53.35909 ]
 [52.96475 ]
 [52.53044 ]
 [51.76836 ]
 [51.374283]], R is [[55.12724686]
 [54.95853424]
 [54.82499313]
 [54.69218826]
 [54.14526749]].
[2019-03-27 08:10:24,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0284617e-16 1.0000000e+00 1.6350614e-22 6.3902294e-16 6.3509772e-25], sum to 1.0000
[2019-03-27 08:10:24,228] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4155
[2019-03-27 08:10:24,237] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4964281780333137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 693679.6201163708, 693679.6201163714, 183038.5277597571], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5029800.0000, 
sim time next is 5030400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4974896336573949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695163.3212670541, 695163.3212670541, 183203.817596582], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.3945658236836084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19310092257418168, 0.19310092257418168, 0.2734385337262418], 
reward next is 0.7266, 
noisyNet noise sample is [array([-1.1990262], dtype=float32), 0.24951907]. 
=============================================
[2019-03-27 08:10:24,261] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 08:10:24,263] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:10:24,264] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:10:24,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:10:24,265] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:10:24,266] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:10:24,265] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:10:24,267] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:10:24,267] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:10:24,270] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:10:24,267] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:10:24,286] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run74
[2019-03-27 08:10:24,287] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run74
[2019-03-27 08:10:24,332] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run74
[2019-03-27 08:10:24,357] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run74
[2019-03-27 08:10:24,380] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run74
[2019-03-27 08:11:23,747] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07424952], dtype=float32), 0.04699331]
[2019-03-27 08:11:23,748] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.40000000000001, 54.0, 1.0, 2.0, 0.7232981556311453, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.987911723825651, 6.9112, 168.9124380862216, 1907731.662145639, 1853309.892080652, 390452.5410672174]
[2019-03-27 08:11:23,749] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:11:23,751] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2251501e-11 9.9998140e-01 1.0391103e-15 1.8652565e-05 7.0834573e-19], sampled 0.4728626047157459
[2019-03-27 08:11:23,752] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1907731.662145639 W.
[2019-03-27 08:11:35,474] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07424952], dtype=float32), 0.04699331]
[2019-03-27 08:11:35,476] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.68480934833334, 84.41629099833334, 1.0, 2.0, 0.8862048540143966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1238648.309155172, 1238648.309155173, 266176.5515038724]
[2019-03-27 08:11:35,479] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:11:35,481] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.0143519e-16 1.0000000e+00 1.4119227e-21 5.6548006e-13 1.1972579e-24], sampled 0.3681962008474784
[2019-03-27 08:11:39,383] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07424952], dtype=float32), 0.04699331]
[2019-03-27 08:11:39,384] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.89675476666667, 82.67202751333332, 1.0, 2.0, 0.5151798643881561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719891.0055148106, 719891.0055148099, 186007.6144742721]
[2019-03-27 08:11:39,385] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:11:39,389] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9283047e-17 1.0000000e+00 4.4829189e-23 1.1892747e-15 5.2766031e-26], sampled 0.24316101696179115
[2019-03-27 08:11:45,805] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07424952], dtype=float32), 0.04699331]
[2019-03-27 08:11:45,806] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.260586885, 70.98576683, 1.0, 2.0, 0.9662350081924123, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991552542161, 6.9112, 168.9123159415399, 2247742.688611558, 2180494.553790877, 453114.6494149651]
[2019-03-27 08:11:45,807] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:11:45,809] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.7781981e-11 9.9992144e-01 4.4592875e-15 7.8506127e-05 2.5705268e-18], sampled 0.475901169701173
[2019-03-27 08:11:45,811] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2247742.688611558 W.
[2019-03-27 08:12:15,757] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07424952], dtype=float32), 0.04699331]
[2019-03-27 08:12:15,758] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.09988257666667, 90.65140761666667, 1.0, 2.0, 0.3444233843083704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535134.6091489093, 535134.6091489099, 169548.0431914947]
[2019-03-27 08:12:15,760] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:12:15,764] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3338756e-17 1.0000000e+00 1.6473754e-23 2.0508327e-16 2.2091429e-26], sampled 0.9036118006494502
[2019-03-27 08:12:19,984] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7899.1850 3162581864.6341 1739.0000
[2019-03-27 08:12:20,363] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9878 2779190307.3385 933.0000
[2019-03-27 08:12:20,538] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.3603 2927072149.8624 1328.0000
[2019-03-27 08:12:20,700] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.2231 3007234716.7147 1760.0000
[2019-03-27 08:12:20,791] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.6789 2842464344.5060 1124.0000
[2019-03-27 08:12:21,812] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1825000, evaluation results [1825000.0, 7899.184951539714, 3162581864.634082, 1739.0, 8259.360298660742, 2927072149.8624206, 1328.0, 8659.987755389953, 2779190307.3384666, 933.0, 8000.223138099288, 3007234716.7146506, 1760.0, 8499.67889956988, 2842464344.505979, 1124.0]
[2019-03-27 08:12:23,038] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.2156109e-16 1.0000000e+00 2.7282867e-22 6.8595225e-12 4.1135339e-25], sum to 1.0000
[2019-03-27 08:12:23,046] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1416
[2019-03-27 08:12:23,050] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5156445368819844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720540.5398331, 720540.5398331007, 186083.7356781453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4741200.0000, 
sim time next is 4741800.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.5152865481673516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720040.1315345133, 720040.131534514, 186026.0277832874], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.41600788935825495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2000111476484759, 0.2000111476484761, 0.27765078773624985], 
reward next is 0.7223, 
noisyNet noise sample is [array([0.9184543], dtype=float32), -0.35058585]. 
=============================================
[2019-03-27 08:12:30,806] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9219665e-09 9.8527038e-01 2.5610634e-14 1.4729611e-02 4.4718820e-17], sum to 1.0000
[2019-03-27 08:12:30,814] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2991
[2019-03-27 08:12:30,824] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2168848.062229631 W.
[2019-03-27 08:12:30,832] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.9098672866749888, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.987916200705976, 6.9112, 168.9124997946566, 2168848.062229631, 2114423.096238452, 437471.5677229593], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4873800.0000, 
sim time next is 4874400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.7343396887599756, 1.0, 1.0, 0.7343396887599756, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2053541.27691964, 2053541.27691964, 389169.2867075477], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.7, 1.0, 1.0, 0.6799273358553922, 1.0, 0.5, 0.6799273358553922, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5704281324776778, 0.5704281324776778, 0.5808496816530563], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1990519], dtype=float32), -0.2320343]. 
=============================================
[2019-03-27 08:12:33,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4549546e-10 6.3064891e-01 3.5273366e-13 3.6935106e-01 1.9286742e-17], sum to 1.0000
[2019-03-27 08:12:33,287] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5329
[2019-03-27 08:12:33,295] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2598283.094562865 W.
[2019-03-27 08:12:33,300] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.92892545249342, 1.0, 2.0, 0.92892545249342, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2598283.094562865, 2598283.094562865, 487520.2237186099], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5238000.0000, 
sim time next is 5238600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.638782177534392, 1.0, 2.0, 0.638782177534392, 1.0, 1.0, 1.03, 7.000411038510226, 6.9112, 170.5573041426782, 2680179.453116419, 2616273.957358514, 502468.5810978932], 
processed observation next is [1.0, 0.6521739130434783, 0.7156398104265403, 0.67, 1.0, 1.0, 0.5647978042583036, 1.0, 1.0, 0.5647978042583036, 1.0, 0.5, 1.0365853658536586, 0.008921103851022582, 0.0, 0.8375144448122397, 0.7444942925323386, 0.7267427659329205, 0.7499531061162585], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.22350425], dtype=float32), -0.93064374]. 
=============================================
[2019-03-27 08:12:34,193] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2639297e-16 1.0000000e+00 2.2820224e-22 2.0538540e-13 1.8313119e-25], sum to 1.0000
[2019-03-27 08:12:34,201] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6702
[2019-03-27 08:12:34,206] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.7022977460847524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 981482.5938460127, 981482.5938460134, 221339.6337884504], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4936800.0000, 
sim time next is 4937400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.7105480137788664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 993017.9860857257, 993017.9860857251, 223136.1282060435], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.6512626672034535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27583832946825715, 0.275838329468257, 0.333038997322453], 
reward next is 0.6670, 
noisyNet noise sample is [array([1.6729801], dtype=float32), -0.67300797]. 
=============================================
[2019-03-27 08:12:34,243] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2744984e-17 1.0000000e+00 4.9385501e-24 1.0766069e-15 4.5436844e-27], sum to 1.0000
[2019-03-27 08:12:34,254] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2448
[2019-03-27 08:12:34,258] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4808737448191366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672062.9188923375, 672062.9188923368, 180669.213913349], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5110800.0000, 
sim time next is 5111400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.482364311720772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674146.5682088706, 674146.5682088699, 180894.5739038551], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.84, 1.0, 1.0, 0.37634254424189395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18726293561357515, 0.18726293561357496, 0.2699919013490375], 
reward next is 0.7300, 
noisyNet noise sample is [array([-1.5473285], dtype=float32), -1.4198947]. 
=============================================
[2019-03-27 08:12:45,671] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1965163e-18 1.0000000e+00 4.4070248e-23 4.8831015e-16 5.0658912e-26], sum to 1.0000
[2019-03-27 08:12:45,678] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2260
[2019-03-27 08:12:45,686] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4817170333074691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673240.9676341783, 673240.9676341789, 180796.5302469205], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5110200.0000, 
sim time next is 5110800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4808737448191366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672062.9188923375, 672062.9188923368, 180669.213913349], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3745466805049838, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1866841441367604, 0.1866841441367602, 0.2696555431542522], 
reward next is 0.7303, 
noisyNet noise sample is [array([-1.312143], dtype=float32), -0.3553776]. 
=============================================
[2019-03-27 08:12:48,596] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.4758894e-10 7.2860271e-01 5.9014012e-14 2.7139726e-01 1.6115109e-18], sum to 1.0000
[2019-03-27 08:12:48,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6752
[2019-03-27 08:12:48,613] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2249889.436073042 W.
[2019-03-27 08:12:48,617] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.8044814878339489, 1.0, 2.0, 0.8044814878339489, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2249889.436073042, 2249889.436073042, 422064.2843439923], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5235600.0000, 
sim time next is 5236200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.9482641868946028, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005990546891507, 6.9112, 168.9123931453095, 2222590.472349328, 2155343.02023274, 447782.2813852729], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.67, 1.0, 1.0, 0.9376676950537383, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00947905468915069, 0.0, 0.8294371787716465, 0.6173862423192578, 0.5987063945090944, 0.6683317632616014], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.9969858], dtype=float32), -1.0127391]. 
=============================================
[2019-03-27 08:12:54,490] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.2425665e-15 1.0000000e+00 2.5388956e-21 9.2432388e-11 1.4330941e-24], sum to 1.0000
[2019-03-27 08:12:54,499] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5119
[2019-03-27 08:12:54,504] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 86.16666666666667, 1.0, 2.0, 0.5740438784564088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 802176.1999237338, 802176.1999237344, 196010.3177817738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5274600.0000, 
sim time next is 5275200.0000, 
raw observation next is [28.6, 86.33333333333334, 1.0, 2.0, 0.5755005646675427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804212.5623390183, 804212.5623390183, 196270.7866972732], 
processed observation next is [1.0, 0.043478260869565216, 0.5545023696682465, 0.8633333333333334, 1.0, 1.0, 0.4885548971898105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2233923784275051, 0.2233923784275051, 0.2929414726824973], 
reward next is 0.7071, 
noisyNet noise sample is [array([-1.1626778], dtype=float32), 0.19762151]. 
=============================================
[2019-03-27 08:12:56,230] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3763455e-14 1.0000000e+00 1.6914681e-22 3.8001577e-12 8.8328386e-26], sum to 1.0000
[2019-03-27 08:12:56,235] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7364
[2019-03-27 08:12:56,240] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.15, 82.5, 1.0, 2.0, 0.615268035246576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 859806.676429168, 859806.6764291673, 203625.032445763], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5351400.0000, 
sim time next is 5352000.0000, 
raw observation next is [30.06666666666666, 83.0, 1.0, 2.0, 0.6145502430742251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 858803.1918004336, 858803.191800433, 203488.1768498952], 
processed observation next is [1.0, 0.9565217391304348, 0.6240126382306473, 0.83, 1.0, 1.0, 0.5356027024990664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2385564421667871, 0.23855644216678693, 0.30371369679088833], 
reward next is 0.6963, 
noisyNet noise sample is [array([1.3170666], dtype=float32), -0.08842694]. 
=============================================
[2019-03-27 08:12:56,269] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[67.17421 ]
 [67.19849 ]
 [67.09167 ]
 [67.29761 ]
 [67.279915]], R is [[67.17617798]
 [67.20050049]
 [67.22437286]
 [67.2481842 ]
 [67.27223206]].
[2019-03-27 08:13:06,997] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.7179400e-12 3.7420623e-02 1.1935619e-14 9.6257937e-01 4.8307384e-19], sum to 1.0000
[2019-03-27 08:13:07,008] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4136
[2019-03-27 08:13:07,012] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.65, 49.0, 1.0, 2.0, 0.9994210840900284, 1.0, 2.0, 0.9994210840900284, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2795685.965357002, 2795685.965357002, 528639.0776236602], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5491800.0000, 
sim time next is 5492400.0000, 
raw observation next is [36.76666666666667, 48.0, 1.0, 2.0, 1.007495861515072, 1.0, 2.0, 1.007495861515072, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2818299.063193251, 2818299.06319325, 533533.9739737515], 
processed observation next is [1.0, 0.5652173913043478, 0.9415481832543446, 0.48, 1.0, 1.0, 1.0090311584518938, 1.0, 1.0, 1.0090311584518938, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7828608508870142, 0.7828608508870138, 0.7963193641399277], 
reward next is 0.2037, 
noisyNet noise sample is [array([0.75743747], dtype=float32), -0.45015937]. 
=============================================
[2019-03-27 08:13:15,207] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 08:13:15,208] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:13:15,209] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:13:15,209] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:13:15,210] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:13:15,211] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:13:15,212] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:13:15,213] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:13:15,212] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:13:15,216] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:13:15,218] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:13:15,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run75
[2019-03-27 08:13:15,261] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run75
[2019-03-27 08:13:15,261] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run75
[2019-03-27 08:13:15,300] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run75
[2019-03-27 08:13:15,301] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run75
[2019-03-27 08:13:51,066] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07458458], dtype=float32), 0.047704704]
[2019-03-27 08:13:51,069] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.48333333333333, 75.66666666666667, 1.0, 2.0, 0.5849447147882951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817415.0284312845, 817415.0284312845, 197974.7118572583]
[2019-03-27 08:13:51,072] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:13:51,076] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7298092e-18 1.0000000e+00 3.9714926e-24 1.8910400e-15 1.0132376e-27], sampled 0.16215116578670474
[2019-03-27 08:13:51,874] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07458458], dtype=float32), 0.047704704]
[2019-03-27 08:13:51,875] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.45, 83.5, 1.0, 2.0, 0.4556311104113762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 653633.1843704322, 653633.1843704316, 179113.0291219978]
[2019-03-27 08:13:51,877] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:13:51,880] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.300542e-18 1.000000e+00 4.765382e-24 4.590698e-16 2.325499e-27], sampled 0.2967864238695287
[2019-03-27 08:14:03,053] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07458458], dtype=float32), 0.047704704]
[2019-03-27 08:14:03,059] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.13333333333333, 67.33333333333334, 1.0, 2.0, 0.5610765992587474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 784048.8715080783, 784048.8715080777, 193718.2594112341]
[2019-03-27 08:14:03,060] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:14:03,062] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.7506765e-18 1.0000000e+00 2.5911107e-24 1.4891312e-16 1.6546779e-27], sampled 0.18556027117850915
[2019-03-27 08:14:57,209] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07458458], dtype=float32), 0.047704704]
[2019-03-27 08:14:57,211] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.4, 61.0, 1.0, 2.0, 0.4492118888303161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 640416.241973249, 640416.2419732496, 177662.1532866675]
[2019-03-27 08:14:57,211] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:14:57,214] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8470199e-18 1.0000000e+00 9.1637061e-25 8.0755029e-18 9.2568677e-28], sampled 0.6129370082983235
[2019-03-27 08:15:11,571] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.3443 2927300617.9197 1337.0000
[2019-03-27 08:15:11,665] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.9257 2842635740.9591 1131.0000
[2019-03-27 08:15:11,685] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.4430 3007568936.5579 1768.0000
[2019-03-27 08:15:11,704] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.1852 3163992065.0998 1773.0000
[2019-03-27 08:15:11,749] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9878 2779190307.3385 933.0000
[2019-03-27 08:15:12,765] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1850000, evaluation results [1850000.0, 7885.185155142531, 3163992065.0997953, 1773.0, 8253.344317653808, 2927300617.919703, 1337.0, 8659.987755389953, 2779190307.3384666, 933.0, 7998.443036756619, 3007568936.55793, 1768.0, 8495.925739946044, 2842635740.959133, 1131.0]
[2019-03-27 08:15:12,882] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4021287e-19 1.0000000e+00 8.0777542e-25 1.5232241e-16 2.9514580e-28], sum to 1.0000
[2019-03-27 08:15:12,891] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0481
[2019-03-27 08:15:12,901] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 73.33333333333334, 1.0, 2.0, 0.5421571164561202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757601.3433202925, 757601.3433202932, 190464.949132379], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5649000.0000, 
sim time next is 5649600.0000, 
raw observation next is [29.83333333333334, 72.66666666666667, 1.0, 2.0, 0.541256075815571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756341.7958374814, 756341.795837482, 190312.8282231646], 
processed observation next is [0.0, 0.391304347826087, 0.6129541864139023, 0.7266666666666667, 1.0, 1.0, 0.4472964768862301, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21009494328818928, 0.21009494328818942, 0.28404899734800687], 
reward next is 0.7160, 
noisyNet noise sample is [array([0.5843677], dtype=float32), -1.1670384]. 
=============================================
[2019-03-27 08:15:17,894] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2267296e-18 1.0000000e+00 4.6011786e-25 8.8023373e-17 3.7478269e-27], sum to 1.0000
[2019-03-27 08:15:17,903] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6972
[2019-03-27 08:15:17,907] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.61666666666667, 81.0, 1.0, 2.0, 0.5182035140574606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724117.5684371626, 724117.5684371626, 186497.1799061705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5727000.0000, 
sim time next is 5727600.0000, 
raw observation next is [27.8, 80.0, 1.0, 2.0, 0.5186626171841852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724759.320264166, 724759.3202641654, 186571.6622047571], 
processed observation next is [0.0, 0.30434782608695654, 0.5165876777251186, 0.8, 1.0, 1.0, 0.4200754423905845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20132203340671279, 0.20132203340671262, 0.2784651674697867], 
reward next is 0.7215, 
noisyNet noise sample is [array([1.1555003], dtype=float32), 1.7626543]. 
=============================================
[2019-03-27 08:15:18,633] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3349846e-17 1.0000000e+00 3.9574734e-24 6.0528915e-17 8.8671348e-27], sum to 1.0000
[2019-03-27 08:15:18,644] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1754
[2019-03-27 08:15:18,653] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 72.0, 1.0, 2.0, 0.5227318440542547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730447.4574323051, 730447.4574323044, 187234.2666590692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5733000.0000, 
sim time next is 5733600.0000, 
raw observation next is [29.4, 71.0, 1.0, 2.0, 0.5216146103174774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728885.7378993308, 728885.7378993302, 187051.8852675358], 
processed observation next is [0.0, 0.34782608695652173, 0.5924170616113744, 0.71, 1.0, 1.0, 0.42363206062346664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2024682605275919, 0.20246826052759173, 0.2791819183097549], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.5991683], dtype=float32), -3.511977]. 
=============================================
[2019-03-27 08:15:19,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9368911e-16 1.0000000e+00 6.1530520e-22 1.4325235e-12 2.1901814e-24], sum to 1.0000
[2019-03-27 08:15:19,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3809
[2019-03-27 08:15:19,891] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 92.66666666666667, 1.0, 2.0, 0.655766349758343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 916425.4863003239, 916425.4863003232, 211586.4064488214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6496800.0000, 
sim time next is 6497400.0000, 
raw observation next is [26.11666666666667, 92.83333333333333, 1.0, 2.0, 0.6530760839191702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 912664.2571650674, 912664.257165068, 211042.0464341715], 
processed observation next is [1.0, 0.17391304347826086, 0.43680884676145365, 0.9283333333333332, 1.0, 1.0, 0.5820193782158677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2535178492125187, 0.25351784921251885, 0.31498812900622614], 
reward next is 0.6850, 
noisyNet noise sample is [array([1.9973395], dtype=float32), 1.2760919]. 
=============================================
[2019-03-27 08:15:21,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.4522511e-17 1.0000000e+00 4.9235634e-22 1.2555414e-13 8.9389348e-26], sum to 1.0000
[2019-03-27 08:15:21,609] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8978
[2019-03-27 08:15:21,614] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 88.33333333333334, 1.0, 2.0, 0.52873246730019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738835.4375962825, 738835.4375962825, 188220.286648948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6480600.0000, 
sim time next is 6481200.0000, 
raw observation next is [26.83333333333333, 88.66666666666667, 1.0, 2.0, 0.5291646017967019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739439.5000042234, 739439.5000042241, 188291.7101906741], 
processed observation next is [1.0, 0.0, 0.470774091627172, 0.8866666666666667, 1.0, 1.0, 0.4327284358996408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20539986111228428, 0.20539986111228448, 0.28103240326966283], 
reward next is 0.7190, 
noisyNet noise sample is [array([-1.2329482], dtype=float32), -0.8795519]. 
=============================================
[2019-03-27 08:15:21,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8844561e-17 1.0000000e+00 4.7308620e-23 1.0462989e-12 1.7729036e-26], sum to 1.0000
[2019-03-27 08:15:21,951] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2728
[2019-03-27 08:15:21,958] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.15, 86.5, 1.0, 2.0, 0.5655401374016147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790288.5435167112, 790288.5435167112, 194501.6358680622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5862600.0000, 
sim time next is 5863200.0000, 
raw observation next is [28.06666666666666, 86.66666666666667, 1.0, 2.0, 0.5626046578740379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786184.9720289028, 786184.9720289034, 193986.0249515066], 
processed observation next is [1.0, 0.8695652173913043, 0.5292259083728275, 0.8666666666666667, 1.0, 1.0, 0.47301766008920226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.218384714452473, 0.21838471445247318, 0.28953138052463673], 
reward next is 0.7105, 
noisyNet noise sample is [array([-0.5149281], dtype=float32), 0.34365377]. 
=============================================
[2019-03-27 08:15:27,507] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.7807765e-15 1.0000000e+00 6.5290380e-21 1.5668881e-10 1.9876827e-23], sum to 1.0000
[2019-03-27 08:15:27,513] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4634
[2019-03-27 08:15:27,520] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 88.5, 1.0, 2.0, 0.7604487179169431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1062791.022941659, 1062791.022941659, 234435.0638031898], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5898600.0000, 
sim time next is 5899200.0000, 
raw observation next is [27.6, 87.66666666666666, 1.0, 2.0, 0.7530764573281162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1052482.562953504, 1052482.562953503, 232720.0655230329], 
processed observation next is [1.0, 0.2608695652173913, 0.5071090047393366, 0.8766666666666666, 1.0, 1.0, 0.7025017558170075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29235626748708443, 0.2923562674870842, 0.3473433813776611], 
reward next is 0.6527, 
noisyNet noise sample is [array([-1.4807775], dtype=float32), 0.4431851]. 
=============================================
[2019-03-27 08:15:28,084] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1402592e-16 1.0000000e+00 5.1217081e-22 4.6371706e-13 3.5935979e-26], sum to 1.0000
[2019-03-27 08:15:28,093] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1920
[2019-03-27 08:15:28,098] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.68333333333334, 88.5, 1.0, 2.0, 0.5570933010978342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778480.5678235178, 778480.5678235178, 193024.8443718945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5953800.0000, 
sim time next is 5954400.0000, 
raw observation next is [27.6, 89.0, 1.0, 2.0, 0.5566925219880691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777920.3151722233, 777920.3151722233, 192955.2918158165], 
processed observation next is [1.0, 0.9565217391304348, 0.5071090047393366, 0.89, 1.0, 1.0, 0.46589460480490247, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21608897643672867, 0.21608897643672867, 0.28799297285942765], 
reward next is 0.7120, 
noisyNet noise sample is [array([0.6416783], dtype=float32), 1.501157]. 
=============================================
[2019-03-27 08:15:37,750] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1221635e-18 1.0000000e+00 7.7689817e-25 4.5501451e-16 1.3886574e-25], sum to 1.0000
[2019-03-27 08:15:37,759] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2673
[2019-03-27 08:15:37,766] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 76.5, 1.0, 2.0, 0.5417929320526041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757092.2567262761, 757092.2567262755, 190403.6895709978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6255000.0000, 
sim time next is 6255600.0000, 
raw observation next is [29.4, 75.66666666666666, 1.0, 2.0, 0.5417871942861617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757084.2360113957, 757084.2360113951, 190402.7467325003], 
processed observation next is [0.0, 0.391304347826087, 0.5924170616113744, 0.7566666666666666, 1.0, 1.0, 0.44793637865802616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21030117666983214, 0.21030117666983197, 0.2841832040783587], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.9514934], dtype=float32), -1.8522315]. 
=============================================
[2019-03-27 08:15:40,238] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5283877e-15 1.0000000e+00 7.9551069e-23 5.9968315e-11 9.1254029e-25], sum to 1.0000
[2019-03-27 08:15:40,243] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2505
[2019-03-27 08:15:40,247] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 86.0, 1.0, 2.0, 0.5259561978209946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734954.6169261284, 734954.616926129, 187762.9799009111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6120000.0000, 
sim time next is 6120600.0000, 
raw observation next is [27.2, 86.0, 1.0, 2.0, 0.5261270172167318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735193.3971707496, 735193.3971707496, 187791.0509652534], 
processed observation next is [1.0, 0.8695652173913043, 0.4881516587677725, 0.86, 1.0, 1.0, 0.42906869544184556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.204220388102986, 0.204220388102986, 0.28028515069440807], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.27787313], dtype=float32), -0.0435358]. 
=============================================
[2019-03-27 08:15:40,550] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4100605e-19 1.0000000e+00 3.7420734e-25 8.2136636e-18 1.3060452e-27], sum to 1.0000
[2019-03-27 08:15:40,557] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0527
[2019-03-27 08:15:40,562] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 59.0, 1.0, 2.0, 0.3515686018820105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542502.2869446904, 542502.286944691, 170049.3311366432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6861600.0000, 
sim time next is 6862200.0000, 
raw observation next is [27.4, 57.66666666666667, 1.0, 2.0, 0.3490455866151417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539549.9280122808, 539549.9280122814, 169833.1768647651], 
processed observation next is [0.0, 0.43478260869565216, 0.4976303317535545, 0.5766666666666667, 1.0, 1.0, 0.21571757423511045, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14987498000341135, 0.1498749800034115, 0.25348235352950016], 
reward next is 0.7465, 
noisyNet noise sample is [array([0.43317637], dtype=float32), -0.10917854]. 
=============================================
[2019-03-27 08:15:44,941] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3293019e-17 1.0000000e+00 1.1909588e-23 1.7608126e-14 1.7458491e-27], sum to 1.0000
[2019-03-27 08:15:44,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9675
[2019-03-27 08:15:44,957] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666666, 89.66666666666667, 1.0, 2.0, 0.5205903980642519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727454.0499943167, 727454.0499943167, 186885.3165544713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6223200.0000, 
sim time next is 6223800.0000, 
raw observation next is [26.53333333333333, 89.83333333333333, 1.0, 2.0, 0.520294911051758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727041.005907795, 727041.0059077957, 186837.2215072147], 
processed observation next is [0.0, 0.0, 0.45655608214849913, 0.8983333333333333, 1.0, 1.0, 0.4220420615081421, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20195583497438752, 0.2019558349743877, 0.2788615246376339], 
reward next is 0.7211, 
noisyNet noise sample is [array([0.55012393], dtype=float32), -1.4530928]. 
=============================================
[2019-03-27 08:15:56,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.71772352e-16 1.00000000e+00 1.24451974e-23 1.81483225e-15
 8.34730162e-27], sum to 1.0000
[2019-03-27 08:15:56,059] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0100
[2019-03-27 08:15:56,068] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 83.0, 1.0, 2.0, 0.5123525211006227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715938.8617114736, 715938.8617114729, 185553.8411422499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6395400.0000, 
sim time next is 6396000.0000, 
raw observation next is [27.1, 83.0, 1.0, 2.0, 0.5122285982526144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715765.6390373865, 715765.6390373865, 185533.9812585267], 
processed observation next is [1.0, 0.0, 0.4834123222748816, 0.83, 1.0, 1.0, 0.4123236123525474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19882378862149624, 0.19882378862149624, 0.2769163899380996], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.62908006], dtype=float32), -0.5875435]. 
=============================================
[2019-03-27 08:15:56,084] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.67717 ]
 [69.48511 ]
 [70.46872 ]
 [72.13233 ]
 [74.366974]], R is [[68.12934875]
 [68.17111206]
 [68.21242523]
 [68.2532959 ]
 [68.29375458]].
[2019-03-27 08:16:01,670] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0462147e-14 1.0000000e+00 1.1650386e-18 1.1256769e-09 1.0270198e-22], sum to 1.0000
[2019-03-27 08:16:01,677] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3530
[2019-03-27 08:16:01,681] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.06666666666667, 68.0, 1.0, 2.0, 0.4845150334527664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 677027.5858352553, 677027.585835256, 181205.7253695938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6546000.0000, 
sim time next is 6546600.0000, 
raw observation next is [28.95, 68.5, 1.0, 2.0, 0.4841375757129702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676499.9847174002, 676499.9847174002, 181148.3032504882], 
processed observation next is [1.0, 0.782608695652174, 0.5710900473933649, 0.685, 1.0, 1.0, 0.3784790068830966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18791666242150004, 0.18791666242150004, 0.27037060186640033], 
reward next is 0.7296, 
noisyNet noise sample is [array([0.51669854], dtype=float32), 0.71756166]. 
=============================================
[2019-03-27 08:16:02,407] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.43562387e-17 1.00000000e+00 1.60780808e-24 2.33350326e-14
 1.25594976e-26], sum to 1.0000
[2019-03-27 08:16:02,415] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7436
[2019-03-27 08:16:02,419] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.21666666666667, 90.16666666666666, 1.0, 2.0, 0.3429413209932025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531982.5763130432, 531982.5763130425, 169270.3924334412], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7257000.0000, 
sim time next is 7257600.0000, 
raw observation next is [22.2, 90.0, 1.0, 2.0, 0.3477453214577864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539988.0873005774, 539988.0873005774, 169934.8979234898], 
processed observation next is [1.0, 0.0, 0.2511848341232228, 0.9, 1.0, 1.0, 0.2141509897081764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14999669091682707, 0.14999669091682707, 0.25363417600520866], 
reward next is 0.7464, 
noisyNet noise sample is [array([-0.4919653], dtype=float32), 2.8199115]. 
=============================================
[2019-03-27 08:16:05,760] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 08:16:05,762] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:16:05,762] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:16:05,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:16:05,764] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:16:05,767] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:16:05,768] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:16:05,766] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:16:05,768] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:16:05,769] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:16:05,769] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:16:05,788] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run76
[2019-03-27 08:16:05,788] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run76
[2019-03-27 08:16:05,832] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run76
[2019-03-27 08:16:05,855] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run76
[2019-03-27 08:16:05,880] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run76
[2019-03-27 08:17:27,431] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07578898], dtype=float32), 0.04764359]
[2019-03-27 08:17:27,431] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.250088285, 52.30314440666667, 1.0, 2.0, 0.9492790530633898, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005990644586506, 6.9112, 168.9123159618752, 2224010.880669889, 2156763.389973661, 448095.7507259299]
[2019-03-27 08:17:27,432] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:17:27,436] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0913380e-10 9.9569821e-01 1.1124911e-14 4.3018097e-03 3.8782730e-19], sampled 0.7147511346605913
[2019-03-27 08:17:27,437] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2224010.880669889 W.
[2019-03-27 08:17:33,926] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07578898], dtype=float32), 0.04764359]
[2019-03-27 08:17:33,928] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.24761802333333, 82.77409081, 1.0, 2.0, 0.5600577080434939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 782624.5476071058, 782624.5476071051, 193539.7813292411]
[2019-03-27 08:17:33,930] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:17:33,932] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.9124128e-18 1.0000000e+00 5.4340431e-24 2.5980311e-15 2.9497936e-27], sampled 0.8108369421545776
[2019-03-27 08:17:59,411] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07578898], dtype=float32), 0.04764359]
[2019-03-27 08:17:59,412] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.6, 86.0, 1.0, 2.0, 0.5855701220190787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818289.3233511612, 818289.3233511612, 198085.244060002]
[2019-03-27 08:17:59,412] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:17:59,416] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3787071e-17 1.0000000e+00 1.5681035e-23 6.3324543e-15 1.0910715e-26], sampled 0.9336986252128121
[2019-03-27 08:18:02,152] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07578898], dtype=float32), 0.04764359]
[2019-03-27 08:18:02,154] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.9, 81.66666666666667, 1.0, 2.0, 0.5235415690042754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731579.3287968005, 731579.3287968005, 187367.2348529376]
[2019-03-27 08:18:02,154] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:18:02,157] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.25552914e-17 1.00000000e+00 1.07834122e-23 1.56973569e-14
 2.63454255e-27], sampled 0.8712804942537863
[2019-03-27 08:18:02,366] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7900.5863 3162760954.2798 1734.0000
[2019-03-27 08:18:02,471] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4779 2927245054.7920 1335.0000
[2019-03-27 08:18:02,601] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7697 2779149029.7625 932.0000
[2019-03-27 08:18:02,625] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.0480 3007344840.9204 1760.0000
[2019-03-27 08:18:02,695] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.5753 2842274541.0152 1125.0000
[2019-03-27 08:18:03,716] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1875000, evaluation results [1875000.0, 7900.586302977564, 3162760954.279808, 1734.0, 8254.477937879865, 2927245054.7920403, 1335.0, 8660.769670351696, 2779149029.762541, 932.0, 8000.047974230473, 3007344840.920367, 1760.0, 8496.5753074713, 2842274541.0152373, 1125.0]
[2019-03-27 08:18:04,995] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6502633e-12 9.9895954e-01 3.4451154e-16 1.0405009e-03 1.5311037e-20], sum to 1.0000
[2019-03-27 08:18:05,004] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9514
[2019-03-27 08:18:05,010] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 67.0, 1.0, 2.0, 0.4630501993997781, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564983626, 647024.9865095612, 647024.9865095618, 178010.7382028114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6715200.0000, 
sim time next is 6715800.0000, 
raw observation next is [29.2, 67.0, 1.0, 2.0, 0.4556760540037368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510428, 636717.9265236373, 636717.926523638, 176944.2551226636], 
processed observation next is [1.0, 0.7391304347826086, 0.5829383886255924, 0.67, 1.0, 1.0, 0.344188016871972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451522879, 0.17686609070101034, 0.17686609070101053, 0.2640959031681546], 
reward next is 0.7359, 
noisyNet noise sample is [array([-1.3616395], dtype=float32), 0.20580499]. 
=============================================
[2019-03-27 08:18:05,230] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7138302e-14 1.0000000e+00 3.0648329e-19 1.3579990e-09 4.3030974e-23], sum to 1.0000
[2019-03-27 08:18:05,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9998
[2019-03-27 08:18:05,241] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.26666666666667, 67.0, 1.0, 2.0, 0.4522053941760107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643380.8895357632, 643380.8895357632, 177930.5097590841], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6720000.0000, 
sim time next is 6720600.0000, 
raw observation next is [28.13333333333333, 67.0, 1.0, 2.0, 0.4472556753310333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 639636.9826076968, 639636.9826076961, 177635.2927738737], 
processed observation next is [1.0, 0.782608695652174, 0.532385466034755, 0.67, 1.0, 1.0, 0.33404298232654606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1776769396132491, 0.1776769396132489, 0.26512730264757267], 
reward next is 0.7349, 
noisyNet noise sample is [array([1.2677271], dtype=float32), 1.6475923]. 
=============================================
[2019-03-27 08:18:24,450] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3718728e-16 1.0000000e+00 1.8373255e-22 1.4570390e-14 6.0750504e-25], sum to 1.0000
[2019-03-27 08:18:24,459] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2627
[2019-03-27 08:18:24,463] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 92.5, 1.0, 2.0, 0.5898481219932419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 830319.3067673407, 830319.30676734, 199647.9751432616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7093800.0000, 
sim time next is 7094400.0000, 
raw observation next is [24.6, 92.66666666666667, 1.0, 2.0, 0.569235914044757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802345.5808369551, 802345.5808369551, 196039.0806612568], 
processed observation next is [1.0, 0.08695652173913043, 0.36492890995260674, 0.9266666666666667, 1.0, 1.0, 0.48100712535512896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22287377245470974, 0.22287377245470974, 0.2925956427779952], 
reward next is 0.7074, 
noisyNet noise sample is [array([-0.33680242], dtype=float32), -0.45778948]. 
=============================================
[2019-03-27 08:18:24,756] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9568129e-17 1.0000000e+00 2.9959406e-25 1.6461957e-14 3.9336865e-27], sum to 1.0000
[2019-03-27 08:18:24,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4158
[2019-03-27 08:18:24,768] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 88.0, 1.0, 2.0, 0.4907757578077594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685778.7101230004, 685778.7101230004, 182163.2575080206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7680000.0000, 
sim time next is 7680600.0000, 
raw observation next is [25.8, 88.0, 1.0, 2.0, 0.4890337036597263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683343.6919329992, 683343.6919329999, 181895.5481705506], 
processed observation next is [1.0, 0.9130434782608695, 0.42180094786729866, 0.88, 1.0, 1.0, 0.3843779562165377, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1898176922036109, 0.18981769220361108, 0.2714858927918666], 
reward next is 0.7285, 
noisyNet noise sample is [array([-0.7362844], dtype=float32), 2.0916858]. 
=============================================
[2019-03-27 08:18:25,122] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3461918e-09 9.5718604e-01 3.6263687e-15 4.2814028e-02 1.8733616e-18], sum to 1.0000
[2019-03-27 08:18:25,133] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5949
[2019-03-27 08:18:25,141] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1889129.867450148 W.
[2019-03-27 08:18:25,148] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.65, 62.0, 1.0, 2.0, 0.7100053173681377, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.989458031784235, 6.9112, 168.9124904115764, 1889129.867450148, 1833611.079467937, 387449.3987116829], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7734600.0000, 
sim time next is 7735200.0000, 
raw observation next is [31.7, 61.66666666666667, 1.0, 2.0, 0.5342042317122674, 1.0, 1.0, 0.5342042317122674, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1493483.25634735, 1493483.25634735, 311221.51641357], 
processed observation next is [1.0, 0.5217391304347826, 0.7014218009478673, 0.6166666666666667, 1.0, 1.0, 0.43880027917140646, 1.0, 0.5, 0.43880027917140646, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.41485646009648613, 0.41485646009648613, 0.46450972599040297], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2310355], dtype=float32), -0.8667088]. 
=============================================
[2019-03-27 08:18:26,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5964783e-10 9.5863724e-01 6.3077752e-15 4.1362703e-02 3.8205755e-17], sum to 1.0000
[2019-03-27 08:18:26,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7991
[2019-03-27 08:18:26,724] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1835932.068531176 W.
[2019-03-27 08:18:26,727] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.43333333333334, 54.83333333333333, 1.0, 2.0, 0.6636907755771911, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.950388157430513, 6.9112, 168.9127221974791, 1835932.068531176, 1808130.679463312, 380031.6686700461], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7037400.0000, 
sim time next is 7038000.0000, 
raw observation next is [30.6, 54.0, 1.0, 2.0, 0.6887253877329754, 1.0, 1.0, 0.6887253877329754, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1931437.587881985, 1931437.587881985, 370021.4178786284], 
processed observation next is [1.0, 0.4782608695652174, 0.6492890995260664, 0.54, 1.0, 1.0, 0.6249703466662354, 1.0, 0.5, 0.6249703466662354, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5365104410783291, 0.5365104410783291, 0.5522707729531767], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.43777657], dtype=float32), -0.14417489]. 
=============================================
[2019-03-27 08:18:26,739] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[52.17696 ]
 [51.808174]
 [52.899086]
 [52.76859 ]
 [52.906258]], R is [[51.9318428 ]
 [51.6493721 ]
 [51.13287735]
 [50.6215477 ]
 [50.34686279]].
[2019-03-27 08:18:29,352] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3014346e-18 1.0000000e+00 3.9182520e-24 2.5200488e-15 1.4857354e-27], sum to 1.0000
[2019-03-27 08:18:29,362] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6566
[2019-03-27 08:18:29,368] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.36666666666667, 91.0, 1.0, 2.0, 0.3542422841174849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 545710.5119515782, 545710.5119515775, 170289.0408707763], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7251600.0000, 
sim time next is 7252200.0000, 
raw observation next is [22.35, 91.0, 1.0, 2.0, 0.3538511214882691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 545368.6149059576, 545368.6149059576, 170268.0646785123], 
processed observation next is [1.0, 0.9565217391304348, 0.25829383886255936, 0.91, 1.0, 1.0, 0.22150737528707118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15149128191832156, 0.15149128191832156, 0.2541314398186751], 
reward next is 0.7459, 
noisyNet noise sample is [array([1.1274188], dtype=float32), 0.9660105]. 
=============================================
[2019-03-27 08:18:31,962] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2875202e-16 1.0000000e+00 1.4392370e-22 5.5371305e-13 2.9902773e-25], sum to 1.0000
[2019-03-27 08:18:31,972] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8558
[2019-03-27 08:18:31,978] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.08333333333333, 75.33333333333334, 1.0, 2.0, 0.6437289394030644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 908660.9997198978, 908660.9997198983, 210359.1509873515], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7114200.0000, 
sim time next is 7114800.0000, 
raw observation next is [27.16666666666667, 74.66666666666667, 1.0, 2.0, 0.9057054555282176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1279651.503588422, 1279651.503588422, 273555.2547712193], 
processed observation next is [1.0, 0.34782608695652173, 0.4865718799368091, 0.7466666666666667, 1.0, 1.0, 0.886392115094238, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35545875099678387, 0.35545875099678387, 0.40829142503167054], 
reward next is 0.5917, 
noisyNet noise sample is [array([0.7584187], dtype=float32), -0.49587217]. 
=============================================
[2019-03-27 08:18:33,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6659483e-16 1.0000000e+00 9.5272656e-23 9.9759109e-14 9.2018783e-27], sum to 1.0000
[2019-03-27 08:18:33,141] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2142
[2019-03-27 08:18:33,148] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333334, 86.0, 1.0, 2.0, 0.6243938726880052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872564.8218607658, 872564.8218607658, 205370.5781021543], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7885200.0000, 
sim time next is 7885800.0000, 
raw observation next is [26.8, 85.5, 1.0, 2.0, 0.633665521371371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 885526.9719471707, 885526.9719471707, 207176.9686434698], 
processed observation next is [1.0, 0.2608695652173913, 0.4691943127962086, 0.855, 1.0, 1.0, 0.5586331582787603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24597971442976962, 0.24597971442976962, 0.30921935618428326], 
reward next is 0.6908, 
noisyNet noise sample is [array([0.33323947], dtype=float32), 0.6839625]. 
=============================================
[2019-03-27 08:18:34,476] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8148514e-16 1.0000000e+00 3.0336766e-23 1.0885059e-12 6.2735119e-26], sum to 1.0000
[2019-03-27 08:18:34,486] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2496
[2019-03-27 08:18:34,491] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 88.0, 1.0, 2.0, 0.6885796863193041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 962302.4996188185, 962302.4996188191, 218396.3702338618], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7180200.0000, 
sim time next is 7180800.0000, 
raw observation next is [25.8, 88.33333333333333, 1.0, 2.0, 0.6130224763090107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 856667.3532408532, 856667.3532408539, 203188.4528057531], 
processed observation next is [1.0, 0.08695652173913043, 0.42180094786729866, 0.8833333333333333, 1.0, 1.0, 0.5337620196494105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23796315367801477, 0.23796315367801496, 0.3032663474712733], 
reward next is 0.6967, 
noisyNet noise sample is [array([1.3470887], dtype=float32), 1.7791002]. 
=============================================
[2019-03-27 08:18:37,987] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:18:37,987] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:18:38,056] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-03-27 08:18:40,319] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:18:40,320] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:18:40,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run10
[2019-03-27 08:18:42,638] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0223659e-15 1.0000000e+00 4.2748625e-21 1.5284149e-10 1.4635351e-24], sum to 1.0000
[2019-03-27 08:18:42,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5288
[2019-03-27 08:18:42,651] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333334, 61.66666666666667, 1.0, 2.0, 0.8903003222909005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1340734.917294767, 1340734.917294767, 280916.8603492433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7314600.0000, 
sim time next is 7315200.0000, 
raw observation next is [27.6, 62.0, 1.0, 2.0, 0.8260160980538479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1242852.539960537, 1242852.539960537, 262490.9453557746], 
processed observation next is [1.0, 0.6956521739130435, 0.5071090047393366, 0.62, 1.0, 1.0, 0.7903808410287324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34523681665570477, 0.34523681665570477, 0.3917775303817531], 
reward next is 0.6082, 
noisyNet noise sample is [array([0.9316137], dtype=float32), -0.004693997]. 
=============================================
[2019-03-27 08:18:48,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.0830807e-19 1.0000000e+00 5.1441367e-26 7.6714963e-19 7.1447222e-29], sum to 1.0000
[2019-03-27 08:18:48,257] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5593
[2019-03-27 08:18:48,262] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 95.0, 1.0, 2.0, 0.3224116906649874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 505903.6909419893, 505903.6909419887, 167383.6035632438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7448400.0000, 
sim time next is 7449000.0000, 
raw observation next is [21.21666666666667, 95.0, 1.0, 2.0, 0.3230113846010633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506686.1879506215, 506686.1879506215, 167439.300956639], 
processed observation next is [0.0, 0.21739130434782608, 0.20458135860979476, 0.95, 1.0, 1.0, 0.18435106578441363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1407461633196171, 0.1407461633196171, 0.24990940441289403], 
reward next is 0.7501, 
noisyNet noise sample is [array([1.2389536], dtype=float32), 0.06165448]. 
=============================================
[2019-03-27 08:18:48,272] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.08337 ]
 [78.01563 ]
 [77.99851 ]
 [78.013054]
 [77.90197 ]], R is [[78.09698486]
 [78.06619263]
 [78.03568268]
 [78.00544739]
 [77.97550201]].
[2019-03-27 08:18:48,823] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8216059e-18 1.0000000e+00 1.5776256e-23 2.6775889e-15 1.5016669e-27], sum to 1.0000
[2019-03-27 08:18:48,834] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1497
[2019-03-27 08:18:48,838] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 95.0, 1.0, 2.0, 0.523412068540784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758117.3739358892, 758117.3739358892, 190729.2154654737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7614600.0000, 
sim time next is 7615200.0000, 
raw observation next is [23.6, 95.0, 1.0, 2.0, 0.4882253590733958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708545.1194241378, 708545.1194241378, 185105.9904259079], 
processed observation next is [1.0, 0.13043478260869565, 0.3175355450236968, 0.95, 1.0, 1.0, 0.3834040470763805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19681808872892714, 0.19681808872892714, 0.2762775976506088], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.9269355], dtype=float32), -0.7338731]. 
=============================================
[2019-03-27 08:18:54,132] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 08:18:54,134] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:18:54,135] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:18:54,135] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:18:54,136] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:18:54,136] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:18:54,138] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:18:54,139] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:18:54,139] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:18:54,141] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:18:54,141] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:18:54,165] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run77
[2019-03-27 08:18:54,185] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run77
[2019-03-27 08:18:54,187] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run77
[2019-03-27 08:18:54,187] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run77
[2019-03-27 08:18:54,248] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run77
[2019-03-27 08:19:27,389] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07681475], dtype=float32), 0.04825853]
[2019-03-27 08:19:27,391] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.3342318, 92.85313067499999, 1.0, 2.0, 0.5933284767263931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 829135.2605219762, 829135.2605219767, 199505.0707006649]
[2019-03-27 08:19:27,394] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:19:27,396] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9602420e-17 1.0000000e+00 1.2695306e-23 1.7423004e-15 1.1064792e-26], sampled 0.36851706863981126
[2019-03-27 08:19:29,481] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07681475], dtype=float32), 0.04825853]
[2019-03-27 08:19:29,482] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.70153843333333, 76.87995709333333, 1.0, 2.0, 0.5194261614451512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725826.6323562429, 725826.6323562429, 186694.2106111368]
[2019-03-27 08:19:29,483] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:19:29,486] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5771318e-18 1.0000000e+00 1.1574425e-24 1.1832878e-15 3.9170483e-28], sampled 0.7740861741060722
[2019-03-27 08:19:38,288] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07681475], dtype=float32), 0.04825853]
[2019-03-27 08:19:38,289] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.81724983333334, 99.57199266999999, 1.0, 2.0, 0.2796184585588952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 454614.0386742093, 454614.0386742087, 163868.4697091805]
[2019-03-27 08:19:38,291] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:19:38,293] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0602983e-18 1.0000000e+00 2.5175857e-25 8.9008613e-18 2.5695098e-28], sampled 0.9116865681935966
[2019-03-27 08:19:53,918] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07681475], dtype=float32), 0.04825853]
[2019-03-27 08:19:53,919] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.01961645666667, 83.19268434333333, 1.0, 2.0, 0.7960991271336686, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1112641.570493343, 1112641.570493344, 242956.0467882848]
[2019-03-27 08:19:53,919] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:19:53,921] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.2740941e-16 1.0000000e+00 8.8844246e-22 6.9689532e-12 7.8507567e-26], sampled 0.13041288523635042
[2019-03-27 08:19:56,517] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07681475], dtype=float32), 0.04825853]
[2019-03-27 08:19:56,518] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.27522639166667, 60.45107459666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.290416777418445, 6.9112, 168.9104594451123, 2552951.292317091, 2283925.835884304, 475295.7907677775]
[2019-03-27 08:19:56,520] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:19:56,524] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.4251661e-11 9.9973816e-01 3.1221937e-15 2.6180738e-04 1.0146903e-18], sampled 0.26388928615086005
[2019-03-27 08:19:56,526] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2552951.292317091 W.
[2019-03-27 08:20:04,202] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07681475], dtype=float32), 0.04825853]
[2019-03-27 08:20:04,203] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.323015855, 88.790607205, 1.0, 2.0, 0.5615812500009877, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9752809134944065, 6.9112, 6.9112, 168.912558094266, 1570088.881201358, 1570088.881201358, 343574.7224218966]
[2019-03-27 08:20:04,204] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:20:04,205] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.7282883e-14 1.0000000e+00 5.0655424e-19 7.9648765e-10 6.4249431e-22], sampled 0.07355303790336265
[2019-03-27 08:20:04,210] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07681475], dtype=float32), 0.04825853]
[2019-03-27 08:20:04,210] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.56666666666667, 68.33333333333333, 1.0, 2.0, 0.6494150374880562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 907545.8124607228, 907545.8124607228, 210313.7189740366]
[2019-03-27 08:20:04,211] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:20:04,213] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.5444963e-18 1.0000000e+00 3.9204943e-24 2.3900984e-14 6.3052707e-28], sampled 0.9814886105502542
[2019-03-27 08:20:11,888] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07681475], dtype=float32), 0.04825853]
[2019-03-27 08:20:11,888] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.7, 74.66666666666667, 1.0, 2.0, 0.5610340105444547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783989.3360427199, 783989.3360427205, 193710.5342757134]
[2019-03-27 08:20:11,890] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:20:11,893] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.8952193e-18 1.0000000e+00 1.7801974e-24 1.6906872e-15 5.8199326e-28], sampled 0.8661611367718898
[2019-03-27 08:20:47,518] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9509 2779224265.4428 933.0000
[2019-03-27 08:20:49,431] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7893.9568 3162564717.0263 1743.0000
[2019-03-27 08:20:49,603] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6921 2842256059.6141 1130.0000
[2019-03-27 08:20:49,639] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.3300 2927216827.2872 1337.0000
[2019-03-27 08:20:49,663] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.8530 3007324363.7740 1758.0000
[2019-03-27 08:20:50,680] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1900000, evaluation results [1900000.0, 7893.956785115495, 3162564717.026286, 1743.0, 8256.330046891024, 2927216827.2871614, 1337.0, 8659.950916828071, 2779224265.4427943, 933.0, 8002.8530250747235, 3007324363.774006, 1758.0, 8497.692134722647, 2842256059.614111, 1130.0]
[2019-03-27 08:20:56,582] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2944932e-14 1.0000000e+00 2.2474091e-19 1.7435291e-09 2.3131802e-22], sum to 1.0000
[2019-03-27 08:20:56,588] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6617
[2019-03-27 08:20:56,593] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333333, 78.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.952574564016906, 6.9112, 168.9127672577143, 1483127.534722766, 1453775.027893622, 311346.4053433249], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7636800.0000, 
sim time next is 7637400.0000, 
raw observation next is [27.2, 76.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.125627535964178, 6.9112, 168.911620036081, 1605980.179568488, 1453859.10915226, 311346.4802504213], 
processed observation next is [1.0, 0.391304347826087, 0.4881516587677725, 0.765, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.021442753596417764, 0.0, 0.8294333824514035, 0.4461056054356911, 0.40384975254229444, 0.46469623917973324], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.0464547], dtype=float32), 0.88828963]. 
=============================================
[2019-03-27 08:20:57,564] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5383795e-08 7.2945166e-01 3.2273240e-12 2.7054834e-01 4.5993481e-16], sum to 1.0000
[2019-03-27 08:20:57,569] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9450
[2019-03-27 08:20:57,575] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.86666666666667, 59.0, 1.0, 2.0, 0.4768033124657135, 1.0, 2.0, 0.4768033124657135, 1.0, 1.0, 0.8062865840988465, 6.9112, 6.9112, 170.5573041426782, 1999982.117725643, 1999982.117725643, 395678.3735991768], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7654200.0000, 
sim time next is 7654800.0000, 
raw observation next is [30.73333333333333, 60.0, 1.0, 2.0, 0.730298880125587, 1.0, 2.0, 0.730298880125587, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2042230.602927827, 2042230.602927827, 387355.0321263777], 
processed observation next is [1.0, 0.6086956521739131, 0.6556082148499209, 0.6, 1.0, 1.0, 0.6750588917175746, 1.0, 1.0, 0.6750588917175746, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5672862785910631, 0.5672862785910631, 0.5781418389945936], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2707525], dtype=float32), 0.6641908]. 
=============================================
[2019-03-27 08:20:57,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:20:57,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:20:57,827] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run10
[2019-03-27 08:21:04,479] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.2080353e-17 1.0000000e+00 1.0048955e-21 2.8450903e-12 1.0564711e-25], sum to 1.0000
[2019-03-27 08:21:04,486] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7425
[2019-03-27 08:21:04,490] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.76666666666667, 88.66666666666667, 1.0, 2.0, 0.6319917193248277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 883186.9154398027, 883186.9154398027, 206847.6459180047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7789800.0000, 
sim time next is 7790400.0000, 
raw observation next is [25.7, 89.0, 1.0, 2.0, 0.6132574665387287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 856995.8724743897, 856995.872474389, 203233.1221820646], 
processed observation next is [1.0, 0.17391304347826086, 0.4170616113744076, 0.89, 1.0, 1.0, 0.5340451404081068, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23805440902066383, 0.23805440902066363, 0.303333018182186], 
reward next is 0.6967, 
noisyNet noise sample is [array([-1.424991], dtype=float32), 0.7800651]. 
=============================================
[2019-03-27 08:21:05,690] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:05,691] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:05,753] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run10
[2019-03-27 08:21:08,370] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5072668e-15 1.0000000e+00 7.2577319e-22 5.4950308e-11 1.8713268e-23], sum to 1.0000
[2019-03-27 08:21:08,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7999
[2019-03-27 08:21:08,385] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 88.66666666666667, 1.0, 2.0, 0.6556249728870197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 916227.8286220971, 916227.8286220978, 211556.9035580132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7881000.0000, 
sim time next is 7881600.0000, 
raw observation next is [26.4, 88.33333333333334, 1.0, 2.0, 0.5889801326749963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 823056.3979289265, 823056.3979289272, 198705.2032854495], 
processed observation next is [1.0, 0.21739130434782608, 0.45023696682464454, 0.8833333333333334, 1.0, 1.0, 0.5047953405722846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2286267772024796, 0.2286267772024798, 0.2965749302767903], 
reward next is 0.7034, 
noisyNet noise sample is [array([-1.0585754], dtype=float32), 1.8540015]. 
=============================================
[2019-03-27 08:21:09,008] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:09,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:09,075] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run10
[2019-03-27 08:21:11,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:11,216] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:11,283] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run10
[2019-03-27 08:21:11,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.39248733e-15 1.00000000e+00 1.21907661e-22 1.03893057e-13
 5.72702109e-26], sum to 1.0000
[2019-03-27 08:21:11,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7387
[2019-03-27 08:21:11,600] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 89.66666666666667, 1.0, 2.0, 0.3878659190502755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598953.3325073369, 598953.3325073362, 174934.9719188603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [22.48333333333333, 89.83333333333333, 1.0, 2.0, 0.3817879508531122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 588945.2340165173, 588945.2340165167, 174030.2213120737], 
processed observation next is [1.0, 0.08695652173913043, 0.26461295418641384, 0.8983333333333333, 1.0, 1.0, 0.25516620584712313, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16359589833792146, 0.16359589833792132, 0.2597465989732443], 
reward next is 0.7403, 
noisyNet noise sample is [array([0.4997246], dtype=float32), -2.2248893]. 
=============================================
[2019-03-27 08:21:12,145] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1635582e-17 1.0000000e+00 3.4169995e-24 1.5334218e-15 1.4925180e-27], sum to 1.0000
[2019-03-27 08:21:12,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4212
[2019-03-27 08:21:12,156] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 84.0, 1.0, 2.0, 0.2407599575040754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 398168.2722757019, 398168.2722757013, 159920.8054862081], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 684000.0000, 
sim time next is 684600.0000, 
raw observation next is [19.38333333333333, 84.66666666666667, 1.0, 2.0, 0.2412925513793596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 399165.0872869539, 399165.0872869533, 159963.1122620658], 
processed observation next is [1.0, 0.9565217391304348, 0.11769352290679291, 0.8466666666666667, 1.0, 1.0, 0.08589464021609589, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11087919091304274, 0.11087919091304257, 0.23875091382397884], 
reward next is 0.7612, 
noisyNet noise sample is [array([0.3315103], dtype=float32), -0.16264275]. 
=============================================
[2019-03-27 08:21:12,666] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:12,667] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:12,742] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run10
[2019-03-27 08:21:12,764] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:12,766] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:12,835] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run10
[2019-03-27 08:21:13,156] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:13,156] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:13,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:13,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:13,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run10
[2019-03-27 08:21:13,261] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run10
[2019-03-27 08:21:13,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:13,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:13,333] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9869333e-15 1.0000000e+00 1.8042998e-21 4.0388887e-11 5.3435548e-24], sum to 1.0000
[2019-03-27 08:21:13,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5255
[2019-03-27 08:21:13,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run10
[2019-03-27 08:21:13,340] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.98333333333333, 67.83333333333333, 1.0, 2.0, 0.8706346101013434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1332474.388643566, 1332474.388643566, 277886.5639423959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 39000.0000, 
sim time next is 39600.0000, 
raw observation next is [26.2, 67.0, 1.0, 2.0, 0.7945696492078416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1213461.36399597, 1213461.36399597, 256228.012275726], 
processed observation next is [1.0, 0.4782608695652174, 0.44075829383886256, 0.67, 1.0, 1.0, 0.7524935532624597, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33707260110999165, 0.33707260110999165, 0.3824298690682478], 
reward next is 0.6176, 
noisyNet noise sample is [array([-0.2846956], dtype=float32), -0.36329582]. 
=============================================
[2019-03-27 08:21:13,474] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:13,476] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:13,499] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run10
[2019-03-27 08:21:13,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:13,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:13,645] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run10
[2019-03-27 08:21:13,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:13,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:13,717] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run10
[2019-03-27 08:21:13,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:13,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:14,011] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run10
[2019-03-27 08:21:14,180] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:21:14,180] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:14,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run10
[2019-03-27 08:21:16,696] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7541377e-15 1.0000000e+00 1.1070942e-21 4.5092174e-13 1.2610488e-25], sum to 1.0000
[2019-03-27 08:21:16,706] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4720
[2019-03-27 08:21:16,712] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333334, 81.66666666666666, 1.0, 2.0, 0.3699397906542067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564259.9537591914, 564259.9537591914, 171702.2886318751], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 70800.0000, 
sim time next is 71400.0000, 
raw observation next is [23.76666666666667, 82.33333333333334, 1.0, 2.0, 0.3676520187867933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561973.0346438176, 561973.0346438176, 171539.9440124251], 
processed observation next is [1.0, 0.8260869565217391, 0.32543443917851517, 0.8233333333333335, 1.0, 1.0, 0.2381349623937269, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15610362073439377, 0.15610362073439377, 0.256029767182724], 
reward next is 0.7440, 
noisyNet noise sample is [array([-0.42788568], dtype=float32), -0.053128086]. 
=============================================
[2019-03-27 08:21:17,002] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0090554e-18 1.0000000e+00 3.8976643e-24 3.7479048e-16 1.4178454e-26], sum to 1.0000
[2019-03-27 08:21:17,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3135
[2019-03-27 08:21:17,014] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 84.0, 1.0, 2.0, 0.3407849861179714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539199.0849194026, 539199.0849194026, 170068.0473758531], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 28200.0000, 
sim time next is 28800.0000, 
raw observation next is [22.3, 84.0, 1.0, 2.0, 0.3294673707365959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520693.2050942397, 520693.2050942397, 168598.3714326102], 
processed observation next is [1.0, 0.34782608695652173, 0.25592417061611383, 0.84, 1.0, 1.0, 0.19212936233324804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14463700141506658, 0.14463700141506658, 0.25163936034717943], 
reward next is 0.7484, 
noisyNet noise sample is [array([-1.0088413], dtype=float32), -0.106140226]. 
=============================================
[2019-03-27 08:21:17,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7303277e-15 1.0000000e+00 1.1651796e-20 1.1855618e-09 1.1350555e-24], sum to 1.0000
[2019-03-27 08:21:17,448] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6661
[2019-03-27 08:21:17,453] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 63.0, 1.0, 2.0, 0.6877045906094447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1035345.402640093, 1035345.402640094, 227732.9374760468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 46800.0000, 
sim time next is 47400.0000, 
raw observation next is [27.5, 62.83333333333333, 1.0, 2.0, 0.639856102148038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 961578.9153855874, 961578.9153855868, 216900.8837594358], 
processed observation next is [1.0, 0.5652173913043478, 0.5023696682464456, 0.6283333333333333, 1.0, 1.0, 0.5660916893349855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2671052542737743, 0.26710525427377413, 0.32373266232751613], 
reward next is 0.6763, 
noisyNet noise sample is [array([0.9180913], dtype=float32), -2.3511326]. 
=============================================
[2019-03-27 08:21:39,545] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.9310360e-19 1.0000000e+00 1.7923049e-25 2.7380942e-17 8.5357955e-29], sum to 1.0000
[2019-03-27 08:21:39,553] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6051
[2019-03-27 08:21:39,558] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.75, 85.66666666666667, 1.0, 2.0, 0.2470723540427138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 406555.8672708419, 406555.8672708425, 160615.5944239727], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 427800.0000, 
sim time next is 428400.0000, 
raw observation next is [19.7, 86.0, 1.0, 2.0, 0.246315552570411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 405354.9084954147, 405354.9084954141, 160541.0956815287], 
processed observation next is [1.0, 1.0, 0.1327014218009479, 0.86, 1.0, 1.0, 0.09194644888001323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11259858569317076, 0.11259858569317059, 0.2396135756440727], 
reward next is 0.7604, 
noisyNet noise sample is [array([0.18480255], dtype=float32), 0.2569981]. 
=============================================
[2019-03-27 08:21:41,283] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5173487e-17 1.0000000e+00 3.3694067e-23 7.1264033e-14 2.5276103e-27], sum to 1.0000
[2019-03-27 08:21:41,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8172
[2019-03-27 08:21:41,292] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2533692e-17 1.0000000e+00 3.0962024e-23 2.2673225e-15 4.7653226e-26], sum to 1.0000
[2019-03-27 08:21:41,298] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.98333333333333, 58.5, 1.0, 2.0, 0.6424025593383613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1053425.796239427, 1053425.796239427, 225013.9144379167], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 568200.0000, 
sim time next is 568800.0000, 
raw observation next is [23.9, 59.0, 1.0, 2.0, 0.6052822334205809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 992495.5967563349, 992495.5967563356, 216736.5010347012], 
processed observation next is [1.0, 0.6086956521739131, 0.33175355450236965, 0.59, 1.0, 1.0, 0.5244364258079287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2756932213212041, 0.27569322132120433, 0.323487314977166], 
reward next is 0.6765, 
noisyNet noise sample is [array([0.20676373], dtype=float32), 1.3592728]. 
=============================================
[2019-03-27 08:21:41,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1093
[2019-03-27 08:21:41,306] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.55, 73.0, 1.0, 2.0, 0.4506235576429891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740363.6387297891, 740363.6387297891, 187328.642472771], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 466200.0000, 
sim time next is 466800.0000, 
raw observation next is [21.66666666666666, 72.33333333333333, 1.0, 2.0, 0.4295398179584428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705541.7323196357, 705541.7323196363, 183950.0469274246], 
processed observation next is [1.0, 0.391304347826087, 0.22590837282780388, 0.7233333333333333, 1.0, 1.0, 0.3126985758535456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19598381453323213, 0.1959838145332323, 0.2745523088469024], 
reward next is 0.7254, 
noisyNet noise sample is [array([-0.16197574], dtype=float32), -0.70304775]. 
=============================================
[2019-03-27 08:21:42,633] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 08:21:42,635] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:21:42,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:42,636] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:21:42,638] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:21:42,640] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:21:42,642] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:42,643] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:21:42,643] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:42,643] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:42,641] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:21:42,666] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run78
[2019-03-27 08:21:42,666] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run78
[2019-03-27 08:21:42,709] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run78
[2019-03-27 08:21:42,732] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run78
[2019-03-27 08:21:42,734] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run78
[2019-03-27 08:21:47,918] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07805694], dtype=float32), 0.049001202]
[2019-03-27 08:21:47,920] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.34787222, 90.12892260000001, 1.0, 2.0, 0.324782832843093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506131.5350430132, 506131.5350430132, 167305.648956229]
[2019-03-27 08:21:47,922] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:21:47,925] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.1336669e-19 1.0000000e+00 8.7820057e-26 9.8240585e-18 5.6794320e-29], sampled 0.5880405829647398
[2019-03-27 08:22:08,203] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07805694], dtype=float32), 0.049001202]
[2019-03-27 08:22:08,203] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.67338495, 89.93941396, 1.0, 2.0, 0.3721195634125791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558070.0055290555, 558070.0055290555, 170862.8167458052]
[2019-03-27 08:22:08,205] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:22:08,207] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.1400773e-18 1.0000000e+00 1.0561049e-24 4.4308062e-17 1.1510034e-27], sampled 0.861959211044994
[2019-03-27 08:22:54,389] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07805694], dtype=float32), 0.049001202]
[2019-03-27 08:22:54,391] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.2, 70.0, 1.0, 2.0, 0.9265305754449458, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005989426530532, 6.9112, 168.912315949302, 2192172.486509915, 2124925.859946246, 441539.8054702004]
[2019-03-27 08:22:54,393] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:22:54,395] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1636871e-11 9.9610871e-01 2.0545527e-15 3.8912352e-03 6.7353685e-20], sampled 0.11768703423451288
[2019-03-27 08:22:54,395] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2192172.486509915 W.
[2019-03-27 08:23:00,689] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07805694], dtype=float32), 0.049001202]
[2019-03-27 08:23:00,690] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.5, 78.0, 1.0, 2.0, 0.7696950076438381, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.001237669652204, 6.9112, 168.9123469982968, 1972661.262647991, 1908785.671431304, 400670.0877041352]
[2019-03-27 08:23:00,694] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:23:00,699] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0329757e-12 1.0000000e+00 1.8467320e-17 5.6820500e-08 1.5562888e-20], sampled 0.6016082141184625
[2019-03-27 08:23:00,700] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1972661.262647991 W.
[2019-03-27 08:23:01,076] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07805694], dtype=float32), 0.049001202]
[2019-03-27 08:23:01,076] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.962974695, 66.87594303, 1.0, 2.0, 0.5455145602643316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762294.6629740179, 762294.6629740179, 191034.684832222]
[2019-03-27 08:23:01,079] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:23:01,083] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.3365601e-18 1.0000000e+00 1.3286908e-24 8.0940503e-15 2.2350619e-28], sampled 0.3961028487560184
[2019-03-27 08:23:24,672] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07805694], dtype=float32), 0.049001202]
[2019-03-27 08:23:24,673] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.98333333333333, 73.16666666666667, 1.0, 2.0, 0.3983812793188612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593862.1654945082, 593862.1654945075, 173955.9070935067]
[2019-03-27 08:23:24,674] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:23:24,677] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.5053273e-19 1.0000000e+00 9.3056294e-26 1.1233216e-17 7.8210625e-29], sampled 0.3170664317614783
[2019-03-27 08:23:28,963] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07805694], dtype=float32), 0.049001202]
[2019-03-27 08:23:28,964] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.35, 60.83333333333333, 1.0, 2.0, 0.8747900870821401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1222684.705714996, 1222684.705714996, 263100.9473799565]
[2019-03-27 08:23:28,966] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:23:28,967] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.88315541e-17 1.00000000e+00 1.21342024e-23 2.20504767e-14
 5.89924129e-27], sampled 0.6288388686723523
[2019-03-27 08:23:38,800] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 08:23:39,521] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8257.4179 2927197484.3661 1335.0000
[2019-03-27 08:23:39,619] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7892.9726 3163719192.3438 1752.0000
[2019-03-27 08:23:39,685] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.9617 2842462665.4956 1129.0000
[2019-03-27 08:23:39,742] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7551 3007614834.1001 1765.0000
[2019-03-27 08:23:40,763] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1925000, evaluation results [1925000.0, 7892.972626544291, 3163719192.3437977, 1752.0, 8257.417936607377, 2927197484.366148, 1335.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.755125529797, 3007614834.1000705, 1765.0, 8495.961716204047, 2842462665.495616, 1129.0]
[2019-03-27 08:23:41,046] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9331219e-18 1.0000000e+00 4.7049026e-26 3.9910684e-17 3.5270872e-28], sum to 1.0000
[2019-03-27 08:23:41,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5381
[2019-03-27 08:23:41,062] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.81666666666667, 87.0, 1.0, 2.0, 0.2311405328509986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383424.2429438421, 383424.2429438421, 158904.2438024592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 517800.0000, 
sim time next is 518400.0000, 
raw observation next is [18.8, 87.0, 1.0, 2.0, 0.2299358285746499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381481.9020643638, 381481.9020643638, 158786.4280343929], 
processed observation next is [1.0, 0.0, 0.09004739336492901, 0.87, 1.0, 1.0, 0.07221184165620467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10596719501787884, 0.10596719501787884, 0.2369946687080491], 
reward next is 0.7630, 
noisyNet noise sample is [array([-1.924184], dtype=float32), -1.2203127]. 
=============================================
[2019-03-27 08:23:41,249] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0208494e-13 9.9999964e-01 3.0637479e-17 3.3969707e-07 3.6198139e-22], sum to 1.0000
[2019-03-27 08:23:41,259] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6879
[2019-03-27 08:23:41,267] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 73.0, 1.0, 2.0, 1.032628219743388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1443443.342415204, 1443443.342415204, 309022.4345520949], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1256400.0000, 
sim time next is 1257000.0000, 
raw observation next is [28.41666666666667, 73.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.940261929533539, 6.9112, 168.9125988634404, 1474386.536788575, 1453769.047397491, 311347.8203567014], 
processed observation next is [1.0, 0.5652173913043478, 0.5458135860979465, 0.73, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0029061929533538587, 0.0, 0.8294381889418473, 0.40955181577460414, 0.40382473538819197, 0.4646982393383603], 
reward next is 0.3900, 
noisyNet noise sample is [array([0.9731589], dtype=float32), 0.21574868]. 
=============================================
[2019-03-27 08:23:41,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[67.25603 ]
 [64.93335 ]
 [62.33754 ]
 [57.766468]
 [53.15507 ]], R is [[68.27336121]
 [68.12940216]
 [68.03210449]
 [67.94789124]
 [67.26840973]].
[2019-03-27 08:23:44,519] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1527192e-16 1.0000000e+00 2.0270883e-22 1.6601073e-14 4.4174559e-25], sum to 1.0000
[2019-03-27 08:23:44,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6317
[2019-03-27 08:23:44,532] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 60.0, 1.0, 2.0, 0.4543864610132667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748939.0051099604, 748939.0051099599, 187930.2080675032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 556800.0000, 
sim time next is 557400.0000, 
raw observation next is [23.48333333333333, 58.5, 1.0, 2.0, 0.4597961620168833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758150.0351357044, 758150.0351357044, 188822.7764651052], 
processed observation next is [1.0, 0.43478260869565216, 0.3120063191153238, 0.585, 1.0, 1.0, 0.3491520024299799, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21059723198214012, 0.21059723198214012, 0.281825039500157], 
reward next is 0.7182, 
noisyNet noise sample is [array([-0.7123722], dtype=float32), -0.3721872]. 
=============================================
[2019-03-27 08:23:47,345] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1220836e-17 1.0000000e+00 6.9841297e-25 2.4543864e-15 1.8758400e-27], sum to 1.0000
[2019-03-27 08:23:47,353] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4359
[2019-03-27 08:23:47,360] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.88333333333333, 92.83333333333333, 1.0, 2.0, 0.3061218539966665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487057.8767615511, 487057.8767615511, 166124.2753755978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1360200.0000, 
sim time next is 1360800.0000, 
raw observation next is [20.9, 93.0, 1.0, 2.0, 0.3099032748595594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 492672.6424365093, 492672.6424365099, 166528.201158081], 
processed observation next is [1.0, 0.782608695652174, 0.1895734597156398, 0.93, 1.0, 1.0, 0.16855816248139682, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13685351178791924, 0.1368535117879194, 0.2485495539672851], 
reward next is 0.7515, 
noisyNet noise sample is [array([-0.34536585], dtype=float32), -0.6145442]. 
=============================================
[2019-03-27 08:24:10,852] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.81021047e-17 1.00000000e+00 6.97236762e-24 1.09173236e-13
 5.95050415e-26], sum to 1.0000
[2019-03-27 08:24:10,858] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1697
[2019-03-27 08:24:10,863] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.83333333333334, 1.0, 2.0, 0.4068980605363464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 623561.2886847962, 623561.2886847956, 177085.8042402147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1012200.0000, 
sim time next is 1012800.0000, 
raw observation next is [21.7, 97.66666666666667, 1.0, 2.0, 0.3574286508168133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548152.3398437325, 548152.3398437331, 170420.9135442379], 
processed observation next is [1.0, 0.7391304347826086, 0.2274881516587678, 0.9766666666666667, 1.0, 1.0, 0.22581765158652203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15226453884548127, 0.1522645388454814, 0.2543595724540864], 
reward next is 0.7456, 
noisyNet noise sample is [array([1.9237027], dtype=float32), -0.45713824]. 
=============================================
[2019-03-27 08:24:12,262] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.6268229e-19 1.0000000e+00 4.4064488e-25 2.7154733e-15 6.4116254e-29], sum to 1.0000
[2019-03-27 08:24:12,273] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1130
[2019-03-27 08:24:12,277] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 97.0, 1.0, 2.0, 0.3745609537191627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 567166.5495109085, 567166.5495109091, 171831.886341989], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1036800.0000, 
sim time next is 1037400.0000, 
raw observation next is [22.23333333333333, 97.0, 1.0, 2.0, 0.375666336588196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 568340.9784093015, 568340.9784093022, 171919.3288672753], 
processed observation next is [1.0, 0.0, 0.25276461295418634, 0.97, 1.0, 1.0, 0.2477907669737301, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15787249400258374, 0.15787249400258394, 0.25659601323473924], 
reward next is 0.7434, 
noisyNet noise sample is [array([0.74725896], dtype=float32), -0.13486233]. 
=============================================
[2019-03-27 08:24:15,133] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8261949e-18 1.0000000e+00 1.7553546e-24 3.7197743e-16 3.6813691e-28], sum to 1.0000
[2019-03-27 08:24:15,134] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0134
[2019-03-27 08:24:15,140] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.88333333333334, 84.66666666666667, 1.0, 2.0, 0.3060661005861678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 486952.4796583273, 486952.4796583279, 166116.3205843818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1116600.0000, 
sim time next is 1117200.0000, 
raw observation next is [21.76666666666667, 85.33333333333334, 1.0, 2.0, 0.3049440339954287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485473.150321774, 485473.150321774, 166013.7063149755], 
processed observation next is [1.0, 0.9565217391304348, 0.23064770932069528, 0.8533333333333334, 1.0, 1.0, 0.1625831734884683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13485365286715945, 0.13485365286715945, 0.24778165121638135], 
reward next is 0.7522, 
noisyNet noise sample is [array([-0.5123127], dtype=float32), 0.5446601]. 
=============================================
[2019-03-27 08:24:15,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9219498e-12 9.9999928e-01 1.3698272e-16 7.3049392e-07 4.1203058e-22], sum to 1.0000
[2019-03-27 08:24:15,258] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7222
[2019-03-27 08:24:15,262] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.41666666666667, 73.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.940261929533538, 6.9112, 168.9125988634404, 1474386.536788574, 1453769.047397491, 311347.8203566567], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1257000.0000, 
sim time next is 1257600.0000, 
raw observation next is [28.43333333333333, 73.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.676057095263893, 6.9112, 168.9028346419046, 2706537.704553578, 1454562.36681619, 310071.1608202346], 
processed observation next is [1.0, 0.5652173913043478, 0.546603475513428, 0.73, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.17648570952638928, 0.0, 0.8293902421440266, 0.7518160290426605, 0.4040451018933861, 0.4627927773436338], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0115163], dtype=float32), 1.0076445]. 
=============================================
[2019-03-27 08:24:23,182] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4100733e-18 1.0000000e+00 2.4710879e-24 1.2905811e-15 3.9455268e-28], sum to 1.0000
[2019-03-27 08:24:23,191] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3376
[2019-03-27 08:24:23,198] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 97.16666666666667, 1.0, 2.0, 0.4246287772600349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619220.5251318091, 619220.5251318091, 175949.3788216078], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1980600.0000, 
sim time next is 1981200.0000, 
raw observation next is [23.3, 97.33333333333334, 1.0, 2.0, 0.427930599859608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622307.7078989099, 622307.7078989092, 176200.3431244663], 
processed observation next is [1.0, 0.9565217391304348, 0.3033175355450238, 0.9733333333333334, 1.0, 1.0, 0.3107597588669976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17286325219414164, 0.17286325219414145, 0.26298558675293476], 
reward next is 0.7370, 
noisyNet noise sample is [array([-0.23881495], dtype=float32), -0.6330651]. 
=============================================
[2019-03-27 08:24:28,033] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8662995e-17 1.0000000e+00 7.4101925e-24 1.8581160e-17 9.6541969e-27], sum to 1.0000
[2019-03-27 08:24:28,044] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7061
[2019-03-27 08:24:28,049] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.73333333333333, 88.66666666666666, 1.0, 2.0, 0.3913579781120874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 584088.8963221629, 584088.8963221624, 173081.8393349934], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1424400.0000, 
sim time next is 1425000.0000, 
raw observation next is [23.86666666666667, 88.83333333333334, 1.0, 2.0, 0.3972205839056678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589641.9198454384, 589641.9198454384, 173491.0654401467], 
processed observation next is [0.0, 0.4782608695652174, 0.33017377567140627, 0.8883333333333334, 1.0, 1.0, 0.27375973964538286, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16378942217928846, 0.16378942217928846, 0.25894188871663687], 
reward next is 0.7411, 
noisyNet noise sample is [array([0.6534167], dtype=float32), -0.94155556]. 
=============================================
[2019-03-27 08:24:28,058] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.01739 ]
 [75.02216 ]
 [75.019585]
 [75.01999 ]
 [74.96206 ]], R is [[74.99002075]
 [74.98178864]
 [74.97433472]
 [74.96682739]
 [74.96102142]].
[2019-03-27 08:24:29,520] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6436685e-17 1.0000000e+00 3.6896451e-23 9.9536502e-15 1.3251615e-26], sum to 1.0000
[2019-03-27 08:24:29,529] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2181
[2019-03-27 08:24:29,533] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.13333333333334, 94.0, 1.0, 2.0, 0.3217289673402233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507216.5041706824, 507216.5041706824, 167538.7064395515], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1365600.0000, 
sim time next is 1366200.0000, 
raw observation next is [21.15, 94.0, 1.0, 2.0, 0.3223590488058941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508003.9912409307, 508003.9912409307, 167594.3001819901], 
processed observation next is [1.0, 0.8260869565217391, 0.2014218009478673, 0.94, 1.0, 1.0, 0.1835651190432459, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14111221978914743, 0.14111221978914743, 0.25014074654028373], 
reward next is 0.7499, 
noisyNet noise sample is [array([0.4304427], dtype=float32), -0.04665613]. 
=============================================
[2019-03-27 08:24:30,144] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7878383e-19 1.0000000e+00 1.3066601e-24 8.7895153e-18 2.0077436e-27], sum to 1.0000
[2019-03-27 08:24:30,154] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3567
[2019-03-27 08:24:30,159] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.68333333333333, 97.5, 1.0, 2.0, 0.3156211437177996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498074.5004025648, 498074.5004025642, 166860.1739553702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1401000.0000, 
sim time next is 1401600.0000, 
raw observation next is [20.76666666666667, 97.0, 1.0, 2.0, 0.316605561680278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499321.9557494573, 499321.9557494573, 166946.7863432244], 
processed observation next is [0.0, 0.21739130434782608, 0.18325434439178534, 0.97, 1.0, 1.0, 0.1766332068437084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13870054326373812, 0.13870054326373812, 0.2491743079749618], 
reward next is 0.7508, 
noisyNet noise sample is [array([0.66700006], dtype=float32), -0.9357375]. 
=============================================
[2019-03-27 08:24:32,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1618918e-18 1.0000000e+00 3.1481141e-25 9.9630935e-18 6.7356256e-28], sum to 1.0000
[2019-03-27 08:24:32,110] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8174
[2019-03-27 08:24:32,115] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 70.0, 1.0, 2.0, 0.4273507734078753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617285.1253990417, 617285.1253990423, 175593.3555833625], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1434600.0000, 
sim time next is 1435200.0000, 
raw observation next is [27.46666666666667, 69.66666666666667, 1.0, 2.0, 0.4273767754111061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617098.6455318729, 617098.6455318729, 175568.7072981955], 
processed observation next is [0.0, 0.6086956521739131, 0.500789889415482, 0.6966666666666668, 1.0, 1.0, 0.3100925004953085, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17141629042552023, 0.17141629042552023, 0.26204284671372463], 
reward next is 0.7380, 
noisyNet noise sample is [array([-1.2978439], dtype=float32), -0.96093214]. 
=============================================
[2019-03-27 08:24:34,116] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 08:24:34,118] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:24:34,119] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:24:34,119] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:24:34,120] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:24:34,121] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:24:34,121] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:24:34,122] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:24:34,122] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:24:34,121] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:24:34,127] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:24:34,145] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run79
[2019-03-27 08:24:34,145] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run79
[2019-03-27 08:24:34,179] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run79
[2019-03-27 08:24:34,203] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run79
[2019-03-27 08:24:34,229] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run79
[2019-03-27 08:24:40,755] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07844134], dtype=float32), 0.048712015]
[2019-03-27 08:24:40,756] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.96973903166667, 86.09206871333333, 1.0, 2.0, 0.2866307750253436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 464040.1980651133, 464040.1980651133, 164536.3755159934]
[2019-03-27 08:24:40,759] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:24:40,762] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8524713e-18 1.0000000e+00 9.5321998e-25 1.4358626e-17 1.6850816e-27], sampled 0.982049773317493
[2019-03-27 08:24:55,408] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07844134], dtype=float32), 0.048712015]
[2019-03-27 08:24:55,414] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.03333333333333, 64.66666666666667, 1.0, 2.0, 0.2399977308822128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 397493.534020919, 397493.5340209184, 159799.8853159153]
[2019-03-27 08:24:55,416] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:24:55,418] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.8579901e-19 1.0000000e+00 1.8496644e-25 1.0139697e-17 2.3524883e-28], sampled 0.6897806995053536
[2019-03-27 08:25:09,534] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07844134], dtype=float32), 0.048712015]
[2019-03-27 08:25:09,534] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.69351683666667, 87.52826478166668, 1.0, 2.0, 0.613810389494428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 857768.8659729769, 857768.8659729762, 203348.1544145555]
[2019-03-27 08:25:09,536] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:25:09,538] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8239948e-18 1.0000000e+00 1.3432510e-24 1.3802514e-15 5.2576303e-28], sampled 0.2660782085915293
[2019-03-27 08:25:57,273] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07844134], dtype=float32), 0.048712015]
[2019-03-27 08:25:57,274] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 63.5, 1.0, 2.0, 0.5563401364978955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777427.7126325896, 777427.7126325902, 192894.3859055719]
[2019-03-27 08:25:57,275] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:25:57,277] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.8200799e-18 1.0000000e+00 1.8327106e-24 3.3580116e-16 1.3294291e-27], sampled 0.8522477714069735
[2019-03-27 08:26:30,574] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.2831 2927118181.2779 1331.0000
[2019-03-27 08:26:30,579] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.2079 2842205829.7021 1127.0000
[2019-03-27 08:26:30,594] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7894.5497 3162482777.0826 1745.0000
[2019-03-27 08:26:30,758] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.6369 3007431345.8331 1762.0000
[2019-03-27 08:26:30,853] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 08:26:31,871] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1950000, evaluation results [1950000.0, 7894.549742486639, 3162482777.08264, 1745.0, 8255.283067365877, 2927118181.2779207, 1331.0, 8660.716715102415, 2779131942.069542, 933.0, 8000.636864729579, 3007431345.8330708, 1762.0, 8499.207899387366, 2842205829.702149, 1127.0]
[2019-03-27 08:26:32,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6890650e-19 1.0000000e+00 4.2948442e-25 4.5907631e-18 1.3051929e-28], sum to 1.0000
[2019-03-27 08:26:32,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6895
[2019-03-27 08:26:32,182] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 69.66666666666667, 1.0, 2.0, 0.4425385865042282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 629922.9750266817, 629922.9750266817, 176577.7746709021], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1438800.0000, 
sim time next is 1439400.0000, 
raw observation next is [27.93333333333334, 69.83333333333333, 1.0, 2.0, 0.4476488768239278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634772.7633866058, 634772.7633866058, 176999.4665341146], 
processed observation next is [0.0, 0.6521739130434783, 0.5229067930489735, 0.6983333333333333, 1.0, 1.0, 0.33451671906497327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1763257676073905, 0.1763257676073905, 0.26417830825987254], 
reward next is 0.7358, 
noisyNet noise sample is [array([0.6252286], dtype=float32), -1.6579438]. 
=============================================
[2019-03-27 08:26:33,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8439294e-19 1.0000000e+00 1.0610274e-24 9.3242207e-17 1.9869120e-28], sum to 1.0000
[2019-03-27 08:26:33,416] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5870
[2019-03-27 08:26:33,422] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 88.0, 1.0, 2.0, 0.3937778143103614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587622.5538527947, 587622.5538527947, 173401.3058275043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1452600.0000, 
sim time next is 1453200.0000, 
raw observation next is [23.5, 89.33333333333333, 1.0, 2.0, 0.3907755687023597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584580.0612230237, 584580.0612230243, 173168.24205628], 
processed observation next is [0.0, 0.8260869565217391, 0.31279620853080575, 0.8933333333333333, 1.0, 1.0, 0.26599466108718034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1623833503397288, 0.16238335033972898, 0.25846006277056716], 
reward next is 0.7415, 
noisyNet noise sample is [array([-2.0700173], dtype=float32), 0.055522975]. 
=============================================
[2019-03-27 08:26:33,452] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.4408421e-18 1.0000000e+00 3.0822228e-25 2.1343506e-17 1.3053466e-29], sum to 1.0000
[2019-03-27 08:26:33,461] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2802
[2019-03-27 08:26:33,465] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 74.83333333333333, 1.0, 2.0, 0.4201252165336955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 612031.5313201292, 612031.5313201298, 175240.9820735452], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1446600.0000, 
sim time next is 1447200.0000, 
raw observation next is [26.1, 76.0, 1.0, 2.0, 0.4176003942456843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 609794.8782963161, 609794.8782963167, 175070.5709150185], 
processed observation next is [0.0, 0.782608695652174, 0.4360189573459717, 0.76, 1.0, 1.0, 0.2983137280068485, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16938746619342115, 0.16938746619342132, 0.2612993595746545], 
reward next is 0.7387, 
noisyNet noise sample is [array([-0.00742062], dtype=float32), -1.8195741]. 
=============================================
[2019-03-27 08:26:35,115] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6824824e-13 9.9999988e-01 1.5745090e-18 8.3461394e-08 1.6768905e-22], sum to 1.0000
[2019-03-27 08:26:35,125] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1609
[2019-03-27 08:26:35,132] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 85.16666666666667, 1.0, 2.0, 0.886336154583685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1238831.934994465, 1238831.934994465, 266203.9479863754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1860600.0000, 
sim time next is 1861200.0000, 
raw observation next is [26.5, 85.0, 1.0, 2.0, 0.7470040219412213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1043991.693378904, 1043991.693378904, 231315.5883647308], 
processed observation next is [1.0, 0.5652173913043478, 0.4549763033175356, 0.85, 1.0, 1.0, 0.695185568603881, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2899976926052511, 0.2899976926052511, 0.34524714681303104], 
reward next is 0.6548, 
noisyNet noise sample is [array([-0.10311161], dtype=float32), -0.14246824]. 
=============================================
[2019-03-27 08:26:39,566] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4872876e-17 1.0000000e+00 2.6645952e-24 3.2643218e-15 5.1273666e-26], sum to 1.0000
[2019-03-27 08:26:39,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8163
[2019-03-27 08:26:39,579] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 85.0, 1.0, 2.0, 0.6278567526424211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958441.4162134654, 958441.4162134654, 216058.5645501286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1586400.0000, 
sim time next is 1587000.0000, 
raw observation next is [23.48333333333333, 85.0, 1.0, 2.0, 0.6512030949133567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 993527.5107505558, 993527.5107505564, 221069.339980343], 
processed observation next is [1.0, 0.34782608695652173, 0.3120063191153238, 0.85, 1.0, 1.0, 0.5797627649558514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2759798640973766, 0.2759798640973768, 0.3299542387766314], 
reward next is 0.6700, 
noisyNet noise sample is [array([-0.35083026], dtype=float32), -0.62180215]. 
=============================================
[2019-03-27 08:26:39,593] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.14224]
 [70.7069 ]
 [71.18036]
 [71.52039]
 [71.48835]], R is [[69.6494751 ]
 [69.63050079]
 [69.63259125]
 [69.65524292]
 [69.69783783]].
[2019-03-27 08:26:46,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1145385e-16 1.0000000e+00 1.8307937e-19 2.1744569e-08 7.9972131e-24], sum to 1.0000
[2019-03-27 08:26:46,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9416
[2019-03-27 08:26:46,974] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.31666666666667, 77.33333333333333, 1.0, 2.0, 0.4976794542121823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695428.65238503, 695428.6523850306, 183234.7198751005], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1705800.0000, 
sim time next is 1706400.0000, 
raw observation next is [28.2, 78.0, 1.0, 2.0, 0.5045633154050175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705050.9564022244, 705050.956402225, 184315.1357676586], 
processed observation next is [1.0, 0.782608695652174, 0.5355450236966824, 0.78, 1.0, 1.0, 0.4030883318132741, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19584748788950676, 0.19584748788950693, 0.27509721756366956], 
reward next is 0.7249, 
noisyNet noise sample is [array([1.206454], dtype=float32), 0.29965216]. 
=============================================
[2019-03-27 08:26:50,054] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5796608e-16 1.0000000e+00 1.7131450e-21 3.1622472e-12 3.9211507e-25], sum to 1.0000
[2019-03-27 08:26:50,062] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2792
[2019-03-27 08:26:50,076] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 96.0, 1.0, 2.0, 0.691170751088145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965925.2059823722, 965925.2059823722, 218951.8724616951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2517600.0000, 
sim time next is 2518200.0000, 
raw observation next is [26.3, 96.0, 1.0, 2.0, 0.6858248433928761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 958450.8197995113, 958450.8197995106, 217816.1786139738], 
processed observation next is [1.0, 0.13043478260869565, 0.4454976303317536, 0.96, 1.0, 1.0, 0.621475714931176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2662363388331976, 0.2662363388331974, 0.3250987740507072], 
reward next is 0.6749, 
noisyNet noise sample is [array([-1.3992207], dtype=float32), -0.8093612]. 
=============================================
[2019-03-27 08:26:53,161] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2115042e-17 1.0000000e+00 1.3609909e-21 3.4109644e-12 6.4432467e-24], sum to 1.0000
[2019-03-27 08:26:53,167] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2213
[2019-03-27 08:26:53,175] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 90.0, 1.0, 2.0, 0.8642812292572144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1238442.543499014, 1238442.543499014, 264706.07610416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1850400.0000, 
sim time next is 1851000.0000, 
raw observation next is [24.71666666666667, 89.66666666666667, 1.0, 2.0, 0.8951863576699861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1278924.57930032, 1278924.57930032, 272699.238558078], 
processed observation next is [1.0, 0.43478260869565216, 0.3704581358609796, 0.8966666666666667, 1.0, 1.0, 0.8737185032168507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35525682758342225, 0.35525682758342225, 0.40701378889265377], 
reward next is 0.5930, 
noisyNet noise sample is [array([-1.1885022], dtype=float32), -0.98546576]. 
=============================================
[2019-03-27 08:26:53,202] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[67.011856]
 [67.10589 ]
 [67.35777 ]
 [67.573044]
 [67.81622 ]], R is [[66.65952301]
 [66.59784698]
 [66.52636719]
 [66.45907593]
 [66.39730072]].
[2019-03-27 08:26:53,958] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6424065e-10 9.9921787e-01 1.0841904e-14 7.8206067e-04 2.0141795e-19], sum to 1.0000
[2019-03-27 08:26:53,966] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7431
[2019-03-27 08:26:53,973] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2026100.733961663 W.
[2019-03-27 08:26:53,982] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.1, 76.0, 1.0, 2.0, 0.7245363103086147, 1.0, 1.0, 0.7245363103086147, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2026100.733961663, 2026100.733961663, 384804.0116007682], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2548800.0000, 
sim time next is 2549400.0000, 
raw observation next is [29.21666666666667, 75.16666666666667, 1.0, 2.0, 0.7827601102308145, 1.0, 2.0, 0.7827601102308145, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2189084.726835355, 2189084.726835356, 411562.7713873102], 
processed observation next is [1.0, 0.5217391304347826, 0.5837282780410744, 0.7516666666666667, 1.0, 1.0, 0.738265193049174, 1.0, 1.0, 0.738265193049174, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6080790907875987, 0.6080790907875989, 0.6142727931153883], 
reward next is 0.3857, 
noisyNet noise sample is [array([0.6364809], dtype=float32), -1.7238668]. 
=============================================
[2019-03-27 08:26:54,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4866566e-17 1.0000000e+00 7.2558976e-24 7.3224143e-15 1.5723429e-25], sum to 1.0000
[2019-03-27 08:26:54,643] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1621
[2019-03-27 08:26:54,649] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 95.16666666666667, 1.0, 2.0, 0.3628187414379867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558332.0404360032, 558332.0404360032, 171333.4046020497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1825800.0000, 
sim time next is 1826400.0000, 
raw observation next is [21.9, 95.33333333333334, 1.0, 2.0, 0.3455716688716803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531477.8385304972, 531477.8385304979, 169093.2739835533], 
processed observation next is [1.0, 0.13043478260869565, 0.23696682464454974, 0.9533333333333335, 1.0, 1.0, 0.21153213117069913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1476327329251381, 0.1476327329251383, 0.2523780208709751], 
reward next is 0.7476, 
noisyNet noise sample is [array([1.1791096], dtype=float32), -0.10101669]. 
=============================================
[2019-03-27 08:26:55,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1417317e-17 1.0000000e+00 1.0581737e-24 2.0874333e-15 3.7581937e-26], sum to 1.0000
[2019-03-27 08:26:55,872] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6803
[2019-03-27 08:26:55,877] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 90.33333333333333, 1.0, 2.0, 0.444090748929569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636703.5922903854, 636703.5922903861, 177381.4598196742], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2616000.0000, 
sim time next is 2616600.0000, 
raw observation next is [24.78333333333333, 89.66666666666667, 1.0, 2.0, 0.4503228765350923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642213.2562571305, 642213.2562571305, 177850.370632033], 
processed observation next is [0.0, 0.2608695652173913, 0.37361769352290675, 0.8966666666666667, 1.0, 1.0, 0.33773840546396655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17839257118253626, 0.17839257118253626, 0.26544831437616867], 
reward next is 0.7346, 
noisyNet noise sample is [array([0.05832662], dtype=float32), -1.9251833]. 
=============================================
[2019-03-27 08:27:04,620] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5524064e-11 4.3161181e-01 5.4440811e-15 5.6838822e-01 1.5667359e-18], sum to 1.0000
[2019-03-27 08:27:04,628] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1123
[2019-03-27 08:27:04,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2406504.115592012 W.
[2019-03-27 08:27:04,644] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.2, 64.0, 1.0, 2.0, 0.8604275326200709, 1.0, 2.0, 0.8604275326200709, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2406504.115592012, 2406504.115592012, 450363.1790297951], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2372400.0000, 
sim time next is 2373000.0000, 
raw observation next is [32.23333333333333, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.168611390361627, 6.9112, 168.9113470114156, 2466582.065755346, 2283967.306090678, 475520.9072624546], 
processed observation next is [1.0, 0.4782608695652174, 0.7266982622432857, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.025741139036162687, 0.0, 0.8294320417753052, 0.6851616849320405, 0.6344353628029661, 0.7097326974066487], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3633881], dtype=float32), 0.5180277]. 
=============================================
[2019-03-27 08:27:04,658] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[52.68482 ]
 [52.314346]
 [54.501026]
 [53.28154 ]
 [50.740208]], R is [[52.34206009]
 [51.81864166]
 [51.58556366]
 [51.40916824]
 [51.30821228]].
[2019-03-27 08:27:08,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8227270e-17 1.0000000e+00 1.2144351e-22 4.9512635e-13 2.0643866e-25], sum to 1.0000
[2019-03-27 08:27:08,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8006
[2019-03-27 08:27:08,947] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 91.16666666666667, 1.0, 2.0, 0.6528246097881507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 912312.6748109472, 912312.6748109466, 210990.3965070712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2182200.0000, 
sim time next is 2182800.0000, 
raw observation next is [26.2, 90.33333333333334, 1.0, 2.0, 0.6942852340193061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 970279.7476539674, 970279.7476539667, 219615.0888536948], 
processed observation next is [1.0, 0.2608695652173913, 0.44075829383886256, 0.9033333333333334, 1.0, 1.0, 0.6316689566497663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26952215212610203, 0.26952215212610187, 0.3277837147070071], 
reward next is 0.6722, 
noisyNet noise sample is [array([-0.6001665], dtype=float32), 0.34588343]. 
=============================================
[2019-03-27 08:27:11,139] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6309224e-19 1.0000000e+00 6.6650661e-25 9.8051983e-16 3.0401927e-27], sum to 1.0000
[2019-03-27 08:27:11,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4055
[2019-03-27 08:27:11,152] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.05, 76.83333333333334, 1.0, 2.0, 0.5691461926551131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795329.5492814106, 795329.5492814106, 195139.2908913219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2124600.0000, 
sim time next is 2125200.0000, 
raw observation next is [30.1, 76.66666666666667, 1.0, 2.0, 0.5696814698117408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796077.8302688397, 796077.8302688404, 195234.1858322718], 
processed observation next is [0.0, 0.6086956521739131, 0.6255924170616115, 0.7666666666666667, 1.0, 1.0, 0.48154393953221786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22113273063023325, 0.22113273063023345, 0.29139430721234594], 
reward next is 0.7086, 
noisyNet noise sample is [array([0.87907004], dtype=float32), -0.6567805]. 
=============================================
[2019-03-27 08:27:19,922] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3656125e-16 1.0000000e+00 2.3115132e-22 4.5053981e-14 8.3578204e-26], sum to 1.0000
[2019-03-27 08:27:19,931] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9221
[2019-03-27 08:27:19,940] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3045115160978191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 484916.7636848349, 484916.7636848342, 165975.4354292633], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3015000.0000, 
sim time next is 3015600.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.304628873137007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485103.6508356546, 485103.6508356546, 165988.9401123832], 
processed observation next is [1.0, 0.9130434782608695, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16220346161085178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13475101412101517, 0.13475101412101517, 0.2477446867349003], 
reward next is 0.7523, 
noisyNet noise sample is [array([1.0043108], dtype=float32), 0.33047384]. 
=============================================
[2019-03-27 08:27:20,881] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.9325506e-10 9.8022151e-01 5.8846537e-14 1.9778466e-02 3.8595973e-17], sum to 1.0000
[2019-03-27 08:27:20,894] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1091
[2019-03-27 08:27:20,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1940685.515503744 W.
[2019-03-27 08:27:20,908] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.7, 63.0, 1.0, 2.0, 0.746846400509211, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.993915777005796, 6.9112, 168.9124644311731, 1940685.515503744, 1882004.267389913, 395592.1116899138], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2284800.0000, 
sim time next is 2285400.0000, 
raw observation next is [31.9, 62.5, 1.0, 2.0, 0.80460964833794, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.993028026947324, 6.9112, 168.9124025204879, 2021524.818780294, 1963473.390554522, 409470.4652642377], 
processed observation next is [1.0, 0.43478260869565216, 0.7109004739336492, 0.625, 1.0, 1.0, 0.7645899377565543, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008182802694732417, 0.0, 0.8294372248080643, 0.561534671883415, 0.5454092751540339, 0.6111499481555787], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0742545], dtype=float32), -1.6172177]. 
=============================================
[2019-03-27 08:27:21,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2835039e-09 7.0455933e-01 4.2990595e-14 2.9544070e-01 1.1894569e-19], sum to 1.0000
[2019-03-27 08:27:21,088] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0626
[2019-03-27 08:27:21,097] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2349110.924547524 W.
[2019-03-27 08:27:21,102] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.86666666666667, 65.0, 1.0, 2.0, 0.8399262962247587, 1.0, 2.0, 0.8399262962247587, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2349110.924547524, 2349110.924547524, 439777.9661499332], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2298000.0000, 
sim time next is 2298600.0000, 
raw observation next is [31.9, 65.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.341380017236858, 6.9112, 168.9105467405358, 2590853.38951398, 2285673.233498991, 475239.053546845], 
processed observation next is [1.0, 0.6086956521739131, 0.7109004739336492, 0.65, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.04301800172368582, 0.0, 0.8294281120789199, 0.7196814970872167, 0.6349092315274975, 0.7093120202191716], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.96452373], dtype=float32), -0.16301091]. 
=============================================
[2019-03-27 08:27:22,444] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5421617e-16 1.0000000e+00 8.2337515e-23 5.0904910e-12 1.3767263e-25], sum to 1.0000
[2019-03-27 08:27:22,455] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5521
[2019-03-27 08:27:22,462] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 98.0, 1.0, 2.0, 0.6486595879903222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 974129.8355814173, 974129.8355814167, 218717.7123971211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3061200.0000, 
sim time next is 3061800.0000, 
raw observation next is [22.5, 97.0, 1.0, 2.0, 0.6609350207209435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 990986.3569492468, 990986.3569492475, 221214.5477177738], 
processed observation next is [1.0, 0.43478260869565216, 0.2654028436018958, 0.97, 1.0, 1.0, 0.591487976772221, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2752739880414575, 0.27527398804145764, 0.33017096674294594], 
reward next is 0.6698, 
noisyNet noise sample is [array([0.74393296], dtype=float32), -2.0267951]. 
=============================================
[2019-03-27 08:27:23,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.61719527e-16 1.00000000e+00 5.04206979e-23 1.07496356e-11
 4.76176102e-26], sum to 1.0000
[2019-03-27 08:27:23,230] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4735
[2019-03-27 08:27:23,235] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.95, 77.5, 1.0, 2.0, 0.5752929894162296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803922.383998383, 803922.383998383, 196233.4834917669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2320200.0000, 
sim time next is 2320800.0000, 
raw observation next is [29.86666666666667, 77.66666666666667, 1.0, 2.0, 0.5745063385012651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802822.6916874194, 802822.6916874194, 196092.6855707108], 
processed observation next is [1.0, 0.8695652173913043, 0.6145339652448659, 0.7766666666666667, 1.0, 1.0, 0.48735703433887356, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2230063032465054, 0.2230063032465054, 0.29267565010553853], 
reward next is 0.7073, 
noisyNet noise sample is [array([-0.43505082], dtype=float32), 0.4761109]. 
=============================================
[2019-03-27 08:27:25,335] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 08:27:25,336] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:27:25,337] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:27:25,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:27:25,338] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:27:25,339] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:27:25,339] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:27:25,339] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:27:25,340] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:27:25,339] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:27:25,344] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:27:25,368] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run80
[2019-03-27 08:27:25,381] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run80
[2019-03-27 08:27:25,397] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run80
[2019-03-27 08:27:25,415] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run80
[2019-03-27 08:27:25,431] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run80
[2019-03-27 08:27:38,095] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07931116], dtype=float32), 0.048110608]
[2019-03-27 08:27:38,096] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.21702311, 61.59341714, 1.0, 2.0, 0.2798044281530094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 453577.809103294, 453577.8091032946, 163823.20143921]
[2019-03-27 08:27:38,098] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:27:38,101] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.0911232e-18 1.0000000e+00 2.0232622e-24 1.4563115e-17 4.9489717e-27], sampled 0.3168258419129376
[2019-03-27 08:27:39,827] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07931116], dtype=float32), 0.048110608]
[2019-03-27 08:27:39,828] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.57374767833333, 93.70500456333335, 1.0, 2.0, 0.3507765026424214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544907.3573852736, 544907.3573852743, 170343.4118023678]
[2019-03-27 08:27:39,828] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:27:39,831] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5890945e-17 1.0000000e+00 2.0769401e-23 1.9178103e-15 2.9080649e-26], sampled 0.9730938676883965
[2019-03-27 08:28:30,911] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07931116], dtype=float32), 0.048110608]
[2019-03-27 08:28:30,912] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.797883635, 71.976893855, 1.0, 2.0, 0.6504453248240265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 908986.2365926445, 908986.2365926451, 210520.6550591639]
[2019-03-27 08:28:30,913] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:28:30,915] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.7332057e-17 1.0000000e+00 2.7249152e-23 1.8973779e-14 2.1665629e-26], sampled 0.9534530242935708
[2019-03-27 08:28:38,431] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07931116], dtype=float32), 0.048110608]
[2019-03-27 08:28:38,434] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.02439718333333, 70.8035426, 1.0, 2.0, 0.8549661088193095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1194961.352367655, 1194961.352367655, 257858.236148958]
[2019-03-27 08:28:38,436] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:28:38,439] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1457925e-16 1.0000000e+00 1.3429347e-22 1.2579048e-13 1.1877181e-25], sampled 0.09593265765489989
[2019-03-27 08:28:53,498] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07931116], dtype=float32), 0.048110608]
[2019-03-27 08:28:53,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.0, 80.83333333333334, 1.0, 2.0, 0.617857969467958, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.954309657537559, 6.9112, 168.9126363819221, 1727557.394840662, 1696973.97802626, 368135.1838151357]
[2019-03-27 08:28:53,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:28:53,504] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1814510e-13 1.0000000e+00 2.5641569e-18 5.5447963e-08 1.6721327e-21], sampled 0.3650388305521931
[2019-03-27 08:28:53,506] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1727557.394840662 W.
[2019-03-27 08:28:55,086] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07931116], dtype=float32), 0.048110608]
[2019-03-27 08:28:55,087] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.88333333333334, 61.16666666666667, 1.0, 2.0, 0.7777679479980708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1087008.492002412, 1087008.492002412, 238529.706753104]
[2019-03-27 08:28:55,089] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:28:55,091] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.3828902e-16 1.0000000e+00 6.5570644e-22 1.5766507e-12 2.0295844e-25], sampled 0.0006961753281521377
[2019-03-27 08:29:06,150] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07931116], dtype=float32), 0.048110608]
[2019-03-27 08:29:06,152] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.55554269, 58.40683691, 1.0, 2.0, 0.4416642283374638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 637339.3420464537, 637339.3420464537, 177552.4928110014]
[2019-03-27 08:29:06,153] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:29:06,155] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8001314e-18 1.0000000e+00 1.3683237e-24 9.8697020e-17 2.1792937e-27], sampled 0.7054290460968625
[2019-03-27 08:29:20,771] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7915.8627 3161126389.1585 1702.0000
[2019-03-27 08:29:20,785] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8261.0114 2926907519.7145 1320.0000
[2019-03-27 08:29:20,841] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8504.6850 2841805996.0789 1117.0000
[2019-03-27 08:29:21,027] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2126 2779308430.3584 932.0000
[2019-03-27 08:29:21,139] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8006.9893 3006256132.8030 1736.0000
[2019-03-27 08:29:22,159] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1975000, evaluation results [1975000.0, 7915.862684250322, 3161126389.158522, 1702.0, 8261.01137113911, 2926907519.714461, 1320.0, 8659.212639998108, 2779308430.3584, 932.0, 8006.989325400407, 3006256132.8030105, 1736.0, 8504.68504094935, 2841805996.0789433, 1117.0]
[2019-03-27 08:29:23,062] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0404908e-10 2.0307277e-01 3.6310381e-14 7.9692721e-01 8.7085955e-18], sum to 1.0000
[2019-03-27 08:29:23,075] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6014
[2019-03-27 08:29:23,085] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.3, 64.0, 1.0, 2.0, 0.8747993816966769, 1.0, 2.0, 0.8747993816966769, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2446739.664603344, 2446739.664603344, 457930.9353785737], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2374200.0000, 
sim time next is 2374800.0000, 
raw observation next is [32.33333333333334, 64.0, 1.0, 2.0, 0.8743335742484046, 1.0, 2.0, 0.8743335742484046, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2445435.565856071, 2445435.565856071, 457683.9913074456], 
processed observation next is [1.0, 0.4782608695652174, 0.7314375987361774, 0.64, 1.0, 1.0, 0.8485946677691621, 1.0, 1.0, 0.8485946677691621, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.679287657182242, 0.679287657182242, 0.6831104347872323], 
reward next is 0.3169, 
noisyNet noise sample is [array([-2.114815], dtype=float32), -0.39914563]. 
=============================================
[2019-03-27 08:29:25,707] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0899129e-15 1.0000000e+00 6.8323834e-21 9.3989934e-11 7.3336746e-24], sum to 1.0000
[2019-03-27 08:29:25,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7543
[2019-03-27 08:29:25,723] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 96.0, 1.0, 2.0, 0.6481873136697233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 905829.3588800053, 905829.358880006, 210059.9434906598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2519400.0000, 
sim time next is 2520000.0000, 
raw observation next is [26.3, 96.0, 1.0, 2.0, 0.6430358996279928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 898627.3108131714, 898627.310813172, 209030.992515251], 
processed observation next is [1.0, 0.17391304347826086, 0.4454976303317536, 0.96, 1.0, 1.0, 0.5699227706361358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24961869744810317, 0.24961869744810333, 0.311986555992912], 
reward next is 0.6880, 
noisyNet noise sample is [array([-0.30392557], dtype=float32), 1.1566105]. 
=============================================
[2019-03-27 08:29:25,738] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[66.21624]
 [66.01005]
 [65.81408]
 [65.62035]
 [65.30389]], R is [[66.11464691]
 [66.1399765 ]
 [66.16187286]
 [66.17515564]
 [66.18660736]].
[2019-03-27 08:29:27,761] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8893075e-17 1.0000000e+00 2.5243695e-22 1.8048941e-11 2.3142170e-26], sum to 1.0000
[2019-03-27 08:29:27,768] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7098
[2019-03-27 08:29:27,771] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.412121919392743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607224.7099600784, 607224.7099600784, 174988.1797634892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2831400.0000, 
sim time next is 2832000.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4140642209017884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610085.4948599548, 610085.4948599555, 175257.3911826799], 
processed observation next is [1.0, 0.782608695652174, 0.3364928909952607, 0.89, 1.0, 1.0, 0.29405327819492577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1694681930166541, 0.1694681930166543, 0.2615781957950446], 
reward next is 0.7384, 
noisyNet noise sample is [array([-1.3465184], dtype=float32), 0.85078007]. 
=============================================
[2019-03-27 08:29:27,794] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.460464]
 [69.324066]
 [69.1115  ]
 [68.872635]
 [68.5838  ]], R is [[69.59617615]
 [69.63903809]
 [69.68186951]
 [69.72465515]
 [69.76809692]].
[2019-03-27 08:29:28,224] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7537076e-17 1.0000000e+00 5.2251729e-23 1.3687843e-11 4.0192517e-27], sum to 1.0000
[2019-03-27 08:29:28,236] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3733
[2019-03-27 08:29:28,242] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 93.0, 1.0, 2.0, 0.5543193037997713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774602.7785973914, 774602.7785973914, 192544.4804045955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2491200.0000, 
sim time next is 2491800.0000, 
raw observation next is [26.98333333333333, 93.0, 1.0, 2.0, 0.5552882462485712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775957.2680658587, 775957.2680658587, 192711.916653557], 
processed observation next is [1.0, 0.8695652173913043, 0.4778830963665086, 0.93, 1.0, 1.0, 0.46420270632357974, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21554368557384965, 0.21554368557384965, 0.2876297263485926], 
reward next is 0.7124, 
noisyNet noise sample is [array([0.434462], dtype=float32), 2.0115843]. 
=============================================
[2019-03-27 08:29:30,286] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2714585e-17 1.0000000e+00 2.9814190e-23 8.7939892e-15 2.1411887e-28], sum to 1.0000
[2019-03-27 08:29:30,296] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2166
[2019-03-27 08:29:30,300] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.5809147418937404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811781.300739505, 811781.300739505, 197244.8818838761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3254400.0000, 
sim time next is 3255000.0000, 
raw observation next is [32.83333333333334, 64.33333333333333, 1.0, 2.0, 0.5874568731785758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820926.9322557616, 820926.9322557609, 198432.8783152514], 
processed observation next is [0.0, 0.6956521739130435, 0.7551342812006324, 0.6433333333333333, 1.0, 1.0, 0.5029600881669587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22803525895993376, 0.22803525895993357, 0.29616847509739014], 
reward next is 0.7038, 
noisyNet noise sample is [array([-1.8601888], dtype=float32), 0.21696629]. 
=============================================
[2019-03-27 08:29:30,316] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.10944 ]
 [74.01496 ]
 [73.979324]
 [73.9347  ]
 [73.88172 ]], R is [[74.11252594]
 [74.07700348]
 [74.04180908]
 [74.00701904]
 [73.97268677]].
[2019-03-27 08:29:33,883] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8505178e-18 1.0000000e+00 9.3418817e-24 4.6055722e-16 6.8718059e-27], sum to 1.0000
[2019-03-27 08:29:33,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2242
[2019-03-27 08:29:33,901] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 72.66666666666667, 1.0, 2.0, 0.5287594422156511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738873.1446716838, 738873.1446716838, 188225.5981751533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3320400.0000, 
sim time next is 3321000.0000, 
raw observation next is [30.0, 72.0, 1.0, 2.0, 0.5350461803463259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747661.1423160979, 747661.1423160972, 189270.173455255], 
processed observation next is [0.0, 0.43478260869565216, 0.6208530805687204, 0.72, 1.0, 1.0, 0.43981467511605526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20768365064336053, 0.20768365064336033, 0.2824927962018731], 
reward next is 0.7175, 
noisyNet noise sample is [array([-0.19244225], dtype=float32), 0.04639482]. 
=============================================
[2019-03-27 08:29:33,916] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.68919]
 [73.68332]
 [73.64656]
 [73.57502]
 [73.56514]], R is [[73.66768646]
 [73.65007019]
 [73.63453674]
 [73.62017822]
 [73.60683441]].
[2019-03-27 08:29:38,035] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7486433e-18 1.0000000e+00 1.1478576e-23 3.0982337e-16 9.5580013e-28], sum to 1.0000
[2019-03-27 08:29:38,043] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8783
[2019-03-27 08:29:38,051] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 96.0, 1.0, 2.0, 0.4352432743349033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 627590.2333733159, 627590.2333733152, 176570.8904051139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2684400.0000, 
sim time next is 2685000.0000, 
raw observation next is [23.83333333333333, 95.0, 1.0, 2.0, 0.4375903340815071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 629907.8198955805, 629907.8198955805, 176771.0168835794], 
processed observation next is [0.0, 0.043478260869565216, 0.32859399684044216, 0.95, 1.0, 1.0, 0.3223979928692857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17497439441543902, 0.17497439441543902, 0.26383733863220804], 
reward next is 0.7362, 
noisyNet noise sample is [array([0.82477736], dtype=float32), -0.4941326]. 
=============================================
[2019-03-27 08:29:38,063] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.95765 ]
 [74.062546]
 [74.108536]
 [74.16969 ]
 [74.26838 ]], R is [[73.87463379]
 [73.87234497]
 [73.87029266]
 [73.86855316]
 [73.86721802]].
[2019-03-27 08:29:42,797] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6680394e-18 1.0000000e+00 2.7349216e-24 2.7557507e-17 1.7224110e-28], sum to 1.0000
[2019-03-27 08:29:42,806] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4195
[2019-03-27 08:29:42,817] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 100.0, 1.0, 2.0, 0.422891704354851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616621.4234102467, 616621.4234102473, 175696.9767287749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2715000.0000, 
sim time next is 2715600.0000, 
raw observation next is [22.66666666666667, 100.0, 1.0, 2.0, 0.4157842069799606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610144.6145788797, 610144.6145788797, 175191.8476672653], 
processed observation next is [0.0, 0.43478260869565216, 0.27330173775671435, 1.0, 1.0, 1.0, 0.2961255505782658, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1694846151607999, 0.1694846151607999, 0.26148036965263477], 
reward next is 0.7385, 
noisyNet noise sample is [array([-0.47771344], dtype=float32), -1.5781423]. 
=============================================
[2019-03-27 08:29:56,711] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3536940e-18 1.0000000e+00 1.5012976e-23 8.1536397e-16 3.7330322e-26], sum to 1.0000
[2019-03-27 08:29:56,719] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0523
[2019-03-27 08:29:56,725] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 96.0, 1.0, 2.0, 0.3134351607399103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493290.6173517926, 493290.6173517926, 166472.5207403959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2960400.0000, 
sim time next is 2961000.0000, 
raw observation next is [21.0, 97.0, 1.0, 2.0, 0.3152768574074239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494738.7411713288, 494738.7411713288, 166542.4331262729], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 0.97, 1.0, 1.0, 0.17503235832219746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1374274281031469, 0.1374274281031469, 0.24857079571085508], 
reward next is 0.7514, 
noisyNet noise sample is [array([0.8276645], dtype=float32), -1.540876]. 
=============================================
[2019-03-27 08:29:56,747] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.405174]
 [73.36503 ]
 [73.37945 ]
 [73.28365 ]
 [73.235374]], R is [[73.47756958]
 [73.49433136]
 [73.50849915]
 [73.5256958 ]
 [73.5426712 ]].
[2019-03-27 08:29:58,781] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0642213e-17 1.0000000e+00 1.9648629e-23 4.9873014e-16 7.6166493e-26], sum to 1.0000
[2019-03-27 08:29:58,788] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5663
[2019-03-27 08:29:58,795] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3370668014896229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 523520.2192488816, 523520.2192488823, 168611.7350252176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3046800.0000, 
sim time next is 3047400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3338099701619507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518466.0327573584, 518466.0327573584, 168212.4646880443], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 1.0, 1.0, 1.0, 0.19736140983367553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14401834243259956, 0.14401834243259956, 0.2510633801314094], 
reward next is 0.7489, 
noisyNet noise sample is [array([-0.3801037], dtype=float32), 0.12143394]. 
=============================================
[2019-03-27 08:30:05,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6364145e-18 1.0000000e+00 1.6172503e-24 4.2055343e-15 7.1330737e-27], sum to 1.0000
[2019-03-27 08:30:05,408] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8538
[2019-03-27 08:30:05,414] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.3958722029835822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 609819.8823658074, 609819.8823658074, 175881.7781585366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3136200.0000, 
sim time next is 3136800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3788639691754646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 583606.3098482878, 583606.3098482878, 173538.625778144], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.94, 1.0, 1.0, 0.25164333635598146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1621128638467466, 0.1621128638467466, 0.2590128742957373], 
reward next is 0.7410, 
noisyNet noise sample is [array([0.04263608], dtype=float32), -1.7438143]. 
=============================================
[2019-03-27 08:30:05,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7084289e-19 1.0000000e+00 8.8770555e-26 1.4121895e-17 1.0520135e-28], sum to 1.0000
[2019-03-27 08:30:05,786] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7989
[2019-03-27 08:30:05,789] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.4756729766528394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664668.4457758405, 664668.44577584, 179872.4325457314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3225000.0000, 
sim time next is 3225600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.476802473447324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666247.2117308204, 666247.211730821, 180041.4710373539], 
processed observation next is [0.0, 0.34782608695652173, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3696415342738844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18506866992522789, 0.18506866992522808, 0.26871861348858794], 
reward next is 0.7313, 
noisyNet noise sample is [array([-0.9263382], dtype=float32), 0.8332966]. 
=============================================
[2019-03-27 08:30:06,259] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2048028e-18 1.0000000e+00 1.1208820e-24 2.5451578e-15 4.2118181e-27], sum to 1.0000
[2019-03-27 08:30:06,269] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6180
[2019-03-27 08:30:06,274] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3871935551678807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596439.9825081596, 596439.9825081596, 174684.215109048], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3137400.0000, 
sim time next is 3138000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3944646639552937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607638.0174091477, 607638.0174091477, 175684.5971737737], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.94, 1.0, 1.0, 0.27043935416300446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16878833816920769, 0.16878833816920769, 0.26221581667727417], 
reward next is 0.7378, 
noisyNet noise sample is [array([1.4193392], dtype=float32), -1.2202739]. 
=============================================
[2019-03-27 08:30:06,288] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.90602]
 [70.92583]
 [70.93204]
 [71.03786]
 [70.91972]], R is [[70.86143494]
 [70.8921051 ]
 [70.92417145]
 [70.95241547]
 [70.9879837 ]].
[2019-03-27 08:30:10,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9351907e-14 1.0000000e+00 2.6790926e-21 7.0105484e-12 9.4494059e-24], sum to 1.0000
[2019-03-27 08:30:10,641] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3765
[2019-03-27 08:30:10,646] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 93.66666666666667, 1.0, 2.0, 0.9263271978219202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104049, 1294761.493988137, 1294761.493988137, 277281.5064062881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3379800.0000, 
sim time next is 3380400.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.8665246254648504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1211125.569893053, 1211125.569893053, 260894.5689703742], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.94, 1.0, 1.0, 0.8391862957407836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33642376941473695, 0.33642376941473695, 0.38939487906026], 
reward next is 0.6106, 
noisyNet noise sample is [array([0.31212962], dtype=float32), 0.7343529]. 
=============================================
[2019-03-27 08:30:11,583] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3768587e-16 1.0000000e+00 1.8697356e-23 2.6612856e-15 2.9683184e-26], sum to 1.0000
[2019-03-27 08:30:11,593] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8038
[2019-03-27 08:30:11,601] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 77.33333333333333, 1.0, 2.0, 0.5156798263094226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720589.8685582536, 720589.8685582536, 186090.4566880261], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3231600.0000, 
sim time next is 3232200.0000, 
raw observation next is [28.83333333333334, 78.16666666666667, 1.0, 2.0, 0.5251002316093973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733758.1036003145, 733758.1036003145, 187623.5510522571], 
processed observation next is [0.0, 0.391304347826087, 0.5655608214849924, 0.7816666666666667, 1.0, 1.0, 0.42783160434867146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2038216954445318, 0.2038216954445318, 0.28003515082426433], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.88019234], dtype=float32), 0.84998405]. 
=============================================
[2019-03-27 08:30:15,143] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4337460e-18 1.0000000e+00 2.3302724e-23 4.0543997e-16 1.9254074e-27], sum to 1.0000
[2019-03-27 08:30:15,151] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7894
[2019-03-27 08:30:15,157] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4210486180772605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616494.8503476972, 616494.8503476972, 175757.6330680395], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3301800.0000, 
sim time next is 3302400.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4214444541305093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617074.541790927, 617074.541790927, 175813.1796244131], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.83, 1.0, 1.0, 0.302945125458445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17140959494192415, 0.17140959494192415, 0.26240773078270613], 
reward next is 0.7376, 
noisyNet noise sample is [array([-0.12907903], dtype=float32), 2.0359004]. 
=============================================
[2019-03-27 08:30:15,889] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 08:30:15,893] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:30:15,894] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:30:15,894] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:30:15,895] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:30:15,896] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:30:15,896] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:30:15,897] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:30:15,896] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:30:15,899] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:30:15,900] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:30:16,578] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run81
[2019-03-27 08:30:16,589] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run81
[2019-03-27 08:30:16,727] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run81
[2019-03-27 08:30:16,756] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run81
[2019-03-27 08:30:16,767] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run81
[2019-03-27 08:30:23,883] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07964427], dtype=float32), 0.048342124]
[2019-03-27 08:30:23,885] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.05, 90.16666666666666, 1.0, 2.0, 0.2663896637024622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 439063.387354731, 439063.3873547304, 162547.9275643638]
[2019-03-27 08:30:23,886] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:30:23,890] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3848022e-18 1.0000000e+00 9.1777695e-25 2.1890448e-17 1.6362928e-27], sampled 0.3235834263847559
[2019-03-27 08:30:28,164] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07964427], dtype=float32), 0.048342124]
[2019-03-27 08:30:28,168] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [15.993084455, 81.473429615, 1.0, 2.0, 0.1775421030461157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 296981.9616397497, 296981.9616397503, 115867.7926088872]
[2019-03-27 08:30:28,169] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:30:28,172] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9604149e-18 1.0000000e+00 1.1602805e-24 4.5694652e-18 4.1185854e-27], sampled 0.007647595213709613
[2019-03-27 08:30:38,579] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07964427], dtype=float32), 0.048342124]
[2019-03-27 08:30:38,580] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.680579955, 88.27558598499999, 1.0, 2.0, 0.3776282670482094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579743.6775730613, 579743.6775730619, 173152.2153068141]
[2019-03-27 08:30:38,581] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:30:38,585] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.9875527e-18 1.0000000e+00 2.9868702e-24 5.2495433e-17 6.0895224e-27], sampled 0.023031883605532433
[2019-03-27 08:30:52,147] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07964427], dtype=float32), 0.048342124]
[2019-03-27 08:30:52,148] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.83333333333334, 74.5, 1.0, 2.0, 0.781209157663075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1091820.399190727, 1091820.399190727, 239349.6336851854]
[2019-03-27 08:30:52,150] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:30:52,153] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7720336e-16 1.0000000e+00 2.0671795e-22 1.4534618e-13 2.0603114e-25], sampled 0.45712766031274044
[2019-03-27 08:30:57,784] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07964427], dtype=float32), 0.048342124]
[2019-03-27 08:30:57,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.23264858666667, 87.95178727500002, 1.0, 2.0, 0.4169069704936611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613097.6177798514, 613097.6177798514, 175509.2606081345]
[2019-03-27 08:30:57,788] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:30:57,793] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.1962375e-18 1.0000000e+00 1.4093483e-24 4.4534961e-17 2.0818937e-27], sampled 0.1771197266453408
[2019-03-27 08:31:11,165] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07964427], dtype=float32), 0.048342124]
[2019-03-27 08:31:11,167] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.2, 50.33333333333333, 1.0, 2.0, 0.9623212817544873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1345103.616518617, 1345103.616518618, 287658.173882579]
[2019-03-27 08:31:11,168] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:31:11,173] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4524476e-16 1.0000000e+00 4.7870140e-22 2.3460504e-12 1.5514065e-25], sampled 0.14682214085141587
[2019-03-27 08:31:22,408] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07964427], dtype=float32), 0.048342124]
[2019-03-27 08:31:22,411] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.93333333333333, 56.83333333333333, 1.0, 2.0, 0.7158204193374791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1000389.844773415, 1000389.844773415, 224305.0734456373]
[2019-03-27 08:31:22,412] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:31:22,415] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.6310125e-17 1.0000000e+00 2.4294491e-23 7.3453833e-15 2.7477813e-26], sampled 0.5959419741965939
[2019-03-27 08:32:02,781] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07964427], dtype=float32), 0.048342124]
[2019-03-27 08:32:02,786] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.13510488, 67.63861346333333, 1.0, 2.0, 0.7121839045058921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1086651.044471485, 1086651.044471485, 235157.3826850862]
[2019-03-27 08:32:02,789] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:32:02,792] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.02342485e-16 1.00000000e+00 1.12341732e-22 3.50523545e-14
 1.33430275e-25], sampled 0.748960106715725
[2019-03-27 08:32:09,725] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07964427], dtype=float32), 0.048342124]
[2019-03-27 08:32:09,727] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.3553802, 94.68753291, 1.0, 2.0, 0.5668345009385402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792097.9678507406, 792097.9678507406, 194730.7315517076]
[2019-03-27 08:32:09,729] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:32:09,732] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.5982648e-18 1.0000000e+00 2.3699401e-24 5.0834947e-16 1.6573881e-27], sampled 0.7395058449857256
[2019-03-27 08:32:13,213] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.6822 3006934479.2111 1746.0000
[2019-03-27 08:32:13,368] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.1021 2927044718.8542 1329.0000
[2019-03-27 08:32:13,394] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7909.9676 3161268558.4742 1704.0000
[2019-03-27 08:32:13,420] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.7750 2779185308.3295 931.0000
[2019-03-27 08:32:13,708] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.6438 2842066528.1889 1119.0000
[2019-03-27 08:32:14,727] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2000000, evaluation results [2000000.0, 7909.967633572679, 3161268558.474223, 1704.0, 8258.102069916216, 2927044718.8541775, 1329.0, 8661.774954330687, 2779185308.3295407, 931.0, 8002.68216263681, 3006934479.211082, 1746.0, 8500.643773987837, 2842066528.1888666, 1119.0]
[2019-03-27 08:32:25,210] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.0057861e-10 5.7834959e-01 4.4794358e-14 4.2165041e-01 1.6953485e-17], sum to 1.0000
[2019-03-27 08:32:25,222] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8342
[2019-03-27 08:32:25,226] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.7717149125046807, 1.0, 2.0, 0.7717149125046807, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2158164.357418783, 2158164.357418783, 406342.6368219682], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3500400.0000, 
sim time next is 3501000.0000, 
raw observation next is [32.5, 65.0, 1.0, 2.0, 0.7474018531795822, 1.0, 2.0, 0.7474018531795822, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2090104.54879036, 2090104.54879036, 395084.396269067], 
processed observation next is [1.0, 0.5217391304347826, 0.7393364928909952, 0.65, 1.0, 1.0, 0.6956648833488942, 1.0, 1.0, 0.6956648833488942, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5805845968862111, 0.5805845968862111, 0.5896782033866672], 
reward next is 0.4103, 
noisyNet noise sample is [array([-1.3360243], dtype=float32), -0.23384544]. 
=============================================
[2019-03-27 08:32:25,249] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[52.895065]
 [51.437904]
 [51.255486]
 [53.456078]
 [53.097492]], R is [[53.92432785]
 [53.7786026 ]
 [53.24081802]
 [53.03102875]
 [52.82555389]].
[2019-03-27 08:32:25,496] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6106672e-10 6.4259022e-01 8.2332074e-13 3.5740981e-01 7.9385855e-18], sum to 1.0000
[2019-03-27 08:32:25,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5925
[2019-03-27 08:32:25,513] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2640524.073583466 W.
[2019-03-27 08:32:25,517] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.00000000000001, 1.0, 2.0, 0.9440112990201459, 1.0, 2.0, 0.9440112990201459, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2640524.073583466, 2640524.073583467, 496074.4848203336], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3504000.0000, 
sim time next is 3504600.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.9873842317602702, 1.0, 2.0, 0.9873842317602702, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2761977.991839231, 2761977.991839231, 521409.8742680211], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.63, 1.0, 1.0, 0.9848002792292413, 1.0, 1.0, 0.9848002792292413, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7672161088442309, 0.7672161088442309, 0.778223692937345], 
reward next is 0.2218, 
noisyNet noise sample is [array([-1.1031997], dtype=float32), 1.4866787]. 
=============================================
[2019-03-27 08:32:32,813] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5827470e-14 9.9999964e-01 1.5849685e-20 4.1454300e-07 4.3212356e-25], sum to 1.0000
[2019-03-27 08:32:32,822] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8783
[2019-03-27 08:32:32,827] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 66.5, 1.0, 2.0, 0.5543957507269193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774709.6440743197, 774709.6440743197, 192558.1793052313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3609000.0000, 
sim time next is 3609600.0000, 
raw observation next is [31.33333333333333, 66.33333333333334, 1.0, 2.0, 0.5509519625397519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769895.5689331697, 769895.5689331697, 191964.598209349], 
processed observation next is [1.0, 0.782608695652174, 0.6840442338072668, 0.6633333333333334, 1.0, 1.0, 0.45897826812018294, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21385988025921382, 0.21385988025921382, 0.28651432568559554], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.6532182], dtype=float32), -0.8003687]. 
=============================================
[2019-03-27 08:32:38,280] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5760687e-17 1.0000000e+00 4.2079403e-23 5.0818904e-14 5.4473592e-27], sum to 1.0000
[2019-03-27 08:32:38,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3332
[2019-03-27 08:32:38,291] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.66666666666667, 58.33333333333334, 1.0, 2.0, 0.6080301507260163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 849688.0446379943, 849688.0446379943, 202251.8993186689], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3946800.0000, 
sim time next is 3947400.0000, 
raw observation next is [34.5, 59.5, 1.0, 2.0, 0.6041731759856634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 844295.9964301536, 844295.996430153, 201526.8635109771], 
processed observation next is [0.0, 0.6956521739130435, 0.8341232227488152, 0.595, 1.0, 1.0, 0.5231002120309198, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23452666567504266, 0.23452666567504252, 0.30078636344921955], 
reward next is 0.6992, 
noisyNet noise sample is [array([-0.9832562], dtype=float32), -0.29840556]. 
=============================================
[2019-03-27 08:32:41,403] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2234685e-11 1.6438307e-01 6.3137099e-15 8.3561695e-01 1.4433290e-19], sum to 1.0000
[2019-03-27 08:32:41,410] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9076
[2019-03-27 08:32:41,418] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.16666666666667, 65.83333333333334, 1.0, 2.0, 0.8521296077177928, 1.0, 2.0, 0.8521296077177928, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2383273.765484213, 2383273.765484213, 446064.543138886], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3762600.0000, 
sim time next is 3763200.0000, 
raw observation next is [34.33333333333334, 64.66666666666667, 1.0, 2.0, 0.9995241248613094, 1.0, 2.0, 0.9995241248613094, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2795974.524421774, 2795974.524421774, 528709.655531684], 
processed observation next is [1.0, 0.5652173913043478, 0.8262243285939973, 0.6466666666666667, 1.0, 1.0, 0.9994266564594089, 1.0, 1.0, 0.9994266564594089, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7766595901171593, 0.7766595901171593, 0.7891188888532598], 
reward next is 0.2109, 
noisyNet noise sample is [array([0.37090796], dtype=float32), -0.30861008]. 
=============================================
[2019-03-27 08:32:42,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3051508e-19 1.0000000e+00 2.9182694e-24 1.0784813e-15 4.4949457e-28], sum to 1.0000
[2019-03-27 08:32:42,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2741
[2019-03-27 08:32:42,931] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.6017386789956021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 840892.5852947794, 840892.58529478, 201070.2543914712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3963600.0000, 
sim time next is 3964200.0000, 
raw observation next is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.610584911719432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 853259.6139355367, 853259.6139355374, 202733.9930727872], 
processed observation next is [0.0, 0.9130434782608695, 0.6761453396524489, 0.7433333333333333, 1.0, 1.0, 0.5308251948426891, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.237016559426538, 0.23701655942653818, 0.302588049362369], 
reward next is 0.6974, 
noisyNet noise sample is [array([0.786214], dtype=float32), 0.20852882]. 
=============================================
[2019-03-27 08:32:53,470] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.88981594e-10 7.03211188e-01 1.08026787e-13 2.96788782e-01
 1.11433165e-17], sum to 1.0000
[2019-03-27 08:32:53,481] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0028
[2019-03-27 08:32:53,489] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1883433.520777391 W.
[2019-03-27 08:32:53,497] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.4490420870803418, 1.0, 1.0, 0.4490420870803418, 1.0, 2.0, 0.7663974141811588, 6.911200000000001, 6.9112, 170.5573041426782, 1883433.520777391, 1883433.520777391, 379526.1512912458], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4006800.0000, 
sim time next is 4007400.0000, 
raw observation next is [27.33333333333333, 82.33333333333333, 1.0, 2.0, 0.8422789389506785, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.97794780466012, 6.9112, 168.9125594962162, 2074246.070950715, 2026892.992341959, 419533.880891729], 
processed observation next is [1.0, 0.391304347826087, 0.494470774091627, 0.8233333333333333, 1.0, 1.0, 0.8099746252417813, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006674780466012021, 0.0, 0.8294379956307542, 0.5761794641529764, 0.5630258312060997, 0.6261699714801925], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38104716], dtype=float32), -1.7250326]. 
=============================================
[2019-03-27 08:32:56,919] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1402564e-11 9.9966264e-01 8.5993067e-16 3.3737163e-04 5.8852999e-18], sum to 1.0000
[2019-03-27 08:32:56,930] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9081
[2019-03-27 08:32:56,939] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2138145.069752628 W.
[2019-03-27 08:32:56,945] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5097090445901955, 1.0, 2.0, 0.5097090445901955, 1.0, 2.0, 0.8851960470963245, 6.9112, 6.9112, 170.5573041426782, 2138145.069752628, 2138145.069752628, 421925.1472324607], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4092000.0000, 
sim time next is 4092600.0000, 
raw observation next is [29.66666666666666, 79.83333333333334, 1.0, 2.0, 0.9506082917187195, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005990672409787, 6.9112, 168.9123931444886, 2225871.285584567, 2158623.744421615, 448466.6063867041], 
processed observation next is [1.0, 0.34782608695652173, 0.6050552922590835, 0.7983333333333335, 1.0, 1.0, 0.940491917733397, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009479067240978712, 0.0, 0.8294371787676154, 0.6182975793290464, 0.5996177067837819, 0.6693531438607524], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7554956], dtype=float32), 1.9791757]. 
=============================================
[2019-03-27 08:33:08,213] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 08:33:08,215] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:33:08,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:33:08,216] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:33:08,216] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:33:08,218] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:33:08,220] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:33:08,221] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:33:08,218] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:33:08,223] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:33:08,223] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:33:08,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run82
[2019-03-27 08:33:08,272] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run82
[2019-03-27 08:33:08,298] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run82
[2019-03-27 08:33:08,319] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run82
[2019-03-27 08:33:08,341] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run82
[2019-03-27 08:33:49,267] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08258368], dtype=float32), 0.04806263]
[2019-03-27 08:33:49,268] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.20143726833333, 89.36395275333334, 1.0, 2.0, 0.3196854241177085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 508149.7692672746, 508149.7692672741, 167679.8389375714]
[2019-03-27 08:33:49,269] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:33:49,273] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6175638e-18 1.0000000e+00 8.6137280e-25 2.8748887e-17 1.0058550e-27], sampled 0.7878804385408225
[2019-03-27 08:33:52,911] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.08258368], dtype=float32), 0.04806263]
[2019-03-27 08:33:52,911] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.38333333333333, 52.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.550807437239588, 6.9112, 168.9091933074823, 1907815.150650235, 1454065.730368294, 311354.2379922065]
[2019-03-27 08:33:52,912] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:33:52,915] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5571926e-14 1.0000000e+00 1.6979743e-19 1.9276925e-09 8.1103989e-23], sampled 0.7731684159423062
[2019-03-27 08:33:52,918] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1907815.150650235 W.
[2019-03-27 08:34:12,634] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.08258368], dtype=float32), 0.04806263]
[2019-03-27 08:34:12,635] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.86666666666667, 62.0, 1.0, 2.0, 0.6396839940254995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893941.131453299, 893941.131453299, 208370.610908574]
[2019-03-27 08:34:12,636] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:34:12,640] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3533728e-17 1.0000000e+00 1.1635078e-23 7.1967974e-16 1.7389574e-26], sampled 0.21567756874198685
[2019-03-27 08:34:18,640] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08258368], dtype=float32), 0.04806263]
[2019-03-27 08:34:18,643] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.04388931, 73.49793082, 1.0, 2.0, 0.770320206265518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1174186.767264524, 1174186.767264525, 249565.3232312451]
[2019-03-27 08:34:18,644] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:34:18,645] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4480363e-14 1.0000000e+00 2.7525751e-19 1.6542230e-09 3.7937343e-23], sampled 0.9051123219167265
[2019-03-27 08:34:38,911] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08258368], dtype=float32), 0.04806263]
[2019-03-27 08:34:38,912] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.33301969333333, 79.43400746666669, 1.0, 2.0, 0.4859236410237202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 689653.3168326166, 689653.3168326173, 182786.2502555551]
[2019-03-27 08:34:38,914] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:34:38,918] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4287542e-17 1.0000000e+00 1.8385585e-23 8.1466180e-15 1.4731341e-26], sampled 0.5400221504363794
[2019-03-27 08:35:03,531] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.1220 2927413994.2331 1335.0000
[2019-03-27 08:35:03,807] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7888.1153 3163621011.0227 1761.0000
[2019-03-27 08:35:03,918] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.4611 2842201953.3729 1127.0000
[2019-03-27 08:35:04,148] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7079 2779145200.9796 933.0000
[2019-03-27 08:35:04,227] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.6105 3007459386.0130 1765.0000
[2019-03-27 08:35:05,244] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2025000, evaluation results [2025000.0, 7888.115254509449, 3163621011.022683, 1761.0, 8252.121972389952, 2927413994.23307, 1335.0, 8660.70791380935, 2779145200.9795885, 933.0, 7999.610504761743, 3007459386.012983, 1765.0, 8499.461076338568, 2842201953.372852, 1127.0]
[2019-03-27 08:35:08,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5495247e-18 1.0000000e+00 5.4620214e-22 4.3460274e-13 3.8848776e-27], sum to 1.0000
[2019-03-27 08:35:08,386] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0519
[2019-03-27 08:35:08,392] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 68.33333333333334, 1.0, 2.0, 0.5427259812815993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758396.5493518121, 758396.5493518121, 190562.2585090613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4393200.0000, 
sim time next is 4393800.0000, 
raw observation next is [31.0, 71.0, 1.0, 2.0, 0.5569137909536337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778229.628973624, 778229.628973624, 192994.92605801], 
processed observation next is [1.0, 0.8695652173913043, 0.6682464454976303, 0.71, 1.0, 1.0, 0.4661611939200406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21617489693711778, 0.21617489693711778, 0.28805212844479106], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.665251], dtype=float32), -0.08304025]. 
=============================================
[2019-03-27 08:35:13,421] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4073165e-18 1.0000000e+00 1.2877531e-23 1.4023255e-13 3.5226131e-26], sum to 1.0000
[2019-03-27 08:35:13,432] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4410
[2019-03-27 08:35:13,439] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6172952702121124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862640.7875831608, 862640.7875831615, 204012.5713064935], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4404000.0000, 
sim time next is 4404600.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6171607559031557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862452.7339116574, 862452.733911658, 203986.8338748405], 
processed observation next is [1.0, 1.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5387478986785008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23957020386434927, 0.23957020386434946, 0.3044579610072246], 
reward next is 0.6955, 
noisyNet noise sample is [array([1.3978832], dtype=float32), 1.7557218]. 
=============================================
[2019-03-27 08:35:14,163] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1691045e-10 5.5160958e-01 5.6909840e-14 4.4839039e-01 1.2360058e-17], sum to 1.0000
[2019-03-27 08:35:14,169] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6328
[2019-03-27 08:35:14,173] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.16666666666667, 62.5, 1.0, 2.0, 0.9026449679653278, 1.0, 2.0, 0.9026449679653278, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2524700.088127796, 2524700.088127797, 472947.4173075923], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4626600.0000, 
sim time next is 4627200.0000, 
raw observation next is [34.33333333333334, 62.00000000000001, 1.0, 2.0, 1.008739779127733, 1.0, 2.0, 1.008739779127733, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2821782.642121956, 2821782.642121956, 534297.1937547228], 
processed observation next is [1.0, 0.5652173913043478, 0.8262243285939973, 0.6200000000000001, 1.0, 1.0, 1.0105298543707628, 1.0, 1.0, 1.0105298543707628, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7838285117005434, 0.7838285117005434, 0.7974584981413774], 
reward next is 0.2025, 
noisyNet noise sample is [array([0.6087568], dtype=float32), 0.085337184]. 
=============================================
[2019-03-27 08:35:15,002] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.6581138e-17 1.0000000e+00 4.4526503e-23 4.8023761e-15 3.0168215e-27], sum to 1.0000
[2019-03-27 08:35:15,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9908
[2019-03-27 08:35:15,018] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 79.83333333333334, 1.0, 2.0, 0.6225070610073073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 869927.0001083423, 869927.0001083417, 205014.3152212159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4399800.0000, 
sim time next is 4400400.0000, 
raw observation next is [30.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6216299483060438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868700.7713449328, 868700.7713449328, 204845.1600819521], 
processed observation next is [1.0, 0.9565217391304348, 0.6524486571879939, 0.8066666666666668, 1.0, 1.0, 0.5441324678386069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2413057698180369, 0.2413057698180369, 0.305739044898436], 
reward next is 0.6943, 
noisyNet noise sample is [array([-1.5442622], dtype=float32), 0.053025775]. 
=============================================
[2019-03-27 08:35:38,927] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2143082e-20 1.0000000e+00 6.2549206e-26 7.4452687e-17 7.0123067e-29], sum to 1.0000
[2019-03-27 08:35:38,941] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9107
[2019-03-27 08:35:38,946] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5177056254068089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723421.6011666113, 723421.6011666113, 186417.1241739583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5054400.0000, 
sim time next is 5055000.0000, 
raw observation next is [31.16666666666667, 63.0, 1.0, 2.0, 0.5227724539329466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730504.2237892565, 730504.2237892571, 187241.4677359061], 
processed observation next is [0.0, 0.5217391304347826, 0.6761453396524489, 0.63, 1.0, 1.0, 0.4250270529312609, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20291783994146015, 0.20291783994146032, 0.2794648772177703], 
reward next is 0.7205, 
noisyNet noise sample is [array([0.5782655], dtype=float32), 0.5220332]. 
=============================================
[2019-03-27 08:35:38,959] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.2662  ]
 [72.163155]
 [72.10713 ]
 [72.05305 ]
 [72.003174]], R is [[72.30899811]
 [72.30767059]
 [72.30625916]
 [72.3048172 ]
 [72.30344391]].
[2019-03-27 08:35:39,802] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.48624669e-18 1.00000000e+00 4.74932130e-24 7.84245376e-14
 1.43321765e-27], sum to 1.0000
[2019-03-27 08:35:39,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5878
[2019-03-27 08:35:39,815] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 89.5, 1.0, 2.0, 0.5553076913136273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775984.4504366948, 775984.4504366942, 192714.954714195], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5607000.0000, 
sim time next is 5607600.0000, 
raw observation next is [27.26666666666667, 89.66666666666667, 1.0, 2.0, 0.5543166012770465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774599.0007298482, 774599.0007298482, 192543.4819928273], 
processed observation next is [1.0, 0.9130434782608695, 0.4913112164297, 0.8966666666666667, 1.0, 1.0, 0.46303204973138135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2151663890916245, 0.2151663890916245, 0.28737833133257806], 
reward next is 0.7126, 
noisyNet noise sample is [array([0.03488224], dtype=float32), -0.5011005]. 
=============================================
[2019-03-27 08:35:49,968] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8235387e-18 1.0000000e+00 8.0845011e-26 2.4378118e-17 5.7363926e-29], sum to 1.0000
[2019-03-27 08:35:49,976] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8248
[2019-03-27 08:35:49,982] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5090546872603902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711329.0709204307, 711329.0709204313, 185026.9719908704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5044200.0000, 
sim time next is 5044800.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5069270928919654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708355.0793879728, 708355.0793879734, 184688.7124087242], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.4059362564963438, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19676529982999244, 0.1967652998299926, 0.2756547946398869], 
reward next is 0.7243, 
noisyNet noise sample is [array([-0.07895386], dtype=float32), 0.9878545]. 
=============================================
[2019-03-27 08:35:50,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2246480e-11 6.9229223e-05 5.7142862e-14 9.9993074e-01 3.7997940e-19], sum to 1.0000
[2019-03-27 08:35:50,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1446
[2019-03-27 08:35:50,680] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3601445.395979505 W.
[2019-03-27 08:35:50,686] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.01666666666667, 60.66666666666666, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.876258036378407, 6.9112, 170.5573041426782, 3601445.395979505, 2910135.068470608, 548113.5546518321], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5395800.0000, 
sim time next is 5396400.0000, 
raw observation next is [35.2, 60.0, 1.0, 2.0, 0.9326326773408885, 1.0, 2.0, 0.7869063781847069, 1.0, 1.0, 1.03, 7.005116082200111, 6.9112, 170.5573041426782, 3302495.319174009, 3235219.409232098, 604883.9244097867], 
processed observation next is [1.0, 0.4782608695652174, 0.8672985781990523, 0.6, 1.0, 1.0, 0.9188345510131187, 1.0, 1.0, 0.7432606966080806, 1.0, 0.5, 1.0365853658536586, 0.009391608220011083, 0.0, 0.8375144448122397, 0.9173598108816692, 0.8986720581200273, 0.9028118274772936], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03813162], dtype=float32), 0.17444369]. 
=============================================
[2019-03-27 08:35:51,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6681530e-12 1.8799536e-05 9.0089933e-15 9.9998116e-01 3.3897819e-20], sum to 1.0000
[2019-03-27 08:35:51,073] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1436
[2019-03-27 08:35:51,080] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.33333333333334, 62.66666666666667, 1.0, 2.0, 0.7594476352670405, 1.0, 2.0, 0.7003138571477829, 1.0, 2.0, 1.03, 7.00510242001499, 6.9112, 170.5573041426782, 2938655.542103964, 2871389.418940998, 540765.9599817084], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5401200.0000, 
sim time next is 5401800.0000, 
raw observation next is [36.5, 62.0, 1.0, 2.0, 0.9731125870056264, 1.0, 2.0, 0.9731125870056264, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2722012.889226072, 2722012.889226072, 512972.620551136], 
processed observation next is [1.0, 0.5217391304347826, 0.9289099526066351, 0.62, 1.0, 1.0, 0.967605526512803, 1.0, 1.0, 0.967605526512803, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7561146914516866, 0.7561146914516866, 0.765630776941994], 
reward next is 0.2344, 
noisyNet noise sample is [array([-0.88941175], dtype=float32), -0.12599337]. 
=============================================
[2019-03-27 08:35:58,961] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 08:35:58,963] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:35:58,964] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:35:58,964] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:35:58,965] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:35:58,965] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:35:58,966] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:35:58,968] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:35:58,967] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:35:58,970] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:35:58,971] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:35:59,001] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run83
[2019-03-27 08:35:59,027] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run83
[2019-03-27 08:35:59,028] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run83
[2019-03-27 08:35:59,050] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run83
[2019-03-27 08:35:59,052] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run83
[2019-03-27 08:36:00,217] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08367299], dtype=float32), 0.048550937]
[2019-03-27 08:36:00,218] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.46666666666667, 86.0, 1.0, 2.0, 0.3241420795069231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518568.2885860677, 518568.2885860677, 168497.6843657705]
[2019-03-27 08:36:00,219] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:36:00,221] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0126588e-18 1.0000000e+00 3.4639430e-25 9.3095683e-19 1.2982020e-27], sampled 0.495467341271817
[2019-03-27 08:36:03,611] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08367299], dtype=float32), 0.048550937]
[2019-03-27 08:36:03,612] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.45, 86.0, 1.0, 2.0, 0.2878227717398767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460736.3540841753, 460736.3540841753, 164305.3936473863]
[2019-03-27 08:36:03,613] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:36:03,617] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.2172211e-19 1.0000000e+00 1.0864494e-25 1.9157007e-19 4.0748884e-28], sampled 0.5318933275629733
[2019-03-27 08:36:55,949] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08367299], dtype=float32), 0.048550937]
[2019-03-27 08:36:55,951] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.65608087166667, 77.75327554833333, 1.0, 2.0, 0.5463410047690894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763449.9403626783, 763449.940362679, 191174.3474485157]
[2019-03-27 08:36:55,954] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:36:55,956] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2564388e-18 1.0000000e+00 4.2085143e-25 6.0254311e-17 3.3236202e-28], sampled 0.16216768189872943
[2019-03-27 08:37:04,743] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.08367299], dtype=float32), 0.048550937]
[2019-03-27 08:37:04,744] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.36666666666667, 59.33333333333334, 1.0, 2.0, 0.6524210767908528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 911748.5013931317, 911748.5013931317, 210916.1828674794]
[2019-03-27 08:37:04,745] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:37:04,747] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.1179294e-18 1.0000000e+00 2.2169117e-24 1.4307511e-16 3.4789297e-27], sampled 0.8470376906037574
[2019-03-27 08:37:47,291] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08367299], dtype=float32), 0.048550937]
[2019-03-27 08:37:47,292] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.976060345, 77.336656325, 1.0, 2.0, 0.3081288939660075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496279.0829853188, 496279.0829853182, 166829.3095631175]
[2019-03-27 08:37:47,294] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:37:47,295] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.8751634e-19 1.0000000e+00 2.4121779e-25 4.7862201e-18 3.3768225e-28], sampled 0.3884061089363222
[2019-03-27 08:37:54,512] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9621 3007591021.8951 1764.0000
[2019-03-27 08:37:54,710] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9283 2927272965.6207 1338.0000
[2019-03-27 08:37:54,829] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 08:37:54,922] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.7065 3163821436.8195 1770.0000
[2019-03-27 08:37:54,947] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.8567 2842401976.2845 1130.0000
[2019-03-27 08:37:55,964] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2050000, evaluation results [2050000.0, 7885.7065086729335, 3163821436.8195333, 1770.0, 8252.928342174982, 2927272965.6207337, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7997.96212227918, 3007591021.8950796, 1764.0, 8497.856650703563, 2842401976.284467, 1130.0]
[2019-03-27 08:37:57,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6780689e-15 1.0000000e+00 4.5699265e-22 4.9571718e-12 3.0607827e-24], sum to 1.0000
[2019-03-27 08:37:57,214] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7180
[2019-03-27 08:37:57,219] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.9514258394251024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1329864.757766333, 1329864.757766333, 284472.7960258182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5196000.0000, 
sim time next is 5196600.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.9401650299888292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1314115.109905137, 1314115.109905137, 281222.5073732879], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.9279096746853364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36503197497364914, 0.36503197497364914, 0.41973508563177303], 
reward next is 0.5803, 
noisyNet noise sample is [array([0.01746508], dtype=float32), -0.60598385]. 
=============================================
[2019-03-27 08:37:59,011] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9546804e-17 1.0000000e+00 2.3981976e-23 1.1482219e-14 1.7726660e-27], sum to 1.0000
[2019-03-27 08:37:59,019] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5194
[2019-03-27 08:37:59,025] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.53333333333333, 80.66666666666667, 1.0, 2.0, 0.5457163671402518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762576.7663983349, 762576.7663983356, 191068.8977701827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5262000.0000, 
sim time next is 5262600.0000, 
raw observation next is [28.51666666666667, 80.83333333333333, 1.0, 2.0, 0.5466881652551274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763935.2324387603, 763935.2324387596, 191234.4228071294], 
processed observation next is [1.0, 0.9130434782608695, 0.5505529225908374, 0.8083333333333332, 1.0, 1.0, 0.4538411629579848, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21220423123298898, 0.2122042312329888, 0.28542451165243193], 
reward next is 0.7146, 
noisyNet noise sample is [array([1.0388553], dtype=float32), 0.45968086]. 
=============================================
[2019-03-27 08:38:06,351] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1094788e-15 1.0000000e+00 2.7056922e-22 2.9652007e-11 9.9278884e-25], sum to 1.0000
[2019-03-27 08:38:06,363] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9636
[2019-03-27 08:38:06,371] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 92.0, 1.0, 2.0, 0.9211757236426925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287556.723439645, 1287556.723439645, 275832.0907661517], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5457600.0000, 
sim time next is 5458200.0000, 
raw observation next is [27.68333333333333, 92.0, 1.0, 2.0, 0.979417190625841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1369015.16930083, 1369015.169300831, 292720.3300779311], 
processed observation next is [1.0, 0.17391304347826086, 0.5110584518167456, 0.92, 1.0, 1.0, 0.975201434488965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3802819914724528, 0.380281991472453, 0.43689601504168823], 
reward next is 0.5631, 
noisyNet noise sample is [array([-1.094212], dtype=float32), 0.41941863]. 
=============================================
[2019-03-27 08:38:10,200] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8044892e-17 1.0000000e+00 2.7324201e-23 1.3609394e-13 5.2582148e-27], sum to 1.0000
[2019-03-27 08:38:10,208] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8982
[2019-03-27 08:38:10,211] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.26666666666667, 83.66666666666667, 1.0, 2.0, 0.587162889794152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820515.9538384827, 820515.9538384827, 198378.926590865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5442000.0000, 
sim time next is 5442600.0000, 
raw observation next is [29.18333333333334, 84.33333333333334, 1.0, 2.0, 0.5873787972320692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820817.7848168855, 820817.7848168849, 198418.3753768629], 
processed observation next is [1.0, 1.0, 0.5821484992101109, 0.8433333333333334, 1.0, 1.0, 0.5028660207615291, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22800494022691264, 0.22800494022691248, 0.2961468289206909], 
reward next is 0.7039, 
noisyNet noise sample is [array([-0.1540712], dtype=float32), 1.5442263]. 
=============================================
[2019-03-27 08:38:11,704] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.1831821e-18 1.0000000e+00 3.4370536e-24 5.5875131e-17 3.3479523e-27], sum to 1.0000
[2019-03-27 08:38:11,712] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7301
[2019-03-27 08:38:11,717] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.03333333333333, 64.33333333333333, 1.0, 2.0, 0.5284826783616547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738486.2684343568, 738486.2684343574, 188179.487977749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5679600.0000, 
sim time next is 5680200.0000, 
raw observation next is [30.81666666666667, 65.66666666666667, 1.0, 2.0, 0.5311470644255317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742210.7044822134, 742210.7044822134, 188620.4143638385], 
processed observation next is [0.0, 0.7391304347826086, 0.6595576619273303, 0.6566666666666667, 1.0, 1.0, 0.43511694509100207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20616964013394815, 0.20616964013394815, 0.2815230065131918], 
reward next is 0.7185, 
noisyNet noise sample is [array([0.5230155], dtype=float32), -0.15536994]. 
=============================================
[2019-03-27 08:38:13,564] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3200616e-17 1.0000000e+00 1.0449967e-23 1.7979018e-15 1.0128479e-26], sum to 1.0000
[2019-03-27 08:38:13,571] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8451
[2019-03-27 08:38:13,576] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 89.16666666666667, 1.0, 2.0, 0.5113257757610636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714503.6504446712, 714503.6504446717, 185389.3033097809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5713800.0000, 
sim time next is 5714400.0000, 
raw observation next is [26.1, 89.33333333333334, 1.0, 2.0, 0.5114946839534134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714739.7545637256, 714739.7545637263, 185416.266998658], 
processed observation next is [0.0, 0.13043478260869565, 0.4360189573459717, 0.8933333333333334, 1.0, 1.0, 0.4114393782571245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19853882071214599, 0.19853882071214618, 0.27674069701292237], 
reward next is 0.7233, 
noisyNet noise sample is [array([1.2925802], dtype=float32), 0.20889501]. 
=============================================
[2019-03-27 08:38:14,419] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7694505e-19 1.0000000e+00 1.0562011e-25 2.1638214e-18 5.7778808e-28], sum to 1.0000
[2019-03-27 08:38:14,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2531
[2019-03-27 08:38:14,432] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 68.0, 1.0, 2.0, 0.5548741811939527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775378.4442552031, 775378.4442552031, 192640.7616979877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5654400.0000, 
sim time next is 5655000.0000, 
raw observation next is [31.33333333333333, 67.5, 1.0, 2.0, 0.5570370219494095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 778401.8946878753, 778401.8946878746, 193015.5251236052], 
processed observation next is [0.0, 0.43478260869565216, 0.6840442338072668, 0.675, 1.0, 1.0, 0.46630966499928855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2162227485244098, 0.21622274852440962, 0.28808287331881377], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.26824766], dtype=float32), 1.4034663]. 
=============================================
[2019-03-27 08:38:14,448] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.8271  ]
 [70.82275 ]
 [70.81512 ]
 [70.81003 ]
 [70.784805]], R is [[70.82955933]
 [70.83374023]
 [70.83838654]
 [70.84345245]
 [70.84902954]].
[2019-03-27 08:38:16,505] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1095402e-17 1.0000000e+00 9.1692301e-25 4.1895818e-17 6.7251504e-28], sum to 1.0000
[2019-03-27 08:38:16,512] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4347
[2019-03-27 08:38:16,517] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.76666666666667, 62.0, 1.0, 2.0, 0.5078006300671732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709576.1251180142, 709576.1251180142, 184827.4236625128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6273600.0000, 
sim time next is 6274200.0000, 
raw observation next is [30.73333333333333, 62.0, 1.0, 2.0, 0.5066521727252193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707970.7914016352, 707970.7914016346, 184645.0240866598], 
processed observation next is [0.0, 0.6086956521739131, 0.6556082148499209, 0.62, 1.0, 1.0, 0.4056050273797823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1966585531671209, 0.19665855316712072, 0.2755895881890445], 
reward next is 0.7244, 
noisyNet noise sample is [array([1.8372498], dtype=float32), 1.1794968]. 
=============================================
[2019-03-27 08:38:19,332] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7546600e-18 1.0000000e+00 1.5160661e-22 6.8256026e-12 2.7953360e-26], sum to 1.0000
[2019-03-27 08:38:19,342] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3251
[2019-03-27 08:38:19,348] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 79.33333333333334, 1.0, 2.0, 0.5527707946466707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772438.1135362382, 772438.1135362382, 192277.751057258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5599200.0000, 
sim time next is 5599800.0000, 
raw observation next is [28.9, 81.5, 1.0, 2.0, 0.5558250749686544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776707.7043472845, 776707.7043472845, 192805.4338136173], 
processed observation next is [1.0, 0.8260869565217391, 0.5687203791469194, 0.815, 1.0, 1.0, 0.4648494879140414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21575214009646793, 0.21575214009646793, 0.2877693041994288], 
reward next is 0.7122, 
noisyNet noise sample is [array([1.1352785], dtype=float32), 0.51857454]. 
=============================================
[2019-03-27 08:38:23,109] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.4650901e-19 1.0000000e+00 2.2496903e-27 5.9491181e-19 2.0958281e-29], sum to 1.0000
[2019-03-27 08:38:23,118] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9001
[2019-03-27 08:38:23,129] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 87.0, 1.0, 2.0, 0.5142490701586254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718589.9122754354, 718589.9122754347, 185858.3229558025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5702400.0000, 
sim time next is 5703000.0000, 
raw observation next is [26.5, 87.0, 1.0, 2.0, 0.5141382698666679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718435.0322735875, 718435.0322735881, 185840.5033543064], 
processed observation next is [0.0, 0.0, 0.4549763033175356, 0.87, 1.0, 1.0, 0.41462442152610585, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19956528674266322, 0.19956528674266336, 0.2773738856034424], 
reward next is 0.7226, 
noisyNet noise sample is [array([-1.4911029], dtype=float32), 1.2035272]. 
=============================================
[2019-03-27 08:38:23,146] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.88572 ]
 [76.29264 ]
 [76.29635 ]
 [76.293144]
 [76.290276]], R is [[73.75211334]
 [73.73719788]
 [73.722435  ]
 [73.70773315]
 [73.69294739]].
[2019-03-27 08:38:26,269] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.6407701e-20 1.0000000e+00 2.5635610e-25 2.3279683e-18 4.6452438e-28], sum to 1.0000
[2019-03-27 08:38:26,277] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5286
[2019-03-27 08:38:26,281] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 85.33333333333334, 1.0, 2.0, 0.5352230429333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747908.3730064328, 747908.3730064334, 189298.3995895732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5692800.0000, 
sim time next is 5693400.0000, 
raw observation next is [27.25, 85.5, 1.0, 2.0, 0.5337506857643307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745850.2126275769, 745850.2126275762, 189052.6055247622], 
processed observation next is [0.0, 0.9130434782608695, 0.490521327014218, 0.855, 1.0, 1.0, 0.4382538382702779, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20718061461877135, 0.20718061461877116, 0.2821680679474063], 
reward next is 0.7178, 
noisyNet noise sample is [array([-1.0786144], dtype=float32), -0.9384151]. 
=============================================
[2019-03-27 08:38:26,968] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7821666e-18 1.0000000e+00 6.4823400e-26 3.0129479e-18 1.5220184e-28], sum to 1.0000
[2019-03-27 08:38:26,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4918
[2019-03-27 08:38:26,979] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.6, 60.0, 1.0, 2.0, 0.5201989665103799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726906.8906554563, 726906.8906554563, 186821.6285277968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5742000.0000, 
sim time next is 5742600.0000, 
raw observation next is [31.8, 59.16666666666666, 1.0, 2.0, 0.5258469851833755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734801.9538435276, 734801.9538435276, 187744.7910346609], 
processed observation next is [0.0, 0.4782608695652174, 0.7061611374407584, 0.5916666666666666, 1.0, 1.0, 0.42873130744984994, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20411165384542432, 0.20411165384542432, 0.28021610602188196], 
reward next is 0.7198, 
noisyNet noise sample is [array([-0.8526304], dtype=float32), 0.15433528]. 
=============================================
[2019-03-27 08:38:26,982] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0813661e-16 1.0000000e+00 5.6003262e-21 1.8866207e-10 3.3372830e-25], sum to 1.0000
[2019-03-27 08:38:26,991] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0815
[2019-03-27 08:38:26,995] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 71.5, 1.0, 2.0, 0.5065285289715604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707797.9601494962, 707797.9601494956, 184625.4918272529], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6460200.0000, 
sim time next is 6460800.0000, 
raw observation next is [28.9, 72.0, 1.0, 2.0, 0.5039603006425784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704208.0551312147, 704208.0551312154, 184219.2377788638], 
processed observation next is [1.0, 0.782608695652174, 0.5687203791469194, 0.72, 1.0, 1.0, 0.4023618080031064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19561334864755964, 0.19561334864755983, 0.27495408623711015], 
reward next is 0.7250, 
noisyNet noise sample is [array([-1.0786585], dtype=float32), -2.0559223]. 
=============================================
[2019-03-27 08:38:43,438] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7880459e-15 1.0000000e+00 1.6638419e-21 3.8126034e-11 2.2135558e-25], sum to 1.0000
[2019-03-27 08:38:43,446] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8526
[2019-03-27 08:38:43,450] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.08333333333333, 77.0, 1.0, 2.0, 0.5356847083883551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748553.7210083208, 748553.7210083215, 189376.7250287128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6029400.0000, 
sim time next is 6030000.0000, 
raw observation next is [28.9, 78.0, 1.0, 2.0, 0.5386367175919351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752680.2535426113, 752680.2535426107, 189871.5060304962], 
processed observation next is [1.0, 0.8260869565217391, 0.5687203791469194, 0.78, 1.0, 1.0, 0.44414062360474105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2090778482062809, 0.20907784820628075, 0.2833903075082033], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.4289249], dtype=float32), 0.49605447]. 
=============================================
[2019-03-27 08:38:43,462] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[69.64145 ]
 [69.69631 ]
 [69.895325]
 [70.20214 ]
 [69.95103 ]], R is [[70.03689575]
 [70.05387115]
 [70.0712204 ]
 [70.08833313]
 [70.10494232]].
[2019-03-27 08:38:49,752] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 08:38:49,755] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:38:49,756] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:38:49,756] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:38:49,760] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:38:49,761] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:38:49,761] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:38:49,763] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:38:49,762] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:38:49,764] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:38:49,764] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:38:49,784] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run84
[2019-03-27 08:38:49,808] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run84
[2019-03-27 08:38:49,844] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run84
[2019-03-27 08:38:49,868] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run84
[2019-03-27 08:38:49,869] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run84
[2019-03-27 08:39:27,461] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08409764], dtype=float32), 0.04863524]
[2019-03-27 08:39:27,462] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.26160631, 85.44829846833335, 1.0, 2.0, 0.5693725904878851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795646.0379589617, 795646.0379589623, 195178.3009411791]
[2019-03-27 08:39:27,465] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:39:27,468] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.3706892e-18 1.0000000e+00 1.7531797e-24 3.8585802e-16 1.4100937e-27], sampled 0.38253201995084896
[2019-03-27 08:39:34,927] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08409764], dtype=float32), 0.04863524]
[2019-03-27 08:39:34,928] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.0, 54.0, 1.0, 2.0, 0.7184639733668926, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976436371746, 6.9112, 168.9123160401738, 1900966.745057717, 1833729.334088657, 388697.9469364996]
[2019-03-27 08:39:34,928] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:39:34,932] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3675571e-12 9.9999869e-01 3.9344217e-17 1.2648087e-06 6.2941586e-21], sampled 0.025460738534954452
[2019-03-27 08:39:34,933] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1900966.745057717 W.
[2019-03-27 08:39:35,644] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08409764], dtype=float32), 0.04863524]
[2019-03-27 08:39:35,645] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3944646639552937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607638.0174091477, 607638.0174091477, 175684.5971737737]
[2019-03-27 08:39:35,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:39:35,648] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.9017802e-19 1.0000000e+00 1.0772219e-25 3.3705705e-19 2.9156706e-28], sampled 0.29512150447209284
[2019-03-27 08:39:41,747] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.08409764], dtype=float32), 0.04863524]
[2019-03-27 08:39:41,747] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.76666666666667, 50.66666666666667, 1.0, 2.0, 0.5739973384623477, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9968435530885904, 6.9112, 6.9112, 168.9128792359254, 1604828.465414307, 1604828.465414307, 351220.1427853213]
[2019-03-27 08:39:41,749] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:39:41,753] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0182312e-14 1.0000000e+00 8.0774045e-20 3.7777151e-10 9.7134696e-23], sampled 0.3754417508782073
[2019-03-27 08:39:55,132] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08409764], dtype=float32), 0.04863524]
[2019-03-27 08:39:55,133] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.28101704333334, 68.51361143333332, 1.0, 2.0, 0.6809635026957829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 951653.9771170792, 951653.9771170798, 216800.0595145834]
[2019-03-27 08:39:55,134] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:39:55,136] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9684613e-18 1.0000000e+00 5.9115244e-25 4.3852709e-17 7.5527856e-28], sampled 0.03483990554336236
[2019-03-27 08:40:40,570] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08409764], dtype=float32), 0.04863524]
[2019-03-27 08:40:40,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.80541444333333, 83.84320250333334, 1.0, 2.0, 0.4635216896861318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 657078.3894411754, 657078.3894411761, 179288.105302956]
[2019-03-27 08:40:40,573] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:40:40,576] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0672851e-19 1.0000000e+00 3.7691774e-26 9.3557788e-19 4.6689940e-29], sampled 0.028663883780512656
[2019-03-27 08:40:45,169] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 08:40:45,475] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.3909 3164355625.4987 1778.0000
[2019-03-27 08:40:45,518] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 08:40:45,525] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 08:40:45,609] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4786 2842417325.8547 1130.0000
[2019-03-27 08:40:46,627] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2075000, evaluation results [2075000.0, 7884.390924019143, 3164355625.4986963, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.478631916734, 2842417325.8546886, 1130.0]
[2019-03-27 08:40:47,707] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8881910e-17 1.0000000e+00 1.1384911e-22 1.0777859e-12 1.1075481e-25], sum to 1.0000
[2019-03-27 08:40:47,721] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0006
[2019-03-27 08:40:47,726] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666667, 91.66666666666667, 1.0, 2.0, 0.846577179794729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1183229.856326668, 1183229.856326669, 255664.8578708396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6153000.0000, 
sim time next is 6153600.0000, 
raw observation next is [26.63333333333333, 91.33333333333334, 1.0, 2.0, 0.738013600574525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1031420.832863585, 1031420.832863585, 229264.418308013], 
processed observation next is [1.0, 0.21739130434782608, 0.46129541864139006, 0.9133333333333334, 1.0, 1.0, 0.6843537356319579, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2865057869065514, 0.2865057869065514, 0.3421856989671836], 
reward next is 0.6578, 
noisyNet noise sample is [array([-0.85278237], dtype=float32), -1.8255936]. 
=============================================
[2019-03-27 08:40:47,776] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0451399e-19 1.0000000e+00 9.5659215e-27 6.7892135e-18 1.8091661e-28], sum to 1.0000
[2019-03-27 08:40:47,789] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0331
[2019-03-27 08:40:47,795] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.55, 77.16666666666667, 1.0, 2.0, 0.5265024074298735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735718.1373429642, 735718.1373429642, 187852.7289801917], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6289800.0000, 
sim time next is 6290400.0000, 
raw observation next is [28.4, 78.33333333333334, 1.0, 2.0, 0.5282310838648453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738134.5754765758, 738134.5754765758, 188137.5857425934], 
processed observation next is [0.0, 0.8260869565217391, 0.5450236966824644, 0.7833333333333334, 1.0, 1.0, 0.4316037154998136, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2050373820768266, 0.2050373820768266, 0.28080236677999015], 
reward next is 0.7192, 
noisyNet noise sample is [array([-0.00916428], dtype=float32), 0.8698635]. 
=============================================
[2019-03-27 08:40:50,939] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8173453e-17 1.0000000e+00 1.8872416e-23 1.4205176e-14 2.0968379e-27], sum to 1.0000
[2019-03-27 08:40:50,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1985
[2019-03-27 08:40:50,955] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 85.66666666666666, 1.0, 2.0, 0.5305047447765191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741312.8309831853, 741312.8309831853, 188513.7156267586], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6208800.0000, 
sim time next is 6209400.0000, 
raw observation next is [27.33333333333333, 85.83333333333334, 1.0, 2.0, 0.5306538950960878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741521.3223099911, 741521.3223099911, 188538.4028576358], 
processed observation next is [1.0, 0.8695652173913043, 0.494470774091627, 0.8583333333333334, 1.0, 1.0, 0.4345227651760094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20597814508610865, 0.20597814508610865, 0.28140060128005345], 
reward next is 0.7186, 
noisyNet noise sample is [array([-0.72560215], dtype=float32), 0.1269128]. 
=============================================
[2019-03-27 08:40:52,438] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.5558542e-17 1.0000000e+00 1.5430027e-24 1.7225060e-17 4.5366181e-27], sum to 1.0000
[2019-03-27 08:40:52,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0251
[2019-03-27 08:40:52,454] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 90.66666666666667, 1.0, 2.0, 0.5251425489204847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733817.2568692582, 733817.2568692588, 187629.3871789379], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6236400.0000, 
sim time next is 6237000.0000, 
raw observation next is [26.55, 90.5, 1.0, 2.0, 0.5249050761389132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733485.3054963945, 733485.3054963939, 187590.4168267733], 
processed observation next is [0.0, 0.17391304347826086, 0.4573459715639811, 0.905, 1.0, 1.0, 0.4275964772757991, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20374591819344293, 0.20374591819344276, 0.279985696756378], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.2524241], dtype=float32), -0.043868497]. 
=============================================
[2019-03-27 08:40:52,470] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[73.15893 ]
 [73.155266]
 [72.94023 ]
 [72.77677 ]
 [72.72527 ]], R is [[73.22881317]
 [73.21648407]
 [73.20428467]
 [73.19219971]
 [73.18023682]].
[2019-03-27 08:41:08,330] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0542823e-11 9.9979168e-01 1.9601198e-15 2.0830291e-04 5.3849727e-19], sum to 1.0000
[2019-03-27 08:41:08,338] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0888
[2019-03-27 08:41:08,344] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2012089.963460932 W.
[2019-03-27 08:41:08,348] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.18333333333334, 78.16666666666667, 1.0, 2.0, 0.7195307408797614, 1.0, 2.0, 0.7195307408797614, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2012089.963460932, 2012089.963460932, 382592.4774041418], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6599400.0000, 
sim time next is 6600000.0000, 
raw observation next is [28.36666666666667, 77.33333333333334, 1.0, 2.0, 0.6548094253897051, 1.0, 2.0, 0.6548094253897051, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1830949.151118025, 1830949.151118025, 355374.5921067133], 
processed observation next is [1.0, 0.391304347826087, 0.543443917851501, 0.7733333333333334, 1.0, 1.0, 0.5841077414333796, 1.0, 1.0, 0.5841077414333796, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5085969864216736, 0.5085969864216736, 0.5304098389652437], 
reward next is 0.4696, 
noisyNet noise sample is [array([2.0874634], dtype=float32), 1.0991635]. 
=============================================
[2019-03-27 08:41:08,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[47.4454  ]
 [48.11497 ]
 [50.842518]
 [53.782734]
 [57.429565]], R is [[48.43880463]
 [48.3833847 ]
 [47.89955139]
 [47.42055511]
 [46.9463501 ]].
[2019-03-27 08:41:08,704] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5827308e-16 1.0000000e+00 1.3983167e-24 4.8000527e-14 1.3575393e-26], sum to 1.0000
[2019-03-27 08:41:08,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6019
[2019-03-27 08:41:08,714] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.23333333333333, 93.33333333333334, 1.0, 2.0, 0.4954802713285547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692354.6392340903, 692354.639234091, 182890.7910992971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6658800.0000, 
sim time next is 6659400.0000, 
raw observation next is [25.16666666666667, 93.66666666666667, 1.0, 2.0, 0.4947021790865072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691267.0256857803, 691267.0256857803, 182770.0045269446], 
processed observation next is [1.0, 0.043478260869565216, 0.39178515007898923, 0.9366666666666668, 1.0, 1.0, 0.3912074446825388, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1920186182460501, 0.1920186182460501, 0.2727910515327531], 
reward next is 0.7272, 
noisyNet noise sample is [array([0.3877631], dtype=float32), -1.3626662]. 
=============================================
[2019-03-27 08:41:08,720] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9504549e-18 1.0000000e+00 2.6252167e-24 3.6619071e-17 3.9574089e-26], sum to 1.0000
[2019-03-27 08:41:08,725] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9072
[2019-03-27 08:41:08,730] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 90.0, 1.0, 2.0, 0.3282137759430772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517660.6663926761, 517660.6663926768, 168344.5824127825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7272000.0000, 
sim time next is 7272600.0000, 
raw observation next is [21.56666666666667, 90.16666666666667, 1.0, 2.0, 0.3597690894197039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 567524.7436648917, 567524.7436648923, 172379.4989227491], 
processed observation next is [1.0, 0.17391304347826086, 0.22116903633491333, 0.9016666666666667, 1.0, 1.0, 0.2286374571321734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15764576212913659, 0.15764576212913675, 0.25728283421305836], 
reward next is 0.7427, 
noisyNet noise sample is [array([-1.0056643], dtype=float32), -0.8844508]. 
=============================================
[2019-03-27 08:41:11,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3389155e-16 1.0000000e+00 8.7618080e-23 1.3400757e-12 1.5850618e-25], sum to 1.0000
[2019-03-27 08:41:11,315] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4321
[2019-03-27 08:41:11,322] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.95, 91.5, 1.0, 2.0, 0.7075476536035944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 988822.9160131195, 988822.9160131201, 222481.1445952237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6579000.0000, 
sim time next is 6579600.0000, 
raw observation next is [25.93333333333333, 91.66666666666666, 1.0, 2.0, 0.7026311555265342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 981948.7591616382, 981948.7591616388, 221412.5333054489], 
processed observation next is [1.0, 0.13043478260869565, 0.42812006319115314, 0.9166666666666665, 1.0, 1.0, 0.6417242837669087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2727635442115662, 0.27276354421156634, 0.330466467620073], 
reward next is 0.6695, 
noisyNet noise sample is [array([0.28566122], dtype=float32), 1.249011]. 
=============================================
[2019-03-27 08:41:15,198] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2567811e-10 3.0591112e-01 1.6104427e-13 6.9408888e-01 8.9143427e-18], sum to 1.0000
[2019-03-27 08:41:15,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2001
[2019-03-27 08:41:15,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2274062.632035853 W.
[2019-03-27 08:41:15,221] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.16666666666666, 62.0, 1.0, 2.0, 0.8131171174649838, 1.0, 2.0, 0.8131171174649838, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2274062.632035853, 2274062.632035853, 426301.1408032597], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6619200.0000, 
sim time next is 6619800.0000, 
raw observation next is [31.08333333333334, 62.5, 1.0, 2.0, 1.001142649181777, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.982180975751982, 6.9112, 168.9125342470112, 2296601.374101086, 2246245.152811195, 464511.3672795756], 
processed observation next is [1.0, 0.6086956521739131, 0.6721958925750398, 0.625, 1.0, 1.0, 1.001376685761177, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007098097575198192, 0.0, 0.8294378716455985, 0.6379448261391906, 0.6239569868919985, 0.6933005481784711], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.91652936], dtype=float32), -0.14153427]. 
=============================================
[2019-03-27 08:41:17,454] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.2434339e-15 1.0000000e+00 1.1397486e-19 1.8518540e-09 1.5625988e-22], sum to 1.0000
[2019-03-27 08:41:17,461] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2237
[2019-03-27 08:41:17,465] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 85.0, 1.0, 2.0, 0.5787102127954293, 1.0, 1.0, 0.5787102127954293, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1618003.255834149, 1618003.255834149, 326517.406592723], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6684600.0000, 
sim time next is 6685200.0000, 
raw observation next is [27.2, 84.0, 1.0, 2.0, 1.023733988689424, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9127400751879, 1431002.300741326, 1431002.300741325, 306239.4205479156], 
processed observation next is [1.0, 0.391304347826087, 0.4881516587677725, 0.84, 1.0, 1.0, 1.0285951670956914, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294388823561747, 0.3975006390948128, 0.3975006390948125, 0.4570737620118144], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33502603], dtype=float32), -0.12433394]. 
=============================================
[2019-03-27 08:41:18,185] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3641072e-14 1.0000000e+00 1.8256737e-20 2.2718435e-10 1.2298571e-22], sum to 1.0000
[2019-03-27 08:41:18,191] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1312
[2019-03-27 08:41:18,198] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.25, 52.5, 1.0, 2.0, 0.8659127067145393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1344645.596476443, 1344645.596476443, 278803.5744350529], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6780600.0000, 
sim time next is 6781200.0000, 
raw observation next is [28.33333333333334, 52.0, 1.0, 2.0, 0.8793776658722838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1366267.782784853, 1366267.782784853, 282892.902940032], 
processed observation next is [1.0, 0.4782608695652174, 0.5418641390205374, 0.52, 1.0, 1.0, 0.854671886593113, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37951882855134805, 0.37951882855134805, 0.4222282133433314], 
reward next is 0.5778, 
noisyNet noise sample is [array([1.2136643], dtype=float32), 0.82234615]. 
=============================================
[2019-03-27 08:41:18,562] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8335008e-19 1.0000000e+00 7.5348399e-25 5.3717138e-18 9.0037487e-27], sum to 1.0000
[2019-03-27 08:41:18,570] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0366
[2019-03-27 08:41:18,575] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 84.33333333333333, 1.0, 2.0, 0.4236879052625635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619356.4395952794, 619356.4395952794, 176004.5588267967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6916200.0000, 
sim time next is 6916800.0000, 
raw observation next is [24.8, 84.66666666666667, 1.0, 2.0, 0.4240072970887338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619883.8812531757, 619883.8812531751, 176057.0589635421], 
processed observation next is [0.0, 0.043478260869565216, 0.3744075829383887, 0.8466666666666667, 1.0, 1.0, 0.3060328880587155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17218996701477102, 0.17218996701477085, 0.2627717297963315], 
reward next is 0.7372, 
noisyNet noise sample is [array([0.24880166], dtype=float32), 0.98268104]. 
=============================================
[2019-03-27 08:41:24,750] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0643087e-14 1.0000000e+00 1.7707132e-19 7.6885573e-09 9.5275566e-23], sum to 1.0000
[2019-03-27 08:41:24,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2307
[2019-03-27 08:41:24,761] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.31666666666667, 46.66666666666667, 1.0, 2.0, 0.985090117182746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1538419.695313547, 1538419.695313547, 317393.348743286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6792600.0000, 
sim time next is 6793200.0000, 
raw observation next is [29.3, 47.0, 1.0, 2.0, 0.9482753016335264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1478974.296279009, 1478974.296279009, 305009.4952735879], 
processed observation next is [1.0, 0.6521739130434783, 0.5876777251184835, 0.47, 1.0, 1.0, 0.9376810863054534, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.4108261934108358, 0.4108261934108358, 0.45523805264714606], 
reward next is 0.5448, 
noisyNet noise sample is [array([0.21069317], dtype=float32), 0.39725834]. 
=============================================
[2019-03-27 08:41:28,038] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5338574e-19 1.0000000e+00 1.7623364e-27 6.2924922e-19 4.7499590e-30], sum to 1.0000
[2019-03-27 08:41:28,048] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5880
[2019-03-27 08:41:28,051] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.13333333333333, 55.66666666666667, 1.0, 2.0, 0.3564536971821201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546862.1566660238, 546862.1566660238, 170319.1914857418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6891600.0000, 
sim time next is 6892200.0000, 
raw observation next is [28.0, 56.5, 1.0, 2.0, 0.3590881788372061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 550403.7658441856, 550403.765844185, 170601.182203602], 
processed observation next is [0.0, 0.782608695652174, 0.5260663507109005, 0.565, 1.0, 1.0, 0.2278170829363929, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1528899349567182, 0.15288993495671807, 0.25462863015462983], 
reward next is 0.7454, 
noisyNet noise sample is [array([-2.914013], dtype=float32), -0.3378902]. 
=============================================
[2019-03-27 08:41:29,292] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.1996305e-20 1.0000000e+00 4.3823867e-28 6.1030060e-21 1.1122370e-29], sum to 1.0000
[2019-03-27 08:41:29,298] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1450
[2019-03-27 08:41:29,303] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 52.33333333333334, 1.0, 2.0, 0.3546870426126819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546388.899086117, 546388.8990861165, 170345.2886972882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6889200.0000, 
sim time next is 6889800.0000, 
raw observation next is [28.53333333333333, 53.16666666666666, 1.0, 2.0, 0.3535565091441538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544047.8425581695, 544047.8425581695, 170133.5252672262], 
processed observation next is [0.0, 0.7391304347826086, 0.5513428120063191, 0.5316666666666666, 1.0, 1.0, 0.221152420655607, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15112440071060265, 0.15112440071060265, 0.2539306347272033], 
reward next is 0.7461, 
noisyNet noise sample is [array([0.5642095], dtype=float32), -0.30581775]. 
=============================================
[2019-03-27 08:41:32,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1411419e-19 1.0000000e+00 6.4366810e-26 2.3807470e-19 9.6249582e-28], sum to 1.0000
[2019-03-27 08:41:32,474] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3798
[2019-03-27 08:41:32,480] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 85.33333333333333, 1.0, 2.0, 0.4173424736751459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612707.8473380536, 612707.8473380536, 175442.9996377701], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6933000.0000, 
sim time next is 6933600.0000, 
raw observation next is [24.8, 84.0, 1.0, 2.0, 0.4177037662227866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612920.5840332492, 612920.5840332485, 175454.1703259616], 
processed observation next is [0.0, 0.2608695652173913, 0.3744075829383887, 0.84, 1.0, 1.0, 0.2984382725575742, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17025571778701365, 0.17025571778701346, 0.26187189600889793], 
reward next is 0.7381, 
noisyNet noise sample is [array([-0.18008798], dtype=float32), -0.61051977]. 
=============================================
[2019-03-27 08:41:37,511] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 08:41:37,512] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:41:37,512] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:41:37,512] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:41:37,513] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:41:37,513] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:41:37,515] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:41:37,514] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:41:37,515] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:41:37,519] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:41:37,515] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:41:37,538] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run85
[2019-03-27 08:41:37,557] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run85
[2019-03-27 08:41:37,558] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run85
[2019-03-27 08:41:37,572] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run85
[2019-03-27 08:41:37,572] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run85
[2019-03-27 08:42:20,769] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0843864], dtype=float32), 0.04869316]
[2019-03-27 08:42:20,770] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.3, 81.0, 1.0, 2.0, 0.6068593332839735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 848051.2397130394, 848051.2397130394, 202023.4969981935]
[2019-03-27 08:42:20,773] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:42:20,775] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1406458e-17 1.0000000e+00 5.0078511e-24 2.9932241e-16 1.1433546e-26], sampled 0.30896460478122456
[2019-03-27 08:43:04,961] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0843864], dtype=float32), 0.04869316]
[2019-03-27 08:43:04,962] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.98595531666667, 86.86378243, 1.0, 2.0, 0.6920692632050678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 967181.4667597561, 967181.4667597555, 219144.6949629623]
[2019-03-27 08:43:04,963] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:43:04,966] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.5553008e-17 1.0000000e+00 2.5879312e-23 1.0115026e-14 3.7386756e-26], sampled 0.3213862212930869
[2019-03-27 08:43:06,252] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0843864], dtype=float32), 0.04869316]
[2019-03-27 08:43:06,253] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.05, 62.0, 1.0, 2.0, 0.8320837384579247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1162961.811734711, 1162961.81173471, 251939.7569854887]
[2019-03-27 08:43:06,255] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:43:06,257] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.7230641e-17 1.0000000e+00 6.9352864e-23 5.6150529e-14 8.5698379e-26], sampled 0.25086867027031057
[2019-03-27 08:43:17,470] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0843864], dtype=float32), 0.04869316]
[2019-03-27 08:43:17,472] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.42736344, 79.324558205, 1.0, 2.0, 0.3585019204291374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556108.7795039389, 556108.7795039394, 171256.3594473632]
[2019-03-27 08:43:17,473] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:43:17,476] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.0926288e-19 1.0000000e+00 2.3141121e-25 8.3426562e-19 7.2977987e-28], sampled 0.5519596709862388
[2019-03-27 08:43:30,269] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0843864], dtype=float32), 0.04869316]
[2019-03-27 08:43:30,270] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.55, 90.0, 1.0, 2.0, 0.5954824827290106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 832146.5131714323, 832146.5131714329, 199899.7811755465]
[2019-03-27 08:43:30,273] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:43:30,275] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3958221e-17 1.0000000e+00 7.0070302e-24 4.3040812e-16 1.4946041e-26], sampled 0.042221195476398465
[2019-03-27 08:43:32,600] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7896.4047 3162833542.5378 1737.0000
[2019-03-27 08:43:33,143] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.3187 2927313633.6593 1337.0000
[2019-03-27 08:43:33,368] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.6243 3007322407.8087 1757.0000
[2019-03-27 08:43:33,389] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.6616 2842381010.7434 1127.0000
[2019-03-27 08:43:33,497] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9767 2779207024.4606 933.0000
[2019-03-27 08:43:34,517] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2100000, evaluation results [2100000.0, 7896.404655565501, 3162833542.5378156, 1737.0, 8253.318728200957, 2927313633.659319, 1337.0, 8659.976662972434, 2779207024.4606214, 933.0, 8002.624253328319, 3007322407.8086977, 1757.0, 8499.66156311722, 2842381010.743352, 1127.0]
[2019-03-27 08:43:36,371] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2650151e-16 1.0000000e+00 8.8447211e-23 3.0127990e-14 3.5956413e-26], sum to 1.0000
[2019-03-27 08:43:36,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5132
[2019-03-27 08:43:36,383] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 90.33333333333334, 1.0, 2.0, 0.5001853017388527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 698931.3315636899, 698931.3315636904, 183625.0480110853], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7186800.0000, 
sim time next is 7187400.0000, 
raw observation next is [25.8, 90.5, 1.0, 2.0, 0.4939772251370141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690253.6895862136, 690253.6895862136, 182658.1550271027], 
processed observation next is [1.0, 0.17391304347826086, 0.42180094786729866, 0.905, 1.0, 1.0, 0.39033400618917363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19173713599617043, 0.19173713599617043, 0.2726241119807503], 
reward next is 0.7274, 
noisyNet noise sample is [array([0.9875759], dtype=float32), -0.3146384]. 
=============================================
[2019-03-27 08:43:40,048] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4948289e-20 1.0000000e+00 1.2907813e-25 5.4983479e-17 3.8509378e-29], sum to 1.0000
[2019-03-27 08:43:40,058] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5486
[2019-03-27 08:43:40,064] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 92.0, 1.0, 2.0, 0.3660153972335382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 558996.2606867763, 558996.2606867768, 171270.7980575553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7245600.0000, 
sim time next is 7246200.0000, 
raw observation next is [22.51666666666667, 92.0, 1.0, 2.0, 0.3657264946643853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558837.8636058999, 558837.8636058999, 171265.5716764953], 
processed observation next is [1.0, 0.8695652173913043, 0.2661927330173777, 0.92, 1.0, 1.0, 0.2358150538125124, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15523273989052774, 0.15523273989052774, 0.2556202562335751], 
reward next is 0.7444, 
noisyNet noise sample is [array([1.0481987], dtype=float32), -0.47185558]. 
=============================================
[2019-03-27 08:43:42,951] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:43:42,953] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:43:43,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-03-27 08:43:44,155] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:43:44,156] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:43:44,211] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run11
[2019-03-27 08:43:44,621] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2104751: loss 89.6235
[2019-03-27 08:43:44,623] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2104751: learning rate 0.0000
[2019-03-27 08:43:45,940] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2105469: loss 90.5090
[2019-03-27 08:43:45,945] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2105469: learning rate 0.0000
[2019-03-27 08:43:47,475] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0169418e-16 1.0000000e+00 1.7248368e-21 4.1550884e-11 5.8476701e-25], sum to 1.0000
[2019-03-27 08:43:47,479] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4239
[2019-03-27 08:43:47,487] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.96666666666667, 57.5, 1.0, 2.0, 1.000270505240883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1526743.240083183, 1526743.240083183, 318064.6699848701], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7308600.0000, 
sim time next is 7309200.0000, 
raw observation next is [27.93333333333333, 58.00000000000001, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.964758930878284, 6.9112, 168.9124963630703, 1622594.904888347, 1584598.459476157, 331221.0055080327], 
processed observation next is [1.0, 0.6086956521739131, 0.522906793048973, 0.5800000000000001, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.005355893087828356, 0.0, 0.8294376856181053, 0.45072080691342975, 0.44016623874337696, 0.49435970971348164], 
reward next is 0.2378, 
noisyNet noise sample is [array([1.5200772], dtype=float32), -0.34384045]. 
=============================================
[2019-03-27 08:43:47,675] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8150371e-18 1.0000000e+00 3.0769246e-25 2.0554292e-17 4.9299194e-28], sum to 1.0000
[2019-03-27 08:43:47,686] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4834
[2019-03-27 08:43:47,692] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 87.33333333333333, 1.0, 2.0, 0.3237276441616025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510450.7968378516, 510450.7968378516, 167787.0528761033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7282200.0000, 
sim time next is 7282800.0000, 
raw observation next is [22.0, 87.0, 1.0, 2.0, 0.3220762306654523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 507762.1145266998, 507762.1145267004, 167580.1688882732], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.87, 1.0, 1.0, 0.1832243742957257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14104503181297218, 0.14104503181297234, 0.2501196550571242], 
reward next is 0.7499, 
noisyNet noise sample is [array([-1.7297131], dtype=float32), -0.47314206]. 
=============================================
[2019-03-27 08:43:50,403] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3732339e-16 1.0000000e+00 2.4707027e-23 4.8691880e-16 2.3711133e-26], sum to 1.0000
[2019-03-27 08:43:50,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0942
[2019-03-27 08:43:50,419] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 89.66666666666667, 1.0, 2.0, 0.4051620648402793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618301.9594440827, 618301.9594440827, 176551.4574145646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7364400.0000, 
sim time next is 7365000.0000, 
raw observation next is [22.6, 89.83333333333333, 1.0, 2.0, 0.3908969466713055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599718.7713122347, 599718.7713122341, 174926.9392147202], 
processed observation next is [1.0, 0.21739130434782608, 0.27014218009478685, 0.8983333333333333, 1.0, 1.0, 0.2661408996039825, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16658854758673186, 0.1665885475867317, 0.26108498390256746], 
reward next is 0.7389, 
noisyNet noise sample is [array([-0.2990767], dtype=float32), 2.0427535]. 
=============================================
[2019-03-27 08:43:50,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.1861 ]
 [74.12251]
 [74.03065]
 [73.91172]
 [74.09924]], R is [[74.28219604]
 [74.27586365]
 [74.26737976]
 [74.26100922]
 [74.23957062]].
[2019-03-27 08:44:01,325] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2112676: loss 0.0042
[2019-03-27 08:44:01,327] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2112677: learning rate 0.0000
[2019-03-27 08:44:02,916] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2113420: loss 0.0028
[2019-03-27 08:44:02,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2113421: learning rate 0.0000
[2019-03-27 08:44:05,725] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5852783e-18 1.0000000e+00 1.7458128e-23 3.3426094e-17 2.4366001e-27], sum to 1.0000
[2019-03-27 08:44:05,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:05,912] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:05,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1842
[2019-03-27 08:44:05,924] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 89.5, 1.0, 2.0, 0.2566018689352981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417774.758054382, 417774.758054382, 161494.9195042748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 361800.0000, 
sim time next is 362400.0000, 
raw observation next is [20.03333333333333, 89.66666666666667, 1.0, 2.0, 0.2571424099729964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 418631.7409646709, 418631.7409646703, 161548.2531185804], 
processed observation next is [1.0, 0.17391304347826086, 0.14849921011058448, 0.8966666666666667, 1.0, 1.0, 0.1049908553891523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11628659471240858, 0.11628659471240842, 0.24111679569937375], 
reward next is 0.7589, 
noisyNet noise sample is [array([0.3532878], dtype=float32), 1.7888573]. 
=============================================
[2019-03-27 08:44:05,971] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run11
[2019-03-27 08:44:07,236] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.1976776e-10 9.7427666e-01 1.8924710e-13 2.5723342e-02 9.4375303e-18], sum to 1.0000
[2019-03-27 08:44:07,243] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0440
[2019-03-27 08:44:07,252] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1999985.289317887 W.
[2019-03-27 08:44:07,257] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.86666666666667, 59.0, 1.0, 2.0, 0.7152061018202923, 1.0, 2.0, 0.7152061018202923, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1999985.289317887, 1999985.289317887, 380691.9953225951], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7654200.0000, 
sim time next is 7654800.0000, 
raw observation next is [30.73333333333333, 60.0, 1.0, 2.0, 0.8401204402352489, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.968705905362222, 6.9112, 168.9126140700616, 2071225.003993993, 2030428.41823736, 419224.8345462689], 
processed observation next is [1.0, 0.6086956521739131, 0.6556082148499209, 0.6, 1.0, 1.0, 0.807374024379818, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.005750590536222155, 0.0, 0.8294382636133191, 0.5753402788872203, 0.5640078939548222, 0.6257087082780133], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2503398], dtype=float32), 0.2449022]. 
=============================================
[2019-03-27 08:44:07,634] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2115727: loss 82.2714
[2019-03-27 08:44:07,636] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2115728: learning rate 0.0000
[2019-03-27 08:44:09,042] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4395625e-17 1.0000000e+00 5.8349276e-24 8.6009793e-12 3.6506205e-26], sum to 1.0000
[2019-03-27 08:44:09,049] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4045
[2019-03-27 08:44:09,053] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 85.66666666666667, 1.0, 2.0, 0.5095597186048926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712035.0144809416, 712035.0144809422, 185107.3575488465], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7672800.0000, 
sim time next is 7673400.0000, 
raw observation next is [26.55, 86.5, 1.0, 2.0, 0.5077434834523744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709496.244525124, 709496.244525124, 184818.1776674127], 
processed observation next is [1.0, 0.8260869565217391, 0.4573459715639811, 0.865, 1.0, 1.0, 0.40691985958117394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19708229014586776, 0.19708229014586776, 0.2758480263692727], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.31799492], dtype=float32), -1.1736523]. 
=============================================
[2019-03-27 08:44:11,167] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0789549e-10 9.8055512e-01 2.0104812e-14 1.9444861e-02 5.7133940e-18], sum to 1.0000
[2019-03-27 08:44:11,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1038
[2019-03-27 08:44:11,181] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2075801.764940711 W.
[2019-03-27 08:44:11,191] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.3, 73.5, 1.0, 2.0, 0.8433904516563306, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984768976232193, 6.9112, 168.9124571960734, 2075801.764940711, 2023609.555435839, 419614.9202709057], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7813800.0000, 
sim time next is 7814400.0000, 
raw observation next is [29.4, 73.0, 1.0, 2.0, 0.8687200295884188, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.985042788762417, 6.9112, 168.9124573416385, 2111254.106747428, 2058867.645738787, 426292.7408397796], 
processed observation next is [1.0, 0.43478260869565216, 0.5924170616113744, 0.73, 1.0, 1.0, 0.8418313609499021, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007384278876241712, 0.0, 0.829437494005011, 0.5864594740965078, 0.5719076793718852, 0.6362578221489248], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6140597], dtype=float32), 0.0052584126]. 
=============================================
[2019-03-27 08:44:11,461] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.7768354e-19 1.0000000e+00 1.5594360e-23 1.3030182e-14 5.9118118e-27], sum to 1.0000
[2019-03-27 08:44:11,472] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9825
[2019-03-27 08:44:11,475] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 91.0, 1.0, 2.0, 0.5253104860156839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734052.0078323279, 734052.0078323279, 187656.7099398702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7772400.0000, 
sim time next is 7773000.0000, 
raw observation next is [26.4, 90.66666666666667, 1.0, 2.0, 0.5228763296238115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730649.426023216, 730649.426023216, 187257.9797993716], 
processed observation next is [1.0, 1.0, 0.45023696682464454, 0.9066666666666667, 1.0, 1.0, 0.4251522043660379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20295817389533777, 0.20295817389533777, 0.27948952208861433], 
reward next is 0.7205, 
noisyNet noise sample is [array([0.30125523], dtype=float32), -1.3278104]. 
=============================================
[2019-03-27 08:44:11,489] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.453514]
 [71.39266 ]
 [71.41475 ]
 [71.432655]
 [71.4544  ]], R is [[71.4944458 ]
 [71.49942017]
 [71.50396729]
 [71.50826263]
 [71.51274109]].
[2019-03-27 08:44:12,973] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:12,974] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:13,041] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run11
[2019-03-27 08:44:14,728] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2119168: loss 82.6657
[2019-03-27 08:44:14,730] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2119168: learning rate 0.0000
[2019-03-27 08:44:16,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:16,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:16,229] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run11
[2019-03-27 08:44:16,804] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2120172: loss 0.0452
[2019-03-27 08:44:16,806] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2120172: learning rate 0.0000
[2019-03-27 08:44:17,881] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2120756: loss 83.6545
[2019-03-27 08:44:17,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2120756: learning rate 0.0000
[2019-03-27 08:44:18,393] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2120996: loss 0.0343
[2019-03-27 08:44:18,396] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2120996: learning rate 0.0000
[2019-03-27 08:44:18,462] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.05078864e-16 1.00000000e+00 6.33235562e-23 9.66753262e-13
 7.21195868e-26], sum to 1.0000
[2019-03-27 08:44:18,470] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3379
[2019-03-27 08:44:18,481] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 89.5, 1.0, 2.0, 0.5937695231609816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 829751.8329669524, 829751.8329669529, 199584.9481805256], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7878600.0000, 
sim time next is 7879200.0000, 
raw observation next is [26.26666666666667, 89.33333333333333, 1.0, 2.0, 0.693426089341136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 969078.5251910627, 969078.5251910621, 219431.0477504276], 
processed observation next is [1.0, 0.17391304347826086, 0.44391785150079005, 0.8933333333333333, 1.0, 1.0, 0.6306338425796819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26918847921973965, 0.2691884792197395, 0.32750902649317554], 
reward next is 0.6725, 
noisyNet noise sample is [array([0.85500574], dtype=float32), 2.3156722]. 
=============================================
[2019-03-27 08:44:19,240] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:19,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:19,297] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run11
[2019-03-27 08:44:20,922] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2122307: loss 82.7997
[2019-03-27 08:44:20,924] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2122308: learning rate 0.0000
[2019-03-27 08:44:21,533] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:21,535] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:21,609] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run11
[2019-03-27 08:44:21,949] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.18433875e-17 1.00000000e+00 5.60762879e-24 2.39910277e-16
 8.20385605e-27], sum to 1.0000
[2019-03-27 08:44:21,955] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0468
[2019-03-27 08:44:21,959] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.03333333333333, 86.66666666666667, 1.0, 2.0, 0.2377911770333683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 393784.5974520952, 393784.5974520946, 159595.9677909474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 686400.0000, 
sim time next is 687000.0000, 
raw observation next is [18.91666666666667, 87.33333333333334, 1.0, 2.0, 0.2361683106435371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 391235.6798511617, 391235.6798511617, 159430.0451352234], 
processed observation next is [1.0, 0.9565217391304348, 0.09557661927330202, 0.8733333333333334, 1.0, 1.0, 0.07972085619703265, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10867657773643381, 0.10867657773643381, 0.23795529124660209], 
reward next is 0.7620, 
noisyNet noise sample is [array([-1.6088436], dtype=float32), -0.034322567]. 
=============================================
[2019-03-27 08:44:21,972] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.17422 ]
 [75.173134]
 [75.17957 ]
 [75.193   ]
 [75.18254 ]], R is [[75.18287659]
 [75.19284058]
 [75.20240784]
 [75.21166229]
 [75.22079468]].
[2019-03-27 08:44:22,409] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2123081: loss 0.0034
[2019-03-27 08:44:22,410] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2123081: learning rate 0.0000
[2019-03-27 08:44:22,564] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:22,566] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:22,603] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run11
[2019-03-27 08:44:22,889] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:22,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:22,915] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:22,915] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:22,946] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run11
[2019-03-27 08:44:22,965] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:22,967] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:22,978] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2123407: loss 86.9785
[2019-03-27 08:44:22,979] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2123407: learning rate 0.0000
[2019-03-27 08:44:22,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run11
[2019-03-27 08:44:23,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run11
[2019-03-27 08:44:23,105] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:23,105] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:23,138] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run11
[2019-03-27 08:44:23,397] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:23,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:23,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run11
[2019-03-27 08:44:23,676] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.5730470e-15 1.0000000e+00 4.3676324e-21 1.7728900e-11 5.6395067e-24], sum to 1.0000
[2019-03-27 08:44:23,676] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9740
[2019-03-27 08:44:23,680] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 64.33333333333334, 1.0, 2.0, 0.8970109981965372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1356564.801339249, 1356564.80133925, 283653.159163759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 44400.0000, 
sim time next is 45000.0000, 
raw observation next is [27.1, 64.0, 1.0, 2.0, 0.8708015056141055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1315394.22188225, 1315394.221882251, 275750.6015969994], 
processed observation next is [1.0, 0.5217391304347826, 0.4834123222748816, 0.64, 1.0, 1.0, 0.8443391633904885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3653872838561805, 0.36538728385618086, 0.41156806208507374], 
reward next is 0.5884, 
noisyNet noise sample is [array([-0.2906976], dtype=float32), -2.2173023]. 
=============================================
[2019-03-27 08:44:23,701] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[63.081814]
 [62.96269 ]
 [62.612164]
 [62.59437 ]
 [62.99599 ]], R is [[63.23789597]
 [63.18215561]
 [63.12640381]
 [63.03168106]
 [62.91769791]].
[2019-03-27 08:44:23,738] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:23,739] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:23,767] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run11
[2019-03-27 08:44:23,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:23,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:23,875] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run11
[2019-03-27 08:44:23,899] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 08:44:23,899] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:23,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run11
[2019-03-27 08:44:24,185] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2124001: loss 87.5402
[2019-03-27 08:44:24,186] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2124001: learning rate 0.0000
[2019-03-27 08:44:24,427] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2124163: loss 87.4871
[2019-03-27 08:44:24,429] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2124163: learning rate 0.0000
[2019-03-27 08:44:24,467] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2124190: loss 91.0155
[2019-03-27 08:44:24,467] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2124190: learning rate 0.0000
[2019-03-27 08:44:24,629] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2124300: loss 88.5651
[2019-03-27 08:44:24,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2124300: learning rate 0.0000
[2019-03-27 08:44:24,765] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2124396: loss 89.1042
[2019-03-27 08:44:24,767] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2124396: learning rate 0.0000
[2019-03-27 08:44:25,030] A3C_AGENT_WORKER-Thread-22 INFO:Local step 132500, global step 2124577: loss 88.4790
[2019-03-27 08:44:25,032] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 132500, global step 2124577: learning rate 0.0000
[2019-03-27 08:44:25,444] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2124796: loss 89.4865
[2019-03-27 08:44:25,447] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2124796: learning rate 0.0000
[2019-03-27 08:44:25,692] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2124913: loss 88.1659
[2019-03-27 08:44:25,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2124914: learning rate 0.0000
[2019-03-27 08:44:25,729] A3C_AGENT_WORKER-Thread-21 INFO:Local step 132500, global step 2124932: loss 91.0210
[2019-03-27 08:44:25,730] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 132500, global step 2124933: learning rate 0.0000
[2019-03-27 08:44:25,876] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 08:44:25,881] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:44:25,882] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:44:25,882] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:25,883] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:25,885] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:44:25,884] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:44:25,886] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:44:25,886] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:25,889] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:25,888] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:44:25,911] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run86
[2019-03-27 08:44:25,912] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run86
[2019-03-27 08:44:25,958] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run86
[2019-03-27 08:44:25,959] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run86
[2019-03-27 08:44:26,013] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run86
[2019-03-27 08:44:56,668] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08568506], dtype=float32), 0.049825884]
[2019-03-27 08:44:56,669] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.26666666666667, 96.0, 1.0, 2.0, 0.4668021613574072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655153.3928290574, 655153.3928290581, 178931.0186076815]
[2019-03-27 08:44:56,670] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:44:56,673] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.5366910e-19 1.0000000e+00 1.1067203e-25 2.4102732e-18 1.5914271e-28], sampled 0.3422094659670073
[2019-03-27 08:44:57,216] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08568506], dtype=float32), 0.049825884]
[2019-03-27 08:44:57,217] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 75.16666666666667, 1.0, 2.0, 0.5585308538496467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780490.1379214241, 780490.1379214241, 193275.1673035178]
[2019-03-27 08:44:57,220] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:44:57,224] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.5975081e-19 1.0000000e+00 8.7629979e-26 3.1788979e-18 9.4388256e-29], sampled 0.019221937734395955
[2019-03-27 08:45:15,694] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08568506], dtype=float32), 0.049825884]
[2019-03-27 08:45:15,695] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.2153261, 89.2245147, 1.0, 2.0, 0.5386695346907484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752726.1277532113, 752726.1277532119, 189874.4756388918]
[2019-03-27 08:45:15,697] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:45:15,703] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2249433e-18 1.0000000e+00 3.2706997e-25 1.1618215e-17 4.8358204e-28], sampled 0.1050411623813613
[2019-03-27 08:45:26,523] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08568506], dtype=float32), 0.049825884]
[2019-03-27 08:45:26,525] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.27116354333333, 70.57738182333333, 1.0, 2.0, 0.5662235033255647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 791243.8380683677, 791243.8380683683, 194620.7208798142]
[2019-03-27 08:45:26,526] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:45:26,530] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0927390e-19 1.0000000e+00 4.2145365e-26 3.6713482e-17 1.7805800e-29], sampled 0.2619179957180069
[2019-03-27 08:45:53,565] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08568506], dtype=float32), 0.049825884]
[2019-03-27 08:45:53,566] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.06666666666667, 93.16666666666667, 1.0, 2.0, 0.7470131953901096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1044004.520233728, 1044004.520233727, 231319.4086159989]
[2019-03-27 08:45:53,566] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:45:53,570] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.3092726e-18 1.0000000e+00 2.5210180e-24 3.7937335e-16 3.4604262e-27], sampled 0.3601928255195296
[2019-03-27 08:46:21,267] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.1357 2927273831.4906 1338.0000
[2019-03-27 08:46:21,683] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.9050 2842349719.2819 1129.0000
[2019-03-27 08:46:21,698] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.3310 3007641474.8118 1762.0000
[2019-03-27 08:46:21,778] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5310 2779331587.3780 933.0000
[2019-03-27 08:46:21,814] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.7695 3163717900.4754 1764.0000
[2019-03-27 08:46:22,830] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2125000, evaluation results [2125000.0, 7885.769494314334, 3163717900.475423, 1764.0, 8258.135681360105, 2927273831.490609, 1338.0, 8660.53097408367, 2779331587.377975, 933.0, 7999.33095301167, 3007641474.8117747, 1762.0, 8496.905041200442, 2842349719.2819376, 1129.0]
[2019-03-27 08:46:23,757] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2125424: loss 0.0003
[2019-03-27 08:46:23,760] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2125424: learning rate 0.0000
[2019-03-27 08:46:26,334] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2126540: loss 0.0032
[2019-03-27 08:46:26,337] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2126540: learning rate 0.0000
[2019-03-27 08:46:27,669] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2127166: loss 0.0002
[2019-03-27 08:46:27,671] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2127166: learning rate 0.0000
[2019-03-27 08:46:28,197] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2127411: loss 0.0018
[2019-03-27 08:46:28,199] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2127411: learning rate 0.0000
[2019-03-27 08:46:31,209] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2128802: loss 0.0005
[2019-03-27 08:46:31,215] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2128802: learning rate 0.0000
[2019-03-27 08:46:32,223] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4663511e-19 1.0000000e+00 2.5147064e-25 3.7745827e-21 5.8362484e-29], sum to 1.0000
[2019-03-27 08:46:32,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4854
[2019-03-27 08:46:32,239] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.48333333333333, 93.0, 1.0, 2.0, 0.292651773939553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469284.2277431712, 469284.2277431712, 164903.6525903517], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 204600.0000, 
sim time next is 205200.0000, 
raw observation next is [20.5, 93.0, 1.0, 2.0, 0.2927604075809669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469301.1687311629, 469301.1687311629, 164903.6486414795], 
processed observation next is [0.0, 0.391304347826087, 0.1706161137440759, 0.93, 1.0, 1.0, 0.14790410551923722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13036143575865636, 0.13036143575865636, 0.2461248487186261], 
reward next is 0.7539, 
noisyNet noise sample is [array([-0.5140767], dtype=float32), 1.2201512]. 
=============================================
[2019-03-27 08:46:33,713] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2722240e-17 1.0000000e+00 7.7059688e-24 4.1660947e-15 8.0154609e-28], sum to 1.0000
[2019-03-27 08:46:33,719] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8352
[2019-03-27 08:46:33,723] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.36666666666667, 97.0, 1.0, 2.0, 0.3802825846655978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572803.730547473, 572803.730547473, 172234.9270182879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1039800.0000, 
sim time next is 1040400.0000, 
raw observation next is [22.4, 97.0, 1.0, 2.0, 0.3809588480346077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573181.2049272379, 573181.2049272379, 172248.3978779776], 
processed observation next is [1.0, 0.043478260869565216, 0.2606635071090047, 0.97, 1.0, 1.0, 0.254167286788684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1592170013686772, 0.1592170013686772, 0.2570871610119069], 
reward next is 0.7429, 
noisyNet noise sample is [array([-0.06653043], dtype=float32), -1.4809706]. 
=============================================
[2019-03-27 08:46:33,800] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2130017: loss 0.0063
[2019-03-27 08:46:33,806] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2130019: learning rate 0.0000
[2019-03-27 08:46:35,257] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2130709: loss 0.0002
[2019-03-27 08:46:35,261] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2130710: learning rate 0.0000
[2019-03-27 08:46:35,905] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1962675e-16 1.0000000e+00 3.6074896e-23 1.0400061e-15 2.3097707e-27], sum to 1.0000
[2019-03-27 08:46:35,915] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8001
[2019-03-27 08:46:35,918] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 73.0, 1.0, 2.0, 0.4791762917907562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773024.1119324766, 773024.1119324766, 191641.6318986931], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 394200.0000, 
sim time next is 394800.0000, 
raw observation next is [22.83333333333333, 73.0, 1.0, 2.0, 0.5187384732909831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836339.7480974811, 836339.7480974811, 198703.6201582337], 
processed observation next is [1.0, 0.5652173913043478, 0.2812006319115322, 0.73, 1.0, 1.0, 0.42016683529034105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23231659669374474, 0.23231659669374474, 0.2965725674003488], 
reward next is 0.7034, 
noisyNet noise sample is [array([0.07485447], dtype=float32), -0.9653552]. 
=============================================
[2019-03-27 08:46:37,618] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2131808: loss 0.0013
[2019-03-27 08:46:37,624] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2131810: learning rate 0.0000
[2019-03-27 08:46:38,151] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2132055: loss 0.0004
[2019-03-27 08:46:38,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2132057: learning rate 0.0000
[2019-03-27 08:46:38,316] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2132133: loss 0.0007
[2019-03-27 08:46:38,317] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2132133: learning rate 0.0000
[2019-03-27 08:46:38,579] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2132258: loss 0.0006
[2019-03-27 08:46:38,582] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2132258: learning rate 0.0000
[2019-03-27 08:46:38,648] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2132286: loss 0.0005
[2019-03-27 08:46:38,650] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2132286: learning rate 0.0000
[2019-03-27 08:46:39,281] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133000, global step 2132582: loss 0.0002
[2019-03-27 08:46:39,286] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133000, global step 2132582: learning rate 0.0000
[2019-03-27 08:46:39,320] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.9745612e-21 1.0000000e+00 4.4647491e-28 2.9252635e-20 1.2489758e-31], sum to 1.0000
[2019-03-27 08:46:39,326] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7215
[2019-03-27 08:46:39,332] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 85.0, 1.0, 2.0, 0.2886586707403454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462513.7222599693, 462513.72225997, 164432.0790236724], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 329400.0000, 
sim time next is 330000.0000, 
raw observation next is [21.43333333333333, 85.33333333333333, 1.0, 2.0, 0.2878063677158854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461361.9366500977, 461361.9366500982, 164355.0262740652], 
processed observation next is [0.0, 0.8260869565217391, 0.21484992101105835, 0.8533333333333333, 1.0, 1.0, 0.14193538279022339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12815609351391602, 0.12815609351391616, 0.2453060093642764], 
reward next is 0.7547, 
noisyNet noise sample is [array([-0.19658305], dtype=float32), 0.37829387]. 
=============================================
[2019-03-27 08:46:39,362] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.77188 ]
 [78.75508 ]
 [78.73912 ]
 [78.700516]
 [78.63921 ]], R is [[78.7512207 ]
 [78.71828461]
 [78.6855545 ]
 [78.65299225]
 [78.62050629]].
[2019-03-27 08:46:39,739] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2132794: loss 0.0002
[2019-03-27 08:46:39,746] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2132796: learning rate 0.0000
[2019-03-27 08:46:40,049] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2132938: loss 0.0002
[2019-03-27 08:46:40,056] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2132939: learning rate 0.0000
[2019-03-27 08:46:40,090] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133000, global step 2132957: loss 0.0002
[2019-03-27 08:46:40,091] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133000, global step 2132957: learning rate 0.0000
[2019-03-27 08:46:41,066] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2133414: loss 0.0131
[2019-03-27 08:46:41,070] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2133414: learning rate 0.0000
[2019-03-27 08:46:43,551] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2134583: loss 4.0884
[2019-03-27 08:46:43,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2134583: learning rate 0.0000
[2019-03-27 08:46:43,878] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2628100e-17 1.0000000e+00 1.0078889e-23 4.2221395e-15 2.0910089e-25], sum to 1.0000
[2019-03-27 08:46:43,887] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0641
[2019-03-27 08:46:43,893] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 79.0, 1.0, 2.0, 0.5194579589806202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832403.5516147703, 832403.5516147703, 198503.980414632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 403200.0000, 
sim time next is 403800.0000, 
raw observation next is [22.25, 79.50000000000001, 1.0, 2.0, 0.6505882583214113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1042132.917662223, 1042132.917662223, 225708.4408434813], 
processed observation next is [1.0, 0.6956521739130435, 0.2535545023696683, 0.7950000000000002, 1.0, 1.0, 0.579021997977604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28948136601728414, 0.28948136601728414, 0.33687826991564374], 
reward next is 0.6631, 
noisyNet noise sample is [array([-0.1024808], dtype=float32), 0.3806465]. 
=============================================
[2019-03-27 08:46:44,750] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2135148: loss 0.0111
[2019-03-27 08:46:44,754] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2135148: learning rate 0.0000
[2019-03-27 08:46:45,331] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2135401: loss 4.1318
[2019-03-27 08:46:45,336] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2135402: learning rate 0.0000
[2019-03-27 08:46:48,186] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2136742: loss 0.0158
[2019-03-27 08:46:48,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2136743: learning rate 0.0000
[2019-03-27 08:46:48,969] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.42016652e-20 1.00000000e+00 1.13576465e-26 1.23582231e-19
 4.18211324e-30], sum to 1.0000
[2019-03-27 08:46:48,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8499
[2019-03-27 08:46:48,985] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 88.0, 1.0, 2.0, 0.2858749732194508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459951.6349758428, 459951.6349758428, 164268.9991846066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 878400.0000, 
sim time next is 879000.0000, 
raw observation next is [20.88333333333333, 87.83333333333334, 1.0, 2.0, 0.2841133831069969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457440.0960175311, 457440.0960175318, 164099.2225834128], 
processed observation next is [0.0, 0.17391304347826086, 0.18878357030015785, 0.8783333333333334, 1.0, 1.0, 0.13748600374336975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12706669333820308, 0.12706669333820328, 0.24492421281106388], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.72158164], dtype=float32), -1.0980997]. 
=============================================
[2019-03-27 08:46:49,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.4416 ]
 [76.31745]
 [76.29119]
 [76.24879]
 [76.31663]], R is [[76.50512695]
 [76.49489594]
 [76.48472595]
 [76.47462463]
 [76.46456909]].
[2019-03-27 08:46:49,472] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9728200e-18 1.0000000e+00 3.2921798e-25 1.7972623e-17 1.2504085e-27], sum to 1.0000
[2019-03-27 08:46:49,479] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5773
[2019-03-27 08:46:49,486] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333334, 72.33333333333334, 1.0, 2.0, 0.3499500078984439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 577136.3519913014, 577136.351991302, 172492.5667516778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 634800.0000, 
sim time next is 635400.0000, 
raw observation next is [21.5, 71.5, 1.0, 2.0, 0.3807037680022441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627504.771609925, 627504.771609925, 176693.1715568655], 
processed observation next is [1.0, 0.34782608695652173, 0.21800947867298584, 0.715, 1.0, 1.0, 0.2538599614484869, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17430688100275693, 0.17430688100275693, 0.2637211515774112], 
reward next is 0.7363, 
noisyNet noise sample is [array([0.69165903], dtype=float32), -0.5390772]. 
=============================================
[2019-03-27 08:46:50,871] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2137997: loss 0.0026
[2019-03-27 08:46:50,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2137997: learning rate 0.0000
[2019-03-27 08:46:52,465] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2138746: loss 0.0272
[2019-03-27 08:46:52,471] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2138748: learning rate 0.0000
[2019-03-27 08:46:54,628] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2139761: loss 0.0155
[2019-03-27 08:46:54,633] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2139762: learning rate 0.0000
[2019-03-27 08:46:55,147] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2140004: loss 0.0181
[2019-03-27 08:46:55,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2140005: learning rate 0.0000
[2019-03-27 08:46:55,336] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2140097: loss 0.0153
[2019-03-27 08:46:55,338] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2140097: learning rate 0.0000
[2019-03-27 08:46:55,697] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2140264: loss 0.0139
[2019-03-27 08:46:55,698] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2140264: learning rate 0.0000
[2019-03-27 08:46:55,704] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2140266: loss 0.0157
[2019-03-27 08:46:55,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2140266: learning rate 0.0000
[2019-03-27 08:46:56,484] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133500, global step 2140628: loss 0.0114
[2019-03-27 08:46:56,490] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133500, global step 2140630: learning rate 0.0000
[2019-03-27 08:46:56,738] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2140750: loss 0.0089
[2019-03-27 08:46:56,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2140751: learning rate 0.0000
[2019-03-27 08:46:57,091] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2140917: loss 0.0116
[2019-03-27 08:46:57,097] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2140920: learning rate 0.0000
[2019-03-27 08:46:57,266] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133500, global step 2140998: loss 0.0141
[2019-03-27 08:46:57,268] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133500, global step 2140998: learning rate 0.0000
[2019-03-27 08:46:58,210] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2141439: loss 0.0023
[2019-03-27 08:46:58,215] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2141440: learning rate 0.0000
[2019-03-27 08:47:00,919] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2142691: loss 0.0298
[2019-03-27 08:47:00,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2142691: learning rate 0.0000
[2019-03-27 08:47:01,821] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2143121: loss 0.0026
[2019-03-27 08:47:01,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2143122: learning rate 0.0000
[2019-03-27 08:47:02,607] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2143492: loss 0.0299
[2019-03-27 08:47:02,615] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2143492: learning rate 0.0000
[2019-03-27 08:47:05,197] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2144708: loss 0.0028
[2019-03-27 08:47:05,199] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2144708: learning rate 0.0000
[2019-03-27 08:47:07,938] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2145993: loss 4.0431
[2019-03-27 08:47:07,941] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2145993: learning rate 0.0000
[2019-03-27 08:47:09,598] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2146766: loss 0.0015
[2019-03-27 08:47:09,600] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2146767: learning rate 0.0000
[2019-03-27 08:47:11,788] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2147793: loss 0.0005
[2019-03-27 08:47:11,791] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2147793: learning rate 0.0000
[2019-03-27 08:47:12,279] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2148020: loss 0.0006
[2019-03-27 08:47:12,281] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2148020: learning rate 0.0000
[2019-03-27 08:47:12,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9271404e-19 1.0000000e+00 2.8904024e-26 1.4108527e-19 1.9296492e-28], sum to 1.0000
[2019-03-27 08:47:12,513] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4316
[2019-03-27 08:47:12,519] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 72.66666666666667, 1.0, 2.0, 0.2990771172722378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475818.0563652861, 475818.0563652861, 165316.3083429248], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 908400.0000, 
sim time next is 909000.0000, 
raw observation next is [23.75, 72.0, 1.0, 2.0, 0.3003861962009937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 477419.4241391998, 477419.4241391998, 165421.8145714712], 
processed observation next is [0.0, 0.5217391304347826, 0.3246445497630332, 0.72, 1.0, 1.0, 0.15709180265179964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13261650670533326, 0.13261650670533326, 0.24689823070368833], 
reward next is 0.7531, 
noisyNet noise sample is [array([0.05794033], dtype=float32), -1.3934901]. 
=============================================
[2019-03-27 08:47:12,528] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[76.2479  ]
 [76.256035]
 [76.23793 ]
 [76.19359 ]
 [76.2024  ]], R is [[76.23129272]
 [76.22223663]
 [76.21346283]
 [76.20490265]
 [76.1964035 ]].
[2019-03-27 08:47:12,558] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2148150: loss 0.0006
[2019-03-27 08:47:12,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2148151: learning rate 0.0000
[2019-03-27 08:47:12,676] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2148206: loss 0.0005
[2019-03-27 08:47:12,677] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2148206: learning rate 0.0000
[2019-03-27 08:47:12,699] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2148215: loss 0.0009
[2019-03-27 08:47:12,701] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2148215: learning rate 0.0000
[2019-03-27 08:47:13,646] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134000, global step 2148657: loss 0.0020
[2019-03-27 08:47:13,650] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134000, global step 2148658: learning rate 0.0000
[2019-03-27 08:47:13,802] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2148729: loss 0.0011
[2019-03-27 08:47:13,804] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2148729: learning rate 0.0000
[2019-03-27 08:47:14,230] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2148931: loss 0.0007
[2019-03-27 08:47:14,232] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2148932: learning rate 0.0000
[2019-03-27 08:47:14,246] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134000, global step 2148936: loss 0.0008
[2019-03-27 08:47:14,248] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134000, global step 2148937: learning rate 0.0000
[2019-03-27 08:47:15,239] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2149411: loss 3.8035
[2019-03-27 08:47:15,242] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2149411: learning rate 0.0000
[2019-03-27 08:47:16,526] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 08:47:16,528] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:47:16,530] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:47:16,530] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:47:16,531] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:47:16,531] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:47:16,533] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:47:16,534] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:47:16,532] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:47:16,538] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:47:16,540] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:47:16,559] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run87
[2019-03-27 08:47:16,584] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run87
[2019-03-27 08:47:16,607] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run87
[2019-03-27 08:47:16,637] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run87
[2019-03-27 08:47:16,638] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run87
[2019-03-27 08:47:23,271] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08581722], dtype=float32), 0.04949684]
[2019-03-27 08:47:23,273] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.03772096, 84.917156675, 1.0, 2.0, 0.2738540149215576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 444010.0314252596, 444010.0314252602, 163190.8806463836]
[2019-03-27 08:47:23,275] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:47:23,277] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.3802940e-19 1.0000000e+00 2.0016356e-25 2.0907852e-18 5.1811363e-28], sampled 0.16365260540940585
[2019-03-27 08:47:33,946] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08581722], dtype=float32), 0.04949684]
[2019-03-27 08:47:33,948] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.94355945333333, 74.82484829, 1.0, 2.0, 0.4452937487179847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 699927.5965911798, 699927.5965911798, 184552.9813536255]
[2019-03-27 08:47:33,953] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:47:33,955] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.3988880e-19 1.0000000e+00 2.0666717e-25 1.3316939e-18 5.2873773e-28], sampled 0.5715079305603619
[2019-03-27 08:47:53,048] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08581722], dtype=float32), 0.04949684]
[2019-03-27 08:47:53,049] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.13333333333333, 85.66666666666666, 1.0, 2.0, 0.4574746083730273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656662.5372029713, 656662.5372029713, 179434.412032361]
[2019-03-27 08:47:53,053] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:47:53,056] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6558979e-18 1.0000000e+00 5.8860299e-25 1.3695617e-16 3.3062329e-28], sampled 0.09268136185385434
[2019-03-27 08:48:27,867] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08581722], dtype=float32), 0.04949684]
[2019-03-27 08:48:27,868] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.90000000000001, 47.0, 1.0, 2.0, 0.669346844018579, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005973369574648, 6.9112, 168.9123160559816, 1832235.048614313, 1764999.813322123, 378142.6319065233]
[2019-03-27 08:48:27,869] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:48:27,871] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.7462253e-11 9.9906904e-01 2.0317934e-15 9.3096693e-04 1.9896960e-19], sampled 0.8889653366488252
[2019-03-27 08:48:27,874] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1832235.048614313 W.
[2019-03-27 08:48:53,744] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08581722], dtype=float32), 0.04949684]
[2019-03-27 08:48:53,745] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.15839606666667, 94.31140883666666, 1.0, 2.0, 0.5469434816002803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764292.1366755594, 764292.13667556, 191276.8786889266]
[2019-03-27 08:48:53,747] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:48:53,751] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2422700e-18 1.0000000e+00 3.7164384e-25 4.7027865e-18 8.6843860e-28], sampled 0.7038454686127882
[2019-03-27 08:49:09,968] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08581722], dtype=float32), 0.04949684]
[2019-03-27 08:49:09,970] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.78727269333334, 83.76104619666665, 1.0, 2.0, 0.4878440262586547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685645.8308558045, 685645.8308558045, 182223.0981327837]
[2019-03-27 08:49:09,972] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:49:09,975] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.1539323e-18 1.0000000e+00 1.3611187e-24 2.4449842e-16 1.5623802e-27], sampled 0.7215621661259486
[2019-03-27 08:49:10,319] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08581722], dtype=float32), 0.04949684]
[2019-03-27 08:49:10,320] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.16666666666666, 66.0, 1.0, 2.0, 0.3234745646630728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508887.3206039656, 508887.3206039656, 167642.6049288637]
[2019-03-27 08:49:10,322] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:49:10,324] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6941452e-19 1.0000000e+00 6.0685159e-26 3.2121010e-19 1.9017121e-28], sampled 0.7789965714685283
[2019-03-27 08:49:12,068] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.1725 2779322230.8975 933.0000
[2019-03-27 08:49:12,297] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4092 2842340237.9865 1129.0000
[2019-03-27 08:49:12,347] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.5794 3007466187.1991 1766.0000
[2019-03-27 08:49:12,348] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.4028 3163669719.4832 1765.0000
[2019-03-27 08:49:12,372] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9350 2927404666.0075 1338.0000
[2019-03-27 08:49:13,389] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2150000, evaluation results [2150000.0, 7886.402839507556, 3163669719.4832387, 1765.0, 8254.934989388352, 2927404666.00751, 1338.0, 8661.172501768742, 2779322230.897455, 933.0, 7999.579361471634, 3007466187.1990886, 1766.0, 8497.409233554366, 2842340237.986515, 1129.0]
[2019-03-27 08:49:15,038] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2150750: loss 0.0004
[2019-03-27 08:49:15,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2150750: learning rate 0.0000
[2019-03-27 08:49:15,852] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2151078: loss 3.7762
[2019-03-27 08:49:15,854] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2151078: learning rate 0.0000
[2019-03-27 08:49:16,790] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2151480: loss 0.0004
[2019-03-27 08:49:16,794] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2151480: learning rate 0.0000
[2019-03-27 08:49:17,419] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2015466e-18 1.0000000e+00 3.0044801e-24 3.0903489e-16 5.8068709e-28], sum to 1.0000
[2019-03-27 08:49:17,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4087
[2019-03-27 08:49:17,437] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4583264292921354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 649845.9318804906, 649845.9318804899, 178539.1992395844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1296000.0000, 
sim time next is 1296600.0000, 
raw observation next is [24.3, 94.00000000000001, 1.0, 2.0, 0.4584754386161387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650208.60854035, 650208.60854035, 178580.432649693], 
processed observation next is [1.0, 0.0, 0.3507109004739337, 0.9400000000000002, 1.0, 1.0, 0.34756076941703457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18061350237231943, 0.18061350237231943, 0.26653795917864626], 
reward next is 0.7335, 
noisyNet noise sample is [array([-0.24101888], dtype=float32), -1.4287047]. 
=============================================
[2019-03-27 08:49:19,213] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2152620: loss 3.7370
[2019-03-27 08:49:19,214] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2152620: learning rate 0.0000
[2019-03-27 08:49:22,150] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2153984: loss 0.0417
[2019-03-27 08:49:22,152] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2153985: learning rate 0.0000
[2019-03-27 08:49:23,801] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2154759: loss 3.7905
[2019-03-27 08:49:23,804] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2154760: learning rate 0.0000
[2019-03-27 08:49:25,816] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2155701: loss 3.7345
[2019-03-27 08:49:25,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2155701: learning rate 0.0000
[2019-03-27 08:49:26,472] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2156006: loss 3.7329
[2019-03-27 08:49:26,474] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2156007: learning rate 0.0000
[2019-03-27 08:49:26,616] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2156075: loss 3.7392
[2019-03-27 08:49:26,620] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2156078: learning rate 0.0000
[2019-03-27 08:49:26,712] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2156119: loss 3.7430
[2019-03-27 08:49:26,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2156119: learning rate 0.0000
[2019-03-27 08:49:26,820] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2156169: loss 3.7100
[2019-03-27 08:49:26,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2156170: learning rate 0.0000
[2019-03-27 08:49:27,151] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7568362e-19 1.0000000e+00 4.8443369e-26 1.9782020e-18 3.6964665e-29], sum to 1.0000
[2019-03-27 08:49:27,161] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6155
[2019-03-27 08:49:27,169] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 96.0, 1.0, 2.0, 0.3584876783872445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549730.4218779436, 549730.4218779436, 170551.8880829734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1467000.0000, 
sim time next is 1467600.0000, 
raw observation next is [21.86666666666667, 96.0, 1.0, 2.0, 0.3577710798312892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549178.7174393281, 549178.7174393281, 170521.7732477924], 
processed observation next is [0.0, 1.0, 0.23538704581358633, 0.96, 1.0, 1.0, 0.22623021666420384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1525496437331467, 0.1525496437331467, 0.25451010932506324], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.79244244], dtype=float32), -0.37450927]. 
=============================================
[2019-03-27 08:49:27,688] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134500, global step 2156574: loss 3.6736
[2019-03-27 08:49:27,692] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134500, global step 2156575: learning rate 0.0000
[2019-03-27 08:49:27,805] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2156628: loss 3.6418
[2019-03-27 08:49:27,807] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2156629: learning rate 0.0000
[2019-03-27 08:49:28,255] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2156836: loss 3.6325
[2019-03-27 08:49:28,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2156837: learning rate 0.0000
[2019-03-27 08:49:28,428] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134500, global step 2156920: loss 3.6886
[2019-03-27 08:49:28,431] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134500, global step 2156920: learning rate 0.0000
[2019-03-27 08:49:29,628] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2157478: loss 0.0656
[2019-03-27 08:49:29,630] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2157479: learning rate 0.0000
[2019-03-27 08:49:31,036] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1146963e-18 1.0000000e+00 2.4909167e-23 1.8178125e-14 6.4682326e-27], sum to 1.0000
[2019-03-27 08:49:31,046] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1254
[2019-03-27 08:49:31,051] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 91.33333333333334, 1.0, 2.0, 0.471137929398644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662245.1635979016, 662245.1635979016, 179702.9095486352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1285800.0000, 
sim time next is 1286400.0000, 
raw observation next is [24.8, 91.66666666666667, 1.0, 2.0, 0.4715397438129229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662967.2775267791, 662967.2775267785, 179783.0596463284], 
processed observation next is [1.0, 0.9130434782608695, 0.3744075829383887, 0.9166666666666667, 1.0, 1.0, 0.3633008961601481, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18415757709077196, 0.18415757709077182, 0.2683329248452663], 
reward next is 0.7317, 
noisyNet noise sample is [array([0.381469], dtype=float32), -0.77886844]. 
=============================================
[2019-03-27 08:49:32,609] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136000, global step 2158863: loss 0.0013
[2019-03-27 08:49:32,613] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136000, global step 2158863: learning rate 0.0000
[2019-03-27 08:49:32,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1515328e-20 1.0000000e+00 6.8153712e-25 1.7905568e-16 5.7294408e-28], sum to 1.0000
[2019-03-27 08:49:32,696] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0599
[2019-03-27 08:49:32,700] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 94.0, 1.0, 2.0, 0.4601472896373651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651474.5764389839, 651474.5764389839, 178684.6456285214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1294800.0000, 
sim time next is 1295400.0000, 
raw observation next is [24.31666666666667, 94.0, 1.0, 2.0, 0.4592063636277687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650618.5539348436, 650618.5539348436, 178607.5266730238], 
processed observation next is [1.0, 1.0, 0.3515007898894157, 0.94, 1.0, 1.0, 0.3484414019611671, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18072737609301212, 0.18072737609301212, 0.2665783980194385], 
reward next is 0.7334, 
noisyNet noise sample is [array([0.67677206], dtype=float32), 1.6639234]. 
=============================================
[2019-03-27 08:49:33,071] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2159078: loss 0.0653
[2019-03-27 08:49:33,078] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2159078: learning rate 0.0000
[2019-03-27 08:49:34,010] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136000, global step 2159522: loss 0.0017
[2019-03-27 08:49:34,011] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136000, global step 2159522: learning rate 0.0000
[2019-03-27 08:49:34,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4561453e-16 1.0000000e+00 7.0634217e-22 2.6476616e-11 2.0231377e-25], sum to 1.0000
[2019-03-27 08:49:34,129] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4291
[2019-03-27 08:49:34,138] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 92.66666666666667, 1.0, 2.0, 0.8556390976285596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1309805.610083696, 1309805.610083695, 273562.6269897884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1339800.0000, 
sim time next is 1340400.0000, 
raw observation next is [22.26666666666667, 92.33333333333334, 1.0, 2.0, 0.6524538832873579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1001912.57165928, 1001912.571659281, 222064.0386547018], 
processed observation next is [1.0, 0.5217391304347826, 0.2543443917851502, 0.9233333333333335, 1.0, 1.0, 0.5812697389004311, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2783090476831333, 0.27830904768313364, 0.33143886366373404], 
reward next is 0.6686, 
noisyNet noise sample is [array([-1.2321966], dtype=float32), -0.19657512]. 
=============================================
[2019-03-27 08:49:34,833] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4038206e-18 1.0000000e+00 1.4024408e-24 4.3903425e-17 1.1318455e-27], sum to 1.0000
[2019-03-27 08:49:34,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7134
[2019-03-27 08:49:34,846] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.05, 76.83333333333334, 1.0, 2.0, 0.5691461926551131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795329.5492814106, 795329.5492814106, 195139.2908913219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2124600.0000, 
sim time next is 2125200.0000, 
raw observation next is [30.1, 76.66666666666667, 1.0, 2.0, 0.5696814698117408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796077.8302688397, 796077.8302688404, 195234.1858322718], 
processed observation next is [0.0, 0.6086956521739131, 0.6255924170616115, 0.7666666666666667, 1.0, 1.0, 0.48154393953221786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22113273063023325, 0.22113273063023345, 0.29139430721234594], 
reward next is 0.7086, 
noisyNet noise sample is [array([-0.26032475], dtype=float32), 0.89063865]. 
=============================================
[2019-03-27 08:49:36,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.5772380e-20 1.0000000e+00 9.9694781e-26 3.0558818e-17 2.8772740e-29], sum to 1.0000
[2019-03-27 08:49:36,382] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3325
[2019-03-27 08:49:36,386] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666667, 96.33333333333333, 1.0, 2.0, 0.4164400034122114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611138.6189723732, 611138.6189723732, 175287.062005289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1632000.0000, 
sim time next is 1632600.0000, 
raw observation next is [23.15, 96.5, 1.0, 2.0, 0.4165286868239017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611177.3138799083, 611177.3138799083, 175288.096109286], 
processed observation next is [1.0, 0.9130434782608695, 0.2962085308056872, 0.965, 1.0, 1.0, 0.2970225142456647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1697714760777523, 0.1697714760777523, 0.26162402404371043], 
reward next is 0.7384, 
noisyNet noise sample is [array([0.10634076], dtype=float32), -0.31583187]. 
=============================================
[2019-03-27 08:49:36,497] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2160663: loss 0.0510
[2019-03-27 08:49:36,502] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2160663: learning rate 0.0000
[2019-03-27 08:49:39,358] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2162001: loss 0.0009
[2019-03-27 08:49:39,361] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2162001: learning rate 0.0000
[2019-03-27 08:49:41,022] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2162777: loss 0.0536
[2019-03-27 08:49:41,024] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2162777: learning rate 0.0000
[2019-03-27 08:49:43,000] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2163700: loss 0.0560
[2019-03-27 08:49:43,003] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2163700: learning rate 0.0000
[2019-03-27 08:49:43,572] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2163970: loss 0.0582
[2019-03-27 08:49:43,577] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2163971: learning rate 0.0000
[2019-03-27 08:49:43,854] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2164099: loss 0.0584
[2019-03-27 08:49:43,856] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2164099: learning rate 0.0000
[2019-03-27 08:49:43,879] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2164109: loss 0.0542
[2019-03-27 08:49:43,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2164110: learning rate 0.0000
[2019-03-27 08:49:43,964] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2164150: loss 0.0536
[2019-03-27 08:49:43,966] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2164150: learning rate 0.0000
[2019-03-27 08:49:44,864] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135000, global step 2164563: loss 0.0499
[2019-03-27 08:49:44,869] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135000, global step 2164564: learning rate 0.0000
[2019-03-27 08:49:45,114] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2164690: loss 0.0456
[2019-03-27 08:49:45,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2164690: learning rate 0.0000
[2019-03-27 08:49:45,482] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2164858: loss 0.0442
[2019-03-27 08:49:45,484] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2164858: learning rate 0.0000
[2019-03-27 08:49:45,657] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135000, global step 2164940: loss 0.0433
[2019-03-27 08:49:45,661] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135000, global step 2164940: learning rate 0.0000
[2019-03-27 08:49:46,607] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2165386: loss 0.0012
[2019-03-27 08:49:46,613] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2165388: learning rate 0.0000
[2019-03-27 08:49:48,029] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5139205e-17 1.0000000e+00 1.9478995e-23 2.2572097e-14 9.6465813e-26], sum to 1.0000
[2019-03-27 08:49:48,037] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9075
[2019-03-27 08:49:48,042] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 85.0, 1.0, 2.0, 0.6278567526424211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958441.4162134654, 958441.4162134654, 216058.5645501286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1586400.0000, 
sim time next is 1587000.0000, 
raw observation next is [23.48333333333333, 85.0, 1.0, 2.0, 0.6512030949133567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 993527.5107505558, 993527.5107505564, 221069.339980343], 
processed observation next is [1.0, 0.34782608695652173, 0.3120063191153238, 0.85, 1.0, 1.0, 0.5797627649558514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2759798640973766, 0.2759798640973768, 0.3299542387766314], 
reward next is 0.6700, 
noisyNet noise sample is [array([0.84416264], dtype=float32), 0.1525159]. 
=============================================
[2019-03-27 08:49:48,060] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.62127]
 [70.29785]
 [70.86713]
 [71.27767]
 [71.24823]], R is [[69.04537964]
 [69.03244781]
 [69.04051971]
 [69.0690918 ]
 [69.11754608]].
[2019-03-27 08:49:50,303] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136500, global step 2167115: loss 0.1947
[2019-03-27 08:49:50,306] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136500, global step 2167116: learning rate 0.0000
[2019-03-27 08:49:50,328] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2167123: loss 0.0021
[2019-03-27 08:49:50,331] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2167124: learning rate 0.0000
[2019-03-27 08:49:51,872] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136500, global step 2167851: loss 0.0845
[2019-03-27 08:49:51,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136500, global step 2167852: learning rate 0.0000
[2019-03-27 08:49:53,605] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2168664: loss 0.0015
[2019-03-27 08:49:53,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2168664: learning rate 0.0000
[2019-03-27 08:49:56,522] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.73886454e-19 1.00000000e+00 3.19007461e-25 1.14202335e-14
 1.06331050e-28], sum to 1.0000
[2019-03-27 08:49:56,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5545
[2019-03-27 08:49:56,528] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 89.66666666666667, 1.0, 2.0, 0.5123012526966388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715867.1973606171, 715867.1973606177, 185545.9182122971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1718400.0000, 
sim time next is 1719000.0000, 
raw observation next is [26.2, 90.0, 1.0, 2.0, 0.5119653103027512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715397.6081871933, 715397.6081871933, 185492.0489836382], 
processed observation next is [1.0, 0.9130434782608695, 0.44075829383886256, 0.9, 1.0, 1.0, 0.41200639795512184, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1987215578297759, 0.1987215578297759, 0.27685380445319135], 
reward next is 0.7231, 
noisyNet noise sample is [array([1.1399093], dtype=float32), 1.1701096]. 
=============================================
[2019-03-27 08:49:56,558] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.07922 ]
 [72.92899 ]
 [72.99865 ]
 [72.866905]
 [73.069176]], R is [[73.10076141]
 [73.09281921]
 [73.08520508]
 [73.07797241]
 [73.07108307]].
[2019-03-27 08:49:56,635] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136000, global step 2170065: loss 0.0007
[2019-03-27 08:49:56,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136000, global step 2170066: learning rate 0.0000
[2019-03-27 08:49:58,080] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2170748: loss 0.0069
[2019-03-27 08:49:58,081] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2170748: learning rate 0.0000
[2019-03-27 08:49:58,931] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9077192e-15 1.0000000e+00 1.1496548e-22 1.7163057e-12 3.6566018e-25], sum to 1.0000
[2019-03-27 08:49:58,941] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7300
[2019-03-27 08:49:58,945] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.48333333333333, 84.16666666666666, 1.0, 2.0, 0.6487338541182628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1018754.640624687, 1018754.640624687, 223550.733427743], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1777800.0000, 
sim time next is 1778400.0000, 
raw observation next is [22.4, 84.0, 1.0, 2.0, 0.6676535462019483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051126.069954027, 1051126.069954027, 228135.0950940979], 
processed observation next is [1.0, 0.6086956521739131, 0.2606635071090047, 0.84, 1.0, 1.0, 0.5995825857854798, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2919794638761186, 0.2919794638761186, 0.3405001419314894], 
reward next is 0.6595, 
noisyNet noise sample is [array([1.1787976], dtype=float32), -0.25594005]. 
=============================================
[2019-03-27 08:49:59,960] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2171628: loss 0.0024
[2019-03-27 08:49:59,962] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2171628: learning rate 0.0000
[2019-03-27 08:50:00,359] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2171845: loss 0.0019
[2019-03-27 08:50:00,362] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2171845: learning rate 0.0000
[2019-03-27 08:50:00,682] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2172033: loss 0.0010
[2019-03-27 08:50:00,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2172033: learning rate 0.0000
[2019-03-27 08:50:00,725] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2172059: loss 0.0013
[2019-03-27 08:50:00,727] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2172059: learning rate 0.0000
[2019-03-27 08:50:00,739] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2172068: loss 0.0020
[2019-03-27 08:50:00,740] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2172068: learning rate 0.0000
[2019-03-27 08:50:01,414] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135500, global step 2172486: loss 0.0006
[2019-03-27 08:50:01,415] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135500, global step 2172487: learning rate 0.0000
[2019-03-27 08:50:01,568] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2172574: loss 0.0006
[2019-03-27 08:50:01,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2172574: learning rate 0.0000
[2019-03-27 08:50:01,937] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2172751: loss 0.0008
[2019-03-27 08:50:01,939] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2172752: learning rate 0.0000
[2019-03-27 08:50:02,084] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135500, global step 2172817: loss 0.0011
[2019-03-27 08:50:02,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2752522e-17 1.0000000e+00 1.5200785e-23 4.6028370e-14 2.2042341e-27], sum to 1.0000
[2019-03-27 08:50:02,087] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135500, global step 2172818: learning rate 0.0000
[2019-03-27 08:50:02,094] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8582
[2019-03-27 08:50:02,097] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 90.66666666666667, 1.0, 2.0, 0.5097320774808656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712275.9414929863, 712275.9414929857, 185134.6251956974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2587800.0000, 
sim time next is 2588400.0000, 
raw observation next is [25.8, 91.0, 1.0, 2.0, 0.5082032329436343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710138.8910195903, 710138.8910195897, 184890.9914090777], 
processed observation next is [1.0, 1.0, 0.42180094786729866, 0.91, 1.0, 1.0, 0.40747377463088474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1972608030609973, 0.19726080306099714, 0.27595670359563834], 
reward next is 0.7240, 
noisyNet noise sample is [array([0.7897879], dtype=float32), 0.33945084]. 
=============================================
[2019-03-27 08:50:03,204] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136000, global step 2173349: loss 0.0027
[2019-03-27 08:50:03,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136000, global step 2173349: learning rate 0.0000
[2019-03-27 08:50:03,476] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.9034990e-18 1.0000000e+00 6.6320727e-24 4.8652890e-16 2.4004283e-25], sum to 1.0000
[2019-03-27 08:50:03,488] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0983
[2019-03-27 08:50:03,492] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.61666666666667, 94.5, 1.0, 2.0, 0.3781118518647175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 570729.9378662504, 570729.937866251, 172088.9409996399], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1836600.0000, 
sim time next is 1837200.0000, 
raw observation next is [22.73333333333333, 94.0, 1.0, 2.0, 0.3728431648453216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561758.9606749591, 561758.9606749597, 171270.8390035334], 
processed observation next is [1.0, 0.2608695652173913, 0.27646129541864134, 0.94, 1.0, 1.0, 0.24438935523532723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1560441557430442, 0.15604415574304437, 0.2556281179157215], 
reward next is 0.7444, 
noisyNet noise sample is [array([-0.2927079], dtype=float32), -0.4132793]. 
=============================================
[2019-03-27 08:50:06,717] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 08:50:06,722] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:50:06,722] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:50:06,724] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:50:06,725] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:50:06,725] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:50:06,726] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:50:06,726] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:50:06,728] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:50:06,728] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:50:06,730] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:50:06,766] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run88
[2019-03-27 08:50:06,789] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run88
[2019-03-27 08:50:06,824] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run88
[2019-03-27 08:50:06,843] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run88
[2019-03-27 08:50:06,844] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run88
[2019-03-27 08:50:13,116] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08684376], dtype=float32), 0.049450334]
[2019-03-27 08:50:13,118] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.09121241333333, 86.27250939666668, 1.0, 2.0, 0.2637651167340033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 431831.0437453695, 431831.0437453695, 162289.6633288545]
[2019-03-27 08:50:13,119] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:50:13,121] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.2295850e-19 1.0000000e+00 5.6662184e-26 1.1204323e-18 9.1204086e-29], sampled 0.7816633380461461
[2019-03-27 08:51:07,502] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08684376], dtype=float32), 0.049450334]
[2019-03-27 08:51:07,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.13579098833333, 82.09538961166668, 1.0, 2.0, 0.6446476233066668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900880.6114150877, 900880.6114150877, 209359.5862041396]
[2019-03-27 08:51:07,505] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:51:07,508] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6684652e-18 1.0000000e+00 4.6124081e-25 3.8965951e-16 2.4053837e-28], sampled 0.7282520074406315
[2019-03-27 08:51:25,703] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08684376], dtype=float32), 0.049450334]
[2019-03-27 08:51:25,704] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.1, 54.33333333333333, 1.0, 2.0, 0.632854756359278, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.989564146253324, 6.9112, 168.912404082622, 1769524.01597594, 1713929.975358309, 370792.7128534766]
[2019-03-27 08:51:25,705] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:51:25,708] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6879014e-12 9.9999356e-01 2.3609730e-17 6.4956075e-06 2.0456185e-21], sampled 0.2938445309520338
[2019-03-27 08:51:25,709] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1769524.01597594 W.
[2019-03-27 08:52:02,426] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.7019 2842409241.9523 1130.0000
[2019-03-27 08:52:02,531] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.2618 3007648558.5379 1764.0000
[2019-03-27 08:52:02,585] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927288408.7684 1338.0000
[2019-03-27 08:52:02,615] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.6173 2779166974.3587 932.0000
[2019-03-27 08:52:02,641] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7893.0061 3163436049.2878 1758.0000
[2019-03-27 08:52:03,658] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2175000, evaluation results [2175000.0, 7893.006088841335, 3163436049.2878323, 1758.0, 8254.40669044213, 2927288408.768366, 1338.0, 8659.617327087306, 2779166974.3587074, 932.0, 8000.261842967669, 3007648558.537912, 1764.0, 8497.70185504854, 2842409241.9522696, 1130.0]
[2019-03-27 08:52:03,949] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136000, global step 2175138: loss 0.0029
[2019-03-27 08:52:03,952] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136000, global step 2175138: learning rate 0.0000
[2019-03-27 08:52:04,282] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137000, global step 2175286: loss 0.0562
[2019-03-27 08:52:04,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137000, global step 2175289: learning rate 0.0000
[2019-03-27 08:52:05,918] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9345215e-19 1.0000000e+00 6.1705004e-27 3.6974320e-18 8.2127407e-29], sum to 1.0000
[2019-03-27 08:52:05,925] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2841
[2019-03-27 08:52:05,935] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3937032667045529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587464.4672788488, 587464.4672788488, 173385.4817238476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2741400.0000, 
sim time next is 2742000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3931210197645402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586595.8043169726, 586595.804316972, 173306.165648713], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2688205057404099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16294327897693683, 0.16294327897693667, 0.2586659188786761], 
reward next is 0.7413, 
noisyNet noise sample is [array([-0.13622507], dtype=float32), 0.7075781]. 
=============================================
[2019-03-27 08:52:05,950] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.66899 ]
 [75.64884 ]
 [75.63151 ]
 [75.59066 ]
 [75.522675]], R is [[75.67636871]
 [75.66082764]
 [75.64537048]
 [75.63008118]
 [75.61492157]].
[2019-03-27 08:52:05,999] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137000, global step 2176067: loss 0.0684
[2019-03-27 08:52:06,002] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137000, global step 2176067: learning rate 0.0000
[2019-03-27 08:52:07,260] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136000, global step 2176574: loss 0.0018
[2019-03-27 08:52:07,262] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136000, global step 2176574: learning rate 0.0000
[2019-03-27 08:52:08,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1244014e-18 1.0000000e+00 7.8644928e-24 9.8197870e-18 2.2130375e-28], sum to 1.0000
[2019-03-27 08:52:08,223] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2244
[2019-03-27 08:52:08,227] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 94.33333333333334, 1.0, 2.0, 0.500182295465546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698927.1293813366, 698927.1293813366, 183624.9002663266], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2014800.0000, 
sim time next is 2015400.0000, 
raw observation next is [25.5, 94.16666666666667, 1.0, 2.0, 0.5021788138397962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701717.8754074966, 701717.8754074973, 183938.4756841463], 
processed observation next is [0.0, 0.30434782608695654, 0.40758293838862564, 0.9416666666666668, 1.0, 1.0, 0.40021543836120027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19492163205763793, 0.19492163205763813, 0.27453503833454673], 
reward next is 0.7255, 
noisyNet noise sample is [array([0.27326015], dtype=float32), 1.3700411]. 
=============================================
[2019-03-27 08:52:10,511] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.0215098e-19 1.0000000e+00 4.7420593e-25 1.0641599e-18 1.8220206e-28], sum to 1.0000
[2019-03-27 08:52:10,522] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6492
[2019-03-27 08:52:10,527] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.55, 94.0, 1.0, 2.0, 0.5061863276668502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707319.625740364, 707319.6257403634, 184571.1629275885], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2021400.0000, 
sim time next is 2022000.0000, 
raw observation next is [25.53333333333333, 94.0, 1.0, 2.0, 0.5061495915595228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707268.2754399147, 707268.275439914, 184565.3033677166], 
processed observation next is [0.0, 0.391304347826087, 0.4091627172195892, 0.94, 1.0, 1.0, 0.40499950790303946, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19646340984442073, 0.19646340984442054, 0.2754706020413681], 
reward next is 0.7245, 
noisyNet noise sample is [array([1.2057598], dtype=float32), -1.2983629]. 
=============================================
[2019-03-27 08:52:10,546] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.47665]
 [73.45873]
 [73.44447]
 [73.40614]
 [73.33317]], R is [[73.48616791]
 [73.47582245]
 [73.46565247]
 [73.45566559]
 [73.44580841]].
[2019-03-27 08:52:11,003] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136500, global step 2178319: loss 0.1910
[2019-03-27 08:52:11,005] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136500, global step 2178319: learning rate 0.0000
[2019-03-27 08:52:11,858] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136000, global step 2178715: loss 0.0006
[2019-03-27 08:52:11,860] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136000, global step 2178715: learning rate 0.0000
[2019-03-27 08:52:12,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6147917e-17 1.0000000e+00 4.9263414e-23 3.6415813e-14 2.4100783e-27], sum to 1.0000
[2019-03-27 08:52:12,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7414
[2019-03-27 08:52:12,624] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666666, 86.16666666666667, 1.0, 2.0, 0.7564965720769556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1057264.817172797, 1057264.817172797, 233509.6306334129], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2257800.0000, 
sim time next is 2258400.0000, 
raw observation next is [26.13333333333333, 86.33333333333334, 1.0, 2.0, 0.7262191595118556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014929.476273502, 1014929.476273502, 226603.468039346], 
processed observation next is [1.0, 0.13043478260869565, 0.43759873617693507, 0.8633333333333334, 1.0, 1.0, 0.6701435656769344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2819248545204172, 0.2819248545204172, 0.33821413140200896], 
reward next is 0.6618, 
noisyNet noise sample is [array([-0.7422388], dtype=float32), 0.55528873]. 
=============================================
[2019-03-27 08:52:13,573] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0546856e-20 1.0000000e+00 1.0287900e-26 1.2648745e-17 2.2423623e-29], sum to 1.0000
[2019-03-27 08:52:13,581] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6916
[2019-03-27 08:52:13,586] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 94.0, 1.0, 2.0, 0.4698459700214774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658284.6062091653, 658284.6062091653, 179234.0391854889], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2071200.0000, 
sim time next is 2071800.0000, 
raw observation next is [24.55, 94.0, 1.0, 2.0, 0.4691981276105786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 657875.5376726583, 657875.5376726589, 179202.5531599442], 
processed observation next is [0.0, 1.0, 0.3625592417061612, 0.94, 1.0, 1.0, 0.36047967181997426, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18274320490907175, 0.18274320490907192, 0.267466497253648], 
reward next is 0.7325, 
noisyNet noise sample is [array([-0.6014892], dtype=float32), -0.19687557]. 
=============================================
[2019-03-27 08:52:13,675] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136000, global step 2179568: loss 0.0006
[2019-03-27 08:52:13,676] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136000, global step 2179568: learning rate 0.0000
[2019-03-27 08:52:14,186] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136000, global step 2179805: loss 0.0004
[2019-03-27 08:52:14,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136000, global step 2179806: learning rate 0.0000
[2019-03-27 08:52:14,582] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136000, global step 2179989: loss 0.0005
[2019-03-27 08:52:14,583] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136000, global step 2179989: learning rate 0.0000
[2019-03-27 08:52:14,588] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4185526e-19 1.0000000e+00 2.0707764e-24 4.7332747e-18 3.5906502e-27], sum to 1.0000
[2019-03-27 08:52:14,596] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2737
[2019-03-27 08:52:14,598] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 97.33333333333334, 1.0, 2.0, 0.4583683932920895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648304.8621784303, 648304.8621784303, 178340.272852537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2089200.0000, 
sim time next is 2089800.0000, 
raw observation next is [23.9, 97.5, 1.0, 2.0, 0.4569873957359317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 646711.1113267598, 646711.1113267604, 178184.7499755667], 
processed observation next is [0.0, 0.17391304347826086, 0.33175355450236965, 0.975, 1.0, 1.0, 0.3457679466697972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1796419753685444, 0.17964197536854454, 0.26594738802323387], 
reward next is 0.7341, 
noisyNet noise sample is [array([1.3754948], dtype=float32), 1.3226774]. 
=============================================
[2019-03-27 08:52:14,641] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136000, global step 2180019: loss 0.0004
[2019-03-27 08:52:14,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136000, global step 2180020: learning rate 0.0000
[2019-03-27 08:52:14,781] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136000, global step 2180083: loss 0.0004
[2019-03-27 08:52:14,781] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136000, global step 2180083: learning rate 0.0000
[2019-03-27 08:52:15,685] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136000, global step 2180510: loss 0.0006
[2019-03-27 08:52:15,688] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136000, global step 2180510: learning rate 0.0000
[2019-03-27 08:52:15,812] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136000, global step 2180569: loss 0.0005
[2019-03-27 08:52:15,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136000, global step 2180569: learning rate 0.0000
[2019-03-27 08:52:16,198] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136000, global step 2180752: loss 0.0005
[2019-03-27 08:52:16,200] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136000, global step 2180752: learning rate 0.0000
[2019-03-27 08:52:16,250] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136000, global step 2180773: loss 0.0005
[2019-03-27 08:52:16,253] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136000, global step 2180773: learning rate 0.0000
[2019-03-27 08:52:17,832] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136500, global step 2181512: loss 0.2154
[2019-03-27 08:52:17,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136500, global step 2181514: learning rate 0.0000
[2019-03-27 08:52:21,353] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137500, global step 2183117: loss 0.1148
[2019-03-27 08:52:21,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137500, global step 2183117: learning rate 0.0000
[2019-03-27 08:52:21,798] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136500, global step 2183321: loss 0.0751
[2019-03-27 08:52:21,799] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136500, global step 2183321: learning rate 0.0000
[2019-03-27 08:52:22,233] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3813733e-15 1.0000000e+00 6.5656278e-21 4.4340179e-10 3.5376452e-24], sum to 1.0000
[2019-03-27 08:52:22,240] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9310
[2019-03-27 08:52:22,246] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 87.0, 1.0, 2.0, 0.5415113386926087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756698.6227808386, 756698.6227808392, 190356.3363476639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2485800.0000, 
sim time next is 2486400.0000, 
raw observation next is [27.63333333333333, 87.66666666666666, 1.0, 2.0, 0.5447932851181566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761286.4016643214, 761286.401664322, 190912.3790330254], 
processed observation next is [1.0, 0.782608695652174, 0.5086887835703, 0.8766666666666666, 1.0, 1.0, 0.4515581748411525, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21146844490675595, 0.21146844490675612, 0.284943849303023], 
reward next is 0.7151, 
noisyNet noise sample is [array([0.8362512], dtype=float32), -1.5632722]. 
=============================================
[2019-03-27 08:52:22,932] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137500, global step 2183854: loss 0.1239
[2019-03-27 08:52:22,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137500, global step 2183855: learning rate 0.0000
[2019-03-27 08:52:24,774] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136500, global step 2184714: loss 0.0680
[2019-03-27 08:52:24,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136500, global step 2184714: learning rate 0.0000
[2019-03-27 08:52:28,166] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137000, global step 2186309: loss 0.1228
[2019-03-27 08:52:28,169] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137000, global step 2186310: learning rate 0.0000
[2019-03-27 08:52:29,094] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136500, global step 2186750: loss 0.0304
[2019-03-27 08:52:29,097] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136500, global step 2186750: learning rate 0.0000
[2019-03-27 08:52:31,098] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136500, global step 2187687: loss 0.0292
[2019-03-27 08:52:31,100] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136500, global step 2187687: learning rate 0.0000
[2019-03-27 08:52:31,390] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136500, global step 2187826: loss 0.0144
[2019-03-27 08:52:31,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136500, global step 2187826: learning rate 0.0000
[2019-03-27 08:52:31,807] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136500, global step 2188022: loss 0.0173
[2019-03-27 08:52:31,810] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136500, global step 2188023: learning rate 0.0000
[2019-03-27 08:52:31,995] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136500, global step 2188109: loss 0.0615
[2019-03-27 08:52:31,998] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136500, global step 2188111: learning rate 0.0000
[2019-03-27 08:52:32,035] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136500, global step 2188127: loss 0.0046
[2019-03-27 08:52:32,037] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136500, global step 2188127: learning rate 0.0000
[2019-03-27 08:52:32,926] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136500, global step 2188542: loss 0.0194
[2019-03-27 08:52:32,927] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136500, global step 2188542: learning rate 0.0000
[2019-03-27 08:52:33,037] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136500, global step 2188598: loss 0.0026
[2019-03-27 08:52:33,042] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136500, global step 2188599: learning rate 0.0000
[2019-03-27 08:52:33,208] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136500, global step 2188681: loss 0.0967
[2019-03-27 08:52:33,212] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136500, global step 2188681: learning rate 0.0000
[2019-03-27 08:52:33,547] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136500, global step 2188839: loss 0.0427
[2019-03-27 08:52:33,550] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136500, global step 2188841: learning rate 0.0000
[2019-03-27 08:52:33,640] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1508383e-17 1.0000000e+00 1.7558770e-24 6.3901049e-13 4.2320283e-28], sum to 1.0000
[2019-03-27 08:52:33,647] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6359
[2019-03-27 08:52:33,655] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.63333333333334, 78.66666666666667, 1.0, 2.0, 0.5723458124047743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 799802.4074891193, 799802.4074891187, 195707.1499520162], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2413200.0000, 
sim time next is 2413800.0000, 
raw observation next is [29.55, 79.0, 1.0, 2.0, 0.5718864987981999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799160.316084342, 799160.316084342, 195625.3288737926], 
processed observation next is [1.0, 0.9565217391304348, 0.5995260663507109, 0.79, 1.0, 1.0, 0.48420060096168654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22198897669009499, 0.22198897669009499, 0.2919781027967054], 
reward next is 0.7080, 
noisyNet noise sample is [array([-1.2314823], dtype=float32), -2.2338595]. 
=============================================
[2019-03-27 08:52:33,679] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7002399e-18 1.0000000e+00 2.1464178e-25 5.5344132e-18 3.3202530e-26], sum to 1.0000
[2019-03-27 08:52:33,690] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3143
[2019-03-27 08:52:33,698] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 74.5, 1.0, 2.0, 0.5357670944871656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748668.886054139, 748668.886054139, 189390.4782181815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [29.66666666666667, 73.0, 1.0, 2.0, 0.5341717959314043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746438.8684780421, 746438.8684780427, 189123.865751679], 
processed observation next is [0.0, 0.43478260869565216, 0.6050552922590839, 0.73, 1.0, 1.0, 0.4387611999173545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20734413013278946, 0.20734413013278963, 0.2822744264950433], 
reward next is 0.7177, 
noisyNet noise sample is [array([-0.9300092], dtype=float32), 0.31212488]. 
=============================================
[2019-03-27 08:52:34,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1809344e-09 9.9892032e-01 1.2087444e-15 1.0797057e-03 3.2380591e-18], sum to 1.0000
[2019-03-27 08:52:34,533] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1231
[2019-03-27 08:52:34,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1994242.139152866 W.
[2019-03-27 08:52:34,548] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.4, 89.0, 1.0, 2.0, 0.7131542284592175, 1.0, 2.0, 0.7131542284592175, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1994242.139152866, 1994242.139152866, 379800.5714264498], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2469600.0000, 
sim time next is 2470200.0000, 
raw observation next is [26.51666666666667, 88.5, 1.0, 2.0, 0.6808141627902243, 1.0, 2.0, 0.6808141627902243, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1903727.045357011, 1903727.045357011, 366013.9957649666], 
processed observation next is [1.0, 0.6086956521739131, 0.45576619273301755, 0.885, 1.0, 1.0, 0.6154387503496679, 1.0, 1.0, 0.6154387503496679, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5288130681547253, 0.5288130681547253, 0.5462895459178606], 
reward next is 0.4537, 
noisyNet noise sample is [array([-0.88821065], dtype=float32), 0.1868854]. 
=============================================
[2019-03-27 08:52:35,002] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137000, global step 2189501: loss 0.0886
[2019-03-27 08:52:35,004] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137000, global step 2189501: learning rate 0.0000
[2019-03-27 08:52:37,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9674489e-17 1.0000000e+00 4.1043057e-23 5.5700679e-15 2.1841885e-26], sum to 1.0000
[2019-03-27 08:52:37,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1929
[2019-03-27 08:52:37,592] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 95.0, 1.0, 2.0, 0.5464744912966144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763636.5397815894, 763636.53978159, 191197.9855881855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2511000.0000, 
sim time next is 2511600.0000, 
raw observation next is [26.43333333333333, 95.0, 1.0, 2.0, 0.5453855525236866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762114.3246316728, 762114.3246316728, 191012.578711956], 
processed observation next is [1.0, 0.043478260869565216, 0.4518167456556081, 0.95, 1.0, 1.0, 0.45227175002853803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.211698423508798, 0.211698423508798, 0.28509340106262093], 
reward next is 0.7149, 
noisyNet noise sample is [array([0.78835475], dtype=float32), -0.6393171]. 
=============================================
[2019-03-27 08:52:37,854] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138000, global step 2190841: loss 0.0448
[2019-03-27 08:52:37,857] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138000, global step 2190843: learning rate 0.0000
[2019-03-27 08:52:38,904] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137000, global step 2191331: loss 0.0756
[2019-03-27 08:52:38,906] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137000, global step 2191331: learning rate 0.0000
[2019-03-27 08:52:39,380] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138000, global step 2191555: loss 0.0490
[2019-03-27 08:52:39,385] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138000, global step 2191556: learning rate 0.0000
[2019-03-27 08:52:41,669] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1320493e-18 1.0000000e+00 1.6393066e-24 2.4817243e-17 6.3455877e-29], sum to 1.0000
[2019-03-27 08:52:41,680] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5554
[2019-03-27 08:52:41,684] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4768080947514077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 666383.3298784441, 666383.3298784435, 180058.3522723851], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2629200.0000, 
sim time next is 2629800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4770567381362009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666730.5403852839, 666730.5403852839, 180095.5526592168], 
processed observation next is [0.0, 0.43478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.36994787727253126, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18520292788480108, 0.18520292788480108, 0.2687993323271893], 
reward next is 0.7312, 
noisyNet noise sample is [array([0.66119957], dtype=float32), -1.4919357]. 
=============================================
[2019-03-27 08:52:41,778] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137000, global step 2192673: loss 0.0964
[2019-03-27 08:52:41,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137000, global step 2192675: learning rate 0.0000
[2019-03-27 08:52:42,739] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.2235719e-18 1.0000000e+00 1.3095294e-24 1.1764022e-14 4.3109378e-29], sum to 1.0000
[2019-03-27 08:52:42,749] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2366
[2019-03-27 08:52:42,757] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.85, 84.0, 1.0, 2.0, 0.5411116308934741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756139.8791816902, 756139.8791816895, 190288.0578679675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2575800.0000, 
sim time next is 2576400.0000, 
raw observation next is [27.76666666666667, 84.33333333333334, 1.0, 2.0, 0.5410689179681824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756080.1716485576, 756080.1716485576, 190280.7610229312], 
processed observation next is [1.0, 0.8260869565217391, 0.515007898894155, 0.8433333333333334, 1.0, 1.0, 0.4470709855038341, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21002226990237713, 0.21002226990237713, 0.2840011358551212], 
reward next is 0.7160, 
noisyNet noise sample is [array([0.44093266], dtype=float32), 0.38727105]. 
=============================================
[2019-03-27 08:52:44,529] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137500, global step 2193964: loss 0.0522
[2019-03-27 08:52:44,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137500, global step 2193967: learning rate 0.0000
[2019-03-27 08:52:45,063] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8334707e-16 1.0000000e+00 2.0469509e-21 1.0507432e-11 1.0403663e-24], sum to 1.0000
[2019-03-27 08:52:45,071] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2161
[2019-03-27 08:52:45,075] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.7982621086786734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1175395.883493164, 1175395.883493165, 251692.2016356019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2822400.0000, 
sim time next is 2823000.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.8220417272166356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1210859.835505645, 1210859.835505645, 258005.9333863896], 
processed observation next is [1.0, 0.6956521739130435, 0.3364928909952607, 0.89, 1.0, 1.0, 0.7855924424296814, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3363499543071236, 0.3363499543071236, 0.38508348266625314], 
reward next is 0.6149, 
noisyNet noise sample is [array([-0.79607385], dtype=float32), 0.9993201]. 
=============================================
[2019-03-27 08:52:45,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[66.15791 ]
 [66.265656]
 [66.50412 ]
 [66.80609 ]
 [66.3639  ]], R is [[65.8994751 ]
 [65.86481476]
 [65.83095551]
 [65.80677795]
 [65.82639313]].
[2019-03-27 08:52:46,309] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137000, global step 2194796: loss 0.0497
[2019-03-27 08:52:46,312] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137000, global step 2194797: learning rate 0.0000
[2019-03-27 08:52:48,479] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137000, global step 2195810: loss 0.0582
[2019-03-27 08:52:48,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137000, global step 2195810: learning rate 0.0000
[2019-03-27 08:52:48,669] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137000, global step 2195901: loss 0.0597
[2019-03-27 08:52:48,672] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137000, global step 2195901: learning rate 0.0000
[2019-03-27 08:52:49,010] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137000, global step 2196058: loss 0.0568
[2019-03-27 08:52:49,014] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137000, global step 2196059: learning rate 0.0000
[2019-03-27 08:52:49,337] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137000, global step 2196200: loss 0.0556
[2019-03-27 08:52:49,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137000, global step 2196201: learning rate 0.0000
[2019-03-27 08:52:49,413] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137000, global step 2196238: loss 0.0552
[2019-03-27 08:52:49,415] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137000, global step 2196238: learning rate 0.0000
[2019-03-27 08:52:50,241] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137000, global step 2196622: loss 0.0532
[2019-03-27 08:52:50,246] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137000, global step 2196622: learning rate 0.0000
[2019-03-27 08:52:50,296] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137000, global step 2196647: loss 0.0477
[2019-03-27 08:52:50,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137000, global step 2196647: learning rate 0.0000
[2019-03-27 08:52:50,485] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137000, global step 2196735: loss 0.0498
[2019-03-27 08:52:50,487] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137000, global step 2196735: learning rate 0.0000
[2019-03-27 08:52:51,172] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137000, global step 2197051: loss 0.0434
[2019-03-27 08:52:51,174] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137000, global step 2197051: learning rate 0.0000
[2019-03-27 08:52:51,795] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137500, global step 2197351: loss 0.0303
[2019-03-27 08:52:51,799] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137500, global step 2197353: learning rate 0.0000
[2019-03-27 08:52:52,276] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0382358e-18 1.0000000e+00 3.6672881e-25 1.6143345e-17 6.1975112e-27], sum to 1.0000
[2019-03-27 08:52:52,285] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7337
[2019-03-27 08:52:52,289] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3923020949218254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585374.0190396183, 585374.0190396183, 173194.7922291267], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2746800.0000, 
sim time next is 2747400.0000, 
raw observation next is [22.83333333333334, 95.0, 1.0, 2.0, 0.3911062162407326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584174.1603886123, 584174.1603886123, 173103.7404002603], 
processed observation next is [0.0, 0.8260869565217391, 0.2812006319115327, 0.95, 1.0, 1.0, 0.2663930316153405, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16227060010794786, 0.16227060010794786, 0.2583637916421796], 
reward next is 0.7416, 
noisyNet noise sample is [array([1.2238985], dtype=float32), -0.728795]. 
=============================================
[2019-03-27 08:52:54,640] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6323584e-16 1.0000000e+00 3.2279403e-23 4.5510674e-15 8.3918764e-26], sum to 1.0000
[2019-03-27 08:52:54,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2970
[2019-03-27 08:52:54,654] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.303212021231997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482846.8119557668, 482846.8119557668, 165826.1685252583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2938800.0000, 
sim time next is 2939400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3027017108702418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482035.3906956342, 482035.3906956336, 165767.8371366799], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15988157936173708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13389871963767616, 0.133898719637676, 0.2474146822935521], 
reward next is 0.7526, 
noisyNet noise sample is [array([-1.5742936], dtype=float32), -1.4928334]. 
=============================================
[2019-03-27 08:52:55,343] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138500, global step 2199018: loss 0.2212
[2019-03-27 08:52:55,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138500, global step 2199018: learning rate 0.0000
[2019-03-27 08:52:55,833] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137500, global step 2199248: loss 0.0416
[2019-03-27 08:52:55,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137500, global step 2199249: learning rate 0.0000
[2019-03-27 08:52:56,877] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138500, global step 2199740: loss 0.1269
[2019-03-27 08:52:56,880] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138500, global step 2199741: learning rate 0.0000
[2019-03-27 08:52:57,442] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 08:52:57,445] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:52:57,446] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:52:57,446] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:52:57,447] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:52:57,447] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:52:57,448] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:52:57,448] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:52:57,450] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:52:57,453] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:52:57,455] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:52:57,476] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run89
[2019-03-27 08:52:57,501] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run89
[2019-03-27 08:52:57,502] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run89
[2019-03-27 08:52:57,523] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run89
[2019-03-27 08:52:57,567] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run89
[2019-03-27 08:53:04,267] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08759177], dtype=float32), 0.04952745]
[2019-03-27 08:53:04,267] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.450705845, 89.747846565, 1.0, 2.0, 0.3151754492827822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499505.1032921563, 499505.1032921563, 167008.936899451]
[2019-03-27 08:53:04,268] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:53:04,274] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2785812e-19 1.0000000e+00 3.5587839e-26 6.8977294e-19 6.5933857e-29], sampled 0.6342716930038544
[2019-03-27 08:53:38,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08759177], dtype=float32), 0.04952745]
[2019-03-27 08:53:38,669] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.62844002, 95.70972872499999, 1.0, 2.0, 0.2730017872310614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 442985.898904201, 442985.898904201, 163119.0849739021]
[2019-03-27 08:53:38,670] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:53:38,672] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0370368e-19 1.0000000e+00 3.1085767e-26 8.0650581e-20 1.2604794e-28], sampled 0.8349302991568606
[2019-03-27 08:54:32,300] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08759177], dtype=float32), 0.04952745]
[2019-03-27 08:54:32,302] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.01666666666667, 64.5, 1.0, 2.0, 0.748110334537165, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.970134664785429, 6.9112, 168.9125565900638, 1942454.316357361, 1900644.135774483, 396627.1976818895]
[2019-03-27 08:54:32,304] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 08:54:32,308] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.9409136e-12 9.9999869e-01 5.4927225e-17 1.2530058e-06 2.6499699e-20], sampled 0.17208351212283002
[2019-03-27 08:54:32,308] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1942454.316357361 W.
[2019-03-27 08:54:53,194] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.8870 2779313266.0555 933.0000
[2019-03-27 08:54:53,213] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.5796 3007558846.9224 1767.0000
[2019-03-27 08:54:53,259] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.9988 3163870346.3065 1763.0000
[2019-03-27 08:54:53,276] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.7897 2927379194.9526 1336.0000
[2019-03-27 08:54:53,412] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5660 2842298553.9909 1128.0000
[2019-03-27 08:54:54,432] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2200000, evaluation results [2200000.0, 7890.998830603325, 3163870346.3064613, 1763.0, 8252.789659823651, 2927379194.952586, 1336.0, 8658.887049518678, 2779313266.0555253, 933.0, 7999.579635962973, 3007558846.9223795, 1767.0, 8497.566021738017, 2842298553.990859, 1128.0]
[2019-03-27 08:54:55,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6718518e-17 1.0000000e+00 1.6274874e-23 3.4231452e-16 1.5583989e-26], sum to 1.0000
[2019-03-27 08:54:55,149] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9125
[2019-03-27 08:54:55,154] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 97.0, 1.0, 2.0, 0.3374536420638731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522158.545047688, 522158.5450476874, 168445.3945084656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3051000.0000, 
sim time next is 3051600.0000, 
raw observation next is [21.66666666666667, 96.0, 1.0, 2.0, 0.3408426909329262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526683.8322111608, 526683.8322111614, 168784.6009892069], 
processed observation next is [1.0, 0.30434782608695654, 0.22590837282780438, 0.96, 1.0, 1.0, 0.2058345673890677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1463010645031002, 0.14630106450310038, 0.251917314909264], 
reward next is 0.7481, 
noisyNet noise sample is [array([-0.3163087], dtype=float32), -2.3402038]. 
=============================================
[2019-03-27 08:54:55,831] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137500, global step 2200636: loss 0.0115
[2019-03-27 08:54:55,833] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137500, global step 2200636: learning rate 0.0000
[2019-03-27 08:54:58,768] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.1737409e-18 1.0000000e+00 1.8809805e-24 2.2723514e-13 9.7512859e-26], sum to 1.0000
[2019-03-27 08:54:58,777] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4808
[2019-03-27 08:54:58,783] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 89.83333333333334, 1.0, 2.0, 0.705100973192129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1075383.100681327, 1075383.100681328, 233407.1101784439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2905800.0000, 
sim time next is 2906400.0000, 
raw observation next is [22.66666666666667, 90.66666666666667, 1.0, 2.0, 0.6911292910529765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1055892.195042201, 1055892.195042202, 230315.1323058094], 
processed observation next is [1.0, 0.6521739130434783, 0.27330173775671435, 0.9066666666666667, 1.0, 1.0, 0.6278666157264777, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2933033875117225, 0.2933033875117228, 0.3437539288146409], 
reward next is 0.6562, 
noisyNet noise sample is [array([0.38894105], dtype=float32), 0.65791357]. 
=============================================
[2019-03-27 08:54:59,011] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138000, global step 2202023: loss 0.0821
[2019-03-27 08:54:59,015] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138000, global step 2202024: learning rate 0.0000
[2019-03-27 08:55:00,514] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137500, global step 2202723: loss 0.0182
[2019-03-27 08:55:00,519] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137500, global step 2202724: learning rate 0.0000
[2019-03-27 08:55:02,684] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137500, global step 2203750: loss 0.0292
[2019-03-27 08:55:02,686] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137500, global step 2203751: learning rate 0.0000
[2019-03-27 08:55:02,885] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137500, global step 2203845: loss 0.0245
[2019-03-27 08:55:02,890] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137500, global step 2203849: learning rate 0.0000
[2019-03-27 08:55:03,288] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137500, global step 2204033: loss 0.0183
[2019-03-27 08:55:03,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137500, global step 2204034: learning rate 0.0000
[2019-03-27 08:55:03,538] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137500, global step 2204147: loss 0.0215
[2019-03-27 08:55:03,540] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137500, global step 2204148: learning rate 0.0000
[2019-03-27 08:55:03,632] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137500, global step 2204194: loss 0.0235
[2019-03-27 08:55:03,634] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137500, global step 2204195: learning rate 0.0000
[2019-03-27 08:55:04,359] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137500, global step 2204537: loss 0.0202
[2019-03-27 08:55:04,364] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137500, global step 2204539: learning rate 0.0000
[2019-03-27 08:55:04,456] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137500, global step 2204582: loss 0.0236
[2019-03-27 08:55:04,457] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137500, global step 2204582: learning rate 0.0000
[2019-03-27 08:55:04,775] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137500, global step 2204731: loss 0.0208
[2019-03-27 08:55:04,780] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137500, global step 2204731: learning rate 0.0000
[2019-03-27 08:55:05,090] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7814334e-17 1.0000000e+00 2.0636387e-25 3.2931102e-17 1.6571597e-28], sum to 1.0000
[2019-03-27 08:55:05,102] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6561
[2019-03-27 08:55:05,108] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3060776121772938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487409.9922117725, 487409.9922117725, 166155.9808691909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3017400.0000, 
sim time next is 3018000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3060337571106814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487339.9860093145, 487339.9860093145, 166150.8971664898], 
processed observation next is [1.0, 0.9565217391304348, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16389609290443544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1353722183359207, 0.1353722183359207, 0.24798641368132807], 
reward next is 0.7520, 
noisyNet noise sample is [array([-1.3052309], dtype=float32), -0.45085555]. 
=============================================
[2019-03-27 08:55:05,121] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[76.425674]
 [76.39895 ]
 [76.34737 ]
 [76.36003 ]
 [76.35771 ]], R is [[76.41608429]
 [76.40393066]
 [76.39194489]
 [76.38018799]
 [76.36864471]].
[2019-03-27 08:55:05,279] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137500, global step 2204966: loss 0.0162
[2019-03-27 08:55:05,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137500, global step 2204967: learning rate 0.0000
[2019-03-27 08:55:06,063] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138000, global step 2205337: loss 0.0782
[2019-03-27 08:55:06,067] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138000, global step 2205338: learning rate 0.0000
[2019-03-27 08:55:06,871] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.553285e-18 1.000000e+00 5.007734e-25 6.600749e-17 2.759858e-29], sum to 1.0000
[2019-03-27 08:55:06,880] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8299
[2019-03-27 08:55:06,884] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 55.66666666666667, 1.0, 2.0, 0.5987374763543966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836696.9383127566, 836696.9383127566, 200511.287819026], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3856800.0000, 
sim time next is 3857400.0000, 
raw observation next is [35.0, 55.5, 1.0, 2.0, 0.5924232812887424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827869.8199108832, 827869.8199108832, 199343.5388423999], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.555, 1.0, 1.0, 0.5089437123960752, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2299638388641342, 0.2299638388641342, 0.2975276699140297], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.04252792], dtype=float32), 0.968496]. 
=============================================
[2019-03-27 08:55:08,671] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4609465e-17 1.0000000e+00 6.5465389e-23 3.4880137e-15 7.9550565e-28], sum to 1.0000
[2019-03-27 08:55:08,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9194
[2019-03-27 08:55:08,685] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 97.0, 1.0, 2.0, 0.3947736472976615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591416.8127952688, 591416.8127952688, 173817.2642209453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3097800.0000, 
sim time next is 3098400.0000, 
raw observation next is [22.33333333333334, 98.0, 1.0, 2.0, 0.3925455952621777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589003.3438173933, 589003.3438173933, 173623.5774825238], 
processed observation next is [1.0, 0.8695652173913043, 0.2575039494470777, 0.98, 1.0, 1.0, 0.268127223207443, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16361203994927592, 0.16361203994927592, 0.2591396678843639], 
reward next is 0.7409, 
noisyNet noise sample is [array([1.1691606], dtype=float32), -0.23795857]. 
=============================================
[2019-03-27 08:55:08,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2264459e-18 1.0000000e+00 9.9675060e-25 7.5748182e-16 1.4532464e-27], sum to 1.0000
[2019-03-27 08:55:08,851] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7097
[2019-03-27 08:55:08,858] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3856295680600514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580829.6822645275, 580829.6822645282, 172949.5892816836], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3100800.0000, 
sim time next is 3101400.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3853117113113794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 580352.2812850083, 580352.2812850077, 172906.8068618597], 
processed observation next is [1.0, 0.9130434782608695, 0.2417061611374408, 1.0, 1.0, 1.0, 0.25941170037515593, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16120896702361343, 0.16120896702361326, 0.2580698609878503], 
reward next is 0.7419, 
noisyNet noise sample is [array([0.41225326], dtype=float32), -1.6431743]. 
=============================================
[2019-03-27 08:55:09,406] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.6893582e-17 1.0000000e+00 7.8878983e-24 1.6857605e-15 3.4895912e-26], sum to 1.0000
[2019-03-27 08:55:09,414] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6091
[2019-03-27 08:55:09,420] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 97.0, 1.0, 2.0, 0.3947736472976615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591416.8127952688, 591416.8127952688, 173817.2642209453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3097800.0000, 
sim time next is 3098400.0000, 
raw observation next is [22.33333333333334, 98.0, 1.0, 2.0, 0.3925455952621777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589003.3438173933, 589003.3438173933, 173623.5774825238], 
processed observation next is [1.0, 0.8695652173913043, 0.2575039494470777, 0.98, 1.0, 1.0, 0.268127223207443, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16361203994927592, 0.16361203994927592, 0.2591396678843639], 
reward next is 0.7409, 
noisyNet noise sample is [array([0.6969065], dtype=float32), 0.97585416]. 
=============================================
[2019-03-27 08:55:10,176] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139000, global step 2207197: loss 0.0147
[2019-03-27 08:55:10,179] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139000, global step 2207199: learning rate 0.0000
[2019-03-27 08:55:10,346] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138000, global step 2207277: loss 0.0566
[2019-03-27 08:55:10,350] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138000, global step 2207278: learning rate 0.0000
[2019-03-27 08:55:11,580] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139000, global step 2207849: loss 0.0088
[2019-03-27 08:55:11,584] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139000, global step 2207851: learning rate 0.0000
[2019-03-27 08:55:12,097] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5614162e-18 1.0000000e+00 6.6819150e-26 1.9250658e-16 8.6721333e-29], sum to 1.0000
[2019-03-27 08:55:12,106] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9174
[2019-03-27 08:55:12,111] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5984022742334975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836228.3306087685, 836228.3306087685, 200449.1914506043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3945000.0000, 
sim time next is 3945600.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5985692231426372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836461.7227368458, 836461.7227368458, 200480.2081664106], 
processed observation next is [0.0, 0.6956521739130435, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5163484616176351, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23235047853801274, 0.23235047853801274, 0.29922419129315014], 
reward next is 0.7008, 
noisyNet noise sample is [array([-2.5015302], dtype=float32), -1.19382]. 
=============================================
[2019-03-27 08:55:13,108] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138000, global step 2208561: loss 0.0463
[2019-03-27 08:55:13,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138000, global step 2208561: learning rate 0.0000
[2019-03-27 08:55:13,572] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0159404e-17 1.0000000e+00 2.1582832e-24 9.3143067e-16 5.9285208e-28], sum to 1.0000
[2019-03-27 08:55:13,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6530
[2019-03-27 08:55:13,588] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5379667811787028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751743.7664364893, 751743.7664364887, 189758.3783296175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3369600.0000, 
sim time next is 3370200.0000, 
raw observation next is [26.95, 89.33333333333334, 1.0, 2.0, 0.5373291143985411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750852.3889982343, 750852.3889982337, 189651.3903222882], 
processed observation next is [1.0, 0.0, 0.476303317535545, 0.8933333333333334, 1.0, 1.0, 0.44256519807053146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2085701080550651, 0.20857010805506493, 0.28306177660043014], 
reward next is 0.7169, 
noisyNet noise sample is [array([-2.5877051], dtype=float32), -0.19654195]. 
=============================================
[2019-03-27 08:55:14,952] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.8194454e-17 1.0000000e+00 8.5823718e-25 3.7886428e-16 2.5104398e-26], sum to 1.0000
[2019-03-27 08:55:14,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8496
[2019-03-27 08:55:14,967] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 90.0, 1.0, 2.0, 0.4657045598329232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655042.1432493017, 655042.1432493011, 178953.5943287883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3207600.0000, 
sim time next is 3208200.0000, 
raw observation next is [25.0, 89.83333333333333, 1.0, 2.0, 0.4636767306277315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653047.3189498187, 653047.3189498187, 178765.5235225545], 
processed observation next is [0.0, 0.13043478260869565, 0.38388625592417064, 0.8983333333333333, 1.0, 1.0, 0.35382738629847166, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1814020330416163, 0.1814020330416163, 0.2668142142127679], 
reward next is 0.7332, 
noisyNet noise sample is [array([-0.2543323], dtype=float32), 0.29924414]. 
=============================================
[2019-03-27 08:55:16,747] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138500, global step 2210246: loss 0.1357
[2019-03-27 08:55:16,749] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138500, global step 2210246: learning rate 0.0000
[2019-03-27 08:55:17,617] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138000, global step 2210655: loss 0.0555
[2019-03-27 08:55:17,621] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138000, global step 2210655: learning rate 0.0000
[2019-03-27 08:55:19,880] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138000, global step 2211719: loss 0.0484
[2019-03-27 08:55:19,884] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138000, global step 2211720: learning rate 0.0000
[2019-03-27 08:55:20,032] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138000, global step 2211791: loss 0.0475
[2019-03-27 08:55:20,036] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138000, global step 2211791: learning rate 0.0000
[2019-03-27 08:55:20,417] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138000, global step 2211970: loss 0.0480
[2019-03-27 08:55:20,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138000, global step 2211971: learning rate 0.0000
[2019-03-27 08:55:20,742] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138000, global step 2212120: loss 0.0479
[2019-03-27 08:55:20,745] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138000, global step 2212121: loss 0.0424
[2019-03-27 08:55:20,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138000, global step 2212122: learning rate 0.0000
[2019-03-27 08:55:20,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138000, global step 2212122: learning rate 0.0000
[2019-03-27 08:55:21,559] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138000, global step 2212505: loss 0.0435
[2019-03-27 08:55:21,561] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138000, global step 2212506: learning rate 0.0000
[2019-03-27 08:55:21,650] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138000, global step 2212542: loss 0.0446
[2019-03-27 08:55:21,658] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138000, global step 2212544: learning rate 0.0000
[2019-03-27 08:55:21,961] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138000, global step 2212689: loss 0.0403
[2019-03-27 08:55:21,964] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138000, global step 2212691: learning rate 0.0000
[2019-03-27 08:55:22,497] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138000, global step 2212944: loss 0.0448
[2019-03-27 08:55:22,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138000, global step 2212944: learning rate 0.0000
[2019-03-27 08:55:23,539] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138500, global step 2213430: loss 0.0853
[2019-03-27 08:55:23,543] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138500, global step 2213432: learning rate 0.0000
[2019-03-27 08:55:25,483] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0062451e-09 1.7919591e-01 1.2985683e-14 8.2080412e-01 2.4903170e-18], sum to 1.0000
[2019-03-27 08:55:25,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1929
[2019-03-27 08:55:25,497] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.5, 69.0, 1.0, 2.0, 0.9211550885756841, 1.0, 2.0, 0.9211550885756841, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2576526.319265397, 2576526.319265397, 483172.6087417267], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3411000.0000, 
sim time next is 3411600.0000, 
raw observation next is [32.66666666666667, 68.33333333333333, 1.0, 2.0, 0.9950268975481965, 1.0, 2.0, 0.9950268975481965, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2783380.390082579, 2783380.390082578, 525993.3462272363], 
processed observation next is [1.0, 0.4782608695652174, 0.7472353870458138, 0.6833333333333332, 1.0, 1.0, 0.9940083102990319, 1.0, 1.0, 0.9940083102990319, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.773161219467383, 0.7731612194673828, 0.7850646958615467], 
reward next is 0.2149, 
noisyNet noise sample is [array([0.32457972], dtype=float32), 0.622166]. 
=============================================
[2019-03-27 08:55:27,522] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139500, global step 2215297: loss 75.4767
[2019-03-27 08:55:27,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139500, global step 2215298: learning rate 0.0000
[2019-03-27 08:55:27,558] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138500, global step 2215316: loss 0.0543
[2019-03-27 08:55:27,560] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138500, global step 2215316: learning rate 0.0000
[2019-03-27 08:55:28,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5305926e-10 1.3706526e-01 5.8010994e-16 8.6293477e-01 3.6511034e-19], sum to 1.0000
[2019-03-27 08:55:28,700] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7489
[2019-03-27 08:55:28,706] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333334, 68.83333333333333, 1.0, 2.0, 0.8680672093392559, 1.0, 2.0, 0.8680672093392559, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2427892.057205006, 2427892.057205005, 454371.1590902022], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3430200.0000, 
sim time next is 3430800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.8201682136983778, 1.0, 2.0, 0.8201682136983778, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2293800.686208172, 2293800.686208173, 429811.5172961856], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.7833351972269612, 1.0, 1.0, 0.7833351972269612, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6371668572800477, 0.6371668572800481, 0.6415097273077397], 
reward next is 0.3585, 
noisyNet noise sample is [array([0.6197162], dtype=float32), 0.37065163]. 
=============================================
[2019-03-27 08:55:28,935] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139500, global step 2215953: loss 72.6265
[2019-03-27 08:55:28,937] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139500, global step 2215953: learning rate 0.0000
[2019-03-27 08:55:30,469] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138500, global step 2216664: loss 0.2036
[2019-03-27 08:55:30,471] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138500, global step 2216665: learning rate 0.0000
[2019-03-27 08:55:32,928] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1918096e-09 2.2737063e-02 4.2408466e-14 9.7726291e-01 1.0538295e-18], sum to 1.0000
[2019-03-27 08:55:32,935] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7945
[2019-03-27 08:55:32,939] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.8444077657219283, 1.0, 2.0, 0.8444077657219283, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2361656.568857902, 2361656.568857902, 442075.3196580821], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3510000.0000, 
sim time next is 3510600.0000, 
raw observation next is [33.0, 62.33333333333333, 1.0, 2.0, 0.887560910160994, 1.0, 2.0, 0.887560910160994, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2482468.032528446, 2482468.032528445, 464754.1972922588], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.6233333333333333, 1.0, 1.0, 0.8645312170614385, 1.0, 1.0, 0.8645312170614385, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6895744534801239, 0.6895744534801237, 0.6936629810332221], 
reward next is 0.3063, 
noisyNet noise sample is [array([0.04038368], dtype=float32), 0.0023000133]. 
=============================================
[2019-03-27 08:55:33,859] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139000, global step 2218246: loss 0.0075
[2019-03-27 08:55:33,862] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139000, global step 2218246: learning rate 0.0000
[2019-03-27 08:55:34,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7741506e-16 1.0000000e+00 2.8089812e-22 1.8977925e-12 3.0845164e-25], sum to 1.0000
[2019-03-27 08:55:34,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4443
[2019-03-27 08:55:34,376] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5202722969842882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727009.395026469, 727009.395026469, 186833.1483374799], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3802200.0000, 
sim time next is 3802800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5210283407119982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728066.2248126034, 728066.2248126034, 186956.1836537216], 
processed observation next is [0.0, 0.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42292571170120263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20224061800350096, 0.20224061800350096, 0.27903908008018147], 
reward next is 0.7210, 
noisyNet noise sample is [array([-0.0413385], dtype=float32), -0.62929267]. 
=============================================
[2019-03-27 08:55:34,815] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138500, global step 2218689: loss 0.0509
[2019-03-27 08:55:34,819] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138500, global step 2218692: learning rate 0.0000
[2019-03-27 08:55:36,853] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138500, global step 2219641: loss 0.1215
[2019-03-27 08:55:36,856] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138500, global step 2219644: learning rate 0.0000
[2019-03-27 08:55:37,118] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138500, global step 2219764: loss 0.0932
[2019-03-27 08:55:37,120] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138500, global step 2219764: learning rate 0.0000
[2019-03-27 08:55:37,581] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138500, global step 2219981: loss 0.0155
[2019-03-27 08:55:37,584] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138500, global step 2219984: learning rate 0.0000
[2019-03-27 08:55:37,909] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138500, global step 2220136: loss 0.3378
[2019-03-27 08:55:37,911] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138500, global step 2220137: learning rate 0.0000
[2019-03-27 08:55:38,056] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138500, global step 2220205: loss 0.0494
[2019-03-27 08:55:38,061] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138500, global step 2220205: learning rate 0.0000
[2019-03-27 08:55:38,627] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138500, global step 2220474: loss 0.0115
[2019-03-27 08:55:38,628] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138500, global step 2220474: learning rate 0.0000
[2019-03-27 08:55:38,631] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138500, global step 2220476: loss 0.1187
[2019-03-27 08:55:38,632] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138500, global step 2220476: learning rate 0.0000
[2019-03-27 08:55:38,935] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.6034606e-13 9.9999177e-01 1.3715103e-17 8.1949001e-06 4.1866651e-20], sum to 1.0000
[2019-03-27 08:55:38,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3415
[2019-03-27 08:55:38,953] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1840157.167277658 W.
[2019-03-27 08:55:38,958] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.66666666666667, 84.0, 1.0, 2.0, 0.6750083154378027, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.00597413667824, 6.9112, 168.9115794952229, 1840157.167277658, 1772921.680966864, 379329.1464800265], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4000800.0000, 
sim time next is 4001400.0000, 
raw observation next is [29.5, 84.0, 1.0, 2.0, 0.5918073956925175, 1.0, 1.0, 0.5918073956925175, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1654649.652520709, 1654649.652520709, 331257.5305922606], 
processed observation next is [1.0, 0.30434782608695654, 0.5971563981042655, 0.84, 1.0, 1.0, 0.50820168155725, 1.0, 0.5, 0.50820168155725, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4596249034779747, 0.4596249034779747, 0.49441422476456803], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7063609], dtype=float32), -1.1965538]. 
=============================================
[2019-03-27 08:55:39,070] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138500, global step 2220673: loss 0.1022
[2019-03-27 08:55:39,072] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138500, global step 2220674: learning rate 0.0000
[2019-03-27 08:55:39,472] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138500, global step 2220865: loss 0.0671
[2019-03-27 08:55:39,475] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138500, global step 2220865: learning rate 0.0000
[2019-03-27 08:55:39,999] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5549511e-10 2.9036891e-01 1.6749538e-14 7.0963103e-01 4.2370565e-18], sum to 1.0000
[2019-03-27 08:55:40,012] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2510
[2019-03-27 08:55:40,020] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2268829.100133194 W.
[2019-03-27 08:55:40,025] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333334, 67.66666666666667, 1.0, 2.0, 0.8112475053216117, 1.0, 2.0, 0.8112475053216117, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2268829.100133194, 2268829.100133194, 425385.8306925289], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3666000.0000, 
sim time next is 3666600.0000, 
raw observation next is [31.5, 66.5, 1.0, 2.0, 0.8223045985602618, 1.0, 2.0, 0.8223045985602618, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2299781.105796539, 2299781.105796539, 430877.0932296309], 
processed observation next is [1.0, 0.43478260869565216, 0.6919431279620853, 0.665, 1.0, 1.0, 0.7859091548918817, 1.0, 1.0, 0.7859091548918817, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6388280849434831, 0.6388280849434831, 0.6431001391487028], 
reward next is 0.3569, 
noisyNet noise sample is [array([-1.9847672], dtype=float32), 1.688853]. 
=============================================
[2019-03-27 08:55:40,448] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139000, global step 2221321: loss 0.0073
[2019-03-27 08:55:40,451] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139000, global step 2221322: learning rate 0.0000
[2019-03-27 08:55:44,463] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8811153e-16 1.0000000e+00 2.5574714e-21 2.7604964e-13 4.4340348e-25], sum to 1.0000
[2019-03-27 08:55:44,470] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1990
[2019-03-27 08:55:44,475] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5860023686779927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818893.5877447528, 818893.5877447533, 198166.8269466856], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3981600.0000, 
sim time next is 3982200.0000, 
raw observation next is [29.16666666666667, 83.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 10.90949810313094, 6.9112, 170.0914505937701, 4311808.468985038, 1455487.907060017, 303199.9251334339], 
processed observation next is [1.0, 0.08695652173913043, 0.581358609794629, 0.8316666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.39982981031309395, 0.0, 0.8352268906184235, 1.197724574718066, 0.4043021964055603, 0.45253720169169237], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6057329], dtype=float32), -0.13860206]. 
=============================================
[2019-03-27 08:55:44,677] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139000, global step 2223281: loss 0.0047
[2019-03-27 08:55:44,680] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139000, global step 2223281: learning rate 0.0000
[2019-03-27 08:55:44,791] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140000, global step 2223336: loss 0.0236
[2019-03-27 08:55:44,796] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140000, global step 2223337: learning rate 0.0000
[2019-03-27 08:55:45,444] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6933030e-10 8.8048869e-01 7.7348643e-14 1.1951126e-01 6.5312601e-18], sum to 1.0000
[2019-03-27 08:55:45,451] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6861
[2019-03-27 08:55:45,461] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1894402.24835589 W.
[2019-03-27 08:55:45,465] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 66.5, 1.0, 2.0, 0.6774823582698715, 1.0, 2.0, 0.6774823582698715, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1894402.24835589, 1894402.24835589, 364627.9116812331], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3749400.0000, 
sim time next is 3750000.0000, 
raw observation next is [30.33333333333333, 65.33333333333334, 1.0, 2.0, 0.6771051832137586, 1.0, 2.0, 0.6771051832137586, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1893346.645071503, 1893346.645071502, 364472.2299387094], 
processed observation next is [1.0, 0.391304347826087, 0.6366508688783569, 0.6533333333333334, 1.0, 1.0, 0.6109701002575404, 1.0, 1.0, 0.6109701002575404, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5259296236309731, 0.5259296236309728, 0.5439884028935962], 
reward next is 0.4560, 
noisyNet noise sample is [array([0.22909917], dtype=float32), -0.66789573]. 
=============================================
[2019-03-27 08:55:45,483] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[47.167194]
 [45.994167]
 [46.400158]
 [45.006954]
 [46.494846]], R is [[49.10617828]
 [48.61511612]
 [48.12896729]
 [48.13444901]
 [47.65310669]].
[2019-03-27 08:55:45,848] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1732764e-18 1.0000000e+00 4.2968477e-23 8.9512653e-16 5.6450081e-27], sum to 1.0000
[2019-03-27 08:55:45,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8137
[2019-03-27 08:55:45,858] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 79.0, 1.0, 2.0, 0.5354309014307969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748198.9319862404, 748198.9319862397, 189334.0538590824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4531200.0000, 
sim time next is 4531800.0000, 
raw observation next is [28.83333333333334, 79.0, 1.0, 2.0, 0.5412194199662537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756290.5553582262, 756290.5553582262, 190306.7846473849], 
processed observation next is [0.0, 0.43478260869565216, 0.5655608214849924, 0.79, 1.0, 1.0, 0.4472523132123538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2100807098217295, 0.2100807098217295, 0.2840399770856491], 
reward next is 0.7160, 
noisyNet noise sample is [array([1.0323094], dtype=float32), -1.4034809]. 
=============================================
[2019-03-27 08:55:46,025] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140000, global step 2223906: loss 0.0224
[2019-03-27 08:55:46,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140000, global step 2223908: learning rate 0.0000
[2019-03-27 08:55:46,233] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2764714e-16 1.0000000e+00 2.2041304e-22 2.7217881e-12 1.6935445e-25], sum to 1.0000
[2019-03-27 08:55:46,237] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8080
[2019-03-27 08:55:46,241] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 74.83333333333334, 1.0, 2.0, 0.6414056247883516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 917541.7052253456, 917541.7052253456, 211475.3498743518], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3730200.0000, 
sim time next is 3730800.0000, 
raw observation next is [26.66666666666667, 75.66666666666667, 1.0, 2.0, 0.6117656248956994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876283.5734749376, 876283.5734749376, 205718.1153941139], 
processed observation next is [1.0, 0.17391304347826086, 0.4628751974723541, 0.7566666666666667, 1.0, 1.0, 0.532247740838192, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2434121037430382, 0.2434121037430382, 0.3070419632747969], 
reward next is 0.6930, 
noisyNet noise sample is [array([0.17066517], dtype=float32), -0.3862421]. 
=============================================
[2019-03-27 08:55:47,082] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4472869e-14 1.0000000e+00 1.4025132e-20 1.4757334e-10 7.6335022e-23], sum to 1.0000
[2019-03-27 08:55:47,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2924
[2019-03-27 08:55:47,100] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2972819.990297092 W.
[2019-03-27 08:55:47,106] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.051227352278316, 6.9112, 168.9009588348533, 2972819.990297092, 1454718.769020088, 309141.0851418076], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3745200.0000, 
sim time next is 3745800.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.5581561602653209, 1.0, 1.0, 0.5581561602653209, 1.0, 1.0, 0.9500072313307921, 6.9112, 6.9112, 170.5573041426782, 2341574.71270409, 2341574.71270409, 453699.5269789604], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7, 1.0, 1.0, 0.46765802441604926, 1.0, 0.5, 0.46765802441604926, 1.0, 0.5, 0.9390332089399904, 0.0, 0.0, 0.8375144448122397, 0.6504374201955805, 0.6504374201955805, 0.6771634731029259], 
reward next is 0.3228, 
noisyNet noise sample is [array([-0.42766735], dtype=float32), 0.24722539]. 
=============================================
[2019-03-27 08:55:47,479] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139000, global step 2224590: loss 0.0047
[2019-03-27 08:55:47,481] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139000, global step 2224590: learning rate 0.0000
[2019-03-27 08:55:48,370] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 08:55:48,372] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:55:48,374] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:55:48,374] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:55:48,377] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:55:48,376] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:55:48,380] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:55:48,378] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:55:48,381] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:55:48,380] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:55:48,383] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:55:48,399] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run90
[2019-03-27 08:55:48,422] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run90
[2019-03-27 08:55:48,423] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run90
[2019-03-27 08:55:48,444] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run90
[2019-03-27 08:55:48,445] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run90
[2019-03-27 08:56:10,828] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08851752], dtype=float32), 0.04878047]
[2019-03-27 08:56:10,828] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.23200773166667, 52.48536661166667, 1.0, 2.0, 0.288663441557477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 467271.2434653273, 467271.2434653266, 164757.948683057]
[2019-03-27 08:56:10,831] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:56:10,833] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4545578e-19 1.0000000e+00 7.0202131e-26 7.5071195e-19 1.8151249e-28], sampled 0.07882031657943445
[2019-03-27 08:56:26,952] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.08851752], dtype=float32), 0.04878047]
[2019-03-27 08:56:26,953] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.83333333333333, 94.0, 1.0, 2.0, 0.494326796292502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690742.3178418219, 690742.3178418219, 182711.1493999072]
[2019-03-27 08:56:26,953] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 08:56:26,958] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8844552e-18 1.0000000e+00 8.1413571e-25 7.9000281e-18 2.3523098e-27], sampled 0.30847247437939185
[2019-03-27 08:56:47,683] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08851752], dtype=float32), 0.04878047]
[2019-03-27 08:56:47,687] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.66666666666667, 67.33333333333333, 1.0, 2.0, 0.5787500503042655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 808755.1679201908, 808755.1679201914, 196853.5394561493]
[2019-03-27 08:56:47,688] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 08:56:47,691] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6991545e-18 1.0000000e+00 7.3353774e-25 7.1322159e-16 4.8819729e-28], sampled 0.45326517896178464
[2019-03-27 08:57:43,471] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7902.8314 3162705023.5838 1732.0000
[2019-03-27 08:57:44,094] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.7761 2779317170.1548 933.0000
[2019-03-27 08:57:44,114] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.6918 3007125014.7213 1759.0000
[2019-03-27 08:57:44,269] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.2248 2842064055.9215 1123.0000
[2019-03-27 08:57:44,356] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.1984 2927160408.6947 1332.0000
[2019-03-27 08:57:45,370] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2225000, evaluation results [2225000.0, 7902.831360598799, 3162705023.5837936, 1732.0, 8260.198377632762, 2927160408.694727, 1332.0, 8658.77612680328, 2779317170.154837, 933.0, 8002.691815144936, 3007125014.721309, 1759.0, 8499.224817098855, 2842064055.9214735, 1123.0]
[2019-03-27 08:57:45,432] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0364273e-15 1.0000000e+00 4.3824502e-22 5.1275374e-13 6.8386241e-26], sum to 1.0000
[2019-03-27 08:57:45,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4541
[2019-03-27 08:57:45,441] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5435752090157918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759583.6708194437, 759583.6708194437, 190705.1947949817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4050600.0000, 
sim time next is 4051200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5446465093164412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761081.2257639385, 761081.2257639385, 190886.9765032092], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45138133652583273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21141145160109404, 0.21141145160109404, 0.28490593507941675], 
reward next is 0.7151, 
noisyNet noise sample is [array([-1.0956417], dtype=float32), -0.9777111]. 
=============================================
[2019-03-27 08:57:48,325] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139500, global step 2226339: loss 68.4926
[2019-03-27 08:57:48,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139500, global step 2226341: learning rate 0.0000
[2019-03-27 08:57:49,195] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139000, global step 2226661: loss 0.0053
[2019-03-27 08:57:49,198] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139000, global step 2226662: learning rate 0.0000
[2019-03-27 08:57:51,270] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139000, global step 2227614: loss 0.0062
[2019-03-27 08:57:51,275] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139000, global step 2227615: learning rate 0.0000
[2019-03-27 08:57:51,530] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139000, global step 2227735: loss 0.0052
[2019-03-27 08:57:51,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139000, global step 2227735: learning rate 0.0000
[2019-03-27 08:57:51,992] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139000, global step 2227952: loss 0.0047
[2019-03-27 08:57:51,995] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139000, global step 2227952: learning rate 0.0000
[2019-03-27 08:57:52,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0954279e-14 1.0000000e+00 2.6020103e-21 1.5807927e-10 3.0162738e-24], sum to 1.0000
[2019-03-27 08:57:52,419] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7102
[2019-03-27 08:57:52,422] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 1.012963877539185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1415937.514453931, 1415937.514453931, 302902.2274587305], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4695000.0000, 
sim time next is 4695600.0000, 
raw observation next is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.746256089327783, 6.9112, 168.9086595192302, 2046563.186868673, 1454160.727247422, 311352.2580886176], 
processed observation next is [1.0, 0.34782608695652173, 0.5892575039494474, 0.7766666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.08350560893277832, 0.0, 0.8294188449583357, 0.568489774130187, 0.4039335353465061, 0.4647048628188322], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.03893663], dtype=float32), 0.1711811]. 
=============================================
[2019-03-27 08:57:52,500] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139000, global step 2228186: loss 0.0046
[2019-03-27 08:57:52,502] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139000, global step 2228186: learning rate 0.0000
[2019-03-27 08:57:52,606] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139000, global step 2228238: loss 0.0044
[2019-03-27 08:57:52,608] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139000, global step 2228239: learning rate 0.0000
[2019-03-27 08:57:53,018] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139000, global step 2228428: loss 0.0045
[2019-03-27 08:57:53,020] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139000, global step 2228428: learning rate 0.0000
[2019-03-27 08:57:53,082] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139000, global step 2228460: loss 0.0045
[2019-03-27 08:57:53,085] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139000, global step 2228461: learning rate 0.0000
[2019-03-27 08:57:53,612] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139000, global step 2228705: loss 0.0045
[2019-03-27 08:57:53,616] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139000, global step 2228705: learning rate 0.0000
[2019-03-27 08:57:53,850] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139000, global step 2228818: loss 0.0045
[2019-03-27 08:57:53,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139000, global step 2228819: learning rate 0.0000
[2019-03-27 08:57:55,265] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139500, global step 2229480: loss 52.8817
[2019-03-27 08:57:55,269] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139500, global step 2229481: learning rate 0.0000
[2019-03-27 08:57:59,310] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139500, global step 2231295: loss 72.3262
[2019-03-27 08:57:59,314] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139500, global step 2231296: learning rate 0.0000
[2019-03-27 08:57:59,399] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140500, global step 2231336: loss 0.4691
[2019-03-27 08:57:59,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140500, global step 2231336: learning rate 0.0000
[2019-03-27 08:58:00,878] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140500, global step 2232024: loss 0.9217
[2019-03-27 08:58:00,881] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140500, global step 2232025: learning rate 0.0000
[2019-03-27 08:58:02,341] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139500, global step 2232707: loss 86.6870
[2019-03-27 08:58:02,344] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139500, global step 2232710: learning rate 0.0000
[2019-03-27 08:58:05,895] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140000, global step 2234364: loss 0.0174
[2019-03-27 08:58:05,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140000, global step 2234367: learning rate 0.0000
[2019-03-27 08:58:06,581] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139500, global step 2234684: loss 91.6964
[2019-03-27 08:58:06,587] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139500, global step 2234684: learning rate 0.0000
[2019-03-27 08:58:08,582] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139500, global step 2235615: loss 91.2115
[2019-03-27 08:58:08,588] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139500, global step 2235615: learning rate 0.0000
[2019-03-27 08:58:08,782] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139500, global step 2235708: loss 72.1390
[2019-03-27 08:58:08,784] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139500, global step 2235708: learning rate 0.0000
[2019-03-27 08:58:09,270] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139500, global step 2235937: loss 54.6181
[2019-03-27 08:58:09,276] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139500, global step 2235937: learning rate 0.0000
[2019-03-27 08:58:09,716] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139500, global step 2236144: loss 56.0403
[2019-03-27 08:58:09,719] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139500, global step 2236145: learning rate 0.0000
[2019-03-27 08:58:09,831] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139500, global step 2236200: loss 50.8629
[2019-03-27 08:58:09,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139500, global step 2236201: learning rate 0.0000
[2019-03-27 08:58:10,201] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139500, global step 2236373: loss 48.8980
[2019-03-27 08:58:10,202] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139500, global step 2236373: learning rate 0.0000
[2019-03-27 08:58:10,342] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139500, global step 2236438: loss 49.0712
[2019-03-27 08:58:10,344] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139500, global step 2236438: learning rate 0.0000
[2019-03-27 08:58:10,988] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139500, global step 2236740: loss 55.7302
[2019-03-27 08:58:10,989] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139500, global step 2236740: learning rate 0.0000
[2019-03-27 08:58:11,074] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139500, global step 2236781: loss 47.1908
[2019-03-27 08:58:11,077] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139500, global step 2236781: learning rate 0.0000
[2019-03-27 08:58:12,372] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140000, global step 2237370: loss 0.0167
[2019-03-27 08:58:12,373] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140000, global step 2237370: learning rate 0.0000
[2019-03-27 08:58:14,867] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6895938e-17 1.0000000e+00 1.7009993e-22 3.2642378e-13 5.4263936e-26], sum to 1.0000
[2019-03-27 08:58:14,880] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1244
[2019-03-27 08:58:14,885] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6246589710612662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872935.4382042255, 872935.4382042255, 205430.1732259692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4318200.0000, 
sim time next is 4318800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.624745264154756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873056.0788470854, 873056.0788470854, 205446.8751922994], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5478858604274168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24251557745752372, 0.24251557745752372, 0.30663712715268565], 
reward next is 0.6934, 
noisyNet noise sample is [array([-0.50071794], dtype=float32), 0.026928326]. 
=============================================
[2019-03-27 08:58:16,231] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140000, global step 2239180: loss 0.0195
[2019-03-27 08:58:16,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140000, global step 2239180: learning rate 0.0000
[2019-03-27 08:58:16,551] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141000, global step 2239326: loss 0.0198
[2019-03-27 08:58:16,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141000, global step 2239326: learning rate 0.0000
[2019-03-27 08:58:18,046] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141000, global step 2240019: loss 0.0220
[2019-03-27 08:58:18,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141000, global step 2240020: learning rate 0.0000
[2019-03-27 08:58:19,361] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140000, global step 2240641: loss 0.0161
[2019-03-27 08:58:19,365] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140000, global step 2240641: learning rate 0.0000
[2019-03-27 08:58:23,249] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140500, global step 2242463: loss -53.8610
[2019-03-27 08:58:23,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140500, global step 2242463: learning rate 0.0000
[2019-03-27 08:58:23,717] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140000, global step 2242672: loss 0.0165
[2019-03-27 08:58:23,720] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140000, global step 2242674: learning rate 0.0000
[2019-03-27 08:58:25,651] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140000, global step 2243584: loss 0.0110
[2019-03-27 08:58:25,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140000, global step 2243585: learning rate 0.0000
[2019-03-27 08:58:25,720] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3886144e-10 6.5503567e-01 9.2373171e-14 3.4496433e-01 4.3245340e-17], sum to 1.0000
[2019-03-27 08:58:25,729] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9480
[2019-03-27 08:58:25,737] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.669585499833891, 1.0, 2.0, 0.6553827894312082, 1.0, 2.0, 1.03, 7.005095334123508, 6.9112, 170.5573041426782, 2749908.365580036, 2682647.318329374, 511893.6141709324], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5245200.0000, 
sim time next is 5245800.0000, 
raw observation next is [30.83333333333334, 70.83333333333334, 1.0, 2.0, 0.42215687925416, 1.0, 2.0, 0.42215687925416, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1180058.275384133, 1180058.275384133, 277859.2760179454], 
processed observation next is [1.0, 0.7391304347826086, 0.6603475513428123, 0.7083333333333335, 1.0, 1.0, 0.30380346898091565, 1.0, 1.0, 0.30380346898091565, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3277939653844814, 0.3277939653844814, 0.414715337340217], 
reward next is 0.5853, 
noisyNet noise sample is [array([-1.2826163], dtype=float32), 0.7149743]. 
=============================================
[2019-03-27 08:58:25,901] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140000, global step 2243694: loss 0.0130
[2019-03-27 08:58:25,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140000, global step 2243696: learning rate 0.0000
[2019-03-27 08:58:26,466] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140000, global step 2243963: loss 0.0111
[2019-03-27 08:58:26,469] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140000, global step 2243964: learning rate 0.0000
[2019-03-27 08:58:26,851] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140000, global step 2244140: loss 0.0087
[2019-03-27 08:58:26,854] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140000, global step 2244141: learning rate 0.0000
[2019-03-27 08:58:27,206] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140000, global step 2244304: loss 0.0090
[2019-03-27 08:58:27,211] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140000, global step 2244306: learning rate 0.0000
[2019-03-27 08:58:27,364] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140000, global step 2244365: loss 0.0088
[2019-03-27 08:58:27,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140000, global step 2244366: learning rate 0.0000
[2019-03-27 08:58:27,549] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140000, global step 2244450: loss 0.0093
[2019-03-27 08:58:27,551] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140000, global step 2244451: learning rate 0.0000
[2019-03-27 08:58:28,260] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140000, global step 2244773: loss 0.0064
[2019-03-27 08:58:28,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140000, global step 2244773: learning rate 0.0000
[2019-03-27 08:58:28,303] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140000, global step 2244794: loss 0.0068
[2019-03-27 08:58:28,309] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140000, global step 2244794: learning rate 0.0000
[2019-03-27 08:58:29,813] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140500, global step 2245506: loss 4.0060
[2019-03-27 08:58:29,818] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140500, global step 2245507: learning rate 0.0000
[2019-03-27 08:58:31,054] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3727483e-14 9.9999988e-01 1.5609872e-18 1.3150785e-07 1.7379337e-21], sum to 1.0000
[2019-03-27 08:58:31,058] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9990
[2019-03-27 08:58:31,064] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5410592027650925, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9396409041903849, 6.911199999999999, 6.9112, 168.9129564433118, 1512671.72282889, 1512671.722828891, 331329.9534609204], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4606800.0000, 
sim time next is 4607400.0000, 
raw observation next is [29.83333333333333, 84.83333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.149824441929244, 6.9112, 168.9115092345015, 1623157.807619071, 1453870.866187732, 311356.3865145182], 
processed observation next is [1.0, 0.30434782608695654, 0.6129541864139019, 0.8483333333333333, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.023862444192924402, 0.0, 0.829432838364923, 0.4508771687830753, 0.4038530183854811, 0.4647110246485346], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.12713315], dtype=float32), 0.120939575]. 
=============================================
[2019-03-27 08:58:31,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5699036e-17 1.0000000e+00 2.7349647e-23 1.8514007e-14 1.2873349e-24], sum to 1.0000
[2019-03-27 08:58:31,129] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2311
[2019-03-27 08:58:31,137] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 88.16666666666667, 1.0, 2.0, 0.5706578546402168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 797442.7510791074, 797442.7510791081, 195406.6287452879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4582200.0000, 
sim time next is 4582800.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.5749748095201753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803477.5868381604, 803477.5868381604, 196176.1679645576], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.89, 1.0, 1.0, 0.4879214572532233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22318821856615567, 0.22318821856615567, 0.29280025069336957], 
reward next is 0.7072, 
noisyNet noise sample is [array([1.0385808], dtype=float32), -0.0020977582]. 
=============================================
[2019-03-27 08:58:33,706] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140500, global step 2247325: loss 1.5673
[2019-03-27 08:58:33,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140500, global step 2247325: learning rate 0.0000
[2019-03-27 08:58:33,730] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141500, global step 2247336: loss -48.5280
[2019-03-27 08:58:33,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141500, global step 2247336: learning rate 0.0000
[2019-03-27 08:58:35,408] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141500, global step 2248122: loss -73.5630
[2019-03-27 08:58:35,410] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141500, global step 2248123: learning rate 0.0000
[2019-03-27 08:58:36,817] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140500, global step 2248779: loss 58.2441
[2019-03-27 08:58:36,820] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140500, global step 2248779: learning rate 0.0000
[2019-03-27 08:58:39,431] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 08:58:39,433] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 08:58:39,433] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:58:39,435] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 08:58:39,436] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 08:58:39,438] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 08:58:39,438] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:58:39,438] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 08:58:39,439] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:58:39,440] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:58:39,438] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 08:58:39,469] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run91
[2019-03-27 08:58:39,497] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run91
[2019-03-27 08:58:39,518] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run91
[2019-03-27 08:58:39,519] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run91
[2019-03-27 08:58:39,519] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run91
[2019-03-27 08:59:24,017] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0904552], dtype=float32), 0.048567433]
[2019-03-27 08:59:24,019] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.31722724333333, 100.0, 1.0, 2.0, 0.465230549156577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764319.8337367129, 764319.8337367129, 189758.1360794865]
[2019-03-27 08:59:24,019] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 08:59:24,021] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6303313e-18 1.0000000e+00 9.7942751e-25 2.0148807e-18 4.0780473e-27], sampled 0.10862347580841913
[2019-03-27 08:59:55,119] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0904552], dtype=float32), 0.048567433]
[2019-03-27 08:59:55,120] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.15057937666667, 60.36499749333333, 1.0, 2.0, 0.5891545935551069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823300.2886589341, 823300.2886589341, 198741.912558321]
[2019-03-27 08:59:55,121] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 08:59:55,124] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4270284e-18 1.0000000e+00 1.0366701e-24 3.4255472e-17 2.4682013e-27], sampled 0.964212099412897
[2019-03-27 09:00:02,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0904552], dtype=float32), 0.048567433]
[2019-03-27 09:00:02,527] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.91588835666667, 69.07599738666667, 1.0, 2.0, 0.5276006141080744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737253.2698531506, 737253.26985315, 188033.467045875]
[2019-03-27 09:00:02,529] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:00:02,532] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5498740e-18 1.0000000e+00 7.3633288e-25 1.0458972e-17 1.8761691e-27], sampled 0.5673816470187069
[2019-03-27 09:00:05,440] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0904552], dtype=float32), 0.048567433]
[2019-03-27 09:00:05,442] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.5, 83.0, 1.0, 2.0, 0.5555106039364517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776268.1032273677, 776268.1032273677, 192750.558847794]
[2019-03-27 09:00:05,443] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:00:05,444] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2373047e-17 1.0000000e+00 2.1898400e-23 1.5359399e-14 1.0993940e-26], sampled 0.5174407283594569
[2019-03-27 09:00:34,568] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.6871 3163115219.3008 1750.0000
[2019-03-27 09:00:35,034] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 09:00:35,280] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.1255 3007618601.0223 1767.0000
[2019-03-27 09:00:35,450] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.5480 2842460214.3059 1128.0000
[2019-03-27 09:00:35,481] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 09:00:36,499] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2250000, evaluation results [2250000.0, 7890.687067744974, 3163115219.300782, 1750.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7999.125455620987, 3007618601.0223403, 1767.0, 8496.548017675907, 2842460214.3059278, 1128.0]
[2019-03-27 09:00:37,603] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4302102e-17 1.0000000e+00 1.7187858e-23 4.5848748e-14 3.2819229e-25], sum to 1.0000
[2019-03-27 09:00:37,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4676
[2019-03-27 09:00:37,619] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 86.5, 1.0, 2.0, 0.5061648481581459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707289.6013497357, 707289.6013497364, 184567.5614508101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4930200.0000, 
sim time next is 4930800.0000, 
raw observation next is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5048256229970798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705417.6133108041, 705417.6133108041, 184355.5239324152], 
processed observation next is [1.0, 0.043478260869565216, 0.44707740916271754, 0.8733333333333333, 1.0, 1.0, 0.4034043650567226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19594933703077894, 0.19594933703077894, 0.2751574984065899], 
reward next is 0.7248, 
noisyNet noise sample is [array([-0.27276763], dtype=float32), 0.39551485]. 
=============================================
[2019-03-27 09:00:37,636] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141000, global step 2250519: loss 0.0141
[2019-03-27 09:00:37,641] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141000, global step 2250520: learning rate 0.0000
[2019-03-27 09:00:38,198] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140500, global step 2250770: loss -29.5818
[2019-03-27 09:00:38,200] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140500, global step 2250770: learning rate 0.0000
[2019-03-27 09:00:40,145] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140500, global step 2251611: loss -90.6313
[2019-03-27 09:00:40,147] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140500, global step 2251611: learning rate 0.0000
[2019-03-27 09:00:40,438] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140500, global step 2251715: loss -15.0216
[2019-03-27 09:00:40,441] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140500, global step 2251716: learning rate 0.0000
[2019-03-27 09:00:41,039] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140500, global step 2251978: loss -0.4088
[2019-03-27 09:00:41,041] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140500, global step 2251979: learning rate 0.0000
[2019-03-27 09:00:41,296] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.984889e-17 1.000000e+00 9.524068e-23 3.667996e-13 1.314644e-25], sum to 1.0000
[2019-03-27 09:00:41,301] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5020
[2019-03-27 09:00:41,304] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6864991842597744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 959393.647256703, 959393.6472567037, 217958.2614225175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4863600.0000, 
sim time next is 4864200.0000, 
raw observation next is [27.16666666666666, 87.33333333333334, 1.0, 2.0, 0.7132153483236026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 996747.4373574523, 996747.4373574523, 223723.5666201256], 
processed observation next is [1.0, 0.30434782608695654, 0.4865718799368086, 0.8733333333333334, 1.0, 1.0, 0.6544763232814489, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2768742881548479, 0.2768742881548479, 0.33391577107481435], 
reward next is 0.6661, 
noisyNet noise sample is [array([-0.08890682], dtype=float32), -0.16276672]. 
=============================================
[2019-03-27 09:00:41,442] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140500, global step 2252162: loss 0.8650
[2019-03-27 09:00:41,446] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140500, global step 2252162: learning rate 0.0000
[2019-03-27 09:00:41,702] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140500, global step 2252283: loss -97.0769
[2019-03-27 09:00:41,704] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140500, global step 2252285: loss -25.7141
[2019-03-27 09:00:41,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140500, global step 2252285: learning rate 0.0000
[2019-03-27 09:00:41,707] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140500, global step 2252286: learning rate 0.0000
[2019-03-27 09:00:41,984] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140500, global step 2252418: loss -54.1731
[2019-03-27 09:00:41,986] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140500, global step 2252420: learning rate 0.0000
[2019-03-27 09:00:42,688] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140500, global step 2252751: loss 2.4492
[2019-03-27 09:00:42,692] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140500, global step 2252752: learning rate 0.0000
[2019-03-27 09:00:42,739] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140500, global step 2252773: loss -7.1635
[2019-03-27 09:00:42,740] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140500, global step 2252773: learning rate 0.0000
[2019-03-27 09:00:43,237] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4720967e-16 1.0000000e+00 6.1804311e-23 3.0305673e-14 7.2773454e-26], sum to 1.0000
[2019-03-27 09:00:43,245] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7996
[2019-03-27 09:00:43,252] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 86.5, 1.0, 2.0, 0.6465221921074618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903501.3935972198, 903501.3935972198, 209724.8204450519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4861800.0000, 
sim time next is 4862400.0000, 
raw observation next is [27.0, 87.33333333333333, 1.0, 2.0, 0.6417696448468935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 896857.0028543914, 896857.0028543914, 208777.9244044003], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.8733333333333333, 1.0, 1.0, 0.5683971624661367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24912694523733092, 0.24912694523733092, 0.31160884239462733], 
reward next is 0.6884, 
noisyNet noise sample is [array([-1.3033513], dtype=float32), 0.48498502]. 
=============================================
[2019-03-27 09:00:43,952] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9946261e-18 1.0000000e+00 9.9039071e-25 7.4690010e-15 1.2838174e-26], sum to 1.0000
[2019-03-27 09:00:43,961] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4437
[2019-03-27 09:00:43,966] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 81.5, 1.0, 2.0, 0.6105353937534638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 853190.387444691, 853190.387444691, 202717.0632754363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4851000.0000, 
sim time next is 4851600.0000, 
raw observation next is [27.0, 82.33333333333334, 1.0, 2.0, 0.6128355261769016, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 856405.9947276551, 856405.9947276545, 203153.6686659883], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.8233333333333335, 1.0, 1.0, 0.5335367785263875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2378905540910153, 0.23789055409101514, 0.3032144308447587], 
reward next is 0.6968, 
noisyNet noise sample is [array([0.21944255], dtype=float32), -1.7399999]. 
=============================================
[2019-03-27 09:00:44,195] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141000, global step 2253456: loss 0.0034
[2019-03-27 09:00:44,197] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141000, global step 2253456: learning rate 0.0000
[2019-03-27 09:00:45,246] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1961120e-10 9.9723637e-01 2.7335225e-14 2.7635791e-03 3.5857204e-17], sum to 1.0000
[2019-03-27 09:00:45,255] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4559
[2019-03-27 09:00:45,262] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2168850.69945757 W.
[2019-03-27 09:00:45,266] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.9098691707569699, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.987920963327386, 6.9112, 168.9124997661259, 2168850.69945757, 2114422.354717175, 437471.9293143235], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4873800.0000, 
sim time next is 4874400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.7343399460721347, 1.0, 1.0, 0.7343399460721347, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2053541.997168872, 2053541.997168871, 389169.4020773647], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.7, 1.0, 1.0, 0.6799276458700418, 1.0, 0.5, 0.6799276458700418, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.570428332546909, 0.5704283325469086, 0.580849853846813], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1171107], dtype=float32), -1.1951158]. 
=============================================
[2019-03-27 09:00:47,825] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142000, global step 2255148: loss 0.3264
[2019-03-27 09:00:47,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142000, global step 2255148: learning rate 0.0000
[2019-03-27 09:00:48,199] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141000, global step 2255321: loss 0.0031
[2019-03-27 09:00:48,201] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141000, global step 2255321: learning rate 0.0000
[2019-03-27 09:00:49,434] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142000, global step 2255898: loss 0.3723
[2019-03-27 09:00:49,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142000, global step 2255898: learning rate 0.0000
[2019-03-27 09:00:50,947] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141000, global step 2256603: loss 0.0050
[2019-03-27 09:00:50,951] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141000, global step 2256603: learning rate 0.0000
[2019-03-27 09:00:52,566] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6987303e-14 1.0000000e+00 6.9180394e-21 3.0988975e-11 7.3739051e-23], sum to 1.0000
[2019-03-27 09:00:52,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0083
[2019-03-27 09:00:52,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1670926.500087064 W.
[2019-03-27 09:00:52,584] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.65, 90.0, 1.0, 2.0, 0.5976199588551261, 0.0, 2.0, 0.0, 1.0, 2.0, 1.017765769310614, 6.911200000000001, 6.9112, 168.9128980057304, 1670926.500087064, 1670926.500087064, 361728.8488257495], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5797800.0000, 
sim time next is 5798400.0000, 
raw observation next is [26.6, 90.33333333333334, 1.0, 2.0, 0.5643689251983437, 1.0, 1.0, 0.5643689251983437, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1577877.245966569, 1577877.245966569, 321462.4116890439], 
processed observation next is [1.0, 0.08695652173913043, 0.4597156398104266, 0.9033333333333334, 1.0, 1.0, 0.4751432833714984, 1.0, 0.5, 0.4751432833714984, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4382992349907136, 0.4382992349907136, 0.47979464431200586], 
reward next is 0.5202, 
noisyNet noise sample is [array([0.40340686], dtype=float32), 3.4228082]. 
=============================================
[2019-03-27 09:00:55,083] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141500, global step 2258545: loss -73.9274
[2019-03-27 09:00:55,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141500, global step 2258545: learning rate 0.0000
[2019-03-27 09:00:55,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.32920546e-11 8.88832688e-01 1.68894399e-13 1.11167304e-01
 1.28161730e-18], sum to 1.0000
[2019-03-27 09:00:55,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9386
[2019-03-27 09:00:55,132] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.587618523585989, 1.0, 2.0, 0.587618523585989, 1.0, 2.0, 1.020499047053601, 6.9112, 6.9112, 170.5573041426782, 2465297.035612545, 2465297.035612545, 481022.4698213142], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5235000.0000, 
sim time next is 5235600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.804625848905278, 1.0, 2.0, 0.804625848905278, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2250293.533449958, 2250293.533449959, 422134.9213148356], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.67, 1.0, 1.0, 0.7646094565123831, 1.0, 1.0, 0.7646094565123831, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6250815370694328, 0.625081537069433, 0.6300521213654263], 
reward next is 0.3699, 
noisyNet noise sample is [array([0.82100296], dtype=float32), -1.5566753]. 
=============================================
[2019-03-27 09:00:55,483] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141000, global step 2258731: loss 0.0026
[2019-03-27 09:00:55,485] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141000, global step 2258732: learning rate 0.0000
[2019-03-27 09:00:57,285] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141000, global step 2259570: loss 0.0039
[2019-03-27 09:00:57,286] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141000, global step 2259570: learning rate 0.0000
[2019-03-27 09:00:57,527] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141000, global step 2259684: loss 0.0031
[2019-03-27 09:00:57,529] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141000, global step 2259685: learning rate 0.0000
[2019-03-27 09:00:57,726] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.9221750e-19 1.0000000e+00 1.4641255e-26 6.6981700e-19 6.0864709e-30], sum to 1.0000
[2019-03-27 09:00:57,734] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4290
[2019-03-27 09:00:57,738] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5224280315099159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730022.7743752343, 730022.7743752343, 187184.4101223437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5084400.0000, 
sim time next is 5085000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5223618816257681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729930.3070669887, 729930.3070669881, 187173.6109945841], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4245323875009254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20275841862971908, 0.2027584186297189, 0.27936359849937925], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.3101269], dtype=float32), -0.21308564]. 
=============================================
[2019-03-27 09:00:57,754] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[74.820625]
 [74.82567 ]
 [74.8066  ]
 [74.75376 ]
 [74.76368 ]], R is [[74.79090881]
 [74.76361847]
 [74.73657227]
 [74.71006012]
 [74.68380737]].
[2019-03-27 09:00:58,241] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141000, global step 2260019: loss 0.0036
[2019-03-27 09:00:58,245] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141000, global step 2260019: learning rate 0.0000
[2019-03-27 09:00:58,620] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141000, global step 2260194: loss 0.0038
[2019-03-27 09:00:58,622] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141000, global step 2260195: learning rate 0.0000
[2019-03-27 09:00:58,980] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141000, global step 2260359: loss 0.0031
[2019-03-27 09:00:58,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141000, global step 2260359: learning rate 0.0000
[2019-03-27 09:00:58,987] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141000, global step 2260359: loss 0.0035
[2019-03-27 09:00:58,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141000, global step 2260360: learning rate 0.0000
[2019-03-27 09:00:59,290] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141000, global step 2260506: loss 0.0034
[2019-03-27 09:00:59,291] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141000, global step 2260506: learning rate 0.0000
[2019-03-27 09:00:59,685] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7840683e-17 1.0000000e+00 1.9988393e-21 2.7183481e-12 4.4546167e-25], sum to 1.0000
[2019-03-27 09:00:59,693] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6249
[2019-03-27 09:00:59,701] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333333, 95.66666666666667, 1.0, 2.0, 0.6432529430951973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898930.7522360573, 898930.7522360573, 209072.2470168466], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5892000.0000, 
sim time next is 5892600.0000, 
raw observation next is [25.61666666666667, 95.83333333333333, 1.0, 2.0, 0.6389563118543841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 892923.7877631879, 892923.7877631885, 208219.7802836018], 
processed observation next is [1.0, 0.17391304347826086, 0.41311216429699865, 0.9583333333333333, 1.0, 1.0, 0.5650076046438361, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2480343854897744, 0.24803438548977458, 0.3107757914680624], 
reward next is 0.6892, 
noisyNet noise sample is [array([-0.03947516], dtype=float32), -1.598012]. 
=============================================
[2019-03-27 09:00:59,880] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141000, global step 2260778: loss 0.0038
[2019-03-27 09:00:59,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141000, global step 2260782: learning rate 0.0000
[2019-03-27 09:00:59,988] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141000, global step 2260836: loss 0.0035
[2019-03-27 09:00:59,989] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141000, global step 2260836: learning rate 0.0000
[2019-03-27 09:01:01,266] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141500, global step 2261414: loss -73.9694
[2019-03-27 09:01:01,268] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141500, global step 2261414: learning rate 0.0000
[2019-03-27 09:01:02,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7959103e-19 1.0000000e+00 4.3107964e-26 4.7748223e-18 1.9793762e-29], sum to 1.0000
[2019-03-27 09:01:02,911] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7488
[2019-03-27 09:01:02,916] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 71.33333333333333, 1.0, 2.0, 0.5319240645207146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 743296.8435203222, 743296.8435203228, 188749.0839518509], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5164800.0000, 
sim time next is 5165400.0000, 
raw observation next is [29.33333333333333, 72.66666666666667, 1.0, 2.0, 0.5296395360667537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740103.3907889776, 740103.3907889769, 188370.2028500197], 
processed observation next is [0.0, 0.782608695652174, 0.5892575039494469, 0.7266666666666667, 1.0, 1.0, 0.4333006458635587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20558427521916045, 0.20558427521916026, 0.28114955649256673], 
reward next is 0.7189, 
noisyNet noise sample is [array([-0.05093855], dtype=float32), -0.2916971]. 
=============================================
[2019-03-27 09:01:05,098] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142500, global step 2263212: loss -118.4620
[2019-03-27 09:01:05,100] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142500, global step 2263213: learning rate 0.0000
[2019-03-27 09:01:05,408] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141500, global step 2263355: loss -73.5009
[2019-03-27 09:01:05,412] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141500, global step 2263355: learning rate 0.0000
[2019-03-27 09:01:06,756] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142500, global step 2263980: loss -96.9037
[2019-03-27 09:01:06,758] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142500, global step 2263980: learning rate 0.0000
[2019-03-27 09:01:08,113] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141500, global step 2264610: loss -72.3787
[2019-03-27 09:01:08,114] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141500, global step 2264610: learning rate 0.0000
[2019-03-27 09:01:12,061] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142000, global step 2266461: loss 0.6397
[2019-03-27 09:01:12,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142000, global step 2266461: learning rate 0.0000
[2019-03-27 09:01:12,815] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141500, global step 2266811: loss -70.4546
[2019-03-27 09:01:12,817] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141500, global step 2266811: learning rate 0.0000
[2019-03-27 09:01:14,600] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141500, global step 2267646: loss -47.4586
[2019-03-27 09:01:14,602] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141500, global step 2267647: learning rate 0.0000
[2019-03-27 09:01:14,884] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141500, global step 2267779: loss -69.3678
[2019-03-27 09:01:14,888] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141500, global step 2267780: learning rate 0.0000
[2019-03-27 09:01:15,293] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141500, global step 2267971: loss -69.8268
[2019-03-27 09:01:15,297] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141500, global step 2267971: learning rate 0.0000
[2019-03-27 09:01:15,738] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141500, global step 2268178: loss -57.6363
[2019-03-27 09:01:15,742] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141500, global step 2268179: learning rate 0.0000
[2019-03-27 09:01:16,068] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141500, global step 2268333: loss -68.5765
[2019-03-27 09:01:16,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141500, global step 2268333: learning rate 0.0000
[2019-03-27 09:01:16,154] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141500, global step 2268372: loss -68.4592
[2019-03-27 09:01:16,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141500, global step 2268373: learning rate 0.0000
[2019-03-27 09:01:16,376] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141500, global step 2268476: loss -67.8523
[2019-03-27 09:01:16,377] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141500, global step 2268476: learning rate 0.0000
[2019-03-27 09:01:16,960] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141500, global step 2268747: loss -68.0353
[2019-03-27 09:01:16,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141500, global step 2268747: learning rate 0.0000
[2019-03-27 09:01:17,000] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141500, global step 2268768: loss -68.5037
[2019-03-27 09:01:17,005] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141500, global step 2268768: learning rate 0.0000
[2019-03-27 09:01:18,146] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142000, global step 2269304: loss 0.5657
[2019-03-27 09:01:18,149] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142000, global step 2269304: learning rate 0.0000
[2019-03-27 09:01:22,337] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142000, global step 2271257: loss 0.4312
[2019-03-27 09:01:22,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142000, global step 2271257: learning rate 0.0000
[2019-03-27 09:01:22,471] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143000, global step 2271315: loss 0.0135
[2019-03-27 09:01:22,473] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143000, global step 2271315: learning rate 0.0000
[2019-03-27 09:01:23,746] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.2127157e-20 1.0000000e+00 1.2078409e-24 2.2855731e-14 6.7733473e-28], sum to 1.0000
[2019-03-27 09:01:23,752] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4546
[2019-03-27 09:01:23,761] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.01666666666667, 88.16666666666667, 1.0, 2.0, 0.5691164941061998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795288.0327493323, 795288.032749333, 195133.373759114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5602200.0000, 
sim time next is 5602800.0000, 
raw observation next is [27.93333333333334, 88.33333333333334, 1.0, 2.0, 0.5678499471285307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793517.4885544695, 793517.4885544688, 194909.1621618885], 
processed observation next is [1.0, 0.8695652173913043, 0.5229067930489735, 0.8833333333333334, 1.0, 1.0, 0.4793372856970249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22042152459846376, 0.22042152459846356, 0.29090919725655], 
reward next is 0.7091, 
noisyNet noise sample is [array([-0.0889646], dtype=float32), -0.65332]. 
=============================================
[2019-03-27 09:01:23,973] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143000, global step 2272016: loss 0.0113
[2019-03-27 09:01:23,977] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143000, global step 2272018: learning rate 0.0000
[2019-03-27 09:01:25,020] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142000, global step 2272499: loss 0.3894
[2019-03-27 09:01:25,021] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142000, global step 2272499: learning rate 0.0000
[2019-03-27 09:01:29,327] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142500, global step 2274522: loss -78.0408
[2019-03-27 09:01:29,329] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142500, global step 2274522: learning rate 0.0000
[2019-03-27 09:01:29,765] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142000, global step 2274725: loss 0.3173
[2019-03-27 09:01:29,769] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142000, global step 2274725: learning rate 0.0000
[2019-03-27 09:01:30,352] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 09:01:30,354] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:01:30,355] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:01:30,356] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:01:30,357] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:01:30,357] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:01:30,358] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:01:30,360] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:01:30,361] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:01:30,360] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:01:30,363] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:01:30,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run92
[2019-03-27 09:01:30,418] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run92
[2019-03-27 09:01:30,444] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run92
[2019-03-27 09:01:30,444] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run92
[2019-03-27 09:01:30,496] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run92
[2019-03-27 09:01:32,271] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08995455], dtype=float32), 0.04910696]
[2019-03-27 09:01:32,273] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.3, 89.0, 1.0, 2.0, 0.3446772152033712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535782.4271607496, 535782.427160749, 169607.1258193017]
[2019-03-27 09:01:32,273] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:01:32,275] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0216934e-19 1.0000000e+00 4.4163752e-26 2.5483100e-19 1.0245123e-28], sampled 0.7664318848479804
[2019-03-27 09:02:01,119] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08995455], dtype=float32), 0.04910696]
[2019-03-27 09:02:01,121] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.33209577666667, 88.71340266166666, 1.0, 2.0, 0.3498934285173539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544920.7074019046, 544920.7074019052, 170377.996173106]
[2019-03-27 09:02:01,123] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:02:01,127] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.2553077e-20 1.0000000e+00 1.4659921e-26 2.6236301e-20 4.0475271e-29], sampled 0.934267667726785
[2019-03-27 09:02:01,789] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08995455], dtype=float32), 0.04910696]
[2019-03-27 09:02:01,790] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.43333333333334, 91.66666666666667, 1.0, 2.0, 0.5300725391990955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740708.6681487986, 740708.6681487993, 188441.9285015988]
[2019-03-27 09:02:01,791] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:02:01,795] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.0114892e-20 1.0000000e+00 1.4856966e-26 3.4358823e-19 1.6734276e-29], sampled 0.982579886310639
[2019-03-27 09:02:02,722] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08995455], dtype=float32), 0.04910696]
[2019-03-27 09:02:02,724] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.27538881, 84.32896772, 1.0, 2.0, 0.8544661106717522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1194262.126430102, 1194262.126430101, 257723.6940712951]
[2019-03-27 09:02:02,726] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:02:02,729] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0309935e-18 1.0000000e+00 7.8510603e-25 1.9264017e-17 1.8579080e-27], sampled 0.9948794542861057
[2019-03-27 09:03:16,656] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08995455], dtype=float32), 0.04910696]
[2019-03-27 09:03:16,657] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.57000671333333, 93.73742747666667, 1.0, 2.0, 0.3853277433111837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584264.4780244398, 584264.4780244398, 173368.8459239634]
[2019-03-27 09:03:16,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:03:16,661] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6713395e-19 1.0000000e+00 6.4913971e-26 2.6786458e-19 2.1471762e-28], sampled 0.5319297220984864
[2019-03-27 09:03:26,143] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8724 2842497929.8887 1131.0000
[2019-03-27 09:03:26,364] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.7818 3163790683.3244 1772.0000
[2019-03-27 09:03:26,466] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8893 2779197741.5739 933.0000
[2019-03-27 09:03:26,482] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1616 2927470683.5367 1337.0000
[2019-03-27 09:03:26,530] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6958 3007725035.1041 1766.0000
[2019-03-27 09:03:27,551] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2275000, evaluation results [2275000.0, 7884.781775176379, 3163790683.3243723, 1772.0, 8255.161616161979, 2927470683.5367417, 1337.0, 8659.889250812355, 2779197741.573899, 933.0, 7997.695836624704, 3007725035.1041055, 1766.0, 8496.87235025343, 2842497929.888741, 1131.0]
[2019-03-27 09:03:28,690] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142000, global step 2275515: loss 0.2689
[2019-03-27 09:03:28,695] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142000, global step 2275516: learning rate 0.0000
[2019-03-27 09:03:29,009] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142000, global step 2275659: loss 0.2690
[2019-03-27 09:03:29,010] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142000, global step 2275659: learning rate 0.0000
[2019-03-27 09:03:29,637] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142000, global step 2275940: loss 0.2835
[2019-03-27 09:03:29,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142000, global step 2275940: learning rate 0.0000
[2019-03-27 09:03:30,228] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142000, global step 2276208: loss 0.3200
[2019-03-27 09:03:30,233] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142000, global step 2276211: learning rate 0.0000
[2019-03-27 09:03:30,530] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142000, global step 2276345: loss 0.3205
[2019-03-27 09:03:30,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142000, global step 2276346: learning rate 0.0000
[2019-03-27 09:03:30,561] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142000, global step 2276361: loss 0.3323
[2019-03-27 09:03:30,564] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142000, global step 2276361: learning rate 0.0000
[2019-03-27 09:03:30,733] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142000, global step 2276442: loss 0.3334
[2019-03-27 09:03:30,735] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142000, global step 2276442: learning rate 0.0000
[2019-03-27 09:03:31,571] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142000, global step 2276760: loss 0.3391
[2019-03-27 09:03:31,581] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142000, global step 2276760: learning rate 0.0000
[2019-03-27 09:03:31,672] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142000, global step 2276799: loss 0.3436
[2019-03-27 09:03:31,676] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142000, global step 2276800: learning rate 0.0000
[2019-03-27 09:03:32,824] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142500, global step 2277322: loss -48.7038
[2019-03-27 09:03:32,827] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142500, global step 2277322: learning rate 0.0000
[2019-03-27 09:03:37,098] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142500, global step 2279322: loss -60.1489
[2019-03-27 09:03:37,100] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142500, global step 2279324: learning rate 0.0000
[2019-03-27 09:03:37,579] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143500, global step 2279547: loss 226.3483
[2019-03-27 09:03:37,581] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143500, global step 2279547: learning rate 0.0000
[2019-03-27 09:03:39,025] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143500, global step 2280218: loss 229.8763
[2019-03-27 09:03:39,026] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143500, global step 2280218: learning rate 0.0000
[2019-03-27 09:03:39,868] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142500, global step 2280610: loss -135.5108
[2019-03-27 09:03:39,869] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142500, global step 2280610: learning rate 0.0000
[2019-03-27 09:03:43,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.4299231e-16 1.0000000e+00 1.0180017e-22 1.6413110e-14 8.1915082e-26], sum to 1.0000
[2019-03-27 09:03:43,129] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1561
[2019-03-27 09:03:43,138] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 87.0, 1.0, 2.0, 1.032136021574284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1442754.863287279, 1442754.86328728, 308868.8246922487], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6683400.0000, 
sim time next is 6684000.0000, 
raw observation next is [26.86666666666667, 86.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.301098715168418, 6.9112, 168.910803126166, 1730548.414733676, 1453944.373242454, 311349.028635154], 
processed observation next is [1.0, 0.34782608695652173, 0.4723538704581361, 0.86, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.038989871516841836, 0.0, 0.8294293710497379, 0.4807078929815767, 0.40387343701179274, 0.4647000427390358], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1845894], dtype=float32), 1.7270198]. 
=============================================
[2019-03-27 09:03:43,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[61.653572]
 [63.061428]
 [64.345406]
 [64.04084 ]
 [63.84387 ]], R is [[59.67278671]
 [59.61506271]
 [59.62273407]
 [59.72974396]
 [59.8231926 ]].
[2019-03-27 09:03:44,045] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143000, global step 2282536: loss 0.0070
[2019-03-27 09:03:44,050] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143000, global step 2282536: learning rate 0.0000
[2019-03-27 09:03:44,506] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142500, global step 2282750: loss -160.3954
[2019-03-27 09:03:44,512] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142500, global step 2282750: learning rate 0.0000
[2019-03-27 09:03:46,085] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142500, global step 2283490: loss -81.7208
[2019-03-27 09:03:46,087] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142500, global step 2283491: learning rate 0.0000
[2019-03-27 09:03:46,182] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142500, global step 2283532: loss -54.8066
[2019-03-27 09:03:46,186] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142500, global step 2283532: learning rate 0.0000
[2019-03-27 09:03:46,953] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6802229e-19 1.0000000e+00 1.1303661e-26 1.7272022e-19 1.0976580e-28], sum to 1.0000
[2019-03-27 09:03:46,960] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0597
[2019-03-27 09:03:46,967] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 83.33333333333334, 1.0, 2.0, 0.405761655036166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642075.9602737251, 642075.9602737251, 178947.0463925825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6747600.0000, 
sim time next is 6748200.0000, 
raw observation next is [22.25, 83.5, 1.0, 2.0, 0.3659360682601785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579438.8657669369, 579438.8657669376, 173408.5297280036], 
processed observation next is [1.0, 0.08695652173913043, 0.2535545023696683, 0.835, 1.0, 1.0, 0.23606755212069697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1609552404908158, 0.160955240490816, 0.25881870108657257], 
reward next is 0.7412, 
noisyNet noise sample is [array([-1.7577066], dtype=float32), 0.14833798]. 
=============================================
[2019-03-27 09:03:46,981] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142500, global step 2283909: loss -139.5157
[2019-03-27 09:03:46,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142500, global step 2283909: learning rate 0.0000
[2019-03-27 09:03:47,618] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142500, global step 2284205: loss -113.2534
[2019-03-27 09:03:47,621] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142500, global step 2284206: learning rate 0.0000
[2019-03-27 09:03:47,791] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142500, global step 2284286: loss -112.3673
[2019-03-27 09:03:47,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142500, global step 2284287: learning rate 0.0000
[2019-03-27 09:03:47,890] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142500, global step 2284326: loss -231.7947
[2019-03-27 09:03:47,897] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142500, global step 2284328: learning rate 0.0000
[2019-03-27 09:03:48,038] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142500, global step 2284398: loss -123.3653
[2019-03-27 09:03:48,040] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142500, global step 2284399: learning rate 0.0000
[2019-03-27 09:03:48,561] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142500, global step 2284641: loss -121.6957
[2019-03-27 09:03:48,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142500, global step 2284641: learning rate 0.0000
[2019-03-27 09:03:48,782] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142500, global step 2284741: loss -158.1383
[2019-03-27 09:03:48,786] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142500, global step 2284742: learning rate 0.0000
[2019-03-27 09:03:49,914] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143000, global step 2285263: loss 0.0019
[2019-03-27 09:03:49,918] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143000, global step 2285265: learning rate 0.0000
[2019-03-27 09:03:54,247] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143000, global step 2287293: loss 0.0018
[2019-03-27 09:03:54,251] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143000, global step 2287293: learning rate 0.0000
[2019-03-27 09:03:54,335] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144000, global step 2287331: loss 2.7631
[2019-03-27 09:03:54,336] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144000, global step 2287331: learning rate 0.0000
[2019-03-27 09:03:55,884] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144000, global step 2288041: loss 2.6153
[2019-03-27 09:03:55,887] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144000, global step 2288041: learning rate 0.0000
[2019-03-27 09:03:57,021] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143000, global step 2288572: loss 0.0028
[2019-03-27 09:03:57,023] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143000, global step 2288573: learning rate 0.0000
[2019-03-27 09:03:57,733] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.7312319e-12 9.9997211e-01 2.3440255e-16 2.7920083e-05 3.9203651e-19], sum to 1.0000
[2019-03-27 09:03:57,740] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8385
[2019-03-27 09:03:57,746] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2053164.773003296 W.
[2019-03-27 09:03:57,750] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.3, 84.0, 1.0, 2.0, 0.4894701207203381, 1.0, 2.0, 0.4894701207203381, 1.0, 1.0, 0.8500477294487828, 6.9112, 6.9112, 170.5573041426782, 2053164.773003296, 2053164.773003296, 407940.3007109011], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6166800.0000, 
sim time next is 6167400.0000, 
raw observation next is [28.38333333333334, 83.33333333333333, 1.0, 2.0, 0.4613292367214885, 1.0, 2.0, 0.4613292367214885, 1.0, 2.0, 0.8011763202753286, 6.911200000000001, 6.9112, 170.5573041426782, 1935016.497194414, 1935016.497194413, 389434.3251885694], 
processed observation next is [1.0, 0.391304347826087, 0.544233807266983, 0.8333333333333333, 1.0, 1.0, 0.3509990803873355, 1.0, 1.0, 0.3509990803873355, 1.0, 1.0, 0.7575320978967423, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5375045825540039, 0.5375045825540037, 0.5812452614754767], 
reward next is 0.4188, 
noisyNet noise sample is [array([0.6013091], dtype=float32), -0.06834726]. 
=============================================
[2019-03-27 09:04:01,458] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143500, global step 2290648: loss 202.5378
[2019-03-27 09:04:01,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143500, global step 2290650: learning rate 0.0000
[2019-03-27 09:04:01,723] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143000, global step 2290775: loss 0.0041
[2019-03-27 09:04:01,726] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143000, global step 2290775: learning rate 0.0000
[2019-03-27 09:04:03,361] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143000, global step 2291538: loss 0.0077
[2019-03-27 09:04:03,363] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143000, global step 2291539: learning rate 0.0000
[2019-03-27 09:04:03,408] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143000, global step 2291559: loss 0.0074
[2019-03-27 09:04:03,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143000, global step 2291559: learning rate 0.0000
[2019-03-27 09:04:04,317] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143000, global step 2291981: loss 0.0032
[2019-03-27 09:04:04,319] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143000, global step 2291982: learning rate 0.0000
[2019-03-27 09:04:04,526] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2154402e-17 1.0000000e+00 2.1395927e-24 1.0127685e-15 1.3945545e-26], sum to 1.0000
[2019-03-27 09:04:04,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6425
[2019-03-27 09:04:04,542] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 91.33333333333334, 1.0, 2.0, 0.7212835638125493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008028.447919334, 1008028.447919334, 225505.3897366343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6492000.0000, 
sim time next is 6492600.0000, 
raw observation next is [26.25, 91.5, 1.0, 2.0, 0.7653272860402697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1069612.665953984, 1069612.665953983, 235576.9830237671], 
processed observation next is [1.0, 0.13043478260869565, 0.4431279620853081, 0.915, 1.0, 1.0, 0.7172617904099634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2971146294316622, 0.29711462943166195, 0.3516074373489061], 
reward next is 0.6484, 
noisyNet noise sample is [array([0.08974735], dtype=float32), 0.6577296]. 
=============================================
[2019-03-27 09:04:04,913] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143000, global step 2292265: loss 0.0015
[2019-03-27 09:04:04,915] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143000, global step 2292265: learning rate 0.0000
[2019-03-27 09:04:05,020] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143000, global step 2292317: loss 0.0016
[2019-03-27 09:04:05,021] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143000, global step 2292317: learning rate 0.0000
[2019-03-27 09:04:05,194] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143000, global step 2292398: loss 0.0013
[2019-03-27 09:04:05,196] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143000, global step 2292398: learning rate 0.0000
[2019-03-27 09:04:05,214] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143000, global step 2292406: loss 0.0014
[2019-03-27 09:04:05,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143000, global step 2292406: learning rate 0.0000
[2019-03-27 09:04:05,701] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143000, global step 2292633: loss 0.0014
[2019-03-27 09:04:05,706] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143000, global step 2292633: learning rate 0.0000
[2019-03-27 09:04:06,112] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143000, global step 2292835: loss 0.0013
[2019-03-27 09:04:06,115] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143000, global step 2292836: learning rate 0.0000
[2019-03-27 09:04:07,181] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143500, global step 2293340: loss 176.5594
[2019-03-27 09:04:07,182] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143500, global step 2293340: learning rate 0.0000
[2019-03-27 09:04:07,418] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3767275e-11 9.9747640e-01 3.3612381e-15 2.5235908e-03 1.0565337e-18], sum to 1.0000
[2019-03-27 09:04:07,427] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1480
[2019-03-27 09:04:07,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2074934.739041447 W.
[2019-03-27 09:04:07,442] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.6, 61.0, 1.0, 2.0, 0.4946550130024051, 1.0, 2.0, 0.4946550130024051, 1.0, 2.0, 0.8412069596252166, 6.9112, 6.9112, 170.5573041426782, 2074934.739041447, 2074934.739041447, 408242.2671291169], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6537600.0000, 
sim time next is 6538200.0000, 
raw observation next is [30.48333333333333, 61.66666666666667, 1.0, 2.0, 0.4585951403497804, 1.0, 2.0, 0.4585951403497804, 1.0, 2.0, 0.7795711967781829, 6.9112, 6.9112, 170.5573041426782, 1923538.207079008, 1923538.207079008, 384894.9747595496], 
processed observation next is [1.0, 0.6956521739130435, 0.6437598736176934, 0.6166666666666667, 1.0, 1.0, 0.34770498837322944, 1.0, 1.0, 0.34770498837322944, 1.0, 1.0, 0.7311843863148572, 0.0, 0.0, 0.8375144448122397, 0.5343161686330578, 0.5343161686330578, 0.5744701115814174], 
reward next is 0.4255, 
noisyNet noise sample is [array([-0.08664457], dtype=float32), -1.0229629]. 
=============================================
[2019-03-27 09:04:09,489] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8375386e-19 1.0000000e+00 1.1402067e-24 1.5519795e-19 1.6469557e-29], sum to 1.0000
[2019-03-27 09:04:09,495] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6206
[2019-03-27 09:04:09,498] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 80.5, 1.0, 2.0, 0.5284218663403171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738401.2619552452, 738401.2619552445, 188168.6547549789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6377400.0000, 
sim time next is 6378000.0000, 
raw observation next is [27.83333333333334, 81.0, 1.0, 2.0, 0.5292891957104711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739613.664593591, 739613.664593591, 188311.9368965194], 
processed observation next is [0.0, 0.8260869565217391, 0.5181674565560824, 0.81, 1.0, 1.0, 0.4328785490487603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2054482401648864, 0.2054482401648864, 0.28106259238286474], 
reward next is 0.7189, 
noisyNet noise sample is [array([-0.7290428], dtype=float32), 1.252504]. 
=============================================
[2019-03-27 09:04:09,506] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[74.247444]
 [74.26706 ]
 [74.288345]
 [74.28529 ]
 [74.24854 ]], R is [[74.190979  ]
 [74.16822052]
 [74.14581299]
 [74.1234436 ]
 [74.10162354]].
[2019-03-27 09:04:11,170] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144500, global step 2295338: loss -143.7588
[2019-03-27 09:04:11,174] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144500, global step 2295338: learning rate 0.0000
[2019-03-27 09:04:11,367] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143500, global step 2295426: loss 90.4770
[2019-03-27 09:04:11,369] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143500, global step 2295426: learning rate 0.0000
[2019-03-27 09:04:12,622] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144500, global step 2296008: loss 1.8609
[2019-03-27 09:04:12,626] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144500, global step 2296010: learning rate 0.0000
[2019-03-27 09:04:13,989] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143500, global step 2296644: loss 150.2484
[2019-03-27 09:04:13,992] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143500, global step 2296644: learning rate 0.0000
[2019-03-27 09:04:17,766] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144000, global step 2298557: loss 2.5941
[2019-03-27 09:04:17,771] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144000, global step 2298558: learning rate 0.0000
[2019-03-27 09:04:18,460] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143500, global step 2298881: loss 105.9528
[2019-03-27 09:04:18,461] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143500, global step 2298881: learning rate 0.0000
[2019-03-27 09:04:19,922] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143500, global step 2299569: loss 230.3115
[2019-03-27 09:04:19,923] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143500, global step 2299570: learning rate 0.0000
[2019-03-27 09:04:19,983] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143500, global step 2299593: loss 137.1897
[2019-03-27 09:04:19,986] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143500, global step 2299593: learning rate 0.0000
[2019-03-27 09:04:20,857] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 09:04:20,859] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:04:20,860] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:04:20,860] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:04:20,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:04:20,861] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:04:20,862] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:04:20,863] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:04:20,861] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:04:20,865] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:04:20,867] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:04:20,890] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run93
[2019-03-27 09:04:20,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run93
[2019-03-27 09:04:20,934] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run93
[2019-03-27 09:04:20,961] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run93
[2019-03-27 09:04:20,962] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run93
[2019-03-27 09:05:30,278] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0897115], dtype=float32), 0.04898311]
[2019-03-27 09:05:30,279] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 74.0, 1.0, 2.0, 0.5173717296712005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722954.869546444, 722954.8695464433, 186363.1535348479]
[2019-03-27 09:05:30,279] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:05:30,282] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.0557061e-18 1.0000000e+00 6.6219611e-24 4.0661114e-15 1.5618797e-27], sampled 0.6726243243430547
[2019-03-27 09:05:38,686] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0897115], dtype=float32), 0.04898311]
[2019-03-27 09:05:38,687] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.1, 78.0, 1.0, 2.0, 0.8321405498318927, 1.0, 2.0, 0.8321405498318927, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 172.86236624, 2327286.528976803, 2327286.528976804, 436305.5611697389]
[2019-03-27 09:05:38,688] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:05:38,691] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.8442908e-14 1.0000000e+00 5.1186489e-19 1.9128833e-10 1.2602820e-21], sampled 0.32992811834676705
[2019-03-27 09:05:38,691] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2327286.528976803 W.
[2019-03-27 09:05:40,059] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0897115], dtype=float32), 0.04898311]
[2019-03-27 09:05:40,061] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.33333333333334, 60.66666666666666, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 8.085974487966848, 6.9112, 170.5573041426782, 3751848.879103872, 2910310.126602346, 546722.1174133196]
[2019-03-27 09:05:40,062] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:05:40,065] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.2987005e-10 1.6996015e-02 4.5697788e-13 9.8300397e-01 4.7848951e-17], sampled 0.964568285999077
[2019-03-27 09:05:40,067] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 0, 0, 1, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3751848.879103872 W.
[2019-03-27 09:05:42,162] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0897115], dtype=float32), 0.04898311]
[2019-03-27 09:05:42,166] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.78333333333333, 91.83333333333333, 1.0, 2.0, 0.6261777719606247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 875058.7748724855, 875058.7748724855, 205723.8766774568]
[2019-03-27 09:05:42,167] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:05:42,170] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.6622385e-19 1.0000000e+00 1.1971340e-25 7.3279641e-18 1.1127087e-28], sampled 0.6934673320744176
[2019-03-27 09:06:16,126] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 09:06:16,268] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.2371 2842352627.0384 1129.0000
[2019-03-27 09:06:16,475] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1803 3007736924.2222 1766.0000
[2019-03-27 09:06:16,597] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.0853 3163908455.3641 1773.0000
[2019-03-27 09:06:16,633] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6185 2779263580.8788 933.0000
[2019-03-27 09:06:17,649] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2300000, evaluation results [2300000.0, 7886.085287547239, 3163908455.364106, 1773.0, 8255.065342017213, 2927317329.746172, 1338.0, 8660.61847634223, 2779263580.878825, 933.0, 7998.180259296095, 3007736924.2222114, 1766.0, 8497.237117740022, 2842352627.0384016, 1129.0]
[2019-03-27 09:06:17,672] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143500, global step 2300013: loss 265.3642
[2019-03-27 09:06:17,681] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143500, global step 2300015: learning rate 0.0000
[2019-03-27 09:06:18,193] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143500, global step 2300246: loss 327.1627
[2019-03-27 09:06:18,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143500, global step 2300249: learning rate 0.0000
[2019-03-27 09:06:18,358] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143500, global step 2300325: loss 221.4998
[2019-03-27 09:06:18,360] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143500, global step 2300325: learning rate 0.0000
[2019-03-27 09:06:18,509] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4495151e-11 9.9961519e-01 2.9694237e-15 3.8475450e-04 5.7798051e-19], sum to 1.0000
[2019-03-27 09:06:18,518] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1518
[2019-03-27 09:06:18,522] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.08333333333334, 65.0, 1.0, 2.0, 0.3913714350622452, 1.0, 1.0, 0.3913714350622452, 1.0, 1.0, 0.6747407881813341, 6.9112, 6.9112, 170.5573041426782, 1641358.153791781, 1641358.153791781, 347490.4237773959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6613800.0000, 
sim time next is 6614400.0000, 
raw observation next is [31.16666666666667, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 9.837520405507934, 6.9112, 168.8959291874857, 3530869.812966831, 1455046.676214867, 306836.2144988256], 
processed observation next is [1.0, 0.5652173913043478, 0.6761453396524489, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.29263204055079345, 0.0, 0.8293563332014992, 0.9807971702685642, 0.4041796322819075, 0.4579644992519785], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.08421558], dtype=float32), 0.571462]. 
=============================================
[2019-03-27 09:06:18,565] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143500, global step 2300418: loss 126.5466
[2019-03-27 09:06:18,568] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143500, global step 2300418: learning rate 0.0000
[2019-03-27 09:06:18,578] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143500, global step 2300421: loss 123.7948
[2019-03-27 09:06:18,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143500, global step 2300421: learning rate 0.0000
[2019-03-27 09:06:19,139] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143500, global step 2300675: loss 72.1627
[2019-03-27 09:06:19,140] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143500, global step 2300675: learning rate 0.0000
[2019-03-27 09:06:19,634] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143500, global step 2300893: loss 165.1245
[2019-03-27 09:06:19,639] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143500, global step 2300894: learning rate 0.0000
[2019-03-27 09:06:20,240] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144000, global step 2301162: loss 2.7060
[2019-03-27 09:06:20,243] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144000, global step 2301164: learning rate 0.0000
[2019-03-27 09:06:21,083] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0434923e-10 9.4848800e-01 5.0436167e-14 5.1512040e-02 9.5837609e-18], sum to 1.0000
[2019-03-27 09:06:21,096] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9575
[2019-03-27 09:06:21,105] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.9, 64.0, 1.0, 2.0, 0.4811418315863739, 1.0, 2.0, 0.4811418315863739, 1.0, 2.0, 0.8167051864209228, 6.911199999999999, 6.9112, 170.5573041426782, 2018197.456317983, 2018197.456317983, 399034.5609049259], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6708600.0000, 
sim time next is 6709200.0000, 
raw observation next is [29.86666666666667, 64.33333333333333, 1.0, 2.0, 0.7246234631797361, 1.0, 2.0, 0.7246234631797361, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2026344.679633987, 2026344.679633987, 384833.8853790989], 
processed observation next is [1.0, 0.6521739130434783, 0.6145339652448659, 0.6433333333333333, 1.0, 1.0, 0.6682210399755857, 1.0, 1.0, 0.6682210399755857, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.562873522120552, 0.562873522120552, 0.5743789334016401], 
reward next is 0.4256, 
noisyNet noise sample is [array([1.8405632], dtype=float32), -0.47107974]. 
=============================================
[2019-03-27 09:06:24,408] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145000, global step 2303007: loss 0.0029
[2019-03-27 09:06:24,409] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145000, global step 2303007: learning rate 0.0000
[2019-03-27 09:06:24,467] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8941199e-18 1.0000000e+00 1.2258546e-26 1.1137069e-20 3.0032118e-29], sum to 1.0000
[2019-03-27 09:06:24,477] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6585
[2019-03-27 09:06:24,481] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 78.66666666666667, 1.0, 2.0, 0.3776137079235777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570824.3978629658, 570824.3978629658, 172123.2898209765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6853200.0000, 
sim time next is 6853800.0000, 
raw observation next is [24.8, 78.33333333333333, 1.0, 2.0, 0.3801581559053236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573762.7834650266, 573762.7834650266, 172355.1561369583], 
processed observation next is [0.0, 0.30434782608695654, 0.3744075829383887, 0.7833333333333333, 1.0, 1.0, 0.2532025974762935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15937855096250741, 0.15937855096250741, 0.2572465016969527], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.28253138], dtype=float32), 0.10850969]. 
=============================================
[2019-03-27 09:06:25,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.3393951e-21 1.0000000e+00 3.5576552e-27 8.1963156e-20 2.8818656e-29], sum to 1.0000
[2019-03-27 09:06:25,058] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9463
[2019-03-27 09:06:25,063] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.45, 79.33333333333334, 1.0, 2.0, 0.415919574448055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610821.5112986965, 610821.5112986965, 175269.8816157605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6909000.0000, 
sim time next is 6909600.0000, 
raw observation next is [25.4, 79.66666666666667, 1.0, 2.0, 0.4163573938097708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611451.8569809191, 611451.8569809191, 175329.2391634624], 
processed observation next is [0.0, 1.0, 0.4028436018957346, 0.7966666666666667, 1.0, 1.0, 0.2968161371202058, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16984773805025533, 0.16984773805025533, 0.2616854315872573], 
reward next is 0.7383, 
noisyNet noise sample is [array([-0.8139543], dtype=float32), -2.0576925]. 
=============================================
[2019-03-27 09:06:25,164] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144000, global step 2303336: loss 2.7379
[2019-03-27 09:06:25,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144000, global step 2303336: learning rate 0.0000
[2019-03-27 09:06:25,937] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145000, global step 2303697: loss 0.0051
[2019-03-27 09:06:25,939] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145000, global step 2303698: learning rate 0.0000
[2019-03-27 09:06:27,822] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144000, global step 2304580: loss 2.8556
[2019-03-27 09:06:27,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144000, global step 2304580: learning rate 0.0000
[2019-03-27 09:06:28,315] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.6918921e-17 1.0000000e+00 1.7026938e-22 1.2038680e-14 4.8891183e-25], sum to 1.0000
[2019-03-27 09:06:28,323] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0652
[2019-03-27 09:06:28,328] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 62.66666666666667, 1.0, 2.0, 0.892804598005513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1391396.86081757, 1391396.86081757, 287437.4119942768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6774000.0000, 
sim time next is 6774600.0000, 
raw observation next is [26.36666666666667, 61.33333333333333, 1.0, 2.0, 0.891837052721122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1390055.033050526, 1390055.033050526, 287160.9570079824], 
processed observation next is [1.0, 0.391304347826087, 0.4486571879936811, 0.6133333333333333, 1.0, 1.0, 0.8696831960495446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.38612639806959054, 0.38612639806959054, 0.4285984432954961], 
reward next is 0.5714, 
noisyNet noise sample is [array([0.72915816], dtype=float32), 0.42278373]. 
=============================================
[2019-03-27 09:06:31,881] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144500, global step 2306622: loss -35.8136
[2019-03-27 09:06:31,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144500, global step 2306623: learning rate 0.0000
[2019-03-27 09:06:31,897] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5214460e-21 1.0000000e+00 1.7337372e-27 9.8075247e-22 4.0302537e-30], sum to 1.0000
[2019-03-27 09:06:31,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2200
[2019-03-27 09:06:31,910] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.86666666666667, 31.16666666666666, 1.0, 2.0, 0.2530402991085502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 416885.9759888996, 416885.9759889003, 161192.1970138435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6876600.0000, 
sim time next is 6877200.0000, 
raw observation next is [29.83333333333333, 32.33333333333334, 1.0, 2.0, 0.2543191272974628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 417827.8009668326, 417827.800966832, 161338.7275149887], 
processed observation next is [0.0, 0.6086956521739131, 0.6129541864139019, 0.3233333333333334, 1.0, 1.0, 0.10158930999694314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1160632780463424, 0.11606327804634221, 0.2408040709178936], 
reward next is 0.7592, 
noisyNet noise sample is [array([0.12396135], dtype=float32), 0.053161435]. 
=============================================
[2019-03-27 09:06:32,463] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144000, global step 2306900: loss 3.0259
[2019-03-27 09:06:32,467] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144000, global step 2306901: learning rate 0.0000
[2019-03-27 09:06:32,725] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9952065e-20 1.0000000e+00 1.9703831e-27 1.2964540e-21 3.3376817e-30], sum to 1.0000
[2019-03-27 09:06:32,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3972
[2019-03-27 09:06:32,740] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.13333333333333, 55.66666666666667, 1.0, 2.0, 0.3564536971821201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546862.1566660238, 546862.1566660238, 170319.1914857418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6891600.0000, 
sim time next is 6892200.0000, 
raw observation next is [28.0, 56.5, 1.0, 2.0, 0.3590881788372061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 550403.7658441856, 550403.765844185, 170601.182203602], 
processed observation next is [0.0, 0.782608695652174, 0.5260663507109005, 0.565, 1.0, 1.0, 0.2278170829363929, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1528899349567182, 0.15288993495671807, 0.25462863015462983], 
reward next is 0.7454, 
noisyNet noise sample is [array([-1.489331], dtype=float32), 0.08562577]. 
=============================================
[2019-03-27 09:06:33,738] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144000, global step 2307545: loss 2.9784
[2019-03-27 09:06:33,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144000, global step 2307547: learning rate 0.0000
[2019-03-27 09:06:33,746] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144000, global step 2307548: loss 2.9913
[2019-03-27 09:06:33,747] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144000, global step 2307548: learning rate 0.0000
[2019-03-27 09:06:34,541] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144000, global step 2308002: loss 2.9486
[2019-03-27 09:06:34,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144000, global step 2308003: learning rate 0.0000
[2019-03-27 09:06:34,975] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144000, global step 2308212: loss 2.9571
[2019-03-27 09:06:34,978] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144000, global step 2308212: learning rate 0.0000
[2019-03-27 09:06:35,232] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144000, global step 2308330: loss 2.8312
[2019-03-27 09:06:35,235] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144000, global step 2308330: learning rate 0.0000
[2019-03-27 09:06:35,386] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144000, global step 2308417: loss 2.8169
[2019-03-27 09:06:35,388] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144000, global step 2308417: learning rate 0.0000
[2019-03-27 09:06:35,424] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144000, global step 2308437: loss 2.8553
[2019-03-27 09:06:35,428] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144000, global step 2308438: learning rate 0.0000
[2019-03-27 09:06:35,943] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144000, global step 2308744: loss 2.7836
[2019-03-27 09:06:35,946] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144000, global step 2308744: learning rate 0.0000
[2019-03-27 09:06:36,282] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144000, global step 2308951: loss 2.8225
[2019-03-27 09:06:36,285] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144000, global step 2308952: learning rate 0.0000
[2019-03-27 09:06:36,952] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144500, global step 2309283: loss 39.0877
[2019-03-27 09:06:36,954] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144500, global step 2309283: learning rate 0.0000
[2019-03-27 09:06:37,557] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3638137e-20 1.0000000e+00 1.0803535e-25 2.4373071e-19 4.6225116e-28], sum to 1.0000
[2019-03-27 09:06:37,568] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1532
[2019-03-27 09:06:37,574] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 77.0, 1.0, 2.0, 0.4275852769461792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 621469.6596054427, 621469.6596054421, 176109.5352132057], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6937200.0000, 
sim time next is 6937800.0000, 
raw observation next is [26.3, 75.83333333333334, 1.0, 2.0, 0.4289399245452645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622872.2189962886, 622872.218996288, 176230.1622233329], 
processed observation next is [0.0, 0.30434782608695654, 0.4454976303317536, 0.7583333333333334, 1.0, 1.0, 0.31197581270513797, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1730200608323024, 0.17302006083230223, 0.26303009287064616], 
reward next is 0.7370, 
noisyNet noise sample is [array([1.5187389], dtype=float32), 0.015089345]. 
=============================================
[2019-03-27 09:06:39,718] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.7535704e-17 1.0000000e+00 1.6707665e-23 8.2055925e-16 8.9224119e-26], sum to 1.0000
[2019-03-27 09:06:39,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8884
[2019-03-27 09:06:39,730] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 81.0, 1.0, 2.0, 0.580456258075747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 829709.4124886855, 829709.4124886855, 199544.0875822252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7020000.0000, 
sim time next is 7020600.0000, 
raw observation next is [26.05, 80.0, 1.0, 2.0, 0.6126808539520981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876006.4563941075, 876006.4563941075, 205693.3901600246], 
processed observation next is [1.0, 0.2608695652173913, 0.43364928909952616, 0.8, 1.0, 1.0, 0.533350426448311, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.243335126776141, 0.243335126776141, 0.3070050599403352], 
reward next is 0.6930, 
noisyNet noise sample is [array([0.33249572], dtype=float32), -2.772257]. 
=============================================
[2019-03-27 09:06:40,300] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145500, global step 2311169: loss 99.6136
[2019-03-27 09:06:40,301] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145500, global step 2311169: learning rate 0.0000
[2019-03-27 09:06:40,562] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5928537e-09 4.1980034e-01 6.3005855e-14 5.8019966e-01 1.3994177e-16], sum to 1.0000
[2019-03-27 09:06:40,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1595
[2019-03-27 09:06:40,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2105433.657678193 W.
[2019-03-27 09:06:40,583] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.06666666666667, 71.33333333333334, 1.0, 2.0, 0.7528780176671517, 1.0, 2.0, 0.7528780176671517, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2105433.657678193, 2105433.657678193, 397581.9660388918], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7816800.0000, 
sim time next is 7817400.0000, 
raw observation next is [30.3, 71.0, 1.0, 2.0, 0.9279305002347475, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.995237126304629, 6.9112, 168.912456810683, 2194131.772890856, 2134513.119527673, 442326.1278331828], 
processed observation next is [1.0, 0.4782608695652174, 0.6350710900473934, 0.71, 1.0, 1.0, 0.9131692773912621, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008403712630462934, 0.0, 0.8294374913977766, 0.6094810480252378, 0.5929203109799092, 0.6601882504972878], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.76744837], dtype=float32), -0.5498833]. 
=============================================
[2019-03-27 09:06:40,871] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144500, global step 2311507: loss -18.7564
[2019-03-27 09:06:40,875] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144500, global step 2311507: learning rate 0.0000
[2019-03-27 09:06:41,486] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145500, global step 2311793: loss -0.0118
[2019-03-27 09:06:41,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145500, global step 2311793: learning rate 0.0000
[2019-03-27 09:06:43,339] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144500, global step 2312657: loss -47.5278
[2019-03-27 09:06:43,342] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144500, global step 2312658: learning rate 0.0000
[2019-03-27 09:06:46,189] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.44138045e-18 1.00000000e+00 2.96716678e-24 7.99346422e-16
 1.08399395e-26], sum to 1.0000
[2019-03-27 09:06:46,193] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1155
[2019-03-27 09:06:46,199] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 88.0, 1.0, 2.0, 0.2823665952888118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457425.8603242738, 457425.8603242744, 164084.863050205], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7406400.0000, 
sim time next is 7407000.0000, 
raw observation next is [20.6, 87.5, 1.0, 2.0, 0.2810551699175988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 455297.3336452136, 455297.3336452136, 163942.1032784081], 
processed observation next is [1.0, 0.7391304347826086, 0.17535545023696694, 0.875, 1.0, 1.0, 0.1338014095392757, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12647148156811489, 0.12647148156811489, 0.24468970638568371], 
reward next is 0.7553, 
noisyNet noise sample is [array([0.9949957], dtype=float32), -0.90407914]. 
=============================================
[2019-03-27 09:06:46,207] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.7189  ]
 [69.936485]
 [68.909546]
 [68.70067 ]
 [68.63165 ]], R is [[71.32011414]
 [71.36200714]
 [71.39629364]
 [71.38777924]
 [71.37182617]].
[2019-03-27 09:06:46,251] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145000, global step 2314375: loss 0.0098
[2019-03-27 09:06:46,252] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145000, global step 2314376: learning rate 0.0000
[2019-03-27 09:06:47,242] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144500, global step 2315048: loss 127.1454
[2019-03-27 09:06:47,245] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144500, global step 2315048: learning rate 0.0000
[2019-03-27 09:06:48,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:06:48,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:06:48,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-03-27 09:06:48,245] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144500, global step 2315525: loss -58.0445
[2019-03-27 09:06:48,249] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144500, global step 2315526: learning rate 0.0000
[2019-03-27 09:06:48,401] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144500, global step 2315615: loss 95.5879
[2019-03-27 09:06:48,402] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144500, global step 2315615: learning rate 0.0000
[2019-03-27 09:06:49,011] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144500, global step 2315973: loss 44.9076
[2019-03-27 09:06:49,013] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144500, global step 2315973: learning rate 0.0000
[2019-03-27 09:06:49,152] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.3525591e-19 1.0000000e+00 2.8252055e-24 2.9348710e-16 2.3585997e-27], sum to 1.0000
[2019-03-27 09:06:49,156] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3097
[2019-03-27 09:06:49,159] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 89.33333333333334, 1.0, 2.0, 0.5550310196432704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775597.6895465234, 775597.6895465241, 192663.0975303033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7183200.0000, 
sim time next is 7183800.0000, 
raw observation next is [25.8, 89.5, 1.0, 2.0, 0.5770454712803359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 806372.2559995108, 806372.2559995108, 196541.5839879837], 
processed observation next is [1.0, 0.13043478260869565, 0.42180094786729866, 0.895, 1.0, 1.0, 0.490416230458236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22399229333319742, 0.22399229333319742, 0.2933456477432593], 
reward next is 0.7067, 
noisyNet noise sample is [array([0.8269109], dtype=float32), -0.17769633]. 
=============================================
[2019-03-27 09:06:49,223] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:06:49,223] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:06:49,267] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144500, global step 2316130: loss 44.7580
[2019-03-27 09:06:49,269] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144500, global step 2316130: learning rate 0.0000
[2019-03-27 09:06:49,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run12
[2019-03-27 09:06:49,366] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144500, global step 2316172: loss -39.4670
[2019-03-27 09:06:49,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144500, global step 2316172: learning rate 0.0000
[2019-03-27 09:06:49,605] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144500, global step 2316316: loss 94.5640
[2019-03-27 09:06:49,609] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144500, global step 2316316: learning rate 0.0000
[2019-03-27 09:06:49,616] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144500, global step 2316323: loss 112.9614
[2019-03-27 09:06:49,618] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144500, global step 2316324: learning rate 0.0000
[2019-03-27 09:06:49,901] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144500, global step 2316487: loss 94.5722
[2019-03-27 09:06:49,906] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144500, global step 2316488: learning rate 0.0000
[2019-03-27 09:06:50,414] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144500, global step 2316793: loss -75.3772
[2019-03-27 09:06:50,416] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144500, global step 2316793: learning rate 0.0000
[2019-03-27 09:06:50,563] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145000, global step 2316873: loss 0.0187
[2019-03-27 09:06:50,565] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145000, global step 2316873: learning rate 0.0000
[2019-03-27 09:06:55,395] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145000, global step 2319130: loss 0.0258
[2019-03-27 09:06:55,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145000, global step 2319130: learning rate 0.0000
[2019-03-27 09:06:57,645] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4723792e-20 1.0000000e+00 8.2043260e-27 4.2327535e-21 8.9501087e-30], sum to 1.0000
[2019-03-27 09:06:57,649] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6016
[2019-03-27 09:06:57,656] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 82.33333333333334, 1.0, 2.0, 0.4333419437761548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 621602.6372978551, 621602.6372978558, 175889.6279018857], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7550400.0000, 
sim time next is 7551000.0000, 
raw observation next is [25.9, 81.5, 1.0, 2.0, 0.4383957447182011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 626024.6875124621, 626024.6875124628, 176245.690836109], 
processed observation next is [0.0, 0.391304347826087, 0.42654028436018954, 0.815, 1.0, 1.0, 0.3233683671303627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1738957465312395, 0.17389574653123968, 0.2630532699046403], 
reward next is 0.7369, 
noisyNet noise sample is [array([-0.4746191], dtype=float32), -2.050215]. 
=============================================
[2019-03-27 09:06:57,687] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[76.53708 ]
 [76.581245]
 [76.60134 ]
 [76.59426 ]
 [76.64271 ]], R is [[76.46824646]
 [76.44104767]
 [76.41462708]
 [76.38894653]
 [76.36391449]].
[2019-03-27 09:06:58,077] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145000, global step 2320383: loss 0.0209
[2019-03-27 09:06:58,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145000, global step 2320383: learning rate 0.0000
[2019-03-27 09:07:02,482] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145500, global step 2322444: loss -3.6824
[2019-03-27 09:07:02,484] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145500, global step 2322444: learning rate 0.0000
[2019-03-27 09:07:03,231] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145000, global step 2322793: loss 0.0130
[2019-03-27 09:07:03,235] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145000, global step 2322793: learning rate 0.0000
[2019-03-27 09:07:04,387] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145000, global step 2323335: loss 0.0066
[2019-03-27 09:07:04,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145000, global step 2323337: learning rate 0.0000
[2019-03-27 09:07:04,607] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5144318e-20 1.0000000e+00 4.8373332e-28 5.6614366e-21 1.1512916e-30], sum to 1.0000
[2019-03-27 09:07:04,614] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9458
[2019-03-27 09:07:04,617] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145000, global step 2323441: loss 0.0091
[2019-03-27 09:07:04,620] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 90.66666666666667, 1.0, 2.0, 0.2943811953996872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471934.3171387933, 471934.3171387933, 165087.7251199895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 250800.0000, 
sim time next is 251400.0000, 
raw observation next is [20.73333333333333, 90.83333333333334, 1.0, 2.0, 0.293988935910098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471426.8767904231, 471426.8767904231, 165053.0886931898], 
processed observation next is [0.0, 0.9130434782608695, 0.18167456556082143, 0.9083333333333334, 1.0, 1.0, 0.1493842601326482, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13095191021956198, 0.13095191021956198, 0.2463478935719251], 
reward next is 0.7537, 
noisyNet noise sample is [array([1.7544694], dtype=float32), 0.6194962]. 
=============================================
[2019-03-27 09:07:04,622] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145000, global step 2323441: learning rate 0.0000
[2019-03-27 09:07:05,370] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145000, global step 2323796: loss 0.0082
[2019-03-27 09:07:05,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145000, global step 2323796: learning rate 0.0000
[2019-03-27 09:07:05,988] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145000, global step 2324046: loss 0.0068
[2019-03-27 09:07:05,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145000, global step 2324046: learning rate 0.0000
[2019-03-27 09:07:06,092] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145000, global step 2324099: loss 0.0075
[2019-03-27 09:07:06,093] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145000, global step 2324099: learning rate 0.0000
[2019-03-27 09:07:06,262] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145000, global step 2324175: loss 0.0076
[2019-03-27 09:07:06,266] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145000, global step 2324177: learning rate 0.0000
[2019-03-27 09:07:06,384] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145000, global step 2324234: loss 0.0059
[2019-03-27 09:07:06,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145000, global step 2324234: learning rate 0.0000
[2019-03-27 09:07:06,870] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145000, global step 2324461: loss 0.0046
[2019-03-27 09:07:06,872] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145000, global step 2324462: learning rate 0.0000
[2019-03-27 09:07:07,488] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2498560e-21 1.0000000e+00 6.3513516e-29 3.8137893e-22 1.2935871e-31], sum to 1.0000
[2019-03-27 09:07:07,500] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7687
[2019-03-27 09:07:07,504] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 86.0, 1.0, 2.0, 0.286087918019572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459034.5913646868, 459034.5913646868, 164199.5907626482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 331200.0000, 
sim time next is 331800.0000, 
raw observation next is [21.26666666666667, 86.0, 1.0, 2.0, 0.2851089079714543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457733.6440263194, 457733.64402632, 164113.0896229131], 
processed observation next is [0.0, 0.8695652173913043, 0.2069510268562403, 0.86, 1.0, 1.0, 0.13868543129090877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12714823445175538, 0.12714823445175555, 0.24494490988494494], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.06057378], dtype=float32), 1.2637974]. 
=============================================
[2019-03-27 09:07:07,641] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145000, global step 2324826: loss 0.0079
[2019-03-27 09:07:07,648] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145000, global step 2324827: learning rate 0.0000
[2019-03-27 09:07:07,893] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.5620460e-21 1.0000000e+00 3.5075648e-28 4.7432475e-20 4.7393529e-30], sum to 1.0000
[2019-03-27 09:07:07,903] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3649
[2019-03-27 09:07:07,908] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 92.66666666666667, 1.0, 2.0, 0.4066744334745181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597597.7098401077, 597597.7098401077, 174041.3197998646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7515600.0000, 
sim time next is 7516200.0000, 
raw observation next is [23.6, 92.83333333333333, 1.0, 2.0, 0.4075648238224539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598425.8607098386, 598425.860709838, 174103.2615890897], 
processed observation next is [0.0, 1.0, 0.3175355450236968, 0.9283333333333332, 1.0, 1.0, 0.28622267930416134, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16622940575273293, 0.16622940575273276, 0.25985561431207416], 
reward next is 0.7401, 
noisyNet noise sample is [array([-0.20003755], dtype=float32), -1.7189419]. 
=============================================
[2019-03-27 09:07:08,026] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 09:07:08,028] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:07:08,028] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:07:08,029] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:07:08,030] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:07:08,032] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:07:08,029] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:07:08,033] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:07:08,034] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:07:08,034] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:07:08,035] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:07:08,067] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run94
[2019-03-27 09:07:08,091] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run94
[2019-03-27 09:07:08,125] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run94
[2019-03-27 09:07:08,125] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run94
[2019-03-27 09:07:08,126] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run94
[2019-03-27 09:07:22,700] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09050963], dtype=float32), 0.04935605]
[2019-03-27 09:07:22,702] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.06666666666667, 79.66666666666667, 1.0, 2.0, 0.3149469962394668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506127.721767401, 506127.7217674004, 167559.4655563725]
[2019-03-27 09:07:22,703] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:07:22,706] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7742390e-20 1.0000000e+00 5.4729995e-27 3.6889594e-21 1.9198389e-29], sampled 0.23166160182636786
[2019-03-27 09:07:42,614] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09050963], dtype=float32), 0.04935605]
[2019-03-27 09:07:42,616] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.88948762, 68.84951403333332, 1.0, 2.0, 0.5695396857999208, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9694714058692874, 6.911199999999999, 6.9112, 168.9129390540196, 1592356.047868269, 1592356.04786827, 344279.7826058371]
[2019-03-27 09:07:42,619] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:07:42,622] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5219315e-13 1.0000000e+00 5.5516252e-18 2.7271511e-08 9.3202495e-22], sampled 0.3992645934288629
[2019-03-27 09:07:46,837] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09050963], dtype=float32), 0.04935605]
[2019-03-27 09:07:46,851] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.05, 75.5, 1.0, 2.0, 0.5169776376342268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722403.9936076605, 722403.9936076599, 186297.8087675325]
[2019-03-27 09:07:46,852] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:07:46,855] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0269388e-20 1.0000000e+00 9.8789929e-27 1.1056561e-19 1.0972430e-29], sampled 0.562030317651437
[2019-03-27 09:07:52,151] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09050963], dtype=float32), 0.04935605]
[2019-03-27 09:07:52,153] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.1, 89.0, 1.0, 2.0, 0.5150687696933378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719735.7137975541, 719735.7137975547, 185989.9926691432]
[2019-03-27 09:07:52,154] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:07:52,156] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9816980e-19 1.0000000e+00 6.1679148e-26 1.1279887e-18 5.4470005e-29], sampled 0.05436932332539457
[2019-03-27 09:08:44,986] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09050963], dtype=float32), 0.04935605]
[2019-03-27 09:08:44,988] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.76666666666667, 92.66666666666667, 1.0, 2.0, 0.5516146446212815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770821.9312190055, 770821.9312190055, 192075.0497460303]
[2019-03-27 09:08:44,989] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:08:44,993] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7232261e-19 1.0000000e+00 1.4625074e-25 1.0085531e-18 3.4472622e-28], sampled 0.27831580795798894
[2019-03-27 09:09:01,458] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09050963], dtype=float32), 0.04935605]
[2019-03-27 09:09:01,459] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.48333333333333, 61.83333333333334, 1.0, 2.0, 0.3206721525030944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508560.6693830197, 508560.6693830197, 167695.2509674311]
[2019-03-27 09:09:01,460] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:09:01,465] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1936174e-19 1.0000000e+00 2.2020501e-26 8.1065445e-20 4.0342704e-29], sampled 0.3054317018665199
[2019-03-27 09:09:02,876] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09050963], dtype=float32), 0.04935605]
[2019-03-27 09:09:02,877] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.46666666666667, 70.33333333333333, 1.0, 2.0, 0.3274287213617471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 515076.5831913422, 515076.5831913415, 168117.6013359843]
[2019-03-27 09:09:02,880] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:09:02,885] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0582876e-20 1.0000000e+00 9.9379572e-27 7.8538020e-21 3.2362650e-29], sampled 0.9044784815908922
[2019-03-27 09:09:03,482] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.6789 3007576135.5823 1765.0000
[2019-03-27 09:09:03,649] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2106 2779290265.5642 933.0000
[2019-03-27 09:09:03,681] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 09:09:03,755] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-03-27 09:09:04,014] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.3012 3164007742.7704 1777.0000
[2019-03-27 09:09:05,032] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2325000, evaluation results [2325000.0, 7885.301242010743, 3164007742.7704353, 1777.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8659.21064550443, 2779290265.564204, 933.0, 7998.678862721622, 3007576135.5823054, 1765.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 09:09:05,162] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145500, global step 2325068: loss -132.7002
[2019-03-27 09:09:05,172] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145500, global step 2325068: learning rate 0.0000
[2019-03-27 09:09:08,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:08,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:08,929] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run12
[2019-03-27 09:09:09,634] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9788797e-18 1.0000000e+00 2.1051684e-24 2.3036340e-15 4.2489159e-28], sum to 1.0000
[2019-03-27 09:09:09,643] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3992
[2019-03-27 09:09:09,647] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 81.0, 1.0, 2.0, 0.5111447883724254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714250.6617795123, 714250.6617795129, 185361.1814248074], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7759800.0000, 
sim time next is 7760400.0000, 
raw observation next is [27.53333333333333, 82.33333333333333, 1.0, 2.0, 0.5133104802856304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717277.9236559131, 717277.9236559131, 185708.3142314998], 
processed observation next is [1.0, 0.8260869565217391, 0.5039494470774091, 0.8233333333333333, 1.0, 1.0, 0.4136270846814824, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1992438676821981, 0.1992438676821981, 0.2771765884052236], 
reward next is 0.7228, 
noisyNet noise sample is [array([-0.5339161], dtype=float32), -0.49234563]. 
=============================================
[2019-03-27 09:09:09,918] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145500, global step 2327245: loss 109.0790
[2019-03-27 09:09:09,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145500, global step 2327246: learning rate 0.0000
[2019-03-27 09:09:12,484] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145500, global step 2328421: loss -84.6926
[2019-03-27 09:09:12,487] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145500, global step 2328422: learning rate 0.0000
[2019-03-27 09:09:12,878] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.2222337e-09 9.2486840e-01 8.8630858e-14 7.5131595e-02 5.1687982e-17], sum to 1.0000
[2019-03-27 09:09:12,887] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4027
[2019-03-27 09:09:12,898] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2063110.535126957 W.
[2019-03-27 09:09:12,903] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.33333333333333, 63.0, 1.0, 2.0, 0.7377583324574822, 1.0, 2.0, 0.7377583324574822, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2063110.535126957, 2063110.535126957, 390698.6410356828], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7656600.0000, 
sim time next is 7657200.0000, 
raw observation next is [30.2, 64.0, 1.0, 2.0, 0.6212209785956618, 1.0, 2.0, 0.6212209785956618, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1736954.500810243, 1736954.500810243, 342215.3884496593], 
processed observation next is [1.0, 0.6521739130434783, 0.6303317535545023, 0.64, 1.0, 1.0, 0.5436397332477854, 1.0, 1.0, 0.5436397332477854, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4824873613361786, 0.4824873613361786, 0.5107692364920288], 
reward next is 0.4892, 
noisyNet noise sample is [array([-2.1373918], dtype=float32), 0.14527304]. 
=============================================
[2019-03-27 09:09:14,240] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:14,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:14,320] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run12
[2019-03-27 09:09:16,948] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145500, global step 2330626: loss 22.2679
[2019-03-27 09:09:16,950] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145500, global step 2330626: learning rate 0.0000
[2019-03-27 09:09:18,170] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145500, global step 2331199: loss -85.4343
[2019-03-27 09:09:18,173] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145500, global step 2331200: learning rate 0.0000
[2019-03-27 09:09:18,451] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145500, global step 2331334: loss -38.0554
[2019-03-27 09:09:18,453] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145500, global step 2331334: learning rate 0.0000
[2019-03-27 09:09:18,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:18,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:18,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run12
[2019-03-27 09:09:18,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6587615e-19 1.0000000e+00 8.2235686e-26 1.2787352e-17 1.0583052e-27], sum to 1.0000
[2019-03-27 09:09:18,840] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5124
[2019-03-27 09:09:18,846] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.56666666666667, 96.0, 1.0, 2.0, 0.3148618225490976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 500013.017037639, 500013.0170376384, 167063.0082945245], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 169800.0000, 
sim time next is 170400.0000, 
raw observation next is [20.53333333333333, 96.0, 1.0, 2.0, 0.3128135820312611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497148.8268888287, 497148.8268888287, 166856.0252595158], 
processed observation next is [1.0, 1.0, 0.17219589257503945, 0.96, 1.0, 1.0, 0.17206455666416998, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13809689635800798, 0.13809689635800798, 0.2490388436709191], 
reward next is 0.7510, 
noisyNet noise sample is [array([1.3732821], dtype=float32), -0.18789893]. 
=============================================
[2019-03-27 09:09:19,007] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145500, global step 2331599: loss 158.0244
[2019-03-27 09:09:19,010] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145500, global step 2331599: learning rate 0.0000
[2019-03-27 09:09:19,443] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145500, global step 2331853: loss 9.1860
[2019-03-27 09:09:19,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145500, global step 2331853: learning rate 0.0000
[2019-03-27 09:09:19,535] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145500, global step 2331907: loss -41.8446
[2019-03-27 09:09:19,537] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145500, global step 2331907: learning rate 0.0000
[2019-03-27 09:09:19,564] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145500, global step 2331924: loss -11.7625
[2019-03-27 09:09:19,564] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145500, global step 2331925: learning rate 0.0000
[2019-03-27 09:09:19,746] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145500, global step 2332037: loss -58.0120
[2019-03-27 09:09:19,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145500, global step 2332038: learning rate 0.0000
[2019-03-27 09:09:20,115] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145500, global step 2332222: loss 101.5977
[2019-03-27 09:09:20,120] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145500, global step 2332224: learning rate 0.0000
[2019-03-27 09:09:20,679] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:20,679] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:20,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run12
[2019-03-27 09:09:20,874] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145500, global step 2332562: loss -51.0399
[2019-03-27 09:09:20,877] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145500, global step 2332562: learning rate 0.0000
[2019-03-27 09:09:24,882] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4047283e-11 9.9982136e-01 5.5773827e-16 1.7866422e-04 8.2783337e-20], sum to 1.0000
[2019-03-27 09:09:24,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4491
[2019-03-27 09:09:24,904] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2294945.825365482 W.
[2019-03-27 09:09:24,908] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.93333333333334, 71.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.096169706042566, 6.9112, 168.9062952121816, 2294945.825365482, 1454320.684822947, 311295.0907047388], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7908000.0000, 
sim time next is 7908600.0000, 
raw observation next is [29.96666666666667, 71.16666666666667, 1.0, 2.0, 0.6732460318533, 1.0, 1.0, 0.6732460318533, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1882546.062568797, 1882546.062568797, 362881.3484336244], 
processed observation next is [1.0, 0.5217391304347826, 0.6192733017377569, 0.7116666666666667, 1.0, 1.0, 0.6063205203051807, 1.0, 0.5, 0.6063205203051807, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5229294618246658, 0.5229294618246658, 0.5416139528860066], 
reward next is 0.4584, 
noisyNet noise sample is [array([-0.7361901], dtype=float32), -1.0190059]. 
=============================================
[2019-03-27 09:09:25,217] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:25,218] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:25,269] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run12
[2019-03-27 09:09:26,377] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:26,378] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:26,428] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run12
[2019-03-27 09:09:26,649] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:26,649] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:26,692] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run12
[2019-03-27 09:09:27,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:27,010] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:27,055] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run12
[2019-03-27 09:09:27,327] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:27,327] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:27,328] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:27,329] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:27,378] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run12
[2019-03-27 09:09:27,414] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run12
[2019-03-27 09:09:27,468] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:27,468] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:27,477] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run12
[2019-03-27 09:09:27,580] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:27,580] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:27,599] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run12
[2019-03-27 09:09:27,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3870797e-20 1.0000000e+00 1.1981979e-26 1.2100120e-18 7.2659646e-28], sum to 1.0000
[2019-03-27 09:09:27,670] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3544
[2019-03-27 09:09:27,677] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.98333333333333, 74.5, 1.0, 2.0, 0.478043141022105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778574.8402033462, 778574.8402033462, 191805.6140737044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 382200.0000, 
sim time next is 382800.0000, 
raw observation next is [22.06666666666667, 74.0, 1.0, 2.0, 0.4619755834979364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752241.4872019907, 752241.4872019907, 189065.6488971295], 
processed observation next is [1.0, 0.43478260869565216, 0.2448657187993683, 0.74, 1.0, 1.0, 0.3517778114432969, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20895596866721963, 0.20895596866721963, 0.28218753566735744], 
reward next is 0.7178, 
noisyNet noise sample is [array([-1.6531088], dtype=float32), 0.06944957]. 
=============================================
[2019-03-27 09:09:27,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:27,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:27,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run12
[2019-03-27 09:09:27,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5279355e-18 1.0000000e+00 2.3283963e-24 3.3563082e-16 1.9465807e-27], sum to 1.0000
[2019-03-27 09:09:27,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7798
[2019-03-27 09:09:27,954] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 73.0, 1.0, 2.0, 0.301360185706277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 487300.2156215706, 487300.2156215701, 166165.7320035826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 391200.0000, 
sim time next is 391800.0000, 
raw observation next is [22.68333333333333, 73.0, 1.0, 2.0, 0.3120328401531444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504413.3474573876, 504413.3474573876, 167408.9133559373], 
processed observation next is [1.0, 0.5217391304347826, 0.27409162717219576, 0.73, 1.0, 1.0, 0.17112390379896916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14011481873816323, 0.14011481873816323, 0.24986404978498103], 
reward next is 0.7501, 
noisyNet noise sample is [array([0.6676557], dtype=float32), 0.54289573]. 
=============================================
[2019-03-27 09:09:28,080] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 09:09:28,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:28,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run12
[2019-03-27 09:09:28,254] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.042556e-21 1.000000e+00 5.994999e-27 4.052808e-23 4.260663e-30], sum to 1.0000
[2019-03-27 09:09:28,258] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6816
[2019-03-27 09:09:28,261] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 80.0, 1.0, 2.0, 0.2871132877756963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 460869.582396531, 460869.5823965304, 164326.3750118936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 806400.0000, 
sim time next is 807000.0000, 
raw observation next is [22.26666666666667, 78.83333333333334, 1.0, 2.0, 0.287628349372697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 461493.1833494597, 461493.183349459, 164367.5519948861], 
processed observation next is [0.0, 0.34782608695652173, 0.2543443917851502, 0.7883333333333334, 1.0, 1.0, 0.14172090285867106, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1281925509304055, 0.1281925509304053, 0.24532470446997925], 
reward next is 0.7547, 
noisyNet noise sample is [array([1.8710821], dtype=float32), 0.33737984]. 
=============================================
[2019-03-27 09:09:28,280] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.603745]
 [76.55919 ]
 [76.57318 ]
 [76.581924]
 [76.58864 ]], R is [[76.61152649]
 [76.60015106]
 [76.58906555]
 [76.57828522]
 [76.56783295]].
[2019-03-27 09:09:34,186] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6408983e-21 1.0000000e+00 2.0390333e-28 3.8909982e-22 2.9607115e-31], sum to 1.0000
[2019-03-27 09:09:34,189] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0072
[2019-03-27 09:09:34,197] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 85.33333333333334, 1.0, 2.0, 0.2900106673345639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463633.9896210026, 463633.989621002, 164497.3970264621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 286800.0000, 
sim time next is 287400.0000, 
raw observation next is [21.75, 84.66666666666666, 1.0, 2.0, 0.2924925719225698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466992.0263415549, 466992.0263415549, 164721.9358920194], 
processed observation next is [0.0, 0.30434782608695654, 0.2298578199052133, 0.8466666666666666, 1.0, 1.0, 0.14758141195490335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12972000731709857, 0.12972000731709857, 0.24585363565973042], 
reward next is 0.7541, 
noisyNet noise sample is [array([-0.5828349], dtype=float32), 0.83064747]. 
=============================================
[2019-03-27 09:09:35,000] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3263626e-19 1.0000000e+00 7.0451393e-25 5.3695652e-17 2.0046957e-29], sum to 1.0000
[2019-03-27 09:09:35,009] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6723
[2019-03-27 09:09:35,016] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 96.0, 1.0, 2.0, 0.3819850059386324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576836.0541452381, 576836.0541452381, 172637.409321775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [22.4, 96.0, 1.0, 2.0, 0.3803188587064071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 574320.0518680543, 574320.0518680537, 172414.0207110183], 
processed observation next is [1.0, 0.8695652173913043, 0.2606635071090047, 0.96, 1.0, 1.0, 0.2533962153089242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1595333477411262, 0.15953334774112604, 0.25733435927017656], 
reward next is 0.7427, 
noisyNet noise sample is [array([-0.387984], dtype=float32), 2.250449]. 
=============================================
[2019-03-27 09:09:38,655] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8918008e-20 1.0000000e+00 2.4292484e-28 1.4771218e-21 2.2841132e-29], sum to 1.0000
[2019-03-27 09:09:38,668] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6977
[2019-03-27 09:09:38,674] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 92.0, 1.0, 2.0, 0.3002782339955276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479492.7533179404, 479492.753317941, 165603.6485975491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 212400.0000, 
sim time next is 213000.0000, 
raw observation next is [20.88333333333334, 91.66666666666667, 1.0, 2.0, 0.301606091918902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481326.6202784381, 481326.6202784388, 165731.2482351321], 
processed observation next is [0.0, 0.4782608695652174, 0.18878357030015835, 0.9166666666666667, 1.0, 1.0, 0.15856155652879755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1337018389662328, 0.133701838966233, 0.2473600719927345], 
reward next is 0.7526, 
noisyNet noise sample is [array([0.6839312], dtype=float32), -0.19450577]. 
=============================================
[2019-03-27 09:09:38,686] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.822136]
 [77.756584]
 [77.75092 ]
 [77.74093 ]
 [77.736305]], R is [[77.82535553]
 [77.79993439]
 [77.77464294]
 [77.74950409]
 [77.72450256]].
[2019-03-27 09:09:39,527] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.02557982e-20 1.00000000e+00 4.07820288e-27 1.19536646e-20
 2.56591252e-30], sum to 1.0000
[2019-03-27 09:09:39,535] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8157
[2019-03-27 09:09:39,540] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.41666666666667, 93.0, 1.0, 2.0, 0.2898629806330963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465430.5006459423, 465430.5006459423, 164640.5959671809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 202200.0000, 
sim time next is 202800.0000, 
raw observation next is [20.43333333333333, 93.0, 1.0, 2.0, 0.2930741634207988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470432.0629111052, 470432.0629111046, 164986.5679789401], 
processed observation next is [0.0, 0.34782608695652173, 0.1674565560821484, 0.93, 1.0, 1.0, 0.14828212460337203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13067557303086255, 0.13067557303086239, 0.2462486089237912], 
reward next is 0.7538, 
noisyNet noise sample is [array([0.7510354], dtype=float32), 0.70422435]. 
=============================================
[2019-03-27 09:09:39,628] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.9629177e-21 1.0000000e+00 7.8848521e-26 1.3267942e-20 9.7532011e-30], sum to 1.0000
[2019-03-27 09:09:39,635] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8000
[2019-03-27 09:09:39,641] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.96666666666667, 96.0, 1.0, 2.0, 0.2910544883419042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 468160.597263258, 468160.5972632574, 164832.3844922244], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 181200.0000, 
sim time next is 181800.0000, 
raw observation next is [19.95, 96.0, 1.0, 2.0, 0.2897101704510642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466149.7125251241, 466149.7125251241, 164693.7541414512], 
processed observation next is [0.0, 0.08695652173913043, 0.14454976303317538, 0.96, 1.0, 1.0, 0.14422912102537852, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1294860312569789, 0.1294860312569789, 0.24581157334544956], 
reward next is 0.7542, 
noisyNet noise sample is [array([-2.3510864], dtype=float32), 2.061072]. 
=============================================
[2019-03-27 09:09:44,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.9525139e-19 1.0000000e+00 5.5372657e-25 4.3874772e-18 1.8784178e-27], sum to 1.0000
[2019-03-27 09:09:44,439] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7047
[2019-03-27 09:09:44,445] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.7, 93.0, 1.0, 2.0, 0.2202070177699711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 366528.4819288982, 366528.4819288982, 157696.0047098754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 698400.0000, 
sim time next is 699000.0000, 
raw observation next is [17.68333333333333, 93.0, 1.0, 2.0, 0.2643340343026741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 440045.684673672, 440045.6846736726, 161907.7451246051], 
processed observation next is [1.0, 0.08695652173913043, 0.037124802527646036, 0.93, 1.0, 1.0, 0.11365546301526999, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12223491240935334, 0.1222349124093535, 0.24165335093224644], 
reward next is 0.7583, 
noisyNet noise sample is [array([-1.6882051], dtype=float32), -0.2852413]. 
=============================================
[2019-03-27 09:09:44,460] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.41737]
 [73.34092]
 [73.34801]
 [73.46915]
 [73.47608]], R is [[73.64130402]
 [73.66952515]
 [73.69728851]
 [73.72464752]
 [73.75166321]].
[2019-03-27 09:09:45,049] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3994539e-21 1.0000000e+00 1.9691648e-28 1.5639823e-22 2.4702510e-30], sum to 1.0000
[2019-03-27 09:09:45,058] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0831
[2019-03-27 09:09:45,063] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.86666666666667, 78.66666666666667, 1.0, 2.0, 0.303940100671897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482007.6237752045, 482007.6237752045, 165730.9130084243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 295800.0000, 
sim time next is 296400.0000, 
raw observation next is [22.93333333333334, 78.33333333333334, 1.0, 2.0, 0.3050516831218206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483565.4332241096, 483565.4332241096, 165839.4096343256], 
processed observation next is [0.0, 0.43478260869565216, 0.28593996840442376, 0.7833333333333334, 1.0, 1.0, 0.16271287123110917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13432373145114154, 0.13432373145114154, 0.24752150691690386], 
reward next is 0.7525, 
noisyNet noise sample is [array([-0.07762472], dtype=float32), 1.3253556]. 
=============================================
[2019-03-27 09:09:56,128] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1217712e-16 1.0000000e+00 7.6009285e-23 3.1586242e-12 2.8596299e-25], sum to 1.0000
[2019-03-27 09:09:56,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8316
[2019-03-27 09:09:56,141] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 93.66666666666667, 1.0, 2.0, 0.7483828928246812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1132725.695171514, 1132725.695171513, 242998.8884753696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1338000.0000, 
sim time next is 1338600.0000, 
raw observation next is [22.48333333333333, 93.33333333333333, 1.0, 2.0, 0.7187401081279726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1092218.434412109, 1092218.434412109, 236227.0199749477], 
processed observation next is [1.0, 0.4782608695652174, 0.26461295418641384, 0.9333333333333332, 1.0, 1.0, 0.6611326603951477, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30339400955891915, 0.30339400955891915, 0.3525776417536533], 
reward next is 0.6474, 
noisyNet noise sample is [array([0.6215635], dtype=float32), 0.27805278]. 
=============================================
[2019-03-27 09:09:57,001] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 09:09:57,003] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:09:57,003] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:57,003] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:09:57,004] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:09:57,005] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:09:57,006] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:09:57,005] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:57,007] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:57,007] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:57,008] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:09:57,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run95
[2019-03-27 09:09:57,057] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run95
[2019-03-27 09:09:57,058] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run95
[2019-03-27 09:09:57,102] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run95
[2019-03-27 09:09:57,103] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run95
[2019-03-27 09:10:05,445] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0914976], dtype=float32), 0.05023826]
[2019-03-27 09:10:05,466] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.33772216333333, 67.97927718666666, 1.0, 2.0, 0.289527848048624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479911.3591907573, 479911.3591907579, 164909.0922686436]
[2019-03-27 09:10:05,467] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:10:05,469] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.1369761e-20 1.0000000e+00 3.8181758e-27 9.0220323e-21 1.2239416e-29], sampled 0.26739584540836103
[2019-03-27 09:10:09,338] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0914976], dtype=float32), 0.05023826]
[2019-03-27 09:10:09,339] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.95, 91.66666666666667, 1.0, 2.0, 0.4345367053114944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633853.5197102248, 633853.5197102248, 177381.9318182447]
[2019-03-27 09:10:09,341] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:10:09,345] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.5940195e-20 1.0000000e+00 8.9311249e-27 2.8673053e-20 1.9156544e-29], sampled 0.5263554036103144
[2019-03-27 09:10:34,612] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0914976], dtype=float32), 0.05023826]
[2019-03-27 09:10:34,613] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.6, 79.66666666666667, 1.0, 2.0, 0.5319290492692991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 743303.811515374, 743303.8115153746, 188750.7687660515]
[2019-03-27 09:10:34,613] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:10:34,616] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7091081e-17 1.0000000e+00 6.3211090e-23 3.3019852e-13 2.7819629e-27], sampled 0.20783498802640932
[2019-03-27 09:10:49,544] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0914976], dtype=float32), 0.05023826]
[2019-03-27 09:10:49,545] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.7, 63.0, 1.0, 2.0, 0.6857766294551695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958383.409673713, 958383.409673713, 217808.5364077304]
[2019-03-27 09:10:49,546] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:10:49,547] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.5336789e-17 1.0000000e+00 3.0769980e-23 2.0145581e-14 3.1456516e-26], sampled 0.2941486058591547
[2019-03-27 09:10:51,846] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0914976], dtype=float32), 0.05023826]
[2019-03-27 09:10:51,847] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.84791878833333, 59.13494659666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.051638964804582, 6.9112, 168.9121840575168, 1553454.938237976, 1453823.158695918, 311354.4956189771]
[2019-03-27 09:10:51,850] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:10:51,854] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.2750827e-17 1.0000000e+00 1.8795829e-23 2.5223776e-14 1.6950949e-26], sampled 0.21206323792539927
[2019-03-27 09:10:54,215] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0914976], dtype=float32), 0.05023826]
[2019-03-27 09:10:54,216] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.06666666666667, 71.0, 1.0, 2.0, 0.5502354155651639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768893.9099033487, 768893.9099033494, 191840.6570802042]
[2019-03-27 09:10:54,219] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:10:54,226] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.935150e-20 1.000000e+00 1.294958e-26 7.958280e-19 1.087459e-29], sampled 0.7366995775266569
[2019-03-27 09:11:42,595] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0914976], dtype=float32), 0.05023826]
[2019-03-27 09:11:42,596] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.78333333333333, 82.16666666666667, 1.0, 2.0, 0.7923659102518845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1107421.242851659, 1107421.24285166, 242047.6654163409]
[2019-03-27 09:11:42,596] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:11:42,598] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.602086e-18 1.000000e+00 3.557742e-24 4.631312e-16 6.061988e-27], sampled 0.7084818195876901
[2019-03-27 09:11:52,380] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.6723 3007571258.7513 1765.0000
[2019-03-27 09:11:52,394] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 09:11:52,752] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.4121 3163962175.7704 1776.0000
[2019-03-27 09:11:52,914] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-03-27 09:11:52,950] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.2379 2842306803.2777 1130.0000
[2019-03-27 09:11:53,966] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2350000, evaluation results [2350000.0, 7885.412143441598, 3163962175.7703886, 1776.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.672303914817, 3007571258.7513285, 1765.0, 8495.237943802424, 2842306803.2777457, 1130.0]
[2019-03-27 09:11:56,260] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.8287100e-20 1.0000000e+00 9.5201614e-27 7.7560616e-20 1.7960245e-29], sum to 1.0000
[2019-03-27 09:11:56,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0824
[2019-03-27 09:11:56,276] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.4, 87.66666666666666, 1.0, 2.0, 0.2158356069159676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 359130.1103477972, 359130.1103477978, 157343.2921589192], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 538800.0000, 
sim time next is 539400.0000, 
raw observation next is [18.6, 86.83333333333333, 1.0, 2.0, 0.2176994506504951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 361956.2762085944, 361956.2762085944, 157563.559918423], 
processed observation next is [1.0, 0.21739130434782608, 0.08056872037914704, 0.8683333333333333, 1.0, 1.0, 0.05746921765119889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10054341005794289, 0.10054341005794289, 0.23516949241555674], 
reward next is 0.7648, 
noisyNet noise sample is [array([-0.11125363], dtype=float32), -0.3611117]. 
=============================================
[2019-03-27 09:12:12,865] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.23563148e-21 1.00000000e+00 6.49540674e-27 1.17332803e-21
 1.05032485e-29], sum to 1.0000
[2019-03-27 09:12:12,876] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6674
[2019-03-27 09:12:12,884] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 69.0, 1.0, 2.0, 0.3017484013843819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479727.3012637906, 479727.3012637906, 165589.387032358], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 834000.0000, 
sim time next is 834600.0000, 
raw observation next is [24.15, 69.5, 1.0, 2.0, 0.302115832327624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480020.9036320557, 480020.9036320557, 165605.2726722674], 
processed observation next is [0.0, 0.6521739130434783, 0.34360189573459715, 0.695, 1.0, 1.0, 0.159175701599547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13333913989779325, 0.13333913989779325, 0.2471720487645782], 
reward next is 0.7528, 
noisyNet noise sample is [array([-0.48667312], dtype=float32), 1.3048427]. 
=============================================
[2019-03-27 09:12:13,226] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7518062e-12 9.9999738e-01 2.0765132e-17 2.6258549e-06 1.6719697e-20], sum to 1.0000
[2019-03-27 09:12:13,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0711
[2019-03-27 09:12:13,243] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1722682.795232795 W.
[2019-03-27 09:12:13,248] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.18333333333333, 71.83333333333333, 1.0, 2.0, 0.6161208091805056, 1.0, 2.0, 0.6161208091805056, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1722682.795232795, 1722682.795232795, 340272.0435351115], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1252200.0000, 
sim time next is 1252800.0000, 
raw observation next is [28.2, 72.0, 1.0, 2.0, 0.4139449832348781, 1.0, 2.0, 0.4139449832348781, 1.0, 1.0, 0.6972533460113532, 6.911199999999999, 6.9112, 170.5573041426782, 1736105.206827631, 1736105.206827631, 357516.6553343583], 
processed observation next is [1.0, 0.5217391304347826, 0.5355450236966824, 0.72, 1.0, 1.0, 0.29390961835527485, 1.0, 1.0, 0.29390961835527485, 1.0, 0.5, 0.6307967634284796, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4822514463410086, 0.4822514463410086, 0.5336069482602362], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29695976], dtype=float32), -1.8434787]. 
=============================================
[2019-03-27 09:12:19,136] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6231809e-20 1.0000000e+00 3.2696235e-27 2.8348084e-20 8.5579938e-30], sum to 1.0000
[2019-03-27 09:12:19,147] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6622
[2019-03-27 09:12:19,152] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 90.5, 1.0, 2.0, 0.3385836251916731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524352.7180643793, 524352.7180643787, 168633.4857772651], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 941400.0000, 
sim time next is 942000.0000, 
raw observation next is [22.2, 91.0, 1.0, 2.0, 0.339550640762631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525660.1717307077, 525660.1717307077, 168732.1414536615], 
processed observation next is [0.0, 0.9130434782608695, 0.2511848341232228, 0.91, 1.0, 1.0, 0.20427788043690479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14601671436964103, 0.14601671436964103, 0.25183901709501716], 
reward next is 0.7482, 
noisyNet noise sample is [array([0.33454466], dtype=float32), -0.060380608]. 
=============================================
[2019-03-27 09:12:19,164] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[77.51607 ]
 [77.517365]
 [77.51869 ]
 [77.49591 ]
 [77.44383 ]], R is [[77.48825073]
 [77.46167755]
 [77.4355011 ]
 [77.40964508]
 [77.38405609]].
[2019-03-27 09:12:23,520] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6069535e-19 1.0000000e+00 4.7075059e-26 6.8529329e-19 3.0556549e-28], sum to 1.0000
[2019-03-27 09:12:23,528] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9267
[2019-03-27 09:12:23,537] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.0, 1.0, 2.0, 0.3497808518227349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 537152.0915332764, 537152.091533277, 169531.2477243921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1047600.0000, 
sim time next is 1048200.0000, 
raw observation next is [21.56666666666667, 96.83333333333334, 1.0, 2.0, 0.3545090253303371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546747.7960604882, 546747.7960604876, 170392.7842107117], 
processed observation next is [1.0, 0.13043478260869565, 0.22116903633491333, 0.9683333333333334, 1.0, 1.0, 0.22230003051847846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15187438779458007, 0.1518743877945799, 0.25431758837419655], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.5434586], dtype=float32), 0.06236172]. 
=============================================
[2019-03-27 09:12:28,198] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3718572e-16 1.0000000e+00 1.2758183e-22 7.6075485e-14 7.9712558e-26], sum to 1.0000
[2019-03-27 09:12:28,206] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3318
[2019-03-27 09:12:28,211] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 67.0, 1.0, 2.0, 0.7365807830015667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1138469.343710732, 1138469.343710732, 242778.3475346128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1095600.0000, 
sim time next is 1096200.0000, 
raw observation next is [25.75, 67.0, 1.0, 2.0, 0.7502723978122694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1159155.047152493, 1159155.047152493, 246202.704848598], 
processed observation next is [1.0, 0.6956521739130435, 0.41943127962085314, 0.67, 1.0, 1.0, 0.6991233708581559, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3219875130979147, 0.3219875130979147, 0.36746672365462385], 
reward next is 0.6325, 
noisyNet noise sample is [array([0.37476778], dtype=float32), 1.0937109]. 
=============================================
[2019-03-27 09:12:34,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8367467e-20 1.0000000e+00 4.0651093e-27 1.9670106e-20 3.2021020e-29], sum to 1.0000
[2019-03-27 09:12:34,657] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2006
[2019-03-27 09:12:34,664] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 58.33333333333333, 1.0, 2.0, 0.3491524393875517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537979.8451074867, 537979.845107486, 169654.2106731027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1528800.0000, 
sim time next is 1529400.0000, 
raw observation next is [27.25, 58.66666666666667, 1.0, 2.0, 0.346292124444137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534723.2371210987, 534723.2371210987, 169422.5073354292], 
processed observation next is [0.0, 0.6956521739130435, 0.490521327014218, 0.5866666666666667, 1.0, 1.0, 0.21240014993269515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1485342325336385, 0.1485342325336385, 0.2528694139334764], 
reward next is 0.7471, 
noisyNet noise sample is [array([1.1007456], dtype=float32), 2.183051]. 
=============================================
[2019-03-27 09:12:36,636] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0814546e-16 1.0000000e+00 2.8784494e-21 2.1909814e-11 1.2020132e-23], sum to 1.0000
[2019-03-27 09:12:36,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7889
[2019-03-27 09:12:36,647] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 75.0, 1.0, 2.0, 0.956834170579687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1370563.082739704, 1370563.082739704, 291092.4971310168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1245600.0000, 
sim time next is 1246200.0000, 
raw observation next is [27.01666666666667, 74.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.329390880512875, 6.9112, 168.9110677347442, 1780997.064492748, 1484321.378286425, 316201.3668282327], 
processed observation next is [1.0, 0.43478260869565216, 0.4794628751974725, 0.7433333333333333, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.04181908805128751, 0.0, 0.8294306703989952, 0.4947214068035411, 0.4123114939684514, 0.47194233854960105], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.1400644], dtype=float32), 1.2153252]. 
=============================================
[2019-03-27 09:12:39,264] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4014102e-17 1.0000000e+00 5.3972109e-23 2.6916883e-13 2.6837707e-26], sum to 1.0000
[2019-03-27 09:12:39,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3315
[2019-03-27 09:12:39,278] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 90.0, 1.0, 2.0, 0.7090873024859129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1054767.694420521, 1054767.69442052, 231158.8804639621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1610400.0000, 
sim time next is 1611000.0000, 
raw observation next is [23.55, 90.5, 1.0, 2.0, 0.7373119212329328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1096130.606329716, 1096130.606329715, 237805.394125598], 
processed observation next is [1.0, 0.6521739130434783, 0.3151658767772513, 0.905, 1.0, 1.0, 0.6835083388348587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30448072398047665, 0.30448072398047643, 0.3549334240680567], 
reward next is 0.6451, 
noisyNet noise sample is [array([0.13038139], dtype=float32), 0.1292858]. 
=============================================
[2019-03-27 09:12:39,296] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[65.737885]
 [65.79455 ]
 [65.469666]
 [64.87188 ]
 [65.009026]], R is [[65.54251099]
 [65.54206848]
 [65.56059265]
 [65.57744598]
 [65.54995728]].
[2019-03-27 09:12:39,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8937214e-19 1.0000000e+00 1.4470079e-25 1.7603758e-18 1.1302658e-28], sum to 1.0000
[2019-03-27 09:12:39,375] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6645
[2019-03-27 09:12:39,381] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.38333333333333, 94.00000000000001, 1.0, 2.0, 0.4636476081755364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654985.2056787602, 654985.2056787602, 179015.5343339711], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1293000.0000, 
sim time next is 1293600.0000, 
raw observation next is [24.36666666666667, 94.0, 1.0, 2.0, 0.46242666146235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653740.3159248809, 653740.3159248809, 178897.1077659755], 
processed observation next is [1.0, 1.0, 0.3538704581358612, 0.94, 1.0, 1.0, 0.3523212788703012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1815945322013558, 0.1815945322013558, 0.26701060860593356], 
reward next is 0.7330, 
noisyNet noise sample is [array([-0.72363865], dtype=float32), 0.46875426]. 
=============================================
[2019-03-27 09:12:39,799] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.211663e-17 1.000000e+00 4.024687e-25 2.017091e-15 3.234172e-26], sum to 1.0000
[2019-03-27 09:12:39,811] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5399
[2019-03-27 09:12:39,818] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 93.5, 1.0, 2.0, 0.5559673665526376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 792256.6378224494, 792256.63782245, 194794.0729365264], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1305000.0000, 
sim time next is 1305600.0000, 
raw observation next is [24.26666666666667, 93.33333333333334, 1.0, 2.0, 0.5487567883739879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782188.4906874401, 782188.4906874401, 193552.1787014624], 
processed observation next is [1.0, 0.08695652173913043, 0.34913112164297017, 0.9333333333333335, 1.0, 1.0, 0.4563334799686601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21727458074651115, 0.21727458074651115, 0.2888838488081528], 
reward next is 0.7111, 
noisyNet noise sample is [array([-2.100427], dtype=float32), -0.1276028]. 
=============================================
[2019-03-27 09:12:40,015] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4131264e-18 1.0000000e+00 6.9866649e-24 9.9106042e-16 1.7207109e-27], sum to 1.0000
[2019-03-27 09:12:40,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2865
[2019-03-27 09:12:40,029] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.51666666666667, 91.83333333333334, 1.0, 2.0, 0.5596482083041264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795960.9681756579, 795960.9681756579, 195251.3883863686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1311000.0000, 
sim time next is 1311600.0000, 
raw observation next is [24.53333333333333, 91.66666666666667, 1.0, 2.0, 0.5137428404186826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730800.3274593552, 730800.3274593552, 187428.4019170274], 
processed observation next is [1.0, 0.17391304347826086, 0.36176935229067925, 0.9166666666666667, 1.0, 1.0, 0.4141480005044368, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20300009096093202, 0.20300009096093202, 0.27974388345824985], 
reward next is 0.7203, 
noisyNet noise sample is [array([-1.2753361], dtype=float32), -1.1046726]. 
=============================================
[2019-03-27 09:12:42,862] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5778840e-17 1.0000000e+00 1.4061653e-22 2.5712048e-14 2.0638276e-25], sum to 1.0000
[2019-03-27 09:12:42,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6711
[2019-03-27 09:12:42,878] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.23333333333333, 85.83333333333334, 1.0, 2.0, 0.7594696038246377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1162191.58322926, 1162191.58322926, 247302.023874254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1771800.0000, 
sim time next is 1772400.0000, 
raw observation next is [23.16666666666667, 85.66666666666667, 1.0, 2.0, 0.6935849271315306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1064085.705241788, 1064085.705241787, 231391.2392456986], 
processed observation next is [1.0, 0.5217391304347826, 0.2969984202211693, 0.8566666666666667, 1.0, 1.0, 0.6308252134114827, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29557936256716333, 0.29557936256716305, 0.3453600585756696], 
reward next is 0.6546, 
noisyNet noise sample is [array([1.0591058], dtype=float32), -0.13144453]. 
=============================================
[2019-03-27 09:12:43,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1017788e-19 1.0000000e+00 6.0628436e-27 3.0052214e-19 1.7754117e-29], sum to 1.0000
[2019-03-27 09:12:43,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4052
[2019-03-27 09:12:43,531] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 95.0, 1.0, 2.0, 0.3227843354490936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508922.2850250268, 508922.2850250274, 167669.4591381972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1371600.0000, 
sim time next is 1372200.0000, 
raw observation next is [20.98333333333333, 95.0, 1.0, 2.0, 0.3217925406502481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507551.1779769099, 507551.1779769099, 167569.0005745254], 
processed observation next is [1.0, 0.9130434782608695, 0.1935229067930489, 0.95, 1.0, 1.0, 0.18288257909668446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14098643832691943, 0.14098643832691943, 0.25010298593212743], 
reward next is 0.7499, 
noisyNet noise sample is [array([1.4369382], dtype=float32), -0.10011838]. 
=============================================
[2019-03-27 09:12:46,234] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6912723e-21 1.0000000e+00 6.4874614e-26 2.6096745e-21 1.1221717e-28], sum to 1.0000
[2019-03-27 09:12:46,243] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4071
[2019-03-27 09:12:46,247] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 94.0, 1.0, 2.0, 0.3812029617447436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 574572.6535373455, 574572.6535373455, 172403.5456470895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1458000.0000, 
sim time next is 1458600.0000, 
raw observation next is [22.63333333333333, 94.16666666666667, 1.0, 2.0, 0.3798966029423203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573246.6561010603, 573246.6561010603, 172305.686990211], 
processed observation next is [0.0, 0.9130434782608695, 0.27172195892575024, 0.9416666666666668, 1.0, 1.0, 0.2528874734244823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15923518225029454, 0.15923518225029454, 0.25717266714956866], 
reward next is 0.7428, 
noisyNet noise sample is [array([0.85249865], dtype=float32), 0.1523586]. 
=============================================
[2019-03-27 09:12:47,800] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 09:12:47,801] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:12:47,802] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:12:47,805] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:12:47,805] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:12:47,806] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:12:47,808] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:12:47,809] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:12:47,810] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:12:47,810] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:12:47,811] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:12:47,832] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run96
[2019-03-27 09:12:47,858] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run96
[2019-03-27 09:12:47,859] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run96
[2019-03-27 09:12:47,859] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run96
[2019-03-27 09:12:47,921] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run96
[2019-03-27 09:12:49,719] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09217565], dtype=float32), 0.04994258]
[2019-03-27 09:12:49,719] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.41666666666666, 67.0, 1.0, 2.0, 1.029616093215731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9127667897571, 1439230.034864057, 1439230.034864057, 308076.7752449627]
[2019-03-27 09:12:49,723] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:12:49,725] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.8075610e-16 1.0000000e+00 6.6844019e-22 2.4040273e-12 3.4238087e-25], sampled 0.1744980635016401
[2019-03-27 09:12:57,940] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09217565], dtype=float32), 0.04994258]
[2019-03-27 09:12:57,941] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.1, 50.33333333333334, 1.0, 2.0, 0.5445268599407027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 872902.984441503, 872902.9844415024, 203283.7117461775]
[2019-03-27 09:12:57,942] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:12:57,945] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0776079e-18 1.0000000e+00 2.9874271e-25 9.0916281e-18 5.2354350e-28], sampled 0.7997818204608841
[2019-03-27 09:13:24,055] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09217565], dtype=float32), 0.04994258]
[2019-03-27 09:13:24,055] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.70599142333333, 86.17966403166666, 1.0, 2.0, 0.5627158628446752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786340.4276178183, 786340.4276178183, 194004.3350429283]
[2019-03-27 09:13:24,056] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:13:24,060] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.5350439e-19 1.0000000e+00 1.5191320e-25 9.1886961e-18 1.5656072e-28], sampled 0.9463512690279128
[2019-03-27 09:14:14,075] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.09217565], dtype=float32), 0.04994258]
[2019-03-27 09:14:14,078] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.99246079666667, 87.59102352166667, 1.0, 2.0, 0.6478387848646179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 905342.0889533974, 905342.088953398, 209988.8208934226]
[2019-03-27 09:14:14,079] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:14:14,082] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.31725632e-18 1.00000000e+00 8.42948364e-25 3.41593002e-17
 1.30641885e-27], sampled 0.5506011420772312
[2019-03-27 09:14:39,464] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09217565], dtype=float32), 0.04994258]
[2019-03-27 09:14:39,465] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.73333333333333, 73.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.720829050891753, 6.9112, 168.9085061297949, 2028511.949891668, 1454148.369168947, 311346.7399073906]
[2019-03-27 09:14:39,465] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:14:39,469] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9789146e-15 1.0000000e+00 6.5908220e-21 1.4067090e-12 1.3508877e-23], sampled 0.4928368746938654
[2019-03-27 09:14:39,471] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2028511.949891668 W.
[2019-03-27 09:14:43,266] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6185 2779263580.8788 933.0000
[2019-03-27 09:14:43,781] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0051 3007614899.7657 1766.0000
[2019-03-27 09:14:43,786] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.2659 3163382520.3850 1770.0000
[2019-03-27 09:14:43,797] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3920 2842387436.0031 1129.0000
[2019-03-27 09:14:43,882] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2912 2927339788.0998 1338.0000
[2019-03-27 09:14:44,901] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2375000, evaluation results [2375000.0, 7884.265872442234, 3163382520.3849535, 1770.0, 8254.291174362113, 2927339788.099768, 1338.0, 8660.61847634223, 2779263580.878825, 933.0, 7999.005118545964, 3007614899.7656994, 1766.0, 8497.392015529722, 2842387436.0031424, 1129.0]
[2019-03-27 09:14:46,556] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0141634e-20 1.0000000e+00 3.3494356e-26 2.4213942e-20 3.4680874e-29], sum to 1.0000
[2019-03-27 09:14:46,566] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7241
[2019-03-27 09:14:46,570] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 97.83333333333334, 1.0, 2.0, 0.3187335954136062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502412.6620609707, 502412.6620609707, 167173.1726237352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1482600.0000, 
sim time next is 1483200.0000, 
raw observation next is [20.6, 98.0, 1.0, 2.0, 0.3167240878302783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499821.5339507469, 499821.5339507469, 166990.9866928654], 
processed observation next is [0.0, 0.17391304347826086, 0.17535545023696694, 0.98, 1.0, 1.0, 0.1767760094340702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13883931498631857, 0.13883931498631857, 0.24924027864606776], 
reward next is 0.7508, 
noisyNet noise sample is [array([1.3060558], dtype=float32), 1.8437344]. 
=============================================
[2019-03-27 09:14:57,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5953522e-19 1.0000000e+00 2.6421251e-24 3.3747940e-17 3.1674670e-28], sum to 1.0000
[2019-03-27 09:14:57,405] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8825
[2019-03-27 09:14:57,413] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 99.0, 1.0, 2.0, 0.4281217305497677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616870.4574160738, 616870.4574160744, 175508.2533789157], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1654800.0000, 
sim time next is 1655400.0000, 
raw observation next is [23.3, 99.0, 1.0, 2.0, 0.4283203187018617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 617158.4046353626, 617158.404635362, 175536.2886674038], 
processed observation next is [1.0, 0.13043478260869565, 0.3033175355450238, 0.99, 1.0, 1.0, 0.3112292996407972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1714328901764896, 0.17143289017648944, 0.2619944606976176], 
reward next is 0.7380, 
noisyNet noise sample is [array([-0.0223975], dtype=float32), 0.35534084]. 
=============================================
[2019-03-27 09:15:04,658] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1212398e-19 1.0000000e+00 1.3155219e-25 1.0826057e-18 1.1153520e-28], sum to 1.0000
[2019-03-27 09:15:04,662] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9564
[2019-03-27 09:15:04,666] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 95.0, 1.0, 2.0, 0.3646920546257617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561563.3696320932, 561563.3696320932, 171617.7061094161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1825200.0000, 
sim time next is 1825800.0000, 
raw observation next is [21.9, 95.16666666666667, 1.0, 2.0, 0.3628187414379867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558332.0404360032, 558332.0404360032, 171333.4046020497], 
processed observation next is [1.0, 0.13043478260869565, 0.23696682464454974, 0.9516666666666667, 1.0, 1.0, 0.23231173667227314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15509223345444534, 0.15509223345444534, 0.25572149940604433], 
reward next is 0.7443, 
noisyNet noise sample is [array([-0.21209446], dtype=float32), 0.7086774]. 
=============================================
[2019-03-27 09:15:11,381] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7783180e-17 1.0000000e+00 3.8917473e-26 6.2478904e-17 2.2956536e-28], sum to 1.0000
[2019-03-27 09:15:11,387] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8311
[2019-03-27 09:15:11,392] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 88.66666666666667, 1.0, 2.0, 0.4590128919104424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651900.7031326913, 651900.703132692, 178778.1598062251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1891200.0000, 
sim time next is 1891800.0000, 
raw observation next is [24.9, 89.0, 1.0, 2.0, 0.4575749553836169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650445.6121707486, 650445.612170748, 178641.9379266083], 
processed observation next is [1.0, 0.9130434782608695, 0.3791469194312796, 0.89, 1.0, 1.0, 0.3464758498597794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18067933671409683, 0.18067933671409664, 0.2666297580994154], 
reward next is 0.7334, 
noisyNet noise sample is [array([1.6252121], dtype=float32), 0.7878173]. 
=============================================
[2019-03-27 09:15:13,274] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9832136e-16 1.0000000e+00 2.0204094e-23 4.5354887e-14 5.9849022e-26], sum to 1.0000
[2019-03-27 09:15:13,283] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0387
[2019-03-27 09:15:13,288] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 82.0, 1.0, 2.0, 0.9743470879759385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1390709.438635488, 1390709.438635488, 295639.1465482148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1933200.0000, 
sim time next is 1933800.0000, 
raw observation next is [25.85, 81.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.009109463974895, 6.9112, 168.9124048539953, 1553421.696664891, 1483961.579989437, 316169.3551255277], 
processed observation next is [1.0, 0.391304347826087, 0.4241706161137442, 0.8166666666666668, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.009790946397489541, 0.0, 0.829437236266654, 0.4315060268513586, 0.41221154999706583, 0.4718945598888473], 
reward next is 0.0386, 
noisyNet noise sample is [array([-0.5542713], dtype=float32), -0.15871187]. 
=============================================
[2019-03-27 09:15:14,561] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6009116e-10 9.9944621e-01 7.0637074e-16 5.5381178e-04 1.8454759e-18], sum to 1.0000
[2019-03-27 09:15:14,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1592
[2019-03-27 09:15:14,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1662821.938299289 W.
[2019-03-27 09:15:14,588] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.1, 76.0, 1.0, 2.0, 0.3964853707973026, 1.0, 2.0, 0.3964853707973026, 1.0, 2.0, 0.6660973806490799, 6.9112, 6.9112, 170.5573041426782, 1662821.938299289, 1662821.938299289, 347755.2310690132], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1953000.0000, 
sim time next is 1953600.0000, 
raw observation next is [26.93333333333334, 76.66666666666667, 1.0, 2.0, 0.394903751315024, 1.0, 2.0, 0.394903751315024, 1.0, 2.0, 0.6629268453361036, 6.911199999999999, 6.9112, 170.5573041426782, 1656183.651015708, 1656183.651015708, 346840.9315525449], 
processed observation next is [1.0, 0.6086956521739131, 0.4755134281200636, 0.7666666666666667, 1.0, 1.0, 0.27096837507834215, 1.0, 1.0, 0.27096837507834215, 1.0, 1.0, 0.5889351772391507, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.46005101417103, 0.46005101417103, 0.5176730321679774], 
reward next is 0.4823, 
noisyNet noise sample is [array([0.37572286], dtype=float32), 0.38755682]. 
=============================================
[2019-03-27 09:15:16,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1252221e-19 1.0000000e+00 7.5638114e-25 5.6075844e-17 1.1783662e-27], sum to 1.0000
[2019-03-27 09:15:16,427] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5857
[2019-03-27 09:15:16,432] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 96.66666666666667, 1.0, 2.0, 0.4589800935241213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650967.111097416, 650967.1110974167, 178659.9968710756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1987800.0000, 
sim time next is 1988400.0000, 
raw observation next is [24.03333333333333, 96.33333333333334, 1.0, 2.0, 0.4605817607332746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652516.0638483983, 652516.0638483976, 178803.1379552874], 
processed observation next is [0.0, 0.0, 0.3380726698262243, 0.9633333333333334, 1.0, 1.0, 0.3500985069075597, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18125446218011063, 0.18125446218011043, 0.2668703551571454], 
reward next is 0.7331, 
noisyNet noise sample is [array([0.3726902], dtype=float32), 0.7764805]. 
=============================================
[2019-03-27 09:15:17,203] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.02045282e-19 1.00000000e+00 1.72399871e-24 1.05161415e-17
 4.23508047e-28], sum to 1.0000
[2019-03-27 09:15:17,210] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4653
[2019-03-27 09:15:17,215] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 95.33333333333334, 1.0, 2.0, 0.4674718811668397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656816.7985855665, 656816.7985855665, 179123.1523777576], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1999200.0000, 
sim time next is 1999800.0000, 
raw observation next is [24.3, 95.5, 1.0, 2.0, 0.4666709421010847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656049.2768892684, 656049.2768892691, 179050.9437586584], 
processed observation next is [0.0, 0.13043478260869565, 0.3507109004739337, 0.955, 1.0, 1.0, 0.35743487000130686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18223591024701902, 0.1822359102470192, 0.2672402145651618], 
reward next is 0.7328, 
noisyNet noise sample is [array([0.73091733], dtype=float32), -0.88029337]. 
=============================================
[2019-03-27 09:15:17,482] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3613922e-18 1.0000000e+00 2.6801620e-24 9.0857084e-16 5.8810158e-28], sum to 1.0000
[2019-03-27 09:15:17,491] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7783
[2019-03-27 09:15:17,501] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.1, 77.0, 1.0, 2.0, 0.5793113965352257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809539.9016293401, 809539.9016293401, 196955.0082168444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2410200.0000, 
sim time next is 2410800.0000, 
raw observation next is [30.0, 77.33333333333334, 1.0, 2.0, 0.5780193806038082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807733.7294120664, 807733.729412067, 196722.4401446359], 
processed observation next is [1.0, 0.9130434782608695, 0.6208530805687204, 0.7733333333333334, 1.0, 1.0, 0.4915896151853111, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22437048039224067, 0.22437048039224083, 0.2936155823054267], 
reward next is 0.7064, 
noisyNet noise sample is [array([0.22970569], dtype=float32), -0.3628006]. 
=============================================
[2019-03-27 09:15:17,834] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.0784262e-20 1.0000000e+00 1.3838346e-25 1.4358166e-20 7.6868950e-29], sum to 1.0000
[2019-03-27 09:15:17,841] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7422
[2019-03-27 09:15:17,845] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 94.00000000000001, 1.0, 2.0, 0.5044118205129783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704839.1949613115, 704839.1949613115, 184290.5711192366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2016600.0000, 
sim time next is 2017200.0000, 
raw observation next is [25.6, 94.0, 1.0, 2.0, 0.5050224048964672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 705692.6776508042, 705692.6776508036, 184387.0539760495], 
processed observation next is [0.0, 0.34782608695652173, 0.4123222748815167, 0.94, 1.0, 1.0, 0.4036414516824906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19602574379189006, 0.1960257437918899, 0.2752045581732082], 
reward next is 0.7248, 
noisyNet noise sample is [array([0.20513704], dtype=float32), -0.55341625]. 
=============================================
[2019-03-27 09:15:23,595] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3287709e-18 1.0000000e+00 9.0538020e-25 3.6440685e-18 4.5525652e-28], sum to 1.0000
[2019-03-27 09:15:23,599] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1772382e-19 1.0000000e+00 4.5554931e-24 5.0727964e-18 1.3813122e-27], sum to 1.0000
[2019-03-27 09:15:23,604] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2224
[2019-03-27 09:15:23,608] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8899
[2019-03-27 09:15:23,609] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.33333333333334, 1.0, 2.0, 0.5588145818713958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780886.7647537526, 780886.7647537532, 193324.6083505794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2118000.0000, 
sim time next is 2118600.0000, 
raw observation next is [30.0, 75.5, 1.0, 2.0, 0.5597742409293968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782228.2848523678, 782228.2848523678, 193491.8349713187], 
processed observation next is [0.0, 0.5217391304347826, 0.6208530805687204, 0.755, 1.0, 1.0, 0.46960751919204424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21728563468121329, 0.21728563468121329, 0.28879378353928165], 
reward next is 0.7112, 
noisyNet noise sample is [array([-0.76327324], dtype=float32), -0.72146887]. 
=============================================
[2019-03-27 09:15:23,612] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.6, 75.66666666666666, 1.0, 2.0, 0.5443626739206694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 760684.4562303623, 760684.4562303616, 190839.4769303292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2112000.0000, 
sim time next is 2112600.0000, 
raw observation next is [29.8, 74.83333333333334, 1.0, 2.0, 0.5460826286472423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763088.7591808831, 763088.7591808836, 191132.0399709794], 
processed observation next is [0.0, 0.43478260869565216, 0.6113744075829385, 0.7483333333333334, 1.0, 1.0, 0.45311160077981, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21196909977246753, 0.21196909977246767, 0.285271701449223], 
reward next is 0.7147, 
noisyNet noise sample is [array([0.9058771], dtype=float32), 0.22701278]. 
=============================================
[2019-03-27 09:15:25,166] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0243916e-21 1.0000000e+00 4.2253039e-27 2.9069660e-19 3.8893815e-29], sum to 1.0000
[2019-03-27 09:15:25,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2090
[2019-03-27 09:15:25,180] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.51666666666667, 87.33333333333333, 1.0, 2.0, 0.5458882343340453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762817.0175572584, 762817.0175572584, 191098.1354574556], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2146200.0000, 
sim time next is 2146800.0000, 
raw observation next is [27.43333333333334, 87.66666666666667, 1.0, 2.0, 0.5460637066007709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763062.3082619269, 763062.3082619262, 191127.9139228205], 
processed observation next is [0.0, 0.8695652173913043, 0.49921011058451853, 0.8766666666666667, 1.0, 1.0, 0.45308880313345884, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21196175229497968, 0.2119617522949795, 0.2852655431683888], 
reward next is 0.7147, 
noisyNet noise sample is [array([-1.3851656], dtype=float32), 1.4526346]. 
=============================================
[2019-03-27 09:15:25,963] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8310326e-18 1.0000000e+00 2.4625528e-24 1.2543522e-16 1.1982071e-26], sum to 1.0000
[2019-03-27 09:15:25,971] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4155
[2019-03-27 09:15:25,977] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.76666666666667, 93.16666666666667, 1.0, 2.0, 0.5136989195474245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717820.895321216, 717820.8953212154, 185770.1529917105], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2160600.0000, 
sim time next is 2161200.0000, 
raw observation next is [25.73333333333333, 93.33333333333334, 1.0, 2.0, 0.512815282090766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 716585.7216602801, 716585.7216602807, 185628.2625937236], 
processed observation next is [1.0, 0.0, 0.41864139020537117, 0.9333333333333335, 1.0, 1.0, 0.4130304603503204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19905158935007783, 0.19905158935007797, 0.2770571083488412], 
reward next is 0.7229, 
noisyNet noise sample is [array([-0.38037813], dtype=float32), 0.9095602]. 
=============================================
[2019-03-27 09:15:28,209] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.9184663e-17 1.0000000e+00 1.5217095e-22 2.1278745e-14 5.5032776e-25], sum to 1.0000
[2019-03-27 09:15:28,216] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2217
[2019-03-27 09:15:28,222] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 97.0, 1.0, 2.0, 0.558473430911198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780409.8657194518, 780409.8657194518, 193259.4443489104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2178000.0000, 
sim time next is 2178600.0000, 
raw observation next is [24.71666666666667, 96.16666666666667, 1.0, 2.0, 0.5331504626619314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745011.1813455194, 745011.1813455194, 188950.06243114], 
processed observation next is [1.0, 0.21739130434782608, 0.3704581358609796, 0.9616666666666667, 1.0, 1.0, 0.43753067790594147, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2069475503737554, 0.2069475503737554, 0.2820150185539403], 
reward next is 0.7180, 
noisyNet noise sample is [array([0.45734546], dtype=float32), 0.989787]. 
=============================================
[2019-03-27 09:15:31,710] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1855701e-14 1.0000000e+00 8.0121877e-21 3.1147373e-11 1.7654892e-23], sum to 1.0000
[2019-03-27 09:15:31,720] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3595
[2019-03-27 09:15:31,726] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 81.0, 1.0, 2.0, 0.8873752454859619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1240285.12051485, 1240285.120514849, 266488.8410942963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2430000.0000, 
sim time next is 2430600.0000, 
raw observation next is [28.23333333333333, 81.16666666666667, 1.0, 2.0, 0.9049218598677752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1264824.646383425, 1264824.646383426, 271295.3643626649], 
processed observation next is [1.0, 0.13043478260869565, 0.537124802527646, 0.8116666666666668, 1.0, 1.0, 0.8854480239370786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35134017955095137, 0.35134017955095165, 0.40491845427263423], 
reward next is 0.5951, 
noisyNet noise sample is [array([-1.2470084], dtype=float32), -1.0985721]. 
=============================================
[2019-03-27 09:15:32,471] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2187157e-19 1.0000000e+00 3.0450938e-25 1.1996435e-18 6.7209665e-29], sum to 1.0000
[2019-03-27 09:15:32,482] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4704
[2019-03-27 09:15:32,487] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3968518920899767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592161.0428946876, 592161.042894687, 173816.1965682209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2666400.0000, 
sim time next is 2667000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3966616096263242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591877.1983858881, 591877.1983858881, 173790.0738845282], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2730862766582219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16441033288496892, 0.16441033288496892, 0.2593881699769077], 
reward next is 0.7406, 
noisyNet noise sample is [array([-0.18805644], dtype=float32), -0.5884562]. 
=============================================
[2019-03-27 09:15:32,506] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.91971 ]
 [73.91264 ]
 [73.90377 ]
 [73.88165 ]
 [73.823235]], R is [[73.91253662]
 [73.91397858]
 [73.91539764]
 [73.91674805]
 [73.91797638]].
[2019-03-27 09:15:39,036] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 09:15:39,037] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:15:39,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:15:39,038] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:15:39,039] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:15:39,039] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:15:39,040] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:15:39,040] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:15:39,042] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:15:39,043] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:15:39,044] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:15:39,071] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run97
[2019-03-27 09:15:39,071] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run97
[2019-03-27 09:15:39,072] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run97
[2019-03-27 09:15:39,093] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run97
[2019-03-27 09:15:39,118] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run97
[2019-03-27 09:15:51,020] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:15:51,020] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.2, 81.0, 1.0, 2.0, 0.2958970004740888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 472506.0193142867, 472506.0193142861, 165108.8733975341]
[2019-03-27 09:15:51,021] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:15:51,025] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3055275e-19 1.0000000e+00 2.4146809e-26 9.9540420e-20 6.6382069e-29], sampled 0.6446915295140322
[2019-03-27 09:15:59,564] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:15:59,566] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [18.5, 86.33333333333334, 1.0, 2.0, 0.2281299573396262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 379593.0373698055, 379593.0373698061, 158432.3103236721]
[2019-03-27 09:15:59,567] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:15:59,570] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6021203e-19 1.0000000e+00 7.3436048e-26 1.4332891e-19 4.2286556e-28], sampled 0.5413737167147764
[2019-03-27 09:16:16,942] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:16:16,942] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.6, 94.66666666666667, 1.0, 2.0, 0.49011178109269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684850.6123784357, 684850.6123784357, 182060.3633488516]
[2019-03-27 09:16:16,943] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:16:16,948] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6677627e-18 1.0000000e+00 1.8177755e-24 9.1877545e-17 2.9348320e-27], sampled 0.5132798017853623
[2019-03-27 09:16:29,823] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:16:29,824] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.66666666666667, 76.33333333333334, 1.0, 2.0, 0.556248390816755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777299.4606370268, 777299.4606370262, 192878.5460170452]
[2019-03-27 09:16:29,824] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:16:29,827] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.9673811e-17 1.0000000e+00 6.7306338e-23 5.6571823e-14 1.1177724e-26], sampled 0.10154224622423902
[2019-03-27 09:16:38,185] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:16:38,186] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.268644405, 59.46130319333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.924870197795455, 6.9112, 168.9062061811588, 3003445.097106505, 2284341.309906657, 473761.065821864]
[2019-03-27 09:16:38,187] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:16:38,190] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.8088847e-11 9.9096972e-01 5.7474294e-15 9.0303272e-03 6.8471586e-19], sampled 0.24667199440969367
[2019-03-27 09:16:38,191] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 3003445.097106505 W.
[2019-03-27 09:16:45,473] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:16:45,473] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.40000000000001, 50.33333333333334, 1.0, 2.0, 0.5213648506772945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728536.6129624887, 728536.6129624887, 187010.5197102095]
[2019-03-27 09:16:45,475] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:16:45,479] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.0419007e-19 1.0000000e+00 8.6925816e-26 9.4671863e-18 9.2895841e-29], sampled 0.1650744538060548
[2019-03-27 09:16:52,077] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:16:52,078] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.43333333333333, 83.33333333333334, 1.0, 2.0, 0.6029168836058973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842539.7064008957, 842539.7064008957, 201290.015367486]
[2019-03-27 09:16:52,080] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:16:52,083] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4966583e-18 1.0000000e+00 5.1255897e-25 1.2280928e-17 1.2289207e-27], sampled 0.8622438483451126
[2019-03-27 09:16:52,203] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:16:52,205] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.61147604, 75.35610783999999, 1.0, 2.0, 0.5339112394256279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746074.6451946879, 746074.6451946873, 189078.7606675863]
[2019-03-27 09:16:52,208] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:16:52,212] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7444790e-18 1.0000000e+00 9.5871193e-25 3.7695126e-16 7.2910647e-28], sampled 0.6785063712017559
[2019-03-27 09:16:59,222] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:16:59,223] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.78333333333333, 94.16666666666667, 1.0, 2.0, 0.7210263652793141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1007668.83007156, 1007668.83007156, 225447.9939123006]
[2019-03-27 09:16:59,224] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:16:59,226] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.9705750e-17 1.0000000e+00 2.4657408e-23 2.5687820e-15 4.3525354e-26], sampled 0.517343992122968
[2019-03-27 09:17:00,571] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.09265333], dtype=float32), 0.049605068]
[2019-03-27 09:17:00,571] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.727044615, 69.538471725, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.452461186007302, 6.9112, 168.9096009868136, 1837999.619546233, 1454017.933522033, 311354.5505249962]
[2019-03-27 09:17:00,572] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:17:00,574] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.4780668e-15 1.0000000e+00 3.1830931e-20 5.5946324e-11 4.5040714e-23], sampled 0.3225374533692795
[2019-03-27 09:17:00,575] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1837999.619546233 W.
[2019-03-27 09:17:34,025] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7894.4243 3162987372.7639 1742.0000
[2019-03-27 09:17:34,207] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.0307 3007341812.9815 1761.0000
[2019-03-27 09:17:34,522] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2125 2779237704.4075 933.0000
[2019-03-27 09:17:34,660] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.8303 2842281788.2281 1125.0000
[2019-03-27 09:17:34,679] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.0922 2927484697.3615 1338.0000
[2019-03-27 09:17:35,698] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2400000, evaluation results [2400000.0, 7894.424251843367, 3162987372.763919, 1742.0, 8256.092210881632, 2927484697.3614755, 1338.0, 8659.212480730903, 2779237704.407499, 933.0, 7997.030693054014, 3007341812.981453, 1761.0, 8499.830336207642, 2842281788.2280903, 1125.0]
[2019-03-27 09:17:36,560] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7299876e-19 1.0000000e+00 2.6275710e-24 5.3336126e-18 2.4720836e-27], sum to 1.0000
[2019-03-27 09:17:36,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4225
[2019-03-27 09:17:36,577] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 69.0, 1.0, 2.0, 0.5398229483269797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754338.4574006329, 754338.4574006336, 190071.577557585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3237600.0000, 
sim time next is 3238200.0000, 
raw observation next is [31.0, 68.5, 1.0, 2.0, 0.5450771503215502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761683.2130904343, 761683.2130904343, 190961.1463620042], 
processed observation next is [0.0, 0.4782608695652174, 0.6682464454976303, 0.685, 1.0, 1.0, 0.45190018111030145, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21157867030289843, 0.21157867030289843, 0.2850166363612003], 
reward next is 0.7150, 
noisyNet noise sample is [array([-0.6601277], dtype=float32), 1.1544783]. 
=============================================
[2019-03-27 09:17:36,599] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3580336e-19 1.0000000e+00 3.7944852e-24 1.5044420e-12 3.7226189e-28], sum to 1.0000
[2019-03-27 09:17:36,608] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4828
[2019-03-27 09:17:36,614] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.43333333333334, 70.66666666666667, 1.0, 2.0, 0.5799419195826917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810421.3417291143, 810421.3417291143, 197069.1712000144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2402400.0000, 
sim time next is 2403000.0000, 
raw observation next is [31.3, 71.5, 1.0, 2.0, 0.5793604633452023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809608.4945816617, 809608.4945816617, 196964.4400551743], 
processed observation next is [1.0, 0.8260869565217391, 0.6824644549763034, 0.715, 1.0, 1.0, 0.4932053775243401, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22489124849490602, 0.22489124849490602, 0.29397677620175267], 
reward next is 0.7060, 
noisyNet noise sample is [array([-0.07746589], dtype=float32), 0.36328745]. 
=============================================
[2019-03-27 09:17:36,624] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.28559 ]
 [72.28186 ]
 [72.622025]
 [72.709206]
 [72.74425 ]], R is [[72.03942108]
 [72.02489471]
 [72.01065063]
 [71.9974823 ]
 [71.98527527]].
[2019-03-27 09:17:39,097] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5810521e-17 1.0000000e+00 4.5484501e-21 4.8214530e-13 1.2266089e-24], sum to 1.0000
[2019-03-27 09:17:39,108] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2996
[2019-03-27 09:17:39,116] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 0.9602712200657973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1342236.290751315, 1342236.290751314, 287054.3852909019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2439000.0000, 
sim time next is 2439600.0000, 
raw observation next is [27.6, 84.0, 1.0, 2.0, 0.8848942044591367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1236815.347597194, 1236815.347597193, 265815.9295412017], 
processed observation next is [1.0, 0.21739130434782608, 0.5071090047393366, 0.84, 1.0, 1.0, 0.8613183186254659, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3435598187769983, 0.34355981877699804, 0.39674019334507715], 
reward next is 0.6033, 
noisyNet noise sample is [array([-1.0088068], dtype=float32), 1.6680541]. 
=============================================
[2019-03-27 09:17:40,173] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4412509e-18 1.0000000e+00 3.1298175e-25 5.4899590e-19 8.3408721e-27], sum to 1.0000
[2019-03-27 09:17:40,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3123
[2019-03-27 09:17:40,185] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4228097815742998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619072.0669224252, 619072.0669224246, 176004.9039168248], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3299400.0000, 
sim time next is 3300000.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4225399448233598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 618677.5883841622, 618677.5883841629, 175967.0038830437], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.83, 1.0, 1.0, 0.30426499376308414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17185488566226728, 0.17185488566226748, 0.2626373192284234], 
reward next is 0.7374, 
noisyNet noise sample is [array([-1.353972], dtype=float32), 0.5131654]. 
=============================================
[2019-03-27 09:17:40,200] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.41722 ]
 [73.413734]
 [73.3749  ]
 [73.27726 ]
 [73.09264 ]], R is [[73.39984131]
 [73.40314484]
 [73.40632629]
 [73.4092865 ]
 [73.41182709]].
[2019-03-27 09:17:42,149] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.1399114e-17 1.0000000e+00 2.5578506e-22 1.9833340e-13 2.6738904e-25], sum to 1.0000
[2019-03-27 09:17:42,159] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7474
[2019-03-27 09:17:42,163] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.08333333333333, 92.33333333333333, 1.0, 2.0, 0.55378696632678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773858.6221313174, 773858.6221313174, 192452.5599444059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2490600.0000, 
sim time next is 2491200.0000, 
raw observation next is [27.0, 93.0, 1.0, 2.0, 0.5543193037997713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774602.7785973914, 774602.7785973914, 192544.4804045955], 
processed observation next is [1.0, 0.8695652173913043, 0.4786729857819906, 0.93, 1.0, 1.0, 0.463035305782857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2151674384992754, 0.2151674384992754, 0.2873798214993963], 
reward next is 0.7126, 
noisyNet noise sample is [array([-1.8682675], dtype=float32), 0.73780197]. 
=============================================
[2019-03-27 09:17:47,921] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.0820958e-21 1.0000000e+00 1.4428739e-25 6.3337426e-20 6.2141030e-28], sum to 1.0000
[2019-03-27 09:17:47,929] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3900
[2019-03-27 09:17:47,933] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3960511450640312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590966.8437370453, 590966.8437370453, 173706.3800040062], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2674200.0000, 
sim time next is 2674800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3959781094822624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 590857.7785036261, 590857.7785036255, 173696.3566230513], 
processed observation next is [0.0, 1.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2722627825087499, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1641271606954517, 0.16412716069545155, 0.25924829346724076], 
reward next is 0.7408, 
noisyNet noise sample is [array([0.00826067], dtype=float32), 1.3017129]. 
=============================================
[2019-03-27 09:17:50,890] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2594298e-19 1.0000000e+00 6.2607453e-26 1.9509227e-19 9.0633896e-29], sum to 1.0000
[2019-03-27 09:17:50,898] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3256
[2019-03-27 09:17:50,902] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.5290479725213855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810072.181546793, 810072.1815467936, 196816.9398456459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3054000.0000, 
sim time next is 3054600.0000, 
raw observation next is [22.0, 97.0, 1.0, 2.0, 0.5855272744419193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893171.9750684722, 893171.9750684722, 207240.9124076013], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.97, 1.0, 1.0, 0.500635270411951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24810332640790894, 0.24810332640790894, 0.30931479463821093], 
reward next is 0.6907, 
noisyNet noise sample is [array([0.6715837], dtype=float32), -0.49668935]. 
=============================================
[2019-03-27 09:17:53,153] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1948574e-21 1.0000000e+00 4.1697558e-28 5.2394922e-20 3.1162876e-30], sum to 1.0000
[2019-03-27 09:17:53,163] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7363
[2019-03-27 09:17:53,169] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3848265241419251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579622.1260436398, 579622.1260436404, 172841.3947641684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2723400.0000, 
sim time next is 2724000.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3845615347734362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 579222.9015641103, 579222.9015641103, 172805.6489485745], 
processed observation next is [0.0, 0.5217391304347826, 0.2417061611374408, 1.0, 1.0, 1.0, 0.25850787322100743, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16089525043447508, 0.16089525043447508, 0.25791887902772315], 
reward next is 0.7421, 
noisyNet noise sample is [array([0.3184939], dtype=float32), 1.5474716]. 
=============================================
[2019-03-27 09:17:53,180] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.36207 ]
 [75.36425 ]
 [75.367226]
 [75.34019 ]
 [75.28236 ]], R is [[75.3451004 ]
 [75.3336792 ]
 [75.32240295]
 [75.31134033]
 [75.300354  ]].
[2019-03-27 09:17:58,959] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6107515e-17 1.0000000e+00 4.8610579e-24 3.8374006e-15 3.0193214e-26], sum to 1.0000
[2019-03-27 09:17:58,965] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1652
[2019-03-27 09:17:58,974] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333334, 83.0, 1.0, 2.0, 0.5879318061340045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 909290.8951521824, 909290.8951521824, 209054.5582509945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2805600.0000, 
sim time next is 2806200.0000, 
raw observation next is [23.5, 83.0, 1.0, 2.0, 0.6100250177025361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 939134.7442926532, 939134.7442926537, 213163.2371544995], 
processed observation next is [1.0, 0.4782608695652174, 0.31279620853080575, 0.83, 1.0, 1.0, 0.5301506237379954, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26087076230351475, 0.2608707623035149, 0.31815408530522316], 
reward next is 0.6818, 
noisyNet noise sample is [array([-0.3325558], dtype=float32), -1.5704066]. 
=============================================
[2019-03-27 09:18:05,296] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0755263e-17 1.0000000e+00 1.0005879e-22 8.6305458e-15 8.6276669e-26], sum to 1.0000
[2019-03-27 09:18:05,307] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9714
[2019-03-27 09:18:05,311] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 89.0, 1.0, 2.0, 0.7212752840298917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1098952.566744534, 1098952.566744534, 237177.9138975594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2894400.0000, 
sim time next is 2895000.0000, 
raw observation next is [23.0, 89.83333333333334, 1.0, 2.0, 0.711985854721194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1082040.673469405, 1082040.673469406, 234610.3589276886], 
processed observation next is [1.0, 0.5217391304347826, 0.28909952606635075, 0.8983333333333334, 1.0, 1.0, 0.6529950056881856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3005668537415014, 0.3005668537415017, 0.35016471481744565], 
reward next is 0.6498, 
noisyNet noise sample is [array([1.3702825], dtype=float32), -0.8731521]. 
=============================================
[2019-03-27 09:18:05,328] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[66.671036]
 [66.67818 ]
 [66.892975]
 [67.171814]
 [67.42232 ]], R is [[66.62622833]
 [66.60597229]
 [66.58222961]
 [66.56008911]
 [66.54547119]].
[2019-03-27 09:18:07,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.10989937e-18 1.00000000e+00 1.27137935e-27 1.23679445e-18
 1.64381747e-29], sum to 1.0000
[2019-03-27 09:18:07,402] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2995
[2019-03-27 09:18:07,407] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.314803195304462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497805.83781817, 497805.8378181707, 166861.4792520773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2954400.0000, 
sim time next is 2955000.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3160220621309831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499731.7658908868, 499731.7658908868, 167005.2323440549], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.94, 1.0, 1.0, 0.17593019533853385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1388143794141352, 0.1388143794141352, 0.24926154081202226], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.6789148], dtype=float32), 0.3850704]. 
=============================================
[2019-03-27 09:18:07,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.33905 ]
 [72.49455 ]
 [72.557205]
 [72.6253  ]
 [72.71758 ]], R is [[72.37329102]
 [72.4005127 ]
 [72.42807007]
 [72.45552063]
 [72.48097992]].
[2019-03-27 09:18:14,147] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.3668311e-19 1.0000000e+00 7.9193631e-27 1.8312836e-18 2.6760700e-28], sum to 1.0000
[2019-03-27 09:18:14,156] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5809
[2019-03-27 09:18:14,163] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.3557463601685748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548977.406688696, 548977.4066886967, 170587.4430615893], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3052200.0000, 
sim time next is 3052800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3514967435835583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541706.0223128739, 541706.0223128739, 169964.0671676623], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2186707754018775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1504738950869094, 0.1504738950869094, 0.25367771219054075], 
reward next is 0.7463, 
noisyNet noise sample is [array([1.8139342], dtype=float32), -1.3554341]. 
=============================================
[2019-03-27 09:18:14,820] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4833162e-19 1.0000000e+00 5.9117920e-26 1.3337478e-18 2.5858424e-28], sum to 1.0000
[2019-03-27 09:18:14,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6527
[2019-03-27 09:18:14,835] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5823400120669868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813773.7633415911, 813773.7633415911, 197501.9384960767], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3887400.0000, 
sim time next is 3888000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5827663419174642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814369.7537258837, 814369.753725883, 197579.1329577842], 
processed observation next is [0.0, 0.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4973088456836918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22621382047941213, 0.22621382047941194, 0.2948942282952003], 
reward next is 0.7051, 
noisyNet noise sample is [array([0.9997447], dtype=float32), 0.9450868]. 
=============================================
[2019-03-27 09:18:14,846] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.88764 ]
 [74.93907 ]
 [74.98218 ]
 [75.028275]
 [75.090164]], R is [[73.40174103]
 [73.37294769]
 [73.34454346]
 [73.31638336]
 [73.28827667]].
[2019-03-27 09:18:18,304] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0427874e-19 1.0000000e+00 5.5469667e-26 1.3919186e-18 1.5979048e-29], sum to 1.0000
[2019-03-27 09:18:18,311] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6977
[2019-03-27 09:18:18,315] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 79.0, 1.0, 2.0, 0.5622576004434091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 785699.8138503868, 785699.8138503875, 193925.0032059908], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3351000.0000, 
sim time next is 3351600.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5573115122804657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778785.6072839798, 778785.6072839798, 193062.3429412053], 
processed observation next is [0.0, 0.8260869565217391, 0.5734597156398105, 0.79, 1.0, 1.0, 0.46664037624152493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21632933535666105, 0.21632933535666105, 0.28815275065851537], 
reward next is 0.7118, 
noisyNet noise sample is [array([0.23170146], dtype=float32), 0.33151293]. 
=============================================
[2019-03-27 09:18:21,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9524142e-19 1.0000000e+00 1.7929964e-24 1.7637856e-16 7.5991119e-28], sum to 1.0000
[2019-03-27 09:18:21,993] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7960
[2019-03-27 09:18:21,996] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.0, 1.0, 2.0, 0.5015079647209113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700780.157443193, 700780.157443193, 183832.9873011234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3187800.0000, 
sim time next is 3188400.0000, 
raw observation next is [25.66666666666666, 94.0, 1.0, 2.0, 0.5068186383444803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708203.4797991891, 708203.4797991891, 184671.6829332463], 
processed observation next is [1.0, 0.9130434782608695, 0.4154818325434437, 0.94, 1.0, 1.0, 0.40580558836684366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1967231888331081, 0.1967231888331081, 0.2756293775123079], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.14260162], dtype=float32), -0.57959634]. 
=============================================
[2019-03-27 09:18:29,619] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0867295e-10 5.6361292e-02 3.1346206e-14 9.4363868e-01 8.2037531e-18], sum to 1.0000
[2019-03-27 09:18:29,628] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4279
[2019-03-27 09:18:29,632] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.5, 70.5, 1.0, 2.0, 0.8567607976822198, 1.0, 2.0, 0.8567607976822198, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2396238.901186752, 2396238.901186752, 448455.6694094335], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [31.66666666666666, 70.66666666666666, 1.0, 2.0, 0.8715329980380208, 1.0, 2.0, 0.8715329980380208, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2437594.955798896, 2437594.955798896, 456204.7696639949], 
processed observation next is [1.0, 0.43478260869565216, 0.6998420221169034, 0.7066666666666666, 1.0, 1.0, 0.8452204795638805, 1.0, 1.0, 0.8452204795638805, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6771097099441378, 0.6771097099441378, 0.6809026412895446], 
reward next is 0.3191, 
noisyNet noise sample is [array([0.9034202], dtype=float32), -0.6284742]. 
=============================================
[2019-03-27 09:18:29,652] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[48.15365 ]
 [47.584515]
 [47.14251 ]
 [46.39238 ]
 [47.93908 ]], R is [[49.1348877 ]
 [48.9742012 ]
 [48.81732559]
 [48.70521927]
 [48.54175186]].
[2019-03-27 09:18:29,675] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 09:18:29,677] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:18:29,678] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:18:29,679] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:18:29,679] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:18:29,679] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:18:29,681] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:18:29,682] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:18:29,681] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:18:29,685] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:18:29,686] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:18:29,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run98
[2019-03-27 09:18:29,724] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run98
[2019-03-27 09:18:29,756] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run98
[2019-03-27 09:18:29,757] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run98
[2019-03-27 09:18:29,809] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run98
[2019-03-27 09:18:48,603] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09245676], dtype=float32), 0.049638852]
[2019-03-27 09:18:48,605] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.51666666666667, 91.83333333333334, 1.0, 2.0, 0.5596482083041264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795960.9681756579, 795960.9681756579, 195251.3883863686]
[2019-03-27 09:18:48,606] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:18:48,608] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.4125578e-19 1.0000000e+00 2.7190044e-25 1.5371208e-18 8.7058466e-28], sampled 0.32477158121590277
[2019-03-27 09:19:10,324] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09245676], dtype=float32), 0.049638852]
[2019-03-27 09:19:10,327] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.33333333333334, 86.33333333333334, 1.0, 2.0, 0.5305773171708572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 831678.084157785, 831678.0841577856, 199045.1700357139]
[2019-03-27 09:19:10,328] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:19:10,330] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1989663e-18 1.0000000e+00 5.3124147e-25 2.7984203e-18 1.6487803e-27], sampled 0.9867907248538926
[2019-03-27 09:19:20,350] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09245676], dtype=float32), 0.049638852]
[2019-03-27 09:19:20,351] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.18549679, 82.43472976, 1.0, 2.0, 0.6117922943559565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 854947.5453870803, 854947.5453870803, 202963.8578665476]
[2019-03-27 09:19:20,352] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:19:20,358] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1505961e-19 1.0000000e+00 9.1369981e-26 7.2300855e-18 9.1254196e-29], sampled 0.5122440821611585
[2019-03-27 09:19:24,377] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.09245676], dtype=float32), 0.049638852]
[2019-03-27 09:19:24,378] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.18957514, 80.46572005, 1.0, 2.0, 0.5258355287613676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734785.9394646192, 734785.9394646192, 187743.5380121852]
[2019-03-27 09:19:24,380] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:19:24,384] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3206143e-19 1.0000000e+00 4.1734360e-26 3.5257196e-18 3.3645927e-29], sampled 0.5587944740579586
[2019-03-27 09:19:26,577] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09245676], dtype=float32), 0.049638852]
[2019-03-27 09:19:26,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.55, 61.5, 1.0, 2.0, 0.5719750705302626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799284.133740024, 799284.133740024, 195640.5193339461]
[2019-03-27 09:19:26,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:19:26,584] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7855103e-19 1.0000000e+00 5.4945278e-26 3.6184420e-18 6.8086529e-29], sampled 0.23513618848488194
[2019-03-27 09:19:43,704] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09245676], dtype=float32), 0.049638852]
[2019-03-27 09:19:43,705] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.5223773964484538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729951.9943890804, 729951.994389081, 187176.1437091538]
[2019-03-27 09:19:43,706] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:19:43,708] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5123264e-19 1.0000000e+00 6.0323029e-26 1.6078253e-18 8.2418655e-29], sampled 0.3079915463793963
[2019-03-27 09:20:00,009] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09245676], dtype=float32), 0.049638852]
[2019-03-27 09:20:00,010] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.92716718, 81.39958941, 1.0, 2.0, 0.5621434574628386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793178.8256940077, 793178.8256940077, 194884.8255290409]
[2019-03-27 09:20:00,010] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:20:00,013] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.4128690e-19 1.0000000e+00 3.5184920e-25 3.2064730e-17 5.4245464e-28], sampled 0.8317360932713068
[2019-03-27 09:20:24,649] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.5531 2927360021.9205 1336.0000
[2019-03-27 09:20:25,009] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7895.7721 3162877400.8362 1757.0000
[2019-03-27 09:20:25,358] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9867 3007536415.8830 1765.0000
[2019-03-27 09:20:25,478] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7929 2842254838.1837 1128.0000
[2019-03-27 09:20:25,490] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.0298 2779326866.5862 933.0000
[2019-03-27 09:20:26,507] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2425000, evaluation results [2425000.0, 7895.7720522943355, 3162877400.83619, 1757.0, 8255.55310921197, 2927360021.9205174, 1336.0, 8660.02981325964, 2779326866.5862465, 933.0, 7998.986711916652, 3007536415.882969, 1765.0, 8496.792871053978, 2842254838.1837263, 1128.0]
[2019-03-27 09:20:30,981] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7867173e-16 1.0000000e+00 4.5121046e-22 2.8524201e-12 1.4139687e-24], sum to 1.0000
[2019-03-27 09:20:30,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5112
[2019-03-27 09:20:31,003] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7310351953082055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1021663.377300856, 1021663.377300855, 227686.0590640607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3387600.0000, 
sim time next is 3388200.0000, 
raw observation next is [26.16666666666667, 93.16666666666667, 1.0, 2.0, 0.8525567548986385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1191591.978612995, 1191591.978612995, 257219.8526596784], 
processed observation next is [1.0, 0.21739130434782608, 0.4391785150078992, 0.9316666666666668, 1.0, 1.0, 0.822357536022456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33099777183694307, 0.33099777183694307, 0.3839102278502663], 
reward next is 0.6161, 
noisyNet noise sample is [array([1.3963237], dtype=float32), -0.849689]. 
=============================================
[2019-03-27 09:20:38,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.5902444e-12 9.9079752e-01 4.9881474e-16 9.2024272e-03 3.8984202e-21], sum to 1.0000
[2019-03-27 09:20:38,099] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7714
[2019-03-27 09:20:38,106] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 68.33333333333334, 1.0, 2.0, 0.3962985043907092, 1.0, 2.0, 0.3962985043907092, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1107738.827144096, 1107738.827144096, 271208.1797578615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3690600.0000, 
sim time next is 3691200.0000, 
raw observation next is [31.33333333333334, 69.66666666666667, 1.0, 2.0, 0.5463977463219993, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763529.2587812699, 763529.2587812692, 191187.2551653814], 
processed observation next is [1.0, 0.7391304347826086, 0.6840442338072673, 0.6966666666666668, 1.0, 1.0, 0.4534912606289148, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21209146077257496, 0.21209146077257476, 0.2853541121871364], 
reward next is 0.7146, 
noisyNet noise sample is [array([-1.548665], dtype=float32), 2.0227032]. 
=============================================
[2019-03-27 09:20:52,714] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3719847e-16 1.0000000e+00 2.0801060e-21 1.3409298e-12 2.1907496e-24], sum to 1.0000
[2019-03-27 09:20:52,715] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5903
[2019-03-27 09:20:52,720] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 91.5, 1.0, 2.0, 0.8671886007029677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1212054.125597888, 1212054.125597888, 261072.4786098207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4084200.0000, 
sim time next is 4084800.0000, 
raw observation next is [27.0, 92.33333333333334, 1.0, 2.0, 0.7889234493354715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1102607.515297357, 1102607.515297357, 241211.4365035613], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.9233333333333335, 1.0, 1.0, 0.7456909028138211, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30627986536037693, 0.30627986536037693, 0.36001706940830047], 
reward next is 0.6400, 
noisyNet noise sample is [array([0.6478591], dtype=float32), -0.52055305]. 
=============================================
[2019-03-27 09:21:00,722] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.7571305e-18 1.0000000e+00 1.4801365e-25 1.3152226e-15 6.1068966e-28], sum to 1.0000
[2019-03-27 09:21:00,736] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4672
[2019-03-27 09:21:00,742] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6196549133722872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865939.6209307091, 865939.6209307091, 204465.4127909181], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4312800.0000, 
sim time next is 4313400.0000, 
raw observation next is [31.0, 79.00000000000001, 1.0, 2.0, 0.6225927536582418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 870046.8009729541, 870046.8009729536, 205030.9167263089], 
processed observation next is [1.0, 0.9565217391304348, 0.6682464454976303, 0.7900000000000001, 1.0, 1.0, 0.5452924742870383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24167966693693171, 0.24167966693693155, 0.30601629362135657], 
reward next is 0.6940, 
noisyNet noise sample is [array([1.2303689], dtype=float32), 0.7543126]. 
=============================================
[2019-03-27 09:21:03,100] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0158353e-18 1.0000000e+00 1.8799743e-26 3.7978128e-18 1.7487277e-28], sum to 1.0000
[2019-03-27 09:21:03,105] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4219
[2019-03-27 09:21:03,110] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 73.66666666666667, 1.0, 2.0, 0.6045686963531289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844848.9323900266, 844848.9323900266, 201600.19291027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3962400.0000, 
sim time next is 3963000.0000, 
raw observation next is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.6032994485344105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843074.5298177738, 843074.5298177738, 201362.2137822671], 
processed observation next is [0.0, 0.8695652173913043, 0.6761453396524489, 0.7433333333333333, 1.0, 1.0, 0.5220475283547115, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23418736939382606, 0.23418736939382606, 0.3005406175854733], 
reward next is 0.6995, 
noisyNet noise sample is [array([-0.6321966], dtype=float32), 1.8544546]. 
=============================================
[2019-03-27 09:21:03,123] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[72.844124]
 [72.830696]
 [72.7815  ]
 [72.76455 ]
 [72.87205 ]], R is [[72.82686615]
 [72.79769897]
 [72.76846313]
 [72.73777008]
 [72.69978333]].
[2019-03-27 09:21:04,123] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.4009025e-20 1.0000000e+00 2.2791121e-26 3.3827688e-19 2.9532020e-29], sum to 1.0000
[2019-03-27 09:21:04,134] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2337
[2019-03-27 09:21:04,141] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 72.5, 1.0, 2.0, 0.6221091242827456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869370.6729557896, 869370.6729557896, 204938.1283424667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3955800.0000, 
sim time next is 3956400.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.6284551505297877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 878242.6375546553, 878242.6375546553, 206167.8005014142], 
processed observation next is [0.0, 0.8260869565217391, 0.7156398104265403, 0.75, 1.0, 1.0, 0.552355603047937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24395628820962648, 0.24395628820962648, 0.3077131350767376], 
reward next is 0.6923, 
noisyNet noise sample is [array([-1.3351457], dtype=float32), 1.8372526]. 
=============================================
[2019-03-27 09:21:13,750] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4023384e-11 9.8334646e-01 9.0127253e-15 1.6653590e-02 9.8683322e-17], sum to 1.0000
[2019-03-27 09:21:13,759] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1736
[2019-03-27 09:21:13,768] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1961741.376642342 W.
[2019-03-27 09:21:13,777] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 66.66666666666667, 1.0, 2.0, 0.7015423713427476, 1.0, 2.0, 0.7015423713427476, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1961741.376642342, 1961741.376642342, 374779.6059521053], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4957800.0000, 
sim time next is 4958400.0000, 
raw observation next is [30.0, 67.33333333333334, 1.0, 2.0, 0.6836575140914573, 1.0, 2.0, 0.6836575140914573, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1911684.861419684, 1911684.861419684, 367201.9628994914], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.6733333333333335, 1.0, 1.0, 0.6188644748089847, 1.0, 1.0, 0.6188644748089847, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5310235726165788, 0.5310235726165788, 0.5480626311932708], 
reward next is 0.4519, 
noisyNet noise sample is [array([0.7324668], dtype=float32), 1.6931303]. 
=============================================
[2019-03-27 09:21:20,440] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 09:21:20,441] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:21:20,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:21:20,445] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:21:20,446] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:21:20,447] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:21:20,448] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:21:20,448] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:21:20,449] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:21:20,450] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:21:20,451] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:21:20,473] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run99
[2019-03-27 09:21:20,494] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run99
[2019-03-27 09:21:20,522] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run99
[2019-03-27 09:21:20,522] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run99
[2019-03-27 09:21:20,566] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run99
[2019-03-27 09:21:24,115] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09344193], dtype=float32), 0.049180277]
[2019-03-27 09:21:24,116] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.20061111, 94.16009848, 1.0, 2.0, 0.2874212505770731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463596.4756893592, 463596.4756893592, 164518.151981447]
[2019-03-27 09:21:24,118] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:21:24,121] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5195713e-20 1.0000000e+00 5.4075452e-27 5.5247829e-22 3.6689318e-29], sampled 0.14869593108417356
[2019-03-27 09:21:29,052] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09344193], dtype=float32), 0.049180277]
[2019-03-27 09:21:29,054] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.45, 53.0, 1.0, 2.0, 0.4549435602090475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731227.5970608975, 731227.5970608975, 187390.4359894523]
[2019-03-27 09:21:29,054] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:21:29,058] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2619526e-19 1.0000000e+00 4.0314305e-26 3.2620355e-20 1.6688450e-28], sampled 0.6686801024451265
[2019-03-27 09:21:50,244] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09344193], dtype=float32), 0.049180277]
[2019-03-27 09:21:50,247] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.8, 85.33333333333334, 1.0, 2.0, 0.4848361860005765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678187.6134226498, 678187.6134226498, 181345.4020141984]
[2019-03-27 09:21:50,248] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:21:50,251] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1039534e-19 1.0000000e+00 3.8168382e-26 3.7530050e-20 1.2272757e-28], sampled 0.16300507300371803
[2019-03-27 09:21:50,684] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09344193], dtype=float32), 0.049180277]
[2019-03-27 09:21:50,688] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.2, 90.0, 1.0, 2.0, 0.4881694224103666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 682135.6141260677, 682135.6141260671, 181762.351672346]
[2019-03-27 09:21:50,690] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:21:50,695] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2647835e-20 1.0000000e+00 3.0856328e-26 1.1846719e-19 5.8523895e-29], sampled 0.170695827666829
[2019-03-27 09:21:54,042] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09344193], dtype=float32), 0.049180277]
[2019-03-27 09:21:54,043] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.85, 76.0, 1.0, 2.0, 0.4210547627735298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 620462.7443576868, 620462.7443576861, 176245.9975404632]
[2019-03-27 09:21:54,043] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:21:54,046] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2147619e-19 1.0000000e+00 9.5352502e-26 8.7301304e-19 1.3423058e-28], sampled 0.8839886923174455
[2019-03-27 09:21:56,889] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09344193], dtype=float32), 0.049180277]
[2019-03-27 09:21:56,890] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.41666666666666, 95.0, 1.0, 2.0, 0.5447611747908881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761241.5150704212, 761241.5150704217, 190906.4087500282]
[2019-03-27 09:21:56,890] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:21:56,895] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.9852537e-19 1.0000000e+00 4.0388981e-25 3.2007886e-18 1.0213131e-27], sampled 0.027310243422486224
[2019-03-27 09:21:57,843] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09344193], dtype=float32), 0.049180277]
[2019-03-27 09:21:57,845] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.83333333333334, 61.0, 1.0, 2.0, 0.730794831635214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1021327.293822946, 1021327.293822945, 227630.6386141516]
[2019-03-27 09:21:57,847] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:21:57,849] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6243470e-18 1.0000000e+00 1.0359269e-24 2.7865971e-17 1.7920980e-27], sampled 0.6895567157928587
[2019-03-27 09:22:09,906] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09344193], dtype=float32), 0.049180277]
[2019-03-27 09:22:09,908] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.96666666666667, 63.66666666666667, 1.0, 2.0, 0.8223683659801277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1149375.772440734, 1149375.772440734, 249477.3060450329]
[2019-03-27 09:22:09,910] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:22:09,914] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.9572219e-18 1.0000000e+00 8.9384670e-24 7.5735757e-16 1.4551553e-26], sampled 0.9420493762369614
[2019-03-27 09:22:38,935] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09344193], dtype=float32), 0.049180277]
[2019-03-27 09:22:38,938] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.07019554833334, 87.11123601666667, 1.0, 2.0, 0.7254410318188251, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976872042454, 6.9112, 168.9123160346773, 1910730.40201304, 1843492.681967545, 390262.6616109449]
[2019-03-27 09:22:38,939] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:22:38,942] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0490065e-11 9.9999380e-01 5.5116095e-16 6.1992528e-06 1.5616857e-19], sampled 0.7277941793920387
[2019-03-27 09:22:38,943] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1910730.40201304 W.
[2019-03-27 09:23:15,368] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 09:23:15,759] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.5742 3163974112.6194 1774.0000
[2019-03-27 09:23:15,802] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0142 3007608811.4011 1766.0000
[2019-03-27 09:23:15,941] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.1725 2779322230.8975 933.0000
[2019-03-27 09:23:16,031] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.2329 2842390253.6608 1130.0000
[2019-03-27 09:23:17,047] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2450000, evaluation results [2450000.0, 7883.5741675810295, 3163974112.6193905, 1774.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8661.172501768742, 2779322230.897455, 933.0, 7999.014205657292, 3007608811.4011106, 1766.0, 8497.232853734016, 2842390253.6607604, 1130.0]
[2019-03-27 09:23:23,712] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5952274e-10 2.5876271e-02 3.6979615e-13 9.7412372e-01 2.0149001e-17], sum to 1.0000
[2019-03-27 09:23:23,723] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8171
[2019-03-27 09:23:23,733] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 61.0, 1.0, 2.0, 0.8679780429700873, 1.0, 2.0, 0.7545790609993064, 1.0, 1.0, 1.03, 7.005110980808289, 6.9112, 170.5573041426782, 3166651.744498963, 3099379.488891526, 579636.1503606191], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4380000.0000, 
sim time next is 4380600.0000, 
raw observation next is [33.5, 62.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.263135961817621, 6.9112, 170.5573041426782, 3161729.438839508, 2909623.393549917, 551784.035250625], 
processed observation next is [1.0, 0.6956521739130435, 0.7867298578199052, 0.62, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.03519359618176212, 0.0, 0.8375144448122397, 0.8782581774554189, 0.8082287204305324, 0.823558261568097], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.07565962], dtype=float32), 0.9055283]. 
=============================================
[2019-03-27 09:23:26,436] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7247753e-19 1.0000000e+00 1.3285642e-24 3.3534636e-18 2.9833796e-28], sum to 1.0000
[2019-03-27 09:23:26,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5056
[2019-03-27 09:23:26,451] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5817448407078488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812941.7405118359, 812941.7405118353, 197394.2623503959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5820091056976335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813311.1710184453, 813311.1710184453, 197442.0592235731], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49639651288871506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2259197697273459, 0.2259197697273459, 0.29468964063219866], 
reward next is 0.7053, 
noisyNet noise sample is [array([-1.8988736], dtype=float32), -0.054815847]. 
=============================================
[2019-03-27 09:23:26,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.62582 ]
 [69.629234]
 [69.62295 ]
 [69.6083  ]
 [69.591   ]], R is [[69.70795441]
 [69.71625519]
 [69.72460175]
 [69.73305511]
 [69.74158478]].
[2019-03-27 09:23:29,378] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8666384e-20 1.0000000e+00 1.4434568e-26 4.5325866e-20 9.6568290e-31], sum to 1.0000
[2019-03-27 09:23:29,386] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0333
[2019-03-27 09:23:29,390] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 79.0, 1.0, 2.0, 0.5792908206771125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809511.1376075802, 809511.1376075802, 196950.6193577511], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4473000.0000, 
sim time next is 4473600.0000, 
raw observation next is [29.33333333333334, 79.0, 1.0, 2.0, 0.5745304247726876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 802856.3628796691, 802856.3628796698, 196096.1862347673], 
processed observation next is [0.0, 0.782608695652174, 0.5892575039494474, 0.79, 1.0, 1.0, 0.48738605394299706, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22301565635546364, 0.22301565635546383, 0.2926808749772646], 
reward next is 0.7073, 
noisyNet noise sample is [array([-0.8681608], dtype=float32), -0.83757484]. 
=============================================
[2019-03-27 09:23:40,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3307659e-19 1.0000000e+00 5.3922666e-25 1.7243008e-17 9.5825347e-28], sum to 1.0000
[2019-03-27 09:23:40,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1583
[2019-03-27 09:23:40,398] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4917503530893301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687140.9878012304, 687140.9878012304, 182313.3237966955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4663200.0000, 
sim time next is 4663800.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4912678879094374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686466.6035000489, 686466.6035000489, 182238.9730745555], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.38706974446920167, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19068516763890248, 0.19068516763890248, 0.271998467275456], 
reward next is 0.7280, 
noisyNet noise sample is [array([-1.3412313], dtype=float32), -2.6198022]. 
=============================================
[2019-03-27 09:23:45,859] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8864508e-18 1.0000000e+00 1.0186421e-23 4.7396216e-16 8.1313856e-27], sum to 1.0000
[2019-03-27 09:23:45,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9060
[2019-03-27 09:23:45,871] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 76.5, 1.0, 2.0, 0.4897629725854023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684363.0528513591, 684363.0528513585, 182007.4195761309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4753800.0000, 
sim time next is 4754400.0000, 
raw observation next is [27.33333333333334, 77.33333333333333, 1.0, 2.0, 0.4898574627576148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684495.1298363131, 684495.1298363131, 182021.8596418193], 
processed observation next is [1.0, 0.0, 0.4944707740916275, 0.7733333333333333, 1.0, 1.0, 0.3853704370573673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19013753606564252, 0.19013753606564252, 0.2716744173758497], 
reward next is 0.7283, 
noisyNet noise sample is [array([-1.0968472], dtype=float32), 0.05542276]. 
=============================================
[2019-03-27 09:23:47,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4994006e-19 1.0000000e+00 4.2906357e-25 2.4293639e-17 1.2348229e-27], sum to 1.0000
[2019-03-27 09:23:47,051] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8655
[2019-03-27 09:23:47,057] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.6599435366153074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 922265.5886796128, 922265.5886796123, 212434.3622953894], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4944600.0000, 
sim time next is 4945200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.8223806614057153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1149392.966333013, 1149392.966333013, 249476.1266131804], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.7860007968743558, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3192758239813925, 0.3192758239813925, 0.37235242778086625], 
reward next is 0.6276, 
noisyNet noise sample is [array([0.90689987], dtype=float32), 0.5204285]. 
=============================================
[2019-03-27 09:23:55,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7657978e-17 1.0000000e+00 1.4078338e-23 9.4277282e-17 1.3531491e-27], sum to 1.0000
[2019-03-27 09:23:55,427] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6411
[2019-03-27 09:23:55,436] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.7493853896952534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1047321.46678686, 1047321.46678686, 231863.7486280597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4934400.0000, 
sim time next is 4935000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.723536433732406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1011178.44088567, 1011178.440885671, 226005.2025475325], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.6669113659426578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2808829002460195, 0.2808829002460197, 0.33732119783213804], 
reward next is 0.6627, 
noisyNet noise sample is [array([-1.7140434], dtype=float32), -1.2394307]. 
=============================================
[2019-03-27 09:23:55,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.94552 ]
 [65.078125]
 [65.19633 ]
 [65.746216]
 [67.59442 ]], R is [[64.8887558 ]
 [64.89380646]
 [64.8897934 ]
 [64.84658051]
 [64.77171326]].
[2019-03-27 09:23:56,274] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9288621e-20 1.0000000e+00 3.2143540e-28 1.9367283e-21 2.1844414e-31], sum to 1.0000
[2019-03-27 09:23:56,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5325
[2019-03-27 09:23:56,289] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.56666666666666, 60.33333333333334, 1.0, 2.0, 0.5521515385808276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771572.4550705901, 771572.4550705901, 192170.8571111246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5761200.0000, 
sim time next is 5761800.0000, 
raw observation next is [32.4, 61.5, 1.0, 2.0, 0.5521724500911104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771601.6872731918, 771601.687273191, 192174.6469133919], 
processed observation next is [0.0, 0.6956521739130435, 0.7345971563981042, 0.615, 1.0, 1.0, 0.46044873504953054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21433380202033103, 0.21433380202033084, 0.2868278312140177], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.68916947], dtype=float32), 0.97000265]. 
=============================================
[2019-03-27 09:23:58,487] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.07175394e-10 9.99958038e-01 2.41491528e-15 4.19938806e-05
 5.14233289e-18], sum to 1.0000
[2019-03-27 09:23:58,497] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4941
[2019-03-27 09:23:58,503] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2022787.671103751 W.
[2019-03-27 09:23:58,506] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 68.0, 1.0, 2.0, 0.4822351147304861, 1.0, 1.0, 0.4822351147304861, 1.0, 2.0, 0.8270291853100695, 6.9112, 6.9112, 170.5573041426782, 2022787.671103751, 2022787.671103751, 401255.5540680718], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4962600.0000, 
sim time next is 4963200.0000, 
raw observation next is [30.0, 67.33333333333334, 1.0, 2.0, 0.4607866888966353, 1.0, 2.0, 0.4607866888966353, 1.0, 2.0, 0.7896582725924677, 6.9112, 6.9112, 170.5573041426782, 1932738.761801026, 1932738.761801026, 387335.9416063391], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.6733333333333335, 1.0, 1.0, 0.35034540830919914, 1.0, 1.0, 0.35034540830919914, 1.0, 1.0, 0.7434856982834972, 0.0, 0.0, 0.8375144448122397, 0.5368718782780628, 0.5368718782780628, 0.5781133456811032], 
reward next is 0.4219, 
noisyNet noise sample is [array([0.33236974], dtype=float32), 1.154735]. 
=============================================
[2019-03-27 09:24:02,084] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.4295205e-12 9.9873573e-01 3.5460719e-14 1.2642895e-03 1.5567251e-19], sum to 1.0000
[2019-03-27 09:24:02,097] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4545
[2019-03-27 09:24:02,108] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2771873.116347813 W.
[2019-03-27 09:24:02,114] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.7, 62.33333333333334, 1.0, 2.0, 0.9909177366037719, 1.0, 2.0, 0.9909177366037719, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2771873.116347813, 2771873.116347812, 523519.8436101609], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5836800.0000, 
sim time next is 5837400.0000, 
raw observation next is [32.75, 62.0, 1.0, 2.0, 0.7827706300202606, 1.0, 2.0, 0.7119753545243929, 1.0, 1.0, 1.03, 7.005104259451008, 6.9112, 170.5573041426782, 2987648.016298839, 2920380.57547303, 548745.6279804184], 
processed observation next is [1.0, 0.5652173913043478, 0.7511848341232228, 0.62, 1.0, 1.0, 0.7382778674942898, 1.0, 1.0, 0.6529823548486661, 1.0, 0.5, 1.0365853658536586, 0.009390425945100755, 0.0, 0.8375144448122397, 0.8299022267496775, 0.8112168265202861, 0.8190233253439081], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.3241076], dtype=float32), 0.0060705543]. 
=============================================
[2019-03-27 09:24:03,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.08032232e-18 1.00000000e+00 6.73973273e-22 1.01170184e-13
 2.25318815e-25], sum to 1.0000
[2019-03-27 09:24:03,669] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3960
[2019-03-27 09:24:03,674] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 82.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.151427281635259, 6.9112, 168.9114775842434, 1624295.654549244, 1453871.645141601, 311354.6066453936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5467200.0000, 
sim time next is 5467800.0000, 
raw observation next is [29.7, 81.83333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.398970201970895, 6.9112, 168.9098758912226, 1800026.595570599, 1453991.937601569, 311354.7977091381], 
processed observation next is [1.0, 0.2608695652173913, 0.6066350710900474, 0.8183333333333332, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.048777020197089536, 0.0, 0.8294248179016759, 0.5000073876584997, 0.4038866493337692, 0.464708653297221], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.54972285], dtype=float32), -1.5633881]. 
=============================================
[2019-03-27 09:24:08,686] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9232165e-18 1.0000000e+00 5.8988936e-23 2.8030583e-17 7.0126075e-27], sum to 1.0000
[2019-03-27 09:24:08,694] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4234
[2019-03-27 09:24:08,701] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 93.16666666666667, 1.0, 2.0, 0.7470131953901096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1044004.520233728, 1044004.520233727, 231319.4086159989], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5976600.0000, 
sim time next is 5977200.0000, 
raw observation next is [26.03333333333333, 93.33333333333334, 1.0, 2.0, 0.6938547687521955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 969677.8877956612, 969677.8877956606, 219523.5685585835], 
processed observation next is [1.0, 0.17391304347826086, 0.4328593996840442, 0.9333333333333335, 1.0, 1.0, 0.631150323797826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2693549688321281, 0.26935496883212795, 0.32764711725161716], 
reward next is 0.6724, 
noisyNet noise sample is [array([-0.8333154], dtype=float32), -0.12402058]. 
=============================================
[2019-03-27 09:24:08,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.9612887e-21 1.0000000e+00 6.3043841e-27 1.2520829e-21 3.8334797e-30], sum to 1.0000
[2019-03-27 09:24:08,757] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9111
[2019-03-27 09:24:08,764] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.5363698225970476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749511.4219028318, 749511.4219028311, 189490.8007199865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5163600.0000, 
sim time next is 5164200.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5340302231083116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746240.9685164052, 746240.9685164046, 189099.758521216], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.7, 1.0, 1.0, 0.43859063025097783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20728915792122365, 0.20728915792122352, 0.2822384455540537], 
reward next is 0.7178, 
noisyNet noise sample is [array([-0.5595881], dtype=float32), 1.3766532]. 
=============================================
[2019-03-27 09:24:08,851] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6399557e-21 1.0000000e+00 1.9699548e-27 4.5529437e-22 1.1793582e-30], sum to 1.0000
[2019-03-27 09:24:08,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1871
[2019-03-27 09:24:08,864] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5227833462935365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730519.4496325778, 730519.4496325784, 187242.60886876], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5168400.0000, 
sim time next is 5169000.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5220866714079402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729545.6058289391, 729545.6058289384, 187128.7836591793], 
processed observation next is [0.0, 0.8260869565217391, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.4242008089252291, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20265155717470532, 0.20265155717470512, 0.2792966920286258], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.2677563], dtype=float32), 0.082208574]. 
=============================================
[2019-03-27 09:24:08,878] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.60511]
 [74.59397]
 [74.56827]
 [74.53935]
 [74.50827]], R is [[74.58262634]
 [74.5573349 ]
 [74.53213501]
 [74.50681305]
 [74.48022461]].
[2019-03-27 09:24:10,467] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.0035793e-20 1.0000000e+00 6.3637995e-27 5.3595245e-22 2.5407440e-30], sum to 1.0000
[2019-03-27 09:24:10,479] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8917
[2019-03-27 09:24:10,489] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5225737880560548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730226.5195145251, 730226.5195145257, 187208.2098061929], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5174400.0000, 
sim time next is 5175000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5220775251164347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729532.8207331125, 729532.8207331131, 187127.2038881737], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4241897892969092, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20264800575919792, 0.20264800575919809, 0.2792943341614533], 
reward next is 0.7207, 
noisyNet noise sample is [array([-1.5046202], dtype=float32), -0.6447764]. 
=============================================
[2019-03-27 09:24:10,498] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.321106]
 [73.36044 ]
 [73.38212 ]
 [73.36878 ]
 [73.40528 ]], R is [[73.27336884]
 [73.26121521]
 [73.2489624 ]
 [73.23699188]
 [73.22535706]].
[2019-03-27 09:24:10,883] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 09:24:10,884] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:24:10,885] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:24:10,886] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:24:10,887] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:24:10,887] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:24:10,888] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:24:10,887] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:24:10,888] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:24:10,894] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:24:10,895] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:24:10,916] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run100
[2019-03-27 09:24:10,943] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run100
[2019-03-27 09:24:10,964] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run100
[2019-03-27 09:24:10,965] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run100
[2019-03-27 09:24:11,006] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run100
[2019-03-27 09:24:13,117] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09239335], dtype=float32), 0.049105518]
[2019-03-27 09:24:13,119] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.45, 88.0, 1.0, 2.0, 0.3473432723090848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539420.6966323056, 539420.6966323056, 169890.0225572612]
[2019-03-27 09:24:13,120] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 09:24:13,121] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.2787948e-21 1.0000000e+00 1.5244539e-27 5.2973668e-22 3.4684956e-30], sampled 0.24948472169126146
[2019-03-27 09:24:37,780] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09239335], dtype=float32), 0.049105518]
[2019-03-27 09:24:37,781] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.01666666666667, 90.33333333333333, 1.0, 2.0, 0.3742811289533238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566654.7085138375, 566654.7085138375, 171784.4228135262]
[2019-03-27 09:24:37,782] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:24:37,786] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8753315e-20 1.0000000e+00 5.2929297e-27 7.0891741e-22 2.1696445e-29], sampled 0.08737943249384961
[2019-03-27 09:24:40,358] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09239335], dtype=float32), 0.049105518]
[2019-03-27 09:24:40,359] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.49171078, 89.38106163, 1.0, 2.0, 0.4777858964424099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679819.6497305314, 679819.6497305308, 181751.7135111067]
[2019-03-27 09:24:40,363] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:24:40,367] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6392036e-20 1.0000000e+00 5.4344697e-27 3.9189604e-21 9.7503732e-30], sampled 0.29866655397888897
[2019-03-27 09:25:28,355] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.09239335], dtype=float32), 0.049105518]
[2019-03-27 09:25:28,357] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [37.47255359833333, 60.74726015666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.100534609106591, 6.9112, 168.9117896856631, 2418121.079816409, 2283801.519587165, 475733.5704408539]
[2019-03-27 09:25:28,357] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:25:28,360] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.259385e-13 1.000000e+00 2.941415e-17 3.186077e-08 5.952907e-20], sampled 0.1111906438449316
[2019-03-27 09:25:28,361] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2418121.079816409 W.
[2019-03-27 09:25:29,605] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09239335], dtype=float32), 0.049105518]
[2019-03-27 09:25:29,607] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.96066966833333, 91.97851717333333, 1.0, 2.0, 0.8625558579068295, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005985432654507, 6.9112, 168.9123211249837, 2102626.365600275, 2035382.570358539, 423874.2371061676]
[2019-03-27 09:25:29,611] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:25:29,615] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4248320e-13 1.0000000e+00 4.0117031e-18 1.8599115e-08 7.4238418e-21], sampled 0.5581986688581729
[2019-03-27 09:25:29,616] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2102626.365600275 W.
[2019-03-27 09:25:36,157] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09239335], dtype=float32), 0.049105518]
[2019-03-27 09:25:36,158] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.28333333333334, 52.0, 1.0, 2.0, 0.8887295742691821, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005987070577932, 6.9112, 168.9123159644963, 2139261.145901559, 2072016.190719362, 430953.5988850113]
[2019-03-27 09:25:36,160] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:25:36,163] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.5988871e-13 9.9999940e-01 3.1002044e-17 5.8036738e-07 6.3969164e-21], sampled 0.11937573690851011
[2019-03-27 09:25:36,164] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2139261.145901559 W.
[2019-03-27 09:26:03,456] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.09239335], dtype=float32), 0.049105518]
[2019-03-27 09:26:03,459] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.97699288, 63.58277322, 1.0, 2.0, 0.8919089392227681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1246625.588185843, 1246625.588185842, 267721.1042598347]
[2019-03-27 09:26:03,463] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:26:03,465] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.7905826e-19 1.0000000e+00 4.3497494e-25 6.3517235e-19 1.5907906e-27], sampled 0.9815029414131067
[2019-03-27 09:26:05,604] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0886 2927301717.1279 1338.0000
[2019-03-27 09:26:05,747] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9878 2779131868.1217 933.0000
[2019-03-27 09:26:06,214] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0344 2842517336.0167 1131.0000
[2019-03-27 09:26:06,294] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9780 3007633090.4407 1766.0000
[2019-03-27 09:26:06,329] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.7506 3164198622.8888 1777.0000
[2019-03-27 09:26:07,346] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2475000, evaluation results [2475000.0, 7882.750603703659, 3164198622.8888054, 1777.0, 8255.088644432615, 2927301717.127854, 1338.0, 8659.98775396918, 2779131868.121704, 933.0, 7998.9779682847975, 3007633090.440681, 1766.0, 8496.034359275884, 2842517336.0166836, 1131.0]
[2019-03-27 09:26:08,941] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7678246e-19 1.0000000e+00 6.0228754e-26 5.6333101e-18 4.9524253e-28], sum to 1.0000
[2019-03-27 09:26:08,952] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0461
[2019-03-27 09:26:08,959] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 83.83333333333333, 1.0, 2.0, 0.5586093919426904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780599.9272565171, 780599.9272565171, 193288.5829771732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5269800.0000, 
sim time next is 5270400.0000, 
raw observation next is [28.5, 84.0, 1.0, 2.0, 0.5593961494088633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781699.7455093674, 781699.7455093674, 193425.619918706], 
processed observation next is [1.0, 0.0, 0.5497630331753555, 0.84, 1.0, 1.0, 0.4691519872395943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2171388181970465, 0.2171388181970465, 0.2886949551025463], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.12346859], dtype=float32), -1.9989352]. 
=============================================
[2019-03-27 09:26:11,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.8783887e-17 1.0000000e+00 4.0462602e-22 3.5850060e-15 7.3492661e-26], sum to 1.0000
[2019-03-27 09:26:11,239] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4675
[2019-03-27 09:26:11,246] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.51666666666667, 84.33333333333333, 1.0, 2.0, 0.563889616088756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787981.2414684966, 787981.2414684966, 194211.5965354411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5271000.0000, 
sim time next is 5271600.0000, 
raw observation next is [28.53333333333334, 84.66666666666667, 1.0, 2.0, 0.5661469796991381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791136.8637080308, 791136.8637080308, 194608.7906780089], 
processed observation next is [1.0, 0.0, 0.5513428120063194, 0.8466666666666667, 1.0, 1.0, 0.4772855177098049, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21976023991889743, 0.21976023991889743, 0.2904608816089685], 
reward next is 0.7095, 
noisyNet noise sample is [array([-0.7738595], dtype=float32), -1.0724006]. 
=============================================
[2019-03-27 09:26:12,352] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.40907155e-16 1.00000000e+00 2.56193244e-22 3.26641790e-13
 1.09793716e-23], sum to 1.0000
[2019-03-27 09:26:12,358] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1626
[2019-03-27 09:26:12,365] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333333, 85.33333333333334, 1.0, 2.0, 0.9159688187998503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1280274.478123823, 1280274.478123823, 274372.9186013892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5294400.0000, 
sim time next is 5295000.0000, 
raw observation next is [29.51666666666667, 84.66666666666666, 1.0, 2.0, 0.9259737756808779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1294267.201793007, 1294267.201793007, 277187.3852235004], 
processed observation next is [1.0, 0.2608695652173913, 0.5979462875197474, 0.8466666666666666, 1.0, 1.0, 0.9108117779287686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3595186671647242, 0.3595186671647242, 0.4137125152589558], 
reward next is 0.5863, 
noisyNet noise sample is [array([1.1219424], dtype=float32), 1.1251321]. 
=============================================
[2019-03-27 09:26:12,377] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[57.86965 ]
 [57.88336 ]
 [57.891182]
 [58.088333]
 [58.95752 ]], R is [[58.14627075]
 [58.15529633]
 [58.16738892]
 [58.16875839]
 [58.12409592]].
[2019-03-27 09:26:14,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4875692e-18 1.0000000e+00 3.9097859e-24 3.4188143e-15 5.6090293e-28], sum to 1.0000
[2019-03-27 09:26:14,518] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3165
[2019-03-27 09:26:14,525] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6245572213472688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872793.1887043887, 872793.1887043887, 205411.2944045779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5341200.0000, 
sim time next is 5341800.0000, 
raw observation next is [31.75, 76.5, 1.0, 2.0, 0.6264332755560198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 875415.9782338598, 875415.9782338605, 205774.8840607514], 
processed observation next is [1.0, 0.8260869565217391, 0.7037914691943128, 0.765, 1.0, 1.0, 0.5499196091036382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24317110506496104, 0.24317110506496123, 0.3071266926279872], 
reward next is 0.6929, 
noisyNet noise sample is [array([1.1717428], dtype=float32), -0.3838284]. 
=============================================
[2019-03-27 09:26:14,931] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9226797e-11 9.7315782e-01 1.0604560e-13 2.6842112e-02 5.0333452e-18], sum to 1.0000
[2019-03-27 09:26:14,942] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0437
[2019-03-27 09:26:14,950] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2507252.821038588 W.
[2019-03-27 09:26:14,954] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.3, 52.0, 1.0, 2.0, 0.8964133751278917, 1.0, 2.0, 0.8964133751278917, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2507252.821038588, 2507252.821038588, 469547.7604302666], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5490000.0000, 
sim time next is 5490600.0000, 
raw observation next is [36.41666666666666, 51.0, 1.0, 2.0, 0.5404989358147123, 1.0, 2.0, 0.5404989358147123, 1.0, 1.0, 0.9386679057806865, 6.9112, 6.9112, 170.5573041426782, 2267431.988318386, 2267431.988318386, 444283.0547159095], 
processed observation next is [1.0, 0.5652173913043478, 0.9249605055292255, 0.51, 1.0, 1.0, 0.44638426001772563, 1.0, 1.0, 0.44638426001772563, 1.0, 0.5, 0.9252047631471785, 0.0, 0.0, 0.8375144448122397, 0.6298422189773295, 0.6298422189773295, 0.6631090368894171], 
reward next is 0.3369, 
noisyNet noise sample is [array([0.512924], dtype=float32), -0.31374517]. 
=============================================
[2019-03-27 09:26:23,163] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.92085908e-20 1.00000000e+00 3.38841366e-26 1.65734974e-20
 1.21365725e-29], sum to 1.0000
[2019-03-27 09:26:23,174] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7341
[2019-03-27 09:26:23,182] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.78333333333333, 64.5, 1.0, 2.0, 0.5249184820566064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733504.0449582271, 733504.0449582271, 187592.5416832907], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6267000.0000, 
sim time next is 6267600.0000, 
raw observation next is [30.8, 64.0, 1.0, 2.0, 0.5226111962812262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730278.8104418882, 730278.8104418882, 187214.7367163221], 
processed observation next is [0.0, 0.5652173913043478, 0.6587677725118484, 0.64, 1.0, 1.0, 0.42483276660388697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2028552251227467, 0.2028552251227467, 0.27942498017361506], 
reward next is 0.7206, 
noisyNet noise sample is [array([0.46818915], dtype=float32), -1.2909954]. 
=============================================
[2019-03-27 09:26:32,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5903846e-20 1.0000000e+00 6.7345721e-28 1.7546200e-22 1.4072596e-30], sum to 1.0000
[2019-03-27 09:26:32,310] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9315
[2019-03-27 09:26:32,319] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 70.5, 1.0, 2.0, 0.5158433814178596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720818.4913110913, 720818.4913110913, 186115.4075364385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6367800.0000, 
sim time next is 6368400.0000, 
raw observation next is [28.9, 72.0, 1.0, 2.0, 0.5143807941019103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718774.0400198154, 718774.0400198147, 185879.6017012107], 
processed observation next is [0.0, 0.7391304347826086, 0.5687203791469194, 0.72, 1.0, 1.0, 0.4149166193998919, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19965945556105985, 0.19965945556105966, 0.2774322413450906], 
reward next is 0.7226, 
noisyNet noise sample is [array([-1.5332046], dtype=float32), 0.67254585]. 
=============================================
[2019-03-27 09:26:45,995] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.27445798e-10 9.96671557e-01 1.01248755e-13 3.32841114e-03
 1.56730036e-18], sum to 1.0000
[2019-03-27 09:26:46,003] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3679
[2019-03-27 09:26:46,012] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2127022.625287001 W.
[2019-03-27 09:26:46,016] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.76666666666667, 75.33333333333334, 1.0, 2.0, 0.760590318582893, 1.0, 2.0, 0.760590318582893, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2127022.625287001, 2127022.625287001, 401143.012026643], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5926800.0000, 
sim time next is 5927400.0000, 
raw observation next is [29.8, 75.5, 1.0, 2.0, 0.8532265734456099, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.000469738820355, 6.9112, 168.9124252280908, 2089568.674066606, 2026237.848115383, 421621.8323155305], 
processed observation next is [1.0, 0.6086956521739131, 0.6113744075829385, 0.755, 1.0, 1.0, 0.8231645463200119, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008926973882035494, 0.0, 0.8294373363127902, 0.5804357427962794, 0.5628438466987175, 0.6292863168888515], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.33874804], dtype=float32), 0.7918964]. 
=============================================
[2019-03-27 09:26:48,596] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0296119e-16 1.0000000e+00 2.8579127e-21 7.0169226e-14 5.4520658e-24], sum to 1.0000
[2019-03-27 09:26:48,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0576
[2019-03-27 09:26:48,614] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.18333333333333, 87.16666666666667, 1.0, 2.0, 0.9108189792333158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1273072.091990388, 1273072.091990388, 272930.8928384309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5811000.0000, 
sim time next is 5811600.0000, 
raw observation next is [27.36666666666667, 86.33333333333334, 1.0, 2.0, 0.8800687012622707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1230066.84180666, 1230066.841806659, 264513.9030617427], 
processed observation next is [1.0, 0.2608695652173913, 0.49605055292259104, 0.8633333333333334, 1.0, 1.0, 0.8555044593521333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34168523383518334, 0.34168523383518307, 0.394796870241407], 
reward next is 0.6052, 
noisyNet noise sample is [array([-0.7683177], dtype=float32), -1.1774492]. 
=============================================
[2019-03-27 09:26:52,394] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5751092e-19 1.0000000e+00 1.6055020e-26 1.4312853e-17 8.4413076e-29], sum to 1.0000
[2019-03-27 09:26:52,405] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2943
[2019-03-27 09:26:52,413] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 87.0, 1.0, 2.0, 0.5506779376651988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769512.5102089676, 769512.510208967, 191916.9768486873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5866800.0000, 
sim time next is 5867400.0000, 
raw observation next is [27.56666666666667, 87.0, 1.0, 2.0, 0.5481333624718536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765955.4611601174, 765955.4611601168, 191481.0370986562], 
processed observation next is [1.0, 0.9130434782608695, 0.505529225908373, 0.87, 1.0, 1.0, 0.45558236442392, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2127654058778104, 0.21276540587781023, 0.2857925926845615], 
reward next is 0.7142, 
noisyNet noise sample is [array([-1.8274326], dtype=float32), 0.033833016]. 
=============================================
[2019-03-27 09:26:55,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1648379e-17 1.0000000e+00 9.7591529e-23 1.0576377e-15 2.0301884e-25], sum to 1.0000
[2019-03-27 09:26:55,994] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6231
[2019-03-27 09:26:55,998] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 91.0, 1.0, 2.0, 0.6757427520560813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 944354.6768828112, 944354.6768828105, 215696.0613666146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6067800.0000, 
sim time next is 6068400.0000, 
raw observation next is [26.63333333333333, 90.33333333333333, 1.0, 2.0, 0.6734618237490946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 941165.6530311353, 941165.6530311353, 215220.992789946], 
processed observation next is [1.0, 0.21739130434782608, 0.46129541864139006, 0.9033333333333333, 1.0, 1.0, 0.6065805105410778, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2614349036197598, 0.2614349036197598, 0.3212253623730537], 
reward next is 0.6788, 
noisyNet noise sample is [array([1.0773083], dtype=float32), -1.4383918]. 
=============================================
[2019-03-27 09:26:56,024] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3917535e-17 1.0000000e+00 4.9264755e-22 2.2051423e-12 1.0856410e-26], sum to 1.0000
[2019-03-27 09:26:56,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0020
[2019-03-27 09:26:56,041] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 78.0, 1.0, 2.0, 0.5008950370426334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699923.4032296151, 699923.4032296158, 183738.8669419958], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6198000.0000, 
sim time next is 6198600.0000, 
raw observation next is [28.58333333333333, 78.5, 1.0, 2.0, 0.5156090711698701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720490.9646937236, 720490.964693723, 186079.3352286299], 
processed observation next is [1.0, 0.7391304347826086, 0.5537124802527644, 0.785, 1.0, 1.0, 0.4163964712890001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20013637908158988, 0.20013637908158974, 0.2777303510875073], 
reward next is 0.7223, 
noisyNet noise sample is [array([-0.43937904], dtype=float32), 0.23077524]. 
=============================================
[2019-03-27 09:26:58,684] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.2861925e-10 9.8925835e-01 3.6948881e-13 1.0741606e-02 1.4060717e-17], sum to 1.0000
[2019-03-27 09:26:58,698] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3518
[2019-03-27 09:26:58,709] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2350448.087629128 W.
[2019-03-27 09:26:58,716] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.95, 76.33333333333334, 1.0, 2.0, 0.5602693004287856, 1.0, 2.0, 0.5602693004287856, 1.0, 2.0, 0.973002490955845, 6.911200000000001, 6.9112, 170.5573041426782, 2350448.087629128, 2350448.087629128, 459325.3061944837], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5929800.0000, 
sim time next is 5930400.0000, 
raw observation next is [30.0, 76.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.447265094829202, 6.9112, 168.9100059726377, 2664324.912158622, 2284028.527007465, 474909.4232683441], 
processed observation next is [1.0, 0.6521739130434783, 0.6208530805687204, 0.7666666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.05360650948292021, 0.0, 0.8294254566609754, 0.740090253377395, 0.6344523686131847, 0.7088200347288718], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7879238], dtype=float32), -1.5274976]. 
=============================================
[2019-03-27 09:27:01,910] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0194340e-20 1.0000000e+00 5.9647770e-26 5.1926160e-19 2.4943892e-29], sum to 1.0000
[2019-03-27 09:27:01,920] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4843
[2019-03-27 09:27:01,926] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.85, 84.33333333333333, 1.0, 2.0, 0.538561888943381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752575.652433318, 752575.6524333173, 189858.779793614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6034200.0000, 
sim time next is 6034800.0000, 
raw observation next is [27.8, 84.66666666666667, 1.0, 2.0, 0.5383256757850722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752245.455993728, 752245.4559937286, 189819.0953174918], 
processed observation next is [1.0, 0.8695652173913043, 0.5165876777251186, 0.8466666666666667, 1.0, 1.0, 0.443765874439846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2089570711093689, 0.20895707110936906, 0.2833120825634206], 
reward next is 0.7167, 
noisyNet noise sample is [array([1.3403864], dtype=float32), 0.10588177]. 
=============================================
[2019-03-27 09:27:06,968] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4721608e-22 1.0000000e+00 3.1600430e-29 1.8390626e-24 2.6292450e-32], sum to 1.0000
[2019-03-27 09:27:06,979] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5247
[2019-03-27 09:27:06,985] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 46.0, 1.0, 2.0, 0.3207212474971916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503496.4204848484, 503496.4204848491, 167206.9356454084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6885000.0000, 
sim time next is 6885600.0000, 
raw observation next is [29.26666666666667, 47.0, 1.0, 2.0, 0.3275308529621662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512018.8357576714, 512018.8357576714, 167803.7432995287], 
processed observation next is [0.0, 0.6956521739130435, 0.5860979462875199, 0.47, 1.0, 1.0, 0.18979620838815206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14222745437713094, 0.14222745437713094, 0.25045334820825177], 
reward next is 0.7495, 
noisyNet noise sample is [array([1.920738], dtype=float32), -0.551299]. 
=============================================
[2019-03-27 09:27:12,262] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 09:27:12,266] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 09:27:12,268] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:27:12,269] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 09:27:12,270] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:27:12,270] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 09:27:12,271] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 09:27:12,273] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:27:12,273] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 09:27:12,274] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:27:12,274] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 09:27:13,013] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run101
[2019-03-27 09:27:13,573] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run101
[2019-03-27 09:27:13,712] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run101
[2019-03-27 09:27:13,938] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run101
[2019-03-27 09:27:14,103] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/12/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run101
[2019-03-27 09:27:26,277] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09068068], dtype=float32), 0.04913837]
[2019-03-27 09:27:26,278] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.75, 73.5, 1.0, 2.0, 0.282610234155593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 455831.2377968682, 455831.2377968676, 163990.7953350865]
[2019-03-27 09:27:26,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:27:26,284] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8857605e-21 1.0000000e+00 4.5033211e-28 6.0692165e-24 3.2904513e-30], sampled 0.8127256212104513
[2019-03-27 09:28:05,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09068068], dtype=float32), 0.04913837]
[2019-03-27 09:28:05,017] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.69089758333333, 59.065170605, 1.0, 2.0, 0.6139990566709823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858032.6252735242, 858032.6252735242, 203383.443682475]
[2019-03-27 09:28:05,023] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 09:28:05,026] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.5892707e-21 1.0000000e+00 1.1876444e-27 3.7773174e-22 2.2394121e-30], sampled 0.44900144836865463
[2019-03-27 09:28:19,669] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.09068068], dtype=float32), 0.04913837]
[2019-03-27 09:28:19,671] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.55379211166667, 91.96969407833333, 1.0, 2.0, 0.6278718605711122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 877427.1748871437, 877427.1748871437, 206055.0874711339]
[2019-03-27 09:28:19,672] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 09:28:19,674] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.1551175e-21 1.0000000e+00 1.2145802e-27 9.3760943e-22 1.7308532e-30], sampled 0.5021178377949796
[2019-03-27 09:28:20,043] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09068068], dtype=float32), 0.04913837]
[2019-03-27 09:28:20,044] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.35, 47.0, 1.0, 2.0, 0.8890364108468268, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.991021670721251, 6.9112, 168.9124176945685, 2139690.624573708, 2083062.564752733, 431593.6445966523]
[2019-03-27 09:28:20,044] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 09:28:20,046] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4271272e-14 1.0000000e+00 7.8734587e-19 1.2943248e-09 1.3546348e-22], sampled 0.8656110224405695
[2019-03-27 09:28:20,049] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2139690.624573708 W.
[2019-03-27 09:29:21,819] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 09:29:22,653] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09068068], dtype=float32), 0.04913837]
[2019-03-27 09:29:22,654] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.06666666666667, 70.0, 1.0, 2.0, 0.3477934602638195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540063.777060705, 540063.777060705, 169941.1604578018]
[2019-03-27 09:29:22,655] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 09:29:22,658] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8331314e-21 1.0000000e+00 4.9855210e-28 9.5957593e-24 3.0058120e-30], sampled 0.3074156462912475
[2019-03-27 09:29:22,677] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 09:29:22,778] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6674 3164200272.3547 1778.0000
[2019-03-27 09:29:22,817] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1803 3007736924.2222 1766.0000
[2019-03-27 09:29:22,967] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6842 2927277501.7680 1338.0000
[2019-03-27 09:29:23,985] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2500000, evaluation results [2500000.0, 7882.667391253242, 3164200272.354699, 1778.0, 8253.684237189153, 2927277501.7679873, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.180259296095, 3007736924.2222114, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
