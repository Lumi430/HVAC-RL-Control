Using TensorFlow backend.
[2019-03-27 10:55:51,878] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=5e-05, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-03-27 10:55:51,878] A3C_AGENT_MAIN INFO:Start compiling...
2019-03-27 10:55:51.915936: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-03-27 10:56:07,915] A3C_AGENT_MAIN INFO:Start the learning...
[2019-03-27 10:56:07,916] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-03-27 10:56:07,924] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-03-27 10:56:07,929] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-03-27 10:56:07,933] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-03-27 10:56:07,938] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-03-27 10:56:07,941] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-03-27 10:56:07,941] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:07,942] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-03-27 10:56:08,000] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:08,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-03-27 10:56:08,943] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:08,945] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-03-27 10:56:09,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:09,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-03-27 10:56:09,185] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 10:56:09,185] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 10:56:09,186] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 10:56:09,186] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:09,186] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 10:56:09,187] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:09,187] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:09,187] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 10:56:09,188] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:09,188] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 10:56:09,189] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:09,193] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run1
[2019-03-27 10:56:09,193] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run1
[2019-03-27 10:56:09,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run1
[2019-03-27 10:56:09,212] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run1
[2019-03-27 10:56:09,222] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run1
[2019-03-27 10:56:09,945] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:09,946] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-03-27 10:56:10,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:10,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-03-27 10:56:10,947] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:10,948] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-03-27 10:56:11,015] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:11,016] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-03-27 10:56:11,949] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:11,950] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-03-27 10:56:12,018] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:12,019] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-03-27 10:56:12,951] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:12,952] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-03-27 10:56:13,022] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:13,023] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-03-27 10:56:13,953] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:13,954] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-03-27 10:56:14,044] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:14,045] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-03-27 10:56:14,955] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:14,956] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-03-27 10:56:15,029] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:15,030] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-03-27 10:56:15,957] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:15,958] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-03-27 10:56:16,032] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:16,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-03-27 10:56:16,495] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 10:56:16,496] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.8, 58.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5091764227390839, 6.911200000000001, 6.9112, 168.912956510431, 450426.3930366848, 450426.3930366842, 153170.104678214]
[2019-03-27 10:56:16,497] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 10:56:16,499] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.11486838 0.23355809 0.18212928 0.2949011  0.17454316], sampled 0.44499884193056827
[2019-03-27 10:56:16,959] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:16,960] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-03-27 10:56:17,036] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:17,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-03-27 10:56:17,961] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:17,962] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-03-27 10:56:18,027] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:18,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-03-27 10:56:18,963] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:18,964] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-03-27 10:56:19,023] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:19,024] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-03-27 10:56:19,965] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:19,966] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-03-27 10:56:20,050] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:20,050] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-03-27 10:56:20,967] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:20,970] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-03-27 10:56:21,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:21,034] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-03-27 10:56:21,971] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:21,972] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-03-27 10:56:22,069] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:22,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-03-27 10:56:22,973] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-27 10:56:22,974] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-03-27 10:56:23,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:56:23,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-03-27 10:57:06,646] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 10:57:06,646] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.25, 57.0, 1.0, 2.0, 0.5320095632642428, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743416.3590471958, 743416.3590471958, 188762.6913762711]
[2019-03-27 10:57:06,646] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 10:57:06,649] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.13148376 0.30553964 0.20028293 0.21458279 0.14811084], sampled 0.8767573911181611
[2019-03-27 10:57:10,052] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 10:57:10,053] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.73333333333333, 83.33333333333334, 1.0, 2.0, 0.4089776248950214, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6970734370488211, 6.911200000000001, 6.9112, 168.912956510431, 1143204.49982137, 1143204.49982137, 261541.0316413589]
[2019-03-27 10:57:10,053] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 10:57:10,058] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.08948571 0.23562454 0.17391062 0.26705807 0.23392108], sampled 0.732415168473568
[2019-03-27 10:57:31,912] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 10:57:31,913] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.50108832333333, 73.48737437666666, 1.0, 2.0, 0.2437558280358666, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4104795835109142, 6.911200000000002, 6.9112, 168.912956510431, 681216.2003117409, 681216.2003117396, 204618.0136008176]
[2019-03-27 10:57:31,914] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 10:57:31,917] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.09261948 0.2847226  0.25544655 0.21776325 0.14944808], sampled 0.3179384545807311
[2019-03-27 10:57:43,972] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 10:57:43,972] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.82406073, 87.89447033333333, 1.0, 2.0, 0.2689058626217317, 1.0, 2.0, 0.2689058626217317, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 751520.9438468587, 751520.9438468587, 244635.5795837347]
[2019-03-27 10:57:43,973] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 10:57:43,977] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.10414471 0.27156743 0.21752411 0.23211358 0.17465025], sampled 0.8841240513754886
[2019-03-27 10:58:18,866] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-27 10:58:18,867] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.01666666666667, 63.5, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 506071.4312431387, 506071.4312431387, 234948.6791977551]
[2019-03-27 10:58:18,868] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 10:58:18,872] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.10011182 0.2637999  0.21911778 0.2506876  0.1662829 ], sampled 0.2736083209807366
[2019-03-27 10:58:19,132] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3806.2456 3163874556.7323 711.0000
[2019-03-27 10:58:19,531] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 3813.0823 3124690211.0572 578.0000
[2019-03-27 10:58:19,797] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 3786.2080 3303035222.7812 1078.0000
[2019-03-27 10:58:19,813] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 3759.7781 3247503729.3319 854.0000
[2019-03-27 10:58:19,917] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3617.0632 3460116189.9180 1285.0000
[2019-03-27 10:58:20,934] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3617.0632447809367, 3460116189.9180317, 1285.0, 3759.7780712372687, 3247503729.3319225, 854.0, 3813.082309647565, 3124690211.057214, 578.0, 3786.2079563277844, 3303035222.781221, 1078.0, 3806.2456340764543, 3163874556.7322507, 711.0]
[2019-03-27 10:58:23,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.21132304 0.25834587 0.15999714 0.19060709 0.17972687], sum to 1.0000
[2019-03-27 10:58:23,027] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2622
[2019-03-27 10:58:23,120] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.55, 84.83333333333333, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2587586620815616, 6.911199999999999, 6.9112, 168.912956510431, 458816.9973071057, 458816.9973071063, 184673.2962707995], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3000.0000, 
sim time next is 3600.0000, 
raw observation next is [20.2, 84.0, 1.0, 2.0, 0.2706668065605122, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 442899.9157403773, 442899.9157403773, 163005.268487172], 
processed observation next is [1.0, 0.043478260869565216, 0.15639810426540288, 0.84, 1.0, 1.0, 0.1212853091090508, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12302775437232703, 0.12302775437232703, 0.24329144550324178], 
reward next is 0.7567, 
noisyNet noise sample is [array([-1.5360134], dtype=float32), -2.291032]. 
=============================================
[2019-03-27 10:58:24,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.19232818 0.36325535 0.14166076 0.14113334 0.1616224 ], sum to 1.0000
[2019-03-27 10:58:24,734] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1412
[2019-03-27 10:58:24,850] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.3, 84.0, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 521568.7138816275, 521568.7138816275, 237568.1191710745], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 28800.0000, 
sim time next is 29400.0000, 
raw observation next is [22.51666666666667, 83.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6434952994823167, 6.911199999999999, 6.9112, 168.912956510431, 562377.2670216053, 562377.2670216059, 172906.2109613569], 
processed observation next is [1.0, 0.34782608695652173, 0.2661927330173777, 0.83, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.5652381701003861, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15621590750600148, 0.15621590750600162, 0.2580689715841148], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8183963], dtype=float32), -0.8660778]. 
=============================================
[2019-03-27 10:58:28,773] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.09350508 0.23267788 0.24921596 0.19344755 0.23115356], sum to 1.0000
[2019-03-27 10:58:28,784] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0147
[2019-03-27 10:58:28,791] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.21666666666667, 89.0, 1.0, 2.0, 0.1712430029938425, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3059466491537337, 6.911199999999999, 6.9112, 168.912956510431, 532224.951451482, 532224.9514514826, 195254.1690939064], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 83400.0000, 
sim time next is 84000.0000, 
raw observation next is [22.23333333333333, 89.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6115358605513947, 6.911200000000001, 6.9112, 168.912956510431, 532156.956827821, 532156.9568278204, 167858.1380803412], 
processed observation next is [1.0, 1.0, 0.25276461295418634, 0.89, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5262632445748716, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14782137689661695, 0.14782137689661679, 0.25053453444827045], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.75591314], dtype=float32), -0.49498764]. 
=============================================
[2019-03-27 10:58:28,806] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[1.0799259 ]
 [1.002606  ]
 [0.922774  ]
 [0.70787346]
 [1.2217779 ]], R is [[1.50654054]
 [2.20005083]
 [2.88656425]
 [3.56601739]
 [3.53035736]].
[2019-03-27 10:58:31,404] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.06955639 0.28920725 0.1837159  0.22688204 0.23063844], sum to 1.0000
[2019-03-27 10:58:31,415] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4072
[2019-03-27 10:58:31,422] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.9, 94.0, 1.0, 2.0, 0.7523584046567713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1126767.662118722, 1126767.662118722, 242529.797510205], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 122400.0000, 
sim time next is 123000.0000, 
raw observation next is [22.88333333333333, 94.16666666666667, 1.0, 2.0, 0.3875432880682549, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6739351845086744, 6.9112, 6.9112, 168.912956510431, 1155141.158100615, 1155141.158100615, 259521.9495259863], 
processed observation next is [1.0, 0.43478260869565216, 0.28357030015797774, 0.9416666666666668, 1.0, 1.0, 0.2621003470701866, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.6023599811081395, 0.0, 0.0, 0.8294399451523027, 0.3208725439168375, 0.3208725439168375, 0.38734619332236764], 
reward next is 0.6127, 
noisyNet noise sample is [array([0.91159374], dtype=float32), 0.7332111]. 
=============================================
[2019-03-27 10:58:31,441] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[2.3824162]
 [2.38334  ]
 [2.4355688]
 [2.6407826]
 [2.411475 ]], R is [[3.18974495]
 [3.79586291]
 [4.42225218]
 [5.03616858]
 [5.59652805]].
[2019-03-27 10:58:36,358] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00154268 0.942614   0.01187269 0.00941416 0.03455655], sum to 1.0000
[2019-03-27 10:58:36,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2259
[2019-03-27 10:58:36,472] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.86666666666667, 95.66666666666666, 1.0, 2.0, 0.2835310953100895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457312.0435403627, 457312.0435403627, 164090.6782269985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 192000.0000, 
sim time next is 192600.0000, 
raw observation next is [19.9, 95.5, 1.0, 2.0, 0.2835485482324498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457260.8337499789, 457260.8337499789, 164087.3481484691], 
processed observation next is [0.0, 0.21739130434782608, 0.14218009478672985, 0.955, 1.0, 1.0, 0.13680547979813226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12701689826388304, 0.12701689826388304, 0.24490648977383445], 
reward next is 0.7551, 
noisyNet noise sample is [array([0.09396198], dtype=float32), 0.46984893]. 
=============================================
[2019-03-27 10:58:37,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0566009e-04 9.5442206e-01 4.4019120e-03 2.2270542e-03 3.8743369e-02], sum to 1.0000
[2019-03-27 10:58:37,537] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7849
[2019-03-27 10:58:37,637] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666666, 93.0, 1.0, 2.0, 0.2972232293653841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 474875.4220113505, 474875.4220113511, 165279.0807824007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 208200.0000, 
sim time next is 208800.0000, 
raw observation next is [20.7, 93.0, 1.0, 2.0, 0.296681675757461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 473676.569458692, 473676.5694586913, 165190.2805393742], 
processed observation next is [0.0, 0.43478260869565216, 0.18009478672985785, 0.93, 1.0, 1.0, 0.15262852500898916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13157682484963668, 0.13157682484963648, 0.246552657521454], 
reward next is 0.7534, 
noisyNet noise sample is [array([-0.77126545], dtype=float32), -0.35762256]. 
=============================================
[2019-03-27 10:58:40,056] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9628333e-08 9.9653339e-01 5.5587088e-06 1.4838039e-06 3.4595798e-03], sum to 1.0000
[2019-03-27 10:58:40,066] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5235
[2019-03-27 10:58:40,169] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 89.33333333333334, 1.0, 2.0, 0.2961501490464502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 473780.3036984643, 473780.3036984637, 165208.6884903521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 246000.0000, 
sim time next is 246600.0000, 
raw observation next is [21.0, 89.5, 1.0, 2.0, 0.2948494713123677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471822.59639589, 471822.59639589, 165072.4082548655], 
processed observation next is [0.0, 0.8695652173913043, 0.19431279620853087, 0.895, 1.0, 1.0, 0.150421049773937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13106183233219165, 0.13106183233219165, 0.24637672873860522], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.36378273], dtype=float32), 0.56076795]. 
=============================================
[2019-03-27 10:58:43,790] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7892: loss 0.6850
[2019-03-27 10:58:43,846] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7894: learning rate 0.0000
[2019-03-27 10:58:43,877] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7912: loss 0.5774
[2019-03-27 10:58:43,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7912: learning rate 0.0000
[2019-03-27 10:58:43,949] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7942: loss 0.6871
[2019-03-27 10:58:43,952] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7942: learning rate 0.0000
[2019-03-27 10:58:43,972] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7951: loss 0.6117
[2019-03-27 10:58:43,974] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7951: learning rate 0.0000
[2019-03-27 10:58:43,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9558840e-16 9.9999774e-01 1.8678739e-12 2.1654538e-12 2.2787019e-06], sum to 1.0000
[2019-03-27 10:58:43,989] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0647
[2019-03-27 10:58:44,087] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7966: loss 0.5259
[2019-03-27 10:58:44,091] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 77.0, 1.0, 2.0, 0.3059573726402128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484192.1809023133, 484192.1809023133, 165867.9709453608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 298800.0000, 
sim time next is 299400.0000, 
raw observation next is [23.23333333333333, 77.0, 1.0, 2.0, 0.3069112413170522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 485394.033710158, 485394.0337101585, 165948.6277054654], 
processed observation next is [0.0, 0.4782608695652174, 0.3001579778830963, 0.77, 1.0, 1.0, 0.16495330279162917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13483167603059945, 0.1348316760305996, 0.24768451896338117], 
reward next is 0.7523, 
noisyNet noise sample is [array([-0.9477095], dtype=float32), 1.2208778]. 
=============================================
[2019-03-27 10:58:44,098] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7966: learning rate 0.0000
[2019-03-27 10:58:44,115] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7971: loss 0.5757
[2019-03-27 10:58:44,119] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7972: learning rate 0.0000
[2019-03-27 10:58:44,147] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7985: loss 0.6449
[2019-03-27 10:58:44,154] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7986: loss 0.6612
[2019-03-27 10:58:44,155] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7986: learning rate 0.0000
[2019-03-27 10:58:44,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7986: learning rate 0.0000
[2019-03-27 10:58:44,201] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8007: loss 0.3964
[2019-03-27 10:58:44,206] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8007: learning rate 0.0000
[2019-03-27 10:58:44,223] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 8017: loss 0.5535
[2019-03-27 10:58:44,225] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 8017: learning rate 0.0000
[2019-03-27 10:58:44,235] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8023: loss 0.4639
[2019-03-27 10:58:44,241] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8023: learning rate 0.0000
[2019-03-27 10:58:44,242] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8024: loss 0.6668
[2019-03-27 10:58:44,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8025: learning rate 0.0000
[2019-03-27 10:58:44,264] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8033: loss 0.4597
[2019-03-27 10:58:44,270] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8034: loss 0.5857
[2019-03-27 10:58:44,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8033: learning rate 0.0000
[2019-03-27 10:58:44,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8036: learning rate 0.0000
[2019-03-27 10:58:44,292] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8041: loss 0.4206
[2019-03-27 10:58:44,296] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8042: learning rate 0.0000
[2019-03-27 10:58:44,303] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 8043: loss 0.5439
[2019-03-27 10:58:44,304] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 8043: learning rate 0.0000
[2019-03-27 10:58:45,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0145576e-18 9.9999988e-01 4.0411993e-15 1.3850146e-14 7.5177645e-08], sum to 1.0000
[2019-03-27 10:58:45,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2639
[2019-03-27 10:58:46,082] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.78333333333333, 83.5, 1.0, 2.0, 0.2943319659023842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 470804.6572689256, 470804.6572689256, 164999.0932222761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 327000.0000, 
sim time next is 327600.0000, 
raw observation next is [21.7, 84.0, 1.0, 2.0, 0.2923484757194712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467795.4244397502, 467795.4244397502, 164790.8867092818], 
processed observation next is [0.0, 0.8260869565217391, 0.2274881516587678, 0.84, 1.0, 1.0, 0.14740780207165205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12994317345548617, 0.12994317345548617, 0.24595654732728625], 
reward next is 0.7540, 
noisyNet noise sample is [array([1.6526598], dtype=float32), 0.78929466]. 
=============================================
[2019-03-27 10:58:58,683] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8200718e-14 9.9999440e-01 2.1044007e-10 2.8053393e-12 5.5990881e-06], sum to 1.0000
[2019-03-27 10:58:58,697] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6659
[2019-03-27 10:58:58,812] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.13333333333333, 79.0, 1.0, 2.0, 0.2404901571515648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 397739.4148161089, 397739.4148161095, 159893.8413064448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 505200.0000, 
sim time next is 505800.0000, 
raw observation next is [20.05, 79.5, 1.0, 2.0, 0.2408018876667755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 398325.5686412163, 398325.5686412163, 159918.3715501579], 
processed observation next is [1.0, 0.8695652173913043, 0.14928909952606645, 0.795, 1.0, 1.0, 0.08530347911659697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11064599128922675, 0.11064599128922675, 0.23868413664202673], 
reward next is 0.7613, 
noisyNet noise sample is [array([0.7794962], dtype=float32), 0.40944576]. 
=============================================
[2019-03-27 10:59:04,813] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.0974774e-15 9.9999988e-01 1.4966591e-13 8.2724660e-14 6.0414074e-08], sum to 1.0000
[2019-03-27 10:59:04,824] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3112
[2019-03-27 10:59:04,832] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 75.0, 1.0, 2.0, 0.238549696856602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 394756.0296538163, 394756.0296538163, 159692.2469175723], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 592200.0000, 
sim time next is 592800.0000, 
raw observation next is [20.46666666666667, 75.66666666666666, 1.0, 2.0, 0.2382874569346272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 394479.4976514422, 394479.4976514422, 159654.2079338093], 
processed observation next is [1.0, 0.8695652173913043, 0.16903633491311232, 0.7566666666666666, 1.0, 1.0, 0.08227404449955082, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10957763823651172, 0.10957763823651172, 0.23828986258777507], 
reward next is 0.7617, 
noisyNet noise sample is [array([-0.8822101], dtype=float32), 0.21787007]. 
=============================================
[2019-03-27 10:59:05,010] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15876: loss 0.2073
[2019-03-27 10:59:05,012] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15878: learning rate 0.0000
[2019-03-27 10:59:05,035] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15884: loss 0.2477
[2019-03-27 10:59:05,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15884: learning rate 0.0000
[2019-03-27 10:59:05,079] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15902: loss 0.1943
[2019-03-27 10:59:05,081] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15903: learning rate 0.0000
[2019-03-27 10:59:05,183] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15940: loss 0.2131
[2019-03-27 10:59:05,185] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15940: learning rate 0.0000
[2019-03-27 10:59:05,206] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15949: loss 0.1607
[2019-03-27 10:59:05,210] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15949: learning rate 0.0000
[2019-03-27 10:59:05,227] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15956: loss 0.1176
[2019-03-27 10:59:05,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15956: learning rate 0.0000
[2019-03-27 10:59:05,324] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15986: loss 0.1256
[2019-03-27 10:59:05,328] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15989: learning rate 0.0000
[2019-03-27 10:59:05,334] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 15992: loss 0.1249
[2019-03-27 10:59:05,335] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 15993: learning rate 0.0000
[2019-03-27 10:59:05,365] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 16007: loss 0.1474
[2019-03-27 10:59:05,368] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 16007: learning rate 0.0000
[2019-03-27 10:59:05,383] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16014: loss 0.1602
[2019-03-27 10:59:05,388] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16017: learning rate 0.0000
[2019-03-27 10:59:05,405] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16027: loss 0.1100
[2019-03-27 10:59:05,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16028: learning rate 0.0000
[2019-03-27 10:59:05,434] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16040: loss 0.1244
[2019-03-27 10:59:05,442] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16040: learning rate 0.0000
[2019-03-27 10:59:05,466] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16048: loss 0.0713
[2019-03-27 10:59:05,469] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16048: loss 0.1071
[2019-03-27 10:59:05,470] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16048: learning rate 0.0000
[2019-03-27 10:59:05,474] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16051: learning rate 0.0000
[2019-03-27 10:59:05,514] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16067: loss 0.0758
[2019-03-27 10:59:05,516] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16067: learning rate 0.0000
[2019-03-27 10:59:05,543] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16077: loss 0.0447
[2019-03-27 10:59:05,547] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16077: learning rate 0.0000
[2019-03-27 10:59:06,651] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2163466e-15 9.9999988e-01 8.3319305e-13 1.3375553e-12 1.3841164e-07], sum to 1.0000
[2019-03-27 10:59:06,662] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2612
[2019-03-27 10:59:06,779] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.9, 92.0, 1.0, 2.0, 0.1996633180565149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 333873.5449349684, 333873.5449349678, 155224.6303942206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 622800.0000, 
sim time next is 623400.0000, 
raw observation next is [17.13333333333333, 91.0, 1.0, 2.0, 0.2286987242212161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 382335.3404763004, 382335.3404762998, 157779.9089168127], 
processed observation next is [1.0, 0.21739130434782608, 0.011058451816745531, 0.91, 1.0, 1.0, 0.07072135448339287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10620426124341678, 0.10620426124341661, 0.23549240136837718], 
reward next is 0.7645, 
noisyNet noise sample is [array([-0.22449967], dtype=float32), -1.7376026]. 
=============================================
[2019-03-27 10:59:11,261] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8392889e-20 1.0000000e+00 3.0647487e-13 6.5083158e-16 1.9120609e-08], sum to 1.0000
[2019-03-27 10:59:11,271] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6212
[2019-03-27 10:59:11,376] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 80.0, 1.0, 2.0, 0.2428836359520094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401307.3486973058, 401307.3486973058, 160148.350047122], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [20.0, 80.66666666666667, 1.0, 2.0, 0.2415022161778914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399074.9289187366, 399074.9289187372, 160012.917123301], 
processed observation next is [1.0, 0.9130434782608695, 0.1469194312796209, 0.8066666666666668, 1.0, 1.0, 0.08614724840709805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11085414692187127, 0.11085414692187145, 0.2388252494377627], 
reward next is 0.7612, 
noisyNet noise sample is [array([2.0363345], dtype=float32), -0.28554666]. 
=============================================
[2019-03-27 10:59:11,397] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.02707]
 [77.00478]
 [76.9524 ]
 [76.94358]
 [76.94121]], R is [[77.01519775]
 [77.00601959]
 [76.9966507 ]
 [76.98725891]
 [76.97814941]].
[2019-03-27 10:59:14,934] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9559919e-17 9.9999964e-01 7.5280995e-14 2.1430571e-15 3.1371246e-07], sum to 1.0000
[2019-03-27 10:59:14,944] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3564
[2019-03-27 10:59:14,948] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.53333333333333, 52.5, 1.0, 2.0, 0.4037994560166214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658645.1152586079, 658645.1152586079, 179944.0940086785], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 737400.0000, 
sim time next is 738000.0000, 
raw observation next is [25.6, 52.0, 1.0, 2.0, 0.4115975966727267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671624.6867285226, 671624.6867285226, 181114.0266756926], 
processed observation next is [1.0, 0.5652173913043478, 0.4123222748815167, 0.52, 1.0, 1.0, 0.29108144177436956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18656241298014517, 0.18656241298014517, 0.27031944279954123], 
reward next is 0.7297, 
noisyNet noise sample is [array([-0.13026546], dtype=float32), 0.57095194]. 
=============================================
[2019-03-27 10:59:14,964] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.817055]
 [77.78414 ]
 [77.76563 ]
 [77.85172 ]
 [77.924   ]], R is [[77.79515076]
 [77.74862671]
 [77.70767975]
 [77.67900085]
 [77.64499664]].
[2019-03-27 10:59:26,085] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23867: loss 0.0001
[2019-03-27 10:59:26,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23867: learning rate 0.0000
[2019-03-27 10:59:26,168] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23897: loss 0.0055
[2019-03-27 10:59:26,171] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23897: learning rate 0.0000
[2019-03-27 10:59:26,192] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23907: loss 0.0023
[2019-03-27 10:59:26,194] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23907: learning rate 0.0000
[2019-03-27 10:59:26,215] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23916: loss 0.0016
[2019-03-27 10:59:26,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23917: learning rate 0.0000
[2019-03-27 10:59:26,246] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23926: loss 0.0001
[2019-03-27 10:59:26,249] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23927: learning rate 0.0000
[2019-03-27 10:59:26,283] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23940: loss 0.0049
[2019-03-27 10:59:26,285] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23940: learning rate 0.0000
[2019-03-27 10:59:26,306] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23948: loss 0.0003
[2019-03-27 10:59:26,309] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23949: learning rate 0.0000
[2019-03-27 10:59:26,378] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23977: loss 0.0010
[2019-03-27 10:59:26,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23978: learning rate 0.0000
[2019-03-27 10:59:26,385] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23980: loss 0.0004
[2019-03-27 10:59:26,387] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23982: learning rate 0.0000
[2019-03-27 10:59:26,426] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23992: loss 0.0024
[2019-03-27 10:59:26,433] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23993: learning rate 0.0000
[2019-03-27 10:59:26,540] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24037: loss 0.0113
[2019-03-27 10:59:26,541] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24037: learning rate 0.0000
[2019-03-27 10:59:26,572] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 24047: loss 0.0202
[2019-03-27 10:59:26,575] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24049: loss 0.0012
[2019-03-27 10:59:26,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24049: learning rate 0.0000
[2019-03-27 10:59:26,581] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 24049: learning rate 0.0000
[2019-03-27 10:59:26,634] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24070: loss 0.0041
[2019-03-27 10:59:26,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24071: learning rate 0.0000
[2019-03-27 10:59:26,700] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 24096: loss 0.0038
[2019-03-27 10:59:26,702] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 24096: learning rate 0.0000
[2019-03-27 10:59:26,768] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24121: loss 0.0018
[2019-03-27 10:59:26,773] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24121: learning rate 0.0000
[2019-03-27 10:59:29,091] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 10:59:29,095] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 10:59:29,098] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 10:59:29,098] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:59:29,099] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:59:29,102] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 10:59:29,103] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 10:59:29,104] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:59:29,100] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 10:59:29,105] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:59:29,106] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 10:59:29,124] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run2
[2019-03-27 10:59:29,145] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run2
[2019-03-27 10:59:29,147] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run2
[2019-03-27 10:59:29,183] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run2
[2019-03-27 10:59:29,184] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run2
[2019-03-27 10:59:46,138] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 10:59:46,140] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.03333333333333, 96.16666666666666, 1.0, 2.0, 0.3228718398974434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506304.7508742899, 506304.7508742893, 167405.9365321552]
[2019-03-27 10:59:46,143] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 10:59:46,149] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2723214e-18 1.0000000e+00 6.0274140e-15 9.6171949e-17 5.0986513e-09], sampled 0.7986953756154671
[2019-03-27 10:59:53,868] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 10:59:53,870] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.952756705, 82.34268711333333, 1.0, 2.0, 0.460181059856115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 653169.1821395817, 653169.1821395811, 178900.4746183156]
[2019-03-27 10:59:53,870] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 10:59:53,873] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.9042529e-19 1.0000000e+00 4.8590782e-15 8.4565400e-17 4.4916466e-09], sampled 0.8548041106854944
[2019-03-27 10:59:54,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 10:59:54,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.28333333333333, 67.33333333333333, 1.0, 2.0, 0.3398143783234127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 530412.3743094774, 530412.3743094767, 169230.0043420034]
[2019-03-27 10:59:54,076] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 10:59:54,077] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8461018e-18 1.0000000e+00 1.2160347e-14 2.3707454e-16 7.7980689e-09], sampled 0.9275617308827971
[2019-03-27 11:00:10,186] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 11:00:10,187] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.92023327, 100.0, 1.0, 2.0, 0.5375721636233121, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9254773761002358, 6.911200000000001, 6.9112, 168.9125795505974, 1502915.898622363, 1502915.898622363, 327669.1825684546]
[2019-03-27 11:00:10,190] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:00:10,195] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.9812717e-19 1.0000000e+00 2.5625572e-15 4.9365715e-17 3.8448777e-09], sampled 0.0653452751009932
[2019-03-27 11:00:12,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 11:00:12,668] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.35755840666667, 91.10618950666668, 1.0, 2.0, 0.5091368752701518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711443.9550374905, 711443.955037491, 185038.6332585462]
[2019-03-27 11:00:12,669] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:00:12,670] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1531900e-19 1.0000000e+00 1.5972519e-15 2.6424571e-17 2.5968203e-09], sampled 0.7485325115258563
[2019-03-27 11:00:14,369] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 11:00:14,371] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.51096953666667, 97.82246905666668, 1.0, 2.0, 0.2567743686867262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 422111.0304400459, 422111.0304400453, 161583.2729780254]
[2019-03-27 11:00:14,372] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:00:14,375] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3440784e-19 1.0000000e+00 1.6254404e-15 2.6110838e-17 2.8361116e-09], sampled 0.14665934331995845
[2019-03-27 11:00:27,058] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 11:00:27,059] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.7, 79.0, 1.0, 2.0, 0.5492498238585075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767516.1555313885, 767516.1555313892, 191671.7490947745]
[2019-03-27 11:00:27,061] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:00:27,064] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5029223e-19 1.0000000e+00 1.2315516e-15 2.4157054e-17 2.1226672e-09], sampled 0.34762640357393226
[2019-03-27 11:00:33,500] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 11:00:33,501] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.288835305, 72.07767122999999, 1.0, 2.0, 0.7204240206808393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1006826.625635843, 1006826.625635843, 225327.4467665991]
[2019-03-27 11:00:33,503] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:00:33,506] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.1111542e-20 1.0000000e+00 6.0473091e-16 1.0137145e-17 1.3595775e-09], sampled 0.8230340987757996
[2019-03-27 11:00:54,923] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 11:00:54,924] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 75.0, 1.0, 2.0, 0.5538151683282866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773898.0457974691, 773898.0457974698, 192456.7882176087]
[2019-03-27 11:00:54,925] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:00:54,928] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.9110569e-20 1.0000000e+00 5.0922673e-16 7.9730823e-18 1.2811475e-09], sampled 0.06777238221885518
[2019-03-27 11:01:02,969] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 11:01:02,970] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.3, 63.0, 1.0, 2.0, 0.8922437151877801, 1.0, 2.0, 0.8922437151877801, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2495578.707406246, 2495578.707406246, 467277.6796066762]
[2019-03-27 11:01:02,970] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:01:02,972] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.65887663e-19 1.00000000e+00 4.66080599e-15 1.06460765e-16
 5.87035887e-09], sampled 0.7697430570261521
[2019-03-27 11:01:02,974] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2495578.707406246 W.
[2019-03-27 11:01:14,599] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 11:01:14,599] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.1162169, 82.64930285, 1.0, 2.0, 0.524643990863112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733120.3475137022, 733120.3475137022, 187546.1812557637]
[2019-03-27 11:01:14,599] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:01:14,603] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2241766e-19 1.0000000e+00 9.9377484e-16 1.7866873e-17 1.8325708e-09], sampled 0.6801209394435623
[2019-03-27 11:01:33,978] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04622659], dtype=float32), 0.02883657]
[2019-03-27 11:01:33,978] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.46666666666667, 79.66666666666667, 1.0, 2.0, 0.5796604533276908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810027.8657065062, 810027.8657065062, 197017.4243131048]
[2019-03-27 11:01:33,979] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:01:33,984] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.4377865e-20 1.0000000e+00 5.8315688e-16 9.6680241e-18 1.2923685e-09], sampled 0.7889100027816419
[2019-03-27 11:01:36,266] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 11:01:36,562] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 11:01:36,801] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1306 2842497351.2380 1131.0000
[2019-03-27 11:01:37,101] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-27 11:01:37,115] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 11:01:38,130] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 25000, evaluation results [25000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8496.130590455632, 2842497351.2380195, 1131.0]
[2019-03-27 11:01:40,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2557508e-18 1.0000000e+00 1.5637241e-13 5.8400508e-16 1.8127552e-09], sum to 1.0000
[2019-03-27 11:01:40,921] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4491
[2019-03-27 11:01:40,929] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.4178152766427087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 647662.1629634791, 647662.1629634791, 179440.0576613458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 980400.0000, 
sim time next is 981000.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.4961679268021104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769069.1211663409, 769069.1211663409, 191988.1119634396], 
processed observation next is [1.0, 0.34782608695652173, 0.23696682464454974, 0.93, 1.0, 1.0, 0.3929734057856752, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2136303114350947, 0.2136303114350947, 0.2865494208409546], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.20428577], dtype=float32), 0.88536763]. 
=============================================
[2019-03-27 11:01:40,942] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.379555]
 [78.32193 ]
 [78.295654]
 [78.25473 ]
 [78.24057 ]], R is [[78.43247223]
 [78.38032532]
 [78.34325409]
 [78.30903625]
 [78.27519989]].
[2019-03-27 11:01:43,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7783099e-17 1.0000000e+00 1.5911683e-14 1.4696791e-15 3.0417766e-08], sum to 1.0000
[2019-03-27 11:01:43,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4125
[2019-03-27 11:01:43,891] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 96.0, 1.0, 2.0, 0.3549255071666878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546063.4439535406, 546063.4439535406, 170298.3032891489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1021800.0000, 
sim time next is 1022400.0000, 
raw observation next is [21.8, 96.0, 1.0, 2.0, 0.3541366531440557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544850.3118745922, 544850.3118745928, 170197.4571940295], 
processed observation next is [1.0, 0.8695652173913043, 0.23222748815165886, 0.96, 1.0, 1.0, 0.22185138933018755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1513473088540534, 0.15134730885405356, 0.25402605551347684], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.5207322], dtype=float32), 1.013978]. 
=============================================
[2019-03-27 11:01:49,946] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0556506e-15 1.0000000e+00 2.5675614e-12 2.5707536e-14 2.1526025e-08], sum to 1.0000
[2019-03-27 11:01:49,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4280
[2019-03-27 11:01:49,965] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333334, 74.33333333333334, 1.0, 2.0, 0.3295784873726749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 517333.696293955, 517333.696293955, 168267.787656064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1106400.0000, 
sim time next is 1107000.0000, 
raw observation next is [23.8, 75.0, 1.0, 2.0, 0.3282823112426462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515696.0292191395, 515696.0292191395, 168149.657300041], 
processed observation next is [1.0, 0.8260869565217391, 0.3270142180094788, 0.75, 1.0, 1.0, 0.19070157981041708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1432488970053165, 0.1432488970053165, 0.25096963776125525], 
reward next is 0.7490, 
noisyNet noise sample is [array([-0.52570426], dtype=float32), -1.5077772]. 
=============================================
[2019-03-27 11:01:49,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[68.806305]
 [68.87187 ]
 [68.919205]
 [68.929054]
 [68.94032 ]], R is [[68.96620178]
 [69.02539825]
 [69.08441162]
 [69.14255524]
 [69.20011902]].
[2019-03-27 11:01:50,385] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.31546771e-16 1.00000000e+00 1.09227596e-13 1.35354527e-14
 3.63711328e-09], sum to 1.0000
[2019-03-27 11:01:50,403] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4467
[2019-03-27 11:01:50,407] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 80.0, 1.0, 2.0, 0.3128860056454387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495717.4590429692, 495717.4590429692, 166724.3808079092], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1112400.0000, 
sim time next is 1113000.0000, 
raw observation next is [22.58333333333334, 80.66666666666667, 1.0, 2.0, 0.311493984397065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493855.1373513195, 493855.1373513188, 166592.9363236253], 
processed observation next is [1.0, 0.9130434782608695, 0.2693522906793052, 0.8066666666666668, 1.0, 1.0, 0.17047467999646385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13718198259758874, 0.13718198259758854, 0.2486461736173512], 
reward next is 0.7514, 
noisyNet noise sample is [array([-0.7317194], dtype=float32), 0.7361435]. 
=============================================
[2019-03-27 11:01:50,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.73121 ]
 [74.78043 ]
 [74.806366]
 [74.876884]
 [74.94271 ]], R is [[74.66881561]
 [74.67328644]
 [74.6774292 ]
 [74.68134308]
 [74.68516541]].
[2019-03-27 11:01:55,863] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4018955e-15 9.9999940e-01 4.5650185e-11 1.4419301e-13 5.4976698e-07], sum to 1.0000
[2019-03-27 11:01:55,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5366
[2019-03-27 11:01:55,991] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 66.16666666666667, 1.0, 2.0, 0.3445898396661412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532736.5550255472, 532736.5550255479, 169280.1545796951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1191000.0000, 
sim time next is 1191600.0000, 
raw observation next is [25.7, 67.0, 1.0, 2.0, 0.3443976029830777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532773.1660277712, 532773.1660277712, 169292.8305185069], 
processed observation next is [1.0, 0.8260869565217391, 0.4170616113744076, 0.67, 1.0, 1.0, 0.2101175939555153, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14799254611882534, 0.14799254611882534, 0.2526758664455327], 
reward next is 0.7473, 
noisyNet noise sample is [array([-1.0404881], dtype=float32), -2.2206569]. 
=============================================
[2019-03-27 11:01:56,130] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31826: loss 0.1103
[2019-03-27 11:01:56,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31826: learning rate 0.0000
[2019-03-27 11:01:56,327] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31904: loss 0.0647
[2019-03-27 11:01:56,330] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31904: learning rate 0.0000
[2019-03-27 11:01:56,377] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31924: loss 0.0617
[2019-03-27 11:01:56,380] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31924: learning rate 0.0000
[2019-03-27 11:01:56,407] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31931: loss 0.0486
[2019-03-27 11:01:56,414] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31931: learning rate 0.0000
[2019-03-27 11:01:56,437] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31945: loss 0.0218
[2019-03-27 11:01:56,440] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31946: loss 0.0322
[2019-03-27 11:01:56,441] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31946: learning rate 0.0000
[2019-03-27 11:01:56,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31946: learning rate 0.0000
[2019-03-27 11:01:56,474] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31959: loss 0.0509
[2019-03-27 11:01:56,481] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31959: learning rate 0.0000
[2019-03-27 11:01:56,507] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31970: loss 0.0224
[2019-03-27 11:01:56,515] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31971: learning rate 0.0000
[2019-03-27 11:01:56,541] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31982: loss 0.0170
[2019-03-27 11:01:56,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31982: learning rate 0.0000
[2019-03-27 11:01:56,567] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31990: loss 0.0130
[2019-03-27 11:01:56,572] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31990: learning rate 0.0000
[2019-03-27 11:01:56,611] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32005: loss 0.0021
[2019-03-27 11:01:56,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32005: learning rate 0.0000
[2019-03-27 11:01:56,694] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32041: loss 0.0020
[2019-03-27 11:01:56,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32041: learning rate 0.0000
[2019-03-27 11:01:56,700] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32041: loss 0.0053
[2019-03-27 11:01:56,705] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32041: learning rate 0.0000
[2019-03-27 11:01:56,731] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32051: loss 0.0017
[2019-03-27 11:01:56,733] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32052: learning rate 0.0000
[2019-03-27 11:01:56,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6556959e-17 1.0000000e+00 1.6177901e-13 4.8593001e-15 2.1646445e-09], sum to 1.0000
[2019-03-27 11:01:56,847] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2937
[2019-03-27 11:01:56,854] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 86.16666666666666, 1.0, 2.0, 0.3560965178943104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548579.436239281, 548579.436239281, 170528.3612299317], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1205400.0000, 
sim time next is 1206000.0000, 
raw observation next is [22.9, 87.0, 1.0, 2.0, 0.3565731776975517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549146.8796949072, 549146.8796949066, 170571.0982558838], 
processed observation next is [1.0, 1.0, 0.2843601895734597, 0.87, 1.0, 1.0, 0.2247869610813876, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15254079991525202, 0.15254079991525185, 0.254583728740125], 
reward next is 0.7454, 
noisyNet noise sample is [array([1.2303153], dtype=float32), -1.0119244]. 
=============================================
[2019-03-27 11:01:56,872] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.631615]
 [75.542305]
 [75.43952 ]
 [75.34421 ]
 [75.263466]], R is [[75.71994781]
 [75.70822906]
 [75.69672394]
 [75.68544769]
 [75.67434692]].
[2019-03-27 11:01:56,950] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 32129: loss 0.0113
[2019-03-27 11:01:56,954] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 32130: learning rate 0.0000
[2019-03-27 11:01:57,055] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 32170: loss 0.0111
[2019-03-27 11:01:57,057] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 32171: learning rate 0.0000
[2019-03-27 11:02:05,470] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3775209e-19 1.0000000e+00 4.8935579e-18 1.1461235e-18 5.1495771e-14], sum to 1.0000
[2019-03-27 11:02:05,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5203
[2019-03-27 11:02:05,483] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.01666666666667, 95.0, 1.0, 2.0, 0.7439501344991756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1104125.174937807, 1104125.174937807, 239185.5781017794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1327800.0000, 
sim time next is 1328400.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.7861589445802485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1167462.6030106, 1167462.6030106, 249866.1613141437], 
processed observation next is [1.0, 0.391304347826087, 0.28909952606635075, 0.95, 1.0, 1.0, 0.7423601741930704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32429516750294446, 0.32429516750294446, 0.37293456912558764], 
reward next is 0.6271, 
noisyNet noise sample is [array([-0.47820166], dtype=float32), 1.9357519]. 
=============================================
[2019-03-27 11:02:09,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2880198e-21 1.0000000e+00 1.0842052e-18 4.4033680e-20 1.7064274e-13], sum to 1.0000
[2019-03-27 11:02:09,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6991
[2019-03-27 11:02:09,415] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 97.0, 1.0, 2.0, 0.3111750875570063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 493698.8909508252, 493698.8909508258, 166587.574781193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1382400.0000, 
sim time next is 1383000.0000, 
raw observation next is [20.48333333333333, 97.0, 1.0, 2.0, 0.3109299914006426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493489.0386009165, 493489.0386009165, 166575.1628752845], 
processed observation next is [0.0, 0.0, 0.16982622432859393, 0.97, 1.0, 1.0, 0.16979517036221997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13708028850025458, 0.13708028850025458, 0.2486196460825142], 
reward next is 0.7514, 
noisyNet noise sample is [array([-0.13471068], dtype=float32), 0.49792686]. 
=============================================
[2019-03-27 11:02:09,438] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.91057]
 [78.28334]
 [78.25003]
 [78.20083]
 [78.15836]], R is [[77.52003479]
 [77.49620056]
 [77.47245026]
 [77.44868469]
 [77.42485809]].
[2019-03-27 11:02:12,775] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2600428e-19 1.0000000e+00 8.7882075e-18 9.7240503e-20 1.4912830e-12], sum to 1.0000
[2019-03-27 11:02:12,787] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8252
[2019-03-27 11:02:12,796] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 76.33333333333334, 1.0, 2.0, 0.4295372330662127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 619447.3635123211, 619447.3635123218, 175774.6880538198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1430400.0000, 
sim time next is 1431000.0000, 
raw observation next is [26.6, 75.0, 1.0, 2.0, 0.4296274430356196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619811.9741891421, 619811.9741891414, 175817.0072538997], 
processed observation next is [0.0, 0.5652173913043478, 0.4597156398104266, 0.75, 1.0, 1.0, 0.31280414823568625, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17216999283031725, 0.17216999283031706, 0.2624134436625369], 
reward next is 0.7376, 
noisyNet noise sample is [array([-0.12860686], dtype=float32), 0.15016684]. 
=============================================
[2019-03-27 11:02:12,813] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.41063 ]
 [77.39464 ]
 [77.371544]
 [77.33099 ]
 [77.3024  ]], R is [[77.40084076]
 [77.36447906]
 [77.32878876]
 [77.2937851 ]
 [77.25938416]].
[2019-03-27 11:02:15,961] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0145307e-17 1.0000000e+00 2.9801446e-16 1.2674151e-17 5.7864087e-12], sum to 1.0000
[2019-03-27 11:02:15,972] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2944
[2019-03-27 11:02:16,073] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 96.0, 1.0, 2.0, 0.3413674513089504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530039.9030655492, 530039.9030655499, 169127.8221798184], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1474800.0000, 
sim time next is 1475400.0000, 
raw observation next is [21.43333333333333, 96.0, 1.0, 2.0, 0.3398871078074065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 528223.7530063587, 528223.7530063582, 168995.4965869248], 
processed observation next is [0.0, 0.043478260869565216, 0.21484992101105835, 0.96, 1.0, 1.0, 0.20468326241856205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1467288202795441, 0.14672882027954393, 0.2522320844580967], 
reward next is 0.7478, 
noisyNet noise sample is [array([0.33388245], dtype=float32), 0.30365843]. 
=============================================
[2019-03-27 11:02:17,188] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39784: loss 0.0003
[2019-03-27 11:02:17,191] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39784: learning rate 0.0000
[2019-03-27 11:02:17,483] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39895: loss 0.0348
[2019-03-27 11:02:17,485] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39895: learning rate 0.0000
[2019-03-27 11:02:17,548] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39921: loss 0.0188
[2019-03-27 11:02:17,550] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39921: learning rate 0.0000
[2019-03-27 11:02:17,576] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39928: loss 0.0180
[2019-03-27 11:02:17,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39928: learning rate 0.0000
[2019-03-27 11:02:17,624] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39948: loss 0.0191
[2019-03-27 11:02:17,626] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39948: learning rate 0.0000
[2019-03-27 11:02:17,657] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39961: loss 0.0210
[2019-03-27 11:02:17,658] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39961: loss 0.0124
[2019-03-27 11:02:17,663] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39962: learning rate 0.0000
[2019-03-27 11:02:17,665] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39962: learning rate 0.0000
[2019-03-27 11:02:17,726] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 39986: loss 0.0069
[2019-03-27 11:02:17,728] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 39986: learning rate 0.0000
[2019-03-27 11:02:17,768] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40000: loss 0.0050
[2019-03-27 11:02:17,769] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40000: loss 0.0083
[2019-03-27 11:02:17,771] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40000: learning rate 0.0000
[2019-03-27 11:02:17,773] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40000: learning rate 0.0000
[2019-03-27 11:02:17,802] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40013: loss 0.0017
[2019-03-27 11:02:17,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40013: learning rate 0.0000
[2019-03-27 11:02:17,835] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40024: loss 0.0011
[2019-03-27 11:02:17,838] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40026: learning rate 0.0000
[2019-03-27 11:02:17,898] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40047: loss 0.0010
[2019-03-27 11:02:17,900] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40047: learning rate 0.0000
[2019-03-27 11:02:17,948] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 40065: loss 0.0001
[2019-03-27 11:02:17,956] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 40066: learning rate 0.0000
[2019-03-27 11:02:17,962] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40069: loss 0.0002
[2019-03-27 11:02:17,967] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40070: learning rate 0.0000
[2019-03-27 11:02:18,314] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 40203: loss 0.0094
[2019-03-27 11:02:18,317] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 40203: learning rate 0.0000
[2019-03-27 11:02:28,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.5834827e-21 1.0000000e+00 2.6288538e-17 1.6962910e-19 4.4904019e-14], sum to 1.0000
[2019-03-27 11:02:28,043] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1346
[2019-03-27 11:02:28,158] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 99.0, 1.0, 2.0, 0.4291391812672065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 621195.0623200836, 621195.0623200836, 176011.604917373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1645200.0000, 
sim time next is 1645800.0000, 
raw observation next is [23.2, 99.0, 1.0, 2.0, 0.4299494996397497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622072.2371497632, 622072.2371497625, 176088.7344418158], 
processed observation next is [1.0, 0.043478260869565216, 0.29857819905213273, 0.99, 1.0, 1.0, 0.31319216824066226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17279784365271197, 0.17279784365271178, 0.2628190066295758], 
reward next is 0.7372, 
noisyNet noise sample is [array([1.0889387], dtype=float32), 0.5687917]. 
=============================================
[2019-03-27 11:02:28,522] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9635439e-18 1.0000000e+00 6.6482859e-16 6.1289588e-18 6.8059002e-14], sum to 1.0000
[2019-03-27 11:02:28,536] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9124
[2019-03-27 11:02:28,540] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 99.0, 1.0, 2.0, 0.4658711685958647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671398.7925332873, 671398.7925332873, 181028.0595106483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1652400.0000, 
sim time next is 1653000.0000, 
raw observation next is [23.3, 99.0, 1.0, 2.0, 0.463202052953941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667408.0869076167, 667408.0869076167, 180606.4819490124], 
processed observation next is [1.0, 0.13043478260869565, 0.3033175355450238, 0.99, 1.0, 1.0, 0.3532554854866759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18539113525211576, 0.18539113525211576, 0.26956191335673496], 
reward next is 0.7304, 
noisyNet noise sample is [array([0.10791349], dtype=float32), 0.63771445]. 
=============================================
[2019-03-27 11:02:28,560] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[70.138626]
 [70.13653 ]
 [70.25328 ]
 [70.22586 ]
 [70.21784 ]], R is [[70.24910736]
 [70.27642822]
 [70.30275726]
 [70.32782745]
 [70.35359955]].
[2019-03-27 11:02:29,661] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.1964902e-20 1.0000000e+00 2.1579121e-17 5.4535356e-19 3.7080918e-13], sum to 1.0000
[2019-03-27 11:02:29,672] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9676
[2019-03-27 11:02:29,777] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.45, 99.0, 1.0, 2.0, 0.4810218100538433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688882.3257555472, 688882.3257555466, 182813.539430845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1661400.0000, 
sim time next is 1662000.0000, 
raw observation next is [23.46666666666667, 99.0, 1.0, 2.0, 0.4749539791025801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679709.2318490242, 679709.2318490242, 181816.7391612908], 
processed observation next is [1.0, 0.21739130434782608, 0.31121642969984215, 0.99, 1.0, 1.0, 0.36741443265371093, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18880811995806226, 0.18880811995806226, 0.27136826740491166], 
reward next is 0.7286, 
noisyNet noise sample is [array([-1.5268843], dtype=float32), 1.7402116]. 
=============================================
[2019-03-27 11:02:29,790] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[76.5487 ]
 [76.49427]
 [76.44487]
 [76.47919]
 [76.39288]], R is [[76.55954742]
 [76.52109528]
 [76.48465729]
 [76.44379425]
 [76.41552734]].
[2019-03-27 11:02:37,428] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0152223e-19 1.0000000e+00 8.1005208e-19 8.0409210e-18 3.6509214e-14], sum to 1.0000
[2019-03-27 11:02:37,438] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5330
[2019-03-27 11:02:37,447] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 90.66666666666667, 1.0, 2.0, 0.6036693233599966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 957533.1192305534, 957533.1192305528, 214559.1078550541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1781400.0000, 
sim time next is 1782000.0000, 
raw observation next is [21.0, 92.0, 1.0, 2.0, 0.6004387425506822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 953896.1516256345, 953896.1516256345, 213998.0517939191], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.92, 1.0, 1.0, 0.5186008946393761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26497115322934295, 0.26497115322934295, 0.3194000773043569], 
reward next is 0.6806, 
noisyNet noise sample is [array([-0.33296642], dtype=float32), -0.16523817]. 
=============================================
[2019-03-27 11:02:37,459] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[68.2353  ]
 [68.23226 ]
 [68.21076 ]
 [68.215416]
 [68.21553 ]], R is [[68.25629425]
 [68.25349426]
 [68.24737549]
 [68.23371887]
 [68.22261047]].
[2019-03-27 11:02:38,219] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47719: loss 0.0210
[2019-03-27 11:02:38,223] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47719: learning rate 0.0000
[2019-03-27 11:02:38,733] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47909: loss 0.0017
[2019-03-27 11:02:38,740] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47910: learning rate 0.0000
[2019-03-27 11:02:38,764] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47921: loss 0.0021
[2019-03-27 11:02:38,769] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47923: learning rate 0.0000
[2019-03-27 11:02:38,780] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47927: loss 0.0029
[2019-03-27 11:02:38,785] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47928: learning rate 0.0000
[2019-03-27 11:02:38,839] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47948: loss 0.0024
[2019-03-27 11:02:38,847] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47954: loss 0.0028
[2019-03-27 11:02:38,849] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47954: learning rate 0.0000
[2019-03-27 11:02:38,851] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47954: learning rate 0.0000
[2019-03-27 11:02:38,874] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47962: loss 0.0027
[2019-03-27 11:02:38,876] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47963: learning rate 0.0000
[2019-03-27 11:02:38,907] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47974: loss 0.0035
[2019-03-27 11:02:38,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47974: learning rate 0.0000
[2019-03-27 11:02:39,006] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48010: loss 0.0051
[2019-03-27 11:02:39,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48010: learning rate 0.0000
[2019-03-27 11:02:39,024] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48017: loss 0.0095
[2019-03-27 11:02:39,027] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48017: learning rate 0.0000
[2019-03-27 11:02:39,081] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48039: loss 0.0041
[2019-03-27 11:02:39,084] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48040: learning rate 0.0000
[2019-03-27 11:02:39,105] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48047: loss 0.0130
[2019-03-27 11:02:39,107] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 48047: loss 0.0040
[2019-03-27 11:02:39,109] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48047: learning rate 0.0000
[2019-03-27 11:02:39,110] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 48049: learning rate 0.0000
[2019-03-27 11:02:39,126] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48056: loss 0.0024
[2019-03-27 11:02:39,128] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48056: learning rate 0.0000
[2019-03-27 11:02:39,158] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48064: loss 0.0024
[2019-03-27 11:02:39,159] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48064: learning rate 0.0000
[2019-03-27 11:02:39,621] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 48243: loss 0.0035
[2019-03-27 11:02:39,626] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 48244: learning rate 0.0000
[2019-03-27 11:02:42,168] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0274479e-20 1.0000000e+00 5.9081158e-19 8.6027115e-21 1.7085947e-15], sum to 1.0000
[2019-03-27 11:02:42,182] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6075
[2019-03-27 11:02:42,190] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.78333333333333, 90.33333333333333, 1.0, 2.0, 0.4406602038399923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650784.5697517538, 650784.5697517538, 179247.920661659], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1842600.0000, 
sim time next is 1843200.0000, 
raw observation next is [23.9, 90.0, 1.0, 2.0, 0.4669593329886921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687786.5045120236, 687786.5045120242, 183018.8383844287], 
processed observation next is [1.0, 0.34782608695652173, 0.33175355450236965, 0.9, 1.0, 1.0, 0.3577823289020386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19105180680889547, 0.1910518068088956, 0.2731624453498936], 
reward next is 0.7268, 
noisyNet noise sample is [array([-0.14277199], dtype=float32), 1.2185268]. 
=============================================
[2019-03-27 11:02:42,261] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6568107e-22 1.0000000e+00 4.8356953e-19 5.6218163e-21 5.4947626e-17], sum to 1.0000
[2019-03-27 11:02:42,273] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2272
[2019-03-27 11:02:42,280] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 91.0, 1.0, 2.0, 0.8637238646207229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1248636.462475907, 1248636.462475906, 266115.14753887], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1848600.0000, 
sim time next is 1849200.0000, 
raw observation next is [24.36666666666667, 90.66666666666667, 1.0, 2.0, 0.8773852283162079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1264676.635659685, 1264676.635659684, 269362.8476898289], 
processed observation next is [1.0, 0.391304347826087, 0.3538704581358612, 0.9066666666666667, 1.0, 1.0, 0.852271359417118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3512990654610236, 0.3512990654610233, 0.40203410102959536], 
reward next is 0.5980, 
noisyNet noise sample is [array([0.01302689], dtype=float32), -0.06253269]. 
=============================================
[2019-03-27 11:02:44,244] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 11:02:44,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:02:44,246] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:02:44,247] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:02:44,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:02:44,248] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:02:44,248] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:02:44,250] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:02:44,247] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:02:44,250] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:02:44,255] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:02:44,270] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run3
[2019-03-27 11:02:44,271] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run3
[2019-03-27 11:02:44,271] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run3
[2019-03-27 11:02:44,290] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run3
[2019-03-27 11:02:44,366] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run3
[2019-03-27 11:02:45,222] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0395703], dtype=float32), 0.020377165]
[2019-03-27 11:02:45,223] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.6, 87.33333333333334, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 1.0, 1.0, 0.2908760306804431, 6.911200000000001, 6.9112, 168.912956510431, 509586.3956305113, 509586.3956305107, 192090.2916908408]
[2019-03-27 11:02:45,223] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:02:45,224] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0519491  0.77030176 0.0438594  0.05543393 0.07845581], sampled 0.945743383469739
[2019-03-27 11:03:01,522] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0395703], dtype=float32), 0.020377165]
[2019-03-27 11:03:01,524] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.36666666666667, 97.0, 1.0, 2.0, 0.3802825846655978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572803.730547473, 572803.730547473, 172234.9270182879]
[2019-03-27 11:03:01,525] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:03:01,528] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.6027979e-16 1.0000000e+00 1.0265265e-14 2.2773877e-14 4.8811654e-12], sampled 0.22575921366783558
[2019-03-27 11:03:20,513] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0395703], dtype=float32), 0.020377165]
[2019-03-27 11:03:20,513] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.83652056666667, 61.80642891, 1.0, 2.0, 0.996518214131548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1392934.414614433, 1392934.414614433, 297865.0482547404]
[2019-03-27 11:03:20,513] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:03:20,516] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2247414e-15 1.0000000e+00 2.4693667e-14 1.3093194e-13 9.5644621e-12], sampled 0.2575741071411486
[2019-03-27 11:03:41,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0395703], dtype=float32), 0.020377165]
[2019-03-27 11:03:41,409] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.5182478683829802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724179.5685848698, 724179.5685848705, 186504.5350088705]
[2019-03-27 11:03:41,410] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:03:41,413] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.0607021e-16 1.0000000e+00 9.6532245e-15 3.8636510e-14 4.7746820e-12], sampled 0.15433053910484296
[2019-03-27 11:03:57,049] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0395703], dtype=float32), 0.020377165]
[2019-03-27 11:03:57,050] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.68486720333334, 94.86128136333335, 1.0, 2.0, 0.5711226321134302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798092.4795759658, 798092.4795759658, 195488.1657235927]
[2019-03-27 11:03:57,051] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:03:57,054] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.3974244e-16 1.0000000e+00 1.2207056e-14 4.5637695e-14 5.7215916e-12], sampled 0.7570492722209645
[2019-03-27 11:04:17,678] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0395703], dtype=float32), 0.020377165]
[2019-03-27 11:04:17,680] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.76666666666667, 59.66666666666667, 1.0, 2.0, 0.6124120984077167, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.949277552645894, 6.9112, 168.9126290584913, 1712318.221445834, 1685304.747438604, 366901.8838662416]
[2019-03-27 11:04:17,681] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:04:17,686] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0677162e-14 1.0000000e+00 1.7958746e-13 2.2691085e-12 5.7513116e-11], sampled 0.09994429881952815
[2019-03-27 11:04:17,686] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1712318.221445834 W.
[2019-03-27 11:04:42,946] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0395703], dtype=float32), 0.020377165]
[2019-03-27 11:04:42,947] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.76666666666667, 64.83333333333334, 1.0, 2.0, 0.7427624077875608, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977953676807, 6.9112, 168.9123160291425, 1934970.2280139, 1867731.74062488, 394209.9235198506]
[2019-03-27 11:04:42,948] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:04:42,950] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5712875e-14 1.0000000e+00 1.2208949e-13 1.3173209e-12 4.0889615e-11], sampled 0.8695778594155991
[2019-03-27 11:04:42,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1934970.2280139 W.
[2019-03-27 11:04:51,130] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 11:04:51,190] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6622 3164088880.7966 1778.0000
[2019-03-27 11:04:51,574] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 11:04:51,622] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.1948 2842566134.9955 1131.0000
[2019-03-27 11:04:51,743] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9509 2779224265.4428 933.0000
[2019-03-27 11:04:52,758] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 50000, evaluation results [50000.0, 7882.662179158295, 3164088880.7965546, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8659.950916828071, 2779224265.4427943, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8495.194795952744, 2842566134.995477, 1131.0]
[2019-03-27 11:04:55,038] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4263432e-18 1.0000000e+00 2.3591527e-16 8.5667958e-19 1.1757366e-13], sum to 1.0000
[2019-03-27 11:04:55,050] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5436
[2019-03-27 11:04:55,055] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 96.0, 1.0, 2.0, 0.4667344213826579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657379.6927654248, 657379.6927654248, 179220.1032692424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1908000.0000, 
sim time next is 1908600.0000, 
raw observation next is [24.15, 95.83333333333333, 1.0, 2.0, 0.7425589817473437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1048103.116334434, 1048103.116334434, 231712.7912930236], 
processed observation next is [1.0, 0.08695652173913043, 0.34360189573459715, 0.9583333333333333, 1.0, 1.0, 0.6898300984907755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2911397545373428, 0.2911397545373428, 0.3458399870045128], 
reward next is 0.6542, 
noisyNet noise sample is [array([0.9427494], dtype=float32), 0.3459538]. 
=============================================
[2019-03-27 11:04:56,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2097761e-20 1.0000000e+00 2.5356775e-17 4.4640689e-20 4.5566785e-15], sum to 1.0000
[2019-03-27 11:04:56,091] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2254
[2019-03-27 11:04:56,098] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.86666666666667, 91.33333333333334, 1.0, 2.0, 0.4351395411081841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 637203.9112639973, 637203.9112639967, 177774.0840418001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1921200.0000, 
sim time next is 1921800.0000, 
raw observation next is [23.98333333333333, 90.66666666666666, 1.0, 2.0, 0.4370288705946396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639250.9888726175, 639250.9888726175, 177959.2335691631], 
processed observation next is [1.0, 0.21739130434782608, 0.3357030015797787, 0.9066666666666666, 1.0, 1.0, 0.3217215308369151, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17756971913128264, 0.17756971913128264, 0.2656107963718852], 
reward next is 0.7344, 
noisyNet noise sample is [array([1.4582335], dtype=float32), 0.0666996]. 
=============================================
[2019-03-27 11:05:05,376] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9112679e-21 1.0000000e+00 4.2309157e-20 4.0573443e-21 3.8757390e-15], sum to 1.0000
[2019-03-27 11:05:05,389] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4915
[2019-03-27 11:05:05,395] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 87.16666666666667, 1.0, 2.0, 0.4925048830516131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688195.661996717, 688195.661996717, 182430.0129002606], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2052600.0000, 
sim time next is 2053200.0000, 
raw observation next is [26.0, 87.33333333333334, 1.0, 2.0, 0.4908057616097099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685820.6490539908, 685820.6490539914, 182168.0342245646], 
processed observation next is [0.0, 0.782608695652174, 0.4312796208530806, 0.8733333333333334, 1.0, 1.0, 0.3865129657948312, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19050573584833078, 0.19050573584833094, 0.2718925883948725], 
reward next is 0.7281, 
noisyNet noise sample is [array([-0.11363988], dtype=float32), -0.23737778]. 
=============================================
[2019-03-27 11:05:07,686] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55692: loss 0.0004
[2019-03-27 11:05:07,689] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55692: learning rate 0.0000
[2019-03-27 11:05:08,104] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55855: loss 0.0020
[2019-03-27 11:05:08,108] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55855: learning rate 0.0000
[2019-03-27 11:05:08,191] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55886: loss 0.0031
[2019-03-27 11:05:08,195] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55886: learning rate 0.0000
[2019-03-27 11:05:08,220] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55899: loss 0.0041
[2019-03-27 11:05:08,223] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55899: learning rate 0.0000
[2019-03-27 11:05:08,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.8563300e-20 1.0000000e+00 1.9638851e-19 8.6674807e-19 3.9470864e-16], sum to 1.0000
[2019-03-27 11:05:08,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4951
[2019-03-27 11:05:08,279] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 89.5, 1.0, 2.0, 0.502991258128049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702853.5172770913, 702853.5172770907, 184066.4203599223], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2101800.0000, 
sim time next is 2102400.0000, 
raw observation next is [26.3, 89.0, 1.0, 2.0, 0.5041720142207456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704503.9908351148, 704503.9908351141, 184252.7491142351], 
processed observation next is [0.0, 0.34782608695652173, 0.4454976303317536, 0.89, 1.0, 1.0, 0.40261688460330797, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19569555300975414, 0.19569555300975394, 0.2750041031555748], 
reward next is 0.7250, 
noisyNet noise sample is [array([-0.34546033], dtype=float32), 0.447221]. 
=============================================
[2019-03-27 11:05:08,483] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55998: loss 0.0002
[2019-03-27 11:05:08,486] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55998: learning rate 0.0000
[2019-03-27 11:05:08,489] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55999: loss 0.0002
[2019-03-27 11:05:08,493] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56000: learning rate 0.0000
[2019-03-27 11:05:08,521] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56012: loss 0.0002
[2019-03-27 11:05:08,522] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 56012: loss 0.0012
[2019-03-27 11:05:08,523] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56012: learning rate 0.0000
[2019-03-27 11:05:08,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 56012: learning rate 0.0000
[2019-03-27 11:05:08,542] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56017: loss 0.0008
[2019-03-27 11:05:08,544] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56018: learning rate 0.0000
[2019-03-27 11:05:08,546] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56018: loss 0.0020
[2019-03-27 11:05:08,551] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56019: learning rate 0.0000
[2019-03-27 11:05:08,564] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56025: loss 0.0022
[2019-03-27 11:05:08,568] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56025: learning rate 0.0000
[2019-03-27 11:05:08,576] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56028: loss 0.0021
[2019-03-27 11:05:08,577] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56028: learning rate 0.0000
[2019-03-27 11:05:08,599] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56035: loss 0.0035
[2019-03-27 11:05:08,600] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56035: learning rate 0.0000
[2019-03-27 11:05:08,633] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56045: loss 0.0002
[2019-03-27 11:05:08,641] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56048: learning rate 0.0000
[2019-03-27 11:05:08,702] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56073: loss 0.0030
[2019-03-27 11:05:08,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56073: learning rate 0.0000
[2019-03-27 11:05:09,044] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 56201: loss 0.0098
[2019-03-27 11:05:09,048] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 56201: learning rate 0.0000
[2019-03-27 11:05:19,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7944532e-18 1.0000000e+00 1.5418549e-17 6.9148153e-15 5.2133008e-14], sum to 1.0000
[2019-03-27 11:05:19,218] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2809
[2019-03-27 11:05:19,341] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 86.0, 1.0, 2.0, 0.5111831138728875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714304.2341105256, 714304.2341105263, 185366.8151638563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2250600.0000, 
sim time next is 2251200.0000, 
raw observation next is [26.7, 86.0, 1.0, 2.0, 0.509365435630883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711763.4415499672, 711763.4415499666, 185076.4852775885], 
processed observation next is [1.0, 0.043478260869565216, 0.46445497630331756, 0.86, 1.0, 1.0, 0.4088740188323891, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1977120670972131, 0.19771206709721295, 0.27623356011580374], 
reward next is 0.7238, 
noisyNet noise sample is [array([1.2626694], dtype=float32), -0.26652223]. 
=============================================
[2019-03-27 11:05:28,784] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63676: loss 37.7413
[2019-03-27 11:05:28,787] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63677: learning rate 0.0000
[2019-03-27 11:05:29,170] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63821: loss 36.0447
[2019-03-27 11:05:29,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63822: learning rate 0.0000
[2019-03-27 11:05:29,230] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63845: loss 40.1934
[2019-03-27 11:05:29,232] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63845: learning rate 0.0000
[2019-03-27 11:05:29,383] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63899: loss 42.8739
[2019-03-27 11:05:29,388] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63899: learning rate 0.0000
[2019-03-27 11:05:29,606] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63982: loss 48.0046
[2019-03-27 11:05:29,608] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63983: learning rate 0.0000
[2019-03-27 11:05:29,632] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63996: loss 45.5830
[2019-03-27 11:05:29,635] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63996: learning rate 0.0000
[2019-03-27 11:05:29,644] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64000: loss 36.6096
[2019-03-27 11:05:29,646] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64001: learning rate 0.0000
[2019-03-27 11:05:29,664] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 64011: loss 33.8189
[2019-03-27 11:05:29,665] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 64011: learning rate 0.0000
[2019-03-27 11:05:29,699] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64027: loss 34.5031
[2019-03-27 11:05:29,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64027: learning rate 0.0000
[2019-03-27 11:05:29,705] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64030: loss 40.9082
[2019-03-27 11:05:29,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64032: learning rate 0.0000
[2019-03-27 11:05:29,716] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64033: loss 34.9781
[2019-03-27 11:05:29,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64034: learning rate 0.0000
[2019-03-27 11:05:29,728] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 64034: loss 36.2701
[2019-03-27 11:05:29,730] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 64034: learning rate 0.0000
[2019-03-27 11:05:29,853] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64079: loss 44.8874
[2019-03-27 11:05:29,857] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64079: learning rate 0.0000
[2019-03-27 11:05:29,893] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64092: loss 38.1930
[2019-03-27 11:05:29,902] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64094: learning rate 0.0000
[2019-03-27 11:05:29,964] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64119: loss 36.1250
[2019-03-27 11:05:29,967] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64119: learning rate 0.0000
[2019-03-27 11:05:30,276] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64238: loss 31.3555
[2019-03-27 11:05:30,282] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64238: learning rate 0.0000
[2019-03-27 11:05:42,506] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1340494e-17 1.0000000e+00 1.9590982e-17 8.1639034e-14 9.4968201e-16], sum to 1.0000
[2019-03-27 11:05:42,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8428
[2019-03-27 11:05:42,527] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 87.66666666666667, 1.0, 2.0, 0.5300133249570241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740625.8949625496, 740625.8949625501, 188431.7795252927], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2582400.0000, 
sim time next is 2583000.0000, 
raw observation next is [26.75, 88.0, 1.0, 2.0, 0.527408824582229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736985.1759527229, 736985.1759527235, 188001.5331280662], 
processed observation next is [1.0, 0.9130434782608695, 0.4668246445497631, 0.88, 1.0, 1.0, 0.43061304166533615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20471810443131191, 0.20471810443131208, 0.2805993031762182], 
reward next is 0.7194, 
noisyNet noise sample is [array([-0.71927315], dtype=float32), 0.51921105]. 
=============================================
[2019-03-27 11:05:42,550] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[62.749275]
 [62.55413 ]
 [62.387917]
 [62.79645 ]
 [62.97971 ]], R is [[62.92201996]
 [63.01155853]
 [63.09980011]
 [63.18680191]
 [63.27290344]].
[2019-03-27 11:05:49,715] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71649: loss 0.0597
[2019-03-27 11:05:49,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71649: learning rate 0.0000
[2019-03-27 11:05:49,980] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71752: loss 0.0647
[2019-03-27 11:05:49,985] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71753: learning rate 0.0000
[2019-03-27 11:05:50,297] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71866: loss 0.0400
[2019-03-27 11:05:50,307] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71869: learning rate 0.0000
[2019-03-27 11:05:50,360] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71893: loss 0.0415
[2019-03-27 11:05:50,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71893: learning rate 0.0000
[2019-03-27 11:05:50,473] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71934: loss 0.0331
[2019-03-27 11:05:50,475] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71934: learning rate 0.0000
[2019-03-27 11:05:50,554] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 71964: loss 0.0379
[2019-03-27 11:05:50,556] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 71965: learning rate 0.0000
[2019-03-27 11:05:50,629] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71993: loss 0.0279
[2019-03-27 11:05:50,636] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71994: learning rate 0.0000
[2019-03-27 11:05:50,670] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72010: loss 0.0400
[2019-03-27 11:05:50,674] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72010: learning rate 0.0000
[2019-03-27 11:05:50,684] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 72013: loss 0.0254
[2019-03-27 11:05:50,687] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 72014: learning rate 0.0000
[2019-03-27 11:05:50,721] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72027: loss 0.0271
[2019-03-27 11:05:50,723] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72028: learning rate 0.0000
[2019-03-27 11:05:50,760] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72041: loss 0.0217
[2019-03-27 11:05:50,762] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72041: learning rate 0.0000
[2019-03-27 11:05:50,831] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72067: loss 0.0144
[2019-03-27 11:05:50,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72068: learning rate 0.0000
[2019-03-27 11:05:50,855] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72077: loss 0.0080
[2019-03-27 11:05:50,857] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72077: learning rate 0.0000
[2019-03-27 11:05:50,911] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72096: loss 0.0052
[2019-03-27 11:05:50,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72097: learning rate 0.0000
[2019-03-27 11:05:51,121] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72175: loss 0.0011
[2019-03-27 11:05:51,126] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72177: learning rate 0.0000
[2019-03-27 11:05:51,238] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 72219: loss 0.0003
[2019-03-27 11:05:51,243] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 72220: learning rate 0.0000
[2019-03-27 11:05:58,542] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 11:05:58,544] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:05:58,544] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:05:58,545] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:05:58,547] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:05:58,549] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:05:58,552] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:05:58,553] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:05:58,555] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:05:58,556] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:05:58,557] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:05:58,581] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run4
[2019-03-27 11:05:58,607] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run4
[2019-03-27 11:05:58,610] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run4
[2019-03-27 11:05:58,646] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run4
[2019-03-27 11:05:58,665] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run4
[2019-03-27 11:06:06,558] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04877505], dtype=float32), 0.0226429]
[2019-03-27 11:06:06,558] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.1, 89.66666666666667, 1.0, 2.0, 0.3271820811583991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510491.2262006814, 510491.2262006814, 167658.6260182079]
[2019-03-27 11:06:06,559] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:06:06,560] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.3033599e-18 1.0000000e+00 7.3053002e-18 5.7131970e-15 7.8663256e-16], sampled 0.5324430006613879
[2019-03-27 11:06:30,686] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04877505], dtype=float32), 0.0226429]
[2019-03-27 11:06:30,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.1, 93.66666666666667, 1.0, 2.0, 0.402049920596832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598273.8111152219, 598273.8111152214, 174330.5577556562]
[2019-03-27 11:06:30,690] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:06:30,694] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.4530600e-18 1.0000000e+00 5.2731234e-18 1.1749536e-14 6.8758297e-16], sampled 0.9972056908769603
[2019-03-27 11:06:49,067] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04877505], dtype=float32), 0.0226429]
[2019-03-27 11:06:49,069] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3432310324574373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532561.5974290697, 532561.5974290697, 169320.7402912078]
[2019-03-27 11:06:49,072] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:06:49,075] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1765305e-18 1.0000000e+00 5.5195281e-18 5.5132866e-15 5.8063749e-16], sampled 0.7559293627738409
[2019-03-27 11:06:57,243] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04877505], dtype=float32), 0.0226429]
[2019-03-27 11:06:57,243] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.00000000000001, 1.0, 2.0, 1.039523179617277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0622343713466, 1453087.07360302, 1453087.07360302, 311222.9894951631]
[2019-03-27 11:06:57,243] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:06:57,248] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4907343e-17 1.0000000e+00 2.4735111e-17 5.3102766e-14 2.1769058e-15], sampled 0.8863197887127527
[2019-03-27 11:08:00,134] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04877505], dtype=float32), 0.0226429]
[2019-03-27 11:08:00,135] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.2, 69.5, 1.0, 2.0, 0.5358515673444331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748786.9681297088, 748786.9681297082, 189404.1746039596]
[2019-03-27 11:08:00,138] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:08:00,141] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7142794e-17 1.0000000e+00 2.2513242e-17 3.8257971e-14 2.0215896e-15], sampled 0.7907007529339909
[2019-03-27 11:08:06,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 11:08:06,352] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.8300 2927549971.3083 1338.0000
[2019-03-27 11:08:06,443] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1769 3007673691.3038 1766.0000
[2019-03-27 11:08:06,531] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9871 2779199396.2726 933.0000
[2019-03-27 11:08:06,836] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-27 11:08:07,851] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 75000, evaluation results [75000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8252.829988502745, 2927549971.308261, 1338.0, 8659.987082117317, 2779199396.272587, 933.0, 7998.176928517046, 3007673691.3037777, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-27 11:08:11,611] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.5966471e-20 1.0000000e+00 1.8070500e-19 2.7379493e-16 7.5884656e-17], sum to 1.0000
[2019-03-27 11:08:11,622] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8216
[2019-03-27 11:08:11,637] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.463536188504447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714057.5529147636, 714057.552914763, 186056.7008064638], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2859000.0000, 
sim time next is 2859600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3977644377628135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612691.2463478635, 612691.2463478629, 176141.2239813128], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2744149852564018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17019201287440652, 0.17019201287440636, 0.26289734922584], 
reward next is 0.7371, 
noisyNet noise sample is [array([-0.67232835], dtype=float32), -2.0944762]. 
=============================================
[2019-03-27 11:08:19,903] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79589: loss 0.3084
[2019-03-27 11:08:19,912] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79591: learning rate 0.0000
[2019-03-27 11:08:20,502] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79794: loss 0.2835
[2019-03-27 11:08:20,505] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79795: learning rate 0.0000
[2019-03-27 11:08:20,648] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79822: loss 0.2529
[2019-03-27 11:08:20,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79823: learning rate 0.0000
[2019-03-27 11:08:20,792] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79853: loss 0.2355
[2019-03-27 11:08:20,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79853: learning rate 0.0000
[2019-03-27 11:08:21,131] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79963: loss 0.2973
[2019-03-27 11:08:21,133] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79964: learning rate 0.0000
[2019-03-27 11:08:21,253] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 79980: loss 0.2752
[2019-03-27 11:08:21,254] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 79980: learning rate 0.0000
[2019-03-27 11:08:21,379] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80003: loss 0.2777
[2019-03-27 11:08:21,386] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80005: learning rate 0.0000
[2019-03-27 11:08:21,465] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1817626e-21 1.0000000e+00 9.5501213e-22 3.7401152e-19 1.0141084e-18], sum to 1.0000
[2019-03-27 11:08:21,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6487
[2019-03-27 11:08:21,484] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3062731660905995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 487720.7722757117, 487720.7722757111, 166178.537619431], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3007800.0000, 
sim time next is 3008400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3064534176698839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488008.0487519372, 488008.0487519379, 166199.4117093935], 
processed observation next is [1.0, 0.8260869565217391, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16440170803600465, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13555779131998255, 0.13555779131998275, 0.24805882344685595], 
reward next is 0.7519, 
noisyNet noise sample is [array([1.4810127], dtype=float32), -1.86435]. 
=============================================
[2019-03-27 11:08:21,488] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80021: loss 0.2763
[2019-03-27 11:08:21,489] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80021: learning rate 0.0000
[2019-03-27 11:08:21,607] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 80044: loss 0.2284
[2019-03-27 11:08:21,608] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 80045: learning rate 0.0000
[2019-03-27 11:08:21,697] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80053: loss 0.2276
[2019-03-27 11:08:21,698] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80054: loss 0.2229
[2019-03-27 11:08:21,700] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80054: learning rate 0.0000
[2019-03-27 11:08:21,702] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80054: learning rate 0.0000
[2019-03-27 11:08:21,703] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80054: loss 0.2251
[2019-03-27 11:08:21,778] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80055: learning rate 0.0000
[2019-03-27 11:08:21,936] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80064: loss 0.1920
[2019-03-27 11:08:21,939] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80064: learning rate 0.0000
[2019-03-27 11:08:21,943] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80065: loss 0.2341
[2019-03-27 11:08:22,017] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80065: learning rate 0.0000
[2019-03-27 11:08:22,322] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80165: loss 0.1393
[2019-03-27 11:08:22,326] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80166: learning rate 0.0000
[2019-03-27 11:08:22,539] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 80224: loss 0.1207
[2019-03-27 11:08:22,542] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 80225: learning rate 0.0000
[2019-03-27 11:08:26,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6411485e-19 1.0000000e+00 1.0397104e-19 8.9116651e-18 3.0941809e-17], sum to 1.0000
[2019-03-27 11:08:26,596] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0370
[2019-03-27 11:08:26,601] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666666, 96.0, 1.0, 2.0, 0.6815411511090337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1020290.962484362, 1020290.962484362, 225630.4072453968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3062400.0000, 
sim time next is 3063000.0000, 
raw observation next is [22.83333333333334, 95.0, 1.0, 2.0, 0.6945003252120712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1038103.871645765, 1038103.871645764, 228399.7974447252], 
processed observation next is [1.0, 0.43478260869565216, 0.2812006319115327, 0.95, 1.0, 1.0, 0.631928102665146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28836218656826806, 0.2883621865682678, 0.34089522006675405], 
reward next is 0.6591, 
noisyNet noise sample is [array([-0.14774126], dtype=float32), -0.07245887]. 
=============================================
[2019-03-27 11:08:26,620] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.57245 ]
 [68.650635]
 [68.703995]
 [68.76575 ]
 [68.854515]], R is [[68.4911499 ]
 [68.46947479]
 [68.45461273]
 [68.44361877]
 [68.42779541]].
[2019-03-27 11:08:35,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3486146e-19 1.0000000e+00 1.5529447e-19 5.0211228e-18 5.6088226e-18], sum to 1.0000
[2019-03-27 11:08:35,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0599
[2019-03-27 11:08:35,917] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4862856898744213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679502.5692020772, 679502.5692020766, 181475.2182603159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3199800.0000, 
sim time next is 3200400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4859197774397443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678991.1046314024, 678991.1046314024, 181419.4148364319], 
processed observation next is [0.0, 0.043478260869565216, 0.38388625592417064, 0.94, 1.0, 1.0, 0.38062623787921, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18860864017538956, 0.18860864017538956, 0.27077524602452524], 
reward next is 0.7292, 
noisyNet noise sample is [array([1.7597182], dtype=float32), -0.57369363]. 
=============================================
[2019-03-27 11:08:42,007] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87575: loss 0.0144
[2019-03-27 11:08:42,008] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87575: learning rate 0.0000
[2019-03-27 11:08:42,467] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87747: loss 0.0129
[2019-03-27 11:08:42,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87748: learning rate 0.0000
[2019-03-27 11:08:42,571] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87791: loss 0.0266
[2019-03-27 11:08:42,573] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87791: learning rate 0.0000
[2019-03-27 11:08:42,780] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87865: loss 0.0048
[2019-03-27 11:08:42,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87865: learning rate 0.0000
[2019-03-27 11:08:43,038] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 87966: loss 0.0083
[2019-03-27 11:08:43,039] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 87966: learning rate 0.0000
[2019-03-27 11:08:43,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1480784e-20 1.0000000e+00 1.3008637e-20 8.0187128e-18 9.3856677e-18], sum to 1.0000
[2019-03-27 11:08:43,070] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4111
[2019-03-27 11:08:43,075] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4228097815742998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619072.0669224252, 619072.0669224246, 176004.9039168248], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3299400.0000, 
sim time next is 3300000.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4225399448233598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 618677.5883841622, 618677.5883841629, 175967.0038830437], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.83, 1.0, 1.0, 0.30426499376308414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17185488566226728, 0.17185488566226748, 0.2626373192284234], 
reward next is 0.7374, 
noisyNet noise sample is [array([1.2247345], dtype=float32), 0.63451505]. 
=============================================
[2019-03-27 11:08:43,090] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[72.2397  ]
 [72.21815 ]
 [72.2109  ]
 [72.300705]
 [72.228134]], R is [[72.13549805]
 [72.15145111]
 [72.1671524 ]
 [72.18250275]
 [72.1973114 ]].
[2019-03-27 11:08:43,091] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87982: loss 0.0080
[2019-03-27 11:08:43,092] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87982: learning rate 0.0000
[2019-03-27 11:08:43,143] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 88000: loss 0.0056
[2019-03-27 11:08:43,146] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 88000: learning rate 0.0000
[2019-03-27 11:08:43,164] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88008: loss 0.0041
[2019-03-27 11:08:43,166] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88009: learning rate 0.0000
[2019-03-27 11:08:43,166] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 88010: loss 0.0051
[2019-03-27 11:08:43,169] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 88011: learning rate 0.0000
[2019-03-27 11:08:43,184] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88014: loss 0.0050
[2019-03-27 11:08:43,186] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88015: learning rate 0.0000
[2019-03-27 11:08:43,298] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88056: loss 0.0044
[2019-03-27 11:08:43,302] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88056: learning rate 0.0000
[2019-03-27 11:08:43,313] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88061: loss 0.0036
[2019-03-27 11:08:43,318] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88062: learning rate 0.0000
[2019-03-27 11:08:43,449] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88113: loss 0.0083
[2019-03-27 11:08:43,451] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88114: learning rate 0.0000
[2019-03-27 11:08:43,467] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 88120: loss 0.0098
[2019-03-27 11:08:43,469] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 88121: learning rate 0.0000
[2019-03-27 11:08:43,691] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88205: loss 0.0031
[2019-03-27 11:08:43,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88205: learning rate 0.0000
[2019-03-27 11:08:43,864] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 88268: loss 0.0065
[2019-03-27 11:08:43,866] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 88268: learning rate 0.0000
[2019-03-27 11:08:51,988] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5286500e-07 9.7393894e-01 4.1944087e-08 2.6060887e-02 1.3349380e-09], sum to 1.0000
[2019-03-27 11:08:51,999] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9530
[2019-03-27 11:08:52,007] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3720087.864704024 W.
[2019-03-27 11:08:52,011] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.5, 65.0, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 0.0, 1.0, 0.0, 8.04168823387141, 6.9112, 170.5573041426782, 3720087.864704024, 2910273.157460698, 547016.2756037645], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3418200.0000, 
sim time next is 3418800.0000, 
raw observation next is [33.66666666666666, 64.33333333333334, 1.0, 2.0, 0.6962556046994031, 1.0, 2.0, 0.668717841863964, 1.0, 1.0, 1.03, 7.005097436915748, 6.9112, 170.5573041426782, 2805923.499645561, 2738660.946079281, 520149.9019596566], 
processed observation next is [1.0, 0.5652173913043478, 0.7946287519747232, 0.6433333333333334, 1.0, 1.0, 0.6340428972281964, 1.0, 1.0, 0.6008648697156193, 1.0, 0.5, 1.0365853658536586, 0.009389743691574814, 0.0, 0.8375144448122397, 0.7794231943459892, 0.7607391516886892, 0.7763431372532188], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6886683], dtype=float32), -0.042869553]. 
=============================================
[2019-03-27 11:08:55,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1205249e-13 1.0000000e+00 1.6747066e-12 1.6450675e-08 7.0709215e-13], sum to 1.0000
[2019-03-27 11:08:55,958] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4860
[2019-03-27 11:08:55,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1997568.903432091 W.
[2019-03-27 11:08:55,973] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.66666666666666, 67.33333333333333, 1.0, 2.0, 0.714342793359177, 1.0, 2.0, 0.714342793359177, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1997568.903432091, 1997568.90343209, 380317.5906875111], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3487800.0000, 
sim time next is 3488400.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.6568089427552405, 1.0, 2.0, 0.6568089427552405, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1836544.904654201, 1836544.904654201, 356176.3669000239], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.5865167985002897, 1.0, 1.0, 0.5865167985002897, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5101513624039447, 0.5101513624039447, 0.5316065177612297], 
reward next is 0.4684, 
noisyNet noise sample is [array([-0.17255172], dtype=float32), -0.4450821]. 
=============================================
[2019-03-27 11:08:56,850] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.1493578e-07 9.9521232e-01 3.1930476e-08 4.7871359e-03 1.1950718e-09], sum to 1.0000
[2019-03-27 11:08:56,860] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5956
[2019-03-27 11:08:56,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2224005.979482322 W.
[2019-03-27 11:08:56,874] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 66.0, 1.0, 2.0, 0.9492755530104946, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.983222305065255, 6.9112, 168.9125279994114, 2224005.979482322, 2172911.00707447, 448910.1772906657], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3493800.0000, 
sim time next is 3494400.0000, 
raw observation next is [30.66666666666666, 66.0, 1.0, 2.0, 0.5315491150245698, 1.0, 1.0, 0.5315491150245698, 1.0, 2.0, 0.9154036228880599, 6.911199999999999, 6.9112, 170.5573041426782, 2229853.350816633, 2229853.350816634, 436152.2737624306], 
processed observation next is [1.0, 0.43478260869565216, 0.6524486571879934, 0.66, 1.0, 1.0, 0.43560134340309614, 1.0, 0.5, 0.43560134340309614, 1.0, 1.0, 0.8968336864488534, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6194037085601759, 0.6194037085601761, 0.6509735429290009], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8471111], dtype=float32), -0.22719]. 
=============================================
[2019-03-27 11:09:03,309] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95665: loss 59.7684
[2019-03-27 11:09:03,312] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95665: learning rate 0.0000
[2019-03-27 11:09:03,665] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95800: loss 9.6295
[2019-03-27 11:09:03,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95801: learning rate 0.0000
[2019-03-27 11:09:03,692] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95808: loss 67.8599
[2019-03-27 11:09:03,694] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95809: learning rate 0.0000
[2019-03-27 11:09:03,900] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95887: loss 9.0772
[2019-03-27 11:09:03,900] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95887: learning rate 0.0000
[2019-03-27 11:09:03,993] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95919: loss 1.3199
[2019-03-27 11:09:03,994] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95919: learning rate 0.0000
[2019-03-27 11:09:04,100] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 95959: loss -1.6736
[2019-03-27 11:09:04,107] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 95961: learning rate 0.0000
[2019-03-27 11:09:04,113] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95966: loss -76.7884
[2019-03-27 11:09:04,116] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95967: learning rate 0.0000
[2019-03-27 11:09:04,178] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95987: loss -33.2809
[2019-03-27 11:09:04,181] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95988: learning rate 0.0000
[2019-03-27 11:09:04,248] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96013: loss -60.7916
[2019-03-27 11:09:04,253] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96015: learning rate 0.0000
[2019-03-27 11:09:04,378] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96063: loss 35.5145
[2019-03-27 11:09:04,381] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96064: learning rate 0.0000
[2019-03-27 11:09:04,418] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 96078: loss 2.4677
[2019-03-27 11:09:04,422] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 96080: learning rate 0.0000
[2019-03-27 11:09:04,442] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96087: loss -21.5089
[2019-03-27 11:09:04,446] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96087: learning rate 0.0000
[2019-03-27 11:09:04,448] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 96087: loss -41.5647
[2019-03-27 11:09:04,453] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 96089: learning rate 0.0000
[2019-03-27 11:09:04,536] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 96122: loss -16.6184
[2019-03-27 11:09:04,546] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 96123: learning rate 0.0000
[2019-03-27 11:09:04,755] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96210: loss -47.8701
[2019-03-27 11:09:04,757] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96210: learning rate 0.0000
[2019-03-27 11:09:04,845] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 96251: loss -94.8439
[2019-03-27 11:09:04,847] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 96251: learning rate 0.0000
[2019-03-27 11:09:13,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6299488e-14 1.0000000e+00 8.7554577e-16 3.9499626e-09 2.4541321e-16], sum to 1.0000
[2019-03-27 11:09:13,834] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3714
[2019-03-27 11:09:13,843] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 79.0, 1.0, 2.0, 0.5711560854166213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822849.6763475658, 822849.6763475658, 198649.3753765672], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3733200.0000, 
sim time next is 3733800.0000, 
raw observation next is [26.0, 79.00000000000001, 1.0, 2.0, 0.5126395231903313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738920.2227204571, 738920.2227204578, 188467.3204370738], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.7900000000000001, 1.0, 1.0, 0.4128187026389534, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2052556174223492, 0.2052556174223494, 0.28129450811503554], 
reward next is 0.7187, 
noisyNet noise sample is [array([-2.6558099], dtype=float32), -0.13763751]. 
=============================================
[2019-03-27 11:09:14,711] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 11:09:14,715] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:09:14,716] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:09:14,716] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:09:14,717] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:09:14,716] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:09:14,717] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:09:14,721] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:09:14,721] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:09:14,718] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:09:14,725] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:09:14,742] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run5
[2019-03-27 11:09:14,742] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run5
[2019-03-27 11:09:14,760] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run5
[2019-03-27 11:09:14,761] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run5
[2019-03-27 11:09:14,761] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run5
[2019-03-27 11:09:27,334] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04886394], dtype=float32), 0.02225026]
[2019-03-27 11:09:27,337] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.98289557, 45.69307334, 1.0, 2.0, 0.3279661048509397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544715.0764320814, 544715.0764320814, 169391.1761811881]
[2019-03-27 11:09:27,339] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:09:27,341] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6880385e-15 1.0000000e+00 8.4748870e-16 3.9588735e-11 1.1242353e-16], sampled 0.5436487759307211
[2019-03-27 11:09:41,077] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04886394], dtype=float32), 0.02225026]
[2019-03-27 11:09:41,078] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.19304133333333, 93.46264863333334, 1.0, 2.0, 0.5473322020686087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764835.5254695225, 764835.5254695225, 191340.5101773156]
[2019-03-27 11:09:41,079] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:09:41,083] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.4198117e-16 1.0000000e+00 2.9473470e-16 2.9487319e-11 3.4729142e-17], sampled 0.041301704280981566
[2019-03-27 11:09:48,218] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04886394], dtype=float32), 0.02225026]
[2019-03-27 11:09:48,219] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.45, 78.5, 1.0, 2.0, 0.5000235738297131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698705.2675882919, 698705.2675882919, 183599.5080636797]
[2019-03-27 11:09:48,220] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:09:48,224] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.3690324e-16 1.0000000e+00 2.6551291e-16 5.4503759e-11 2.7403827e-17], sampled 0.6748952988086965
[2019-03-27 11:09:56,933] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04886394], dtype=float32), 0.02225026]
[2019-03-27 11:09:56,934] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.25, 94.00000000000001, 1.0, 2.0, 0.4647424850792408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660948.66166472, 660948.6616647195, 179742.0172203618]
[2019-03-27 11:09:56,938] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:09:56,942] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.3550157e-16 1.0000000e+00 1.7936058e-16 7.2008385e-11 1.4443887e-17], sampled 0.5128790913914282
[2019-03-27 11:09:58,244] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04886394], dtype=float32), 0.02225026]
[2019-03-27 11:09:58,245] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 94.33333333333334, 1.0, 2.0, 0.440941573061626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632578.9880797551, 632578.9880797557, 176978.6645716897]
[2019-03-27 11:09:58,246] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:09:58,249] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.3454813e-16 1.0000000e+00 1.3452614e-16 1.3588475e-11 1.9510214e-17], sampled 0.25704052518953124
[2019-03-27 11:10:34,420] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04886394], dtype=float32), 0.02225026]
[2019-03-27 11:10:34,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.43333333333334, 71.66666666666667, 1.0, 2.0, 0.5642889314817711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 788539.4532640203, 788539.4532640208, 194281.2584381487]
[2019-03-27 11:10:34,424] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:10:34,427] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.2137107e-15 1.0000000e+00 1.1391772e-15 7.2779038e-10 7.4374343e-17], sampled 0.2594026160573598
[2019-03-27 11:10:48,439] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04886394], dtype=float32), 0.02225026]
[2019-03-27 11:10:48,440] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.4, 82.0, 1.0, 2.0, 0.5941795216001496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830325.0013724337, 830325.0013724337, 199665.9228362617]
[2019-03-27 11:10:48,441] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:10:48,444] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0721601e-15 1.0000000e+00 4.6630974e-16 3.9533521e-10 3.1251416e-17], sampled 0.3113439160914492
[2019-03-27 11:11:04,558] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04886394], dtype=float32), 0.02225026]
[2019-03-27 11:11:04,559] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.1, 81.0, 1.0, 2.0, 0.3336830959583217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 522265.7318182595, 522265.7318182601, 168618.4695515548]
[2019-03-27 11:11:04,560] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:11:04,563] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.36208204e-16 1.00000000e+00 2.18290026e-16 1.30723965e-11
 2.94346894e-17], sampled 0.3499541532429521
[2019-03-27 11:11:22,444] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.9747 3163902432.1855 1770.0000
[2019-03-27 11:11:22,445] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.9890 2779165382.7113 931.0000
[2019-03-27 11:11:22,495] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7479 2842270252.3070 1124.0000
[2019-03-27 11:11:22,541] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.2146 3007384787.8960 1759.0000
[2019-03-27 11:11:22,616] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.5938 2927101126.1700 1333.0000
[2019-03-27 11:11:23,630] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 100000, evaluation results [100000.0, 7886.974720661454, 3163902432.185544, 1770.0, 8256.593778212018, 2927101126.1699805, 1333.0, 8660.988992551516, 2779165382.7113156, 931.0, 7997.2145883220155, 3007384787.8959928, 1759.0, 8496.747859148343, 2842270252.3070493, 1124.0]
[2019-03-27 11:11:24,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.6333484e-08 2.9901844e-02 1.4231696e-09 9.7009796e-01 2.1344447e-11], sum to 1.0000
[2019-03-27 11:11:24,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0743
[2019-03-27 11:11:24,715] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.83333333333334, 60.5, 1.0, 2.0, 0.8986701324061196, 1.0, 2.0, 0.8986701324061196, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2513571.281289639, 2513571.281289639, 470772.6374310266], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3769800.0000, 
sim time next is 3770400.0000, 
raw observation next is [33.66666666666667, 61.00000000000001, 1.0, 2.0, 0.9770631992543581, 1.0, 2.0, 0.9770631992543581, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2733075.720474157, 2733075.720474157, 515282.4155214336], 
processed observation next is [1.0, 0.6521739130434783, 0.7946287519747238, 0.6100000000000001, 1.0, 1.0, 0.9723653003064556, 1.0, 1.0, 0.9723653003064556, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7591877001317103, 0.7591877001317103, 0.7690782321215427], 
reward next is 0.2309, 
noisyNet noise sample is [array([0.5372051], dtype=float32), 0.9865837]. 
=============================================
[2019-03-27 11:11:27,579] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2441290e-18 1.0000000e+00 1.3799755e-18 1.0603091e-13 7.6101663e-20], sum to 1.0000
[2019-03-27 11:11:27,592] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6015
[2019-03-27 11:11:27,602] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 85.66666666666666, 1.0, 2.0, 0.5282946385323052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738223.4157592438, 738223.4157592432, 188148.2279317458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3811200.0000, 
sim time next is 3811800.0000, 
raw observation next is [27.16666666666666, 87.33333333333334, 1.0, 2.0, 0.5312906442289708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742411.4091739753, 742411.4091739753, 188644.1013601022], 
processed observation next is [0.0, 0.08695652173913043, 0.4865718799368086, 0.8733333333333334, 1.0, 1.0, 0.4352899328059888, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20622539143721538, 0.20622539143721538, 0.2815583602389585], 
reward next is 0.7184, 
noisyNet noise sample is [array([-0.05083654], dtype=float32), 0.35552067]. 
=============================================
[2019-03-27 11:11:32,975] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103572: loss 0.1151
[2019-03-27 11:11:32,976] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103572: learning rate 0.0000
[2019-03-27 11:11:33,636] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103817: loss 0.0272
[2019-03-27 11:11:33,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103818: learning rate 0.0000
[2019-03-27 11:11:33,748] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103860: loss 0.0138
[2019-03-27 11:11:33,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103860: learning rate 0.0000
[2019-03-27 11:11:33,799] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103879: loss 0.0219
[2019-03-27 11:11:33,804] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103880: learning rate 0.0000
[2019-03-27 11:11:33,832] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103890: loss 0.0231
[2019-03-27 11:11:33,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103890: learning rate 0.0000
[2019-03-27 11:11:33,914] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 103922: loss 0.0209
[2019-03-27 11:11:33,917] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 103923: learning rate 0.0000
[2019-03-27 11:11:33,932] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 103927: loss 0.0185
[2019-03-27 11:11:33,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 103928: learning rate 0.0000
[2019-03-27 11:11:33,973] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103942: loss 0.0194
[2019-03-27 11:11:33,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103944: learning rate 0.0000
[2019-03-27 11:11:34,172] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104017: loss 0.0056
[2019-03-27 11:11:34,174] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104017: learning rate 0.0000
[2019-03-27 11:11:34,307] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104070: loss 0.0082
[2019-03-27 11:11:34,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104070: learning rate 0.0000
[2019-03-27 11:11:34,316] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104071: loss 0.0173
[2019-03-27 11:11:34,318] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104071: learning rate 0.0000
[2019-03-27 11:11:34,348] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104083: loss 0.0114
[2019-03-27 11:11:34,351] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104083: learning rate 0.0000
[2019-03-27 11:11:34,411] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104104: loss 0.0106
[2019-03-27 11:11:34,417] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104106: learning rate 0.0000
[2019-03-27 11:11:34,525] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 104150: loss 0.0057
[2019-03-27 11:11:34,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 104153: learning rate 0.0000
[2019-03-27 11:11:34,637] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.61356888e-20 1.00000000e+00 1.24077205e-20 2.25021036e-15
 2.55795911e-21], sum to 1.0000
[2019-03-27 11:11:34,651] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3697
[2019-03-27 11:11:34,657] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 90.66666666666667, 1.0, 2.0, 0.576724529448313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805923.5967779416, 805923.5967779416, 196489.2596508685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3903600.0000, 
sim time next is 3904200.0000, 
raw observation next is [27.5, 91.5, 1.0, 2.0, 0.5759854728022732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 804890.4365129758, 804890.4365129764, 196356.6652942462], 
processed observation next is [0.0, 0.17391304347826086, 0.5023696682464456, 0.915, 1.0, 1.0, 0.48913912385816044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22358067680915997, 0.2235806768091601, 0.2930696496929048], 
reward next is 0.7069, 
noisyNet noise sample is [array([-1.3241224], dtype=float32), 0.5587265]. 
=============================================
[2019-03-27 11:11:34,739] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104233: loss 0.0078
[2019-03-27 11:11:34,743] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104233: learning rate 0.0000
[2019-03-27 11:11:34,832] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104272: loss 0.0084
[2019-03-27 11:11:34,836] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104272: learning rate 0.0000
[2019-03-27 11:11:39,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0884534e-19 1.0000000e+00 9.0746754e-20 8.6746311e-14 8.6129283e-20], sum to 1.0000
[2019-03-27 11:11:39,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9098
[2019-03-27 11:11:39,814] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 84.0, 1.0, 2.0, 0.6003891332512326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839005.9337217159, 839005.9337217159, 200818.0299813241], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3976800.0000, 
sim time next is 3977400.0000, 
raw observation next is [29.16666666666667, 84.0, 1.0, 2.0, 0.5962188130494563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833175.8885971106, 833175.8885971106, 200043.0258720054], 
processed observation next is [1.0, 0.0, 0.581358609794629, 0.84, 1.0, 1.0, 0.5135166422282607, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23143774683253074, 0.23143774683253074, 0.2985716804059782], 
reward next is 0.7014, 
noisyNet noise sample is [array([1.9944252], dtype=float32), -0.05670108]. 
=============================================
[2019-03-27 11:11:43,594] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3941720e-08 3.2112539e-01 1.4610008e-09 6.7887461e-01 3.7732131e-12], sum to 1.0000
[2019-03-27 11:11:43,604] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5022
[2019-03-27 11:11:43,609] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.290085994528683, 6.9112, 170.5573041426782, 3181057.330623986, 2909645.880646221, 551635.1085216588], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4030200.0000, 
sim time next is 4030800.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.433346904621011, 6.9112, 170.5573041426782, 3283800.490691161, 2909765.423333025, 550822.3930979335], 
processed observation next is [1.0, 0.6521739130434783, 0.8104265402843602, 0.6, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.05221469046210112, 0.0, 0.8375144448122397, 0.9121668029697669, 0.8082681731480625, 0.8221229747730351], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5532826], dtype=float32), 0.15108529]. 
=============================================
[2019-03-27 11:11:47,502] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.07230632e-17 1.00000000e+00 2.65364549e-20 2.57151993e-13
 1.04588765e-20], sum to 1.0000
[2019-03-27 11:11:47,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5436
[2019-03-27 11:11:47,519] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6571349986278583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 918338.9833539705, 918338.9833539705, 211865.1391908308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4082400.0000, 
sim time next is 4083000.0000, 
raw observation next is [27.0, 89.83333333333334, 1.0, 2.0, 0.7336170904601088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1025273.464897259, 1025273.464897259, 228269.2253772327], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.8983333333333334, 1.0, 1.0, 0.6790567354941069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28479818469368307, 0.28479818469368307, 0.34070033638392944], 
reward next is 0.6593, 
noisyNet noise sample is [array([-0.22662303], dtype=float32), 1.4572937]. 
=============================================
[2019-03-27 11:11:47,531] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[62.923016]
 [62.888775]
 [62.9707  ]
 [62.901157]
 [62.876026]], R is [[62.74947357]
 [62.80576324]
 [62.84727478]
 [62.89334869]
 [62.95131683]].
[2019-03-27 11:11:51,096] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6212233e-15 1.0000000e+00 2.3368056e-16 2.7104259e-09 4.4824003e-18], sum to 1.0000
[2019-03-27 11:11:51,099] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4246
[2019-03-27 11:11:51,106] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5810393202224237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 811955.4554037425, 811955.4554037431, 197266.7582163115], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4137000.0000, 
sim time next is 4137600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5796733832508436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810045.9411018521, 810045.9411018527, 197020.3200530737], 
processed observation next is [1.0, 0.9130434782608695, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4935823894588477, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22501276141718113, 0.2250127614171813, 0.2940601791836921], 
reward next is 0.7059, 
noisyNet noise sample is [array([1.7073168], dtype=float32), -1.6314477]. 
=============================================
[2019-03-27 11:11:54,296] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111684: loss -237.3813
[2019-03-27 11:11:54,298] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111684: learning rate 0.0000
[2019-03-27 11:11:54,775] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111865: loss -170.2850
[2019-03-27 11:11:54,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111866: learning rate 0.0000
[2019-03-27 11:11:54,786] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111872: loss -158.7458
[2019-03-27 11:11:54,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111872: learning rate 0.0000
[2019-03-27 11:11:54,825] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 111889: loss -180.2592
[2019-03-27 11:11:54,827] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 111889: learning rate 0.0000
[2019-03-27 11:11:54,910] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111924: loss -226.5126
[2019-03-27 11:11:54,914] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111925: learning rate 0.0000
[2019-03-27 11:11:54,925] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111929: loss -219.9558
[2019-03-27 11:11:54,928] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111930: learning rate 0.0000
[2019-03-27 11:11:55,014] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111959: loss -144.1709
[2019-03-27 11:11:55,017] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111959: learning rate 0.0000
[2019-03-27 11:11:55,065] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 111979: loss -172.7055
[2019-03-27 11:11:55,069] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 111980: learning rate 0.0000
[2019-03-27 11:11:55,128] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112004: loss -144.2962
[2019-03-27 11:11:55,136] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112005: learning rate 0.0000
[2019-03-27 11:11:55,201] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112029: loss -231.8826
[2019-03-27 11:11:55,204] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112030: learning rate 0.0000
[2019-03-27 11:11:55,347] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 112086: loss -158.6559
[2019-03-27 11:11:55,349] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 112086: learning rate 0.0000
[2019-03-27 11:11:55,379] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112096: loss -152.9578
[2019-03-27 11:11:55,385] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112097: learning rate 0.0000
[2019-03-27 11:11:55,413] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 112109: loss -194.0717
[2019-03-27 11:11:55,415] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 112109: learning rate 0.0000
[2019-03-27 11:11:55,448] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 112120: loss -148.2890
[2019-03-27 11:11:55,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 112120: learning rate 0.0000
[2019-03-27 11:11:55,589] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112174: loss -237.7107
[2019-03-27 11:11:55,590] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112174: learning rate 0.0000
[2019-03-27 11:11:55,607] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112182: loss -156.1855
[2019-03-27 11:11:55,612] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112183: learning rate 0.0000
[2019-03-27 11:11:56,368] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7798129e-17 1.0000000e+00 1.6043277e-17 4.2365070e-11 1.2968213e-19], sum to 1.0000
[2019-03-27 11:11:56,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7335
[2019-03-27 11:11:56,386] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.66666666666667, 57.33333333333334, 1.0, 2.0, 0.5838024596998019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815818.2022191727, 815818.2022191727, 197768.6241460485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4213200.0000, 
sim time next is 4213800.0000, 
raw observation next is [34.5, 58.0, 1.0, 2.0, 0.5827109087956605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814292.2606402056, 814292.260640205, 197570.7189601219], 
processed observation next is [1.0, 0.782608695652174, 0.8341232227488152, 0.58, 1.0, 1.0, 0.4972420587899523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22619229462227933, 0.22619229462227916, 0.2948816700897342], 
reward next is 0.7051, 
noisyNet noise sample is [array([1.11719], dtype=float32), -1.6628786]. 
=============================================
[2019-03-27 11:12:00,194] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.7208452e-09 7.3298752e-01 6.4148947e-10 2.6701248e-01 1.6008967e-12], sum to 1.0000
[2019-03-27 11:12:00,204] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3153
[2019-03-27 11:12:00,214] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2546866.037929775 W.
[2019-03-27 11:12:00,220] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.33333333333334, 78.0, 1.0, 2.0, 0.9105617943509718, 1.0, 2.0, 0.9105617943509718, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2546866.037929775, 2546866.037929775, 477304.7231830505], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4264800.0000, 
sim time next is 4265400.0000, 
raw observation next is [32.66666666666666, 76.5, 1.0, 2.0, 0.8880689376422274, 1.0, 2.0, 0.8880689376422274, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2483890.374894882, 2483890.374894882, 465043.4033780129], 
processed observation next is [1.0, 0.34782608695652173, 0.7472353870458132, 0.765, 1.0, 1.0, 0.8651432983641294, 1.0, 1.0, 0.8651432983641294, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6899695485819117, 0.6899695485819117, 0.6940946319074819], 
reward next is 0.3059, 
noisyNet noise sample is [array([1.5784409], dtype=float32), 0.9665265]. 
=============================================
[2019-03-27 11:12:05,595] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.7841832e-08 7.3583186e-01 2.9852683e-09 2.6416811e-01 5.7584627e-12], sum to 1.0000
[2019-03-27 11:12:05,605] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6854
[2019-03-27 11:12:05,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3369606.778936508 W.
[2019-03-27 11:12:05,628] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.33333333333334, 78.0, 1.0, 2.0, 0.9645717241861714, 1.0, 2.0, 0.8028759016073483, 1.0, 1.0, 1.03, 7.005118602669991, 6.9112, 170.5573041426782, 3369606.778936508, 3302329.063479496, 617932.9029756882], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4351200.0000, 
sim time next is 4351800.0000, 
raw observation next is [32.66666666666666, 76.5, 1.0, 2.0, 0.8991787918199398, 1.0, 2.0, 0.7701794354242323, 1.0, 2.0, 1.03, 7.005113442477024, 6.9112, 170.5573041426782, 3232204.6938912, 3164930.674890313, 591626.3985317716], 
processed observation next is [1.0, 0.34782608695652173, 0.7472353870458132, 0.765, 1.0, 1.0, 0.8785286648433009, 1.0, 1.0, 0.7231077535231714, 1.0, 1.0, 1.0365853658536586, 0.00939134424770236, 0.0, 0.8375144448122397, 0.897834637192, 0.8791474096917535, 0.8830244754205546], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46389976], dtype=float32), 1.7406816]. 
=============================================
[2019-03-27 11:12:08,734] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.7115204e-20 1.0000000e+00 8.3796143e-21 1.2693742e-14 4.8066456e-23], sum to 1.0000
[2019-03-27 11:12:08,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6713
[2019-03-27 11:12:08,747] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333334, 63.0, 1.0, 2.0, 0.5460196378479318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763000.7050435139, 763000.7050435132, 191120.8788285246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4389000.0000, 
sim time next is 4389600.0000, 
raw observation next is [31.66666666666667, 63.00000000000001, 1.0, 2.0, 0.5421994546114896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757660.5270548833, 757660.5270548833, 190472.2508625647], 
processed observation next is [1.0, 0.8260869565217391, 0.6998420221169038, 0.6300000000000001, 1.0, 1.0, 0.4484330778451681, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21046125751524536, 0.21046125751524536, 0.28428694158591744], 
reward next is 0.7157, 
noisyNet noise sample is [array([-1.0918468], dtype=float32), 0.9807373]. 
=============================================
[2019-03-27 11:12:15,400] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119699: loss 0.5184
[2019-03-27 11:12:15,404] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119700: learning rate 0.0000
[2019-03-27 11:12:15,751] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 119831: loss 0.3276
[2019-03-27 11:12:15,752] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 119831: learning rate 0.0000
[2019-03-27 11:12:15,806] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119852: loss 0.3567
[2019-03-27 11:12:15,808] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119852: learning rate 0.0000
[2019-03-27 11:12:15,884] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119881: loss 0.3051
[2019-03-27 11:12:15,887] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119881: learning rate 0.0000
[2019-03-27 11:12:15,919] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119893: loss 0.3065
[2019-03-27 11:12:15,921] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119893: learning rate 0.0000
[2019-03-27 11:12:16,004] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 119924: loss 0.2920
[2019-03-27 11:12:16,006] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 119925: learning rate 0.0000
[2019-03-27 11:12:16,130] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 119971: loss 0.2802
[2019-03-27 11:12:16,135] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 119972: learning rate 0.0000
[2019-03-27 11:12:16,155] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119981: loss 0.2395
[2019-03-27 11:12:16,157] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119981: learning rate 0.0000
[2019-03-27 11:12:16,236] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120010: loss 0.1969
[2019-03-27 11:12:16,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120011: learning rate 0.0000
[2019-03-27 11:12:16,309] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120036: loss 0.1549
[2019-03-27 11:12:16,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120037: learning rate 0.0000
[2019-03-27 11:12:16,383] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120066: loss 0.1130
[2019-03-27 11:12:16,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120066: learning rate 0.0000
[2019-03-27 11:12:16,411] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120077: loss 0.1112
[2019-03-27 11:12:16,413] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120077: learning rate 0.0000
[2019-03-27 11:12:16,435] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 120084: loss 0.0886
[2019-03-27 11:12:16,440] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 120084: learning rate 0.0000
[2019-03-27 11:12:16,480] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 120100: loss 0.0674
[2019-03-27 11:12:16,484] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 120100: learning rate 0.0000
[2019-03-27 11:12:16,660] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120171: loss 0.0362
[2019-03-27 11:12:16,668] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120171: learning rate 0.0000
[2019-03-27 11:12:16,757] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 120206: loss 0.0312
[2019-03-27 11:12:16,761] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 120207: learning rate 0.0000
[2019-03-27 11:12:19,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4749690e-21 1.0000000e+00 1.1738199e-22 1.3126442e-18 4.4334405e-26], sum to 1.0000
[2019-03-27 11:12:19,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6449
[2019-03-27 11:12:19,151] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 65.5, 1.0, 2.0, 0.5511196067484896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 770129.9184442759, 770129.9184442766, 191992.5726424357], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4536600.0000, 
sim time next is 4537200.0000, 
raw observation next is [31.33333333333334, 65.0, 1.0, 2.0, 0.546577589218973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763780.6593100135, 763780.6593100135, 191215.6663663194], 
processed observation next is [0.0, 0.5217391304347826, 0.6840442338072673, 0.65, 1.0, 1.0, 0.4537079388180398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21216129425278155, 0.21216129425278155, 0.28539651696465584], 
reward next is 0.7146, 
noisyNet noise sample is [array([0.3149955], dtype=float32), 0.513372]. 
=============================================
[2019-03-27 11:12:23,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6266045e-09 9.1837841e-01 1.5935164e-10 8.1621647e-02 7.4366994e-13], sum to 1.0000
[2019-03-27 11:12:23,896] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5479
[2019-03-27 11:12:23,907] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2621916.693543985 W.
[2019-03-27 11:12:23,912] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 72.33333333333333, 1.0, 2.0, 0.6249106431806821, 1.0, 2.0, 0.6249106431806821, 1.0, 2.0, 1.03, 6.973326719450192, 6.9112, 170.5573041426782, 2621916.693543985, 2577412.797248442, 497188.7478162437], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4617600.0000, 
sim time next is 4618200.0000, 
raw observation next is [32.0, 71.66666666666667, 1.0, 2.0, 0.6290207621878007, 1.0, 2.0, 0.6290207621878007, 1.0, 2.0, 1.03, 6.98135163918916, 6.9112, 170.5573041426782, 2639179.596136135, 2588927.123398704, 498741.9569111126], 
processed observation next is [1.0, 0.43478260869565216, 0.7156398104265403, 0.7166666666666667, 1.0, 1.0, 0.5530370628768683, 1.0, 1.0, 0.5530370628768683, 1.0, 1.0, 1.0365853658536586, 0.007015163918916034, 0.0, 0.8375144448122397, 0.7331054433711486, 0.7191464231663066, 0.7443909804643472], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.44314563], dtype=float32), 0.40163502]. 
=============================================
[2019-03-27 11:12:25,564] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4434467e-09 5.1386468e-02 1.5323842e-10 9.4861346e-01 1.7479533e-13], sum to 1.0000
[2019-03-27 11:12:25,573] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8284
[2019-03-27 11:12:25,582] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 1.027427062719654, 1.0, 2.0, 1.027427062719654, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2874117.363600791, 2874117.363600791, 545789.3618980064], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4631400.0000, 
sim time next is 4632000.0000, 
raw observation next is [35.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.345166792002485, 6.9112, 170.5573041426782, 3220559.906875443, 2909691.841123604, 551332.6821665565], 
processed observation next is [1.0, 0.6086956521739131, 0.8578199052132701, 0.6, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.04339667920024848, 0.0, 0.8375144448122397, 0.8945999741320676, 0.8082477336454457, 0.8228846002485918], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.657297], dtype=float32), 0.32040396]. 
=============================================
[2019-03-27 11:12:25,599] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[26.395998]
 [26.01428 ]
 [25.703672]
 [25.36553 ]
 [27.019203]], R is [[26.25810051]
 [26.1809082 ]
 [26.1108551 ]
 [25.8497467 ]
 [25.59124947]].
[2019-03-27 11:12:29,398] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 11:12:29,399] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:12:29,399] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:12:29,401] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:12:29,402] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:12:29,400] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:12:29,405] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:12:29,406] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:12:29,408] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:12:29,403] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:12:29,410] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:12:29,428] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run6
[2019-03-27 11:12:29,428] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run6
[2019-03-27 11:12:29,466] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run6
[2019-03-27 11:12:29,466] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run6
[2019-03-27 11:12:29,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run6
[2019-03-27 11:13:06,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0497805], dtype=float32), 0.024803145]
[2019-03-27 11:13:06,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.26666666666667, 75.0, 1.0, 2.0, 0.5157724782401008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723250.6288070335, 723250.6288070335, 186427.7242031292]
[2019-03-27 11:13:06,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:13:06,426] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4418872e-18 1.0000000e+00 5.4956762e-20 2.6568442e-14 1.3943984e-21], sampled 0.1392284051865058
[2019-03-27 11:13:27,036] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0497805], dtype=float32), 0.024803145]
[2019-03-27 11:13:27,037] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.36666666666667, 51.66666666666667, 1.0, 2.0, 0.9866423763333035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510428, 1379120.989279607, 1379120.989279607, 294885.9327337413]
[2019-03-27 11:13:27,038] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:13:27,041] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.86261556e-16 1.00000000e+00 1.61177466e-17 1.03595015e-10
 1.66599668e-19], sampled 0.5620020180686387
[2019-03-27 11:14:17,596] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0497805], dtype=float32), 0.024803145]
[2019-03-27 11:14:17,597] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.88333333333333, 95.0, 1.0, 2.0, 0.6139510287585741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857965.4815870208, 857965.4815870208, 203365.0439199614]
[2019-03-27 11:14:17,599] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:14:17,604] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.7315238e-18 1.0000000e+00 7.2616719e-20 3.1840443e-14 2.1926508e-21], sampled 0.503947747625492
[2019-03-27 11:14:33,061] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0497805], dtype=float32), 0.024803145]
[2019-03-27 11:14:33,062] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.33333333333334, 87.66666666666667, 1.0, 2.0, 0.7881605814381655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1109177.750850785, 1109177.750850785, 242093.9954172774]
[2019-03-27 11:14:33,063] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:14:33,068] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.8578405e-18 1.0000000e+00 7.5453580e-20 2.3246540e-14 2.3188357e-21], sampled 0.20632743350444316
[2019-03-27 11:14:36,824] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 11:14:36,831] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7365 2842517667.5603 1131.0000
[2019-03-27 11:14:37,263] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 11:14:37,408] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4179 3163987398.3597 1778.0000
[2019-03-27 11:14:37,543] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 11:14:38,559] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 125000, evaluation results [125000.0, 7883.417948485868, 3163987398.359688, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.736480505688, 2842517667.5603323, 1131.0]
[2019-03-27 11:14:39,454] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0308997e-10 9.9989271e-01 1.4875352e-12 1.0730310e-04 4.6817950e-15], sum to 1.0000
[2019-03-27 11:14:39,463] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3307
[2019-03-27 11:14:39,475] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2315975.758827272 W.
[2019-03-27 11:14:39,480] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.8280897619777856, 1.0, 2.0, 0.8280897619777856, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2315975.758827272, 2315975.758827273, 433779.2539366588], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4698600.0000, 
sim time next is 4699200.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.7891485389275185, 1.0, 2.0, 0.7891485389275185, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2206969.157736342, 2206969.157736342, 414625.1451666319], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.75, 1.0, 1.0, 0.7459620950933957, 1.0, 1.0, 0.7459620950933957, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.613046988260095, 0.613046988260095, 0.6188435002487044], 
reward next is 0.3812, 
noisyNet noise sample is [array([-0.01801053], dtype=float32), -1.1724253]. 
=============================================
[2019-03-27 11:14:43,195] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.7028505e-18 1.0000000e+00 3.0472546e-18 4.6228754e-12 2.9979491e-20], sum to 1.0000
[2019-03-27 11:14:43,205] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6192
[2019-03-27 11:14:43,329] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4857725200252103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678785.2714381652, 678785.2714381646, 181396.6560870234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4756800.0000, 
sim time next is 4757400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.485108267694878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677856.794554996, 677856.794554996, 181295.4846495466], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 1.0, 1.0, 0.37964851529503374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18829355404305442, 0.18829355404305442, 0.2705902755963382], 
reward next is 0.7294, 
noisyNet noise sample is [array([0.33965513], dtype=float32), -0.18996346]. 
=============================================
[2019-03-27 11:14:45,989] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127802: loss -114.8155
[2019-03-27 11:14:45,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127802: learning rate 0.0000
[2019-03-27 11:14:46,078] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127836: loss -76.1431
[2019-03-27 11:14:46,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127838: learning rate 0.0000
[2019-03-27 11:14:46,193] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127879: loss -200.9626
[2019-03-27 11:14:46,197] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127879: learning rate 0.0000
[2019-03-27 11:14:46,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0507067e-11 9.8802626e-01 5.4584609e-12 1.1973670e-02 3.7340533e-15], sum to 1.0000
[2019-03-27 11:14:46,239] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8816
[2019-03-27 11:14:46,246] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 64.5, 1.0, 2.0, 0.5745543103095965, 1.0, 1.0, 0.5745543103095965, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1606375.150602923, 1606375.150602923, 325045.9282208823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4797000.0000, 
sim time next is 4797600.0000, 
raw observation next is [31.66666666666666, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.568299972285091, 6.9112, 168.9093133394991, 1920233.512514393, 1454074.231035263, 311351.783667604], 
processed observation next is [1.0, 0.5217391304347826, 0.6998420221169034, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.06570999722850912, 0.0, 0.8294220555151762, 0.5333981979206648, 0.4039095086209064, 0.4647041547277671], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.40854058], dtype=float32), -3.100892]. 
=============================================
[2019-03-27 11:14:46,261] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127901: loss -87.8709
[2019-03-27 11:14:46,265] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127901: learning rate 0.0000
[2019-03-27 11:14:46,331] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 127931: loss -48.8185
[2019-03-27 11:14:46,333] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 127931: learning rate 0.0000
[2019-03-27 11:14:46,374] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127949: loss -73.5687
[2019-03-27 11:14:46,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127949: learning rate 0.0000
[2019-03-27 11:14:46,408] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127957: loss -232.0975
[2019-03-27 11:14:46,410] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127959: learning rate 0.0000
[2019-03-27 11:14:46,430] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 127962: loss -118.1607
[2019-03-27 11:14:46,431] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 127962: learning rate 0.0000
[2019-03-27 11:14:46,572] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128020: loss -129.1440
[2019-03-27 11:14:46,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128020: learning rate 0.0000
[2019-03-27 11:14:46,594] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 128028: loss -201.3465
[2019-03-27 11:14:46,597] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 128028: learning rate 0.0000
[2019-03-27 11:14:46,622] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128035: loss -270.3700
[2019-03-27 11:14:46,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128035: learning rate 0.0000
[2019-03-27 11:14:46,637] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128041: loss -141.7869
[2019-03-27 11:14:46,638] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128041: learning rate 0.0000
[2019-03-27 11:14:46,773] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 128094: loss -94.3612
[2019-03-27 11:14:46,775] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 128094: learning rate 0.0000
[2019-03-27 11:14:46,801] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 128106: loss 4.7690
[2019-03-27 11:14:46,803] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 128106: learning rate 0.0000
[2019-03-27 11:14:46,814] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 128110: loss -83.2329
[2019-03-27 11:14:46,817] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 128110: learning rate 0.0000
[2019-03-27 11:14:47,087] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128205: loss -178.2947
[2019-03-27 11:14:47,089] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128205: learning rate 0.0000
[2019-03-27 11:14:48,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8896455e-16 1.0000000e+00 3.2746188e-18 7.7707157e-10 4.3981617e-21], sum to 1.0000
[2019-03-27 11:14:48,364] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7988
[2019-03-27 11:14:48,374] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 75.0, 1.0, 2.0, 0.4930706138307717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688986.4353290875, 688986.435329087, 182517.2528753647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4832400.0000, 
sim time next is 4833000.0000, 
raw observation next is [27.75, 75.5, 1.0, 2.0, 0.493124438284351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689061.6706909187, 689061.670690918, 182525.5695888306], 
processed observation next is [1.0, 0.9565217391304348, 0.514218009478673, 0.755, 1.0, 1.0, 0.3893065521498205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1914060196363663, 0.1914060196363661, 0.27242622326691135], 
reward next is 0.7276, 
noisyNet noise sample is [array([1.8013842], dtype=float32), 1.7970555]. 
=============================================
[2019-03-27 11:14:48,385] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[59.448303]
 [59.244907]
 [59.23348 ]
 [59.399307]
 [59.221622]], R is [[59.60486984]
 [59.73640823]
 [59.8669014 ]
 [59.99624634]
 [60.12425232]].
[2019-03-27 11:14:55,282] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7483298e-18 1.0000000e+00 1.5208456e-21 1.4774950e-12 8.5499277e-24], sum to 1.0000
[2019-03-27 11:14:55,294] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3117
[2019-03-27 11:14:55,300] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5100482194782313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712717.851843456, 712717.8518434553, 185185.4043191342], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4921200.0000, 
sim time next is 4921800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5102488134825769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712998.2467218299, 712998.2467218299, 185217.4242867615], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4099383294970806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19805506853384164, 0.19805506853384164, 0.2764439168459127], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.93961066], dtype=float32), -1.1235255]. 
=============================================
[2019-03-27 11:15:00,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8876660e-16 1.0000000e+00 2.1619813e-18 7.5588975e-09 9.8338293e-21], sum to 1.0000
[2019-03-27 11:15:00,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0350
[2019-03-27 11:15:00,632] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.5175459498903098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723198.4008884656, 723198.4008884649, 186391.9799378769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4990200.0000, 
sim time next is 4990800.0000, 
raw observation next is [29.66666666666667, 71.33333333333334, 1.0, 2.0, 0.5206798587619019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727579.1019253226, 727579.1019253231, 186900.4308881945], 
processed observation next is [1.0, 0.782608695652174, 0.6050552922590839, 0.7133333333333334, 1.0, 1.0, 0.4225058539300023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20210530609036736, 0.20210530609036753, 0.27895586699730524], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.5430771], dtype=float32), 0.33233517]. 
=============================================
[2019-03-27 11:15:03,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0424869e-18 1.0000000e+00 1.1729342e-21 1.0534899e-12 3.1945960e-23], sum to 1.0000
[2019-03-27 11:15:03,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4302
[2019-03-27 11:15:03,102] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 84.0, 1.0, 2.0, 0.5230875223035841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730944.6409575349, 730944.6409575342, 187292.8553258931], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5038200.0000, 
sim time next is 5038800.0000, 
raw observation next is [27.66666666666666, 84.0, 1.0, 2.0, 0.5270428143030901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736473.5469318789, 736473.5469318789, 187942.1343033622], 
processed observation next is [0.0, 0.30434782608695654, 0.5102685624012636, 0.84, 1.0, 1.0, 0.4301720654254097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20457598525885526, 0.20457598525885526, 0.2805106482139734], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.7611292], dtype=float32), -0.60225224]. 
=============================================
[2019-03-27 11:15:06,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7330770e-22 1.0000000e+00 1.4904045e-23 1.0637800e-15 6.6270337e-26], sum to 1.0000
[2019-03-27 11:15:06,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6292
[2019-03-27 11:15:06,958] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5158080809492204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720769.1471415638, 720769.1471415644, 186109.5676506199], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5092800.0000, 
sim time next is 5093400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.515613695191797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720497.4283035933, 720497.4283035927, 186078.2184933889], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4164020423997554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20013817452877591, 0.20013817452877575, 0.2777286843184909], 
reward next is 0.7223, 
noisyNet noise sample is [array([-0.12088656], dtype=float32), 0.4910705]. 
=============================================
[2019-03-27 11:15:07,029] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 135793: loss 4.6609
[2019-03-27 11:15:07,031] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 135794: learning rate 0.0000
[2019-03-27 11:15:07,131] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 135831: loss 4.2280
[2019-03-27 11:15:07,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 135831: learning rate 0.0000
[2019-03-27 11:15:07,241] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 135874: loss 4.0149
[2019-03-27 11:15:07,244] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 135877: learning rate 0.0000
[2019-03-27 11:15:07,276] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 135890: loss 3.6077
[2019-03-27 11:15:07,279] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 135890: learning rate 0.0000
[2019-03-27 11:15:07,314] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 135901: loss 3.5648
[2019-03-27 11:15:07,318] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 135901: learning rate 0.0000
[2019-03-27 11:15:07,347] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 135913: loss 3.5685
[2019-03-27 11:15:07,350] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 135914: learning rate 0.0000
[2019-03-27 11:15:07,404] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 135935: loss 3.6076
[2019-03-27 11:15:07,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 135935: learning rate 0.0000
[2019-03-27 11:15:07,437] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 135947: loss 3.7096
[2019-03-27 11:15:07,440] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 135948: learning rate 0.0000
[2019-03-27 11:15:07,572] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 135998: loss 3.3215
[2019-03-27 11:15:07,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 135998: learning rate 0.0000
[2019-03-27 11:15:07,654] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 136031: loss 3.5085
[2019-03-27 11:15:07,657] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 136032: learning rate 0.0000
[2019-03-27 11:15:07,677] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 136039: loss 3.2263
[2019-03-27 11:15:07,681] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 136040: learning rate 0.0000
[2019-03-27 11:15:07,706] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136051: loss 3.4720
[2019-03-27 11:15:07,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136052: learning rate 0.0000
[2019-03-27 11:15:07,767] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 136069: loss 3.2069
[2019-03-27 11:15:07,770] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 136070: learning rate 0.0000
[2019-03-27 11:15:07,819] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136088: loss 3.0271
[2019-03-27 11:15:07,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136090: learning rate 0.0000
[2019-03-27 11:15:07,919] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 136130: loss 3.0712
[2019-03-27 11:15:07,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 136130: learning rate 0.0000
[2019-03-27 11:15:07,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5075224e-18 1.0000000e+00 9.4638138e-21 6.4474500e-13 9.7109503e-23], sum to 1.0000
[2019-03-27 11:15:08,003] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2327
[2019-03-27 11:15:08,007] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4835689770511917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675829.8095865631, 675829.8095865624, 181077.0919183193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5112000.0000, 
sim time next is 5112600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4830505893968829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675104.7403194243, 675104.740319425, 180998.4027749045], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3771693848155216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18752909453317343, 0.18752909453317362, 0.2701468698132903], 
reward next is 0.7299, 
noisyNet noise sample is [array([0.90456617], dtype=float32), -2.2583442]. 
=============================================
[2019-03-27 11:15:08,446] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136323: loss 3.3119
[2019-03-27 11:15:08,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136323: learning rate 0.0000
[2019-03-27 11:15:08,510] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.1535998e-20 1.0000000e+00 1.5443897e-20 6.0120691e-13 7.7735883e-22], sum to 1.0000
[2019-03-27 11:15:08,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3220
[2019-03-27 11:15:08,534] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4811471539177737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672445.3420356801, 672445.3420356801, 180710.5309765235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5116200.0000, 
sim time next is 5116800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4802510403429787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671192.9425618037, 671192.9425618037, 180575.3256395515], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3737964341481671, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18644248404494548, 0.18644248404494548, 0.26951541140231566], 
reward next is 0.7305, 
noisyNet noise sample is [array([-0.29061702], dtype=float32), -0.8313577]. 
=============================================
[2019-03-27 11:15:09,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.1912211e-21 1.0000000e+00 1.1164755e-21 1.4107513e-14 2.8276435e-23], sum to 1.0000
[2019-03-27 11:15:09,412] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5263
[2019-03-27 11:15:09,425] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5180614497356392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723918.9855987724, 723918.9855987731, 186474.3353272583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5124600.0000, 
sim time next is 5125200.0000, 
raw observation next is [28.33333333333334, 77.33333333333333, 1.0, 2.0, 0.5201964524523757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726903.3764013159, 726903.3764013159, 186820.9851021372], 
processed observation next is [0.0, 0.30434782608695654, 0.5418641390205374, 0.7733333333333333, 1.0, 1.0, 0.4219234366896092, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2019176045559211, 0.2019176045559211, 0.2788372911972197], 
reward next is 0.7212, 
noisyNet noise sample is [array([-1.2693901], dtype=float32), 0.41144398]. 
=============================================
[2019-03-27 11:15:26,366] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9117793e-12 9.9999833e-01 3.1860602e-15 1.6884201e-06 3.1357047e-17], sum to 1.0000
[2019-03-27 11:15:26,376] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6761
[2019-03-27 11:15:26,380] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 92.66666666666667, 1.0, 2.0, 0.9176229966449725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129297891087, 1282587.964096309, 1282587.964096308, 274836.8419098538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5375400.0000, 
sim time next is 5376000.0000, 
raw observation next is [28.7, 91.33333333333334, 1.0, 2.0, 0.8701902718187339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565037586, 1216251.911007942, 1216251.911007942, 261874.8851156789], 
processed observation next is [1.0, 0.21739130434782608, 0.5592417061611374, 0.9133333333333334, 1.0, 1.0, 0.8436027371310046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451195382, 0.33784775305776166, 0.33784775305776166, 0.3908580374860879], 
reward next is 0.6091, 
noisyNet noise sample is [array([-0.9831381], dtype=float32), -0.056570195]. 
=============================================
[2019-03-27 11:15:26,393] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[47.24493 ]
 [46.18046 ]
 [45.189198]
 [45.419155]
 [43.478817]], R is [[48.68150711]
 [48.78448486]
 [48.2966423 ]
 [48.34369659]
 [47.86026001]].
[2019-03-27 11:15:28,122] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 143789: loss -102.4300
[2019-03-27 11:15:28,124] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 143789: learning rate 0.0000
[2019-03-27 11:15:28,290] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 143855: loss -50.4680
[2019-03-27 11:15:28,292] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 143855: learning rate 0.0000
[2019-03-27 11:15:28,313] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 143859: loss -80.2798
[2019-03-27 11:15:28,315] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 143860: learning rate 0.0000
[2019-03-27 11:15:28,327] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 143866: loss -80.7506
[2019-03-27 11:15:28,329] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 143866: learning rate 0.0000
[2019-03-27 11:15:28,377] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 143883: loss -119.1420
[2019-03-27 11:15:28,380] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 143884: learning rate 0.0000
[2019-03-27 11:15:28,527] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 143943: loss -111.4434
[2019-03-27 11:15:28,529] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 143943: learning rate 0.0000
[2019-03-27 11:15:28,642] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 143983: loss -75.7048
[2019-03-27 11:15:28,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 143983: learning rate 0.0000
[2019-03-27 11:15:28,669] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 143995: loss -60.7015
[2019-03-27 11:15:28,671] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 143995: learning rate 0.0000
[2019-03-27 11:15:28,710] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 144010: loss -25.0442
[2019-03-27 11:15:28,715] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 144010: learning rate 0.0000
[2019-03-27 11:15:28,724] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 144016: loss -51.4258
[2019-03-27 11:15:28,735] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 144020: learning rate 0.0000
[2019-03-27 11:15:28,739] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 144022: loss -74.3369
[2019-03-27 11:15:28,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 144022: learning rate 0.0000
[2019-03-27 11:15:28,800] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144041: loss -60.4535
[2019-03-27 11:15:28,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144041: learning rate 0.0000
[2019-03-27 11:15:28,822] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 144049: loss -77.7782
[2019-03-27 11:15:28,825] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 144049: learning rate 0.0000
[2019-03-27 11:15:29,054] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 144133: loss -44.3708
[2019-03-27 11:15:29,056] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 144133: learning rate 0.0000
[2019-03-27 11:15:29,071] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 144138: loss -70.5950
[2019-03-27 11:15:29,074] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 144138: learning rate 0.0000
[2019-03-27 11:15:29,335] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144238: loss -35.4321
[2019-03-27 11:15:29,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144238: learning rate 0.0000
[2019-03-27 11:15:30,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.0574970e-11 2.8861096e-02 4.4738714e-12 9.7113895e-01 7.8490276e-16], sum to 1.0000
[2019-03-27 11:15:30,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4357
[2019-03-27 11:15:30,437] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.9, 71.66666666666667, 1.0, 2.0, 0.4351985793139421, 1.0, 2.0, 0.4351985793139421, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1216534.520028472, 1216534.520028472, 281365.5860652915], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5418600.0000, 
sim time next is 5419200.0000, 
raw observation next is [30.9, 72.33333333333334, 1.0, 2.0, 0.2737871322385405, 1.0, 2.0, 0.2737871322385405, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 765171.2453673809, 765171.2453673816, 245022.4245621851], 
processed observation next is [1.0, 0.7391304347826086, 0.6635071090047393, 0.7233333333333334, 1.0, 1.0, 0.12504473763679577, 1.0, 1.0, 0.12504473763679577, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2125475681576058, 0.212547568157606, 0.3657051112868434], 
reward next is 0.6343, 
noisyNet noise sample is [array([-1.2040206], dtype=float32), -1.7896343]. 
=============================================
[2019-03-27 11:15:44,529] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 11:15:44,532] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:15:44,534] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:15:44,535] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:15:44,535] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:15:44,538] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:15:44,536] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:15:44,540] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:15:44,538] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:15:44,543] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:15:44,544] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:15:44,566] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run7
[2019-03-27 11:15:44,567] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run7
[2019-03-27 11:15:44,606] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run7
[2019-03-27 11:15:44,623] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run7
[2019-03-27 11:15:44,624] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run7
[2019-03-27 11:15:48,983] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04907862], dtype=float32), 0.025696296]
[2019-03-27 11:15:48,984] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.09199568833333, 92.80057427166668, 1.0, 2.0, 0.2561211633378004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 421545.8707244291, 421545.8707244284, 161510.4194845505]
[2019-03-27 11:15:48,984] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:15:48,987] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.0106870e-21 1.0000000e+00 6.5605321e-25 2.4801698e-19 8.5856749e-26], sampled 0.7642900873401384
[2019-03-27 11:16:03,223] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04907862], dtype=float32), 0.025696296]
[2019-03-27 11:16:03,225] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.56380227, 89.57400941, 1.0, 2.0, 0.3671231033797055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564191.0020354071, 564191.0020354071, 171813.5676719557]
[2019-03-27 11:16:03,225] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:16:03,230] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2016202e-21 1.0000000e+00 1.2439205e-25 1.9765567e-19 1.4541065e-26], sampled 0.10153960316015875
[2019-03-27 11:16:25,600] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04907862], dtype=float32), 0.025696296]
[2019-03-27 11:16:25,603] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.13333333333334, 93.33333333333334, 1.0, 2.0, 0.4541221734251799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650841.1562148086, 650841.1562148092, 178811.1754443829]
[2019-03-27 11:16:25,603] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:16:25,607] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5749170e-20 1.0000000e+00 4.2110373e-24 1.0699935e-17 4.7960909e-25], sampled 0.5231630109568809
[2019-03-27 11:16:29,385] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04907862], dtype=float32), 0.025696296]
[2019-03-27 11:16:29,386] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.7, 55.66666666666667, 1.0, 2.0, 0.5606866337176662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783503.7323781358, 783503.7323781358, 193650.4464415636]
[2019-03-27 11:16:29,387] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:16:29,389] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.9273462e-20 1.0000000e+00 9.6578584e-24 4.5228178e-17 8.0033129e-25], sampled 0.6899604852546393
[2019-03-27 11:16:45,197] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04907862], dtype=float32), 0.025696296]
[2019-03-27 11:16:45,197] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.0, 63.0, 1.0, 2.0, 1.018884952016716, 1.0, 2.0, 1.018884952016716, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2850194.460825162, 2850194.460825162, 540502.9474276124]
[2019-03-27 11:16:45,197] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:16:45,199] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.5183840e-13 9.9998701e-01 2.3486983e-15 1.3011024e-05 1.9254657e-17], sampled 0.1996379350058941
[2019-03-27 11:16:45,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2850194.460825162 W.
[2019-03-27 11:17:43,389] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04907862], dtype=float32), 0.025696296]
[2019-03-27 11:17:43,389] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.7, 86.5, 1.0, 2.0, 0.6958082316384645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 972409.1466554796, 972409.1466554796, 219946.794560288]
[2019-03-27 11:17:43,391] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:17:43,395] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.7181658e-19 1.0000000e+00 1.0060495e-22 1.7417754e-15 4.7806947e-24], sampled 0.7545193303109388
[2019-03-27 11:17:51,742] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6762 3164080276.7225 1778.0000
[2019-03-27 11:17:52,037] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.8298 2927469099.2865 1338.0000
[2019-03-27 11:17:52,305] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2758 3007633363.4131 1766.0000
[2019-03-27 11:17:52,401] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6656 2842565299.5504 1131.0000
[2019-03-27 11:17:52,442] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.4696 2779281246.4768 933.0000
[2019-03-27 11:17:53,458] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 150000, evaluation results [150000.0, 7882.676158974111, 3164080276.7224894, 1778.0, 8252.829842796136, 2927469099.286453, 1338.0, 8658.469568024992, 2779281246.476782, 933.0, 7998.275801278395, 3007633363.413055, 1766.0, 8496.665578540826, 2842565299.55038, 1131.0]
[2019-03-27 11:17:57,909] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 151700: loss 0.0614
[2019-03-27 11:17:57,917] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 151700: learning rate 0.0000
[2019-03-27 11:17:58,219] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 151815: loss 0.0077
[2019-03-27 11:17:58,225] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 151816: learning rate 0.0000
[2019-03-27 11:17:58,315] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 151849: loss 0.0099
[2019-03-27 11:17:58,318] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 151850: learning rate 0.0000
[2019-03-27 11:17:58,359] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 151868: loss 0.0047
[2019-03-27 11:17:58,364] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 151869: learning rate 0.0000
[2019-03-27 11:17:58,388] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 151879: loss 0.0065
[2019-03-27 11:17:58,390] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 151880: learning rate 0.0000
[2019-03-27 11:17:58,612] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 151961: loss 0.0021
[2019-03-27 11:17:58,615] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 151962: learning rate 0.0000
[2019-03-27 11:17:58,640] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 151971: loss 0.0010
[2019-03-27 11:17:58,642] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 151971: learning rate 0.0000
[2019-03-27 11:17:58,654] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 151977: loss 0.0017
[2019-03-27 11:17:58,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 151977: learning rate 0.0000
[2019-03-27 11:17:58,685] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 151987: loss 0.0038
[2019-03-27 11:17:58,687] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 151987: learning rate 0.0000
[2019-03-27 11:17:58,709] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 151996: loss 0.0023
[2019-03-27 11:17:58,711] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 151996: learning rate 0.0000
[2019-03-27 11:17:58,860] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 152052: loss 0.0004
[2019-03-27 11:17:58,863] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 152052: learning rate 0.0000
[2019-03-27 11:17:58,892] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152064: loss 0.0024
[2019-03-27 11:17:58,893] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152064: learning rate 0.0000
[2019-03-27 11:17:58,914] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 152072: loss 0.0063
[2019-03-27 11:17:58,915] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 152072: learning rate 0.0000
[2019-03-27 11:17:59,123] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 152155: loss 0.0051
[2019-03-27 11:17:59,126] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 152155: learning rate 0.0000
[2019-03-27 11:17:59,181] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 152173: loss 0.0026
[2019-03-27 11:17:59,183] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 152173: learning rate 0.0000
[2019-03-27 11:17:59,384] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152251: loss 0.0270
[2019-03-27 11:17:59,386] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152251: learning rate 0.0000
[2019-03-27 11:17:59,913] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1537665e-23 1.0000000e+00 5.8298458e-27 2.5567216e-20 1.9290467e-27], sum to 1.0000
[2019-03-27 11:17:59,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3080
[2019-03-27 11:17:59,928] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.46666666666667, 87.0, 1.0, 2.0, 0.5132456136693408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717187.2512455037, 717187.2512455037, 185696.9988128121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5707200.0000, 
sim time next is 5707800.0000, 
raw observation next is [26.45, 87.0, 1.0, 2.0, 0.5122068554158608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715735.2463210775, 715735.2463210775, 185530.3552951152], 
processed observation next is [0.0, 0.043478260869565216, 0.45260663507109006, 0.87, 1.0, 1.0, 0.41229741616368765, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19881534620029928, 0.19881534620029928, 0.27691097805241077], 
reward next is 0.7231, 
noisyNet noise sample is [array([-2.1504357], dtype=float32), -0.5490207]. 
=============================================
[2019-03-27 11:18:10,843] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8386521e-19 1.0000000e+00 1.7924923e-22 6.8580205e-15 4.5127541e-24], sum to 1.0000
[2019-03-27 11:18:10,853] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1694
[2019-03-27 11:18:10,860] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 86.33333333333334, 1.0, 2.0, 0.5677987464327612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793445.9136206879, 793445.9136206872, 194900.128771883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5862000.0000, 
sim time next is 5862600.0000, 
raw observation next is [28.15, 86.5, 1.0, 2.0, 0.5655401374016147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790288.5435167112, 790288.5435167112, 194501.6358680622], 
processed observation next is [1.0, 0.8695652173913043, 0.533175355450237, 0.865, 1.0, 1.0, 0.47655438241158393, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21952459542130867, 0.21952459542130867, 0.29030094905680925], 
reward next is 0.7097, 
noisyNet noise sample is [array([1.4437509], dtype=float32), 1.0138714]. 
=============================================
[2019-03-27 11:18:19,347] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 159801: loss 63.5293
[2019-03-27 11:18:19,348] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 159801: learning rate 0.0000
[2019-03-27 11:18:19,462] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 159841: loss 228.6266
[2019-03-27 11:18:19,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 159841: learning rate 0.0000
[2019-03-27 11:18:19,557] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 159878: loss -22.6035
[2019-03-27 11:18:19,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 159878: learning rate 0.0000
[2019-03-27 11:18:19,615] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 159900: loss 32.9208
[2019-03-27 11:18:19,620] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 159902: learning rate 0.0000
[2019-03-27 11:18:19,623] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 159902: loss -62.2901
[2019-03-27 11:18:19,626] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 159903: learning rate 0.0000
[2019-03-27 11:18:19,738] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 159946: loss 130.8495
[2019-03-27 11:18:19,740] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 159946: learning rate 0.0000
[2019-03-27 11:18:19,764] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.0716443e-10 8.7224531e-01 5.2503742e-13 1.2775472e-01 4.0590594e-15], sum to 1.0000
[2019-03-27 11:18:19,774] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1828
[2019-03-27 11:18:19,782] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159961: loss 67.4133
[2019-03-27 11:18:19,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2460545.236777702 W.
[2019-03-27 11:18:19,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159962: learning rate 0.0000
[2019-03-27 11:18:19,792] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.6, 75.0, 1.0, 2.0, 0.8797305262745092, 1.0, 2.0, 0.8797305262745092, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2460545.236777702, 2460545.236777702, 460557.7105731906], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5997600.0000, 
sim time next is 5998200.0000, 
raw observation next is [30.76666666666667, 74.33333333333333, 1.0, 2.0, 0.5391683330250998, 1.0, 2.0, 0.5391683330250998, 1.0, 1.0, 0.9363570887721963, 6.9112, 6.9112, 170.5573041426782, 2261844.962494045, 2261844.962494045, 443289.202857225], 
processed observation next is [1.0, 0.43478260869565216, 0.6571879936808849, 0.7433333333333333, 1.0, 1.0, 0.44478112412662624, 1.0, 1.0, 0.44478112412662624, 1.0, 0.5, 0.9223866936246294, 0.0, 0.0, 0.8375144448122397, 0.6282902673594569, 0.6282902673594569, 0.661625675906306], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.23511887], dtype=float32), 0.25713828]. 
=============================================
[2019-03-27 11:18:19,845] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 159986: loss -19.9155
[2019-03-27 11:18:19,847] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 159987: learning rate 0.0000
[2019-03-27 11:18:19,895] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 160005: loss 152.2621
[2019-03-27 11:18:19,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 160006: learning rate 0.0000
[2019-03-27 11:18:19,938] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 160030: loss -80.5340
[2019-03-27 11:18:19,940] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 160030: learning rate 0.0000
[2019-03-27 11:18:19,944] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 160030: loss 94.7592
[2019-03-27 11:18:19,948] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 160031: learning rate 0.0000
[2019-03-27 11:18:19,980] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160039: loss 42.5024
[2019-03-27 11:18:19,982] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160039: learning rate 0.0000
[2019-03-27 11:18:20,051] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 160065: loss 208.9361
[2019-03-27 11:18:20,053] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 160065: learning rate 0.0000
[2019-03-27 11:18:20,068] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 160069: loss -14.6526
[2019-03-27 11:18:20,069] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 160069: learning rate 0.0000
[2019-03-27 11:18:20,260] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 160144: loss 83.8483
[2019-03-27 11:18:20,263] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 160145: learning rate 0.0000
[2019-03-27 11:18:20,344] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 160175: loss 83.7478
[2019-03-27 11:18:20,350] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 160177: learning rate 0.0000
[2019-03-27 11:18:40,416] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 167786: loss 0.0075
[2019-03-27 11:18:40,418] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 167786: learning rate 0.0000
[2019-03-27 11:18:40,539] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 167833: loss 0.0017
[2019-03-27 11:18:40,541] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 167833: learning rate 0.0000
[2019-03-27 11:18:40,595] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167854: loss 0.0014
[2019-03-27 11:18:40,598] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167854: learning rate 0.0000
[2019-03-27 11:18:40,642] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 167870: loss 0.0037
[2019-03-27 11:18:40,644] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 167870: learning rate 0.0000
[2019-03-27 11:18:40,779] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 167923: loss 0.0162
[2019-03-27 11:18:40,781] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 167923: learning rate 0.0000
[2019-03-27 11:18:40,936] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0282394e-21 1.0000000e+00 1.1754934e-26 1.6117923e-18 3.0050372e-27], sum to 1.0000
[2019-03-27 11:18:40,945] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7222
[2019-03-27 11:18:40,950] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 167982: loss 0.0350
[2019-03-27 11:18:40,952] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 167983: learning rate 0.0000
[2019-03-27 11:18:40,953] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.55, 84.5, 1.0, 2.0, 0.5370017189397386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750394.7318814987, 750394.7318814987, 189596.2391537265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6298200.0000, 
sim time next is 6298800.0000, 
raw observation next is [27.53333333333333, 84.66666666666667, 1.0, 2.0, 0.5358956082893193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748848.5316518892, 748848.5316518886, 189411.0823294646], 
processed observation next is [0.0, 0.9130434782608695, 0.5039494470774091, 0.8466666666666667, 1.0, 1.0, 0.44083808227628823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20801348101441366, 0.2080134810144135, 0.28270310795442477], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.9079023], dtype=float32), 1.4048992]. 
=============================================
[2019-03-27 11:18:41,001] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168000: loss 0.0235
[2019-03-27 11:18:41,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168000: learning rate 0.0000
[2019-03-27 11:18:41,020] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 168011: loss 0.0218
[2019-03-27 11:18:41,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 168012: learning rate 0.0000
[2019-03-27 11:18:41,037] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 168015: loss 0.0211
[2019-03-27 11:18:41,039] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 168015: learning rate 0.0000
[2019-03-27 11:18:41,076] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 168028: loss 0.0163
[2019-03-27 11:18:41,079] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 168028: learning rate 0.0000
[2019-03-27 11:18:41,120] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 168040: loss 0.0132
[2019-03-27 11:18:41,123] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 168041: learning rate 0.0000
[2019-03-27 11:18:41,155] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168053: loss 0.0074
[2019-03-27 11:18:41,159] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168053: learning rate 0.0000
[2019-03-27 11:18:41,254] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168093: loss 0.0021
[2019-03-27 11:18:41,255] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168094: learning rate 0.0000
[2019-03-27 11:18:41,268] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 168099: loss 0.0001
[2019-03-27 11:18:41,270] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 168100: learning rate 0.0000
[2019-03-27 11:18:41,308] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 168112: loss 0.0003
[2019-03-27 11:18:41,311] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 168112: learning rate 0.0000
[2019-03-27 11:18:41,336] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 168122: loss 0.0007
[2019-03-27 11:18:41,337] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 168122: learning rate 0.0000
[2019-03-27 11:18:43,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.528812e-23 1.000000e+00 1.191825e-26 1.931795e-18 6.476179e-28], sum to 1.0000
[2019-03-27 11:18:43,624] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3924
[2019-03-27 11:18:43,635] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 75.66666666666667, 1.0, 2.0, 0.5220087075708716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729436.6244912896, 729436.6244912896, 187116.3963388392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6337200.0000, 
sim time next is 6337800.0000, 
raw observation next is [28.85, 75.0, 1.0, 2.0, 0.5235939093401797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731652.492605104, 731652.4926051034, 187375.4744279057], 
processed observation next is [0.0, 0.34782608695652173, 0.5663507109004741, 0.75, 1.0, 1.0, 0.4260167582411803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2032368035014178, 0.20323680350141762, 0.2796648872058294], 
reward next is 0.7203, 
noisyNet noise sample is [array([3.4297638], dtype=float32), -1.7649586]. 
=============================================
[2019-03-27 11:18:51,483] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3555412e-13 9.9714571e-01 3.7902882e-15 2.8543414e-03 9.8471964e-18], sum to 1.0000
[2019-03-27 11:18:51,498] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8297
[2019-03-27 11:18:51,509] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2130095.7344778 W.
[2019-03-27 11:18:51,512] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 67.0, 1.0, 2.0, 0.8821814335416402, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.979953676574459, 6.9112, 168.9125469600783, 2130095.7344778, 2081319.628260943, 430082.3379684598], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6454200.0000, 
sim time next is 6454800.0000, 
raw observation next is [30.0, 67.0, 1.0, 2.0, 0.8722022331333816, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.977166667062811, 6.9112, 168.9125077726039, 2116128.03718158, 2069329.137497065, 427469.425443136], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.67, 1.0, 1.0, 0.8460267869076887, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006596666706281074, 0.0, 0.8294377416441385, 0.58781334366155, 0.5748136493047402, 0.6380140678255761], 
reward next is 0.0322, 
noisyNet noise sample is [array([-0.0811763], dtype=float32), -1.7061641]. 
=============================================
[2019-03-27 11:18:53,217] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.1134031e-20 1.0000000e+00 2.7389531e-23 6.8573212e-13 1.5777632e-24], sum to 1.0000
[2019-03-27 11:18:53,226] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2111
[2019-03-27 11:18:53,234] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 87.0, 1.0, 2.0, 0.5307494053681528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741654.8323595145, 741654.8323595152, 188554.3217904331], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6476400.0000, 
sim time next is 6477000.0000, 
raw observation next is [27.15, 87.16666666666667, 1.0, 2.0, 0.5316882047698023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 742967.1439800152, 742967.1439800158, 188709.9675605736], 
processed observation next is [1.0, 1.0, 0.485781990521327, 0.8716666666666667, 1.0, 1.0, 0.4357689214094003, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2063797622166709, 0.20637976221667104, 0.2816566680008561], 
reward next is 0.7183, 
noisyNet noise sample is [array([-1.2383854], dtype=float32), 0.36043695]. 
=============================================
[2019-03-27 11:18:53,245] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[71.46608 ]
 [71.37702 ]
 [71.341415]
 [71.29658 ]
 [71.257774]], R is [[71.5368576 ]
 [71.54006195]
 [71.54358673]
 [71.54737854]
 [71.55140686]].
[2019-03-27 11:18:57,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9524525e-12 3.0980542e-01 5.6514217e-13 6.9019461e-01 4.1020323e-16], sum to 1.0000
[2019-03-27 11:18:57,348] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9107
[2019-03-27 11:18:57,353] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.63333333333333, 57.0, 1.0, 2.0, 0.4777642860218745, 1.0, 2.0, 0.4777642860218745, 1.0, 1.0, 0.8133932791722622, 6.911200000000001, 6.9112, 170.5573041426782, 2004016.753131374, 2004016.753131374, 397273.9280251889], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6532800.0000, 
sim time next is 6533400.0000, 
raw observation next is [31.51666666666667, 57.5, 1.0, 2.0, 0.731215709082179, 1.0, 2.0, 0.731215709082179, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2044796.898946657, 2044796.898946657, 387765.6266882062], 
processed observation next is [1.0, 0.6086956521739131, 0.6927330173775673, 0.575, 1.0, 1.0, 0.676163504918288, 1.0, 1.0, 0.676163504918288, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5679991385962936, 0.5679991385962936, 0.5787546666988153], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0037189], dtype=float32), 0.014425032]. 
=============================================
[2019-03-27 11:18:59,513] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 11:18:59,516] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:18:59,518] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:18:59,520] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:18:59,519] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:18:59,523] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:18:59,525] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:18:59,521] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:18:59,525] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:18:59,522] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:18:59,526] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:18:59,540] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run8
[2019-03-27 11:18:59,560] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run8
[2019-03-27 11:18:59,580] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run8
[2019-03-27 11:18:59,580] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run8
[2019-03-27 11:18:59,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run8
[2019-03-27 11:19:05,179] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04636048], dtype=float32), 0.029927375]
[2019-03-27 11:19:05,180] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.3, 82.0, 1.0, 2.0, 0.2994262534653076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475955.9677280707, 475955.9677280707, 165318.6499212074]
[2019-03-27 11:19:05,182] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:19:05,184] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.2157523e-20 1.0000000e+00 1.0617324e-23 8.0407451e-16 1.0010030e-24], sampled 0.8425207557640249
[2019-03-27 11:19:21,873] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04636048], dtype=float32), 0.029927375]
[2019-03-27 11:19:21,873] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.75, 37.5, 1.0, 2.0, 0.3403841515411737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565958.1513310807, 565958.1513310807, 170908.2596571682]
[2019-03-27 11:19:21,874] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:19:21,877] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.096049e-19 1.000000e+00 9.511941e-23 9.547715e-15 6.943781e-24], sampled 0.2954008421394212
[2019-03-27 11:19:35,760] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04636048], dtype=float32), 0.029927375]
[2019-03-27 11:19:35,762] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.5, 73.33333333333334, 1.0, 2.0, 0.5552965038335744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775968.8113924285, 775968.8113924279, 192714.5463767938]
[2019-03-27 11:19:35,764] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:19:35,766] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.5667400e-18 1.0000000e+00 2.2630040e-21 1.9111182e-10 2.1868097e-23], sampled 0.7890528210933822
[2019-03-27 11:19:53,665] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04636048], dtype=float32), 0.029927375]
[2019-03-27 11:19:53,666] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.86961224, 75.97332087000001, 1.0, 2.0, 0.5813340528708607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812367.477945179, 812367.477945179, 197318.8743656096]
[2019-03-27 11:19:53,666] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:19:53,669] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4046346e-19 1.0000000e+00 5.1131742e-23 3.2479913e-13 1.7663976e-24], sampled 0.42437898168251287
[2019-03-27 11:20:47,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04636048], dtype=float32), 0.029927375]
[2019-03-27 11:20:47,659] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.5672905, 75.51891352499999, 1.0, 2.0, 0.4160713129384134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 670969.6376410797, 670969.6376410791, 181413.8516167012]
[2019-03-27 11:20:47,660] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:20:47,666] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.8819667e-18 1.0000000e+00 2.5711270e-21 1.3435514e-12 1.2490577e-22], sampled 0.3183987137264509
[2019-03-27 11:20:57,402] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04636048], dtype=float32), 0.029927375]
[2019-03-27 11:20:57,402] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.24900527166667, 96.51317148333334, 1.0, 2.0, 0.361723346479904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 550974.9599844884, 550974.9599844877, 170543.2022466523]
[2019-03-27 11:20:57,402] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:20:57,406] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8668583e-19 1.0000000e+00 4.1676512e-23 4.3227920e-15 3.4907529e-24], sampled 0.6987499213738138
[2019-03-27 11:21:06,447] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.3478 2779288600.3583 931.0000
[2019-03-27 11:21:06,843] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.6275 3007618014.5512 1761.0000
[2019-03-27 11:21:06,947] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7894.1246 3162974820.2369 1749.0000
[2019-03-27 11:21:06,977] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4083 2927262029.1810 1338.0000
[2019-03-27 11:21:07,208] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.3643 2842535953.5383 1129.0000
[2019-03-27 11:21:08,223] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 175000, evaluation results [175000.0, 7894.124598853989, 3162974820.236948, 1749.0, 8254.40832006541, 2927262029.1809688, 1338.0, 8660.34780084087, 2779288600.358337, 931.0, 8000.627545179604, 3007618014.551236, 1761.0, 8498.36434602056, 2842535953.538322, 1129.0]
[2019-03-27 11:21:10,369] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 175820: loss -281.0684
[2019-03-27 11:21:10,372] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 175820: learning rate 0.0000
[2019-03-27 11:21:10,453] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 175850: loss -355.6978
[2019-03-27 11:21:10,459] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 175853: learning rate 0.0000
[2019-03-27 11:21:10,485] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 175864: loss -268.0729
[2019-03-27 11:21:10,487] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 175864: learning rate 0.0000
[2019-03-27 11:21:10,554] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 175889: loss -342.0921
[2019-03-27 11:21:10,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 175890: learning rate 0.0000
[2019-03-27 11:21:10,637] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 175918: loss -224.2044
[2019-03-27 11:21:10,641] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 175918: learning rate 0.0000
[2019-03-27 11:21:10,695] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 175941: loss -300.8911
[2019-03-27 11:21:10,698] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 175942: learning rate 0.0000
[2019-03-27 11:21:10,768] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 175965: loss -503.9134
[2019-03-27 11:21:10,771] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 175966: learning rate 0.0000
[2019-03-27 11:21:10,925] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 176025: loss -322.2801
[2019-03-27 11:21:10,926] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 176025: loss -328.6733
[2019-03-27 11:21:10,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 176025: learning rate 0.0000
[2019-03-27 11:21:10,928] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 176025: learning rate 0.0000
[2019-03-27 11:21:10,944] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 176032: loss -299.5300
[2019-03-27 11:21:10,946] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 176033: learning rate 0.0000
[2019-03-27 11:21:10,965] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 176037: loss -256.3835
[2019-03-27 11:21:10,967] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 176037: learning rate 0.0000
[2019-03-27 11:21:11,007] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 176051: loss -330.2197
[2019-03-27 11:21:11,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 176051: learning rate 0.0000
[2019-03-27 11:21:11,015] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 176053: loss -394.2799
[2019-03-27 11:21:11,022] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 176053: learning rate 0.0000
[2019-03-27 11:21:11,058] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176070: loss -231.0031
[2019-03-27 11:21:11,059] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176070: learning rate 0.0000
[2019-03-27 11:21:11,110] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 176092: loss -434.4370
[2019-03-27 11:21:11,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 176092: learning rate 0.0000
[2019-03-27 11:21:11,171] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 176117: loss -192.9745
[2019-03-27 11:21:11,176] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 176117: learning rate 0.0000
[2019-03-27 11:21:19,897] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.3442995e-20 1.0000000e+00 1.4807875e-23 2.5868263e-15 8.4180432e-25], sum to 1.0000
[2019-03-27 11:21:19,905] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7503
[2019-03-27 11:21:19,914] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 69.83333333333333, 1.0, 2.0, 0.3731243859180476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 567287.1956160339, 567287.1956160332, 171911.6274366681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6731400.0000, 
sim time next is 6732000.0000, 
raw observation next is [25.7, 70.0, 1.0, 2.0, 0.3701595994005154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564494.007154932, 564494.007154932, 171719.5684334686], 
processed observation next is [1.0, 0.9565217391304348, 0.4170616113744076, 0.7, 1.0, 1.0, 0.24115614385604264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15680389087637, 0.15680389087637, 0.2562978633335352], 
reward next is 0.7437, 
noisyNet noise sample is [array([-0.8380671], dtype=float32), 0.781202]. 
=============================================
[2019-03-27 11:21:19,933] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[67.04086 ]
 [67.1328  ]
 [67.1797  ]
 [67.408485]
 [67.562325]], R is [[67.12976837]
 [67.20188904]
 [67.27292633]
 [67.34274292]
 [67.41134644]].
[2019-03-27 11:21:23,246] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4069684e-17 1.0000000e+00 4.7892420e-22 1.1216363e-14 1.6476395e-23], sum to 1.0000
[2019-03-27 11:21:23,256] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4428
[2019-03-27 11:21:23,262] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 69.33333333333334, 1.0, 2.0, 0.7703379065677078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1201873.870929913, 1201873.870929913, 252678.3682327296], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6771000.0000, 
sim time next is 6771600.0000, 
raw observation next is [25.2, 68.0, 1.0, 2.0, 0.7824940040863644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1220329.309675551, 1220329.309675551, 255869.7009395369], 
processed observation next is [1.0, 0.391304347826087, 0.3933649289099526, 0.68, 1.0, 1.0, 0.7379445832365836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3389803637987642, 0.3389803637987642, 0.38189507602915956], 
reward next is 0.6181, 
noisyNet noise sample is [array([1.109962], dtype=float32), -1.1953508]. 
=============================================
[2019-03-27 11:21:31,367] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 183786: loss 0.0252
[2019-03-27 11:21:31,372] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 183786: learning rate 0.0000
[2019-03-27 11:21:31,474] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 183829: loss 0.0133
[2019-03-27 11:21:31,476] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 183830: learning rate 0.0000
[2019-03-27 11:21:31,496] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 183838: loss 0.0156
[2019-03-27 11:21:31,500] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 183840: learning rate 0.0000
[2019-03-27 11:21:31,523] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 183846: loss 0.0125
[2019-03-27 11:21:31,527] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 183846: learning rate 0.0000
[2019-03-27 11:21:31,675] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 183903: loss 0.0038
[2019-03-27 11:21:31,679] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 183903: learning rate 0.0000
[2019-03-27 11:21:31,733] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 183922: loss 0.0011
[2019-03-27 11:21:31,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 183923: learning rate 0.0000
[2019-03-27 11:21:31,870] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183973: loss 0.0049
[2019-03-27 11:21:31,873] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183974: learning rate 0.0000
[2019-03-27 11:21:31,932] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 183999: loss 0.0016
[2019-03-27 11:21:31,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 183999: learning rate 0.0000
[2019-03-27 11:21:32,067] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 184048: loss 0.0026
[2019-03-27 11:21:32,073] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 184048: learning rate 0.0000
[2019-03-27 11:21:32,077] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 184048: loss 0.0023
[2019-03-27 11:21:32,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 184051: learning rate 0.0000
[2019-03-27 11:21:32,139] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184074: loss 0.0007
[2019-03-27 11:21:32,142] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184074: learning rate 0.0000
[2019-03-27 11:21:32,196] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 184094: loss 0.0006
[2019-03-27 11:21:32,201] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 184095: learning rate 0.0000
[2019-03-27 11:21:32,221] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 184100: loss 0.0026
[2019-03-27 11:21:32,226] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 184100: learning rate 0.0000
[2019-03-27 11:21:32,243] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 184109: loss 0.0019
[2019-03-27 11:21:32,255] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 184110: learning rate 0.0000
[2019-03-27 11:21:32,266] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184116: loss 0.0032
[2019-03-27 11:21:32,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184116: learning rate 0.0000
[2019-03-27 11:21:32,277] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184117: loss 0.0036
[2019-03-27 11:21:32,279] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184118: learning rate 0.0000
[2019-03-27 11:21:41,177] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3070703e-17 1.0000000e+00 7.2193812e-21 4.3461623e-12 1.3179903e-22], sum to 1.0000
[2019-03-27 11:21:41,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7081
[2019-03-27 11:21:41,198] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 69.0, 1.0, 2.0, 0.7189436897528485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1032662.526298912, 1032662.526298912, 228738.5850594525], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7027200.0000, 
sim time next is 7027800.0000, 
raw observation next is [27.86666666666667, 68.0, 1.0, 2.0, 0.657469769871141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 944794.0736584287, 944794.0736584287, 215339.9679860453], 
processed observation next is [1.0, 0.34782608695652173, 0.519747235387046, 0.68, 1.0, 1.0, 0.5873129757483627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2624427982384524, 0.2624427982384524, 0.3214029372926049], 
reward next is 0.6786, 
noisyNet noise sample is [array([0.2898915], dtype=float32), 0.23444639]. 
=============================================
[2019-03-27 11:21:44,481] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.2307405e-20 1.0000000e+00 8.8521465e-23 6.4128699e-15 1.0042348e-24], sum to 1.0000
[2019-03-27 11:21:44,497] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2378
[2019-03-27 11:21:44,502] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.85, 86.16666666666667, 1.0, 2.0, 0.4853674686727924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678219.0997227237, 678219.0997227244, 181334.7618072692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7074600.0000, 
sim time next is 7075200.0000, 
raw observation next is [25.8, 86.33333333333334, 1.0, 2.0, 0.485890345235304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678949.9649365854, 678949.9649365848, 181414.370019879], 
processed observation next is [1.0, 0.9130434782608695, 0.42180094786729866, 0.8633333333333334, 1.0, 1.0, 0.3805907773919326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18859721248238484, 0.18859721248238467, 0.2707677164475806], 
reward next is 0.7292, 
noisyNet noise sample is [array([-1.1379964], dtype=float32), 0.7140759]. 
=============================================
[2019-03-27 11:21:44,955] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8026464e-21 1.0000000e+00 8.0057518e-26 1.9776975e-17 2.4159880e-27], sum to 1.0000
[2019-03-27 11:21:44,966] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1840
[2019-03-27 11:21:44,973] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 87.66666666666667, 1.0, 2.0, 0.4784223821799258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668809.4186767162, 668809.4186767162, 180322.4065576535], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7080000.0000, 
sim time next is 7080600.0000, 
raw observation next is [25.43333333333333, 87.83333333333334, 1.0, 2.0, 0.4763658806335168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 666240.161627436, 666240.1616274355, 180053.5501212139], 
processed observation next is [1.0, 0.9565217391304348, 0.40442338072669815, 0.8783333333333334, 1.0, 1.0, 0.3691155188355624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1850667115631767, 0.18506671156317653, 0.268736641971961], 
reward next is 0.7313, 
noisyNet noise sample is [array([-0.9046549], dtype=float32), 0.8096591]. 
=============================================
[2019-03-27 11:21:47,337] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3546615e-20 1.0000000e+00 2.7450916e-24 9.9892382e-18 3.4500943e-26], sum to 1.0000
[2019-03-27 11:21:47,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5067
[2019-03-27 11:21:47,359] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 77.66666666666667, 1.0, 2.0, 0.483100966773526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683841.17663657, 683841.17663657, 182119.4027895072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7112400.0000, 
sim time next is 7113000.0000, 
raw observation next is [26.83333333333333, 76.83333333333333, 1.0, 2.0, 0.4896129603865392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691997.0485221879, 691997.0485221873, 182991.5503285662], 
processed observation next is [1.0, 0.30434782608695654, 0.470774091627172, 0.7683333333333333, 1.0, 1.0, 0.3850758558873966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19222140236727442, 0.19222140236727425, 0.2731217169083078], 
reward next is 0.7269, 
noisyNet noise sample is [array([-0.18037349], dtype=float32), -0.23683472]. 
=============================================
[2019-03-27 11:21:47,375] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.03624 ]
 [71.015915]
 [70.947334]
 [70.79597 ]
 [70.6994  ]], R is [[71.05220795]
 [71.06987   ]
 [71.08827972]
 [71.10687256]
 [71.11937714]].
[2019-03-27 11:21:48,114] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.9800560e-14 9.9999058e-01 1.1550626e-17 9.3674216e-06 3.2840058e-19], sum to 1.0000
[2019-03-27 11:21:48,126] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3350
[2019-03-27 11:21:48,137] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 74.33333333333333, 1.0, 2.0, 0.9691431375493993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1357649.012873701, 1357649.012873701, 290126.7624945131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7130400.0000, 
sim time next is 7131000.0000, 
raw observation next is [27.31666666666667, 75.16666666666667, 1.0, 2.0, 0.9636727922417372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1348991.688292092, 1348991.688292093, 288357.5987765077], 
processed observation next is [1.0, 0.5217391304347826, 0.4936808846761455, 0.7516666666666667, 1.0, 1.0, 0.9562322798093219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37471991341447, 0.3747199134144703, 0.4303844757858324], 
reward next is 0.5696, 
noisyNet noise sample is [array([-0.8269346], dtype=float32), 0.41908142]. 
=============================================
[2019-03-27 11:21:48,151] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[57.463818]
 [55.21889 ]
 [53.06184 ]
 [52.12012 ]
 [53.489574]], R is [[59.85575485]
 [59.82417297]
 [59.79076004]
 [59.19285202]
 [58.60092545]].
[2019-03-27 11:21:52,452] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.47650961e-20 1.00000000e+00 1.25524754e-23 1.83290677e-16
 3.58236282e-25], sum to 1.0000
[2019-03-27 11:21:52,464] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3566
[2019-03-27 11:21:52,470] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 88.0, 1.0, 2.0, 0.5673907862701171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792875.6144799187, 792875.6144799181, 194825.0522412113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7194000.0000, 
sim time next is 7194600.0000, 
raw observation next is [27.0, 87.5, 1.0, 2.0, 0.5667284434632494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791949.7072414268, 791949.7072414268, 194708.3262203799], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.875, 1.0, 1.0, 0.4779860764617462, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21998602978928522, 0.21998602978928522, 0.29060944211997003], 
reward next is 0.7094, 
noisyNet noise sample is [array([-0.27824685], dtype=float32), 1.27554]. 
=============================================
[2019-03-27 11:21:52,517] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 191791: loss 0.0188
[2019-03-27 11:21:52,524] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 191792: learning rate 0.0000
[2019-03-27 11:21:52,655] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 191847: loss 0.0295
[2019-03-27 11:21:52,656] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 191847: learning rate 0.0000
[2019-03-27 11:21:52,657] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 191847: loss 0.0420
[2019-03-27 11:21:52,659] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 191847: learning rate 0.0000
[2019-03-27 11:21:52,687] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 191856: loss 0.0299
[2019-03-27 11:21:52,693] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 191857: learning rate 0.0000
[2019-03-27 11:21:52,699] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191859: loss 0.0433
[2019-03-27 11:21:52,701] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191859: learning rate 0.0000
[2019-03-27 11:21:52,843] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 191914: loss 0.0731
[2019-03-27 11:21:52,845] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 191915: learning rate 0.0000
[2019-03-27 11:21:52,845] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191915: loss 0.0672
[2019-03-27 11:21:52,851] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191915: learning rate 0.0000
[2019-03-27 11:21:52,979] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 191964: loss 0.0354
[2019-03-27 11:21:52,980] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 191964: learning rate 0.0000
[2019-03-27 11:21:53,166] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 192034: loss 0.0176
[2019-03-27 11:21:53,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 192034: learning rate 0.0000
[2019-03-27 11:21:53,178] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 192040: loss 0.0138
[2019-03-27 11:21:53,182] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 192041: learning rate 0.0000
[2019-03-27 11:21:53,231] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 192058: loss 0.0111
[2019-03-27 11:21:53,234] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 192058: learning rate 0.0000
[2019-03-27 11:21:53,236] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192058: loss 0.0122
[2019-03-27 11:21:53,239] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192058: learning rate 0.0000
[2019-03-27 11:21:53,328] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192093: loss 0.0121
[2019-03-27 11:21:53,331] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192094: learning rate 0.0000
[2019-03-27 11:21:53,334] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 192094: loss 0.0133
[2019-03-27 11:21:53,338] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 192094: learning rate 0.0000
[2019-03-27 11:21:53,383] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 192114: loss 0.0121
[2019-03-27 11:21:53,389] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 192115: learning rate 0.0000
[2019-03-27 11:21:53,484] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 192149: loss 0.0123
[2019-03-27 11:21:53,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 192149: learning rate 0.0000
[2019-03-27 11:22:01,272] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.28775884e-14 9.99999881e-01 8.37084032e-18 1.34623775e-07
 8.65759476e-20], sum to 1.0000
[2019-03-27 11:22:01,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7435
[2019-03-27 11:22:01,293] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.23333333333333, 65.0, 1.0, 2.0, 0.3692330817980358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552508.9387727159, 552508.9387727166, 170337.5364096352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7320000.0000, 
sim time next is 7320600.0000, 
raw observation next is [27.15, 65.5, 1.0, 2.0, 0.3666095040643219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548563.7826171903, 548563.7826171903, 169998.3087747669], 
processed observation next is [1.0, 0.7391304347826086, 0.485781990521327, 0.655, 1.0, 1.0, 0.236878920559424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15237882850477508, 0.15237882850477508, 0.25372881906681627], 
reward next is 0.7463, 
noisyNet noise sample is [array([-1.0724694], dtype=float32), -1.4830037]. 
=============================================
[2019-03-27 11:22:03,482] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.4853752e-22 1.0000000e+00 1.7758208e-26 6.3829384e-18 6.6002587e-28], sum to 1.0000
[2019-03-27 11:22:03,493] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5782
[2019-03-27 11:22:03,499] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666666, 75.33333333333333, 1.0, 2.0, 0.4143900331792998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636312.9102827344, 636312.9102827339, 178298.6179107785], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7353600.0000, 
sim time next is 7354200.0000, 
raw observation next is [24.58333333333334, 76.16666666666667, 1.0, 2.0, 0.4114681303065342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 631072.2264921246, 631072.2264921246, 177794.5795916351], 
processed observation next is [1.0, 0.08695652173913043, 0.3641390205371251, 0.7616666666666667, 1.0, 1.0, 0.2909254582006436, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17529784069225685, 0.17529784069225685, 0.26536504416661955], 
reward next is 0.7346, 
noisyNet noise sample is [array([0.48096472], dtype=float32), 0.45731246]. 
=============================================
[2019-03-27 11:22:03,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3841179e-20 1.0000000e+00 2.3743934e-23 9.9356255e-16 2.6170862e-25], sum to 1.0000
[2019-03-27 11:22:03,848] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1274
[2019-03-27 11:22:03,855] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.95, 73.0, 1.0, 2.0, 0.3558436563160816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547202.70411356, 547202.70411356, 170385.2903284315], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7349400.0000, 
sim time next is 7350000.0000, 
raw observation next is [24.96666666666667, 72.66666666666667, 1.0, 2.0, 0.3542607385432091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 545295.6405269419, 545295.6405269412, 170241.8241660794], 
processed observation next is [1.0, 0.043478260869565216, 0.3823064770932071, 0.7266666666666667, 1.0, 1.0, 0.2220008898110953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15147101125748386, 0.15147101125748366, 0.2540922748747454], 
reward next is 0.7459, 
noisyNet noise sample is [array([0.91469944], dtype=float32), -0.8755196]. 
=============================================
[2019-03-27 11:22:03,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.90375]
 [70.0424 ]
 [70.18447]
 [70.31921]
 [70.77864]], R is [[69.77187347]
 [69.81985474]
 [69.86713409]
 [69.91366577]
 [69.95941925]].
[2019-03-27 11:22:05,468] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3917459e-22 1.0000000e+00 1.0673195e-24 5.2851807e-20 6.4294072e-29], sum to 1.0000
[2019-03-27 11:22:05,476] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8045
[2019-03-27 11:22:05,481] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.43333333333334, 92.0, 1.0, 2.0, 0.5385470384313252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 866532.9639977087, 866532.9639977093, 202344.8030189242], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7375800.0000, 
sim time next is 7376400.0000, 
raw observation next is [20.5, 92.0, 1.0, 2.0, 0.5520598120229266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887120.686063668, 887120.686063668, 204900.8962486372], 
processed observation next is [1.0, 0.391304347826087, 0.1706161137440759, 0.92, 1.0, 1.0, 0.46031302653364653, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24642241279546334, 0.24642241279546334, 0.3058222332069212], 
reward next is 0.6942, 
noisyNet noise sample is [array([0.42243326], dtype=float32), -0.8692554]. 
=============================================
[2019-03-27 11:22:07,010] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.8492857e-22 1.0000000e+00 2.6167281e-24 2.5213083e-15 3.6282703e-25], sum to 1.0000
[2019-03-27 11:22:07,020] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8153
[2019-03-27 11:22:07,024] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.61666666666667, 90.16666666666666, 1.0, 2.0, 0.5934007741524251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954769.8846048559, 954769.8846048552, 213409.0474973637], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7401000.0000, 
sim time next is 7401600.0000, 
raw observation next is [20.6, 90.0, 1.0, 2.0, 0.6085679185062383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 979873.9066435049, 979873.9066435049, 216693.7847596302], 
processed observation next is [1.0, 0.6956521739130435, 0.17535545023696694, 0.9, 1.0, 1.0, 0.5283950825376365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2721871962898625, 0.2721871962898625, 0.32342355934273165], 
reward next is 0.6766, 
noisyNet noise sample is [array([-0.07960723], dtype=float32), 0.7656516]. 
=============================================
[2019-03-27 11:22:12,964] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.4388599e-22 1.0000000e+00 2.4536971e-27 2.1258225e-21 3.4054113e-28], sum to 1.0000
[2019-03-27 11:22:12,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5571
[2019-03-27 11:22:12,980] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 80.33333333333334, 1.0, 2.0, 0.4055258622927768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596066.4046566807, 596066.4046566807, 173904.4246032597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7480200.0000, 
sim time next is 7480800.0000, 
raw observation next is [25.4, 80.0, 1.0, 2.0, 0.4075194854067613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 597900.8704023627, 597900.8704023634, 174040.2020620153], 
processed observation next is [0.0, 0.6086956521739131, 0.4028436018957346, 0.8, 1.0, 1.0, 0.28616805470694134, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1660835751117674, 0.1660835751117676, 0.2597614956149482], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.7929159], dtype=float32), -0.6489056]. 
=============================================
[2019-03-27 11:22:13,581] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 199782: loss 0.0107
[2019-03-27 11:22:13,585] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 199782: learning rate 0.0000
[2019-03-27 11:22:13,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7249526e-24 1.0000000e+00 8.1875278e-29 4.2577695e-23 1.9066996e-31], sum to 1.0000
[2019-03-27 11:22:13,656] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6957
[2019-03-27 11:22:13,661] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 82.33333333333334, 1.0, 2.0, 0.4056674306009919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596655.12751652, 596655.12751652, 173970.7512253905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7500000.0000, 
sim time next is 7500600.0000, 
raw observation next is [24.85, 83.0, 1.0, 2.0, 0.4047198760001976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595707.356162778, 595707.356162778, 173897.1060322049], 
processed observation next is [0.0, 0.8260869565217391, 0.37677725118483424, 0.83, 1.0, 1.0, 0.2827950313255393, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16547426560077166, 0.16547426560077166, 0.2595479194510521], 
reward next is 0.7405, 
noisyNet noise sample is [array([0.78667563], dtype=float32), 0.59765446]. 
=============================================
[2019-03-27 11:22:13,669] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 199812: loss 0.0031
[2019-03-27 11:22:13,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 199813: learning rate 0.0000
[2019-03-27 11:22:13,794] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 199864: loss 0.0015
[2019-03-27 11:22:13,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 199864: learning rate 0.0000
[2019-03-27 11:22:13,810] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199868: loss 0.0004
[2019-03-27 11:22:13,813] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199869: learning rate 0.0000
[2019-03-27 11:22:13,842] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199878: loss 0.0003
[2019-03-27 11:22:13,844] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199879: learning rate 0.0000
[2019-03-27 11:22:13,851] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 199881: loss 0.0005
[2019-03-27 11:22:13,856] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 199881: learning rate 0.0000
[2019-03-27 11:22:13,979] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199928: loss 0.0009
[2019-03-27 11:22:13,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199931: learning rate 0.0000
[2019-03-27 11:22:14,153] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 199993: loss 0.0075
[2019-03-27 11:22:14,156] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 199994: learning rate 0.0000
[2019-03-27 11:22:14,169] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 11:22:14,171] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:22:14,172] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:22:14,175] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:22:14,173] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:22:14,177] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:22:14,178] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:22:14,180] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:22:14,179] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:22:14,187] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:22:14,187] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:22:14,203] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run9
[2019-03-27 11:22:14,203] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run9
[2019-03-27 11:22:14,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run9
[2019-03-27 11:22:14,236] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run9
[2019-03-27 11:22:14,272] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run9
[2019-03-27 11:22:38,242] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04561549], dtype=float32), 0.03221811]
[2019-03-27 11:22:38,243] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.8, 93.33333333333334, 1.0, 2.0, 0.3817276113525269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575046.238524547, 575046.2385245475, 172435.9016830741]
[2019-03-27 11:22:38,243] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:22:38,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.3459053e-25 1.0000000e+00 2.8623634e-29 3.8156811e-22 1.4002340e-30], sampled 0.438309888192444
[2019-03-27 11:23:05,468] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04561549], dtype=float32), 0.03221811]
[2019-03-27 11:23:05,470] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.73333333333333, 63.66666666666667, 1.0, 2.0, 0.8594753590143838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1201267.365923802, 1201267.365923803, 259037.237137003]
[2019-03-27 11:23:05,471] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:23:05,475] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.1242470e-24 1.0000000e+00 2.8222698e-28 3.1902534e-20 9.9219194e-30], sampled 0.4703514714463516
[2019-03-27 11:23:55,352] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04561549], dtype=float32), 0.03221811]
[2019-03-27 11:23:55,354] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.2, 76.83333333333334, 1.0, 2.0, 0.5884754169521629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 822350.821807841, 822350.8218078415, 198618.1587660254]
[2019-03-27 11:23:55,355] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:23:55,357] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7015551e-23 1.0000000e+00 1.3983519e-27 3.2642886e-20 6.1086411e-29], sampled 0.44294829317950224
[2019-03-27 11:24:15,507] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04561549], dtype=float32), 0.03221811]
[2019-03-27 11:24:15,508] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.91143160666666, 84.51938411666666, 1.0, 2.0, 0.4238390751756483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604072.9067492844, 604072.9067492844, 174065.9403923056]
[2019-03-27 11:24:15,510] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:24:15,514] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.9046314e-24 1.0000000e+00 2.4948379e-28 9.1960068e-22 1.6706085e-29], sampled 0.6903569768111294
[2019-03-27 11:24:21,913] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 11:24:22,200] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-03-27 11:24:22,226] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164139523.4469 1778.0000
[2019-03-27 11:24:22,400] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7509 2842578851.4533 1131.0000
[2019-03-27 11:24:22,475] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5195 2779329887.3399 933.0000
[2019-03-27 11:24:23,491] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 200000, evaluation results [200000.0, 7883.41881391125, 3164139523.4469013, 1778.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8660.519511474935, 2779329887.339914, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.750881238198, 2842578851.453314, 1131.0]
[2019-03-27 11:24:23,615] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 200053: loss 0.0071
[2019-03-27 11:24:23,620] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 200053: loss 0.0080
[2019-03-27 11:24:23,622] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 200053: learning rate 0.0000
[2019-03-27 11:24:23,623] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 200053: learning rate 0.0000
[2019-03-27 11:24:23,638] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 200058: loss 0.0065
[2019-03-27 11:24:23,641] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 200059: learning rate 0.0000
[2019-03-27 11:24:23,671] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 200069: loss 0.0023
[2019-03-27 11:24:23,676] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 200071: learning rate 0.0000
[2019-03-27 11:24:23,782] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200111: loss 0.0023
[2019-03-27 11:24:23,783] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200111: learning rate 0.0000
[2019-03-27 11:24:23,798] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 200119: loss 0.0027
[2019-03-27 11:24:23,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 200119: learning rate 0.0000
[2019-03-27 11:24:23,910] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 200154: loss 0.0013
[2019-03-27 11:24:23,912] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 200155: learning rate 0.0000
[2019-03-27 11:24:23,946] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200169: loss 0.0006
[2019-03-27 11:24:23,948] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200169: learning rate 0.0000
[2019-03-27 11:24:27,347] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4122117e-20 1.0000000e+00 1.7084743e-26 4.5473965e-19 1.0901271e-26], sum to 1.0000
[2019-03-27 11:24:27,362] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5244
[2019-03-27 11:24:27,369] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333334, 78.16666666666667, 1.0, 2.0, 0.4630433475360153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649402.900580505, 649402.900580505, 178317.5645487464], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7553400.0000, 
sim time next is 7554000.0000, 
raw observation next is [27.06666666666667, 77.33333333333334, 1.0, 2.0, 0.4687798867984823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655033.6082073301, 655033.6082073307, 178849.1823778388], 
processed observation next is [0.0, 0.43478260869565216, 0.48183254344391807, 0.7733333333333334, 1.0, 1.0, 0.3599757672270872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1819537800575917, 0.18195378005759186, 0.2669390781758788], 
reward next is 0.7331, 
noisyNet noise sample is [array([0.32045448], dtype=float32), 0.07878825]. 
=============================================
[2019-03-27 11:24:27,397] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.460754]
 [74.47763 ]
 [74.46534 ]
 [74.50088 ]
 [74.52116 ]], R is [[74.41951752]
 [74.40917969]
 [74.39989471]
 [74.39155579]
 [74.38404846]].
[2019-03-27 11:24:28,224] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1531016e-24 1.0000000e+00 1.4659858e-27 1.1006304e-21 3.0938906e-29], sum to 1.0000
[2019-03-27 11:24:28,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0664
[2019-03-27 11:24:28,239] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 66.0, 1.0, 2.0, 0.4682348572168473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654271.7951592738, 654271.7951592732, 178768.9049729069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7560000.0000, 
sim time next is 7560600.0000, 
raw observation next is [29.0, 65.5, 1.0, 2.0, 0.4654160733662236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650501.3002128416, 650501.3002128416, 178376.8866243147], 
processed observation next is [0.0, 0.5217391304347826, 0.5734597156398105, 0.655, 1.0, 1.0, 0.3559229799593055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1806948056146782, 0.1806948056146782, 0.2662341591407682], 
reward next is 0.7338, 
noisyNet noise sample is [array([1.4170569], dtype=float32), 0.9193795]. 
=============================================
[2019-03-27 11:24:36,750] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4629270e-16 1.0000000e+00 1.1341148e-19 1.4136532e-09 1.0870984e-21], sum to 1.0000
[2019-03-27 11:24:36,763] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2002
[2019-03-27 11:24:36,768] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333334, 88.0, 1.0, 2.0, 0.4854030803664347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678268.8769390162, 678268.8769390162, 181340.2800587422], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7683600.0000, 
sim time next is 7684200.0000, 
raw observation next is [25.6, 88.0, 1.0, 2.0, 0.4853846873764654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678243.1676352267, 678243.1676352262, 181337.402721698], 
processed observation next is [1.0, 0.9565217391304348, 0.4123222748815167, 0.88, 1.0, 1.0, 0.3799815510559824, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1884008798986741, 0.18840087989867393, 0.2706528398831313], 
reward next is 0.7293, 
noisyNet noise sample is [array([0.33427483], dtype=float32), 0.6460351]. 
=============================================
[2019-03-27 11:24:43,604] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0317894e-17 1.0000000e+00 6.7095246e-19 6.0666818e-09 5.7456793e-21], sum to 1.0000
[2019-03-27 11:24:43,621] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8828
[2019-03-27 11:24:43,630] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 91.0, 1.0, 2.0, 0.5763747279123347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 805434.5934760862, 805434.5934760856, 196420.8555686802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7794000.0000, 
sim time next is 7794600.0000, 
raw observation next is [25.53333333333333, 90.50000000000001, 1.0, 2.0, 0.6111462772103123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 854044.4074519783, 854044.4074519777, 202832.4062318242], 
processed observation next is [1.0, 0.21739130434782608, 0.4091627172195892, 0.9050000000000001, 1.0, 1.0, 0.5315015388076052, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23723455762554954, 0.23723455762554937, 0.30273493467436446], 
reward next is 0.6973, 
noisyNet noise sample is [array([-0.22771081], dtype=float32), -0.94968516]. 
=============================================
[2019-03-27 11:24:43,983] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 207779: loss 0.0683
[2019-03-27 11:24:43,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 207781: learning rate 0.0000
[2019-03-27 11:24:44,101] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 207818: loss 0.0698
[2019-03-27 11:24:44,104] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 207819: learning rate 0.0000
[2019-03-27 11:24:44,181] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207847: loss 0.0651
[2019-03-27 11:24:44,185] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207850: learning rate 0.0000
[2019-03-27 11:24:44,257] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 207880: loss 0.0352
[2019-03-27 11:24:44,258] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 207880: learning rate 0.0000
[2019-03-27 11:24:44,260] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 207880: loss 0.0374
[2019-03-27 11:24:44,264] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 207880: learning rate 0.0000
[2019-03-27 11:24:44,346] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 207909: loss 0.0232
[2019-03-27 11:24:44,347] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 207909: learning rate 0.0000
[2019-03-27 11:24:44,408] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207934: loss 0.0199
[2019-03-27 11:24:44,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207934: learning rate 0.0000
[2019-03-27 11:24:44,417] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 207936: loss 0.0149
[2019-03-27 11:24:44,425] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 207936: learning rate 0.0000
[2019-03-27 11:24:44,731] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 208053: loss 0.0086
[2019-03-27 11:24:44,733] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 208053: learning rate 0.0000
[2019-03-27 11:24:44,746] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 208059: loss 0.0087
[2019-03-27 11:24:44,748] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 208059: learning rate 0.0000
[2019-03-27 11:24:44,760] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 208063: loss 0.0096
[2019-03-27 11:24:44,761] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 208063: learning rate 0.0000
[2019-03-27 11:24:44,855] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 208099: loss 0.0084
[2019-03-27 11:24:44,857] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 208100: learning rate 0.0000
[2019-03-27 11:24:44,866] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208103: loss 0.0079
[2019-03-27 11:24:44,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208105: learning rate 0.0000
[2019-03-27 11:24:44,921] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208127: loss 0.0086
[2019-03-27 11:24:44,922] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 208127: loss 0.0085
[2019-03-27 11:24:44,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208128: learning rate 0.0000
[2019-03-27 11:24:44,929] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 208128: learning rate 0.0000
[2019-03-27 11:24:44,958] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 208143: loss 0.0075
[2019-03-27 11:24:44,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 208143: learning rate 0.0000
[2019-03-27 11:24:45,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2839395e-12 8.8603300e-01 4.6907075e-14 1.1396702e-01 2.7510497e-16], sum to 1.0000
[2019-03-27 11:24:45,379] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0349
[2019-03-27 11:24:45,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2105433.657676104 W.
[2019-03-27 11:24:45,395] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.06666666666667, 71.33333333333334, 1.0, 2.0, 0.7528780176664052, 1.0, 2.0, 0.7528780176664052, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2105433.657676104, 2105433.657676104, 397581.9701123408], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7816800.0000, 
sim time next is 7817400.0000, 
raw observation next is [30.3, 71.0, 1.0, 2.0, 0.7690602648051655, 1.0, 2.0, 0.7690602648051655, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2150732.963238872, 2150732.963238872, 405089.8312558769], 
processed observation next is [1.0, 0.4782608695652174, 0.6350710900473934, 0.71, 1.0, 1.0, 0.7217593551869464, 1.0, 1.0, 0.7217593551869464, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.597425823121909, 0.597425823121909, 0.6046116884416074], 
reward next is 0.3954, 
noisyNet noise sample is [array([0.34018663], dtype=float32), -0.44374356]. 
=============================================
[2019-03-27 11:24:51,254] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6680849e-18 1.0000000e+00 1.0383509e-19 3.7470230e-10 6.4814140e-22], sum to 1.0000
[2019-03-27 11:24:51,259] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0158
[2019-03-27 11:24:51,270] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 81.50000000000001, 1.0, 2.0, 0.868791640269004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1214295.946784884, 1214295.946784884, 261495.5614935686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7891800.0000, 
sim time next is 7892400.0000, 
raw observation next is [27.56666666666667, 81.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.956398564560557, 6.9112, 168.9074413263596, 2195738.066200659, 1454262.442285448, 311349.1207600065], 
processed observation next is [1.0, 0.34782608695652173, 0.505529225908373, 0.81, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.10451985645605566, 0.0, 0.8294128630736498, 0.6099272406112942, 0.40396178952373557, 0.46470018023881565], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.18662454], dtype=float32), -1.8461443]. 
=============================================
[2019-03-27 11:24:55,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,485] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,486] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-03-27 11:24:55,610] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,611] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,613] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-03-27 11:24:55,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,637] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,639] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-03-27 11:24:55,723] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,723] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,725] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-03-27 11:24:55,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,743] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-03-27 11:24:55,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,778] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-03-27 11:24:55,799] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,800] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,802] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-03-27 11:24:55,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-03-27 11:24:55,874] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,874] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-03-27 11:24:55,968] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,969] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-03-27 11:24:55,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:55,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:55,995] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-03-27 11:24:55,995] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-03-27 11:24:56,014] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:56,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:56,039] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-03-27 11:24:56,015] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:56,063] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:56,065] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-03-27 11:24:56,087] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:56,087] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:56,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-03-27 11:24:56,112] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:24:56,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:24:56,114] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-03-27 11:24:57,861] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.08526126 0.3081509  0.02807079 0.55995715 0.01855998], sum to 1.0000
[2019-03-27 11:24:57,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5675
[2019-03-27 11:24:57,878] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.6, 87.33333333333334, 1.0, 2.0, 0.17, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2907084098322713, 6.911199999999999, 6.9112, 168.912956510431, 509014.9601516128, 509014.9601516134, 192052.6046989477], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1200.0000, 
sim time next is 1800.0000, 
raw observation next is [21.25, 86.5, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 493516.8677674692, 493516.8677674692, 231983.6620074319], 
processed observation next is [1.0, 0.0, 0.20616113744075834, 0.865, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.137088018824297, 0.137088018824297, 0.34624427165288346], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8300401], dtype=float32), 0.56811]. 
=============================================
[2019-03-27 11:25:00,366] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2130687e-13 9.9885893e-01 1.8624144e-17 1.1411161e-03 2.7970174e-19], sum to 1.0000
[2019-03-27 11:25:00,377] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9416
[2019-03-27 11:25:00,381] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 68.0, 1.0, 2.0, 0.9719957618421957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1435826.232993936, 1435826.232993936, 302127.7860122453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 57600.0000, 
sim time next is 58200.0000, 
raw observation next is [27.01666666666667, 68.5, 1.0, 2.0, 0.9955614308898375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1470254.569906476, 1470254.569906476, 309575.5239188839], 
processed observation next is [1.0, 0.6956521739130435, 0.4794628751974725, 0.685, 1.0, 1.0, 0.9946523263732981, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.40840404719624335, 0.40840404719624335, 0.46205302077445354], 
reward next is 0.5379, 
noisyNet noise sample is [array([0.32935244], dtype=float32), 0.19445254]. 
=============================================
[2019-03-27 11:25:00,556] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.800484e-13 9.999337e-01 2.712062e-16 6.630091e-05 3.382906e-19], sum to 1.0000
[2019-03-27 11:25:00,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0559
[2019-03-27 11:25:00,572] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 65.33333333333334, 1.0, 2.0, 1.021612509034909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1550388.656170097, 1550388.656170097, 324055.4728581236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 42600.0000, 
sim time next is 43200.0000, 
raw observation next is [26.8, 65.0, 1.0, 2.0, 0.9816847687986171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1487998.301291125, 1487998.301291124, 310516.4289071359], 
processed observation next is [1.0, 0.5217391304347826, 0.4691943127962086, 0.65, 1.0, 1.0, 0.9779334563838761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4133328614697569, 0.4133328614697567, 0.4634573565778148], 
reward next is 0.5365, 
noisyNet noise sample is [array([-0.9299923], dtype=float32), 0.34235173]. 
=============================================
[2019-03-27 11:25:05,907] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6564955e-17 1.0000000e+00 5.8459452e-20 5.4288191e-10 7.6771893e-23], sum to 1.0000
[2019-03-27 11:25:05,915] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0494
[2019-03-27 11:25:05,919] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.91666666666666, 91.83333333333333, 1.0, 2.0, 0.6901428479089019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1041999.03035852, 1041999.03035852, 228648.5417650107], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 118200.0000, 
sim time next is 118800.0000, 
raw observation next is [22.9, 92.0, 1.0, 2.0, 0.734012973731504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1108080.704503797, 1108080.704503796, 239083.8775810624], 
processed observation next is [1.0, 0.391304347826087, 0.2843601895734597, 0.92, 1.0, 1.0, 0.6795337032909686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30780019569549916, 0.3078001956954989, 0.3568416083299439], 
reward next is 0.6432, 
noisyNet noise sample is [array([-0.2004189], dtype=float32), 1.2060007]. 
=============================================
[2019-03-27 11:25:06,835] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0021193e-13 9.9998939e-01 2.5830307e-16 1.0568546e-05 3.5563294e-19], sum to 1.0000
[2019-03-27 11:25:06,850] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7798
[2019-03-27 11:25:06,857] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.71666666666667, 96.0, 1.0, 2.0, 0.824526832537876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1231179.706479469, 1231179.706479469, 260869.5958023983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 139800.0000, 
sim time next is 140400.0000, 
raw observation next is [22.7, 96.0, 1.0, 2.0, 0.9013953810223885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1346780.443844496, 1346780.443844495, 282780.7736669516], 
processed observation next is [1.0, 0.6521739130434783, 0.27488151658767773, 0.96, 1.0, 1.0, 0.8811992542438415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37410567884569335, 0.37410567884569307, 0.4220608562193307], 
reward next is 0.5779, 
noisyNet noise sample is [array([-1.2342347], dtype=float32), 0.6251493]. 
=============================================
[2019-03-27 11:25:14,346] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3850309e-21 1.0000000e+00 2.4401358e-26 1.3112579e-18 3.3999337e-28], sum to 1.0000
[2019-03-27 11:25:14,359] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1541
[2019-03-27 11:25:14,365] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 89.33333333333334, 1.0, 2.0, 0.2961501490464502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 473780.3036984643, 473780.3036984637, 165208.6884903521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 246000.0000, 
sim time next is 246600.0000, 
raw observation next is [21.0, 89.5, 1.0, 2.0, 0.2948494713123677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471822.59639589, 471822.59639589, 165072.4082548655], 
processed observation next is [0.0, 0.8695652173913043, 0.19431279620853087, 0.895, 1.0, 1.0, 0.150421049773937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13106183233219165, 0.13106183233219165, 0.24637672873860522], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.33396468], dtype=float32), 1.9892396]. 
=============================================
[2019-03-27 11:25:25,409] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3907776e-18 1.0000000e+00 7.0272701e-23 1.7974888e-13 1.3624582e-25], sum to 1.0000
[2019-03-27 11:25:25,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9007
[2019-03-27 11:25:25,435] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.85, 73.5, 1.0, 2.0, 0.5136223537737394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826683.9643831351, 826683.9643831351, 197669.9313300248], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 396600.0000, 
sim time next is 397200.0000, 
raw observation next is [22.8, 74.0, 1.0, 2.0, 0.4151639831687682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 667942.247301376, 667942.2473013753, 181178.2066658606], 
processed observation next is [1.0, 0.6086956521739131, 0.2796208530805688, 0.74, 1.0, 1.0, 0.2953782929744195, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1855395131392711, 0.18553951313927092, 0.2704152338296427], 
reward next is 0.7296, 
noisyNet noise sample is [array([-0.23041305], dtype=float32), 0.45537046]. 
=============================================
[2019-03-27 11:25:27,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8483342e-22 1.0000000e+00 5.5156748e-26 1.2586996e-16 4.3750874e-28], sum to 1.0000
[2019-03-27 11:25:27,148] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9441
[2019-03-27 11:25:27,154] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 84.66666666666667, 1.0, 2.0, 0.2497069628794162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 410761.8222188284, 410761.8222188284, 160875.7515958507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 426000.0000, 
sim time next is 426600.0000, 
raw observation next is [19.85, 85.0, 1.0, 2.0, 0.2492489939481574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 410050.3107986607, 410050.3107986607, 160830.0660650723], 
processed observation next is [1.0, 0.9565217391304348, 0.1398104265402845, 0.85, 1.0, 1.0, 0.09548071560018961, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11390286411073908, 0.11390286411073908, 0.2400448747239885], 
reward next is 0.7600, 
noisyNet noise sample is [array([-0.15694769], dtype=float32), 0.098964125]. 
=============================================
[2019-03-27 11:25:30,248] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.4431792e-18 1.0000000e+00 5.2988043e-21 6.1965572e-11 3.5406152e-24], sum to 1.0000
[2019-03-27 11:25:30,260] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8433
[2019-03-27 11:25:30,265] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 53.0, 1.0, 2.0, 0.5912178189896552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 967586.2510941398, 967586.2510941398, 213693.0376814211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 485400.0000, 
sim time next is 486000.0000, 
raw observation next is [25.1, 53.0, 1.0, 2.0, 0.580819256691232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 951152.6045233355, 951152.6045233348, 211520.7097591924], 
processed observation next is [1.0, 0.6521739130434783, 0.38862559241706174, 0.53, 1.0, 1.0, 0.4949629598689542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26420905681203766, 0.26420905681203743, 0.3157025518793916], 
reward next is 0.6843, 
noisyNet noise sample is [array([1.2999983], dtype=float32), 0.9656201]. 
=============================================
[2019-03-27 11:25:30,288] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.95897 ]
 [68.915146]
 [68.86276 ]
 [68.972435]
 [69.06885 ]], R is [[69.07183838]
 [69.06217194]
 [69.04969788]
 [69.02771759]
 [69.00978851]].
[2019-03-27 11:25:31,328] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 11:25:31,330] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:25:31,331] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:25:31,332] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:25:31,333] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:25:31,334] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:25:31,334] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:25:31,339] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:25:31,339] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:25:31,332] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:25:31,341] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:25:31,368] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run10
[2019-03-27 11:25:31,390] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run10
[2019-03-27 11:25:31,391] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run10
[2019-03-27 11:25:31,392] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run10
[2019-03-27 11:25:31,467] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run10
[2019-03-27 11:25:58,998] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:25:58,999] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.31859824333333, 77.91733303333334, 1.0, 2.0, 0.9645455076962872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1348214.546471264, 1348214.546471264, 288308.5385165588]
[2019-03-27 11:25:59,000] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:25:59,004] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.41952347e-17 1.00000000e+00 1.07408554e-20 2.81500018e-10
 5.18225637e-23], sampled 0.559424140320036
[2019-03-27 11:26:00,880] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:26:00,882] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.66666666666666, 85.00000000000001, 1.0, 2.0, 0.4352938397343204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 638214.0137563113, 638214.0137563113, 177892.8444975389]
[2019-03-27 11:26:00,883] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:26:00,886] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1709389e-19 1.0000000e+00 3.6428499e-23 3.0357932e-13 2.4274734e-25], sampled 0.8797838345860114
[2019-03-27 11:26:00,981] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:26:00,982] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.95, 84.0, 1.0, 2.0, 0.4352627572135689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634768.7877001667, 634768.7877001667, 177468.5149334387]
[2019-03-27 11:26:00,985] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:26:00,988] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.1972730e-19 1.0000000e+00 1.0732161e-22 1.9267962e-12 5.9821433e-25], sampled 0.8985353796372889
[2019-03-27 11:26:03,237] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:26:03,238] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.8, 95.0, 1.0, 2.0, 0.4409584748167353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 634962.6307658377, 634962.6307658384, 177279.5159927135]
[2019-03-27 11:26:03,238] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:26:03,240] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8722001e-19 1.0000000e+00 4.9879303e-23 5.7633158e-14 5.1186724e-25], sampled 0.20060853684235358
[2019-03-27 11:26:32,075] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:26:32,077] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.5, 76.5, 1.0, 2.0, 0.5230039235708138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730827.7827572296, 730827.7827572296, 187278.7301356065]
[2019-03-27 11:26:32,078] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:26:32,082] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.6936068e-20 1.0000000e+00 1.3349508e-23 1.3039288e-13 8.8065288e-26], sampled 0.24923924837565647
[2019-03-27 11:26:33,343] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:26:33,344] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.39087775, 72.20740698, 1.0, 2.0, 0.7751809332481026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1083391.035737136, 1083391.035737136, 237927.9489342819]
[2019-03-27 11:26:33,346] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:26:33,349] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4271520e-18 1.0000000e+00 3.1099113e-22 1.2423377e-12 2.7243942e-24], sampled 0.4167731392735986
[2019-03-27 11:26:34,718] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:26:34,719] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.65608087166667, 77.75327554833333, 1.0, 2.0, 0.5463410047690894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763449.9403626783, 763449.940362679, 191174.3474485157]
[2019-03-27 11:26:34,721] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:26:34,723] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2036308e-19 1.0000000e+00 1.8481992e-23 1.8658726e-13 1.2503525e-25], sampled 0.2370637141014672
[2019-03-27 11:26:37,432] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:26:37,433] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 79.0, 1.0, 2.0, 0.5409512605010722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755915.7006757446, 755915.7006757446, 190264.5678600616]
[2019-03-27 11:26:37,434] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:26:37,439] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.4208315e-19 1.0000000e+00 1.8179854e-22 1.3980553e-09 1.3722068e-25], sampled 0.4603718028560839
[2019-03-27 11:26:56,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:26:56,176] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 78.16666666666666, 1.0, 2.0, 0.9465622852256229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1323062.441359849, 1323062.441359849, 283069.1100620496]
[2019-03-27 11:26:56,176] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:26:56,178] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.3300165e-17 1.0000000e+00 1.8043420e-20 1.5915813e-10 1.1723297e-22], sampled 0.40574175311570015
[2019-03-27 11:26:59,484] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:26:59,485] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.18796994, 49.59837172666667, 1.0, 2.0, 0.5552459851692648, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9593830510243752, 6.9112, 6.9112, 168.9126972551531, 1552363.568299222, 1552363.568299222, 338736.5445374223]
[2019-03-27 11:26:59,486] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:26:59,489] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.7593185e-17 1.0000000e+00 2.5008223e-20 1.9820068e-09 1.0089589e-22], sampled 0.3692742210885652
[2019-03-27 11:27:00,377] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:27:00,378] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 84.66666666666667, 1.0, 2.0, 0.8643503535663157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1208084.901076506, 1208084.901076506, 260323.7262303392]
[2019-03-27 11:27:00,379] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:27:00,385] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.8797542e-18 1.0000000e+00 2.9031509e-21 5.8525212e-11 1.7499333e-23], sampled 0.617905820829146
[2019-03-27 11:27:06,913] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:27:06,914] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.98333333333333, 94.16666666666667, 1.0, 2.0, 1.031517588784715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1441889.809772644, 1441889.809772644, 308675.2991374247]
[2019-03-27 11:27:06,914] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:27:06,917] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.2484044e-18 1.0000000e+00 9.7897704e-22 5.5675590e-12 6.7466453e-24], sampled 0.20666758240162053
[2019-03-27 11:27:23,590] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:27:23,592] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.37580732, 59.89976912, 1.0, 2.0, 0.3814610404924417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581704.783949954, 581704.783949954, 173228.4054509278]
[2019-03-27 11:27:23,593] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:27:23,596] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.53342206e-20 1.00000000e+00 1.08869084e-23 5.95974144e-14
 7.82235768e-26], sampled 0.280524938178914
[2019-03-27 11:27:32,642] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:27:32,643] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.03333333333333, 75.33333333333333, 1.0, 2.0, 0.4701109190999512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665329.260874998, 665329.260874998, 180131.2394996943]
[2019-03-27 11:27:32,643] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:27:32,645] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1594472e-19 1.0000000e+00 1.7998707e-23 6.1702555e-15 2.3761760e-25], sampled 0.7206529715370075
[2019-03-27 11:27:36,339] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04732253], dtype=float32), 0.036676764]
[2019-03-27 11:27:36,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.25, 74.5, 1.0, 2.0, 0.8281167878590674, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.98670366124885, 6.9112, 168.9124462458091, 2054424.675878888, 2000859.941766244, 415613.1380108772]
[2019-03-27 11:27:36,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:27:36,347] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.6195805e-14 9.9999380e-01 4.9728110e-17 6.2295376e-06 2.2542298e-19], sampled 0.42801054323582366
[2019-03-27 11:27:36,349] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2054424.675878888 W.
[2019-03-27 11:27:38,689] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.0215 2842394962.3226 1129.0000
[2019-03-27 11:27:39,077] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.7689 3007447925.8643 1766.0000
[2019-03-27 11:27:39,161] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4083 2927262029.1810 1338.0000
[2019-03-27 11:27:39,320] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.6052 3164017496.4476 1772.0000
[2019-03-27 11:27:39,379] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1477 2779198116.1957 933.0000
[2019-03-27 11:27:40,395] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 225000, evaluation results [225000.0, 7885.605244135381, 3164017496.447587, 1772.0, 8254.40832006541, 2927262029.1809688, 1338.0, 8659.147696843493, 2779198116.195674, 933.0, 7997.768946338684, 3007447925.8643103, 1766.0, 8498.021548991537, 2842394962.3226204, 1129.0]
[2019-03-27 11:27:40,634] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3046170e-18 1.0000000e+00 2.2278173e-21 3.0886316e-10 2.1768068e-22], sum to 1.0000
[2019-03-27 11:27:40,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6290
[2019-03-27 11:27:40,648] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.95, 55.0, 1.0, 2.0, 0.5787618109575396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 945408.8146812594, 945408.8146812594, 211038.9238447963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 480600.0000, 
sim time next is 481200.0000, 
raw observation next is [25.1, 54.33333333333333, 1.0, 2.0, 0.5884396747616139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 960885.5958692874, 960885.5958692874, 213052.9043557368], 
processed observation next is [1.0, 0.5652173913043478, 0.38862559241706174, 0.5433333333333333, 1.0, 1.0, 0.5041441864597757, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2669126655192465, 0.2669126655192465, 0.3179894094861743], 
reward next is 0.6820, 
noisyNet noise sample is [array([-0.7612683], dtype=float32), 1.5879443]. 
=============================================
[2019-03-27 11:27:51,495] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7630316e-19 1.0000000e+00 1.0337967e-21 1.3193854e-12 5.4383148e-24], sum to 1.0000
[2019-03-27 11:27:51,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9368
[2019-03-27 11:27:51,505] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 60.66666666666666, 1.0, 2.0, 0.5897084698377402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 966447.4489664057, 966447.4489664051, 213397.3104552402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 643200.0000, 
sim time next is 643800.0000, 
raw observation next is [23.83333333333333, 59.83333333333334, 1.0, 2.0, 0.5921751943221502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970182.2452247476, 970182.2452247476, 213914.8060947911], 
processed observation next is [1.0, 0.43478260869565216, 0.32859399684044216, 0.5983333333333334, 1.0, 1.0, 0.5086448124363254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26949506811798546, 0.26949506811798546, 0.31927582999222553], 
reward next is 0.6807, 
noisyNet noise sample is [array([-0.2164251], dtype=float32), -0.5931192]. 
=============================================
[2019-03-27 11:27:53,325] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2761097e-22 1.0000000e+00 1.2978418e-25 2.6247767e-16 2.2038809e-27], sum to 1.0000
[2019-03-27 11:27:53,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4051
[2019-03-27 11:27:53,346] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.45, 71.5, 1.0, 2.0, 0.2442429345325114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402702.5802681952, 402702.5802681945, 160319.3379870274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 675000.0000, 
sim time next is 675600.0000, 
raw observation next is [21.23333333333333, 72.66666666666667, 1.0, 2.0, 0.2432084654639678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 401205.2548112692, 401205.2548112686, 160211.5489642061], 
processed observation next is [1.0, 0.8260869565217391, 0.2053712480252764, 0.7266666666666667, 1.0, 1.0, 0.08820297043851541, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11144590411424143, 0.11144590411424128, 0.23912171487194941], 
reward next is 0.7609, 
noisyNet noise sample is [array([-0.3021958], dtype=float32), 0.7361861]. 
=============================================
[2019-03-27 11:27:54,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1534946e-20 1.0000000e+00 1.3677914e-24 2.0840835e-15 2.5787821e-26], sum to 1.0000
[2019-03-27 11:27:54,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5048
[2019-03-27 11:27:54,545] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 80.66666666666667, 1.0, 2.0, 0.2415022161778914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399074.9289187366, 399074.9289187372, 160012.917123301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 681000.0000, 
sim time next is 681600.0000, 
raw observation next is [19.9, 81.33333333333334, 1.0, 2.0, 0.2408704641226388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 398089.9218956504, 398089.9218956504, 159948.9190431294], 
processed observation next is [1.0, 0.9130434782608695, 0.14218009478672985, 0.8133333333333335, 1.0, 1.0, 0.08538610135257685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11058053385990288, 0.11058053385990288, 0.2387297299151185], 
reward next is 0.7613, 
noisyNet noise sample is [array([-0.17510736], dtype=float32), 1.197776]. 
=============================================
[2019-03-27 11:28:08,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4035347e-21 1.0000000e+00 3.2078301e-25 2.1313159e-17 4.9203778e-27], sum to 1.0000
[2019-03-27 11:28:08,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6237
[2019-03-27 11:28:08,483] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.88333333333333, 87.83333333333334, 1.0, 2.0, 0.2841133831069969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457440.0960175311, 457440.0960175318, 164099.2225834128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 879000.0000, 
sim time next is 879600.0000, 
raw observation next is [20.86666666666667, 87.66666666666667, 1.0, 2.0, 0.2817372280340162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 453932.0935879869, 453932.0935879875, 163863.0852093133], 
processed observation next is [0.0, 0.17391304347826086, 0.18799368088467638, 0.8766666666666667, 1.0, 1.0, 0.1346231663060436, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12609224821888526, 0.1260922482188854, 0.24457176896912436], 
reward next is 0.7554, 
noisyNet noise sample is [array([0.8314943], dtype=float32), 0.013668079]. 
=============================================
[2019-03-27 11:28:10,962] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.7626246e-22 1.0000000e+00 2.8291944e-25 1.4121241e-18 3.4090506e-28], sum to 1.0000
[2019-03-27 11:28:10,970] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9795
[2019-03-27 11:28:10,974] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 67.5, 1.0, 2.0, 0.311547813275998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491032.7704024446, 491032.7704024446, 166322.9622889166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 916200.0000, 
sim time next is 916800.0000, 
raw observation next is [24.8, 68.0, 1.0, 2.0, 0.3132407388071468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493350.3158975887, 493350.3158975881, 166485.9569494055], 
processed observation next is [0.0, 0.6086956521739131, 0.3744075829383887, 0.68, 1.0, 1.0, 0.17257920338210458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13704175441599686, 0.1370417544159967, 0.24848650290956042], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.14227949], dtype=float32), 1.0347672]. 
=============================================
[2019-03-27 11:28:13,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9866486e-21 1.0000000e+00 4.0239523e-24 5.3416809e-16 1.8123868e-26], sum to 1.0000
[2019-03-27 11:28:13,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6235
[2019-03-27 11:28:13,098] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.5464192945959767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 846897.5477797518, 846897.5477797525, 201097.2470592801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 981600.0000, 
sim time next is 982200.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.5615107131321658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 870243.2071265592, 870243.2071265585, 203992.5564284931], 
processed observation next is [1.0, 0.34782608695652173, 0.23696682464454974, 0.93, 1.0, 1.0, 0.4716996543761034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.241734224201822, 0.2417342242018218, 0.3044665021320792], 
reward next is 0.6955, 
noisyNet noise sample is [array([0.4287734], dtype=float32), 0.51653135]. 
=============================================
[2019-03-27 11:28:22,294] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9233362e-17 1.0000000e+00 2.3729653e-21 2.0237831e-09 3.6520470e-23], sum to 1.0000
[2019-03-27 11:28:22,304] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3090
[2019-03-27 11:28:22,311] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.43333333333334, 72.66666666666666, 1.0, 2.0, 0.4005005941091468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 625421.3745369897, 625421.3745369904, 177405.6012683179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1083000.0000, 
sim time next is 1083600.0000, 
raw observation next is [24.6, 72.0, 1.0, 2.0, 0.446095451224536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695558.2386561892, 695558.2386561886, 184143.6675486088], 
processed observation next is [1.0, 0.5652173913043478, 0.36492890995260674, 0.72, 1.0, 1.0, 0.33264512195727225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19321062184894144, 0.19321062184894128, 0.2748412948486698], 
reward next is 0.7252, 
noisyNet noise sample is [array([-0.14384], dtype=float32), -0.8700781]. 
=============================================
[2019-03-27 11:28:30,798] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.9189344e-19 1.0000000e+00 3.2906765e-23 1.4185932e-13 2.1268402e-25], sum to 1.0000
[2019-03-27 11:28:30,807] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6396
[2019-03-27 11:28:30,814] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.08333333333334, 78.66666666666667, 1.0, 2.0, 0.358055639261249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 550841.4673749172, 550841.4673749172, 170696.5097147721], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1199400.0000, 
sim time next is 1200000.0000, 
raw observation next is [23.96666666666667, 79.33333333333334, 1.0, 2.0, 0.3564349496608108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548636.0145774892, 548636.0145774886, 170519.9841451461], 
processed observation next is [1.0, 0.9130434782608695, 0.33491311216429714, 0.7933333333333334, 1.0, 1.0, 0.2246204212780853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15239889293819145, 0.15239889293819128, 0.25450743902260614], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.01039234], dtype=float32), -0.68096864]. 
=============================================
[2019-03-27 11:28:30,837] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.29234 ]
 [73.274475]
 [73.209274]
 [73.183426]
 [73.142525]], R is [[73.39582825]
 [73.40710449]
 [73.41824341]
 [73.43015289]
 [73.44197083]].
[2019-03-27 11:28:35,400] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.4287473e-21 1.0000000e+00 8.8924850e-24 1.9731425e-13 1.1931949e-25], sum to 1.0000
[2019-03-27 11:28:35,410] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5865
[2019-03-27 11:28:35,417] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 87.33333333333333, 1.0, 2.0, 0.4677127682766395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657033.2627142441, 657033.2627142441, 179143.0367438367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1280400.0000, 
sim time next is 1281000.0000, 
raw observation next is [25.25, 88.16666666666667, 1.0, 2.0, 0.4689321822145334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 659733.1560217439, 659733.1560217432, 179450.6952276578], 
processed observation next is [1.0, 0.8260869565217391, 0.39573459715639814, 0.8816666666666667, 1.0, 1.0, 0.3601592556801608, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18325921000603998, 0.18325921000603979, 0.26783685854874295], 
reward next is 0.7322, 
noisyNet noise sample is [array([0.17157096], dtype=float32), -0.8159136]. 
=============================================
[2019-03-27 11:28:35,435] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[71.41984]
 [72.02445]
 [72.29019]
 [72.94893]
 [73.58377]], R is [[70.54962158]
 [70.57674408]
 [70.60344696]
 [70.62976074]
 [70.65572357]].
[2019-03-27 11:28:41,025] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4721125e-19 1.0000000e+00 5.0619803e-22 5.4898109e-12 6.4183927e-25], sum to 1.0000
[2019-03-27 11:28:41,035] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3267
[2019-03-27 11:28:41,041] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333334, 88.33333333333334, 1.0, 2.0, 0.5523849785694962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 878398.0276198094, 878398.0276198094, 204283.3605974592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1347600.0000, 
sim time next is 1348200.0000, 
raw observation next is [21.4, 88.5, 1.0, 2.0, 0.5589917482240518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889170.9920185768, 889170.9920185768, 205604.6929953521], 
processed observation next is [1.0, 0.6086956521739131, 0.21327014218009477, 0.885, 1.0, 1.0, 0.468664756896448, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24699194222738244, 0.24699194222738244, 0.3068726761124658], 
reward next is 0.6931, 
noisyNet noise sample is [array([0.2790661], dtype=float32), -0.82759106]. 
=============================================
[2019-03-27 11:28:46,166] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 11:28:46,168] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:28:46,168] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:28:46,170] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:28:46,170] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:28:46,171] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:28:46,171] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:28:46,174] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:28:46,177] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:28:46,178] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:28:46,176] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:28:46,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run11
[2019-03-27 11:28:46,216] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run11
[2019-03-27 11:28:46,218] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run11
[2019-03-27 11:28:46,257] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run11
[2019-03-27 11:28:46,278] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run11
[2019-03-27 11:28:50,483] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:28:50,485] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.099808265, 96.30389841, 1.0, 2.0, 0.2756494698410901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 449048.3162560671, 449048.3162560671, 163478.1171710889]
[2019-03-27 11:28:50,486] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:28:50,491] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.8232220e-22 1.0000000e+00 8.5579421e-27 1.4614530e-18 1.4515779e-28], sampled 0.36764461975869056
[2019-03-27 11:28:55,930] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:28:55,930] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.68660017, 58.81805755, 1.0, 2.0, 0.2498988074657309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 414377.5465618901, 414377.5465618895, 160709.8411088227]
[2019-03-27 11:28:55,930] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:28:55,933] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.7936605e-22 1.0000000e+00 5.3643201e-26 1.1638132e-17 7.5766214e-28], sampled 0.09438407201618582
[2019-03-27 11:29:04,621] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:29:04,623] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.55, 90.83333333333334, 1.0, 2.0, 0.2876488060305038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462853.398360409, 462853.398360409, 164467.2992103646]
[2019-03-27 11:29:04,623] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:29:04,626] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6652833e-21 1.0000000e+00 1.3462101e-25 6.1222086e-17 1.6561557e-27], sampled 0.011993099907091187
[2019-03-27 11:29:29,852] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:29:29,855] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.21666666666667, 84.33333333333333, 1.0, 2.0, 0.5265442357096671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735776.6071272029, 735776.6071272029, 187859.0370268381]
[2019-03-27 11:29:29,855] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:29:29,857] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7407263e-22 1.0000000e+00 7.4355810e-27 4.1652461e-18 9.9872010e-29], sampled 0.17368731853936614
[2019-03-27 11:29:34,689] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:29:34,689] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 95.0, 1.0, 2.0, 0.3072213973422602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 484896.8313961297, 484896.8313961304, 165889.6080298697]
[2019-03-27 11:29:34,691] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:29:34,693] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.3736988e-22 1.0000000e+00 2.5074537e-26 3.4574204e-18 3.9619212e-28], sampled 0.2596987403962213
[2019-03-27 11:29:38,668] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:29:38,668] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 84.0, 1.0, 2.0, 0.4781172354825432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668212.2365948637, 668212.2365948637, 180254.5258241533]
[2019-03-27 11:29:38,669] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:29:38,671] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.9344898e-22 1.0000000e+00 3.2583071e-26 5.4841876e-17 3.6943540e-28], sampled 0.11275123120296249
[2019-03-27 11:29:42,180] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:29:42,184] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.0, 63.0, 1.0, 2.0, 0.9696104796022573, 1.0, 2.0, 0.9696104796022573, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2712206.080262655, 2712206.080262655, 510894.1622835617]
[2019-03-27 11:29:42,185] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:29:42,189] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.5551338e-18 1.0000000e+00 1.9933112e-21 4.0923226e-08 2.9801098e-24], sampled 0.8117315343595511
[2019-03-27 11:29:42,191] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2712206.080262655 W.
[2019-03-27 11:29:45,531] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:29:45,534] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.4900976722178233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684830.8911912265, 684830.8911912271, 182058.6909631451]
[2019-03-27 11:29:45,534] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:29:45,540] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5701096e-22 1.0000000e+00 7.0063775e-27 5.9624833e-17 4.9932268e-29], sampled 0.14295165577602076
[2019-03-27 11:30:21,793] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:30:21,796] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.92055123, 96.61080128, 1.0, 2.0, 0.6946778376542697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970828.6711425437, 970828.6711425437, 219703.4499469017]
[2019-03-27 11:30:21,799] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:30:21,802] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.26995420e-22 1.00000000e+00 1.08667685e-26 3.02494954e-17
 1.02588896e-28], sampled 0.21242066027152096
[2019-03-27 11:30:48,217] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:30:48,219] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.77623236333334, 88.05449737666666, 1.0, 2.0, 0.4600746627628207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659865.4743500501, 659865.4743500501, 179754.3089407255]
[2019-03-27 11:30:48,220] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:30:48,224] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0867336e-21 1.0000000e+00 8.0679566e-26 3.8017449e-16 6.1997072e-28], sampled 0.20589545475394522
[2019-03-27 11:30:53,071] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 11:30:53,351] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04686721], dtype=float32), 0.039100055]
[2019-03-27 11:30:53,352] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.2621552, 79.90701206, 1.0, 2.0, 0.528556600585127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738589.6011038579, 738589.6011038579, 188191.603051623]
[2019-03-27 11:30:53,353] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:30:53,355] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7571050e-22 1.0000000e+00 8.5509256e-27 1.2182807e-16 5.0282826e-29], sampled 0.2471310139804339
[2019-03-27 11:30:53,557] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1488 2779263575.6417 933.0000
[2019-03-27 11:30:53,619] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1026 3007788747.8483 1766.0000
[2019-03-27 11:30:53,628] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-03-27 11:30:53,845] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.2199 2842689996.1958 1131.0000
[2019-03-27 11:30:54,861] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 250000, evaluation results [250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8659.14880451904, 2779263575.641703, 933.0, 7998.102600828673, 3007788747.848251, 1766.0, 8497.21993476195, 2842689996.1957846, 1131.0]
[2019-03-27 11:30:59,042] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0442412e-20 1.0000000e+00 6.0471604e-27 1.7034703e-18 8.8017512e-28], sum to 1.0000
[2019-03-27 11:30:59,050] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9987
[2019-03-27 11:30:59,064] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 97.33333333333334, 1.0, 2.0, 0.323469055500303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508083.6761096269, 508083.6761096276, 167562.6779779367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1480800.0000, 
sim time next is 1481400.0000, 
raw observation next is [20.8, 97.5, 1.0, 2.0, 0.3218962846542214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506212.7897439256, 506212.7897439249, 167434.2136142681], 
processed observation next is [0.0, 0.13043478260869565, 0.1848341232227489, 0.975, 1.0, 1.0, 0.18300757187255587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1406146638177571, 0.14061466381775692, 0.24990181136457928], 
reward next is 0.7501, 
noisyNet noise sample is [array([-0.14429091], dtype=float32), -1.6785457]. 
=============================================
[2019-03-27 11:31:02,151] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7351956e-22 1.0000000e+00 3.6132097e-28 2.4598470e-18 4.7958250e-29], sum to 1.0000
[2019-03-27 11:31:02,167] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0707
[2019-03-27 11:31:02,174] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 56.0, 1.0, 2.0, 0.3633649523660085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 553697.7564437406, 553697.7564437406, 170780.7044569965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1525200.0000, 
sim time next is 1525800.0000, 
raw observation next is [28.15, 56.5, 1.0, 2.0, 0.3618000962645297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 552033.1282474456, 552033.128247445, 170661.9828688587], 
processed observation next is [0.0, 0.6521739130434783, 0.533175355450237, 0.565, 1.0, 1.0, 0.23108445333075867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15334253562429043, 0.15334253562429026, 0.25471937741620704], 
reward next is 0.7453, 
noisyNet noise sample is [array([-0.6802628], dtype=float32), 1.7432345]. 
=============================================
[2019-03-27 11:31:04,150] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.3569521e-20 1.0000000e+00 5.9862356e-23 1.0873563e-12 5.5040334e-25], sum to 1.0000
[2019-03-27 11:31:04,162] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7595
[2019-03-27 11:31:04,169] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.68333333333333, 90.0, 1.0, 2.0, 0.3742980431237734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 589032.2880080118, 589032.2880080125, 174214.6120921314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1566600.0000, 
sim time next is 1567200.0000, 
raw observation next is [21.66666666666667, 90.0, 1.0, 2.0, 0.3271658638343183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515053.3206021808, 515053.3206021808, 168124.0166872712], 
processed observation next is [1.0, 0.13043478260869565, 0.22590837282780438, 0.9, 1.0, 1.0, 0.18935646245098586, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14307036683393912, 0.14307036683393912, 0.25093136818995704], 
reward next is 0.7491, 
noisyNet noise sample is [array([0.5749904], dtype=float32), 1.2146436]. 
=============================================
[2019-03-27 11:31:13,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.03912700e-11 1.93307623e-01 2.53147773e-14 8.06692302e-01
 1.26670485e-17], sum to 1.0000
[2019-03-27 11:31:13,861] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3431
[2019-03-27 11:31:13,866] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.06666666666666, 80.0, 1.0, 2.0, 0.6057794111503146, 1.0, 2.0, 0.6057794111503146, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1693745.244263941, 1693745.244263941, 336393.7007332335], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1694400.0000, 
sim time next is 1695000.0000, 
raw observation next is [28.13333333333333, 79.5, 1.0, 2.0, 0.6269475561623723, 1.0, 2.0, 0.6269475561623723, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1752979.298752105, 1752979.298752105, 344415.5715646295], 
processed observation next is [1.0, 0.6086956521739131, 0.532385466034755, 0.795, 1.0, 1.0, 0.5505392242920149, 1.0, 1.0, 0.5505392242920149, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.486938694097807, 0.486938694097807, 0.5140530918875067], 
reward next is 0.4859, 
noisyNet noise sample is [array([-0.96808785], dtype=float32), 1.6473264]. 
=============================================
[2019-03-27 11:31:13,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[59.76956 ]
 [58.960632]
 [57.48801 ]
 [55.63742 ]
 [54.56974 ]], R is [[59.73811722]
 [59.63865662]
 [59.57930756]
 [59.51569366]
 [59.41284943]].
[2019-03-27 11:31:32,461] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0696434e-20 1.0000000e+00 6.5627415e-23 2.8826973e-13 6.2491041e-25], sum to 1.0000
[2019-03-27 11:31:32,471] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4358
[2019-03-27 11:31:32,481] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 95.66666666666666, 1.0, 2.0, 0.4621766040511384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653354.865554823, 653354.865554823, 178856.2033977225], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1989600.0000, 
sim time next is 1990200.0000, 
raw observation next is [24.23333333333333, 95.33333333333334, 1.0, 2.0, 0.4625111874732546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653122.8952995116, 653122.8952995116, 178815.037821896], 
processed observation next is [0.0, 0.0, 0.3475513428120062, 0.9533333333333335, 1.0, 1.0, 0.3524231174376561, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18142302647208655, 0.18142302647208655, 0.2668881161520836], 
reward next is 0.7331, 
noisyNet noise sample is [array([-0.08864455], dtype=float32), -0.53113556]. 
=============================================
[2019-03-27 11:31:42,932] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3944338e-21 1.0000000e+00 8.1728122e-26 3.1287200e-17 6.7459885e-28], sum to 1.0000
[2019-03-27 11:31:42,943] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0851
[2019-03-27 11:31:42,950] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 76.16666666666667, 1.0, 2.0, 0.5627679389941809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786413.2258867561, 786413.2258867561, 194015.3221651925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2121000.0000, 
sim time next is 2121600.0000, 
raw observation next is [30.0, 76.33333333333334, 1.0, 2.0, 0.5635677638004177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 787531.3172691067, 787531.3172691074, 194155.6260370398], 
processed observation next is [0.0, 0.5652173913043478, 0.6208530805687204, 0.7633333333333334, 1.0, 1.0, 0.47417802867520203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21875869924141855, 0.21875869924141875, 0.2897845164731937], 
reward next is 0.7102, 
noisyNet noise sample is [array([-2.09708], dtype=float32), 0.4092382]. 
=============================================
[2019-03-27 11:31:43,525] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5773908e-23 1.0000000e+00 3.6298726e-27 1.6775147e-18 1.9001314e-28], sum to 1.0000
[2019-03-27 11:31:43,537] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9594
[2019-03-27 11:31:43,542] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5565474575879465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777717.5284410662, 777717.5284410662, 192930.6573023678], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2116800.0000, 
sim time next is 2117400.0000, 
raw observation next is [30.0, 75.16666666666667, 1.0, 2.0, 0.5585308538496467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780490.1379214241, 780490.1379214241, 193275.1673035178], 
processed observation next is [0.0, 0.5217391304347826, 0.6208530805687204, 0.7516666666666667, 1.0, 1.0, 0.4681094624694539, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21680281608928448, 0.21680281608928448, 0.28847039896047433], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.8313886], dtype=float32), -0.5876906]. 
=============================================
[2019-03-27 11:31:43,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9748159e-22 1.0000000e+00 9.6641036e-28 6.9720359e-20 1.8748804e-28], sum to 1.0000
[2019-03-27 11:31:43,745] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7160
[2019-03-27 11:31:43,755] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 74.0, 1.0, 2.0, 0.5477626437469091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765437.2362863581, 765437.2362863588, 191418.6536162527], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2113200.0000, 
sim time next is 2113800.0000, 
raw observation next is [30.0, 74.16666666666667, 1.0, 2.0, 0.5504094790552002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769137.2326867845, 769137.2326867852, 191871.7506261404], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.7416666666666667, 1.0, 1.0, 0.4583246735604822, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21364923130188457, 0.21364923130188476, 0.2863757472031946], 
reward next is 0.7136, 
noisyNet noise sample is [array([-1.8497298], dtype=float32), 0.9629577]. 
=============================================
[2019-03-27 11:31:44,969] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5291277e-21 1.0000000e+00 8.2970240e-25 7.7263106e-16 6.7683141e-27], sum to 1.0000
[2019-03-27 11:31:44,977] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7207
[2019-03-27 11:31:44,984] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 89.66666666666667, 1.0, 2.0, 0.5391069384648353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753337.5638576915, 753337.5638576915, 189950.0373977612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [26.85, 90.0, 1.0, 2.0, 0.5382187451808397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752095.9804222521, 752095.9804222527, 189800.6613054311], 
processed observation next is [0.0, 0.9130434782608695, 0.4715639810426541, 0.9, 1.0, 1.0, 0.4436370423865538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20891555011729224, 0.2089155501172924, 0.28328456911258376], 
reward next is 0.7167, 
noisyNet noise sample is [array([0.03907059], dtype=float32), -0.031454273]. 
=============================================
[2019-03-27 11:31:45,002] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.39986 ]
 [74.419014]
 [74.41206 ]
 [74.373344]
 [74.37699 ]], R is [[74.36006165]
 [74.33295441]
 [74.30654907]
 [74.28007507]
 [74.25344086]].
[2019-03-27 11:31:48,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.6804309e-19 1.0000000e+00 7.3582017e-22 6.2512170e-13 4.3346779e-24], sum to 1.0000
[2019-03-27 11:31:48,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9294
[2019-03-27 11:31:48,082] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333333, 95.33333333333334, 1.0, 2.0, 0.6431525871204777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898790.4477961449, 898790.4477961449, 209050.4975111737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2172000.0000, 
sim time next is 2172600.0000, 
raw observation next is [24.9, 95.5, 1.0, 2.0, 0.6251284712388214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873591.8153861965, 873591.8153861965, 205511.5342816726], 
processed observation next is [1.0, 0.13043478260869565, 0.3791469194312796, 0.955, 1.0, 1.0, 0.5483475557094234, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24266439316283236, 0.24266439316283236, 0.30673363325622777], 
reward next is 0.6933, 
noisyNet noise sample is [array([-0.21530992], dtype=float32), -0.17194857]. 
=============================================
[2019-03-27 11:31:49,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5694374e-16 1.0000000e+00 9.8718381e-19 1.5878351e-08 7.6526031e-22], sum to 1.0000
[2019-03-27 11:31:49,548] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2923
[2019-03-27 11:31:49,555] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.4, 78.5, 1.0, 2.0, 0.5573214025742003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778799.4330205028, 778799.4330205034, 193064.8864009527], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2230200.0000, 
sim time next is 2230800.0000, 
raw observation next is [29.23333333333333, 79.33333333333334, 1.0, 2.0, 0.5582713141869784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780127.3244184323, 780127.3244184323, 193229.8405992058], 
processed observation next is [1.0, 0.8260869565217391, 0.5845181674565559, 0.7933333333333334, 1.0, 1.0, 0.4677967640806968, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21670203456067563, 0.21670203456067563, 0.28840274716299374], 
reward next is 0.7116, 
noisyNet noise sample is [array([0.5981636], dtype=float32), 0.9994489]. 
=============================================
[2019-03-27 11:31:49,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8417481e-12 2.4735996e-01 3.9755600e-14 7.5264001e-01 3.0262307e-17], sum to 1.0000
[2019-03-27 11:31:49,704] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6333
[2019-03-27 11:31:49,713] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.03333333333333, 69.0, 1.0, 2.0, 0.6653601453397171, 1.0, 2.0, 0.6653601453397171, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1860476.220404431, 1860476.220404431, 359651.9909728951], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2204400.0000, 
sim time next is 2205000.0000, 
raw observation next is [31.15, 68.5, 1.0, 2.0, 0.6242088568142734, 1.0, 2.0, 0.6242088568142734, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1745315.511021555, 1745315.511021555, 343370.4569407932], 
processed observation next is [1.0, 0.5217391304347826, 0.6753554502369667, 0.685, 1.0, 1.0, 0.5472395865232209, 1.0, 1.0, 0.5472395865232209, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48480986417265415, 0.48480986417265415, 0.5124932193146167], 
reward next is 0.4875, 
noisyNet noise sample is [array([1.0314103], dtype=float32), 0.088064305]. 
=============================================
[2019-03-27 11:31:49,728] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[53.95024 ]
 [53.16186 ]
 [53.113823]
 [52.845596]
 [53.238815]], R is [[55.07330322]
 [54.98577881]
 [54.80861664]
 [54.65777969]
 [54.11120224]].
[2019-03-27 11:31:52,617] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.3182502e-19 1.0000000e+00 4.9773245e-23 4.5677166e-13 1.1920490e-24], sum to 1.0000
[2019-03-27 11:31:52,630] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5365
[2019-03-27 11:31:52,643] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 83.33333333333334, 1.0, 2.0, 0.5399945089657583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754578.278169787, 754578.2781697877, 190099.8050241836], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2238000.0000, 
sim time next is 2238600.0000, 
raw observation next is [27.9, 83.66666666666666, 1.0, 2.0, 0.539062313220602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753275.1832972945, 753275.1832972952, 189942.7619957579], 
processed observation next is [1.0, 0.9130434782608695, 0.5213270142180094, 0.8366666666666666, 1.0, 1.0, 0.44465338942241206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2092431064714707, 0.2092431064714709, 0.283496659695161], 
reward next is 0.7165, 
noisyNet noise sample is [array([1.8598937], dtype=float32), 0.41271242]. 
=============================================
[2019-03-27 11:31:57,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.5032010e-15 1.0000000e+00 4.6180031e-19 2.2980469e-08 1.0710205e-21], sum to 1.0000
[2019-03-27 11:31:57,859] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1699
[2019-03-27 11:31:57,867] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.81666666666667, 67.33333333333334, 1.0, 2.0, 0.5449708193366625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761534.5743908766, 761534.574390876, 190944.5480114991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2310600.0000, 
sim time next is 2311200.0000, 
raw observation next is [31.7, 68.0, 1.0, 2.0, 0.5517458483856501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771005.3408598445, 771005.340859845, 192102.7081934905], 
processed observation next is [1.0, 0.782608695652174, 0.7014218009478673, 0.68, 1.0, 1.0, 0.4599347570911447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2141681502388457, 0.21416815023884583, 0.28672045999028434], 
reward next is 0.7133, 
noisyNet noise sample is [array([1.7369808], dtype=float32), 0.38441625]. 
=============================================
[2019-03-27 11:31:58,071] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.8296094e-17 1.0000000e+00 1.4419807e-20 2.5065799e-10 8.2536579e-23], sum to 1.0000
[2019-03-27 11:31:58,082] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5757
[2019-03-27 11:31:58,099] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666666, 70.66666666666667, 1.0, 2.0, 0.5678245230785808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793481.9475389589, 793481.9475389589, 194905.5421730983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2313600.0000, 
sim time next is 2314200.0000, 
raw observation next is [31.03333333333333, 71.33333333333333, 1.0, 2.0, 0.5675287176369502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793068.4326352568, 793068.4326352568, 194853.256522012], 
processed observation next is [1.0, 0.782608695652174, 0.669826224328594, 0.7133333333333333, 1.0, 1.0, 0.47895026221319303, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2202967868431269, 0.2202967868431269, 0.290825756003003], 
reward next is 0.7092, 
noisyNet noise sample is [array([-0.74269116], dtype=float32), 1.0760907]. 
=============================================
[2019-03-27 11:31:58,700] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8786983e-21 1.0000000e+00 4.5225601e-24 6.1762843e-15 2.1335848e-25], sum to 1.0000
[2019-03-27 11:31:58,716] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8909
[2019-03-27 11:31:58,722] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.95, 77.5, 1.0, 2.0, 0.5752929894162296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803922.383998383, 803922.383998383, 196233.4834917669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2320200.0000, 
sim time next is 2320800.0000, 
raw observation next is [29.86666666666667, 77.66666666666667, 1.0, 2.0, 0.5745063385012651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802822.6916874194, 802822.6916874194, 196092.6855707108], 
processed observation next is [1.0, 0.8695652173913043, 0.6145339652448659, 0.7766666666666667, 1.0, 1.0, 0.48735703433887356, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2230063032465054, 0.2230063032465054, 0.29267565010553853], 
reward next is 0.7073, 
noisyNet noise sample is [array([1.1460798], dtype=float32), -1.9159641]. 
=============================================
[2019-03-27 11:32:00,718] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 11:32:00,720] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:32:00,721] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:32:00,722] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:32:00,723] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:32:00,724] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:32:00,727] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:32:00,725] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:32:00,728] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:32:00,727] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:32:00,732] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:32:00,751] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run12
[2019-03-27 11:32:00,770] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run12
[2019-03-27 11:32:00,771] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run12
[2019-03-27 11:32:00,807] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run12
[2019-03-27 11:32:00,807] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run12
[2019-03-27 11:32:01,673] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04566197], dtype=float32), 0.03467279]
[2019-03-27 11:32:01,673] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.31666666666667, 78.83333333333334, 1.0, 2.0, 0.2108852099016598, 1.0, 2.0, 0.2108852099016598, 0.0, 1.0, 0.0, 6.9112, 6.9112, 172.86236624, 616548.2114392997, 616548.2114392997, 239908.3007776128]
[2019-03-27 11:32:01,673] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:32:01,674] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.12907214 0.2978884  0.04621785 0.48902753 0.03779405], sampled 0.9970122472840312
[2019-03-27 11:33:31,997] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04566197], dtype=float32), 0.03467279]
[2019-03-27 11:33:31,998] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.86905448666667, 59.10317726833333, 1.0, 2.0, 0.8874287573245148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1240359.957744914, 1240359.957744914, 266500.1977003572]
[2019-03-27 11:33:31,999] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:33:32,001] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.4264868e-19 1.0000000e+00 9.9454111e-23 1.6424133e-14 5.1579556e-24], sampled 0.546520581832874
[2019-03-27 11:33:36,818] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04566197], dtype=float32), 0.03467279]
[2019-03-27 11:33:36,819] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.51666666666667, 78.33333333333334, 1.0, 2.0, 0.5453361878470017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762045.3183621697, 762045.3183621702, 191003.2064775471]
[2019-03-27 11:33:36,819] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:33:36,827] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.2926184e-19 1.0000000e+00 2.0992431e-23 8.2109197e-15 9.5883992e-25], sampled 0.31245316299677894
[2019-03-27 11:33:56,232] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04566197], dtype=float32), 0.03467279]
[2019-03-27 11:33:56,233] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.87312455, 90.21351215166668, 1.0, 2.0, 0.9852594002067488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128842185564, 1377186.622556983, 1377186.622556984, 294464.4166317215]
[2019-03-27 11:33:56,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:33:56,238] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8667541e-18 1.0000000e+00 2.3609552e-22 1.0143117e-14 1.7115363e-23], sampled 0.13147836569054938
[2019-03-27 11:34:09,471] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.1084 2927463969.0597 1338.0000
[2019-03-27 11:34:09,670] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 11:34:09,730] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9775 2779206587.5399 933.0000
[2019-03-27 11:34:09,784] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5752 2842451949.6408 1131.0000
[2019-03-27 11:34:10,073] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164139523.4469 1778.0000
[2019-03-27 11:34:11,086] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 275000, evaluation results [275000.0, 7883.41881391125, 3164139523.4469013, 1778.0, 8252.10835660451, 2927463969.059686, 1338.0, 8659.977453803283, 2779206587.539943, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.575228127671, 2842451949.640752, 1131.0]
[2019-03-27 11:34:15,623] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.300080e-21 1.000000e+00 4.888351e-26 6.778755e-19 6.915776e-27], sum to 1.0000
[2019-03-27 11:34:15,636] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1611
[2019-03-27 11:34:15,645] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 81.16666666666667, 1.0, 2.0, 0.9049218598677878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1264824.646383443, 1264824.646383442, 271295.3643619513], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2430600.0000, 
sim time next is 2431200.0000, 
raw observation next is [28.16666666666667, 81.33333333333334, 1.0, 2.0, 0.8485954267895862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1186052.261573756, 1186052.261573756, 256188.9149414295], 
processed observation next is [1.0, 0.13043478260869565, 0.5339652448657191, 0.8133333333333335, 1.0, 1.0, 0.8175848515537183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3294589615482656, 0.3294589615482656, 0.3823715148379545], 
reward next is 0.6176, 
noisyNet noise sample is [array([-0.60344446], dtype=float32), -1.3134292]. 
=============================================
[2019-03-27 11:34:31,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.6555544e-28 1.0000000e+00 2.6835372e-32 8.1933393e-29 5.5880430e-33], sum to 1.0000
[2019-03-27 11:34:31,313] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5445
[2019-03-27 11:34:31,318] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.83333333333334, 1.0, 2.0, 0.4830160154093411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 674932.2969368307, 674932.2969368313, 180977.9562291006], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2635800.0000, 
sim time next is 2636400.0000, 
raw observation next is [27.0, 80.66666666666667, 1.0, 2.0, 0.486316907442497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679546.2044620784, 679546.2044620784, 181480.2396901032], 
processed observation next is [0.0, 0.5217391304347826, 0.4786729857819906, 0.8066666666666668, 1.0, 1.0, 0.3811047077620445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18876283457279955, 0.18876283457279955, 0.2708660293882137], 
reward next is 0.7291, 
noisyNet noise sample is [array([0.20522518], dtype=float32), 1.5315745]. 
=============================================
[2019-03-27 11:34:36,529] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4026580e-27 1.0000000e+00 9.4081478e-32 2.0812822e-27 2.9300800e-33], sum to 1.0000
[2019-03-27 11:34:36,543] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8990
[2019-03-27 11:34:36,549] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4322396927415711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 627312.4138143113, 627312.4138143107, 176654.5404167373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2711400.0000, 
sim time next is 2712000.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4301449196680473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 624343.1823901994, 624343.1823902001, 176365.8898578028], 
processed observation next is [0.0, 0.391304347826087, 0.28909952606635075, 1.0, 1.0, 1.0, 0.3134276140578883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1734286617750554, 0.1734286617750556, 0.2632326714295564], 
reward next is 0.7368, 
noisyNet noise sample is [array([0.08685631], dtype=float32), 0.5874815]. 
=============================================
[2019-03-27 11:34:36,563] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[75.76491 ]
 [75.708015]
 [75.61276 ]
 [75.560356]
 [75.49445 ]], R is [[75.78018188]
 [75.75871277]
 [75.73700714]
 [75.7148056 ]
 [75.69203186]].
[2019-03-27 11:34:37,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4469229e-26 1.0000000e+00 6.5252914e-32 1.1852967e-25 2.7094084e-31], sum to 1.0000
[2019-03-27 11:34:37,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5358
[2019-03-27 11:34:37,200] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3941241737290491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588092.7812811035, 588092.7812811041, 173442.9314072613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2738400.0000, 
sim time next is 2739000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3942585765178038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588293.0676393362, 588293.0676393369, 173461.2461105632], 
processed observation next is [0.0, 0.6956521739130435, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2701910560455468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16341474101092673, 0.16341474101092693, 0.25889738225457193], 
reward next is 0.7411, 
noisyNet noise sample is [array([0.37882206], dtype=float32), 0.16989578]. 
=============================================
[2019-03-27 11:34:37,216] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.14067 ]
 [75.119606]
 [75.09526 ]
 [75.073265]
 [75.0423  ]], R is [[75.1410675 ]
 [75.13078308]
 [75.12073517]
 [75.11083221]
 [75.10092926]].
[2019-03-27 11:34:37,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.5287413e-27 1.0000000e+00 6.0493831e-32 7.0452896e-28 1.2582327e-33], sum to 1.0000
[2019-03-27 11:34:37,268] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2438
[2019-03-27 11:34:37,280] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3855404380879363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580697.1061607042, 580697.1061607042, 172937.7437224951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2720400.0000, 
sim time next is 2721000.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3852357056541121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580237.9726655647, 580237.9726655647, 172896.5633969041], 
processed observation next is [0.0, 0.4782608695652174, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2593201272941109, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16117721462932355, 0.16117721462932355, 0.2580545722341852], 
reward next is 0.7419, 
noisyNet noise sample is [array([0.38940358], dtype=float32), 1.4904287]. 
=============================================
[2019-03-27 11:34:37,303] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.45425 ]
 [74.46352 ]
 [74.457886]
 [74.44582 ]
 [74.41286 ]], R is [[74.43034363]
 [74.42792511]
 [74.42564392]
 [74.42333984]
 [74.42074585]].
[2019-03-27 11:34:50,136] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.3014865e-24 1.0000000e+00 1.7991573e-30 2.6698339e-25 5.0536389e-32], sum to 1.0000
[2019-03-27 11:34:50,144] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3052
[2019-03-27 11:34:50,150] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 89.83333333333333, 1.0, 2.0, 0.6738950353966052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1028959.809565732, 1028959.809565733, 226258.7866490134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2904600.0000, 
sim time next is 2905200.0000, 
raw observation next is [23.0, 89.0, 1.0, 2.0, 0.6851037119232304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044206.227614981, 1044206.227614981, 228627.7600676237], 
processed observation next is [1.0, 0.6521739130434783, 0.28909952606635075, 0.89, 1.0, 1.0, 0.6206068818352173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2900572854486058, 0.2900572854486058, 0.34123546278749806], 
reward next is 0.6588, 
noisyNet noise sample is [array([-0.34363627], dtype=float32), 1.5082432]. 
=============================================
[2019-03-27 11:34:51,906] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8594495e-24 1.0000000e+00 2.4621014e-32 6.3151397e-28 5.2750144e-33], sum to 1.0000
[2019-03-27 11:34:51,920] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8643
[2019-03-27 11:34:51,929] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3027236976142853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482070.9057581156, 482070.9057581156, 165770.3967613646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2943000.0000, 
sim time next is 2943600.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3028660219808765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482297.4615000679, 482297.4615000679, 165786.6792181012], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16007954455527285, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1339715170833522, 0.1339715170833522, 0.24744280480313613], 
reward next is 0.7526, 
noisyNet noise sample is [array([0.6879372], dtype=float32), -0.65682983]. 
=============================================
[2019-03-27 11:35:10,325] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5617588e-25 1.0000000e+00 1.3020841e-29 2.8318184e-25 9.0770132e-31], sum to 1.0000
[2019-03-27 11:35:10,339] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1509
[2019-03-27 11:35:10,345] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 90.33333333333333, 1.0, 2.0, 0.4682276596196061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657270.9418419404, 657270.9418419404, 179156.5997873862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3207000.0000, 
sim time next is 3207600.0000, 
raw observation next is [25.0, 90.0, 1.0, 2.0, 0.4657045598329232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655042.1432493017, 655042.1432493011, 178953.5943287883], 
processed observation next is [0.0, 0.13043478260869565, 0.38388625592417064, 0.9, 1.0, 1.0, 0.3562705540155702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1819561509025838, 0.18195615090258366, 0.2670949169086393], 
reward next is 0.7329, 
noisyNet noise sample is [array([-0.9529804], dtype=float32), 2.0363986]. 
=============================================
[2019-03-27 11:35:15,719] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1552942e-24 1.0000000e+00 1.6412986e-29 4.7444549e-22 5.6027153e-30], sum to 1.0000
[2019-03-27 11:35:15,727] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7590
[2019-03-27 11:35:15,731] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.4829951644860992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674903.1520813071, 674903.1520813066, 180974.3376002834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3284400.0000, 
sim time next is 3285000.0000, 
raw observation next is [26.5, 81.5, 1.0, 2.0, 0.4823792683383901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674042.2694951739, 674042.2694951739, 180880.9422085543], 
processed observation next is [0.0, 0.0, 0.4549763033175356, 0.815, 1.0, 1.0, 0.37636056426312064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18723396374865942, 0.18723396374865942, 0.26997155553515567], 
reward next is 0.7300, 
noisyNet noise sample is [array([0.16653508], dtype=float32), -0.14077148]. 
=============================================
[2019-03-27 11:35:15,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.56946]
 [69.92005]
 [70.76697]
 [71.91053]
 [71.93578]], R is [[69.2554245 ]
 [69.29276276]
 [69.32943726]
 [69.36547852]
 [69.40100098]].
[2019-03-27 11:35:16,543] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3806552e-24 1.0000000e+00 2.0949419e-28 1.1635655e-23 4.7282134e-29], sum to 1.0000
[2019-03-27 11:35:16,553] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6106
[2019-03-27 11:35:16,559] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4792880274897444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669721.429146401, 669721.4291464004, 180414.0429977915], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3286800.0000, 
sim time next is 3287400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4782232211128876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668336.4610090401, 668336.4610090401, 180267.3459203605], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.37135327844926214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1856490169469556, 0.1856490169469556, 0.26905574017964257], 
reward next is 0.7309, 
noisyNet noise sample is [array([-0.7040124], dtype=float32), 1.6582391]. 
=============================================
[2019-03-27 11:35:17,097] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 11:35:17,099] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:35:17,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:35:17,099] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:35:17,100] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:35:17,100] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:35:17,101] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:35:17,104] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:35:17,103] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:35:17,106] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:35:17,105] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:35:17,132] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run13
[2019-03-27 11:35:17,133] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run13
[2019-03-27 11:35:17,151] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run13
[2019-03-27 11:35:17,152] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run13
[2019-03-27 11:35:17,206] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run13
[2019-03-27 11:35:24,361] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:35:24,365] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.5, 89.0, 1.0, 2.0, 0.3471579893600445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570322.9359040478, 570322.9359040478, 172138.6199887624]
[2019-03-27 11:35:24,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:35:24,372] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.5501793e-25 1.0000000e+00 4.3487427e-30 1.3545242e-25 1.5526637e-30], sampled 0.08904352674705573
[2019-03-27 11:35:37,944] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:35:37,945] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.56808124, 88.2143662, 1.0, 2.0, 0.3706114736117022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560895.5810548547, 560895.5810548547, 171278.0252048764]
[2019-03-27 11:35:37,949] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:35:37,951] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.6459927e-26 1.0000000e+00 4.1453728e-31 1.0474896e-26 1.4465510e-31], sampled 0.8006979512620678
[2019-03-27 11:35:38,380] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:35:38,382] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.797098395, 89.04767138, 1.0, 2.0, 0.5873688633291599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835278.3517215736, 835278.3517215736, 200279.1251931332]
[2019-03-27 11:35:38,383] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:35:38,387] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.2167234e-25 1.0000000e+00 2.5200295e-30 2.8628280e-25 9.9977873e-31], sampled 0.5842778847762878
[2019-03-27 11:35:45,016] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:35:45,018] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.2, 79.0, 1.0, 2.0, 0.6426241097807415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 919368.7668865864, 919368.7668865859, 211734.6485727439]
[2019-03-27 11:35:45,019] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:35:45,025] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4059975e-24 1.0000000e+00 8.4305876e-30 8.9532725e-25 3.1174767e-30], sampled 0.380538762275494
[2019-03-27 11:35:47,180] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:35:47,182] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.58734485666667, 88.17563488833335, 1.0, 2.0, 0.6031439490078507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861858.3614272802, 861858.3614272802, 203782.84169078]
[2019-03-27 11:35:47,182] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:35:47,186] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0027726e-24 1.0000000e+00 5.7435493e-30 7.4784852e-25 2.0182070e-30], sampled 0.5480306825661236
[2019-03-27 11:35:51,914] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:35:51,916] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.53333333333333, 89.0, 1.0, 2.0, 0.4871709810520617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680740.0112836091, 680740.0112836091, 181610.072239326]
[2019-03-27 11:35:51,917] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:35:51,920] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0595384e-25 1.0000000e+00 4.0702837e-31 2.3585918e-26 1.5387885e-31], sampled 0.5895182019315717
[2019-03-27 11:36:21,027] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:36:21,028] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.76666666666667, 61.33333333333334, 1.0, 2.0, 0.568328299602719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794186.1915492403, 794186.1915492403, 194994.0513123873]
[2019-03-27 11:36:21,028] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:36:21,031] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0502903e-26 1.0000000e+00 2.4656541e-32 6.3582913e-27 8.0744861e-33], sampled 0.24833137036579878
[2019-03-27 11:36:51,624] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:36:51,625] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.6, 91.66666666666667, 1.0, 2.0, 0.5628163695073484, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9756556505252739, 6.9112, 6.9112, 168.9125680568248, 1573544.632011868, 1573544.632011868, 343958.1833576738]
[2019-03-27 11:36:51,626] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:36:51,628] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8569420e-22 1.0000000e+00 2.1388307e-27 7.5065679e-21 6.5056199e-28], sampled 0.18918975709536678
[2019-03-27 11:37:06,103] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:37:06,103] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.58333333333334, 84.83333333333334, 1.0, 2.0, 0.7434196697875055, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.988017708644009, 6.9112, 168.9124988799364, 1935890.02054784, 1881393.04182162, 395011.1424781819]
[2019-03-27 11:37:06,103] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:37:06,105] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.7280635e-20 1.0000000e+00 2.1664824e-24 1.7307014e-15 2.7027686e-25], sampled 0.3764895939720728
[2019-03-27 11:37:06,106] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1935890.02054784 W.
[2019-03-27 11:37:14,318] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04886941], dtype=float32), 0.035131615]
[2019-03-27 11:37:14,320] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.73333333333333, 79.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.441021626992726, 6.9112, 168.9097459269144, 1829878.926347469, 1454012.373459381, 311346.4737947463]
[2019-03-27 11:37:14,321] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:37:14,324] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8306563e-24 1.0000000e+00 9.1645918e-30 3.6390999e-23 1.9783561e-30], sampled 0.4703476735226677
[2019-03-27 11:37:14,324] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1829878.926347469 W.
[2019-03-27 11:37:24,059] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4308 2779149198.4617 933.0000
[2019-03-27 11:37:24,176] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 11:37:24,920] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 11:37:24,964] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 11:37:25,310] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164134907.6624 1778.0000
[2019-03-27 11:37:26,327] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 300000, evaluation results [300000.0, 7883.418813853433, 3164134907.662386, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8661.430759135581, 2779149198.4616733, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 11:37:30,148] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0964095e-23 1.0000000e+00 9.4635894e-30 2.5442918e-24 8.8804585e-30], sum to 1.0000
[2019-03-27 11:37:30,160] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8059
[2019-03-27 11:37:30,164] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5368056235281804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750120.615587785, 750120.6155877844, 189563.6615732112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3363000.0000, 
sim time next is 3363600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5380746063124562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751894.4923981369, 751894.4923981369, 189776.4799464616], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.89, 1.0, 1.0, 0.44346338109934474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2088595812217047, 0.2088595812217047, 0.28324847753203225], 
reward next is 0.7168, 
noisyNet noise sample is [array([-0.7229671], dtype=float32), 1.2087022]. 
=============================================
[2019-03-27 11:37:39,683] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0930008e-13 9.9995911e-01 3.0682450e-16 4.0846444e-05 5.0888525e-17], sum to 1.0000
[2019-03-27 11:37:39,692] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0817
[2019-03-27 11:37:39,702] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1907319.838367826 W.
[2019-03-27 11:37:39,707] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.6820978810128567, 1.0, 2.0, 0.6820978810128567, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1907319.838367826, 1907319.838367826, 366548.352493737], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3489000.0000, 
sim time next is 3489600.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.4908468313281462, 1.0, 2.0, 0.4908468313281462, 1.0, 1.0, 0.835468798188933, 6.9112, 6.9112, 170.5573041426782, 2058945.16686169, 2058945.16686169, 405834.4662264299], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.38656244738330864, 1.0, 1.0, 0.38656244738330864, 1.0, 0.5, 0.799352192913333, 0.0, 0.0, 0.8375144448122397, 0.5719292130171362, 0.5719292130171362, 0.6057230839200446], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2186815], dtype=float32), 0.6154571]. 
=============================================
[2019-03-27 11:37:42,502] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2334397e-18 1.0000000e+00 4.4630725e-24 1.6487473e-14 1.6050811e-24], sum to 1.0000
[2019-03-27 11:37:42,517] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0519
[2019-03-27 11:37:42,522] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5183974007267487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724388.5905567933, 724388.590556794, 186528.7661862407], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3537000.0000, 
sim time next is 3537600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5177938402131588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723544.9110700776, 723544.911070077, 186431.0019518709], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.419028723148384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.200984697519466, 0.20098469751946582, 0.27825522679383713], 
reward next is 0.7217, 
noisyNet noise sample is [array([-1.5712837], dtype=float32), -0.19518527]. 
=============================================
[2019-03-27 11:37:42,802] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2995575e-19 1.0000000e+00 1.5144807e-24 2.5413159e-15 2.2724546e-25], sum to 1.0000
[2019-03-27 11:37:42,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0194
[2019-03-27 11:37:42,823] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 79.00000000000001, 1.0, 2.0, 0.5485434540216025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766528.7252994437, 766528.7252994437, 191551.2232536246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3532200.0000, 
sim time next is 3532800.0000, 
raw observation next is [28.66666666666667, 79.0, 1.0, 2.0, 0.5439594202376532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760120.7537965934, 760120.7537965934, 190770.0499095085], 
processed observation next is [1.0, 0.9130434782608695, 0.5576619273301741, 0.79, 1.0, 1.0, 0.4505535183586183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21114465383238706, 0.21114465383238706, 0.2847314177753858], 
reward next is 0.7153, 
noisyNet noise sample is [array([-0.8964057], dtype=float32), 1.5563658]. 
=============================================
[2019-03-27 11:37:43,826] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.213678e-18 1.000000e+00 2.543865e-24 9.119181e-16 9.275899e-25], sum to 1.0000
[2019-03-27 11:37:43,832] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7598
[2019-03-27 11:37:43,839] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333334, 79.66666666666667, 1.0, 2.0, 0.6998548810922793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 978067.0431556643, 978067.0431556643, 220810.3659287074], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3554400.0000, 
sim time next is 3555000.0000, 
raw observation next is [26.75, 80.0, 1.0, 2.0, 0.6911932522401791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965956.6661017336, 965956.6661017336, 218952.1802120884], 
processed observation next is [1.0, 0.13043478260869565, 0.4668246445497631, 0.8, 1.0, 1.0, 0.6279436773978062, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26832129613937045, 0.26832129613937045, 0.3267942988240125], 
reward next is 0.6732, 
noisyNet noise sample is [array([-0.13952568], dtype=float32), -0.79726106]. 
=============================================
[2019-03-27 11:37:43,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[61.63864]
 [61.6286 ]
 [61.73197]
 [61.77046]
 [61.82614]], R is [[61.71391678]
 [61.76721191]
 [61.8053627 ]
 [61.84888077]
 [61.88247299]].
[2019-03-27 11:37:48,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.05747125e-17 1.00000000e+00 2.68931928e-23 7.35996507e-12
 2.13791206e-24], sum to 1.0000
[2019-03-27 11:37:48,553] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0904
[2019-03-27 11:37:48,562] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 66.33333333333334, 1.0, 2.0, 0.5509519625397519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769895.5689331697, 769895.5689331697, 191964.5982093485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3609600.0000, 
sim time next is 3610200.0000, 
raw observation next is [31.16666666666667, 66.16666666666666, 1.0, 2.0, 0.5461988551441728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763251.2309266883, 763251.2309266883, 191151.2970614113], 
processed observation next is [1.0, 0.782608695652174, 0.6761453396524489, 0.6616666666666666, 1.0, 1.0, 0.45325163270382257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21201423081296897, 0.21201423081296897, 0.28530044337524074], 
reward next is 0.7147, 
noisyNet noise sample is [array([1.2257433], dtype=float32), 1.066982]. 
=============================================
[2019-03-27 11:37:58,609] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0522793e-10 2.2086237e-01 4.4876014e-13 7.7913755e-01 3.6759549e-14], sum to 1.0000
[2019-03-27 11:37:58,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8784
[2019-03-27 11:37:58,621] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.8379047174107594, 1.0, 2.0, 0.8379047174107594, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2343451.659427491, 2343451.659427491, 438742.149976395], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3753600.0000, 
sim time next is 3754200.0000, 
raw observation next is [31.0, 65.5, 1.0, 2.0, 0.837203825521441, 1.0, 2.0, 0.837203825521441, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2341489.568758868, 2341489.568758868, 438386.1325038921], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.655, 1.0, 1.0, 0.8038600307487241, 1.0, 1.0, 0.8038600307487241, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6504137690996856, 0.6504137690996856, 0.6543076604535704], 
reward next is 0.3457, 
noisyNet noise sample is [array([0.4783126], dtype=float32), -0.39144906]. 
=============================================
[2019-03-27 11:38:10,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1715010e-25 1.0000000e+00 2.8330644e-30 5.9182681e-26 8.4124600e-32], sum to 1.0000
[2019-03-27 11:38:10,476] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6034
[2019-03-27 11:38:10,482] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.66666666666666, 61.0, 1.0, 2.0, 0.5900944955854187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824614.242596752, 824614.242596752, 198915.6553696985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3930000.0000, 
sim time next is 3930600.0000, 
raw observation next is [33.83333333333334, 60.5, 1.0, 2.0, 0.5913252310518421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826334.7751314177, 826334.7751314177, 199141.6164013741], 
processed observation next is [0.0, 0.4782608695652174, 0.8025276461295423, 0.605, 1.0, 1.0, 0.5076207603034242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2295374375365049, 0.2295374375365049, 0.29722629313637927], 
reward next is 0.7028, 
noisyNet noise sample is [array([-1.6112937], dtype=float32), 1.0262086]. 
=============================================
[2019-03-27 11:38:13,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6488269e-21 1.0000000e+00 2.2211153e-27 1.8293686e-20 7.3826156e-27], sum to 1.0000
[2019-03-27 11:38:13,913] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6024
[2019-03-27 11:38:13,918] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333333, 71.66666666666667, 1.0, 2.0, 0.6070122641293909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 848265.0371713643, 848265.037171365, 202059.8077364328], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3966600.0000, 
sim time next is 3967200.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.6084379794734014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850258.1905649487, 850258.1905649487, 202328.6956724103], 
processed observation next is [0.0, 0.9565217391304348, 0.7156398104265403, 0.71, 1.0, 1.0, 0.5282385294860258, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23618283071248575, 0.23618283071248575, 0.3019831278692691], 
reward next is 0.6980, 
noisyNet noise sample is [array([-0.46905777], dtype=float32), -1.8068533]. 
=============================================
[2019-03-27 11:38:16,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5429565e-10 7.9002404e-01 1.4199483e-13 2.0997596e-01 3.4204292e-15], sum to 1.0000
[2019-03-27 11:38:16,857] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9520
[2019-03-27 11:38:16,862] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.884951540733848, 6.9112, 168.9076217917763, 2975106.189480648, 2284315.150242633, 473847.3538919006], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4021800.0000, 
sim time next is 4022400.0000, 
raw observation next is [34.0, 60.00000000000001, 1.0, 2.0, 0.9959977600258184, 1.0, 1.0, 0.9959977600258184, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2786099.2039846, 2786099.2039846, 526575.0690368488], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.6000000000000001, 1.0, 1.0, 0.995178024127492, 1.0, 0.5, 0.995178024127492, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7739164455512778, 0.7739164455512778, 0.7859329388609684], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.31373134], dtype=float32), 1.4998116]. 
=============================================
[2019-03-27 11:38:28,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.3505885e-18 1.0000000e+00 6.9044842e-22 3.0326926e-12 2.2865099e-22], sum to 1.0000
[2019-03-27 11:38:28,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1600
[2019-03-27 11:38:28,085] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 82.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.916186142425426, 6.9112, 168.9128263400949, 1457294.688481161, 1453757.350160787, 311357.4961964511], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4173600.0000, 
sim time next is 4174200.0000, 
raw observation next is [31.0, 81.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.263297578968324, 6.9112, 168.9109766956921, 1703713.217283246, 1453926.004269188, 311358.1073930356], 
processed observation next is [1.0, 0.30434782608695654, 0.6682464454976303, 0.815, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.03520975789683236, 0.0, 0.8294302233555716, 0.47325367146756836, 0.4038683345192189, 0.46471359312393373], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6806873], dtype=float32), 0.67289424]. 
=============================================
[2019-03-27 11:38:31,986] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 11:38:31,988] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:38:31,989] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:38:31,990] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:38:31,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:38:31,992] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:38:31,991] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:38:31,995] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:38:31,994] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:38:31,999] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:38:32,000] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:38:32,023] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run14
[2019-03-27 11:38:32,041] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run14
[2019-03-27 11:38:32,064] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run14
[2019-03-27 11:38:32,064] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run14
[2019-03-27 11:38:32,081] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run14
[2019-03-27 11:38:56,773] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04790888], dtype=float32), 0.034157336]
[2019-03-27 11:38:56,774] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.76666666666667, 51.0, 1.0, 2.0, 0.3213292036642718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 507808.24315071, 507808.2431507093, 167607.9191171762]
[2019-03-27 11:38:56,776] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:38:56,779] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0383651e-22 1.0000000e+00 4.1712513e-28 2.0223614e-22 4.6643144e-28], sampled 0.2221511787635212
[2019-03-27 11:39:06,066] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04790888], dtype=float32), 0.034157336]
[2019-03-27 11:39:06,068] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.390483685, 90.01024492666667, 1.0, 2.0, 0.4550271730488295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656363.4086507313, 656363.4086507313, 179476.601317705]
[2019-03-27 11:39:06,070] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:39:06,075] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5930022e-22 1.0000000e+00 1.2535510e-27 5.8625129e-21 1.6797291e-27], sampled 0.06072963976955459
[2019-03-27 11:39:37,244] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04790888], dtype=float32), 0.034157336]
[2019-03-27 11:39:37,245] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.9, 53.66666666666667, 1.0, 2.0, 0.5755636102660308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804300.6964990197, 804300.6964990197, 196282.3628921823]
[2019-03-27 11:39:37,246] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:39:37,247] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5765187e-20 1.0000000e+00 9.2419166e-26 5.3855482e-16 4.2256283e-26], sampled 0.12930285513791395
[2019-03-27 11:39:48,085] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04790888], dtype=float32), 0.034157336]
[2019-03-27 11:39:48,087] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.3, 47.0, 1.0, 2.0, 0.5719140892007488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799198.8857131194, 799198.8857131194, 195630.2933865023]
[2019-03-27 11:39:48,088] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:39:48,091] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.6468949e-22 1.0000000e+00 3.4464620e-27 7.4854722e-19 1.7398988e-27], sampled 0.15989234427666543
[2019-03-27 11:40:01,728] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04790888], dtype=float32), 0.034157336]
[2019-03-27 11:40:01,730] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.65502773666667, 93.03140682666668, 1.0, 2.0, 0.4476241825265895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651726.6377295029, 651726.6377295034, 179140.178493665]
[2019-03-27 11:40:01,730] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:40:01,733] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0086758e-22 1.0000000e+00 6.3917017e-28 9.1894839e-22 8.5622156e-28], sampled 0.7408668014300046
[2019-03-27 11:40:15,144] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04790888], dtype=float32), 0.034157336]
[2019-03-27 11:40:15,147] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.0, 90.0, 1.0, 2.0, 0.5880944010617803, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9919052982526226, 6.911199999999999, 6.9112, 168.9129565020424, 1644272.71017428, 1644272.710174281, 353575.6686259696]
[2019-03-27 11:40:15,148] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:40:15,151] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0630725e-17 1.0000000e+00 3.4520101e-22 5.4164687e-14 3.3348064e-22], sampled 0.3391383194512332
[2019-03-27 11:40:27,326] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04790888], dtype=float32), 0.034157336]
[2019-03-27 11:40:27,327] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 87.5, 1.0, 2.0, 0.5667284434632494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791949.7072414268, 791949.7072414268, 194708.3262203799]
[2019-03-27 11:40:27,328] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:40:27,330] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1069084e-21 1.0000000e+00 5.5377034e-27 1.7732607e-20 5.7828893e-27], sampled 0.6214496148089051
[2019-03-27 11:40:38,167] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04790888], dtype=float32), 0.034157336]
[2019-03-27 11:40:38,169] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.1, 91.0, 1.0, 2.0, 0.6892488017322556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 963238.0245145586, 963238.0245145586, 218540.254209167]
[2019-03-27 11:40:38,170] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:40:38,172] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.46760639e-21 1.00000000e+00 1.28395455e-26 8.31202908e-20
 1.48139542e-26], sampled 0.9002851329337844
[2019-03-27 11:40:38,780] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6185 2779263580.8788 933.0000
[2019-03-27 11:40:39,251] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.8061 3164166617.5528 1776.0000
[2019-03-27 11:40:39,327] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2899 2927407109.7953 1338.0000
[2019-03-27 11:40:39,635] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 11:40:39,796] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 11:40:40,815] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 325000, evaluation results [325000.0, 7883.806141962522, 3164166617.552804, 1776.0, 8254.289933546808, 2927407109.795325, 1338.0, 8660.61847634223, 2779263580.878825, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 11:40:41,286] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.7010990e-16 1.0000000e+00 2.4657257e-20 5.7021155e-11 4.2759716e-21], sum to 1.0000
[2019-03-27 11:40:41,293] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6356
[2019-03-27 11:40:41,299] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5367848488544159, 0.0, 1.0, 0.0, 1.0, 2.0, 0.929934010559525, 6.911199999999999, 6.9112, 168.912956413896, 1500713.207155931, 1500713.207155932, 328363.513472539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4245600.0000, 
sim time next is 4246200.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.112147662151455, 6.9112, 168.9117142504512, 1596410.698479155, 1453852.55930694, 311352.5950436217], 
processed observation next is [1.0, 0.13043478260869565, 0.6208530805687204, 0.75, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.020094766215145478, 0.0, 0.8294338450870928, 0.44344741624420975, 0.4038479331408167, 0.46470536573674887], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6243012], dtype=float32), 0.081244595]. 
=============================================
[2019-03-27 11:40:41,538] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.8407655e-18 1.0000000e+00 9.6020301e-22 3.8527341e-12 3.5908921e-22], sum to 1.0000
[2019-03-27 11:40:41,548] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7786
[2019-03-27 11:40:41,554] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 79.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.102012236591952, 6.9112, 168.9119325230154, 1589215.580946072, 1453847.633692573, 311353.500307062], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4257600.0000, 
sim time next is 4258200.0000, 
raw observation next is [29.83333333333333, 79.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.503776304258633, 6.9112, 168.9094305431825, 1874428.121968299, 1454042.87232432, 311353.9738187337], 
processed observation next is [1.0, 0.2608695652173913, 0.6129541864139019, 0.79, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.059257630425863274, 0.0, 0.8294226310389178, 0.5206744783245275, 0.40390079786786665, 0.4647074236100503], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.27564743], dtype=float32), -0.1313643]. 
=============================================
[2019-03-27 11:40:47,193] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.79364086e-20 1.00000000e+00 1.43834094e-24 1.01856804e-16
 1.97371125e-24], sum to 1.0000
[2019-03-27 11:40:47,210] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5053
[2019-03-27 11:40:47,217] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6189059455683855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864892.5462591934, 864892.5462591934, 204321.1676632182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4324200.0000, 
sim time next is 4324800.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6196018390965616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 865865.4218062824, 865865.421806283, 204454.7346157438], 
processed observation next is [1.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5416889627669417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24051817272396733, 0.24051817272396747, 0.3051563203220057], 
reward next is 0.6948, 
noisyNet noise sample is [array([0.64687157], dtype=float32), -0.69867915]. 
=============================================
[2019-03-27 11:40:50,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.51942458e-22 1.00000000e+00 1.24781155e-27 1.02044884e-19
 8.02509874e-28], sum to 1.0000
[2019-03-27 11:40:50,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2969
[2019-03-27 11:40:50,907] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 65.0, 1.0, 2.0, 0.5550751307338234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775659.3526796368, 775659.3526796368, 192676.106123211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4386600.0000, 
sim time next is 4387200.0000, 
raw observation next is [32.0, 64.33333333333334, 1.0, 2.0, 0.5525215388099634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772089.6785517579, 772089.6785517579, 192235.1490814873], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.6433333333333334, 1.0, 1.0, 0.4608693238674257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2144693551532661, 0.2144693551532661, 0.28691813295744373], 
reward next is 0.7131, 
noisyNet noise sample is [array([1.3056582], dtype=float32), 1.2936242]. 
=============================================
[2019-03-27 11:40:56,238] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3329282e-26 1.0000000e+00 6.1701492e-34 2.3044328e-31 1.0596225e-34], sum to 1.0000
[2019-03-27 11:40:56,247] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7702
[2019-03-27 11:40:56,254] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.606296123057291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 847263.8716847828, 847263.8716847834, 201925.1362408009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4467600.0000, 
sim time next is 4468200.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.6465546392236101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903546.7570592399, 903546.7570592399, 209737.4679518104], 
processed observation next is [0.0, 0.7391304347826086, 0.6998420221169038, 0.7233333333333334, 1.0, 1.0, 0.5741622159320604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2509852102942333, 0.2509852102942333, 0.3130409969430006], 
reward next is 0.6870, 
noisyNet noise sample is [array([0.51695853], dtype=float32), 1.2197686]. 
=============================================
[2019-03-27 11:40:58,359] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.2509129e-26 1.0000000e+00 5.9578877e-33 1.7888859e-29 5.1250431e-32], sum to 1.0000
[2019-03-27 11:40:58,368] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4048
[2019-03-27 11:40:58,377] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.508249836847253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710204.0348551711, 710204.0348551705, 184898.2290594076], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4496400.0000, 
sim time next is 4497000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5083310982753803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710317.6236566935, 710317.6236566928, 184911.1557499463], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40762782924744606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1973104510157482, 0.197310451015748, 0.2759867996267855], 
reward next is 0.7240, 
noisyNet noise sample is [array([-1.3688439], dtype=float32), 0.0286526]. 
=============================================
[2019-03-27 11:40:58,392] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[72.27458 ]
 [72.36615 ]
 [72.342224]
 [72.23323 ]
 [72.48537 ]], R is [[72.247612  ]
 [72.2491684 ]
 [72.25056458]
 [72.2519455 ]
 [72.25352478]].
[2019-03-27 11:40:59,099] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.65280608e-25 1.00000000e+00 1.93007959e-32 1.09753711e-27
 1.18159054e-32], sum to 1.0000
[2019-03-27 11:40:59,107] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5348
[2019-03-27 11:40:59,110] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5105756537851628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713455.1117163413, 713455.111716342, 185269.0972298737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4492200.0000, 
sim time next is 4492800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5087838850569756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710950.5380309998, 710950.5380310004, 184983.2191573847], 
processed observation next is [0.0, 0.0, 0.4312796208530806, 0.89, 1.0, 1.0, 0.408173355490332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19748626056416663, 0.1974862605641668, 0.27609435695132045], 
reward next is 0.7239, 
noisyNet noise sample is [array([-0.44609147], dtype=float32), 0.95860124]. 
=============================================
[2019-03-27 11:40:59,944] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7983551e-27 1.0000000e+00 5.4247419e-32 5.4062377e-30 1.8710113e-32], sum to 1.0000
[2019-03-27 11:40:59,957] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7638
[2019-03-27 11:40:59,966] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 86.5, 1.0, 2.0, 0.4995824983773147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 698088.7306454465, 698088.730645447, 183529.8919037986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4505400.0000, 
sim time next is 4506000.0000, 
raw observation next is [26.0, 85.66666666666667, 1.0, 2.0, 0.4953754810496339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692208.163827431, 692208.163827431, 182873.8577909603], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.8566666666666667, 1.0, 1.0, 0.3920186518670288, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19228004550761973, 0.19228004550761973, 0.2729460564044183], 
reward next is 0.7271, 
noisyNet noise sample is [array([0.96926713], dtype=float32), -0.52792954]. 
=============================================
[2019-03-27 11:40:59,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[68.36363 ]
 [68.37633 ]
 [68.52028 ]
 [68.600266]
 [68.672   ]], R is [[68.28146362]
 [68.32472229]
 [68.36671448]
 [68.40766907]
 [68.44778442]].
[2019-03-27 11:41:00,466] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9319408e-27 1.0000000e+00 3.4623169e-33 7.8768649e-31 7.6071265e-33], sum to 1.0000
[2019-03-27 11:41:00,478] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9959
[2019-03-27 11:41:00,484] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5053018078412719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706083.230902521, 706083.2309025218, 184430.5922021657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4512600.0000, 
sim time next is 4513200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5056966229924541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706635.1091571717, 706635.1091571723, 184493.0712654031], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40445376264151095, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1962875303214366, 0.19628753032143675, 0.27536279293343746], 
reward next is 0.7246, 
noisyNet noise sample is [array([0.14339457], dtype=float32), -0.7411304]. 
=============================================
[2019-03-27 11:41:10,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1430410e-23 1.0000000e+00 2.1513224e-30 1.8371184e-26 1.0779548e-30], sum to 1.0000
[2019-03-27 11:41:10,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3991
[2019-03-27 11:41:10,855] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 74.0, 1.0, 2.0, 0.4745648029814758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669080.7530330656, 669080.753033065, 180474.7162654271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4651800.0000, 
sim time next is 4652400.0000, 
raw observation next is [26.66666666666667, 78.0, 1.0, 2.0, 0.4718766926789524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665992.9614055092, 665992.9614055092, 180161.2066178872], 
processed observation next is [1.0, 0.8695652173913043, 0.4628751974723541, 0.78, 1.0, 1.0, 0.3637068586493402, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18499804483486365, 0.18499804483486365, 0.2688973233102794], 
reward next is 0.7311, 
noisyNet noise sample is [array([-0.08405694], dtype=float32), 0.23931322]. 
=============================================
[2019-03-27 11:41:19,094] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0837597e-20 1.0000000e+00 7.0429323e-26 3.9487050e-20 1.6992911e-25], sum to 1.0000
[2019-03-27 11:41:19,103] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9500
[2019-03-27 11:41:19,108] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.8709066045831717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217253.692507028, 1217253.692507028, 262062.3140359618], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4780800.0000, 
sim time next is 4781400.0000, 
raw observation next is [30.0, 74.16666666666667, 1.0, 2.0, 1.03242672510641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1443161.494803355, 1443161.494803355, 308963.4938216915], 
processed observation next is [1.0, 0.34782608695652173, 0.6208530805687204, 0.7416666666666667, 1.0, 1.0, 1.039068343501699, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.40087819300093197, 0.40087819300093197, 0.46113954301744997], 
reward next is 0.5389, 
noisyNet noise sample is [array([0.27568918], dtype=float32), -1.6088362]. 
=============================================
[2019-03-27 11:41:21,695] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5655419e-20 1.0000000e+00 1.2047296e-25 5.0413081e-20 6.3997105e-26], sum to 1.0000
[2019-03-27 11:41:21,707] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0442
[2019-03-27 11:41:21,716] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 70.66666666666667, 1.0, 2.0, 0.4989461563590215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697199.2499855738, 697199.2499855738, 183431.1141320092], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4821000.0000, 
sim time next is 4821600.0000, 
raw observation next is [28.66666666666667, 71.33333333333334, 1.0, 2.0, 0.4976041737908742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695323.4254254154, 695323.4254254154, 183221.5086271375], 
processed observation next is [1.0, 0.8260869565217391, 0.5576619273301741, 0.7133333333333334, 1.0, 1.0, 0.3947038238444267, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1931453959515043, 0.1931453959515043, 0.27346493824945894], 
reward next is 0.7265, 
noisyNet noise sample is [array([-0.5184735], dtype=float32), 0.852125]. 
=============================================
[2019-03-27 11:41:40,012] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7761266e-25 1.0000000e+00 8.5093140e-33 3.0753977e-26 4.8449322e-32], sum to 1.0000
[2019-03-27 11:41:40,022] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5829
[2019-03-27 11:41:40,030] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5223575221048355, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729924.2131311878, 729924.2131311885, 187172.8993368681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5086800.0000, 
sim time next is 5087400.0000, 
raw observation next is [27.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5229393191414531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730737.4756872482, 730737.4756872487, 187267.831458377], 
processed observation next is [0.0, 0.9130434782608695, 0.5181674565560824, 0.7983333333333335, 1.0, 1.0, 0.42522809535114825, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2029826321353467, 0.20298263213534687, 0.27950422605727915], 
reward next is 0.7205, 
noisyNet noise sample is [array([-0.34544143], dtype=float32), 1.1947229]. 
=============================================
[2019-03-27 11:41:42,792] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6862223e-24 1.0000000e+00 1.5824643e-32 1.4231233e-26 1.2946831e-31], sum to 1.0000
[2019-03-27 11:41:42,806] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0404
[2019-03-27 11:41:42,816] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5183896779389829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724377.795350636, 724377.7953506367, 186526.8721398202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5098200.0000, 
sim time next is 5098800.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5162498980852747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721386.7339989959, 721386.7339989964, 186180.6622535018], 
processed observation next is [0.0, 0.0, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.41716855190996943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20038520388860995, 0.20038520388861011, 0.27788158545298774], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.4159229], dtype=float32), -0.14348502]. 
=============================================
[2019-03-27 11:41:42,897] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0569709e-25 1.0000000e+00 1.6977826e-31 5.0624914e-26 6.8823901e-31], sum to 1.0000
[2019-03-27 11:41:42,907] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2219
[2019-03-27 11:41:42,912] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 84.0, 1.0, 2.0, 0.4923487007868947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687977.3519527548, 687977.3519527555, 182405.8863755389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5121000.0000, 
sim time next is 5121600.0000, 
raw observation next is [26.66666666666667, 84.0, 1.0, 2.0, 0.4974532312340174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695112.4379774907, 695112.4379774913, 183198.0284346595], 
processed observation next is [0.0, 0.2608695652173913, 0.4628751974723541, 0.84, 1.0, 1.0, 0.39452196534218964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19308678832708076, 0.19308678832708093, 0.273429893186059], 
reward next is 0.7266, 
noisyNet noise sample is [array([1.1782788], dtype=float32), -1.1047804]. 
=============================================
[2019-03-27 11:41:45,441] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1671445e-27 1.0000000e+00 4.7092669e-33 9.9818784e-27 4.9821647e-32], sum to 1.0000
[2019-03-27 11:41:45,453] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9265
[2019-03-27 11:41:45,465] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5510132275605634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769981.2111797953, 769981.2111797959, 191975.1303179837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5150400.0000, 
sim time next is 5151000.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5510141368343565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769982.482251813, 769982.4822518125, 191975.2864645808], 
processed observation next is [0.0, 0.6086956521739131, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45905317690886327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21388402284772584, 0.21388402284772567, 0.2865302783053445], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.6502051], dtype=float32), 1.6216923]. 
=============================================
[2019-03-27 11:41:45,484] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.91587 ]
 [76.85138 ]
 [76.78752 ]
 [76.72089 ]
 [76.679825]], R is [[76.91397095]
 [76.85829926]
 [76.80319214]
 [76.74824524]
 [76.69225311]].
[2019-03-27 11:41:46,646] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 11:41:46,648] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:41:46,649] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:41:46,650] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:41:46,652] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:41:46,653] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:41:46,655] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:41:46,652] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:41:46,656] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:41:46,660] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:41:46,659] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:41:46,679] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run15
[2019-03-27 11:41:46,697] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run15
[2019-03-27 11:41:46,713] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run15
[2019-03-27 11:41:46,715] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run15
[2019-03-27 11:41:46,754] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run15
[2019-03-27 11:41:55,624] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04536258], dtype=float32), 0.034200232]
[2019-03-27 11:41:55,625] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.85, 53.16666666666667, 1.0, 2.0, 0.5712048465812091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 937928.0739887205, 937928.0739887205, 209564.4470087265]
[2019-03-27 11:41:55,625] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:41:55,627] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6992945e-21 1.0000000e+00 2.9209825e-27 7.9069500e-21 7.5843175e-27], sampled 0.44718748309265277
[2019-03-27 11:42:05,884] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04536258], dtype=float32), 0.034200232]
[2019-03-27 11:42:05,885] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.62556146, 100.0, 1.0, 2.0, 0.3418025349504175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 525703.501042766, 525703.5010427654, 168628.2135587927]
[2019-03-27 11:42:05,885] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:42:05,887] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3899095e-23 1.0000000e+00 1.0885962e-29 1.0864624e-24 3.5555896e-29], sampled 0.7235208704499452
[2019-03-27 11:42:52,843] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04536258], dtype=float32), 0.034200232]
[2019-03-27 11:42:52,844] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.01194975804361, 6.9112, 170.5573041426782, 2981584.970544071, 2909413.820619302, 553121.1994788252]
[2019-03-27 11:42:52,845] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:42:52,848] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1754519e-13 9.9998248e-01 1.5933337e-17 1.7497483e-05 1.9997226e-18], sampled 0.8340124394296617
[2019-03-27 11:42:52,850] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2981584.970544071 W.
[2019-03-27 11:43:45,592] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04536258], dtype=float32), 0.034200232]
[2019-03-27 11:43:45,593] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.08333333333334, 81.0, 1.0, 2.0, 0.7775945382702638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1086766.01054644, 1086766.01054644, 238487.8681326684]
[2019-03-27 11:43:45,594] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:43:45,597] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.642351e-21 1.000000e+00 3.199065e-27 2.985999e-20 7.199338e-27], sampled 0.2789958795269868
[2019-03-27 11:43:53,377] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04536258], dtype=float32), 0.034200232]
[2019-03-27 11:43:53,378] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.615596515, 67.51214253, 1.0, 2.0, 0.4074739167265287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593325.7354212253, 593325.7354212247, 173467.6887538533]
[2019-03-27 11:43:53,378] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:43:53,381] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1301626e-20 1.0000000e+00 1.3915149e-26 1.6114980e-17 1.6830075e-26], sampled 0.4765560289031877
[2019-03-27 11:43:53,667] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4454 2779139371.5791 933.0000
[2019-03-27 11:43:54,079] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-03-27 11:43:54,452] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-03-27 11:43:54,486] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 11:43:54,662] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6655 3164065405.3223 1778.0000
[2019-03-27 11:43:55,679] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 350000, evaluation results [350000.0, 7882.665471649815, 3164065405.322316, 1778.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8661.445426124437, 2779139371.579139, 933.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 11:43:59,683] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3601266e-13 9.9991226e-01 1.2799553e-17 8.7708671e-05 3.0638877e-18], sum to 1.0000
[2019-03-27 11:43:59,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0040
[2019-03-27 11:43:59,706] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2178682.591694349 W.
[2019-03-27 11:43:59,717] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.7790443545282706, 1.0, 2.0, 0.7790443545282706, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2178682.591694349, 2178682.591694349, 409804.2279282073], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5229600.0000, 
sim time next is 5230200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.9220601256589935, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005989109479366, 6.9112, 168.9123931538525, 2185915.022528653, 2118668.590154909, 440235.4908649183], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.9060965369385464, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00947891094793656, 0.0, 0.8294371788135965, 0.6071986173690703, 0.5885190528208081, 0.6570678968133109], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8912296], dtype=float32), 0.663649]. 
=============================================
[2019-03-27 11:44:01,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6133090e-19 1.0000000e+00 1.5470864e-25 2.5161476e-16 2.4566593e-25], sum to 1.0000
[2019-03-27 11:44:01,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7118
[2019-03-27 11:44:01,684] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 83.0, 1.0, 2.0, 0.5541845953824116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 774414.4691675132, 774414.4691675126, 192521.3302356667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5266800.0000, 
sim time next is 5267400.0000, 
raw observation next is [28.5, 83.16666666666667, 1.0, 2.0, 0.555245578425055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775897.6224667907, 775897.6224667907, 192704.7612388712], 
processed observation next is [1.0, 1.0, 0.5497630331753555, 0.8316666666666667, 1.0, 1.0, 0.4641512993072951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21552711735188632, 0.21552711735188632, 0.2876190466251809], 
reward next is 0.7124, 
noisyNet noise sample is [array([1.0353099], dtype=float32), 1.0163864]. 
=============================================
[2019-03-27 11:44:02,789] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8855908e-19 1.0000000e+00 2.5260525e-24 1.5736109e-15 4.4197672e-24], sum to 1.0000
[2019-03-27 11:44:02,799] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7051
[2019-03-27 11:44:02,804] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.96666666666667, 86.66666666666667, 1.0, 2.0, 0.9336353031729447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1304982.578765579, 1304982.57876558, 279361.8553803675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5293200.0000, 
sim time next is 5293800.0000, 
raw observation next is [29.15, 86.0, 1.0, 2.0, 0.9083826145249925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1269664.694248074, 1269664.694248074, 272258.5698155818], 
processed observation next is [1.0, 0.2608695652173913, 0.5805687203791469, 0.86, 1.0, 1.0, 0.8896176078614367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35268463729113164, 0.35268463729113164, 0.4063560743516146], 
reward next is 0.5936, 
noisyNet noise sample is [array([0.80240166], dtype=float32), -0.16855045]. 
=============================================
[2019-03-27 11:44:08,462] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8849834e-22 1.0000000e+00 2.1613272e-28 4.4540117e-22 2.6394704e-28], sum to 1.0000
[2019-03-27 11:44:08,473] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6509
[2019-03-27 11:44:08,480] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.6, 80.33333333333334, 1.0, 2.0, 0.6182201795144697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863933.8300315007, 863933.8300315007, 204189.7184561313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5348400.0000, 
sim time next is 5349000.0000, 
raw observation next is [30.5, 80.66666666666666, 1.0, 2.0, 0.6160710798660874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 860929.3464296048, 860929.3464296055, 203778.4519992665], 
processed observation next is [1.0, 0.9130434782608695, 0.6445497630331753, 0.8066666666666665, 1.0, 1.0, 0.5374350359832378, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23914704067489023, 0.23914704067489043, 0.3041469432824873], 
reward next is 0.6959, 
noisyNet noise sample is [array([-1.0318677], dtype=float32), -0.83313096]. 
=============================================
[2019-03-27 11:44:08,498] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[62.824635]
 [62.68766 ]
 [62.812798]
 [62.82814 ]
 [63.06767 ]], R is [[62.88014984]
 [62.94659042]
 [63.01193619]
 [63.07605743]
 [63.13851929]].
[2019-03-27 11:44:25,585] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7483347e-20 1.0000000e+00 7.8843027e-28 1.8630929e-23 5.2820582e-27], sum to 1.0000
[2019-03-27 11:44:25,595] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1793
[2019-03-27 11:44:25,602] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 91.5, 1.0, 2.0, 0.5222979293180688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729840.9116384251, 729840.9116384251, 187163.156377542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5617800.0000, 
sim time next is 5618400.0000, 
raw observation next is [26.1, 91.66666666666666, 1.0, 2.0, 0.522220160378864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729732.2027074888, 729732.2027074888, 187150.3899612127], 
processed observation next is [0.0, 0.0, 0.4360189573459717, 0.9166666666666665, 1.0, 1.0, 0.42436163901067947, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2027033896409691, 0.2027033896409691, 0.27932894024061594], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.15303281], dtype=float32), 1.1887293]. 
=============================================
[2019-03-27 11:44:26,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.43723369e-25 1.00000000e+00 1.66465234e-31 1.28323965e-26
 3.73188353e-31], sum to 1.0000
[2019-03-27 11:44:26,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0130
[2019-03-27 11:44:26,165] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 89.0, 1.0, 2.0, 0.5598397672745595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782319.8850593299, 782319.8850593299, 193502.5209360598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5605200.0000, 
sim time next is 5605800.0000, 
raw observation next is [27.51666666666667, 89.16666666666667, 1.0, 2.0, 0.5582437807077114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780088.8350558716, 780088.8350558716, 193224.4921415917], 
processed observation next is [1.0, 0.9130434782608695, 0.5031595576619274, 0.8916666666666667, 1.0, 1.0, 0.4677635912141101, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21669134307107546, 0.21669134307107546, 0.2883947643904354], 
reward next is 0.7116, 
noisyNet noise sample is [array([0.7487714], dtype=float32), 0.9234145]. 
=============================================
[2019-03-27 11:44:41,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3452393e-10 7.5744164e-01 9.2339365e-15 2.4255832e-01 1.9687055e-15], sum to 1.0000
[2019-03-27 11:44:41,678] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9938
[2019-03-27 11:44:41,689] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3402641.83357515 W.
[2019-03-27 11:44:41,697] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.75, 62.0, 1.0, 2.0, 0.9802928223494027, 1.0, 1.0, 0.8107364506889637, 1.0, 2.0, 1.03, 7.005119843399891, 6.9112, 170.5573041426782, 3402641.83357515, 3335363.229332841, 624493.071077176], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5837400.0000, 
sim time next is 5838000.0000, 
raw observation next is [32.8, 61.66666666666667, 1.0, 2.0, 0.7929503564766356, 1.0, 2.0, 0.7170652177525805, 1.0, 2.0, 1.03, 7.005105062350043, 6.9112, 170.5573041426782, 3009032.235676689, 2941764.219701641, 552293.6330471358], 
processed observation next is [1.0, 0.5652173913043478, 0.7535545023696681, 0.6166666666666667, 1.0, 1.0, 0.7505425981646212, 1.0, 1.0, 0.6591147201838319, 1.0, 1.0, 1.0365853658536586, 0.009390506235004282, 0.0, 0.8375144448122397, 0.8358422876879692, 0.8171567276949002, 0.8243188552942325], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.15312289], dtype=float32), 0.5788796]. 
=============================================
[2019-03-27 11:44:41,712] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[39.04647 ]
 [42.623863]
 [45.938362]
 [47.92403 ]
 [47.013996]], R is [[37.99181366]
 [37.61189651]
 [37.23577881]
 [37.20279694]
 [36.83076859]].
[2019-03-27 11:44:43,251] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.8806772e-19 1.0000000e+00 6.8999645e-27 1.3718677e-16 6.8012654e-26], sum to 1.0000
[2019-03-27 11:44:43,259] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7430
[2019-03-27 11:44:43,265] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333333, 87.0, 1.0, 2.0, 0.5593480216139561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781632.4670210007, 781632.4670210007, 193416.6709352184], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5865000.0000, 
sim time next is 5865600.0000, 
raw observation next is [27.76666666666667, 87.0, 1.0, 2.0, 0.5567330500304989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777976.9696666499, 777976.9696666499, 192961.9888492085], 
processed observation next is [1.0, 0.9130434782608695, 0.515007898894155, 0.87, 1.0, 1.0, 0.4659434337716854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21610471379629165, 0.21610471379629165, 0.28800296843165446], 
reward next is 0.7120, 
noisyNet noise sample is [array([-0.4558211], dtype=float32), 0.9993993]. 
=============================================
[2019-03-27 11:44:44,519] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7113715e-17 1.0000000e+00 6.2084085e-23 2.5612711e-12 6.4954946e-23], sum to 1.0000
[2019-03-27 11:44:44,531] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3606
[2019-03-27 11:44:44,537] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.98333333333333, 94.16666666666667, 1.0, 2.0, 1.031517588784715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1441889.809772644, 1441889.809772644, 308675.2991374247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5883000.0000, 
sim time next is 5883600.0000, 
raw observation next is [25.96666666666667, 94.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.94598345494575, 6.9112, 168.9124660732522, 1478448.336140446, 1453771.827696021, 311349.5976433922], 
processed observation next is [1.0, 0.08695652173913043, 0.42969984202211703, 0.9433333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0034783454945750414, 0.0, 0.8294375368812317, 0.41068009337234607, 0.40382550769333914, 0.46470089200506304], 
reward next is 0.3614, 
noisyNet noise sample is [array([0.42283678], dtype=float32), 0.56002957]. 
=============================================
[2019-03-27 11:44:46,816] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2409357e-11 9.9999380e-01 2.6899996e-16 6.1732831e-06 1.1104267e-17], sum to 1.0000
[2019-03-27 11:44:46,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0136
[2019-03-27 11:44:46,834] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 81.83333333333333, 1.0, 2.0, 0.9218570103631246, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129563643707, 1288509.557498151, 1288509.55749815, 276023.264586437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5903400.0000, 
sim time next is 5904000.0000, 
raw observation next is [29.2, 81.0, 1.0, 2.0, 1.01642992746305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565103945, 1420785.658088205, 1420785.658088206, 303974.6044444874], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.81, 1.0, 1.0, 1.0197950933289759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451521235, 0.3946626828022791, 0.39466268280227945, 0.45369343946938423], 
reward next is 0.5463, 
noisyNet noise sample is [array([0.2942342], dtype=float32), -0.22148055]. 
=============================================
[2019-03-27 11:44:46,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[45.24339 ]
 [44.64639 ]
 [45.42264 ]
 [46.430923]
 [47.422382]], R is [[45.76628494]
 [45.30862427]
 [45.38191605]
 [44.92809677]
 [44.47881699]].
[2019-03-27 11:44:53,196] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4830449e-13 1.0000000e+00 1.0847466e-19 5.7003051e-09 8.3592171e-20], sum to 1.0000
[2019-03-27 11:44:53,204] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0830
[2019-03-27 11:44:53,213] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 84.0, 1.0, 2.0, 0.5854757255287135, 1.0, 1.0, 0.5854757255287135, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1636933.2513944, 1636933.251394401, 328936.4747894946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6011400.0000, 
sim time next is 6012000.0000, 
raw observation next is [25.6, 86.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.910849667507865, 6.9112, 168.907387450318, 2163403.239212387, 1454240.741654191, 311346.2854943802], 
processed observation next is [1.0, 0.6086956521739131, 0.4123222748815167, 0.86, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.09996496675078648, 0.0, 0.8294125985176211, 0.600945344225663, 0.4039557615706086, 0.464695948499075], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1843224], dtype=float32), -0.46692762]. 
=============================================
[2019-03-27 11:44:53,237] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[58.283268]
 [59.779194]
 [60.440617]
 [62.377808]
 [63.213997]], R is [[57.98746872]
 [57.91664124]
 [57.80891037]
 [57.23082352]
 [56.65851593]].
[2019-03-27 11:44:55,935] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4084180e-12 9.9998963e-01 1.9832592e-18 1.0330957e-05 2.2703065e-19], sum to 1.0000
[2019-03-27 11:44:55,947] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4839
[2019-03-27 11:44:55,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2415610.429721225 W.
[2019-03-27 11:44:55,965] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.5757868554181883, 1.0, 2.0, 0.5757868554181883, 1.0, 1.0, 0.9999513522385849, 6.9112, 6.9112, 170.5573041426782, 2415610.429721225, 2415610.429721225, 471510.6471010518], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6019200.0000, 
sim time next is 6019800.0000, 
raw observation next is [31.83333333333334, 70.16666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.888664930941653, 6.9112, 168.907560357128, 2977742.693458932, 2284317.582814919, 473835.8871315635], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.7016666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0977464930941653, 0.0, 0.8294134475692153, 0.8271507481830367, 0.634532661893033, 0.7072177419874082], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6359158], dtype=float32), -0.40978444]. 
=============================================
[2019-03-27 11:45:01,540] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 11:45:01,542] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:45:01,543] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:45:01,544] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:45:01,544] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:45:01,545] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:45:01,546] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:45:01,546] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:45:01,546] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:45:01,548] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:45:01,550] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:45:01,569] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run16
[2019-03-27 11:45:01,588] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run16
[2019-03-27 11:45:01,611] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run16
[2019-03-27 11:45:01,631] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run16
[2019-03-27 11:45:01,648] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run16
[2019-03-27 11:45:02,762] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04811208], dtype=float32), 0.034151375]
[2019-03-27 11:45:02,764] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.24093896, 83.66429344166667, 1.0, 2.0, 0.267514662609828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 438344.1180979457, 438344.1180979457, 162683.5980934746]
[2019-03-27 11:45:02,765] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:45:02,767] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.8278057e-17 1.0000000e+00 4.7676779e-22 1.0125236e-18 2.1819207e-21], sampled 0.4130933339488193
[2019-03-27 11:46:25,128] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04811208], dtype=float32), 0.034151375]
[2019-03-27 11:46:25,130] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.8350389186313528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1167094.389156702, 1167094.389156702, 252690.9798658147]
[2019-03-27 11:46:25,130] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:46:25,133] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.0737901e-18 1.0000000e+00 5.3324773e-24 1.4741486e-15 1.3888283e-23], sampled 0.4811517657623271
[2019-03-27 11:46:35,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04811208], dtype=float32), 0.034151375]
[2019-03-27 11:46:35,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.03333333333333, 75.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.563639722727503, 6.9112, 168.9093027747769, 1916925.148613392, 1454071.966205813, 311355.5190016563]
[2019-03-27 11:46:35,544] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:46:35,545] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0411860e-16 1.0000000e+00 3.2487322e-22 2.6651795e-13 7.1275161e-22], sampled 0.9206974864599703
[2019-03-27 11:46:35,548] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1916925.148613392 W.
[2019-03-27 11:46:37,772] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04811208], dtype=float32), 0.034151375]
[2019-03-27 11:46:37,773] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.0, 89.66666666666667, 1.0, 2.0, 0.5205295522452805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727368.9971543167, 727368.9971543167, 186874.0218219753]
[2019-03-27 11:46:37,775] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:46:37,777] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4776798e-21 1.0000000e+00 3.2696840e-28 1.9355548e-20 1.5834889e-27], sampled 0.44958596127117456
[2019-03-27 11:46:40,739] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04811208], dtype=float32), 0.034151375]
[2019-03-27 11:46:40,741] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.63333333333333, 90.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 12.41096344995458, 6.9112, 175.1458619246602, 5501785.634462872, 1456090.23320191, 297641.6000166234]
[2019-03-27 11:46:40,742] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:46:40,744] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.5504371e-17 1.0000000e+00 1.1496637e-22 5.8638756e-14 2.5890286e-22], sampled 0.26182472058803086
[2019-03-27 11:46:40,745] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 5501785.634462872 W.
[2019-03-27 11:46:41,477] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04811208], dtype=float32), 0.034151375]
[2019-03-27 11:46:41,478] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.73333333333333, 90.0, 1.0, 2.0, 0.6436713855520295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 899515.7632904821, 899515.7632904828, 209156.4573357524]
[2019-03-27 11:46:41,478] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:46:41,482] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2015756e-19 1.0000000e+00 6.6096634e-26 2.6352382e-18 2.5193727e-25], sampled 0.9687174266789873
[2019-03-27 11:46:55,053] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04811208], dtype=float32), 0.034151375]
[2019-03-27 11:46:55,054] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.65, 90.16666666666667, 1.0, 2.0, 0.5392442900929557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753529.5644328541, 753529.5644328541, 189972.5722752487]
[2019-03-27 11:46:55,055] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:46:55,059] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1275868e-21 1.0000000e+00 2.4713280e-28 1.1357248e-20 1.3415130e-27], sampled 0.5849359076063264
[2019-03-27 11:47:04,824] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04811208], dtype=float32), 0.034151375]
[2019-03-27 11:47:04,827] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.72260275333333, 85.38060016666667, 1.0, 2.0, 0.5363858816193545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749533.8703456298, 749533.8703456298, 189491.3442110063]
[2019-03-27 11:47:04,828] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:47:04,831] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.419495e-21 1.000000e+00 8.129613e-28 7.102228e-20 3.639981e-27], sampled 0.5309999765877377
[2019-03-27 11:47:09,111] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2473 2779197326.4671 933.0000
[2019-03-27 11:47:09,193] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.4994 3162889983.2012 1757.0000
[2019-03-27 11:47:09,388] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1165 2927283041.2217 1338.0000
[2019-03-27 11:47:09,581] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5067 2842459250.3785 1129.0000
[2019-03-27 11:47:09,618] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.4541 3007739734.8195 1765.0000
[2019-03-27 11:47:10,635] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 375000, evaluation results [375000.0, 7890.499432072246, 3162889983.2012444, 1757.0, 8255.116518919473, 2927283041.2216587, 1338.0, 8659.247307079566, 2779197326.467062, 933.0, 7998.454101781307, 3007739734.8194776, 1765.0, 8497.506660276686, 2842459250.378501, 1129.0]
[2019-03-27 11:47:15,046] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7400639e-12 9.9859101e-01 3.1472276e-19 1.4090288e-03 1.0152635e-18], sum to 1.0000
[2019-03-27 11:47:15,058] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4532
[2019-03-27 11:47:15,068] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2322656.656817919 W.
[2019-03-27 11:47:15,078] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.9, 72.0, 1.0, 2.0, 0.5536508904480935, 1.0, 2.0, 0.5536508904480935, 1.0, 1.0, 0.9594762986171885, 6.911200000000001, 6.9112, 170.5573041426782, 2322656.656817919, 2322656.656817918, 453818.925794825], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6190200.0000, 
sim time next is 6190800.0000, 
raw observation next is [29.8, 72.33333333333333, 1.0, 2.0, 0.5273051708210434, 1.0, 2.0, 0.5273051708210434, 1.0, 2.0, 0.9135520302894988, 6.9112, 6.9112, 170.5573041426782, 2212034.12285254, 2212034.12285254, 434120.3088958006], 
processed observation next is [1.0, 0.6521739130434783, 0.6113744075829385, 0.7233333333333333, 1.0, 1.0, 0.43048815761571496, 1.0, 1.0, 0.43048815761571496, 1.0, 1.0, 0.8945756466945107, 0.0, 0.0, 0.8375144448122397, 0.6144539230145944, 0.6144539230145944, 0.6479407595459711], 
reward next is 0.3521, 
noisyNet noise sample is [array([-0.19907874], dtype=float32), -0.35868534]. 
=============================================
[2019-03-27 11:47:34,675] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.1152371e-22 1.0000000e+00 3.8037090e-28 1.6602415e-20 6.1211706e-28], sum to 1.0000
[2019-03-27 11:47:34,683] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0367
[2019-03-27 11:47:34,691] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 75.0, 1.0, 2.0, 0.5038158946292242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704006.2027721644, 704006.2027721644, 184196.109626496], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6464400.0000, 
sim time next is 6465000.0000, 
raw observation next is [28.11666666666667, 75.5, 1.0, 2.0, 0.5038047979698823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 703990.6917377937, 703990.691737793, 184194.2844900835], 
processed observation next is [1.0, 0.8260869565217391, 0.5315955766192735, 0.755, 1.0, 1.0, 0.40217445538540036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19555296992716492, 0.19555296992716473, 0.2749168425225127], 
reward next is 0.7251, 
noisyNet noise sample is [array([1.2391219], dtype=float32), -1.2656019]. 
=============================================
[2019-03-27 11:47:34,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[65.46853 ]
 [65.497856]
 [65.56774 ]
 [65.650444]
 [65.54928 ]], R is [[65.54733276]
 [65.61694336]
 [65.68569946]
 [65.753479  ]
 [65.82013702]].
[2019-03-27 11:47:36,227] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2182489e-20 1.0000000e+00 1.1289262e-27 2.8494139e-19 4.5233296e-27], sum to 1.0000
[2019-03-27 11:47:36,237] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3794
[2019-03-27 11:47:36,248] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.58333333333334, 73.5, 1.0, 2.0, 0.5075902849713679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709282.1008905068, 709282.1008905062, 184793.8456243891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6462600.0000, 
sim time next is 6463200.0000, 
raw observation next is [28.46666666666667, 74.0, 1.0, 2.0, 0.5057182101791148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706665.2840431288, 706665.2840431288, 184496.881369619], 
processed observation next is [1.0, 0.8260869565217391, 0.5481832543443919, 0.74, 1.0, 1.0, 0.40447977130013824, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19629591223420245, 0.19629591223420245, 0.27536847965614775], 
reward next is 0.7246, 
noisyNet noise sample is [array([-0.641866], dtype=float32), 0.3521624]. 
=============================================
[2019-03-27 11:47:46,516] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0960635e-21 1.0000000e+00 1.1887918e-26 2.2327541e-19 2.0203370e-26], sum to 1.0000
[2019-03-27 11:47:46,525] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4544
[2019-03-27 11:47:46,529] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333334, 85.83333333333334, 1.0, 2.0, 0.512221510739695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715755.7319229649, 715755.7319229649, 185533.1848453896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6641400.0000, 
sim time next is 6642000.0000, 
raw observation next is [26.8, 86.0, 1.0, 2.0, 0.5111532683365179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714262.5152825058, 714262.5152825058, 185362.1584143999], 
processed observation next is [1.0, 0.9130434782608695, 0.4691943127962086, 0.86, 1.0, 1.0, 0.4110280341403829, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1984062542451405, 0.1984062542451405, 0.27665993793194016], 
reward next is 0.7233, 
noisyNet noise sample is [array([1.053688], dtype=float32), 0.28188938]. 
=============================================
[2019-03-27 11:47:46,549] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[60.84914 ]
 [60.691723]
 [60.75931 ]
 [60.811283]
 [60.843678]], R is [[60.93536377]
 [61.04909515]
 [61.1614418 ]
 [61.27254105]
 [61.38247299]].
[2019-03-27 11:47:51,973] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0181529e-17 1.0000000e+00 9.2477142e-23 2.2395681e-13 8.4629725e-22], sum to 1.0000
[2019-03-27 11:47:51,981] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9295
[2019-03-27 11:47:51,989] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1730548.414733676 W.
[2019-03-27 11:47:51,995] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.86666666666667, 86.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.301098715168418, 6.9112, 168.910803126166, 1730548.414733676, 1453944.373242454, 311349.028635154], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6684000.0000, 
sim time next is 6684600.0000, 
raw observation next is [27.03333333333333, 85.0, 1.0, 2.0, 0.5787102127954293, 1.0, 1.0, 0.5787102127954293, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1618003.255834149, 1618003.255834149, 326517.406592723], 
processed observation next is [1.0, 0.34782608695652173, 0.48025276461295413, 0.85, 1.0, 1.0, 0.49242194312702325, 1.0, 0.5, 0.49242194312702325, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.44944534884281917, 0.44944534884281917, 0.4873394128249597], 
reward next is 0.5127, 
noisyNet noise sample is [array([4.1382627], dtype=float32), -0.5604512]. 
=============================================
[2019-03-27 11:47:52,851] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.3853164e-24 1.0000000e+00 7.4856300e-30 9.1736483e-23 1.1136211e-29], sum to 1.0000
[2019-03-27 11:47:52,858] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2349
[2019-03-27 11:47:52,861] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.61666666666667, 68.66666666666666, 1.0, 2.0, 0.3943057301243676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589150.8722051202, 589150.8722051202, 173563.4790899167], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6727800.0000, 
sim time next is 6728400.0000, 
raw observation next is [26.5, 69.0, 1.0, 2.0, 0.3923439752622888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587336.6333422939, 587336.6333422939, 173431.3393785041], 
processed observation next is [1.0, 0.9130434782608695, 0.4549763033175356, 0.69, 1.0, 1.0, 0.26788430754492626, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16314906481730387, 0.16314906481730387, 0.2588527453410509], 
reward next is 0.7411, 
noisyNet noise sample is [array([1.397728], dtype=float32), 1.3551328]. 
=============================================
[2019-03-27 11:48:06,382] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6628414e-21 1.0000000e+00 3.8306512e-26 1.4766960e-18 5.0191491e-26], sum to 1.0000
[2019-03-27 11:48:06,396] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9194
[2019-03-27 11:48:06,403] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 84.66666666666667, 1.0, 2.0, 0.4240072970887338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619883.8812531757, 619883.8812531751, 176057.0589635421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6916800.0000, 
sim time next is 6917400.0000, 
raw observation next is [24.75, 85.0, 1.0, 2.0, 0.4242072431295988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 620249.6057836277, 620249.6057836271, 176094.3472896515], 
processed observation next is [0.0, 0.043478260869565216, 0.3720379146919432, 0.85, 1.0, 1.0, 0.30627378690313106, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1722915571621188, 0.17229155716211864, 0.2628273840144052], 
reward next is 0.7372, 
noisyNet noise sample is [array([1.8248203], dtype=float32), -1.1945317]. 
=============================================
[2019-03-27 11:48:13,032] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.2798757e-20 1.0000000e+00 1.1081906e-25 3.6804929e-16 1.4170519e-24], sum to 1.0000
[2019-03-27 11:48:13,046] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9031
[2019-03-27 11:48:13,054] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666666, 79.33333333333333, 1.0, 2.0, 0.4525203593305189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646806.3773682071, 646806.3773682065, 178355.5033537819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7000800.0000, 
sim time next is 7001400.0000, 
raw observation next is [26.13333333333334, 79.66666666666667, 1.0, 2.0, 0.4528319381208636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646801.0828001656, 646801.0828001656, 178343.8153820888], 
processed observation next is [1.0, 0.0, 0.43759873617693557, 0.7966666666666667, 1.0, 1.0, 0.34076137122995614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17966696744449046, 0.17966696744449046, 0.26618479907774445], 
reward next is 0.7338, 
noisyNet noise sample is [array([-0.4830686], dtype=float32), -0.14072593]. 
=============================================
[2019-03-27 11:48:16,500] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 11:48:16,501] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:48:16,503] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:48:16,503] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:48:16,504] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:48:16,505] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:48:16,506] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:48:16,507] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:48:16,507] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:48:16,508] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:48:16,508] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:48:16,533] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run17
[2019-03-27 11:48:16,535] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run17
[2019-03-27 11:48:16,578] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run17
[2019-03-27 11:48:16,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run17
[2019-03-27 11:48:16,618] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run17
[2019-03-27 11:48:28,910] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04591579], dtype=float32), 0.036386386]
[2019-03-27 11:48:28,912] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [15.02609264333334, 89.80305093333334, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 267627.2695233424, 267627.2695233424, 110241.7083333356]
[2019-03-27 11:48:28,913] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:48:28,917] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8710801e-21 1.0000000e+00 1.6271737e-27 3.2173711e-21 7.5247215e-27], sampled 0.27288971663057426
[2019-03-27 11:48:40,041] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04591579], dtype=float32), 0.036386386]
[2019-03-27 11:48:40,041] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.7, 47.66666666666667, 1.0, 2.0, 0.2966395030968078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 475660.961676656, 475660.9616766553, 165350.1648805415]
[2019-03-27 11:48:40,042] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:48:40,043] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3654058e-21 1.0000000e+00 5.1758616e-28 1.1090645e-20 2.4633008e-27], sampled 0.7732402146455951
[2019-03-27 11:49:15,539] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04591579], dtype=float32), 0.036386386]
[2019-03-27 11:49:15,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.13333333333333, 51.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.031432568316177, 6.9112, 168.9118535684472, 1539109.908823316, 1453813.343734279, 311353.1167624866]
[2019-03-27 11:49:15,541] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:49:15,543] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.7827025e-15 9.9999952e-01 3.3479352e-21 4.9239929e-07 3.9214108e-21], sampled 0.10558518816209883
[2019-03-27 11:49:28,402] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04591579], dtype=float32), 0.036386386]
[2019-03-27 11:49:28,404] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.95, 49.5, 1.0, 2.0, 0.5818981312617413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813156.0337592819, 813156.0337592819, 197422.1577833838]
[2019-03-27 11:49:28,405] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:49:28,407] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9076299e-21 1.0000000e+00 7.1324991e-28 1.8174829e-18 3.2210208e-27], sampled 0.7928428208939815
[2019-03-27 11:49:33,286] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04591579], dtype=float32), 0.036386386]
[2019-03-27 11:49:33,288] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.030660085, 77.73733333833333, 1.0, 2.0, 0.6955611846822846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 972063.7344387763, 972063.7344387763, 219904.2906665061]
[2019-03-27 11:49:33,290] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:49:33,294] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.8979746e-18 1.0000000e+00 5.3426173e-24 3.2935127e-13 1.6521773e-23], sampled 0.12156367490834696
[2019-03-27 11:49:36,847] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04591579], dtype=float32), 0.036386386]
[2019-03-27 11:49:36,849] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.96666666666667, 55.33333333333334, 1.0, 2.0, 0.5348437087348216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747378.1136402882, 747378.1136402882, 189235.4624219502]
[2019-03-27 11:49:36,850] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:49:36,854] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2308507e-21 1.0000000e+00 6.2200320e-28 3.9213272e-19 3.3426305e-27], sampled 0.7822675339707655
[2019-03-27 11:50:01,058] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04591579], dtype=float32), 0.036386386]
[2019-03-27 11:50:01,059] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.87871423, 85.96236060999999, 1.0, 2.0, 0.5764077105633096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805480.701397891, 805480.701397891, 196433.8561203916]
[2019-03-27 11:50:01,060] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:50:01,067] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9787420e-19 1.0000000e+00 1.0072499e-25 9.4274809e-16 3.9628948e-25], sampled 0.06047047633001623
[2019-03-27 11:50:23,736] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8019.9219 3005387139.0012 1708.0000
[2019-03-27 11:50:24,308] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8513.7367 2840959418.7564 1088.0000
[2019-03-27 11:50:24,403] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8671.8436 2778280218.9823 907.0000
[2019-03-27 11:50:24,542] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7954.3105 3157595980.1145 1611.0000
[2019-03-27 11:50:24,645] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8266.0561 2925881067.0586 1302.0000
[2019-03-27 11:50:25,660] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 400000, evaluation results [400000.0, 7954.310459617444, 3157595980.114548, 1611.0, 8266.056076386432, 2925881067.0586443, 1302.0, 8671.843618867093, 2778280218.982254, 907.0, 8019.921906338169, 3005387139.001247, 1708.0, 8513.736686323018, 2840959418.7563877, 1088.0]
[2019-03-27 11:50:35,266] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3490843e-17 1.0000000e+00 3.9357917e-25 2.0282046e-12 3.0570137e-24], sum to 1.0000
[2019-03-27 11:50:35,279] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7159
[2019-03-27 11:50:35,284] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.71666666666667, 86.0, 1.0, 2.0, 0.473376425093435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661640.8689527683, 661640.8689527677, 179553.2254955652], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7171800.0000, 
sim time next is 7172400.0000, 
raw observation next is [25.73333333333333, 86.0, 1.0, 2.0, 0.4736779639699703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661879.9016697747, 661879.9016697753, 179574.5086698887], 
processed observation next is [1.0, 0.0, 0.41864139020537117, 0.86, 1.0, 1.0, 0.3658770650240606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1838555282416041, 0.18385552824160423, 0.2680216547311772], 
reward next is 0.7320, 
noisyNet noise sample is [array([-0.47382218], dtype=float32), 0.092238106]. 
=============================================
[2019-03-27 11:50:43,960] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7204962e-19 1.0000000e+00 1.2529977e-25 9.2168653e-15 5.1819740e-25], sum to 1.0000
[2019-03-27 11:50:43,970] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8053
[2019-03-27 11:50:43,977] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.23333333333333, 71.16666666666667, 1.0, 2.0, 0.8594900294942697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1323079.873162639, 1323079.873162639, 275564.7865311849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7294200.0000, 
sim time next is 7294800.0000, 
raw observation next is [25.46666666666667, 70.33333333333334, 1.0, 2.0, 0.8244360501719926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1265662.344818273, 1265662.344818273, 265158.5400091611], 
processed observation next is [1.0, 0.43478260869565216, 0.40600315955766203, 0.7033333333333335, 1.0, 1.0, 0.7884771688819188, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35157287356063144, 0.35157287356063144, 0.3957590149390464], 
reward next is 0.6042, 
noisyNet noise sample is [array([0.04432295], dtype=float32), 0.88844216]. 
=============================================
[2019-03-27 11:50:51,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3167061e-20 1.0000000e+00 3.4130472e-27 6.4032599e-16 5.9562955e-26], sum to 1.0000
[2019-03-27 11:50:51,231] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5877
[2019-03-27 11:50:51,236] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.68333333333333, 90.83333333333334, 1.0, 2.0, 0.563201918617114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 903653.6607290354, 903653.6607290361, 207022.7656497492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7398600.0000, 
sim time next is 7399200.0000, 
raw observation next is [20.66666666666667, 90.66666666666667, 1.0, 2.0, 0.5809534376644467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 932794.6921029142, 932794.6921029142, 210679.6656828513], 
processed observation next is [1.0, 0.6521739130434783, 0.17851500789889443, 0.9066666666666667, 1.0, 1.0, 0.49512462369210447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25910963669525394, 0.25910963669525394, 0.3144472622132109], 
reward next is 0.6856, 
noisyNet noise sample is [array([-1.0429115], dtype=float32), 0.24968117]. 
=============================================
[2019-03-27 11:50:52,970] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0681108e-25 1.0000000e+00 8.1460501e-32 4.4217379e-25 1.3754578e-30], sum to 1.0000
[2019-03-27 11:50:52,985] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9137
[2019-03-27 11:50:52,989] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.25, 94.5, 1.0, 2.0, 0.3226904336105356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506466.2105589872, 506466.2105589872, 167429.6081203992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7446600.0000, 
sim time next is 7447200.0000, 
raw observation next is [21.23333333333333, 94.66666666666666, 1.0, 2.0, 0.3226336790354542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506334.9479582717, 506334.9479582717, 167418.5494178582], 
processed observation next is [0.0, 0.17391304347826086, 0.2053712480252764, 0.9466666666666665, 1.0, 1.0, 0.18389599883789662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14064859665507548, 0.14064859665507548, 0.24987843196695253], 
reward next is 0.7501, 
noisyNet noise sample is [array([0.15024999], dtype=float32), -1.3131125]. 
=============================================
[2019-03-27 11:51:02,586] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6247527e-22 1.0000000e+00 5.1989522e-29 9.7934834e-19 1.8265938e-27], sum to 1.0000
[2019-03-27 11:51:02,594] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1854
[2019-03-27 11:51:02,607] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.23333333333333, 75.66666666666667, 1.0, 2.0, 0.4632503953924129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649275.9558403647, 649275.9558403647, 178293.890136891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7584000.0000, 
sim time next is 7584600.0000, 
raw observation next is [27.11666666666667, 76.83333333333333, 1.0, 2.0, 0.4673367974772862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653016.5367273157, 653016.5367273163, 178636.7514710027], 
processed observation next is [0.0, 0.782608695652174, 0.4842022116903636, 0.7683333333333333, 1.0, 1.0, 0.3582371053943207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18139348242425438, 0.18139348242425454, 0.26662201712089956], 
reward next is 0.7334, 
noisyNet noise sample is [array([0.15404832], dtype=float32), -0.7070755]. 
=============================================
[2019-03-27 11:51:15,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6882641e-12 9.9260086e-01 1.6824631e-18 7.3991679e-03 6.2487695e-19], sum to 1.0000
[2019-03-27 11:51:15,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7113
[2019-03-27 11:51:15,249] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2214672.525424509 W.
[2019-03-27 11:51:15,254] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.8, 59.66666666666667, 1.0, 2.0, 0.9426068763149571, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.983035971650087, 6.9112, 168.9124704259058, 2214672.525424509, 2163709.761389691, 446974.6518492237], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7742400.0000, 
sim time next is 7743000.0000, 
raw observation next is [31.7, 59.83333333333333, 1.0, 2.0, 0.9765672249817844, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.982127452967476, 6.9112, 168.9124762333985, 2262204.034646713, 2211885.801449323, 457035.8559499415], 
processed observation next is [1.0, 0.6086956521739131, 0.7014218009478673, 0.5983333333333333, 1.0, 1.0, 0.9717677409419089, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007092745296747615, 0.0, 0.8294375867722014, 0.628390009624087, 0.614412722624812, 0.6821430685820022], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5413119], dtype=float32), 0.2634724]. 
=============================================
[2019-03-27 11:51:15,270] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[59.003464]
 [59.963875]
 [59.56901 ]
 [59.320827]
 [59.836956]], R is [[57.58000946]
 [57.00421143]
 [56.45874405]
 [55.90823746]
 [55.34915543]].
[2019-03-27 11:51:27,289] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4973940e-11 9.6730888e-01 1.5116760e-16 3.2691102e-02 1.9043228e-16], sum to 1.0000
[2019-03-27 11:51:27,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4316
[2019-03-27 11:51:27,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2927567.707923454 W.
[2019-03-27 11:51:27,304] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.1, 70.5, 1.0, 2.0, 0.754169116022583, 1.0, 1.0, 0.6976745975255542, 1.0, 1.0, 1.03, 7.004584918761584, 6.9112, 170.5573041426782, 2927567.707923454, 2860672.291958907, 539004.6149235192], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7911000.0000, 
sim time next is 7911600.0000, 
raw observation next is [30.13333333333333, 70.33333333333333, 1.0, 2.0, 0.5091233529069799, 1.0, 2.0, 0.5091233529069799, 1.0, 2.0, 0.8823138337284274, 6.9112, 6.9112, 170.5573041426782, 2135685.740860901, 2135685.740860901, 421169.1579906097], 
processed observation next is [1.0, 0.5652173913043478, 0.6271721958925749, 0.7033333333333333, 1.0, 1.0, 0.4085823528999757, 1.0, 1.0, 0.4085823528999757, 1.0, 1.0, 0.8564802850346674, 0.0, 0.0, 0.8375144448122397, 0.593246039128028, 0.593246039128028, 0.6286106835680741], 
reward next is 0.3714, 
noisyNet noise sample is [array([-1.5255758], dtype=float32), -1.3639385]. 
=============================================
[2019-03-27 11:51:27,868] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:27,868] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:27,905] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-03-27 11:51:28,148] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:28,149] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:28,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-03-27 11:51:28,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:28,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:28,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-03-27 11:51:28,272] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:28,272] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:28,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-03-27 11:51:28,395] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:28,395] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:28,397] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-03-27 11:51:28,646] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:28,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:28,648] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-03-27 11:51:28,703] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:28,704] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:28,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-03-27 11:51:28,760] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:28,761] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:28,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-03-27 11:51:28,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:28,774] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:28,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-03-27 11:51:29,049] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:29,049] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:29,050] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-03-27 11:51:29,250] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:29,250] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:29,251] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-03-27 11:51:29,368] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:29,368] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:29,372] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-03-27 11:51:29,413] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:29,413] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:29,415] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-03-27 11:51:29,433] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:29,434] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:29,437] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-03-27 11:51:29,438] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:29,438] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:29,460] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-03-27 11:51:29,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 11:51:29,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:29,508] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-03-27 11:51:31,283] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 11:51:31,285] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:51:31,286] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:51:31,287] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:31,288] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:51:31,288] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:31,289] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:31,292] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:51:31,292] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:31,294] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:51:31,294] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:51:31,316] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run18
[2019-03-27 11:51:31,318] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run18
[2019-03-27 11:51:31,351] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run18
[2019-03-27 11:51:31,352] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run18
[2019-03-27 11:51:31,385] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run18
[2019-03-27 11:52:07,631] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0467441], dtype=float32), 0.04325983]
[2019-03-27 11:52:07,634] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.7, 90.0, 1.0, 2.0, 0.3823746062097265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 585083.8903477393, 585083.8903477386, 173579.4436900795]
[2019-03-27 11:52:07,636] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:52:07,640] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.2244871e-24 1.0000000e+00 1.0673768e-31 2.5490645e-22 1.2404156e-30], sampled 0.47556431183798087
[2019-03-27 11:52:13,986] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0467441], dtype=float32), 0.04325983]
[2019-03-27 11:52:13,988] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.71713776, 90.75163695, 1.0, 2.0, 0.4915312072342399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686834.6681825214, 686834.6681825214, 182280.1945672996]
[2019-03-27 11:52:13,991] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:52:13,993] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.60931145e-24 1.00000000e+00 2.61422332e-32 1.47734498e-24
 2.99670867e-31], sampled 0.5230349372685515
[2019-03-27 11:52:17,022] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0467441], dtype=float32), 0.04325983]
[2019-03-27 11:52:17,022] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3476887940397863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535605.8759647226, 535605.8759647219, 169456.450300268]
[2019-03-27 11:52:17,024] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:52:17,025] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.4205575e-23 1.0000000e+00 1.9975572e-30 4.7493467e-21 2.5032542e-29], sampled 0.4757954509141049
[2019-03-27 11:52:22,580] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0467441], dtype=float32), 0.04325983]
[2019-03-27 11:52:22,581] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.65, 57.5, 1.0, 2.0, 0.880374137400622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1230493.995479163, 1230493.995479163, 264601.2516073028]
[2019-03-27 11:52:22,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:52:22,591] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5382150e-23 1.0000000e+00 2.9825672e-31 3.9298420e-22 2.9371011e-30], sampled 0.24530112219488687
[2019-03-27 11:52:33,463] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0467441], dtype=float32), 0.04325983]
[2019-03-27 11:52:33,466] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.975790355, 87.31428223500001, 1.0, 2.0, 0.5006485356524026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699578.8422107549, 699578.8422107556, 183697.096011966]
[2019-03-27 11:52:33,467] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:52:33,471] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2649713e-24 1.0000000e+00 1.3415863e-32 5.1971411e-24 1.4805838e-31], sampled 0.6275006165360816
[2019-03-27 11:53:06,850] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0467441], dtype=float32), 0.04325983]
[2019-03-27 11:53:06,851] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.20017028, 77.38249463, 1.0, 2.0, 0.7300848602106176, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977161399429, 6.9112, 168.9123171412947, 1917228.999533634, 1849991.073768616, 391300.3756311379]
[2019-03-27 11:53:06,851] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:53:06,856] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.5414501e-19 1.0000000e+00 1.7367099e-26 3.2812261e-15 1.7691144e-25], sampled 0.11619768273782494
[2019-03-27 11:53:06,857] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1917228.999533634 W.
[2019-03-27 11:53:27,243] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0467441], dtype=float32), 0.04325983]
[2019-03-27 11:53:27,245] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.33333333333334, 73.33333333333334, 1.0, 2.0, 1.015205323203765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1437112.538026139, 1437112.538026138, 306445.2370564699]
[2019-03-27 11:53:27,246] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:53:27,249] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.8592154e-22 1.0000000e+00 2.7750812e-29 8.6692948e-20 2.7889851e-28], sampled 0.30853377176796204
[2019-03-27 11:53:39,031] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.7891 2842513253.0727 1130.0000
[2019-03-27 11:53:39,215] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6842 2927317744.9770 1338.0000
[2019-03-27 11:53:39,257] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8893 2779197741.5739 933.0000
[2019-03-27 11:53:39,445] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7647 3007775956.8125 1766.0000
[2019-03-27 11:53:39,591] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.6993 3163701439.5191 1768.0000
[2019-03-27 11:53:40,608] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 425000, evaluation results [425000.0, 7889.699330502661, 3163701439.519084, 1768.0, 8253.684239900658, 2927317744.977035, 1338.0, 8659.889250812355, 2779197741.573899, 933.0, 7998.764734894051, 3007775956.812482, 1766.0, 8497.789071899471, 2842513253.07267, 1130.0]
[2019-03-27 11:53:40,820] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8638153e-19 1.0000000e+00 4.0778762e-27 8.0434137e-16 4.3265788e-26], sum to 1.0000
[2019-03-27 11:53:40,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2257
[2019-03-27 11:53:40,840] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.11666666666667, 71.16666666666667, 1.0, 2.0, 0.7830654284616919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1208839.045687737, 1208839.045687737, 254667.3487820172], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 36600.0000, 
sim time next is 37200.0000, 
raw observation next is [25.33333333333334, 70.33333333333334, 1.0, 2.0, 0.7384114937585701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1137441.175536041, 1137441.175536041, 242812.8783525481], 
processed observation next is [1.0, 0.43478260869565216, 0.3996840442338076, 0.7033333333333335, 1.0, 1.0, 0.6848331250103253, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3159558820933447, 0.3159558820933447, 0.36240728112320614], 
reward next is 0.6376, 
noisyNet noise sample is [array([0.413224], dtype=float32), -0.04404481]. 
=============================================
[2019-03-27 11:53:41,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6413465e-24 1.0000000e+00 2.5580438e-31 4.4147504e-22 2.4056565e-30], sum to 1.0000
[2019-03-27 11:53:41,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8233
[2019-03-27 11:53:41,658] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 84.0, 1.0, 2.0, 0.3294673707365959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520693.2050942397, 520693.2050942397, 168598.3714326102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 28800.0000, 
sim time next is 29400.0000, 
raw observation next is [22.51666666666667, 83.0, 1.0, 2.0, 0.3567915519874122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562941.8508880638, 562941.8508880638, 171995.7415537664], 
processed observation next is [1.0, 0.34782608695652173, 0.2661927330173777, 0.83, 1.0, 1.0, 0.22505006263543637, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1563727363577955, 0.1563727363577955, 0.25671006202054686], 
reward next is 0.7433, 
noisyNet noise sample is [array([1.1990863], dtype=float32), 0.24826792]. 
=============================================
[2019-03-27 11:53:49,296] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.3182158e-16 1.0000000e+00 2.7858765e-23 5.2506932e-10 9.4323038e-23], sum to 1.0000
[2019-03-27 11:53:49,313] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1991
[2019-03-27 11:53:49,318] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 96.0, 1.0, 2.0, 0.8055072414105805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1199470.901769697, 1199470.901769696, 255338.0040801441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 130200.0000, 
sim time next is 130800.0000, 
raw observation next is [22.8, 96.0, 1.0, 2.0, 0.5897005694951256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 878133.0604781511, 878133.0604781511, 205617.59681179], 
processed observation next is [1.0, 0.5217391304347826, 0.2796208530805688, 0.96, 1.0, 1.0, 0.5056633367411152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24392585013281975, 0.24392585013281975, 0.30689193553998506], 
reward next is 0.6931, 
noisyNet noise sample is [array([0.19172825], dtype=float32), -1.9894739]. 
=============================================
[2019-03-27 11:54:16,260] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5508121e-21 1.0000000e+00 1.1537818e-27 4.0733374e-19 4.9268200e-27], sum to 1.0000
[2019-03-27 11:54:16,273] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5622
[2019-03-27 11:54:16,282] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 80.0, 1.0, 2.0, 0.2321895276978192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 384136.3550020184, 384136.3550020184, 159105.5177691892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 543600.0000, 
sim time next is 544200.0000, 
raw observation next is [20.2, 79.16666666666667, 1.0, 2.0, 0.2379483029790189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 393328.7558126719, 393328.7558126719, 159667.2780906805], 
processed observation next is [1.0, 0.30434782608695654, 0.15639810426540288, 0.7916666666666667, 1.0, 1.0, 0.08186542527592637, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1092579877257422, 0.1092579877257422, 0.23830937028459778], 
reward next is 0.7617, 
noisyNet noise sample is [array([-0.64397967], dtype=float32), -0.45944625]. 
=============================================
[2019-03-27 11:54:17,750] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1480276e-19 1.0000000e+00 3.1617345e-27 4.1820651e-15 3.3010945e-26], sum to 1.0000
[2019-03-27 11:54:17,761] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0081
[2019-03-27 11:54:17,765] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.66666666666667, 88.66666666666667, 1.0, 2.0, 0.2350682866107374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 389762.4796214353, 389762.4796214346, 159289.5929178243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 688800.0000, 
sim time next is 689400.0000, 
raw observation next is [18.6, 89.0, 1.0, 2.0, 0.2338519942766377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 387843.0326727114, 387843.0326727114, 159164.7167623282], 
processed observation next is [1.0, 1.0, 0.08056872037914704, 0.89, 1.0, 1.0, 0.07693011358631047, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10773417574241984, 0.10773417574241984, 0.23755927874974359], 
reward next is 0.7624, 
noisyNet noise sample is [array([0.8388485], dtype=float32), -0.38480747]. 
=============================================
[2019-03-27 11:54:23,919] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.25812580e-20 1.00000000e+00 1.06447111e-28 1.04837375e-17
 2.98241384e-27], sum to 1.0000
[2019-03-27 11:54:23,931] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7633
[2019-03-27 11:54:23,936] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.45, 71.5, 1.0, 2.0, 0.2442429345325114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402702.5802681952, 402702.5802681945, 160319.3379870274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 675000.0000, 
sim time next is 675600.0000, 
raw observation next is [21.23333333333333, 72.66666666666667, 1.0, 2.0, 0.2432084654639678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 401205.2548112692, 401205.2548112686, 160211.5489642061], 
processed observation next is [1.0, 0.8260869565217391, 0.2053712480252764, 0.7266666666666667, 1.0, 1.0, 0.08820297043851541, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11144590411424143, 0.11144590411424128, 0.23912171487194941], 
reward next is 0.7609, 
noisyNet noise sample is [array([-0.28715527], dtype=float32), 1.045824]. 
=============================================
[2019-03-27 11:54:33,218] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1386512e-23 1.0000000e+00 3.4159577e-31 1.2526705e-21 1.1340182e-29], sum to 1.0000
[2019-03-27 11:54:33,227] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8041
[2019-03-27 11:54:33,232] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 76.5, 1.0, 2.0, 0.287851958334223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461785.5987357184, 461785.5987357184, 164387.0768557938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 808200.0000, 
sim time next is 808800.0000, 
raw observation next is [22.76666666666667, 75.33333333333334, 1.0, 2.0, 0.287771708005965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 461659.387396554, 461659.3873965534, 164378.4425637016], 
processed observation next is [0.0, 0.34782608695652173, 0.2780410742496052, 0.7533333333333334, 1.0, 1.0, 0.1418936241035723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.128238718721265, 0.12823871872126483, 0.2453409590503009], 
reward next is 0.7547, 
noisyNet noise sample is [array([-0.05806204], dtype=float32), -1.7385266]. 
=============================================
[2019-03-27 11:54:40,025] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.73292303e-22 1.00000000e+00 3.02356820e-30 7.90752354e-21
 1.00298856e-28], sum to 1.0000
[2019-03-27 11:54:40,035] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7057
[2019-03-27 11:54:40,043] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2926940999104102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 467221.1734289101, 467221.1734289107, 164736.5207204623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 894000.0000, 
sim time next is 894600.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2930747842748531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 467827.9844454972, 467827.9844454965, 164778.8066060361], 
processed observation next is [0.0, 0.34782608695652173, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14828287262030496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.129952217901527, 0.1299522179015268, 0.24593851732244193], 
reward next is 0.7541, 
noisyNet noise sample is [array([-0.608588], dtype=float32), -0.2598842]. 
=============================================
[2019-03-27 11:54:46,481] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 11:54:46,483] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:54:46,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:54:46,485] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:54:46,486] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:54:46,487] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:54:46,487] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:54:46,488] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:54:46,489] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:54:46,490] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:54:46,491] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:54:46,516] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run19
[2019-03-27 11:54:46,517] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run19
[2019-03-27 11:54:46,517] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run19
[2019-03-27 11:54:46,573] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run19
[2019-03-27 11:54:46,573] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run19
[2019-03-27 11:55:02,444] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:55:02,445] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.7, 72.0, 1.0, 2.0, 0.3444830572521114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535510.6423986418, 535510.6423986418, 169585.8868452894]
[2019-03-27 11:55:02,447] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:55:02,453] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0990059e-21 1.0000000e+00 1.5705241e-28 3.3526195e-18 2.0884879e-27], sampled 0.5105864983490259
[2019-03-27 11:55:39,780] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:55:39,782] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.21161160666666, 77.53184278333333, 1.0, 2.0, 0.5565408327638842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777708.2675453477, 777708.2675453477, 192928.5720197353]
[2019-03-27 11:55:39,784] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:55:39,787] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.3825182e-20 1.0000000e+00 1.6957667e-27 1.7665540e-15 2.2539520e-26], sampled 0.41738107672633007
[2019-03-27 11:55:51,974] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:55:51,975] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [37.96686232833333, 59.25642243999999, 1.0, 2.0, 0.814925437383138, 1.0, 2.0, 0.7280527582058315, 1.0, 2.0, 1.03, 7.004578380851957, 6.9112, 171.5212843490159, 3055174.818586207, 2987906.023366739, 560298.4041683872]
[2019-03-27 11:55:51,976] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:55:51,981] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2038566e-13 9.9574572e-01 1.3162200e-20 4.2543388e-03 1.6724933e-20], sampled 0.8263972391363021
[2019-03-27 11:55:51,982] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 3055174.818586207 W.
[2019-03-27 11:55:56,302] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:55:56,303] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.13333333333334, 66.0, 1.0, 2.0, 0.855941215682043, 1.0, 2.0, 0.855941215682043, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 2393913.873762089, 2393913.873762088, 448506.9745523935]
[2019-03-27 11:55:56,303] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:55:56,304] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9332935e-13 9.9957353e-01 1.1154302e-19 4.2640939e-04 1.4949639e-19], sampled 0.4758370698631168
[2019-03-27 11:55:56,305] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2393913.873762089 W.
[2019-03-27 11:56:04,334] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:56:04,337] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.06666666666667, 79.16666666666667, 1.0, 2.0, 0.6383071406081544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 892016.2079173705, 892016.2079173705, 208099.4249776694]
[2019-03-27 11:56:04,341] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:56:04,345] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.8594206e-23 1.0000000e+00 2.5590394e-31 1.8304785e-20 5.1102896e-30], sampled 0.021269003659422636
[2019-03-27 11:56:13,541] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:56:13,541] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.07307002833333, 57.193421055, 1.0, 2.0, 0.628818774829806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565103889, 878750.9991714676, 878750.9991714676, 206232.4666247382]
[2019-03-27 11:56:13,541] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:56:13,545] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.5202964e-21 1.0000000e+00 3.1884017e-29 1.5119486e-14 2.9053231e-28], sampled 0.37697430584822367
[2019-03-27 11:56:17,237] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:56:17,238] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.50345596666667, 82.63456021166668, 1.0, 2.0, 0.5662989864769298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791349.3579322089, 791349.3579322089, 194634.6832087331]
[2019-03-27 11:56:17,238] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:56:17,241] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5865125e-19 1.0000000e+00 5.6966011e-27 1.9103126e-14 5.9048957e-26], sampled 0.9910417066280831
[2019-03-27 11:56:25,708] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:56:25,711] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.76666666666667, 90.66666666666667, 1.0, 2.0, 0.7407388098241664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1035231.342143353, 1035231.342143353, 229884.528256455]
[2019-03-27 11:56:25,712] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:56:25,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.7263833e-20 1.0000000e+00 3.1511386e-27 6.4545398e-16 4.3644065e-26], sampled 0.1845267884564603
[2019-03-27 11:56:36,475] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:56:36,476] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.55, 89.5, 1.0, 2.0, 0.6000855930853443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838581.5880403393, 838581.5880403393, 200761.6101585499]
[2019-03-27 11:56:36,477] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:56:36,479] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.2469271e-22 1.0000000e+00 1.8656759e-29 3.0913934e-19 2.6159743e-28], sampled 0.7086132020604
[2019-03-27 11:56:44,628] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04665628], dtype=float32), 0.043064497]
[2019-03-27 11:56:44,629] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.00504991, 73.30377535, 1.0, 2.0, 0.413184705532199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614134.3230657767, 614134.3230657767, 175785.8861972548]
[2019-03-27 11:56:44,631] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:56:44,632] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.4871689e-20 1.0000000e+00 1.8394316e-27 6.4032843e-16 2.2662897e-26], sampled 0.3290021335244212
[2019-03-27 11:56:53,579] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7896.0387 3162631775.2351 1750.0000
[2019-03-27 11:56:53,910] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.6664 3007584299.6003 1765.0000
[2019-03-27 11:56:54,098] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.6231 2842387769.0575 1130.0000
[2019-03-27 11:56:54,438] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.6561 2779170422.0656 932.0000
[2019-03-27 11:56:54,621] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0788 2927160314.2148 1335.0000
[2019-03-27 11:56:55,637] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 450000, evaluation results [450000.0, 7896.03868794623, 3162631775.23508, 1750.0, 8255.078779771993, 2927160314.214828, 1335.0, 8659.65613219235, 2779170422.065641, 932.0, 7998.666413030706, 3007584299.60026, 1765.0, 8498.623063381603, 2842387769.057513, 1130.0]
[2019-03-27 11:57:05,672] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7617894e-15 1.0000000e+00 6.0652190e-22 1.1050272e-09 2.5194824e-22], sum to 1.0000
[2019-03-27 11:57:05,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6231
[2019-03-27 11:57:05,694] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 69.0, 1.0, 2.0, 0.7022262944596471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080421.196625199, 1080421.196625199, 233784.8565052438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1087200.0000, 
sim time next is 1087800.0000, 
raw observation next is [25.61666666666667, 68.83333333333333, 1.0, 2.0, 0.6240442131922198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 959612.0107939396, 959612.0107939389, 216011.5901053671], 
processed observation next is [1.0, 0.6086956521739131, 0.41311216429699865, 0.6883333333333332, 1.0, 1.0, 0.5470412207135178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26655889188720544, 0.2665588918872052, 0.32240535836621953], 
reward next is 0.6776, 
noisyNet noise sample is [array([-0.36374542], dtype=float32), -0.9543868]. 
=============================================
[2019-03-27 11:57:08,528] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.9986988e-20 1.0000000e+00 1.5427304e-26 8.7025428e-13 9.9031033e-27], sum to 1.0000
[2019-03-27 11:57:08,544] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7172
[2019-03-27 11:57:08,553] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 79.33333333333334, 1.0, 2.0, 0.4675282410209698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653284.125842108, 653284.1258421086, 178665.6062580036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1273800.0000, 
sim time next is 1274400.0000, 
raw observation next is [26.9, 80.0, 1.0, 2.0, 0.4722703246041702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659912.3674762726, 659912.3674762732, 179365.937694445], 
processed observation next is [1.0, 0.782608695652174, 0.4739336492890995, 0.8, 1.0, 1.0, 0.3641811139809279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18330899096563127, 0.18330899096563144, 0.26771035476782834], 
reward next is 0.7323, 
noisyNet noise sample is [array([-0.8082066], dtype=float32), 0.0622021]. 
=============================================
[2019-03-27 11:57:09,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0905295e-21 1.0000000e+00 1.0467820e-27 4.7337310e-16 1.1535133e-27], sum to 1.0000
[2019-03-27 11:57:09,755] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5205
[2019-03-27 11:57:09,762] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.88333333333333, 92.16666666666667, 1.0, 2.0, 0.3928120115120785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610927.8457835277, 610927.8457835283, 176069.1309924747], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1221000.0000, 
sim time next is 1221600.0000, 
raw observation next is [21.86666666666667, 92.33333333333334, 1.0, 2.0, 0.3665101887854174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569940.3009527042, 569940.3009527042, 172461.4239260066], 
processed observation next is [1.0, 0.13043478260869565, 0.23538704581358633, 0.9233333333333335, 1.0, 1.0, 0.23675926359688845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15831675026464004, 0.15831675026464004, 0.2574051103373233], 
reward next is 0.7426, 
noisyNet noise sample is [array([0.11283784], dtype=float32), 0.2703963]. 
=============================================
[2019-03-27 11:57:10,586] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.6506058e-19 1.0000000e+00 5.1961469e-25 2.7706004e-11 7.8078430e-25], sum to 1.0000
[2019-03-27 11:57:10,596] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1156
[2019-03-27 11:57:10,610] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 79.33333333333334, 1.0, 2.0, 0.3564349496608108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548636.0145774892, 548636.0145774886, 170519.9841451461], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1200000.0000, 
sim time next is 1200600.0000, 
raw observation next is [23.85, 80.0, 1.0, 2.0, 0.355786980345851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547949.502035673, 547949.502035673, 170471.4634428558], 
processed observation next is [1.0, 0.9130434782608695, 0.3293838862559243, 0.8, 1.0, 1.0, 0.22383973535644697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15220819500990918, 0.15220819500990918, 0.2544350200639639], 
reward next is 0.7456, 
noisyNet noise sample is [array([0.04143152], dtype=float32), 1.1078652]. 
=============================================
[2019-03-27 11:57:13,221] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.3038868e-16 1.0000000e+00 2.7822338e-23 2.6714575e-11 9.7280800e-23], sum to 1.0000
[2019-03-27 11:57:13,236] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9552
[2019-03-27 11:57:13,240] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 77.5, 1.0, 2.0, 0.9357744376790966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1351334.99541321, 1351334.995413211, 286461.4781470044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1243800.0000, 
sim time next is 1244400.0000, 
raw observation next is [26.4, 76.66666666666667, 1.0, 2.0, 0.9627624593586369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1386511.577371322, 1386511.577371322, 293996.219844892], 
processed observation next is [1.0, 0.391304347826087, 0.45023696682464454, 0.7666666666666667, 1.0, 1.0, 0.955135493203177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.38514210482536726, 0.38514210482536726, 0.43880032812670444], 
reward next is 0.5612, 
noisyNet noise sample is [array([1.1792854], dtype=float32), -2.2667665]. 
=============================================
[2019-03-27 11:57:15,616] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.3608676e-12 9.9599576e-01 5.1678005e-19 4.0041781e-03 1.6543570e-20], sum to 1.0000
[2019-03-27 11:57:15,626] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6631
[2019-03-27 11:57:15,632] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 73.0, 1.0, 2.0, 1.032627943447695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1443442.95593697, 1443442.955936969, 309022.3477040773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1256400.0000, 
sim time next is 1257000.0000, 
raw observation next is [28.41666666666667, 73.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.940261928691829, 6.9112, 168.9125988634458, 1474386.53619103, 1453769.047397082, 311347.8203654932], 
processed observation next is [1.0, 0.5652173913043478, 0.5458135860979465, 0.73, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0029061928691828777, 0.0, 0.8294381889418738, 0.4095518156086194, 0.40382473538807834, 0.46469823935148236], 
reward next is 0.3900, 
noisyNet noise sample is [array([0.27264595], dtype=float32), -0.12861091]. 
=============================================
[2019-03-27 11:57:15,652] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.89929 ]
 [70.80876 ]
 [70.35026 ]
 [69.129395]
 [66.82927 ]], R is [[70.19464111]
 [70.03147125]
 [69.91558838]
 [69.78979492]
 [69.68387604]].
[2019-03-27 11:57:20,190] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0646319e-18 1.0000000e+00 2.3838023e-25 1.1378700e-11 8.6220120e-25], sum to 1.0000
[2019-03-27 11:57:20,202] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2680
[2019-03-27 11:57:20,208] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4594574191080348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651614.1455646659, 651614.1455646659, 178726.2787576435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1297800.0000, 
sim time next is 1298400.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.4596510825608306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651891.214867351, 651891.2148673504, 178755.0599349044], 
processed observation next is [1.0, 0.0, 0.3507109004739337, 0.94, 1.0, 1.0, 0.3489772079046152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1810808930187086, 0.18108089301870844, 0.26679859691776775], 
reward next is 0.7332, 
noisyNet noise sample is [array([-0.36664075], dtype=float32), 2.0570138]. 
=============================================
[2019-03-27 11:57:22,371] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0032166e-16 1.0000000e+00 1.0623280e-23 2.9180619e-10 5.8879669e-23], sum to 1.0000
[2019-03-27 11:57:22,383] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8979
[2019-03-27 11:57:22,388] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 91.5, 1.0, 2.0, 0.5898153121947395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 940732.0345272601, 940732.0345272601, 212064.7947096643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1355400.0000, 
sim time next is 1356000.0000, 
raw observation next is [20.86666666666667, 91.66666666666666, 1.0, 2.0, 0.607121084351589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 968618.3829843592, 968618.3829843592, 215748.4737343594], 
processed observation next is [1.0, 0.6956521739130435, 0.18799368088467638, 0.9166666666666665, 1.0, 1.0, 0.526651908857336, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26906066194009975, 0.26906066194009975, 0.3220126473647155], 
reward next is 0.6780, 
noisyNet noise sample is [array([0.75151515], dtype=float32), 0.4425406]. 
=============================================
[2019-03-27 11:57:22,402] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.60773]
 [70.51296]
 [70.3111 ]
 [70.05848]
 [70.16933]], R is [[70.6059494 ]
 [70.58337402]
 [70.55666351]
 [70.51875305]
 [70.4564209 ]].
[2019-03-27 11:57:23,703] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4988046e-17 1.0000000e+00 3.9055954e-25 6.3177337e-12 1.0367413e-24], sum to 1.0000
[2019-03-27 11:57:23,714] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1290
[2019-03-27 11:57:23,721] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 92.0, 1.0, 2.0, 0.5936243180710554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 914529.9528314535, 914529.9528314535, 209841.8331625114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1341000.0000, 
sim time next is 1341600.0000, 
raw observation next is [22.13333333333333, 91.66666666666667, 1.0, 2.0, 0.5583146336659826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 862834.1468249941, 862834.1468249935, 203120.4697059995], 
processed observation next is [1.0, 0.5217391304347826, 0.24802527646129527, 0.9166666666666667, 1.0, 1.0, 0.46784895622407535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2396761518958317, 0.23967615189583155, 0.3031648801582082], 
reward next is 0.6968, 
noisyNet noise sample is [array([-0.19529818], dtype=float32), -1.2447516]. 
=============================================
[2019-03-27 11:57:35,158] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.3350484e-21 1.0000000e+00 3.4249137e-28 8.7307681e-18 8.9155010e-27], sum to 1.0000
[2019-03-27 11:57:35,169] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7595
[2019-03-27 11:57:35,177] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.96666666666667, 91.0, 1.0, 2.0, 0.3377897578458568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 526031.0911237993, 526031.0911237986, 168849.4054492384], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1554000.0000, 
sim time next is 1554600.0000, 
raw observation next is [21.93333333333333, 91.0, 1.0, 2.0, 0.3364177975018898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524356.4498101684, 524356.4498101678, 168728.6747441574], 
processed observation next is [0.0, 1.0, 0.23854660347551332, 0.91, 1.0, 1.0, 0.2005033704842046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14565456939171342, 0.14565456939171328, 0.2518338429017275], 
reward next is 0.7482, 
noisyNet noise sample is [array([-0.5546295], dtype=float32), -0.3124957]. 
=============================================
[2019-03-27 11:57:51,737] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1849579e-17 1.0000000e+00 5.3652465e-24 2.5024310e-12 1.2167669e-23], sum to 1.0000
[2019-03-27 11:57:51,751] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5089
[2019-03-27 11:57:51,758] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.38333333333333, 85.33333333333334, 1.0, 2.0, 0.7552224976841946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1153933.697518288, 1153933.697518289, 246009.6674691382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1770600.0000, 
sim time next is 1771200.0000, 
raw observation next is [23.3, 86.0, 1.0, 2.0, 0.7947320367797845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1214071.774762889, 1214071.77476289, 256314.6138661358], 
processed observation next is [1.0, 0.5217391304347826, 0.3033175355450238, 0.86, 1.0, 1.0, 0.7526892009394994, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.337242159656358, 0.33724215965635834, 0.382559125173337], 
reward next is 0.6174, 
noisyNet noise sample is [array([-0.09079181], dtype=float32), -0.52988046]. 
=============================================
[2019-03-27 11:57:58,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5375782e-19 1.0000000e+00 6.3350523e-26 2.8141999e-14 3.5763703e-24], sum to 1.0000
[2019-03-27 11:57:58,818] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9194
[2019-03-27 11:57:58,827] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 90.0, 1.0, 2.0, 0.8642812292572144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1238442.543499014, 1238442.543499014, 264706.07610416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1850400.0000, 
sim time next is 1851000.0000, 
raw observation next is [24.71666666666667, 89.66666666666667, 1.0, 2.0, 0.8951863576699861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1278924.57930032, 1278924.57930032, 272699.238558078], 
processed observation next is [1.0, 0.43478260869565216, 0.3704581358609796, 0.8966666666666667, 1.0, 1.0, 0.8737185032168507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35525682758342225, 0.35525682758342225, 0.40701378889265377], 
reward next is 0.5930, 
noisyNet noise sample is [array([-0.34221858], dtype=float32), 0.77715385]. 
=============================================
[2019-03-27 11:57:58,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[65.11238 ]
 [65.01283 ]
 [65.171265]
 [65.38394 ]
 [65.63334 ]], R is [[64.95695496]
 [64.91230774]
 [64.85768127]
 [64.8070755 ]
 [64.76181793]].
[2019-03-27 11:58:01,144] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 11:58:01,148] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 11:58:01,148] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:58:01,149] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 11:58:01,150] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 11:58:01,151] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 11:58:01,152] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:58:01,154] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:58:01,154] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 11:58:01,152] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:58:01,158] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 11:58:01,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run20
[2019-03-27 11:58:01,177] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run20
[2019-03-27 11:58:01,194] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run20
[2019-03-27 11:58:01,215] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run20
[2019-03-27 11:58:01,256] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run20
[2019-03-27 11:58:29,433] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04823792], dtype=float32), 0.041918106]
[2019-03-27 11:58:29,433] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.06445068, 83.54294985, 1.0, 2.0, 0.510762499278206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713716.2887906792, 713716.2887906792, 185297.215221664]
[2019-03-27 11:58:29,434] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 11:58:29,437] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0271065e-21 1.0000000e+00 3.1120958e-29 2.3386123e-19 3.0799262e-28], sampled 0.4935869487057287
[2019-03-27 11:58:46,221] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04823792], dtype=float32), 0.041918106]
[2019-03-27 11:58:46,223] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.56666666666667, 53.5, 1.0, 2.0, 0.7412079778202797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1035887.355168006, 1035887.355168006, 229989.5322786455]
[2019-03-27 11:58:46,224] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 11:58:46,226] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.4414801e-21 1.0000000e+00 1.4510575e-28 6.5167026e-19 1.2757420e-27], sampled 0.5163840970086406
[2019-03-27 11:58:58,196] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04823792], dtype=float32), 0.041918106]
[2019-03-27 11:58:58,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 76.5, 1.0, 2.0, 0.5078738109730786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709678.4187477212, 709678.4187477205, 184838.7104194204]
[2019-03-27 11:58:58,200] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:58:58,204] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.3212271e-22 1.0000000e+00 1.4109209e-29 7.2390479e-20 1.5287916e-28], sampled 0.9610223478570671
[2019-03-27 11:59:01,831] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04823792], dtype=float32), 0.041918106]
[2019-03-27 11:59:01,835] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.33333333333334, 62.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 6.926419866389494, 6.9112, 170.5573041426782, 2920245.076508351, 2909342.467108976, 553552.3373697674]
[2019-03-27 11:59:01,837] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:59:01,840] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5970398e-12 9.9321306e-01 5.0197947e-19 6.7869481e-03 2.4225390e-19], sampled 0.3950697803495683
[2019-03-27 11:59:01,841] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2920245.076508351 W.
[2019-03-27 11:59:05,693] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04823792], dtype=float32), 0.041918106]
[2019-03-27 11:59:05,694] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.66666666666667, 71.33333333333334, 1.0, 2.0, 0.923201743063156, 1.0, 1.0, 0.923201743063156, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2582256.850896573, 2582256.850896573, 484300.2676521997]
[2019-03-27 11:59:05,694] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 11:59:05,696] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.7415760e-12 6.2739384e-01 4.3765866e-18 3.7260619e-01 2.5422770e-18], sampled 0.541453007750312
[2019-03-27 11:59:05,700] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2582256.850896573 W.
[2019-03-27 11:59:16,361] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04823792], dtype=float32), 0.041918106]
[2019-03-27 11:59:16,362] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.9, 49.5, 1.0, 2.0, 0.6467584991747077, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005971954017816, 6.9112, 168.9123255175515, 1800627.707467256, 1733393.47264989, 373526.7979431372]
[2019-03-27 11:59:16,365] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:59:16,368] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1392765e-17 1.0000000e+00 2.9715617e-24 3.4841292e-12 1.2717990e-23], sampled 0.3287758337280552
[2019-03-27 11:59:16,368] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1800627.707467256 W.
[2019-03-27 11:59:46,850] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04823792], dtype=float32), 0.041918106]
[2019-03-27 11:59:46,852] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.25015088, 58.04665458, 1.0, 2.0, 0.6632559960016909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1020306.283009174, 1020306.283009174, 224685.8683153422]
[2019-03-27 11:59:46,852] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 11:59:46,855] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3861580e-18 1.0000000e+00 1.7579605e-25 1.6427346e-12 6.0664360e-25], sampled 0.6574274738026459
[2019-03-27 11:59:53,732] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04823792], dtype=float32), 0.041918106]
[2019-03-27 11:59:53,735] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.16666666666667, 95.0, 1.0, 2.0, 0.4282902128771636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 632647.3081031461, 632647.3081031456, 177461.1469126499]
[2019-03-27 11:59:53,735] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 11:59:53,739] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0486271e-22 1.0000000e+00 4.0824614e-30 1.0260453e-21 4.0674629e-29], sampled 0.6564961871612713
[2019-03-27 12:00:01,943] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04823792], dtype=float32), 0.041918106]
[2019-03-27 12:00:01,943] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.2, 85.33333333333334, 1.0, 2.0, 0.530549265884409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741375.0652730119, 741375.0652730126, 188520.5219245028]
[2019-03-27 12:00:01,943] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:00:01,945] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.1231045e-22 1.0000000e+00 1.5219654e-29 2.6012090e-19 1.7436452e-28], sampled 0.2813994865745164
[2019-03-27 12:00:08,152] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0391 2927275383.3645 1336.0000
[2019-03-27 12:00:08,987] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.9149 3163012560.9022 1756.0000
[2019-03-27 12:00:09,088] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.1640 2842400975.7195 1130.0000
[2019-03-27 12:00:09,089] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1488 2779280987.4023 933.0000
[2019-03-27 12:00:09,218] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8197 3007605967.1875 1765.0000
[2019-03-27 12:00:10,236] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 475000, evaluation results [475000.0, 7889.914881364264, 3163012560.9022274, 1756.0, 8255.03912645428, 2927275383.3645267, 1336.0, 8659.148803224978, 2779280987.402282, 933.0, 7997.819685169013, 3007605967.187519, 1765.0, 8499.163950451848, 2842400975.7194896, 1130.0]
[2019-03-27 12:00:17,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2928840e-25 1.0000000e+00 1.7142868e-31 8.4904859e-23 3.2906774e-30], sum to 1.0000
[2019-03-27 12:00:17,028] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3155
[2019-03-27 12:00:17,035] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 90.0, 1.0, 2.0, 0.5083006152379825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710275.0138739177, 710275.0138739183, 184907.0869315164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2030400.0000, 
sim time next is 2031000.0000, 
raw observation next is [26.31666666666666, 89.33333333333333, 1.0, 2.0, 0.5098641702574394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712460.5837063199, 712460.5837063199, 185156.2316180599], 
processed observation next is [0.0, 0.5217391304347826, 0.4462875197472351, 0.8933333333333333, 1.0, 1.0, 0.4094749039246257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19790571769619997, 0.19790571769619997, 0.276352584504567], 
reward next is 0.7236, 
noisyNet noise sample is [array([1.2377274], dtype=float32), 1.1670804]. 
=============================================
[2019-03-27 12:00:17,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.499176]
 [75.44583 ]
 [75.42723 ]
 [75.40727 ]
 [75.38895 ]], R is [[75.47408295]
 [75.443367  ]
 [75.41311646]
 [75.38327789]
 [75.35381317]].
[2019-03-27 12:00:21,629] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3295194e-21 1.0000000e+00 1.4619633e-29 8.1139118e-19 8.7967452e-29], sum to 1.0000
[2019-03-27 12:00:21,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3116
[2019-03-27 12:00:21,644] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 94.0, 1.0, 2.0, 0.4687473294656851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657740.7315000216, 657740.7315000216, 179200.0333773325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2072400.0000, 
sim time next is 2073000.0000, 
raw observation next is [24.51666666666667, 94.0, 1.0, 2.0, 0.4682872902829175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657590.738664544, 657590.738664544, 179195.8874800015], 
processed observation next is [0.0, 1.0, 0.36097946287519767, 0.94, 1.0, 1.0, 0.3593822774492981, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18266409407348444, 0.18266409407348444, 0.2674565484776142], 
reward next is 0.7325, 
noisyNet noise sample is [array([0.53363156], dtype=float32), 1.7931958]. 
=============================================
[2019-03-27 12:00:21,656] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.33256]
 [74.32862]
 [74.32826]
 [74.33819]
 [74.34027]], R is [[74.32034302]
 [74.30967712]
 [74.29911804]
 [74.28861237]
 [74.27807617]].
[2019-03-27 12:00:23,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5905894e-22 1.0000000e+00 4.9565190e-31 1.1219136e-23 2.1188421e-30], sum to 1.0000
[2019-03-27 12:00:23,808] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3033
[2019-03-27 12:00:23,814] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 94.0, 1.0, 2.0, 0.4797412250532267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670354.8935108938, 670354.8935108938, 180482.9102815486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2096400.0000, 
sim time next is 2097000.0000, 
raw observation next is [25.05, 93.5, 1.0, 2.0, 0.4820010316194018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673513.5810254433, 673513.5810254439, 180824.2370530177], 
processed observation next is [0.0, 0.2608695652173913, 0.3862559241706162, 0.935, 1.0, 1.0, 0.3759048573727732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18708710584040092, 0.18708710584040106, 0.2698869209746533], 
reward next is 0.7301, 
noisyNet noise sample is [array([0.5077792], dtype=float32), 0.4893864]. 
=============================================
[2019-03-27 12:00:23,834] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.51964]
 [72.53943]
 [72.55059]
 [72.52731]
 [72.55717]], R is [[72.51519775]
 [72.52066803]
 [72.52642059]
 [72.53215027]
 [72.53773499]].
[2019-03-27 12:00:26,658] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1822277e-22 1.0000000e+00 7.5286437e-30 2.0480063e-20 5.9867881e-29], sum to 1.0000
[2019-03-27 12:00:26,668] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3008
[2019-03-27 12:00:26,677] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 88.33333333333333, 1.0, 2.0, 0.5425320004853653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758125.3869766641, 758125.3869766641, 190528.2587951988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2148000.0000, 
sim time next is 2148600.0000, 
raw observation next is [27.18333333333334, 88.66666666666667, 1.0, 2.0, 0.5404045585068844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755151.4774460234, 755151.4774460227, 190168.8372535009], 
processed observation next is [0.0, 0.8695652173913043, 0.4873617693522911, 0.8866666666666667, 1.0, 1.0, 0.446270552417933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20976429929056206, 0.20976429929056187, 0.28383408545298644], 
reward next is 0.7162, 
noisyNet noise sample is [array([1.5657293], dtype=float32), 0.4807738]. 
=============================================
[2019-03-27 12:00:30,955] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5295151e-16 1.0000000e+00 4.3829640e-24 2.3309065e-13 1.2205703e-22], sum to 1.0000
[2019-03-27 12:00:30,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6728
[2019-03-27 12:00:30,973] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 91.16666666666667, 1.0, 2.0, 0.6528246097881507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 912312.6748109472, 912312.6748109466, 210990.3965070712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2182200.0000, 
sim time next is 2182800.0000, 
raw observation next is [26.2, 90.33333333333334, 1.0, 2.0, 0.6942852340193061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 970279.7476539674, 970279.7476539667, 219615.0888536948], 
processed observation next is [1.0, 0.2608695652173913, 0.44075829383886256, 0.9033333333333334, 1.0, 1.0, 0.6316689566497663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26952215212610203, 0.26952215212610187, 0.3277837147070071], 
reward next is 0.6722, 
noisyNet noise sample is [array([0.49182907], dtype=float32), 0.49970728]. 
=============================================
[2019-03-27 12:00:32,243] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1059066e-12 2.9655301e-03 4.4914621e-18 9.9703443e-01 9.5888472e-19], sum to 1.0000
[2019-03-27 12:00:32,254] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3077
[2019-03-27 12:00:32,260] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.6, 71.0, 1.0, 2.0, 0.7327671429376258, 1.0, 2.0, 0.7327671429376258, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2049139.531601125, 2049139.531601125, 388470.0609972263], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2202000.0000, 
sim time next is 2202600.0000, 
raw observation next is [30.7, 70.5, 1.0, 2.0, 0.7437380335726276, 1.0, 2.0, 0.7437380335726276, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2079848.754038865, 2079848.754038865, 393413.0403591064], 
processed observation next is [1.0, 0.4782608695652174, 0.6540284360189573, 0.705, 1.0, 1.0, 0.6912506428585874, 1.0, 1.0, 0.6912506428585874, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5777357650107958, 0.5777357650107958, 0.5871836423270245], 
reward next is 0.4128, 
noisyNet noise sample is [array([-1.2227085], dtype=float32), -0.13173018]. 
=============================================
[2019-03-27 12:00:40,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9293305e-22 1.0000000e+00 6.6255072e-29 1.3355342e-19 2.3512591e-28], sum to 1.0000
[2019-03-27 12:00:40,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0794
[2019-03-27 12:00:40,488] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 81.0, 1.0, 2.0, 0.5220923383277316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729553.5273032588, 729553.5273032588, 187129.9057731547], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2340000.0000, 
sim time next is 2340600.0000, 
raw observation next is [27.76666666666667, 81.16666666666667, 1.0, 2.0, 1.036008902548683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1448172.200901085, 1448172.200901085, 310088.4172271824], 
processed observation next is [1.0, 0.08695652173913043, 0.515007898894155, 0.8116666666666668, 1.0, 1.0, 1.0433842199381722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.40227005580585695, 0.40227005580585695, 0.4628185331748991], 
reward next is 0.5372, 
noisyNet noise sample is [array([-0.8144556], dtype=float32), -0.46277386]. 
=============================================
[2019-03-27 12:00:45,613] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6268388e-17 1.0000000e+00 1.4858843e-25 3.8561673e-16 7.9483953e-25], sum to 1.0000
[2019-03-27 12:00:45,621] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6496
[2019-03-27 12:00:45,629] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 0.8730031739614281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1220185.721320816, 1220185.721320815, 262619.1292893169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2438400.0000, 
sim time next is 2439000.0000, 
raw observation next is [27.6, 84.0, 1.0, 2.0, 0.9602712200657973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1342236.290751315, 1342236.290751314, 287054.3852909019], 
processed observation next is [1.0, 0.21739130434782608, 0.5071090047393366, 0.84, 1.0, 1.0, 0.952134000079274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3728434140975875, 0.37284341409758726, 0.42843938103119683], 
reward next is 0.5716, 
noisyNet noise sample is [array([0.17526712], dtype=float32), 0.37085176]. 
=============================================
[2019-03-27 12:00:45,643] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[56.67687 ]
 [56.590927]
 [58.04502 ]
 [58.302235]
 [58.20769 ]], R is [[56.20388794]
 [56.24988174]
 [56.23602676]
 [56.32876968]
 [56.42824173]].
[2019-03-27 12:00:48,549] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5944678e-15 1.0000000e+00 9.2390501e-21 7.9059387e-10 3.4059820e-21], sum to 1.0000
[2019-03-27 12:00:48,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1713
[2019-03-27 12:00:48,578] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1910008.058600952 W.
[2019-03-27 12:00:48,586] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.8, 87.33333333333334, 1.0, 2.0, 0.4553722594820331, 1.0, 2.0, 0.4553722594820331, 1.0, 2.0, 0.7807215975559593, 6.9112, 6.9112, 170.5573041426782, 1910008.058600952, 1910008.058600952, 384005.0743124086], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2452800.0000, 
sim time next is 2453400.0000, 
raw observation next is [26.7, 87.5, 1.0, 2.0, 0.7804038055498268, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.980564359516936, 6.9112, 168.9124886870541, 1987648.143095796, 1938438.815408602, 403959.2644323172], 
processed observation next is [1.0, 0.391304347826087, 0.46445497630331756, 0.875, 1.0, 1.0, 0.7354262717467792, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006936435951693643, 0.0, 0.8294376479253516, 0.5521244841932766, 0.5384552265023894, 0.6029242752721152], 
reward next is 0.0503, 
noisyNet noise sample is [array([0.77376324], dtype=float32), 1.4717727]. 
=============================================
[2019-03-27 12:00:49,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4065277e-21 1.0000000e+00 6.9749599e-28 6.5099909e-20 5.1592466e-27], sum to 1.0000
[2019-03-27 12:00:49,164] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3627
[2019-03-27 12:00:49,175] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.35, 80.83333333333333, 1.0, 2.0, 0.9601686979998361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104277, 1342092.8981177, 1342092.8981177, 287025.044330258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2429400.0000, 
sim time next is 2430000.0000, 
raw observation next is [28.3, 81.0, 1.0, 2.0, 0.8873752455817311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1240285.120648785, 1240285.120648784, 266488.8411225968], 
processed observation next is [1.0, 0.13043478260869565, 0.5402843601895735, 0.81, 1.0, 1.0, 0.8643075247972664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3445236446246625, 0.34452364462466223, 0.39774453898895046], 
reward next is 0.6023, 
noisyNet noise sample is [array([-0.52573013], dtype=float32), -1.3026708]. 
=============================================
[2019-03-27 12:00:49,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[59.682747]
 [58.976227]
 [58.50489 ]
 [59.777523]
 [61.740612]], R is [[60.36263275]
 [60.33061218]
 [59.72730637]
 [59.64152527]
 [59.14128113]].
[2019-03-27 12:00:57,674] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0848260e-25 1.0000000e+00 1.1162935e-34 6.7021452e-30 2.0983199e-33], sum to 1.0000
[2019-03-27 12:00:57,684] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6733
[2019-03-27 12:00:57,690] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 92.0, 1.0, 2.0, 0.4303863808367035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625771.8717359477, 625771.8717359477, 176534.932103304], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2608200.0000, 
sim time next is 2608800.0000, 
raw observation next is [23.93333333333333, 92.0, 1.0, 2.0, 0.4307682945254191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 626735.3034365972, 626735.3034365979, 176640.1461557788], 
processed observation next is [0.0, 0.17391304347826086, 0.3333333333333332, 0.92, 1.0, 1.0, 0.31417866810291456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17409313984349922, 0.17409313984349942, 0.26364200918772956], 
reward next is 0.7364, 
noisyNet noise sample is [array([-0.4883869], dtype=float32), 0.028035766]. 
=============================================
[2019-03-27 12:00:58,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5820292e-27 1.0000000e+00 2.1964693e-35 3.6322477e-31 2.7607550e-35], sum to 1.0000
[2019-03-27 12:00:58,349] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6179
[2019-03-27 12:00:58,353] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 100.0, 1.0, 2.0, 0.409181697380103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604254.4559888901, 604254.4559888901, 174749.847841923], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2716200.0000, 
sim time next is 2716800.0000, 
raw observation next is [22.33333333333334, 100.0, 1.0, 2.0, 0.4027813427192451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598459.2025114512, 598459.2025114506, 174321.1179528341], 
processed observation next is [0.0, 0.43478260869565216, 0.2575039494470777, 1.0, 1.0, 1.0, 0.28045944905933146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.166238667364292, 0.16623866736429185, 0.26018077306393145], 
reward next is 0.7398, 
noisyNet noise sample is [array([0.90696186], dtype=float32), -0.8466045]. 
=============================================
[2019-03-27 12:01:02,780] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8058533e-24 1.0000000e+00 1.4493664e-33 3.2411996e-26 2.6773206e-32], sum to 1.0000
[2019-03-27 12:01:02,792] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7890
[2019-03-27 12:01:02,798] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3949073917860469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 589260.5812168231, 589260.5812168224, 173549.8204274147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2676600.0000, 
sim time next is 2677200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3950110177672582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589415.3091307866, 589415.3091307866, 173564.0042775566], 
processed observation next is [0.0, 1.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27109761176778097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16372647475855182, 0.16372647475855182, 0.2590507526530696], 
reward next is 0.7409, 
noisyNet noise sample is [array([-1.7203695], dtype=float32), -1.2501894]. 
=============================================
[2019-03-27 12:01:09,601] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1048601e-25 1.0000000e+00 1.6306735e-33 6.6927209e-29 2.4838635e-33], sum to 1.0000
[2019-03-27 12:01:09,611] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2085
[2019-03-27 12:01:09,619] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 98.0, 1.0, 2.0, 0.3659199661941329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556163.915749069, 556163.9157490697, 170946.395624512], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2756400.0000, 
sim time next is 2757000.0000, 
raw observation next is [22.0, 99.0, 1.0, 2.0, 0.370312339705314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 560688.3996660832, 560688.3996660825, 171267.4939061284], 
processed observation next is [0.0, 0.9130434782608695, 0.2417061611374408, 0.99, 1.0, 1.0, 0.24134016831965543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1557467776850231, 0.1557467776850229, 0.25562312523302744], 
reward next is 0.7444, 
noisyNet noise sample is [array([-0.9562645], dtype=float32), -0.6939439]. 
=============================================
[2019-03-27 12:01:09,641] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.193535]
 [74.21194 ]
 [74.23673 ]
 [74.270386]
 [74.284935]], R is [[74.17901611]
 [74.18208313]
 [74.18554688]
 [74.18937683]
 [74.19351959]].
[2019-03-27 12:01:09,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9598816e-22 1.0000000e+00 4.9267021e-29 2.2043154e-22 2.6415655e-28], sum to 1.0000
[2019-03-27 12:01:09,988] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5884
[2019-03-27 12:01:09,995] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 83.0, 1.0, 2.0, 0.6100250177025361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 939134.7442926532, 939134.7442926537, 213163.2371544995], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2806200.0000, 
sim time next is 2806800.0000, 
raw observation next is [23.66666666666667, 83.0, 1.0, 2.0, 0.6701957018074916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1026873.463135121, 1026873.463135121, 225814.6117439364], 
processed observation next is [1.0, 0.4782608695652174, 0.3206951026856243, 0.83, 1.0, 1.0, 0.6026454238644476, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28524262864864475, 0.28524262864864475, 0.3370367339461737], 
reward next is 0.6630, 
noisyNet noise sample is [array([0.04135063], dtype=float32), 0.33801174]. 
=============================================
[2019-03-27 12:01:13,491] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2511069e-25 1.0000000e+00 7.5606070e-33 3.4125932e-26 1.5467739e-32], sum to 1.0000
[2019-03-27 12:01:13,493] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5986
[2019-03-27 12:01:13,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3359076187736589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517463.466596218, 517463.4665962186, 167998.4663704457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2785200.0000, 
sim time next is 2785800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3387719845445776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521876.3759363471, 521876.3759363471, 168348.8799461964], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2033397404151537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14496565998231864, 0.14496565998231864, 0.251266984994323], 
reward next is 0.7487, 
noisyNet noise sample is [array([0.21722837], dtype=float32), 0.38318458]. 
=============================================
[2019-03-27 12:01:15,958] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 12:01:15,959] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:01:15,960] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:01:15,960] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:01:15,961] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:01:15,962] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:01:15,963] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:01:15,963] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:01:15,964] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:01:15,967] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:01:15,967] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:01:15,979] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run21
[2019-03-27 12:01:15,980] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run21
[2019-03-27 12:01:15,981] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run21
[2019-03-27 12:01:16,017] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run21
[2019-03-27 12:01:16,054] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run21
[2019-03-27 12:01:25,238] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04806729], dtype=float32), 0.041203648]
[2019-03-27 12:01:25,242] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.03443443, 79.374680685, 1.0, 2.0, 0.1938042480489663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 324188.4679159505, 324188.4679159511, 147170.1800861565]
[2019-03-27 12:01:25,244] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:01:25,248] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0090089e-26 1.0000000e+00 6.4496565e-34 6.7837354e-30 6.4656473e-33], sampled 0.6117481565889669
[2019-03-27 12:01:38,723] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04806729], dtype=float32), 0.041203648]
[2019-03-27 12:01:38,724] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.88333333333333, 59.66666666666667, 1.0, 2.0, 0.2424906997286516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 401482.6462896516, 401482.6462896523, 160050.6489400008]
[2019-03-27 12:01:38,724] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:01:38,731] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8853554e-27 1.0000000e+00 5.4237200e-35 1.2754335e-30 4.8850505e-34], sampled 0.003988638756967644
[2019-03-27 12:01:54,520] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04806729], dtype=float32), 0.041203648]
[2019-03-27 12:01:54,520] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.53099333, 70.53236496, 1.0, 2.0, 0.6287755122710563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 878690.5163353384, 878690.5163353384, 206219.6012763892]
[2019-03-27 12:01:54,522] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:01:54,523] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6717904e-26 1.0000000e+00 2.0596261e-34 3.1112649e-29 1.7820985e-33], sampled 0.4688124530618447
[2019-03-27 12:02:15,977] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04806729], dtype=float32), 0.041203648]
[2019-03-27 12:02:15,978] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 74.0, 1.0, 2.0, 0.4922411215083137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687826.9787141343, 687826.9787141349, 182389.1445771308]
[2019-03-27 12:02:15,981] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:02:15,985] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.1482549e-27 1.0000000e+00 2.7438931e-35 1.3638127e-29 2.9981562e-34], sampled 0.7972291654846799
[2019-03-27 12:02:23,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04806729], dtype=float32), 0.041203648]
[2019-03-27 12:02:23,216] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.33333333333333, 87.33333333333333, 1.0, 2.0, 0.8885527853180173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1241931.931518586, 1241931.931518586, 266811.2113011674]
[2019-03-27 12:02:23,216] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:02:23,220] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8028025e-24 1.0000000e+00 8.6624988e-32 1.2968866e-25 7.0165272e-31], sampled 0.839560555435873
[2019-03-27 12:02:52,152] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04806729], dtype=float32), 0.041203648]
[2019-03-27 12:02:52,156] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.08333333333334, 93.16666666666667, 1.0, 2.0, 0.5079835403645322, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709831.8005479224, 709831.800547923, 184857.4737637946]
[2019-03-27 12:02:52,160] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:02:52,163] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4214351e-27 1.0000000e+00 1.4635659e-35 2.1511655e-31 1.1488850e-34], sampled 0.34868357044103726
[2019-03-27 12:02:52,569] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04806729], dtype=float32), 0.041203648]
[2019-03-27 12:02:52,570] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.1, 80.0, 1.0, 2.0, 0.9209303220907892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1287213.509919166, 1287213.509919166, 275761.9618564625]
[2019-03-27 12:02:52,572] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:02:52,575] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.6751339e-25 1.0000000e+00 9.3163232e-33 1.7866723e-26 9.4350314e-32], sampled 0.5886398072503076
[2019-03-27 12:03:17,928] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04806729], dtype=float32), 0.041203648]
[2019-03-27 12:03:17,928] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.03333333333333, 76.66666666666667, 1.0, 2.0, 0.5455984745167448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 762411.9656972806, 762411.9656972799, 191048.4754766359]
[2019-03-27 12:03:17,931] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:03:17,934] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2989136e-28 1.0000000e+00 6.9223158e-36 1.8260265e-30 7.8126896e-35], sampled 0.9572756124651407
[2019-03-27 12:03:22,991] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-27 12:03:23,389] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 12:03:23,396] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-03-27 12:03:23,585] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 12:03:23,610] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-27 12:03:24,625] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 500000, evaluation results [500000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 12:03:25,467] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0125785e-24 1.0000000e+00 7.1226343e-34 6.4592328e-27 5.2353616e-33], sum to 1.0000
[2019-03-27 12:03:25,479] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7280
[2019-03-27 12:03:25,486] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.412121919392743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607224.7099600784, 607224.7099600784, 174988.1797634892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2831400.0000, 
sim time next is 2832000.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4140642209017884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610085.4948599548, 610085.4948599555, 175257.3911826799], 
processed observation next is [1.0, 0.782608695652174, 0.3364928909952607, 0.89, 1.0, 1.0, 0.29405327819492577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1694681930166541, 0.1694681930166543, 0.2615781957950446], 
reward next is 0.7384, 
noisyNet noise sample is [array([-0.62166315], dtype=float32), -2.5505254]. 
=============================================
[2019-03-27 12:03:25,507] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.48655]
 [74.28834]
 [74.15768]
 [73.95223]
 [73.64253]], R is [[74.5233078 ]
 [74.51689911]
 [74.51094818]
 [74.50543976]
 [74.50106812]].
[2019-03-27 12:03:35,677] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7301120e-27 1.0000000e+00 1.4664672e-34 4.9928675e-28 5.8955982e-34], sum to 1.0000
[2019-03-27 12:03:35,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2504
[2019-03-27 12:03:35,696] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.4746813606329229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749892.0029466772, 749892.0029466777, 189683.0792734421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2982600.0000, 
sim time next is 2983200.0000, 
raw observation next is [20.66666666666666, 96.0, 1.0, 2.0, 0.4702696594994465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744623.993023068, 744623.9930230685, 189088.5951039933], 
processed observation next is [1.0, 0.5217391304347826, 0.17851500789889393, 0.96, 1.0, 1.0, 0.3617706740957187, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20683999806196332, 0.2068399980619635, 0.28222178373730344], 
reward next is 0.7178, 
noisyNet noise sample is [array([1.282939], dtype=float32), 0.46404427]. 
=============================================
[2019-03-27 12:03:40,662] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9091149e-25 1.0000000e+00 2.2550952e-34 1.6120555e-27 1.7239646e-33], sum to 1.0000
[2019-03-27 12:03:40,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9872
[2019-03-27 12:03:40,683] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4874377884848182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681112.9495931175, 681112.9495931175, 181651.1779037188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3195000.0000, 
sim time next is 3195600.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4862582826163963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679464.2599147927, 679464.259914792, 181471.0371673381], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.38103407544144136, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18874007219855354, 0.18874007219855335, 0.2708522942796091], 
reward next is 0.7291, 
noisyNet noise sample is [array([1.1508157], dtype=float32), 1.733597]. 
=============================================
[2019-03-27 12:03:45,726] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9921309e-18 1.0000000e+00 1.2173599e-25 2.0461754e-16 2.6240052e-24], sum to 1.0000
[2019-03-27 12:03:45,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0519
[2019-03-27 12:03:45,746] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.9502795170227627, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564059738, 1328261.472091809, 1328261.472091809, 284138.5889850455], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3162000.0000, 
sim time next is 3162600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.9645210832580148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104049, 1348180.38500175, 1348180.38500175, 288298.1828114528], 
processed observation next is [1.0, 0.6086956521739131, 0.4312796208530806, 0.84, 1.0, 1.0, 0.9572543171783311, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451521746, 0.374494551389375, 0.374494551389375, 0.4302957952409743], 
reward next is 0.5697, 
noisyNet noise sample is [array([-0.24572805], dtype=float32), 1.1274985]. 
=============================================
[2019-03-27 12:03:51,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0036627e-27 1.0000000e+00 3.5624484e-35 7.5639282e-32 6.5279454e-35], sum to 1.0000
[2019-03-27 12:03:51,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4985
[2019-03-27 12:03:51,165] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.476802473447324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666247.2117308204, 666247.211730821, 180041.4710373539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3225600.0000, 
sim time next is 3226200.0000, 
raw observation next is [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.4784003462120729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668480.6591032878, 668480.6591032878, 180281.2126089916], 
processed observation next is [0.0, 0.34782608695652173, 0.4865718799368086, 0.7816666666666667, 1.0, 1.0, 0.37156668218322036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1856890719731355, 0.1856890719731355, 0.2690764367298382], 
reward next is 0.7309, 
noisyNet noise sample is [array([-0.84669626], dtype=float32), 0.5008626]. 
=============================================
[2019-03-27 12:03:53,942] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.39046426e-26 1.00000000e+00 6.76299920e-34 6.21877483e-28
 1.07245764e-32], sum to 1.0000
[2019-03-27 12:03:53,953] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2490
[2019-03-27 12:03:53,956] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 74.5, 1.0, 2.0, 0.5357670944871656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748668.886054139, 748668.886054139, 189390.4782181815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [29.66666666666667, 73.0, 1.0, 2.0, 0.5341717959314043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746438.8684780421, 746438.8684780427, 189123.865751679], 
processed observation next is [0.0, 0.43478260869565216, 0.6050552922590839, 0.73, 1.0, 1.0, 0.4387611999173545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20734413013278946, 0.20734413013278963, 0.2822744264950433], 
reward next is 0.7177, 
noisyNet noise sample is [array([-0.28895167], dtype=float32), -0.2337117]. 
=============================================
[2019-03-27 12:04:04,066] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6620615e-12 9.9992907e-01 8.6150861e-18 7.0917617e-05 9.6049173e-19], sum to 1.0000
[2019-03-27 12:04:04,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8445
[2019-03-27 12:04:04,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2875629.623502699 W.
[2019-03-27 12:04:04,094] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 61.5, 1.0, 2.0, 1.027967037324676, 1.0, 2.0, 1.027967037324676, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2875629.623502699, 2875629.623502699, 546120.2240069328], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3421800.0000, 
sim time next is 3422400.0000, 
raw observation next is [34.0, 61.0, 1.0, 2.0, 0.7333007214671473, 1.0, 2.0, 0.6872404002478363, 1.0, 1.0, 1.03, 7.005100358033808, 6.9112, 170.5573041426782, 2883733.40499891, 2816468.75891694, 532053.8086713697], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.61, 1.0, 1.0, 0.6786755680327076, 1.0, 1.0, 0.623181205117875, 1.0, 0.5, 1.0365853658536586, 0.00939003580338076, 0.0, 0.8375144448122397, 0.8010370569441417, 0.7823524330324834, 0.7941101621960741], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9821518], dtype=float32), 0.8982801]. 
=============================================
[2019-03-27 12:04:08,031] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7817184e-09 5.4151493e-01 1.2557167e-15 4.5848507e-01 1.5735076e-16], sum to 1.0000
[2019-03-27 12:04:08,041] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7082
[2019-03-27 12:04:08,045] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.9372368636372326, 1.0, 1.0, 0.9372368636372326, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2621555.204485043, 2621555.204485043, 492217.2434753962], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3509400.0000, 
sim time next is 3510000.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.8476794240340053, 1.0, 2.0, 0.8476794240340053, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2370815.488261505, 2370815.488261504, 443756.4944376317], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.63, 1.0, 1.0, 0.81648123377591, 1.0, 1.0, 0.81648123377591, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.658559857850418, 0.6585598578504178, 0.6623231260263159], 
reward next is 0.3377, 
noisyNet noise sample is [array([-0.28104404], dtype=float32), 0.7178991]. 
=============================================
[2019-03-27 12:04:08,063] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[48.036076]
 [48.601295]
 [49.66863 ]
 [49.290188]
 [48.819397]], R is [[49.043396  ]
 [48.55296326]
 [48.0674324 ]
 [47.90050888]
 [47.74026489]].
[2019-03-27 12:04:08,189] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7409334e-24 1.0000000e+00 1.1529215e-31 2.0140339e-27 5.7758225e-31], sum to 1.0000
[2019-03-27 12:04:08,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3201
[2019-03-27 12:04:08,205] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5163119925786053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721473.5317996818, 721473.5317996818, 186191.4310685439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3444000.0000, 
sim time next is 3444600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5171221407009972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722605.9851274666, 722605.9851274659, 186322.3274356583], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4182194466277074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20072388475762962, 0.20072388475762942, 0.2780930260233706], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.348382], dtype=float32), 0.32995793]. 
=============================================
[2019-03-27 12:04:10,680] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2588647e-24 1.0000000e+00 3.4348638e-32 3.7135858e-27 2.2818466e-31], sum to 1.0000
[2019-03-27 12:04:10,687] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9225
[2019-03-27 12:04:10,695] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.5234008158471993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731382.5773714219, 731382.5773714213, 187343.8294250153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3615600.0000, 
sim time next is 3616200.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.5245965319221539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733054.0070592929, 733054.0070592923, 187539.6007289533], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.74, 1.0, 1.0, 0.4272247372556071, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2036261130720258, 0.20362611307202563, 0.27990985183425865], 
reward next is 0.7201, 
noisyNet noise sample is [array([-1.3545038], dtype=float32), 0.98696727]. 
=============================================
[2019-03-27 12:04:20,811] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0371027e-23 1.0000000e+00 1.0133250e-30 1.1454813e-25 9.5089684e-30], sum to 1.0000
[2019-03-27 12:04:20,829] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1958
[2019-03-27 12:04:20,835] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5262438974779616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735356.7785158858, 735356.7785158858, 187809.8021203926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3619800.0000, 
sim time next is 3620400.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5263584410286595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735516.8935091477, 735516.8935091484, 187828.5552171551], 
processed observation next is [1.0, 0.9130434782608695, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.4293475193116379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20431024819698546, 0.20431024819698565, 0.28034112718978377], 
reward next is 0.7197, 
noisyNet noise sample is [array([-0.9407833], dtype=float32), -0.09780831]. 
=============================================
[2019-03-27 12:04:22,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6500753e-21 1.0000000e+00 2.4527028e-29 1.7820473e-22 5.8712875e-28], sum to 1.0000
[2019-03-27 12:04:22,504] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7821
[2019-03-27 12:04:22,512] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6202235523851146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866734.5923844696, 866734.5923844696, 204564.513677921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3649800.0000, 
sim time next is 3650400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6238818424582347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 871848.9869346417, 871848.9869346417, 205269.7875840174], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5468455933231744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2421802741485116, 0.2421802741485116, 0.3063728172895782], 
reward next is 0.6936, 
noisyNet noise sample is [array([-1.2387205], dtype=float32), -0.28328145]. 
=============================================
[2019-03-27 12:04:26,932] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8065689e-25 1.0000000e+00 4.9748883e-33 1.0225211e-28 2.9578541e-32], sum to 1.0000
[2019-03-27 12:04:26,941] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0997
[2019-03-27 12:04:26,947] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 77.33333333333334, 1.0, 2.0, 0.514956382111307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719578.6148643068, 719578.6148643061, 185972.2252876279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3705600.0000, 
sim time next is 3706200.0000, 
raw observation next is [28.0, 76.5, 1.0, 2.0, 0.5105657294396888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713441.2392319824, 713441.2392319817, 185267.6695008594], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.765, 1.0, 1.0, 0.4103201559514323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.198178122008884, 0.1981781220088838, 0.2765189097027752], 
reward next is 0.7235, 
noisyNet noise sample is [array([-0.03643519], dtype=float32), -0.44334894]. 
=============================================
[2019-03-27 12:04:27,272] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0196151e-21 1.0000000e+00 4.7691370e-28 3.2369696e-21 3.4785049e-27], sum to 1.0000
[2019-03-27 12:04:27,280] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7901
[2019-03-27 12:04:27,288] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 72.0, 1.0, 2.0, 0.787689112228807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1100881.498825694, 1100881.498825694, 240908.6796724935], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3742200.0000, 
sim time next is 3742800.0000, 
raw observation next is [28.66666666666666, 71.33333333333333, 1.0, 2.0, 0.7662399748439855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1070888.872574917, 1070888.872574916, 235790.1693665746], 
processed observation next is [1.0, 0.30434782608695654, 0.5576619273301735, 0.7133333333333333, 1.0, 1.0, 0.7183614154746814, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2974691312708103, 0.29746913127081004, 0.3519256259202606], 
reward next is 0.6481, 
noisyNet noise sample is [array([1.7731422], dtype=float32), 0.4608576]. 
=============================================
[2019-03-27 12:04:27,406] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.0935504e-24 1.0000000e+00 3.0697403e-31 3.8131182e-23 3.7070695e-30], sum to 1.0000
[2019-03-27 12:04:27,413] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4248
[2019-03-27 12:04:27,422] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4904130667426979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685271.7453175467, 685271.7453175472, 182107.3996066342], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3712800.0000, 
sim time next is 3713400.0000, 
raw observation next is [27.5, 76.5, 1.0, 2.0, 0.4905703973505663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 685491.6599443932, 685491.6599443926, 182131.5274935262], 
processed observation next is [1.0, 1.0, 0.5023696682464456, 0.765, 1.0, 1.0, 0.3862293943982726, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19041434998455367, 0.1904143499845535, 0.27183810073660625], 
reward next is 0.7282, 
noisyNet noise sample is [array([1.3608778], dtype=float32), 0.6120323]. 
=============================================
[2019-03-27 12:04:30,534] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 12:04:30,536] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:04:30,537] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:04:30,537] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:04:30,538] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:04:30,539] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:04:30,540] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:04:30,541] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:04:30,542] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:04:30,543] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:04:30,542] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:04:30,561] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run22
[2019-03-27 12:04:30,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run22
[2019-03-27 12:04:30,583] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run22
[2019-03-27 12:04:30,621] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run22
[2019-03-27 12:04:30,639] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run22
[2019-03-27 12:05:05,406] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04570151], dtype=float32), 0.038717955]
[2019-03-27 12:05:05,407] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 76.0, 1.0, 2.0, 0.5622836267046358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785736.1964479298, 785736.1964479298, 193930.428104229]
[2019-03-27 12:05:05,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:05:05,411] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4736062e-26 1.0000000e+00 1.8568599e-34 4.5678319e-31 1.7201150e-33], sampled 0.8478449743630626
[2019-03-27 12:05:40,304] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04570151], dtype=float32), 0.038717955]
[2019-03-27 12:05:40,305] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.3, 65.0, 1.0, 2.0, 0.9727762560249966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956502753, 1359726.622369285, 1359726.622369285, 290741.7186665248]
[2019-03-27 12:05:40,307] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:05:40,311] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.8449557e-23 1.0000000e+00 2.3596309e-30 3.7904757e-25 1.8314759e-29], sampled 0.6191195499227933
[2019-03-27 12:06:24,372] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04570151], dtype=float32), 0.038717955]
[2019-03-27 12:06:24,372] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.56666666666667, 77.16666666666667, 1.0, 2.0, 0.5716659712709602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798852.0326710424, 798852.0326710424, 195585.3760171793]
[2019-03-27 12:06:24,373] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:06:24,376] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.3806697e-26 1.0000000e+00 6.7668701e-34 1.5636717e-30 6.1545666e-33], sampled 0.4615431326201834
[2019-03-27 12:06:37,966] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6189 2779204731.3561 933.0000
[2019-03-27 12:06:38,405] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3957 2927346002.7203 1338.0000
[2019-03-27 12:06:38,457] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.5021 3164241877.9541 1777.0000
[2019-03-27 12:06:38,972] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.3805 2842571050.0819 1131.0000
[2019-03-27 12:06:39,006] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4791 3007658023.9129 1766.0000
[2019-03-27 12:06:40,022] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 525000, evaluation results [525000.0, 7883.502076901241, 3164241877.95411, 1777.0, 8254.395746196056, 2927346002.720325, 1338.0, 8660.61889638982, 2779204731.3560643, 933.0, 7997.4790539186915, 3007658023.9129057, 1766.0, 8495.380506668027, 2842571050.0818634, 1131.0]
[2019-03-27 12:06:56,532] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7426527e-09 8.4072340e-01 1.3002474e-14 1.5927659e-01 3.4096579e-15], sum to 1.0000
[2019-03-27 12:06:56,546] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5049
[2019-03-27 12:06:56,554] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2629885.512727182 W.
[2019-03-27 12:06:56,562] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.33333333333334, 63.16666666666666, 1.0, 2.0, 0.940211915807102, 1.0, 2.0, 0.940211915807102, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2629885.512727182, 2629885.512727183, 493910.2406613954], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4032600.0000, 
sim time next is 4033200.0000, 
raw observation next is [32.66666666666667, 66.33333333333334, 1.0, 2.0, 0.9336625973962973, 1.0, 2.0, 0.9336625973962973, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2611547.13762997, 2611547.13762997, 490195.0455982208], 
processed observation next is [1.0, 0.6956521739130435, 0.7472353870458138, 0.6633333333333334, 1.0, 1.0, 0.9200754185497558, 1.0, 1.0, 0.9200754185497558, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7254297604527694, 0.7254297604527694, 0.7316343964152549], 
reward next is 0.2684, 
noisyNet noise sample is [array([-1.6583493], dtype=float32), 1.7512814]. 
=============================================
[2019-03-27 12:06:58,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4715173e-18 1.0000000e+00 1.8256481e-25 4.8507918e-18 1.1159075e-24], sum to 1.0000
[2019-03-27 12:06:58,021] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6815
[2019-03-27 12:06:58,028] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.5409512594049811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755915.6991435413, 755915.6991435406, 190264.567671704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4037400.0000, 
sim time next is 4038000.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5521428835730458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771560.3562344639, 771560.3562344644, 192171.6727590644], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.79, 1.0, 1.0, 0.4604131127386093, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21432232117623995, 0.21432232117624012, 0.286823392177708], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.08749992], dtype=float32), 0.25436893]. 
=============================================
[2019-03-27 12:06:58,037] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[50.14018 ]
 [47.494923]
 [42.551334]
 [37.843338]
 [36.174854]], R is [[51.99259186]
 [52.18869019]
 [51.66680527]
 [51.77373886]
 [51.61681366]].
[2019-03-27 12:07:00,857] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3000255e-11 9.9954551e-01 1.2220168e-17 4.5444217e-04 2.0596673e-16], sum to 1.0000
[2019-03-27 12:07:00,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3566
[2019-03-27 12:07:00,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2697989.445068125 W.
[2019-03-27 12:07:00,885] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.66666666666666, 73.66666666666666, 1.0, 2.0, 0.9645335313050727, 1.0, 2.0, 0.9645335313050727, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2697989.445068125, 2697989.445068125, 507929.1396588092], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4099200.0000, 
sim time next is 4099800.0000, 
raw observation next is [31.83333333333333, 72.33333333333334, 1.0, 2.0, 0.9964682296107795, 1.0, 2.0, 0.9964682296107795, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2787416.714284982, 2787416.714284981, 526860.2211029385], 
processed observation next is [1.0, 0.43478260869565216, 0.7077409162717218, 0.7233333333333334, 1.0, 1.0, 0.9957448549527464, 1.0, 1.0, 0.9957448549527464, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7742824206347172, 0.774282420634717, 0.7863585389596097], 
reward next is 0.2136, 
noisyNet noise sample is [array([0.9321044], dtype=float32), -0.909529]. 
=============================================
[2019-03-27 12:07:03,308] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1872938e-10 7.7955836e-01 6.4748077e-16 2.2044155e-01 8.2290675e-16], sum to 1.0000
[2019-03-27 12:07:03,315] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6837
[2019-03-27 12:07:03,321] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3386326.188248825 W.
[2019-03-27 12:07:03,327] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.5, 67.0, 1.0, 2.0, 0.9725283991545608, 1.0, 2.0, 0.8068542390915429, 1.0, 1.0, 1.03, 7.005119230613124, 6.9112, 170.5573041426782, 3386326.188248825, 3319048.022970603, 621242.9693259349], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4120200.0000, 
sim time next is 4120800.0000, 
raw observation next is [34.33333333333333, 67.0, 1.0, 2.0, 0.818545113236075, 1.0, 2.0, 0.7298625961323001, 1.0, 2.0, 1.03, 7.005107081190268, 6.9112, 170.5573041426782, 3062799.924054031, 2995530.461901608, 561387.5378096575], 
processed observation next is [1.0, 0.6956521739130435, 0.8262243285939966, 0.67, 1.0, 1.0, 0.7813796545012952, 1.0, 1.0, 0.6745332483521688, 1.0, 1.0, 1.0365853658536586, 0.009390708119026758, 0.0, 0.8375144448122397, 0.8507777566816752, 0.8320917949726688, 0.8378918474771008], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8130777], dtype=float32), -0.37710372]. 
=============================================
[2019-03-27 12:07:08,798] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5608787e-20 1.0000000e+00 5.0239110e-29 6.4305926e-19 1.1620993e-27], sum to 1.0000
[2019-03-27 12:07:08,806] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2853
[2019-03-27 12:07:08,813] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.5, 53.0, 1.0, 2.0, 0.552596270508102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104266, 772194.1460556478, 772194.1460556483, 192250.3639880458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4210200.0000, 
sim time next is 4210800.0000, 
raw observation next is [35.33333333333334, 54.0, 1.0, 2.0, 0.5594066550731059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781714.4315185265, 781714.4315185265, 193430.1404196138], 
processed observation next is [1.0, 0.7391304347826086, 0.8736176935229073, 0.54, 1.0, 1.0, 0.4691646446663927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21714289764403513, 0.21714289764403513, 0.28870170211882656], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.13985969], dtype=float32), 1.7728521]. 
=============================================
[2019-03-27 12:07:10,691] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.096981e-10 9.773230e-04 7.566719e-17 9.990227e-01 6.165657e-16], sum to 1.0000
[2019-03-27 12:07:10,701] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8223
[2019-03-27 12:07:10,712] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3560888.220781726 W.
[2019-03-27 12:07:10,717] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.33333333333334, 54.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.819706768753709, 6.9112, 170.5573041426782, 3560888.220781726, 2910087.866630892, 548476.8105712935], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4198800.0000, 
sim time next is 4199400.0000, 
raw observation next is [36.5, 53.5, 1.0, 2.0, 0.9383225202515203, 1.0, 2.0, 0.7897512996400228, 1.0, 1.0, 1.03, 7.005116531194243, 6.9112, 170.5573041426782, 3314450.75516655, 3247174.523591878, 607180.2647353306], 
processed observation next is [1.0, 0.6086956521739131, 0.9289099526066351, 0.535, 1.0, 1.0, 0.9256897834355666, 1.0, 1.0, 0.7466883128193046, 1.0, 0.5, 1.0365853658536586, 0.009391653119424337, 0.0, 0.8375144448122397, 0.9206807653240417, 0.901992923219966, 0.9062392010975083], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.67310506], dtype=float32), 0.33748302]. 
=============================================
[2019-03-27 12:07:12,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3749945e-25 1.0000000e+00 3.0860812e-33 1.3518135e-26 1.6110923e-32], sum to 1.0000
[2019-03-27 12:07:12,401] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9497
[2019-03-27 12:07:12,410] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 73.66666666666667, 1.0, 2.0, 0.635418770645873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887978.1050642777, 887978.1050642777, 207530.7629131568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4220400.0000, 
sim time next is 4221000.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.6333670517561034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 885109.6965596054, 885109.696559606, 207127.5277510237], 
processed observation next is [1.0, 0.8695652173913043, 0.7156398104265403, 0.75, 1.0, 1.0, 0.5582735563326547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2458638045998904, 0.24586380459989055, 0.3091455638074981], 
reward next is 0.6909, 
noisyNet noise sample is [array([-1.3997732], dtype=float32), -0.7699417]. 
=============================================
[2019-03-27 12:07:12,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.640236]
 [64.60674 ]
 [64.782555]
 [64.65672 ]
 [64.58358 ]], R is [[64.79434204]
 [64.83665466]
 [64.87830353]
 [64.92076111]
 [64.96496582]].
[2019-03-27 12:07:23,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2065594e-24 1.0000000e+00 2.0763682e-32 4.6200237e-29 3.1707625e-31], sum to 1.0000
[2019-03-27 12:07:23,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4827
[2019-03-27 12:07:23,312] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 81.5, 1.0, 2.0, 0.6199593662311755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866365.2535100605, 866365.2535100605, 204523.6742513632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4401000.0000, 
sim time next is 4401600.0000, 
raw observation next is [30.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6188710837720963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864843.8086701048, 864843.8086701054, 204314.6587111072], 
processed observation next is [1.0, 0.9565217391304348, 0.6366508688783573, 0.8233333333333335, 1.0, 1.0, 0.5408085346651763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24023439129725135, 0.2402343912972515, 0.30494725180762267], 
reward next is 0.6951, 
noisyNet noise sample is [array([-0.5344457], dtype=float32), -1.0589883]. 
=============================================
[2019-03-27 12:07:32,450] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1607333e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0768688e-38], sum to 1.0000
[2019-03-27 12:07:32,466] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7481
[2019-03-27 12:07:32,477] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5079816534180683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709829.1629385987, 709829.1629385987, 184855.5815085197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4494600.0000, 
sim time next is 4495200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5088732889015167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711075.5085487235, 711075.508548723, 184997.4550269174], 
processed observation next is [0.0, 0.0, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4082810709656828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19752097459686765, 0.1975209745968675, 0.27611560451778716], 
reward next is 0.7239, 
noisyNet noise sample is [array([0.3038854], dtype=float32), -0.5115373]. 
=============================================
[2019-03-27 12:07:45,858] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 12:07:45,860] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:07:45,861] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:07:45,863] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:07:45,865] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:07:45,862] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:07:45,864] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:07:45,866] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:07:45,867] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:07:45,871] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:07:45,869] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:07:45,895] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run23
[2019-03-27 12:07:45,917] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run23
[2019-03-27 12:07:45,918] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run23
[2019-03-27 12:07:45,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run23
[2019-03-27 12:07:45,955] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run23
[2019-03-27 12:08:51,453] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04756491], dtype=float32), 0.038358714]
[2019-03-27 12:08:51,454] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.73726115833333, 88.77549087166668, 1.0, 2.0, 0.5159767291541808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721004.8892341473, 721004.8892341478, 186135.457843699]
[2019-03-27 12:08:51,455] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:08:51,458] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0443060e-28 1.0000000e+00 1.4483024e-37 3.0708820e-35 4.7413275e-36], sampled 0.26732715816290564
[2019-03-27 12:09:04,159] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04756491], dtype=float32), 0.038358714]
[2019-03-27 12:09:04,160] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.7, 55.0, 1.0, 2.0, 0.9774247523582243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1366228.375853533, 1366228.375853533, 292129.1647196301]
[2019-03-27 12:09:04,162] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:09:04,165] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4380782e-27 1.0000000e+00 3.5492482e-36 1.9272902e-33 1.3401323e-34], sampled 0.5333117892855472
[2019-03-27 12:09:07,942] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04756491], dtype=float32), 0.038358714]
[2019-03-27 12:09:07,945] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.838360605, 81.09328178999999, 1.0, 2.0, 0.5285875057582895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738632.8021077587, 738632.8021077581, 188196.0705891772]
[2019-03-27 12:09:07,948] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:09:07,951] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0795367e-27 1.0000000e+00 2.0046196e-36 3.4522146e-33 7.6383491e-35], sampled 0.42776396899626057
[2019-03-27 12:09:16,969] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04756491], dtype=float32), 0.038358714]
[2019-03-27 12:09:16,970] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.15983631, 95.04859486666666, 1.0, 2.0, 0.534956470067527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747535.7391392752, 747535.7391392757, 189254.3797267981]
[2019-03-27 12:09:16,973] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:09:16,976] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.050605e-27 1.000000e+00 8.667505e-36 3.354553e-32 2.804906e-34], sampled 0.2500192816530893
[2019-03-27 12:09:27,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04756491], dtype=float32), 0.038358714]
[2019-03-27 12:09:27,581] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.11666666666667, 76.0, 1.0, 2.0, 0.5806497095865053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811410.798091635, 811410.798091635, 197195.938762238]
[2019-03-27 12:09:27,582] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:09:27,588] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.4524571e-28 1.0000000e+00 1.0104114e-36 3.7393306e-34 3.3266780e-35], sampled 0.5636877247622847
[2019-03-27 12:09:39,504] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04756491], dtype=float32), 0.038358714]
[2019-03-27 12:09:39,505] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.92052297666667, 62.69174727333334, 1.0, 2.0, 0.6243925390618335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 891681.5607427782, 891681.5607427782, 207858.1869627521]
[2019-03-27 12:09:39,506] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:09:39,509] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.6607622e-26 1.0000000e+00 1.7861880e-34 1.2326273e-31 4.2332608e-33], sampled 0.7770496364687173
[2019-03-27 12:09:45,174] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04756491], dtype=float32), 0.038358714]
[2019-03-27 12:09:45,174] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.1, 64.0, 1.0, 2.0, 0.4252759770704103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622855.0853405495, 622855.0853405495, 176374.3981227961]
[2019-03-27 12:09:45,176] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:09:45,180] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1525582e-28 1.0000000e+00 2.7529488e-37 1.1640201e-35 1.0006118e-35], sampled 0.43272656420612543
[2019-03-27 12:09:52,941] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6679 2927328479.9021 1338.0000
[2019-03-27 12:09:53,610] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4308 2779149198.4617 933.0000
[2019-03-27 12:09:53,790] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 12:09:53,851] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-27 12:09:53,917] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5735 3007648438.5815 1766.0000
[2019-03-27 12:09:54,933] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 550000, evaluation results [550000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8253.667895353341, 2927328479.9020863, 1338.0, 8661.430759135581, 2779149198.4616733, 933.0, 7997.573465804901, 3007648438.5815053, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 12:10:01,366] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7877237e-13 1.0000000e+00 9.4615035e-21 1.0896376e-09 9.7237537e-20], sum to 1.0000
[2019-03-27 12:10:01,378] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0612
[2019-03-27 12:10:01,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2131792.14719726 W.
[2019-03-27 12:10:01,396] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.66666666666666, 67.33333333333334, 1.0, 2.0, 0.8833934235216314, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.990311539217683, 6.9112, 168.9124856447903, 2131792.14719726, 2075667.854920994, 430075.5769594263], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4786800.0000, 
sim time next is 4787400.0000, 
raw observation next is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.7550531297074572, 1.0, 1.0, 0.7550531297074572, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2111522.383229103, 2111522.383229103, 398581.4336779327], 
processed observation next is [1.0, 0.391304347826087, 0.6603475513428123, 0.6666666666666665, 1.0, 1.0, 0.7048832888041653, 1.0, 0.5, 0.7048832888041653, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5865339953414175, 0.5865339953414175, 0.5948976622058697], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2149339], dtype=float32), -1.6410059]. 
=============================================
[2019-03-27 12:10:02,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5008290e-22 1.0000000e+00 2.5462467e-29 4.0611013e-24 2.7429389e-29], sum to 1.0000
[2019-03-27 12:10:02,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4944
[2019-03-27 12:10:02,816] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.7134894593213367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 997130.6984637955, 997130.6984637949, 223782.1395880468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4853400.0000, 
sim time next is 4854000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.6306835636937241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 881358.0539590361, 881358.0539590361, 206593.155795314], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.84, 1.0, 1.0, 0.5550404381852098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2448216816552878, 0.2448216816552878, 0.30834799372434923], 
reward next is 0.6917, 
noisyNet noise sample is [array([-0.7039538], dtype=float32), -0.01607419]. 
=============================================
[2019-03-27 12:10:02,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[59.69675 ]
 [60.393284]
 [60.344456]
 [60.28654 ]
 [60.109676]], R is [[59.87525177]
 [59.94249725]
 [60.03979492]
 [60.13534164]
 [60.23077393]].
[2019-03-27 12:10:03,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6493023e-23 1.0000000e+00 1.0468597e-31 1.7023404e-25 5.3530865e-30], sum to 1.0000
[2019-03-27 12:10:03,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6885
[2019-03-27 12:10:03,373] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.5002188123307187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698978.1728208894, 698978.1728208888, 183630.387314573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4820400.0000, 
sim time next is 4821000.0000, 
raw observation next is [28.83333333333334, 70.66666666666667, 1.0, 2.0, 0.4989461563590215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697199.2499855738, 697199.2499855738, 183431.1141320092], 
processed observation next is [1.0, 0.8260869565217391, 0.5655608214849924, 0.7066666666666667, 1.0, 1.0, 0.3963206703120741, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19366645832932605, 0.19366645832932605, 0.2737777822865809], 
reward next is 0.7262, 
noisyNet noise sample is [array([0.87828434], dtype=float32), -0.3530371]. 
=============================================
[2019-03-27 12:10:03,387] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[58.638714]
 [58.63204 ]
 [58.041885]
 [57.688633]
 [57.464268]], R is [[59.21822357]
 [59.35196686]
 [59.48383713]
 [59.61400604]
 [59.74235153]].
[2019-03-27 12:10:10,173] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0404447e-26 1.0000000e+00 1.0199079e-34 1.5982324e-31 1.2307507e-34], sum to 1.0000
[2019-03-27 12:10:10,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8164
[2019-03-27 12:10:10,188] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 72.0, 1.0, 2.0, 0.4948729272264979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691505.6965093168, 691505.6965093162, 182796.6702290063], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4908600.0000, 
sim time next is 4909200.0000, 
raw observation next is [28.33333333333333, 72.66666666666666, 1.0, 2.0, 0.493160040738451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689111.4355003275, 689111.4355003275, 182531.3177667337], 
processed observation next is [1.0, 0.8260869565217391, 0.541864139020537, 0.7266666666666666, 1.0, 1.0, 0.3893494466728325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19141984319453542, 0.19141984319453542, 0.272434802636916], 
reward next is 0.7276, 
noisyNet noise sample is [array([0.67605627], dtype=float32), -0.4800853]. 
=============================================
[2019-03-27 12:10:13,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4651647e-25 1.0000000e+00 8.1923228e-33 5.7103428e-30 8.6383061e-33], sum to 1.0000
[2019-03-27 12:10:13,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7463
[2019-03-27 12:10:13,922] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5169605399671015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722380.0938837458, 722380.0938837464, 186296.2021665199], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4996800.0000, 
sim time next is 4997400.0000, 
raw observation next is [27.83333333333334, 79.00000000000001, 1.0, 2.0, 0.513481379071649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717516.8109010714, 717516.8109010708, 185735.2194554481], 
processed observation next is [1.0, 0.8695652173913043, 0.5181674565560824, 0.7900000000000001, 1.0, 1.0, 0.41383298683331204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19931022525029762, 0.19931022525029746, 0.2772167454558927], 
reward next is 0.7228, 
noisyNet noise sample is [array([-0.61878043], dtype=float32), -0.33001548]. 
=============================================
[2019-03-27 12:10:19,855] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9970041e-27 1.0000000e+00 3.4871000e-35 2.0182377e-30 3.2934479e-33], sum to 1.0000
[2019-03-27 12:10:19,862] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6203
[2019-03-27 12:10:19,869] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 69.0, 1.0, 2.0, 0.5190845397182875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725349.0999853517, 725349.0999853524, 186640.1265021333], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5043000.0000, 
sim time next is 5043600.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.512187673146058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715708.4328341713, 715708.4328341719, 185527.516551328], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.41227430499525053, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19880789800949203, 0.1988078980094922, 0.2769067411213851], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.23088849], dtype=float32), -1.4526521]. 
=============================================
[2019-03-27 12:10:22,122] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.9810505e-26 1.0000000e+00 2.2082479e-35 1.3311619e-31 8.7968230e-34], sum to 1.0000
[2019-03-27 12:10:22,131] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2457
[2019-03-27 12:10:22,134] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5103689724725529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713166.2077364186, 713166.2077364192, 185236.5954961804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5132400.0000, 
sim time next is 5133000.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5102681719364942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713025.3064145406, 713025.3064145413, 185220.5000598621], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.40996165293553516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19806258511515018, 0.19806258511515037, 0.276448507552033], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.82566667], dtype=float32), 0.3125483]. 
=============================================
[2019-03-27 12:10:22,142] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.30598 ]
 [70.19264 ]
 [70.07503 ]
 [69.945915]
 [69.929634]], R is [[70.43013   ]
 [70.44935608]
 [70.46836853]
 [70.48679352]
 [70.50239563]].
[2019-03-27 12:10:30,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3076789e-16 1.0000000e+00 2.0723410e-24 1.8524348e-15 1.8308308e-24], sum to 1.0000
[2019-03-27 12:10:30,369] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8317
[2019-03-27 12:10:30,377] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.15, 57.5, 1.0, 2.0, 0.5751576915604564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104266, 803733.24536951, 803733.24536951, 196212.4904268641], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5333400.0000, 
sim time next is 5334000.0000, 
raw observation next is [34.9, 58.66666666666667, 1.0, 2.0, 0.5801915235347771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810770.2759365819, 810770.2759365826, 197116.7241340523], 
processed observation next is [1.0, 0.7391304347826086, 0.8530805687203791, 0.5866666666666667, 1.0, 1.0, 0.4942066548611772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2252139655379394, 0.2252139655379396, 0.2942040658717198], 
reward next is 0.7058, 
noisyNet noise sample is [array([-0.8137517], dtype=float32), -0.33561298]. 
=============================================
[2019-03-27 12:10:30,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[47.900314]
 [42.28678 ]
 [34.46193 ]
 [27.159927]
 [28.15051 ]], R is [[51.95495224]
 [52.14254761]
 [52.32597351]
 [52.33775711]
 [51.81438065]].
[2019-03-27 12:10:48,758] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.4396085e-18 1.0000000e+00 4.6430874e-26 2.7418410e-21 1.4745960e-26], sum to 1.0000
[2019-03-27 12:10:48,773] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2021
[2019-03-27 12:10:48,779] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.7, 56.33333333333333, 1.0, 2.0, 0.5287573611532542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738870.2356439201, 738870.2356439201, 188227.195097816], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5506800.0000, 
sim time next is 5507400.0000, 
raw observation next is [33.45, 57.66666666666667, 1.0, 2.0, 0.5341408854567207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746395.6597458447, 746395.6597458441, 189120.6253001247], 
processed observation next is [1.0, 0.7391304347826086, 0.7843601895734599, 0.5766666666666667, 1.0, 1.0, 0.4387239583815911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20733212770717907, 0.2073321277071789, 0.2822695900001861], 
reward next is 0.7177, 
noisyNet noise sample is [array([-2.043245], dtype=float32), -1.9721593]. 
=============================================
[2019-03-27 12:10:53,594] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7806399e-24 1.0000000e+00 7.7629326e-34 1.1205318e-31 8.7019400e-33], sum to 1.0000
[2019-03-27 12:10:53,605] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6428
[2019-03-27 12:10:53,610] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 86.0, 1.0, 2.0, 0.5559537743103935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776887.6140542724, 776887.6140542724, 192826.8087903438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5524800.0000, 
sim time next is 5525400.0000, 
raw observation next is [27.73333333333333, 86.5, 1.0, 2.0, 0.5526799677453489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772311.1465064195, 772311.1465064195, 192261.2456072681], 
processed observation next is [1.0, 0.9565217391304348, 0.513428120063191, 0.865, 1.0, 1.0, 0.46106020210282994, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21453087402956098, 0.21453087402956098, 0.28695708299592254], 
reward next is 0.7130, 
noisyNet noise sample is [array([-1.1690487], dtype=float32), -0.5619449]. 
=============================================
[2019-03-27 12:10:54,379] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3251648e-22 1.0000000e+00 1.1839560e-30 3.1063956e-26 4.0461995e-29], sum to 1.0000
[2019-03-27 12:10:54,389] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0887
[2019-03-27 12:10:54,395] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.78333333333333, 91.66666666666667, 1.0, 2.0, 0.5448151806217578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761317.0090949184, 761317.0090949184, 190915.3531416722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5532600.0000, 
sim time next is 5533200.0000, 
raw observation next is [26.7, 92.0, 1.0, 2.0, 0.5432124238834983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759076.5392937129, 759076.5392937136, 190643.3372058023], 
processed observation next is [1.0, 0.043478260869565216, 0.46445497630331756, 0.92, 1.0, 1.0, 0.4496535227512028, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2108545942482536, 0.21085459424825378, 0.28454229433701833], 
reward next is 0.7155, 
noisyNet noise sample is [array([-0.40515164], dtype=float32), 0.18006602]. 
=============================================
[2019-03-27 12:10:55,344] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5250486e-18 1.0000000e+00 3.1162465e-26 8.0733918e-22 1.7008216e-25], sum to 1.0000
[2019-03-27 12:10:55,353] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4917
[2019-03-27 12:10:55,360] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.25, 84.0, 1.0, 2.0, 0.9526288836298518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1331547.379096038, 1331547.379096038, 284826.1462349914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5556600.0000, 
sim time next is 5557200.0000, 
raw observation next is [28.43333333333333, 83.33333333333334, 1.0, 2.0, 0.8754328606615432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1223583.621110263, 1223583.621110263, 263270.877240783], 
processed observation next is [1.0, 0.30434782608695654, 0.546603475513428, 0.8333333333333335, 1.0, 1.0, 0.849919109230775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33988433919729527, 0.33988433919729527, 0.3929416078220642], 
reward next is 0.6071, 
noisyNet noise sample is [array([0.56741226], dtype=float32), 0.57905006]. 
=============================================
[2019-03-27 12:11:00,911] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 12:11:00,911] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:11:00,912] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:11:00,912] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:11:00,914] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:11:00,916] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:11:00,917] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:11:00,918] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:11:00,920] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:11:00,920] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:11:00,925] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:11:00,942] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run24
[2019-03-27 12:11:00,943] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run24
[2019-03-27 12:11:00,961] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run24
[2019-03-27 12:11:01,007] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run24
[2019-03-27 12:11:01,009] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run24
[2019-03-27 12:12:55,768] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05256918], dtype=float32), 0.03819656]
[2019-03-27 12:12:55,770] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.40000000000001, 73.5, 1.0, 2.0, 0.8829044756675694, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005986706683963, 6.9112, 168.9123159659693, 2131107.772918255, 2063863.075893399, 429376.3734437163]
[2019-03-27 12:12:55,772] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:12:55,775] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.7855016e-17 1.0000000e+00 1.7584913e-24 4.1012570e-14 3.9673153e-23], sampled 0.13979260814728134
[2019-03-27 12:12:55,776] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2131107.772918255 W.
[2019-03-27 12:13:03,682] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05256918], dtype=float32), 0.03819656]
[2019-03-27 12:13:03,683] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.06666666666667, 89.33333333333334, 1.0, 2.0, 0.5538310938006488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780730.8434152335, 780730.843415233, 193333.0884085688]
[2019-03-27 12:13:03,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:13:03,691] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9628510e-27 1.0000000e+00 4.3687817e-36 1.1344524e-34 1.8589009e-34], sampled 0.7924520727834043
[2019-03-27 12:13:08,858] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3184 2842623991.1978 1131.0000
[2019-03-27 12:13:08,866] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.1057 3163834931.1104 1774.0000
[2019-03-27 12:13:08,868] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.6679 3007840824.3263 1766.0000
[2019-03-27 12:13:08,878] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 12:13:08,994] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4308 2779149198.4617 933.0000
[2019-03-27 12:13:10,010] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 575000, evaluation results [575000.0, 7885.105673704787, 3163834931.1103544, 1774.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.430759135581, 2779149198.4616733, 933.0, 7998.667917709291, 3007840824.32627, 1766.0, 8497.318449684271, 2842623991.1978283, 1131.0]
[2019-03-27 12:13:14,659] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3548889e-07 2.3195928e-02 1.8731578e-14 9.7680396e-01 5.1313730e-14], sum to 1.0000
[2019-03-27 12:13:14,670] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2928
[2019-03-27 12:13:14,676] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.8, 61.33333333333334, 1.0, 2.0, 1.013764900885757, 1.0, 2.0, 1.013764900885757, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2835855.544624161, 2835855.544624161, 537356.5711919976], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5840400.0000, 
sim time next is 5841000.0000, 
raw observation next is [32.75, 61.5, 1.0, 2.0, 1.014182731038797, 1.0, 2.0, 1.014182731038797, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2837025.68918285, 2837025.689182851, 537612.4488581917], 
processed observation next is [1.0, 0.6086956521739131, 0.7511848341232228, 0.615, 1.0, 1.0, 1.0170876277575867, 1.0, 1.0, 1.0170876277575867, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7880626914396806, 0.7880626914396809, 0.8024066400868534], 
reward next is 0.1976, 
noisyNet noise sample is [array([1.0148109], dtype=float32), 0.91002345]. 
=============================================
[2019-03-27 12:13:14,688] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[31.785706]
 [30.997622]
 [30.462164]
 [30.142042]
 [30.17869 ]], R is [[31.7045269 ]
 [31.38748169]
 [31.07360649]
 [30.94950104]
 [30.64000702]].
[2019-03-27 12:13:17,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1332545e-26 1.0000000e+00 4.3331161e-35 5.6757386e-33 1.5692086e-33], sum to 1.0000
[2019-03-27 12:13:17,110] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0928
[2019-03-27 12:13:17,115] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.06666666666667, 84.0, 1.0, 2.0, 0.5147484859798462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719288.0111071462, 719288.0111071462, 185938.9786653985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5725200.0000, 
sim time next is 5725800.0000, 
raw observation next is [27.25, 83.0, 1.0, 2.0, 0.5161129684948993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721195.3291722643, 721195.3291722643, 186158.9960906226], 
processed observation next is [0.0, 0.2608695652173913, 0.490521327014218, 0.83, 1.0, 1.0, 0.4170035764998786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20033203588118453, 0.20033203588118453, 0.2778492478964516], 
reward next is 0.7222, 
noisyNet noise sample is [array([0.95463264], dtype=float32), 0.41570792]. 
=============================================
[2019-03-27 12:13:20,583] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0967462e-23 1.0000000e+00 7.1080392e-32 1.2096887e-26 9.0314218e-31], sum to 1.0000
[2019-03-27 12:13:20,593] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6216
[2019-03-27 12:13:20,601] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 85.0, 1.0, 2.0, 0.5467542146180874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764027.5621896357, 764027.5621896357, 191245.2266898026], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5779800.0000, 
sim time next is 5780400.0000, 
raw observation next is [27.63333333333333, 85.33333333333333, 1.0, 2.0, 0.5470873843352874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764493.2969627016, 764493.296962701, 191301.9922141657], 
processed observation next is [0.0, 0.9130434782608695, 0.5086887835703, 0.8533333333333333, 1.0, 1.0, 0.454322149801551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.212359249156306, 0.21235924915630583, 0.28552536151368013], 
reward next is 0.7145, 
noisyNet noise sample is [array([-0.9608237], dtype=float32), -0.95955855]. 
=============================================
[2019-03-27 12:13:24,338] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.20147801e-22 1.00000000e+00 2.71298689e-31 4.55990692e-25
 1.15604065e-29], sum to 1.0000
[2019-03-27 12:13:24,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1433
[2019-03-27 12:13:24,352] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 87.0, 1.0, 2.0, 0.5567330500304989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777976.9696666499, 777976.9696666499, 192961.9888492085], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5865600.0000, 
sim time next is 5866200.0000, 
raw observation next is [27.7, 87.0, 1.0, 2.0, 0.5536812715813269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773710.871128941, 773710.871128941, 192433.9467760243], 
processed observation next is [1.0, 0.9130434782608695, 0.5118483412322274, 0.87, 1.0, 1.0, 0.46226659226665884, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21491968642470582, 0.21491968642470582, 0.28721484593436464], 
reward next is 0.7128, 
noisyNet noise sample is [array([0.01465607], dtype=float32), -0.17158534]. 
=============================================
[2019-03-27 12:13:26,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1071858e-24 1.0000000e+00 2.4760386e-34 8.5238123e-31 5.2332280e-32], sum to 1.0000
[2019-03-27 12:13:26,187] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9527
[2019-03-27 12:13:26,194] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 76.83333333333333, 1.0, 2.0, 0.5583851429277124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780286.446919334, 780286.446919334, 193250.050459017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5856600.0000, 
sim time next is 5857200.0000, 
raw observation next is [29.6, 78.0, 1.0, 2.0, 0.5590737159651281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781249.0115661294, 781249.0115661294, 193369.8562955442], 
processed observation next is [1.0, 0.8260869565217391, 0.6018957345971565, 0.78, 1.0, 1.0, 0.4687635132109977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21701361432392482, 0.21701361432392482, 0.28861172581424505], 
reward next is 0.7114, 
noisyNet noise sample is [array([-1.3687781], dtype=float32), 0.8895836]. 
=============================================
[2019-03-27 12:13:27,817] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1219646e-08 9.5712006e-01 6.3005524e-16 4.2879924e-02 4.9975124e-16], sum to 1.0000
[2019-03-27 12:13:27,831] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9296
[2019-03-27 12:13:27,844] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2256029.709349453 W.
[2019-03-27 12:13:27,849] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.15, 77.66666666666666, 1.0, 2.0, 0.537783369432989, 1.0, 2.0, 0.537783369432989, 1.0, 2.0, 0.9339518650271584, 6.9112, 6.9112, 170.5573041426782, 2256029.709349453, 2256029.709349453, 442257.9961741243], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5932200.0000, 
sim time next is 5932800.0000, 
raw observation next is [30.2, 78.0, 1.0, 2.0, 0.5667485019412133, 1.0, 2.0, 0.5667485019412133, 1.0, 2.0, 0.9842547212782493, 6.9112, 6.9112, 170.5573041426782, 2377655.56549043, 2377655.56549043, 464372.9693643238], 
processed observation next is [1.0, 0.6956521739130435, 0.6303317535545023, 0.78, 1.0, 1.0, 0.4780102433026666, 1.0, 1.0, 0.4780102433026666, 1.0, 1.0, 0.9807984405832308, 0.0, 0.0, 0.8375144448122397, 0.6604598793028973, 0.6604598793028973, 0.6930939841258564], 
reward next is 0.3069, 
noisyNet noise sample is [array([1.2847049], dtype=float32), 0.20154637]. 
=============================================
[2019-03-27 12:13:34,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7576491e-22 1.0000000e+00 8.6668044e-30 1.7935575e-24 5.7476508e-29], sum to 1.0000
[2019-03-27 12:13:34,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0290
[2019-03-27 12:13:34,523] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 92.66666666666667, 1.0, 2.0, 0.6213281476404795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 868278.8456684981, 868278.8456684987, 204779.9012124394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5980800.0000, 
sim time next is 5981400.0000, 
raw observation next is [26.35, 92.0, 1.0, 2.0, 0.6178315982302748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 863390.5851196287, 863390.585119628, 204108.6099141939], 
processed observation next is [1.0, 0.21739130434782608, 0.4478672985781992, 0.92, 1.0, 1.0, 0.5395561424461142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23983071808878575, 0.23983071808878556, 0.3046397162898416], 
reward next is 0.6954, 
noisyNet noise sample is [array([-0.3905884], dtype=float32), -0.19212446]. 
=============================================
[2019-03-27 12:13:40,898] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.4519243e-25 1.0000000e+00 5.3513837e-33 1.5095403e-28 5.1676836e-32], sum to 1.0000
[2019-03-27 12:13:40,909] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7442
[2019-03-27 12:13:40,920] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 86.0, 1.0, 2.0, 0.5261270172167318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735193.3971707496, 735193.3971707496, 187791.0509652534], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6120600.0000, 
sim time next is 6121200.0000, 
raw observation next is [27.2, 86.0, 1.0, 2.0, 0.5258787516974358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 734846.3587202765, 734846.3587202758, 187750.2557645941], 
processed observation next is [1.0, 0.8695652173913043, 0.4881516587677725, 0.86, 1.0, 1.0, 0.4287695803583564, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20412398853341013, 0.20412398853340993, 0.2802242623352151], 
reward next is 0.7198, 
noisyNet noise sample is [array([-0.97327846], dtype=float32), 0.44595492]. 
=============================================
[2019-03-27 12:13:43,658] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9773958e-17 1.0000000e+00 1.4597707e-25 4.3196887e-17 1.6692940e-24], sum to 1.0000
[2019-03-27 12:13:43,668] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8296
[2019-03-27 12:13:43,677] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 87.0, 1.0, 2.0, 0.7811103214181025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1091682.194335292, 1091682.194335292, 239328.3936755353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6163200.0000, 
sim time next is 6163800.0000, 
raw observation next is [27.88333333333333, 86.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.15934877929128, 6.9112, 168.9118537293756, 1629919.627037903, 1453875.491732661, 311351.9455118412], 
processed observation next is [1.0, 0.34782608695652173, 0.5205371248025275, 0.865, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.024814877929128, 0.0, 0.8294345299924653, 0.4527554519549731, 0.40385430325907246, 0.46470439628633015], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.765448], dtype=float32), -0.016182385]. 
=============================================
[2019-03-27 12:13:43,904] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.342597e-17 1.000000e+00 7.871148e-26 2.131259e-17 6.117588e-24], sum to 1.0000
[2019-03-27 12:13:43,912] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2875
[2019-03-27 12:13:43,920] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 92.0, 1.0, 2.0, 0.7153428128753893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 999722.0550100794, 999722.0550100801, 224192.0248112488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6147600.0000, 
sim time next is 6148200.0000, 
raw observation next is [26.6, 92.0, 1.0, 2.0, 0.7062129601582489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 986956.7678765169, 986956.7678765175, 222192.2251174253], 
processed observation next is [1.0, 0.13043478260869565, 0.4597156398104266, 0.92, 1.0, 1.0, 0.6460397110340348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2741546577434769, 0.2741546577434771, 0.3316301867424258], 
reward next is 0.6684, 
noisyNet noise sample is [array([-0.7422001], dtype=float32), 1.3910284]. 
=============================================
[2019-03-27 12:13:49,615] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.8027515e-12 9.7981983e-01 3.3729507e-17 2.0180175e-02 3.7942224e-16], sum to 1.0000
[2019-03-27 12:13:49,624] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1891
[2019-03-27 12:13:49,637] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2322628.473141667 W.
[2019-03-27 12:13:49,642] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.9, 72.0, 1.0, 2.0, 0.5536441785518372, 1.0, 2.0, 0.5536441785518372, 1.0, 1.0, 0.959470943410426, 6.9112, 6.9112, 170.5573041426782, 2322628.473141667, 2322628.473141667, 453815.0665762214], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6190200.0000, 
sim time next is 6190800.0000, 
raw observation next is [29.8, 72.33333333333333, 1.0, 2.0, 0.9708546394978772, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.993089940248266, 6.9112, 168.9124695961188, 2254208.470564074, 2196113.095985939, 454950.2649359693], 
processed observation next is [1.0, 0.6521739130434783, 0.6113744075829385, 0.7233333333333333, 1.0, 1.0, 0.9648851078287677, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008188994024826623, 0.0, 0.8294375541801194, 0.6261690196011316, 0.6100314155516496, 0.6790302461730885], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8750781], dtype=float32), 1.9417454]. 
=============================================
[2019-03-27 12:13:52,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5207745e-25 1.0000000e+00 5.7970707e-34 2.1559654e-29 7.3311551e-32], sum to 1.0000
[2019-03-27 12:13:52,786] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0356
[2019-03-27 12:13:52,790] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 88.33333333333334, 1.0, 2.0, 0.5347612867095486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747262.8985139598, 747262.8985139593, 189222.0118799339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6243600.0000, 
sim time next is 6244200.0000, 
raw observation next is [27.25, 88.0, 1.0, 2.0, 0.5356992737036375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748574.0814228959, 748574.0814228959, 189378.8006929159], 
processed observation next is [0.0, 0.2608695652173913, 0.490521327014218, 0.88, 1.0, 1.0, 0.4406015345826957, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2079372448396933, 0.2079372448396933, 0.28265492640733714], 
reward next is 0.7173, 
noisyNet noise sample is [array([0.02778463], dtype=float32), 0.5466093]. 
=============================================
[2019-03-27 12:13:52,962] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2378574e-24 1.0000000e+00 5.2444765e-33 3.6614535e-28 5.2546948e-31], sum to 1.0000
[2019-03-27 12:13:52,974] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1249
[2019-03-27 12:13:52,980] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.51666666666667, 90.83333333333334, 1.0, 2.0, 0.5251045605265967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733764.1547786708, 733764.1547786702, 187623.1721951491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6235800.0000, 
sim time next is 6236400.0000, 
raw observation next is [26.53333333333333, 90.66666666666667, 1.0, 2.0, 0.5251425489204847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733817.2568692582, 733817.2568692588, 187629.3871789379], 
processed observation next is [0.0, 0.17391304347826086, 0.45655608214849913, 0.9066666666666667, 1.0, 1.0, 0.42788258906082494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20383812690812728, 0.20383812690812744, 0.28004386146110133], 
reward next is 0.7200, 
noisyNet noise sample is [array([-1.1679806], dtype=float32), -1.0310055]. 
=============================================
[2019-03-27 12:13:54,344] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3742116e-24 1.0000000e+00 4.2174830e-32 2.0418822e-28 3.0331335e-30], sum to 1.0000
[2019-03-27 12:13:54,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1240
[2019-03-27 12:13:54,363] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.73333333333333, 66.0, 1.0, 2.0, 0.5312558886775424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742362.8257202522, 742362.8257202522, 188638.4128364908], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6265200.0000, 
sim time next is 6265800.0000, 
raw observation next is [30.75, 65.5, 1.0, 2.0, 0.5292678855813084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739583.8760587026, 739583.8760587032, 188309.0389110315], 
processed observation next is [0.0, 0.5217391304347826, 0.6563981042654029, 0.655, 1.0, 1.0, 0.4328528741943475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20543996557186184, 0.205439965571862, 0.2810582670313903], 
reward next is 0.7189, 
noisyNet noise sample is [array([-0.5478818], dtype=float32), -1.0735805]. 
=============================================
[2019-03-27 12:14:01,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5929742e-15 1.0000000e+00 6.7313822e-22 1.2336100e-13 6.7214556e-21], sum to 1.0000
[2019-03-27 12:14:01,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2571
[2019-03-27 12:14:01,567] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 83.66666666666667, 1.0, 2.0, 0.39459431029477, 1.0, 1.0, 0.39459431029477, 1.0, 1.0, 0.6712390800234933, 6.9112, 6.9112, 170.5573041426782, 1654884.886802248, 1654884.886802248, 347958.5346783801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6402000.0000, 
sim time next is 6402600.0000, 
raw observation next is [26.95, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.196529612795024, 6.9112, 168.9111187150474, 1656314.006563246, 1453893.561344344, 311348.4796224652], 
processed observation next is [1.0, 0.08695652173913043, 0.476303317535545, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.028532961279502354, 0.0, 0.8294309207356229, 0.46008722404534613, 0.4038593225956511, 0.4646992233171123], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.656922], dtype=float32), -0.3423609]. 
=============================================
[2019-03-27 12:14:06,843] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.3074212e-19 1.0000000e+00 8.9885160e-26 5.1185150e-18 1.0828668e-24], sum to 1.0000
[2019-03-27 12:14:06,854] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9126
[2019-03-27 12:14:06,861] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 91.5, 1.0, 2.0, 0.7653272860402697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1069612.665953984, 1069612.665953983, 235576.9830237671], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6492600.0000, 
sim time next is 6493200.0000, 
raw observation next is [26.23333333333333, 91.66666666666666, 1.0, 2.0, 0.7544219088537768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1054363.868967868, 1054363.868967868, 233029.7328824497], 
processed observation next is [1.0, 0.13043478260869565, 0.44233807266982617, 0.9166666666666665, 1.0, 1.0, 0.7041227817515383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2928788524910744, 0.2928788524910744, 0.34780557146634283], 
reward next is 0.6522, 
noisyNet noise sample is [array([-0.7702941], dtype=float32), 0.42472687]. 
=============================================
[2019-03-27 12:14:11,816] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7375136e-10 6.5307543e-03 1.1656156e-15 9.9346924e-01 3.9911520e-16], sum to 1.0000
[2019-03-27 12:14:11,826] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9771
[2019-03-27 12:14:11,833] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.71666666666667, 74.33333333333334, 1.0, 2.0, 0.6225474012661518, 1.0, 2.0, 0.6225474012661518, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1740666.234897881, 1740666.234897881, 342724.6950231214], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6513000.0000, 
sim time next is 6513600.0000, 
raw observation next is [28.93333333333334, 72.66666666666667, 1.0, 2.0, 0.62807827961181, 1.0, 2.0, 0.62807827961181, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1756143.453403409, 1756143.453403409, 344849.3226565456], 
processed observation next is [1.0, 0.391304347826087, 0.5703001579778835, 0.7266666666666667, 1.0, 1.0, 0.5519015417009758, 1.0, 1.0, 0.5519015417009758, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4878176259453914, 0.4878176259453914, 0.5147004815769337], 
reward next is 0.4853, 
noisyNet noise sample is [array([0.07385023], dtype=float32), -0.5337798]. 
=============================================
[2019-03-27 12:14:12,062] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3697254e-24 1.0000000e+00 3.6538279e-32 2.1652102e-24 2.5579462e-31], sum to 1.0000
[2019-03-27 12:14:12,075] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0954
[2019-03-27 12:14:12,083] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.71666666666667, 69.5, 1.0, 2.0, 0.4836530697608024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675822.7546329242, 675822.7546329242, 181074.6124154043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6547800.0000, 
sim time next is 6548400.0000, 
raw observation next is [28.6, 70.0, 1.0, 2.0, 0.486308992516264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679535.1411445088, 679535.1411445083, 181478.709931066], 
processed observation next is [1.0, 0.8260869565217391, 0.5545023696682465, 0.7, 1.0, 1.0, 0.38109517170634216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18875976142903023, 0.18875976142903006, 0.27086374616577014], 
reward next is 0.7291, 
noisyNet noise sample is [array([0.3257248], dtype=float32), 0.6197016]. 
=============================================
[2019-03-27 12:14:12,170] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1404755e-18 1.0000000e+00 1.2204334e-25 2.1672198e-15 1.3318422e-24], sum to 1.0000
[2019-03-27 12:14:12,178] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6075
[2019-03-27 12:14:12,184] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.36666666666667, 92.66666666666667, 1.0, 2.0, 0.494836743597242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691455.1192242103, 691455.1192242103, 182791.027977962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6657600.0000, 
sim time next is 6658200.0000, 
raw observation next is [25.3, 93.0, 1.0, 2.0, 0.4951438552691295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691884.3983671973, 691884.3983671967, 182838.6206398201], 
processed observation next is [1.0, 0.043478260869565216, 0.39810426540284366, 0.93, 1.0, 1.0, 0.3917395846616018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19219011065755479, 0.19219011065755462, 0.2728934636415225], 
reward next is 0.7271, 
noisyNet noise sample is [array([-0.93309414], dtype=float32), -1.1766158]. 
=============================================
[2019-03-27 12:14:15,834] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 12:14:15,836] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:14:15,838] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:14:15,839] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:14:15,841] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:14:15,842] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:14:15,843] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:14:15,845] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:14:15,847] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:14:15,840] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:14:15,852] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:14:15,871] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run25
[2019-03-27 12:14:15,891] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run25
[2019-03-27 12:14:15,913] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run25
[2019-03-27 12:14:15,932] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run25
[2019-03-27 12:14:15,950] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run25
[2019-03-27 12:14:29,571] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:14:29,572] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.13333333333333, 91.66666666666667, 1.0, 2.0, 0.4397573496722286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636148.7814546031, 636148.7814546037, 177473.9682542203]
[2019-03-27 12:14:29,572] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:14:29,575] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.3780179e-24 1.0000000e+00 5.3995957e-32 4.8969527e-27 2.8865182e-30], sampled 0.6964416617508434
[2019-03-27 12:14:34,383] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:14:34,385] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.9, 90.33333333333334, 1.0, 2.0, 0.3062226382590891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490072.8575880549, 490072.8575880549, 166374.8759547537]
[2019-03-27 12:14:34,385] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:14:34,388] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.3236321e-21 1.0000000e+00 1.0074751e-28 1.8362824e-21 3.0060117e-27], sampled 0.6592123894795741
[2019-03-27 12:14:40,686] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:14:40,687] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.70876489, 96.02688005499999, 1.0, 2.0, 0.2825052037646801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457385.8231452619, 457385.8231452625, 164085.3341785521]
[2019-03-27 12:14:40,689] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:14:40,693] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2534621e-24 1.0000000e+00 1.6702427e-32 2.4329950e-28 9.7343505e-31], sampled 0.03104474435662341
[2019-03-27 12:14:52,069] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:14:52,070] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.934165815, 80.894425445, 1.0, 2.0, 0.6295298754018902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 879745.1475059509, 879745.1475059515, 206377.0409403563]
[2019-03-27 12:14:52,070] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:14:52,072] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6291747e-17 1.0000000e+00 1.4399110e-24 1.5411083e-13 9.4365813e-24], sampled 0.7187730635364161
[2019-03-27 12:14:59,698] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:14:59,699] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.01662210166667, 90.79062754833333, 1.0, 2.0, 0.5117468335058794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715092.2156469921, 715092.2156469915, 185459.5148879169]
[2019-03-27 12:14:59,702] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:14:59,705] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2619278e-20 1.0000000e+00 4.7948944e-28 1.3605607e-20 1.4617648e-26], sampled 0.31334887278701895
[2019-03-27 12:15:03,054] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:15:03,057] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.64203715333333, 95.32554922333334, 1.0, 2.0, 0.4879822640726207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681874.0075275513, 681874.0075275513, 181736.3935425337]
[2019-03-27 12:15:03,059] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:15:03,062] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6910876e-22 1.0000000e+00 3.3046660e-30 4.2174034e-24 1.3575379e-28], sampled 0.27469112801529716
[2019-03-27 12:15:42,143] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:15:42,145] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.66666666666666, 72.0, 1.0, 2.0, 0.5681879706489141, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9843561869313625, 6.911199999999999, 6.9112, 168.9129227125368, 1588574.006768377, 1588574.006768378, 347115.9228208804]
[2019-03-27 12:15:42,146] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:15:42,150] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.2756435e-18 1.0000000e+00 3.1053189e-25 6.6006550e-15 3.9909471e-24], sampled 0.7096968111769527
[2019-03-27 12:15:54,641] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:15:54,643] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.92716718, 81.39958941, 1.0, 2.0, 0.5621434574628386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793178.8256940077, 793178.8256940077, 194884.8255290409]
[2019-03-27 12:15:54,644] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:15:54,649] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3395060e-23 1.0000000e+00 1.0537269e-31 3.9334182e-26 6.4954462e-30], sampled 0.7849528608481754
[2019-03-27 12:15:56,816] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:15:56,816] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.73027956666667, 88.18478744999999, 1.0, 2.0, 0.514052584313758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718315.2584296493, 718315.2584296493, 185825.3475198069]
[2019-03-27 12:15:56,817] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:15:56,821] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6785525e-20 1.0000000e+00 1.0024956e-27 1.4458150e-19 2.3900384e-26], sampled 0.4694403429681394
[2019-03-27 12:16:02,021] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:16:02,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.35, 70.0, 1.0, 2.0, 0.5491683349319743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767402.2425944128, 767402.2425944128, 191658.0709831761]
[2019-03-27 12:16:02,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:16:02,025] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1520908e-23 1.0000000e+00 9.9984305e-32 2.3989272e-25 6.8400711e-30], sampled 0.7858954941086042
[2019-03-27 12:16:10,353] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0510987], dtype=float32), 0.040941313]
[2019-03-27 12:16:10,353] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.18333333333333, 84.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.526488848046574, 6.9112, 168.9037484822013, 2600379.05638775, 1454500.022858185, 310410.2596684911]
[2019-03-27 12:16:10,353] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:16:10,356] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.2885020e-13 9.9999559e-01 2.3826353e-19 4.4587459e-06 6.6175044e-19], sampled 0.39774563619649594
[2019-03-27 12:16:10,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2600379.05638775 W.
[2019-03-27 12:16:22,930] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8682.3338 2777211002.4541 882.0000
[2019-03-27 12:16:23,107] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8012.5495 3152062179.4013 1471.0000
[2019-03-27 12:16:23,389] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8534.2087 2838954859.0896 1040.0000
[2019-03-27 12:16:23,631] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8041.7833 3003286655.5667 1659.0000
[2019-03-27 12:16:23,729] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8284.6376 2924533574.3757 1273.0000
[2019-03-27 12:16:24,747] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 600000, evaluation results [600000.0, 8012.549549972402, 3152062179.4012556, 1471.0, 8284.637649156304, 2924533574.375663, 1273.0, 8682.33383684999, 2777211002.4541116, 882.0, 8041.783279609745, 3003286655.566671, 1659.0, 8534.208667461591, 2838954859.0895677, 1040.0]
[2019-03-27 12:16:37,493] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.8801589e-19 1.0000000e+00 3.6237454e-26 2.8654400e-17 2.2716571e-25], sum to 1.0000
[2019-03-27 12:16:37,503] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1667
[2019-03-27 12:16:37,511] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 77.5, 1.0, 2.0, 0.3380968728914415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527551.4511512365, 527551.4511512365, 168997.2161876444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6741000.0000, 
sim time next is 6741600.0000, 
raw observation next is [23.56666666666667, 78.0, 1.0, 2.0, 0.3357118292468649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524619.6603584196, 524619.6603584196, 168784.2207337084], 
processed observation next is [1.0, 0.0, 0.31595576619273325, 0.78, 1.0, 1.0, 0.19965280632152393, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14572768343289436, 0.14572768343289436, 0.2519167473637439], 
reward next is 0.7481, 
noisyNet noise sample is [array([1.4666872], dtype=float32), 0.53374666]. 
=============================================
[2019-03-27 12:16:42,463] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8915045e-27 1.0000000e+00 2.1773393e-36 3.0762214e-32 3.3757672e-35], sum to 1.0000
[2019-03-27 12:16:42,476] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7886
[2019-03-27 12:16:42,487] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 79.0, 1.0, 2.0, 0.3749525497400966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 567696.0458414528, 567696.0458414521, 171876.2560999354], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6852600.0000, 
sim time next is 6853200.0000, 
raw observation next is [24.7, 78.66666666666667, 1.0, 2.0, 0.3776137079235777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570824.3978629658, 570824.3978629658, 172123.2898209765], 
processed observation next is [0.0, 0.30434782608695654, 0.3696682464454976, 0.7866666666666667, 1.0, 1.0, 0.2501369974982864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15856233273971274, 0.15856233273971274, 0.25690043256862166], 
reward next is 0.7431, 
noisyNet noise sample is [array([0.7512875], dtype=float32), -0.38621604]. 
=============================================
[2019-03-27 12:16:50,705] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2217429e-24 1.0000000e+00 3.8525628e-32 1.4257205e-26 1.1442386e-29], sum to 1.0000
[2019-03-27 12:16:50,715] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9237
[2019-03-27 12:16:50,721] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.88333333333334, 78.16666666666667, 1.0, 2.0, 0.4264226533309549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 620671.1091066434, 620671.1091066428, 176057.0423806288], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6936600.0000, 
sim time next is 6937200.0000, 
raw observation next is [26.1, 77.0, 1.0, 2.0, 0.4275852769461792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 621469.6596054427, 621469.6596054421, 176109.5352132057], 
processed observation next is [0.0, 0.30434782608695654, 0.4360189573459717, 0.77, 1.0, 1.0, 0.3103437071640713, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17263046100151186, 0.1726304610015117, 0.26285005255702343], 
reward next is 0.7371, 
noisyNet noise sample is [array([-0.5365141], dtype=float32), -0.40192991]. 
=============================================
[2019-03-27 12:17:05,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.06142385e-17 1.00000000e+00 3.00622121e-25 1.49175089e-14
 1.35428209e-24], sum to 1.0000
[2019-03-27 12:17:05,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4992
[2019-03-27 12:17:05,144] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 89.33333333333334, 1.0, 2.0, 0.5550310196432704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775597.6895465234, 775597.6895465241, 192663.0975303033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7183200.0000, 
sim time next is 7183800.0000, 
raw observation next is [25.8, 89.5, 1.0, 2.0, 0.5770454712803359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 806372.2559995108, 806372.2559995108, 196541.5839879837], 
processed observation next is [1.0, 0.13043478260869565, 0.42180094786729866, 0.895, 1.0, 1.0, 0.490416230458236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22399229333319742, 0.22399229333319742, 0.2933456477432593], 
reward next is 0.7067, 
noisyNet noise sample is [array([-0.47944656], dtype=float32), 0.92184305]. 
=============================================
[2019-03-27 12:17:05,867] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2881653e-20 1.0000000e+00 1.3390384e-27 2.2713087e-19 4.4454454e-27], sum to 1.0000
[2019-03-27 12:17:05,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9303
[2019-03-27 12:17:05,884] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 84.33333333333334, 1.0, 2.0, 0.4779966033264941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 667916.3218073163, 667916.3218073157, 180220.2853805003], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7152000.0000, 
sim time next is 7152600.0000, 
raw observation next is [26.1, 84.16666666666666, 1.0, 2.0, 0.4771864483896152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 666783.9172558101, 666783.9172558095, 180098.7201252333], 
processed observation next is [1.0, 0.782608695652174, 0.4360189573459717, 0.8416666666666666, 1.0, 1.0, 0.3701041546862834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1852177547932806, 0.18521775479328043, 0.2688040598884079], 
reward next is 0.7312, 
noisyNet noise sample is [array([-0.6148056], dtype=float32), -0.35756582]. 
=============================================
[2019-03-27 12:17:09,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9302882e-22 1.0000000e+00 1.6093671e-31 1.7936610e-23 3.4192987e-29], sum to 1.0000
[2019-03-27 12:17:09,607] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9080
[2019-03-27 12:17:09,617] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 85.33333333333334, 1.0, 2.0, 0.3194155092829265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503177.2750253567, 503177.2750253567, 167224.0212844154], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7285800.0000, 
sim time next is 7286400.0000, 
raw observation next is [22.3, 85.0, 1.0, 2.0, 0.3587679162030246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 565085.5175363016, 565085.5175363023, 172164.6353962878], 
processed observation next is [1.0, 0.34782608695652173, 0.25592417061611383, 0.85, 1.0, 1.0, 0.2274312243409935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15696819931563935, 0.15696819931563954, 0.25696214238251913], 
reward next is 0.7430, 
noisyNet noise sample is [array([-0.805931], dtype=float32), -0.52061355]. 
=============================================
[2019-03-27 12:17:12,270] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1560371e-16 1.0000000e+00 1.2951916e-24 3.4302402e-14 1.4306314e-23], sum to 1.0000
[2019-03-27 12:17:12,281] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2406
[2019-03-27 12:17:12,286] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.13333333333333, 92.66666666666667, 1.0, 2.0, 0.4581793355792964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724441.5512895911, 724441.5512895911, 186999.0418960373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7388400.0000, 
sim time next is 7389000.0000, 
raw observation next is [21.1, 92.5, 1.0, 2.0, 0.3999967981806018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 633311.3583937364, 633311.3583937371, 178143.4362224409], 
processed observation next is [1.0, 0.5217391304347826, 0.1990521327014219, 0.925, 1.0, 1.0, 0.277104576121207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1759198217760379, 0.1759198217760381, 0.2658857257051357], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.25044385], dtype=float32), -0.21862213]. 
=============================================
[2019-03-27 12:17:12,301] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[66.928055]
 [65.86025 ]
 [66.21754 ]
 [66.584785]
 [66.286095]], R is [[67.83811951]
 [67.88063049]
 [67.8707428 ]
 [67.86841583]
 [67.89082336]].
[2019-03-27 12:17:13,897] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0530633e-17 1.0000000e+00 6.6598588e-24 1.1419685e-11 2.4440242e-23], sum to 1.0000
[2019-03-27 12:17:13,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2296
[2019-03-27 12:17:13,916] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 60.0, 1.0, 2.0, 0.5266473708035019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803208.2514201567, 803208.2514201567, 196035.1856667051], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7304400.0000, 
sim time next is 7305000.0000, 
raw observation next is [27.58333333333334, 59.5, 1.0, 2.0, 0.5314612537988196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 810713.4058482694, 810713.4058482688, 196925.4549290281], 
processed observation next is [1.0, 0.5652173913043478, 0.506319115323855, 0.595, 1.0, 1.0, 0.4354954865046019, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22519816829118594, 0.22519816829118577, 0.2939185894463106], 
reward next is 0.7061, 
noisyNet noise sample is [array([-0.04721137], dtype=float32), 2.8545883]. 
=============================================
[2019-03-27 12:17:13,935] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.64189 ]
 [70.063995]
 [69.42432 ]
 [68.49094 ]
 [67.72118 ]], R is [[70.98764038]
 [70.98517609]
 [70.97621918]
 [70.96102142]
 [70.92010498]].
[2019-03-27 12:17:15,735] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9860392e-20 1.0000000e+00 5.8616452e-27 4.7755924e-16 1.9223847e-26], sum to 1.0000
[2019-03-27 12:17:15,744] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3321
[2019-03-27 12:17:15,750] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 73.33333333333334, 1.0, 2.0, 0.3889834030130439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584808.1625891158, 584808.1625891165, 173276.4010678578], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7330800.0000, 
sim time next is 7331400.0000, 
raw observation next is [25.6, 73.5, 1.0, 2.0, 0.3878582133267932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 583848.2853143984, 583848.285314399, 173211.2121714892], 
processed observation next is [1.0, 0.8695652173913043, 0.4123222748815167, 0.735, 1.0, 1.0, 0.26247977509252196, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16218007925399955, 0.16218007925399974, 0.2585241972708794], 
reward next is 0.7415, 
noisyNet noise sample is [array([0.07333344], dtype=float32), 0.44601518]. 
=============================================
[2019-03-27 12:17:30,503] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 12:17:30,506] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:17:30,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:17:30,507] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:17:30,508] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:17:30,509] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:17:30,509] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:17:30,510] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:17:30,512] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:17:30,511] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:17:30,514] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:17:30,539] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run26
[2019-03-27 12:17:30,560] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run26
[2019-03-27 12:17:30,583] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run26
[2019-03-27 12:17:30,583] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run26
[2019-03-27 12:17:30,583] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run26
[2019-03-27 12:17:31,467] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05263671], dtype=float32), 0.04234776]
[2019-03-27 12:17:31,468] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.55, 84.83333333333333, 1.0, 2.0, 0.2821739936956293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 458816.997346331, 458816.9973463304, 164148.3428346792]
[2019-03-27 12:17:31,468] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:17:31,469] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.4869121e-07 9.9999928e-01 4.9601270e-09 4.3863668e-08 1.4385217e-08], sampled 0.2692665541575301
[2019-03-27 12:17:32,489] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05263671], dtype=float32), 0.04234776]
[2019-03-27 12:17:32,489] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.96666666666667, 81.33333333333334, 1.0, 2.0, 0.4194133435157738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619673.6562750271, 619673.6562750271, 176213.7279296059]
[2019-03-27 12:17:32,489] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:17:32,492] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0061043e-20 1.0000000e+00 2.3686158e-28 2.1634157e-19 4.3852485e-27], sampled 0.7932363077567429
[2019-03-27 12:17:47,744] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05263671], dtype=float32), 0.04234776]
[2019-03-27 12:17:47,744] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.8, 83.16666666666667, 1.0, 2.0, 0.3026255061786514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484424.6163609365, 484424.6163609365, 165967.935274268]
[2019-03-27 12:17:47,745] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:17:47,749] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.7844239e-21 1.0000000e+00 2.2005112e-28 4.5603533e-20 4.9496326e-27], sampled 0.12261638546184439
[2019-03-27 12:18:08,052] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05263671], dtype=float32), 0.04234776]
[2019-03-27 12:18:08,056] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.4, 82.0, 1.0, 2.0, 0.6908237736366518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 965440.0775156941, 965440.0775156947, 218875.538307512]
[2019-03-27 12:18:08,057] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:18:08,059] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2330528e-20 1.0000000e+00 2.7855081e-28 6.1243251e-20 6.3598436e-27], sampled 0.806147316546784
[2019-03-27 12:18:39,443] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05263671], dtype=float32), 0.04234776]
[2019-03-27 12:18:39,444] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.89905606, 75.31605836666665, 1.0, 2.0, 0.7265858378588511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1015442.173555956, 1015442.173555955, 226700.9267974464]
[2019-03-27 12:18:39,445] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:18:39,447] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1454447e-24 1.0000000e+00 6.1870514e-33 1.1270084e-25 4.0757220e-31], sampled 0.16189724225316393
[2019-03-27 12:18:39,484] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05263671], dtype=float32), 0.04234776]
[2019-03-27 12:18:39,486] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.03333333333333, 61.0, 1.0, 2.0, 0.5960389946954556, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564912465, 832924.5059535896, 832924.5059535889, 200008.9936626244]
[2019-03-27 12:18:39,486] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:18:39,488] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0735134e-22 1.0000000e+00 4.2368192e-31 2.4976951e-20 8.0527509e-30], sampled 0.004972015550697262
[2019-03-27 12:18:41,651] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05263671], dtype=float32), 0.04234776]
[2019-03-27 12:18:41,655] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.56486154833333, 89.342767275, 1.0, 2.0, 0.9212713452396849, 1.0, 2.0, 0.9212713452396849, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2576816.398812374, 2576816.398812374, 483712.1388401018]
[2019-03-27 12:18:41,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:18:41,664] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4348831e-11 8.4531633e-04 8.3585021e-18 9.9915469e-01 2.1041016e-18], sampled 0.18639387619465597
[2019-03-27 12:18:56,641] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05263671], dtype=float32), 0.04234776]
[2019-03-27 12:18:56,642] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.225528945, 79.76164535500001, 1.0, 2.0, 0.3822635640600783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586450.7274029538, 586450.7274029538, 173738.1682151462]
[2019-03-27 12:18:56,643] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:18:56,645] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.6684709e-21 1.0000000e+00 4.9349033e-29 1.0248649e-19 9.6492586e-28], sampled 0.5548174847747221
[2019-03-27 12:19:36,763] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7930.0835 3160089951.3595 1681.0000
[2019-03-27 12:19:37,907] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8005.0651 3007055283.3183 1753.0000
[2019-03-27 12:19:38,320] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8277.9398 2925043477.1309 1285.0000
[2019-03-27 12:19:38,502] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8507.2802 2841460812.0607 1104.0000
[2019-03-27 12:19:38,549] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8673.8401 2778110799.6993 903.0000
[2019-03-27 12:19:39,565] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 625000, evaluation results [625000.0, 7930.083536440458, 3160089951.3595037, 1681.0, 8277.939802102024, 2925043477.1308737, 1285.0, 8673.840132223491, 2778110799.6993113, 903.0, 8005.065086639233, 3007055283.318265, 1753.0, 8507.280178943603, 2841460812.0606503, 1104.0]
[2019-03-27 12:20:01,663] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:01,663] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:01,717] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-03-27 12:20:05,711] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:05,714] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:05,745] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-03-27 12:20:07,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:07,071] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:07,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:07,084] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:07,106] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-03-27 12:20:07,146] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-03-27 12:20:07,230] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:07,231] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:07,260] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-03-27 12:20:07,277] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:07,278] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:07,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-03-27 12:20:07,817] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:07,820] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:07,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-03-27 12:20:07,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:07,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:07,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-03-27 12:20:07,955] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:07,955] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:07,972] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-03-27 12:20:07,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:07,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:08,015] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-03-27 12:20:08,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:08,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:08,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-03-27 12:20:09,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:09,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:09,079] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:09,079] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:09,102] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:09,104] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:09,104] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:09,106] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:09,106] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-03-27 12:20:09,107] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:20:09,135] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:09,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-03-27 12:20:09,189] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-03-27 12:20:09,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-03-27 12:20:09,227] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-03-27 12:20:13,913] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4116527e-22 1.0000000e+00 6.5427393e-32 4.5984915e-26 3.8440817e-30], sum to 1.0000
[2019-03-27 12:20:13,930] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7098
[2019-03-27 12:20:13,935] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 84.66666666666666, 1.0, 2.0, 0.2924925719225698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466992.0263415549, 466992.0263415549, 164721.9358920194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 287400.0000, 
sim time next is 288000.0000, 
raw observation next is [21.9, 84.0, 1.0, 2.0, 0.295004249976097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 470387.7664043313, 470387.766404332, 164950.1523273695], 
processed observation next is [0.0, 0.34782608695652173, 0.23696682464454974, 0.84, 1.0, 1.0, 0.1506075300916831, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13066326844564757, 0.13066326844564777, 0.2461942572050291], 
reward next is 0.7538, 
noisyNet noise sample is [array([-0.433833], dtype=float32), -0.3567495]. 
=============================================
[2019-03-27 12:20:13,954] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.20352]
 [74.19987]
 [74.19428]
 [74.19594]
 [74.2094 ]], R is [[74.23942566]
 [74.25117493]
 [74.26314545]
 [74.27528381]
 [74.28755188]].
[2019-03-27 12:20:21,351] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.9621766e-24 1.0000000e+00 1.8363991e-31 3.1679569e-24 5.4230095e-30], sum to 1.0000
[2019-03-27 12:20:21,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4745
[2019-03-27 12:20:21,371] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 93.5, 1.0, 2.0, 0.2908045899499951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467461.3572708468, 467461.3572708468, 164783.221668899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 199800.0000, 
sim time next is 200400.0000, 
raw observation next is [20.33333333333333, 93.33333333333334, 1.0, 2.0, 0.2908920393438755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467476.7471486373, 467476.7471486373, 164783.842805928], 
processed observation next is [0.0, 0.30434782608695654, 0.16271721958925733, 0.9333333333333335, 1.0, 1.0, 0.14565305945045237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1298546519857326, 0.1298546519857326, 0.2459460340386985], 
reward next is 0.7541, 
noisyNet noise sample is [array([1.9357871], dtype=float32), 0.9942768]. 
=============================================
[2019-03-27 12:20:35,387] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.8152845e-20 1.0000000e+00 8.1305444e-28 3.1082809e-18 2.7099424e-26], sum to 1.0000
[2019-03-27 12:20:35,388] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0235
[2019-03-27 12:20:35,394] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 73.0, 1.0, 2.0, 0.2414860320580799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399087.1499936656, 399087.1499936663, 160008.93160913], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 590400.0000, 
sim time next is 591000.0000, 
raw observation next is [20.86666666666667, 73.66666666666667, 1.0, 2.0, 0.2404900417532672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397648.4020922547, 397648.4020922547, 159900.2461510204], 
processed observation next is [1.0, 0.8695652173913043, 0.18799368088467638, 0.7366666666666667, 1.0, 1.0, 0.08492776114851468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11045788947007075, 0.11045788947007075, 0.23865708380749315], 
reward next is 0.7613, 
noisyNet noise sample is [array([-0.30206302], dtype=float32), 0.8323444]. 
=============================================
[2019-03-27 12:20:35,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1123122e-24 1.0000000e+00 4.5708598e-33 2.4506994e-23 7.3642615e-31], sum to 1.0000
[2019-03-27 12:20:35,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.80752]
 [74.7915 ]
 [74.81644]
 [74.82498]
 [74.77695]], R is [[74.78826904]
 [74.80156708]
 [74.81452179]
 [74.82701111]
 [74.83893585]].
[2019-03-27 12:20:35,426] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3642
[2019-03-27 12:20:35,434] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.13333333333333, 88.66666666666666, 1.0, 2.0, 0.256341999938463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 417466.0831842221, 417466.0831842227, 161473.3418926864], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 358800.0000, 
sim time next is 359400.0000, 
raw observation next is [20.11666666666667, 88.83333333333334, 1.0, 2.0, 0.25656065928945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 417798.8141201269, 417798.8141201264, 161494.3555650893], 
processed observation next is [1.0, 0.13043478260869565, 0.15244865718799394, 0.8883333333333334, 1.0, 1.0, 0.10428995095114456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11605522614447969, 0.11605522614447956, 0.2410363515896855], 
reward next is 0.7590, 
noisyNet noise sample is [array([2.387418], dtype=float32), 0.07598976]. 
=============================================
[2019-03-27 12:20:43,158] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 12:20:43,160] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:20:43,162] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:20:43,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:43,163] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:20:43,165] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:20:43,166] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:20:43,165] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:43,167] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:43,168] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:43,166] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:20:43,194] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run27
[2019-03-27 12:20:43,214] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run27
[2019-03-27 12:20:43,233] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run27
[2019-03-27 12:20:43,234] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run27
[2019-03-27 12:20:43,275] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run27
[2019-03-27 12:20:46,667] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:20:46,668] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.33333333333333, 51.66666666666667, 1.0, 2.0, 0.2077089484356219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 347451.5315283163, 347451.531528317, 154722.2642521787]
[2019-03-27 12:20:46,670] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:20:46,675] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2768031e-21 1.0000000e+00 3.6307431e-28 3.1294807e-19 5.5487205e-27], sampled 0.34806912633587483
[2019-03-27 12:20:49,651] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:20:49,652] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.26666666666667, 87.33333333333334, 1.0, 2.0, 0.2630778905028809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 428627.6173948369, 428627.6173948369, 162163.38724596]
[2019-03-27 12:20:49,653] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:20:49,656] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.8606650e-22 1.0000000e+00 1.7841156e-29 1.8006417e-21 3.5306545e-28], sampled 0.4029993581131718
[2019-03-27 12:20:56,483] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:20:56,486] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.224289305, 90.98825701999999, 1.0, 2.0, 0.2422464140528874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402268.8006872411, 402268.8006872404, 159891.1895625441]
[2019-03-27 12:20:56,487] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:20:56,491] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.0773728e-20 1.0000000e+00 3.2134854e-27 1.6175214e-17 3.9752778e-26], sampled 0.002309036281439414
[2019-03-27 12:21:11,357] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:21:11,358] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.65, 75.5, 1.0, 2.0, 0.6619644599299026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 925091.0421754398, 925091.0421754398, 212846.3503275606]
[2019-03-27 12:21:11,358] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:21:11,363] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.0702291e-18 1.0000000e+00 1.3616217e-25 3.9032118e-14 1.0572416e-24], sampled 0.9734319005270992
[2019-03-27 12:21:31,975] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:21:31,976] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.18333333333333, 57.5, 1.0, 2.0, 0.5732231161443503, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9879552861736103, 6.911199999999999, 6.9112, 168.9129417464162, 1602662.19690781, 1602662.196907811, 349136.0164248129]
[2019-03-27 12:21:31,977] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:21:31,980] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6541847e-13 9.9994648e-01 8.8495554e-21 5.3544918e-05 2.0565709e-20], sampled 0.5803900148680303
[2019-03-27 12:21:57,560] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:21:57,563] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.5, 50.0, 1.0, 2.0, 0.5783743899200366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808230.0139801389, 808230.0139801389, 196785.889983388]
[2019-03-27 12:21:57,565] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:21:57,569] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.3750704e-21 1.0000000e+00 3.3316665e-29 2.1006744e-18 5.1124980e-28], sampled 0.298117631950773
[2019-03-27 12:21:58,892] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:21:58,893] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.53676985333333, 88.83308648666667, 1.0, 2.0, 0.5260829427486137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735131.7875794551, 735131.7875794556, 187783.1176343683]
[2019-03-27 12:21:58,895] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:21:58,897] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0201997e-19 1.0000000e+00 2.6647436e-27 6.4863030e-17 3.7724712e-26], sampled 0.7714960563976138
[2019-03-27 12:22:09,701] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:22:09,702] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.41666666666667, 80.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.231930301401244, 6.9112, 168.9107479819956, 1681444.903759142, 1453910.764296407, 311355.9275346305]
[2019-03-27 12:22:09,704] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:22:09,712] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.6085572e-21 1.0000000e+00 8.5223255e-29 6.7464308e-19 1.8478641e-27], sampled 0.719688892151974
[2019-03-27 12:22:09,713] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1681444.903759142 W.
[2019-03-27 12:22:15,294] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:22:15,295] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.74740797666666, 79.30910614, 1.0, 2.0, 0.5538975469089524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774013.2030815291, 774013.2030815291, 192470.9149490075]
[2019-03-27 12:22:15,296] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:22:15,297] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7913911e-20 1.0000000e+00 5.0457126e-28 2.1760650e-17 6.7092920e-27], sampled 0.6003261212238834
[2019-03-27 12:22:21,979] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:22:21,979] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.38732416, 67.789831225, 1.0, 2.0, 0.5899961459442064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824476.7527672226, 824476.7527672226, 198895.4145758357]
[2019-03-27 12:22:21,982] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:22:21,985] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.7277727e-19 1.0000000e+00 9.7105226e-27 9.8071535e-14 4.7234252e-26], sampled 0.8900125766861481
[2019-03-27 12:22:23,727] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:22:23,728] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.68333333333333, 67.33333333333333, 1.0, 2.0, 0.5371825052149364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750647.4480241525, 750647.448024152, 189627.2052148219]
[2019-03-27 12:22:23,730] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:22:23,735] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.6239371e-22 1.0000000e+00 6.7963758e-30 9.8211574e-21 1.5517770e-28], sampled 0.14787576805117264
[2019-03-27 12:22:29,332] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:22:29,336] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.9, 89.5, 1.0, 2.0, 0.9873386130375937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1380094.816084644, 1380094.816084645, 295091.8399055745]
[2019-03-27 12:22:29,340] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:22:29,343] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2691542e-21 1.0000000e+00 9.9369619e-29 8.0636167e-19 1.9479847e-27], sampled 0.6697868085654358
[2019-03-27 12:22:40,612] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:22:40,614] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.2, 75.0, 1.0, 2.0, 0.5764656861303795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805561.7480388987, 805561.7480388987, 196443.0767863482]
[2019-03-27 12:22:40,615] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:22:40,620] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.0322510e-23 1.0000000e+00 8.4112771e-31 8.2812578e-22 2.1106712e-29], sampled 0.060698930683527164
[2019-03-27 12:22:47,347] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05284687], dtype=float32), 0.04752024]
[2019-03-27 12:22:47,348] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.5, 62.33333333333334, 1.0, 2.0, 1.003275475402807, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.971432778326781, 6.9112, 168.9125473854872, 2299586.659413605, 2256855.556915821, 465479.2741816769]
[2019-03-27 12:22:47,348] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:22:47,351] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.36656038e-13 9.85388219e-01 4.81995300e-20 1.46117695e-02
 3.84846394e-20], sampled 0.8892991447749417
[2019-03-27 12:22:47,351] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2299586.659413605 W.
[2019-03-27 12:22:50,481] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8678.5832 2777420930.6362 887.0000
[2019-03-27 12:22:50,522] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7955.3058 3157293960.7628 1618.0000
[2019-03-27 12:22:50,960] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8511.6627 2841099158.1474 1095.0000
[2019-03-27 12:22:51,085] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8011.7727 3006309022.4974 1735.0000
[2019-03-27 12:22:51,227] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8291.3134 2923243179.4277 1247.0000
[2019-03-27 12:22:52,242] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 650000, evaluation results [650000.0, 7955.305848222063, 3157293960.7627554, 1618.0, 8291.313407740121, 2923243179.427679, 1247.0, 8678.583200871995, 2777420930.6361866, 887.0, 8011.772734768953, 3006309022.497417, 1735.0, 8511.662683523322, 2841099158.147402, 1095.0]
[2019-03-27 12:22:54,182] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2882597e-18 1.0000000e+00 2.5825525e-26 1.5700073e-15 5.1067925e-26], sum to 1.0000
[2019-03-27 12:22:54,194] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7876
[2019-03-27 12:22:54,210] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.13333333333333, 56.33333333333334, 1.0, 2.0, 0.2452469301751609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403336.2076768128, 403336.2076768128, 160441.8359325455], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 494400.0000, 
sim time next is 495000.0000, 
raw observation next is [23.9, 57.5, 1.0, 2.0, 0.2428323260854071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399503.9608848941, 399503.9608848948, 160206.8695239888], 
processed observation next is [1.0, 0.7391304347826086, 0.33175355450236965, 0.575, 1.0, 1.0, 0.0877497904643459, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11097332246802614, 0.11097332246802634, 0.2391147306328191], 
reward next is 0.7609, 
noisyNet noise sample is [array([-0.45335737], dtype=float32), -1.0957015]. 
=============================================
[2019-03-27 12:22:54,229] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.128654]
 [75.19182 ]
 [73.716324]
 [73.76752 ]
 [73.668625]], R is [[76.54072571]
 [76.53585815]
 [76.52057648]
 [76.44516754]
 [76.37432861]].
[2019-03-27 12:22:57,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4037674e-21 1.0000000e+00 1.7944357e-29 1.4382081e-18 1.0295428e-28], sum to 1.0000
[2019-03-27 12:22:57,761] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4862
[2019-03-27 12:22:57,767] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.45, 71.5, 1.0, 2.0, 0.2478202630018402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 408548.6347422035, 408548.634742204, 160668.6997043289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 588600.0000, 
sim time next is 589200.0000, 
raw observation next is [21.3, 72.0, 1.0, 2.0, 0.244931129363862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 404123.4828991839, 404123.4828991832, 160374.5148678368], 
processed observation next is [1.0, 0.8260869565217391, 0.2085308056872039, 0.72, 1.0, 1.0, 0.09027846911308673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11225652302755107, 0.11225652302755088, 0.23936494756393553], 
reward next is 0.7606, 
noisyNet noise sample is [array([1.3749129], dtype=float32), -1.7781916]. 
=============================================
[2019-03-27 12:23:24,564] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9655715e-19 1.0000000e+00 2.0964332e-24 3.9526675e-13 2.1044939e-24], sum to 1.0000
[2019-03-27 12:23:24,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8108
[2019-03-27 12:23:24,581] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.83333333333334, 1.0, 2.0, 0.4068980605363464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 623561.2886847962, 623561.2886847956, 177085.8042402147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1012200.0000, 
sim time next is 1012800.0000, 
raw observation next is [21.7, 97.66666666666667, 1.0, 2.0, 0.3574286508168133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548152.3398437325, 548152.3398437331, 170420.9135442379], 
processed observation next is [1.0, 0.7391304347826086, 0.2274881516587678, 0.9766666666666667, 1.0, 1.0, 0.22581765158652203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15226453884548127, 0.1522645388454814, 0.2543595724540864], 
reward next is 0.7456, 
noisyNet noise sample is [array([-0.89171], dtype=float32), -0.50307107]. 
=============================================
[2019-03-27 12:23:25,224] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.5673687e-22 1.0000000e+00 1.1502831e-28 1.7963625e-19 2.3838181e-27], sum to 1.0000
[2019-03-27 12:23:25,233] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3814
[2019-03-27 12:23:25,240] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.0, 1.0, 2.0, 0.3380691997389883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523681.2813667222, 523681.2813667222, 168583.7487958096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 946800.0000, 
sim time next is 947400.0000, 
raw observation next is [21.8, 94.00000000000001, 1.0, 2.0, 0.3376778345863402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523118.7175607817, 523118.7175607817, 168540.317590579], 
processed observation next is [0.0, 1.0, 0.23222748815165886, 0.9400000000000002, 1.0, 1.0, 0.20202148745342194, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1453107548779949, 0.1453107548779949, 0.2515527128217597], 
reward next is 0.7484, 
noisyNet noise sample is [array([-2.3772044], dtype=float32), -0.011309953]. 
=============================================
[2019-03-27 12:23:32,165] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7082761e-17 1.0000000e+00 1.3629059e-24 8.4398431e-14 3.1476712e-24], sum to 1.0000
[2019-03-27 12:23:32,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3025
[2019-03-27 12:23:32,186] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333334, 71.0, 1.0, 2.0, 0.597826819227571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928211.6839156456, 928211.6839156456, 211449.8592102245], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1084800.0000, 
sim time next is 1085400.0000, 
raw observation next is [25.1, 70.5, 1.0, 2.0, 0.6693726472986706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1036894.003653821, 1036894.003653821, 226848.2525376893], 
processed observation next is [1.0, 0.5652173913043478, 0.38862559241706174, 0.705, 1.0, 1.0, 0.6016537919261091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2880261121260614, 0.2880261121260614, 0.33857948139953625], 
reward next is 0.6614, 
noisyNet noise sample is [array([-1.4547654], dtype=float32), 0.8323166]. 
=============================================
[2019-03-27 12:23:36,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.28935875e-23 1.00000000e+00 4.15797931e-31 8.45515475e-24
 1.19142766e-29], sum to 1.0000
[2019-03-27 12:23:37,005] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0020
[2019-03-27 12:23:37,015] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 91.0, 1.0, 2.0, 0.2789237662727559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 449301.7335277609, 449301.7335277603, 163553.2581702353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1144800.0000, 
sim time next is 1145400.0000, 
raw observation next is [20.63333333333334, 90.33333333333334, 1.0, 2.0, 0.3657571851556497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 588589.6711988831, 588589.6711988825, 174144.7103898676], 
processed observation next is [1.0, 0.2608695652173913, 0.17693522906793085, 0.9033333333333334, 1.0, 1.0, 0.2358520303080117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16349713088857865, 0.16349713088857848, 0.25991747819383226], 
reward next is 0.7401, 
noisyNet noise sample is [array([0.4841707], dtype=float32), 0.43218547]. 
=============================================
[2019-03-27 12:23:40,121] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.29665254e-20 1.00000000e+00 5.16231770e-27 4.22831039e-16
 2.40757097e-26], sum to 1.0000
[2019-03-27 12:23:40,130] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4661
[2019-03-27 12:23:40,135] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 96.5, 1.0, 2.0, 0.3155863708731341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 500212.599310751, 500212.5993107517, 167062.6315749347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1380600.0000, 
sim time next is 1381200.0000, 
raw observation next is [20.56666666666667, 96.66666666666666, 1.0, 2.0, 0.3137251415112928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497422.8905395843, 497422.8905395843, 166857.6158303449], 
processed observation next is [1.0, 1.0, 0.17377567140600336, 0.9666666666666666, 1.0, 1.0, 0.17316282109794315, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13817302514988453, 0.13817302514988453, 0.2490412176572312], 
reward next is 0.7510, 
noisyNet noise sample is [array([-0.48927864], dtype=float32), 0.8180016]. 
=============================================
[2019-03-27 12:23:48,480] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.1966144e-27 1.0000000e+00 2.3168210e-35 4.3933246e-32 3.0137510e-34], sum to 1.0000
[2019-03-27 12:23:48,493] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6193
[2019-03-27 12:23:48,501] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 64.66666666666667, 1.0, 2.0, 0.3609115328024661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549785.6662363309, 549785.6662363316, 170443.7135454415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1503600.0000, 
sim time next is 1504200.0000, 
raw observation next is [26.93333333333333, 62.83333333333333, 1.0, 2.0, 0.3593082526810152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548245.3583190968, 548245.3583190962, 170342.747135036], 
processed observation next is [0.0, 0.391304347826087, 0.4755134281200631, 0.6283333333333333, 1.0, 1.0, 0.22808223214580142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15229037731086023, 0.15229037731086004, 0.25424290617169554], 
reward next is 0.7458, 
noisyNet noise sample is [array([0.31465015], dtype=float32), -0.187623]. 
=============================================
[2019-03-27 12:23:49,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5050654e-24 1.0000000e+00 3.4556025e-32 2.4405771e-23 4.1143059e-31], sum to 1.0000
[2019-03-27 12:23:49,579] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7219
[2019-03-27 12:23:49,585] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 94.0, 1.0, 2.0, 0.46242666146235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653740.3159248809, 653740.3159248809, 178897.1077659755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1293600.0000, 
sim time next is 1294200.0000, 
raw observation next is [24.35, 94.0, 1.0, 2.0, 0.4612311418621994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652529.954117756, 652529.954117756, 178782.6678411399], 
processed observation next is [1.0, 1.0, 0.35308056872037924, 0.94, 1.0, 1.0, 0.35088089380987875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18125832058826555, 0.18125832058826555, 0.26683980274797], 
reward next is 0.7332, 
noisyNet noise sample is [array([-0.09493966], dtype=float32), 1.6342498]. 
=============================================
[2019-03-27 12:23:51,238] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8596643e-24 1.0000000e+00 1.9701601e-32 4.4116627e-25 2.3555479e-30], sum to 1.0000
[2019-03-27 12:23:51,246] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9959
[2019-03-27 12:23:51,256] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333334, 93.33333333333334, 1.0, 2.0, 0.5404809540497971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780564.6154391714, 780564.6154391721, 193394.8741250193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1318800.0000, 
sim time next is 1319400.0000, 
raw observation next is [23.85, 93.5, 1.0, 2.0, 0.5543588314973338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802578.0055774802, 802578.0055774802, 196096.477506292], 
processed observation next is [1.0, 0.2608695652173913, 0.3293838862559243, 0.935, 1.0, 1.0, 0.46308292951485996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2229383348826334, 0.2229383348826334, 0.2926813097108836], 
reward next is 0.7073, 
noisyNet noise sample is [array([-0.6709238], dtype=float32), 0.3045068]. 
=============================================
[2019-03-27 12:23:58,180] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 12:23:58,182] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:23:58,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:23:58,184] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:23:58,186] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:23:58,188] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:23:58,189] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:23:58,190] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:23:58,191] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:23:58,192] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:23:58,193] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:23:58,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run28
[2019-03-27 12:23:58,231] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run28
[2019-03-27 12:23:58,250] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run28
[2019-03-27 12:23:58,253] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run28
[2019-03-27 12:23:58,292] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run28
[2019-03-27 12:24:39,560] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0503897], dtype=float32), 0.04773391]
[2019-03-27 12:24:39,561] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.16170872, 96.33593433333334, 1.0, 2.0, 0.6240893034767919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872139.0241326219, 872139.0241326219, 205319.4209693897]
[2019-03-27 12:24:39,561] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:24:39,562] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2844471e-22 1.0000000e+00 1.1039052e-30 1.2824257e-21 1.3994070e-29], sampled 0.1874227594521949
[2019-03-27 12:24:47,454] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0503897], dtype=float32), 0.04773391]
[2019-03-27 12:24:47,455] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.79920378, 84.56896033, 1.0, 2.0, 0.3717642778536489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577478.1238815906, 577478.1238815906, 173100.204563114]
[2019-03-27 12:24:47,456] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:24:47,459] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8910047e-24 1.0000000e+00 5.6863387e-32 2.3086808e-25 1.0591397e-30], sampled 0.4011506038162711
[2019-03-27 12:25:14,815] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0503897], dtype=float32), 0.04773391]
[2019-03-27 12:25:14,815] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.40770053, 81.41286249666666, 1.0, 2.0, 0.5184010489305475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724393.6901547113, 724393.6901547113, 186528.8515455389]
[2019-03-27 12:25:14,816] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:25:14,819] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.50382495e-23 1.00000000e+00 8.40409248e-31 1.56790106e-22
 1.12833096e-29], sampled 0.9537917932784277
[2019-03-27 12:25:17,986] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0503897], dtype=float32), 0.04773391]
[2019-03-27 12:25:17,989] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.70222971333333, 79.95290849, 1.0, 2.0, 0.6759861721367304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 944695.0092512857, 944695.0092512851, 215748.3445045437]
[2019-03-27 12:25:17,991] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:25:17,995] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.4341296e-25 1.0000000e+00 7.6829510e-33 1.0603668e-26 1.5979885e-31], sampled 0.5553596092791158
[2019-03-27 12:25:30,422] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0503897], dtype=float32), 0.04773391]
[2019-03-27 12:25:30,424] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.33333333333334, 87.0, 1.0, 2.0, 0.5848535326754221, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817287.5594533919, 817287.5594533919, 197957.2676872911]
[2019-03-27 12:25:30,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:25:30,430] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4755111e-25 1.0000000e+00 1.4185490e-33 2.1184746e-26 3.0249264e-32], sampled 0.7680462079298274
[2019-03-27 12:26:05,782] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.7620 2927208662.2085 1337.0000
[2019-03-27 12:26:06,816] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.0307 2779208230.3643 931.0000
[2019-03-27 12:26:06,818] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-03-27 12:26:06,904] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.2516 3164050981.0704 1774.0000
[2019-03-27 12:26:06,965] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 12:26:07,981] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 675000, evaluation results [675000.0, 7885.251620818767, 3164050981.070424, 1774.0, 8254.761983435208, 2927208662.2084846, 1337.0, 8662.030679539366, 2779208230.36428, 931.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 12:26:14,229] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.2387536e-26 1.0000000e+00 1.7640185e-34 1.2871697e-29 1.0397075e-32], sum to 1.0000
[2019-03-27 12:26:14,239] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0833
[2019-03-27 12:26:14,247] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 76.33333333333334, 1.0, 2.0, 0.371240135605223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 560304.8119079579, 560304.8119079586, 171176.2902761309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1500000.0000, 
sim time next is 1500600.0000, 
raw observation next is [25.33333333333334, 74.16666666666666, 1.0, 2.0, 0.3686960512751951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557566.4620109147, 557566.4620109147, 170976.1197623294], 
processed observation next is [0.0, 0.34782608695652173, 0.3996840442338076, 0.7416666666666666, 1.0, 1.0, 0.23939283286168087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15487957278080963, 0.15487957278080963, 0.25518823845123795], 
reward next is 0.7448, 
noisyNet noise sample is [array([-1.2736135], dtype=float32), 0.6048041]. 
=============================================
[2019-03-27 12:26:16,051] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.7634895e-27 1.0000000e+00 2.5114451e-34 5.8830100e-29 1.3166418e-32], sum to 1.0000
[2019-03-27 12:26:16,066] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3303
[2019-03-27 12:26:16,080] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.35, 53.0, 1.0, 2.0, 0.373145159209325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561859.0580244237, 561859.0580244237, 171267.7089141779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1521000.0000, 
sim time next is 1521600.0000, 
raw observation next is [29.2, 53.33333333333334, 1.0, 2.0, 0.3717713452669654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561023.7390579762, 561023.7390579769, 171235.8826332963], 
processed observation next is [0.0, 0.6086956521739131, 0.5829383886255924, 0.5333333333333334, 1.0, 1.0, 0.24309800634574147, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15583992751610448, 0.15583992751610468, 0.2555759442288005], 
reward next is 0.7444, 
noisyNet noise sample is [array([-0.83223194], dtype=float32), -1.3593328]. 
=============================================
[2019-03-27 12:26:23,438] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2587359e-20 1.0000000e+00 1.3070165e-28 9.2722128e-19 1.3647820e-27], sum to 1.0000
[2019-03-27 12:26:23,452] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3395
[2019-03-27 12:26:23,458] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 94.0, 1.0, 2.0, 0.5065683184638683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707853.5785466613, 707853.5785466619, 184631.7498514981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1725600.0000, 
sim time next is 1726200.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5047705006445073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705340.5625705486, 705340.5625705493, 184347.0012442233], 
processed observation next is [1.0, 1.0, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4033379525837438, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1959279340473746, 0.1959279340473748, 0.2751447779764527], 
reward next is 0.7249, 
noisyNet noise sample is [array([0.35621423], dtype=float32), 0.16660926]. 
=============================================
[2019-03-27 12:26:24,010] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.2127259e-25 1.0000000e+00 4.2701563e-34 2.8861811e-27 3.8329491e-32], sum to 1.0000
[2019-03-27 12:26:24,021] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3568
[2019-03-27 12:26:24,030] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.81666666666667, 97.33333333333334, 1.0, 2.0, 0.5338018664416964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759378.0177450682, 759378.0177450682, 190781.882326112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1671000.0000, 
sim time next is 1671600.0000, 
raw observation next is [23.93333333333333, 96.66666666666667, 1.0, 2.0, 0.7759857961140745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1102290.8804797, 1102290.8804797, 240568.0929563844], 
processed observation next is [1.0, 0.34782608695652173, 0.3333333333333332, 0.9666666666666667, 1.0, 1.0, 0.7301033688121379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30619191124436107, 0.30619191124436107, 0.3590568551587827], 
reward next is 0.6409, 
noisyNet noise sample is [array([0.03700794], dtype=float32), 0.021678032]. 
=============================================
[2019-03-27 12:26:30,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2058800e-22 1.0000000e+00 2.1717881e-30 7.3809163e-22 9.5473192e-29], sum to 1.0000
[2019-03-27 12:26:30,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2361
[2019-03-27 12:26:30,984] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 94.0, 1.0, 2.0, 0.4742197296898489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662885.1596225568, 662885.1596225568, 179687.1728541443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1735200.0000, 
sim time next is 1735800.0000, 
raw observation next is [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.7321083217970223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024919.433990161, 1024919.433990161, 228165.068627696], 
processed observation next is [1.0, 0.08695652173913043, 0.3641390205371251, 0.9400000000000002, 1.0, 1.0, 0.6772389419241233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28469984277504473, 0.28469984277504473, 0.3405448785488], 
reward next is 0.6595, 
noisyNet noise sample is [array([-0.5807599], dtype=float32), -0.2363884]. 
=============================================
[2019-03-27 12:26:31,926] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1681873e-25 1.0000000e+00 1.2076154e-31 6.0303702e-26 1.3106098e-30], sum to 1.0000
[2019-03-27 12:26:31,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7884
[2019-03-27 12:26:31,954] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 89.0, 1.0, 2.0, 0.5579299271410525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783100.424087762, 783100.424087762, 193610.5593701128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1756800.0000, 
sim time next is 1757400.0000, 
raw observation next is [25.11666666666667, 89.0, 1.0, 2.0, 0.6837319269036024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 962010.7127010258, 962010.7127010258, 218238.0462167196], 
processed observation next is [1.0, 0.34782608695652173, 0.38941548183254365, 0.89, 1.0, 1.0, 0.6189541287995209, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26722519797250716, 0.26722519797250716, 0.32572842718913375], 
reward next is 0.6743, 
noisyNet noise sample is [array([-0.21201472], dtype=float32), 0.66440314]. 
=============================================
[2019-03-27 12:26:32,229] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8921850e-25 1.0000000e+00 1.8985769e-32 1.1068470e-25 3.5720083e-31], sum to 1.0000
[2019-03-27 12:26:32,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3257
[2019-03-27 12:26:32,252] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333333, 91.33333333333334, 1.0, 2.0, 0.6240031988811429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 878010.2438542672, 878010.2438542666, 206078.7569151067], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1752600.0000, 
sim time next is 1753200.0000, 
raw observation next is [24.9, 91.0, 1.0, 2.0, 0.5358367082773587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753192.4941170381, 753192.4941170374, 189967.877889683], 
processed observation next is [1.0, 0.30434782608695654, 0.3791469194312796, 0.91, 1.0, 1.0, 0.44076711840645627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2092201372547328, 0.2092201372547326, 0.28353414610400446], 
reward next is 0.7165, 
noisyNet noise sample is [array([0.42865536], dtype=float32), -1.4310167]. 
=============================================
[2019-03-27 12:26:34,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.0899854e-20 1.0000000e+00 1.4110562e-28 8.6168064e-20 3.9554882e-28], sum to 1.0000
[2019-03-27 12:26:34,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5165
[2019-03-27 12:26:34,570] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 93.33333333333334, 1.0, 2.0, 0.5371679581402501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851030.6025076078, 851030.6025076078, 201084.4899458381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1784400.0000, 
sim time next is 1785000.0000, 
raw observation next is [21.0, 93.66666666666667, 1.0, 2.0, 0.5309232420199458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840383.3991394218, 840383.3991394218, 199840.6788662375], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.9366666666666668, 1.0, 1.0, 0.43484727954210334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23343983309428382, 0.23343983309428382, 0.2982696699496082], 
reward next is 0.7017, 
noisyNet noise sample is [array([0.4423042], dtype=float32), -0.35717657]. 
=============================================
[2019-03-27 12:26:34,596] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[66.593956]
 [66.335205]
 [66.23994 ]
 [65.988495]
 [66.04075 ]], R is [[66.78422546]
 [66.81626129]
 [66.83800507]
 [66.86091614]
 [66.86949921]].
[2019-03-27 12:26:52,348] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9641179e-26 1.0000000e+00 3.4804023e-35 8.5304115e-32 2.6063143e-34], sum to 1.0000
[2019-03-27 12:26:52,358] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8167
[2019-03-27 12:26:52,367] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.85, 92.0, 1.0, 2.0, 0.5068470436227975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708243.1851724104, 708243.1851724099, 184675.9932806488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2028600.0000, 
sim time next is 2029200.0000, 
raw observation next is [25.96666666666667, 91.33333333333334, 1.0, 2.0, 0.5071619266429156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708683.3340249904, 708683.3340249899, 184726.0234999185], 
processed observation next is [0.0, 0.4782608695652174, 0.42969984202211703, 0.9133333333333334, 1.0, 1.0, 0.4062191887264043, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19685648167360845, 0.1968564816736083, 0.27571048283569927], 
reward next is 0.7243, 
noisyNet noise sample is [array([-0.6920147], dtype=float32), -1.0729712]. 
=============================================
[2019-03-27 12:26:53,029] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.1576741e-26 1.0000000e+00 1.2844833e-34 9.8139174e-31 1.3496350e-32], sum to 1.0000
[2019-03-27 12:26:53,040] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9541
[2019-03-27 12:26:53,046] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 88.0, 1.0, 2.0, 0.5448040039119764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761301.3853222219, 761301.3853222213, 190913.5761731488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2147400.0000, 
sim time next is 2148000.0000, 
raw observation next is [27.26666666666667, 88.33333333333333, 1.0, 2.0, 0.5425320004853653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758125.3869766641, 758125.3869766641, 190528.2587951988], 
processed observation next is [0.0, 0.8695652173913043, 0.4913112164297, 0.8833333333333333, 1.0, 1.0, 0.4488337355245365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2105903852712956, 0.2105903852712956, 0.2843705355152221], 
reward next is 0.7156, 
noisyNet noise sample is [array([-0.11563823], dtype=float32), 0.41539967]. 
=============================================
[2019-03-27 12:26:53,067] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.87976 ]
 [74.874466]
 [74.88521 ]
 [74.88752 ]
 [74.83739 ]], R is [[74.86017609]
 [74.82662964]
 [74.79309845]
 [74.75994873]
 [74.72697449]].
[2019-03-27 12:27:03,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.00317033e-25 1.00000000e+00 4.31599685e-33 1.17894944e-29
 6.23108632e-32], sum to 1.0000
[2019-03-27 12:27:03,383] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9368
[2019-03-27 12:27:03,389] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 82.33333333333334, 1.0, 2.0, 0.5445081688488133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 760887.8415388272, 760887.8415388279, 190863.5972684024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2236200.0000, 
sim time next is 2236800.0000, 
raw observation next is [28.2, 82.66666666666667, 1.0, 2.0, 0.5433030048505412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759203.1609540338, 759203.1609540345, 190659.0467544961], 
processed observation next is [1.0, 0.9130434782608695, 0.5355450236966824, 0.8266666666666667, 1.0, 1.0, 0.4497626564464351, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21088976693167608, 0.21088976693167627, 0.28456574142462104], 
reward next is 0.7154, 
noisyNet noise sample is [array([1.6720338], dtype=float32), -0.30175453]. 
=============================================
[2019-03-27 12:27:03,497] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1354095e-09 9.9055111e-01 9.7982527e-16 9.4488496e-03 2.2839487e-15], sum to 1.0000
[2019-03-27 12:27:03,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6984
[2019-03-27 12:27:03,524] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1884143.013920815 W.
[2019-03-27 12:27:03,531] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.96666666666667, 63.33333333333334, 1.0, 2.0, 0.67381664028585, 1.0, 2.0, 0.67381664028585, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1884143.013920815, 1884143.013920815, 363119.937363924], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2290800.0000, 
sim time next is 2291400.0000, 
raw observation next is [31.95, 63.5, 1.0, 2.0, 0.4034714696601834, 1.0, 2.0, 0.4034714696601834, 1.0, 1.0, 0.7006965127457913, 6.9112, 6.9112, 170.5573041426782, 1692144.104514205, 1692144.104514205, 354818.2819055984], 
processed observation next is [1.0, 0.5217391304347826, 0.7132701421800948, 0.635, 1.0, 1.0, 0.28129092730142574, 1.0, 1.0, 0.28129092730142574, 1.0, 0.5, 0.6349957472509649, 0.0, 0.0, 0.8375144448122397, 0.4700400290317236, 0.4700400290317236, 0.5295795252322364], 
reward next is 0.4704, 
noisyNet noise sample is [array([0.30429617], dtype=float32), -0.97119814]. 
=============================================
[2019-03-27 12:27:08,600] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9443835e-20 1.0000000e+00 9.4440840e-29 2.2314055e-22 1.5591414e-27], sum to 1.0000
[2019-03-27 12:27:08,613] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5060
[2019-03-27 12:27:08,620] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 81.0, 1.0, 2.0, 0.740376489022167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1034724.728228783, 1034724.728228783, 229799.892047152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2352000.0000, 
sim time next is 2352600.0000, 
raw observation next is [27.5, 80.5, 1.0, 2.0, 0.7381116248727769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1031557.89457807, 1031557.89457807, 229284.8295948375], 
processed observation next is [1.0, 0.21739130434782608, 0.5023696682464456, 0.805, 1.0, 1.0, 0.6844718371961168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28654385960501944, 0.28654385960501944, 0.34221616357438434], 
reward next is 0.6578, 
noisyNet noise sample is [array([0.4878893], dtype=float32), 0.060131557]. 
=============================================
[2019-03-27 12:27:12,655] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3375926e-21 1.0000000e+00 2.9602726e-30 7.0813827e-24 1.2354018e-28], sum to 1.0000
[2019-03-27 12:27:12,666] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5963
[2019-03-27 12:27:12,670] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 80.0, 1.0, 2.0, 0.5613385714576048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 784415.086834244, 784415.086834244, 193764.4831830124], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2325600.0000, 
sim time next is 2326200.0000, 
raw observation next is [29.01666666666667, 80.16666666666667, 1.0, 2.0, 0.5599498469338116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 782473.7669935346, 782473.7669935353, 193521.8926222118], 
processed observation next is [1.0, 0.9565217391304348, 0.5742496050552924, 0.8016666666666667, 1.0, 1.0, 0.4698190926913392, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21735382416487073, 0.21735382416487092, 0.28883864570479373], 
reward next is 0.7112, 
noisyNet noise sample is [array([-0.49294528], dtype=float32), -0.27566072]. 
=============================================
[2019-03-27 12:27:14,123] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 12:27:14,125] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:27:14,126] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:27:14,126] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:27:14,127] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:27:14,127] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:27:14,128] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:27:14,130] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:27:14,129] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:27:14,135] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:27:14,133] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:27:14,157] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run29
[2019-03-27 12:27:14,176] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run29
[2019-03-27 12:27:14,192] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run29
[2019-03-27 12:27:14,214] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run29
[2019-03-27 12:27:14,215] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run29
[2019-03-27 12:27:18,990] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04955615], dtype=float32), 0.045419443]
[2019-03-27 12:27:18,994] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.43333333333334, 92.33333333333334, 1.0, 2.0, 0.2848866203252448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 457874.7427059273, 457874.7427059267, 164125.7888438743]
[2019-03-27 12:27:18,995] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:27:18,997] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2750074e-27 1.0000000e+00 6.7629410e-36 7.7536985e-34 2.3271769e-34], sampled 0.3654810655195757
[2019-03-27 12:27:25,051] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04955615], dtype=float32), 0.045419443]
[2019-03-27 12:27:25,053] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.8, 88.83333333333334, 1.0, 2.0, 0.2884245597602482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 464411.7613842008, 464411.7613842001, 164574.6568107712]
[2019-03-27 12:27:25,053] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:27:25,055] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.0230985e-28 1.0000000e+00 4.6597428e-36 6.2757566e-35 1.2403563e-34], sampled 0.4273366545187086
[2019-03-27 12:27:28,855] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04955615], dtype=float32), 0.045419443]
[2019-03-27 12:27:28,855] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.3, 96.0, 1.0, 2.0, 0.3843383964763517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 582069.4260071197, 582069.4260071191, 173152.7864218161]
[2019-03-27 12:27:28,856] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:27:28,859] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2898898e-26 1.0000000e+00 4.5468216e-35 2.4569706e-31 1.5976090e-33], sampled 0.7543046706353927
[2019-03-27 12:27:40,695] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04955615], dtype=float32), 0.045419443]
[2019-03-27 12:27:40,696] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.98605178, 96.646942965, 1.0, 2.0, 0.4903783621249701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691027.5823717011, 691027.5823717004, 182847.7908125785]
[2019-03-27 12:27:40,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:27:40,700] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0213326e-25 1.0000000e+00 1.3011380e-33 9.9943990e-30 3.5812435e-32], sampled 0.8440586101199878
[2019-03-27 12:27:42,354] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04955615], dtype=float32), 0.045419443]
[2019-03-27 12:27:42,357] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.07849461, 82.09167602166667, 1.0, 2.0, 0.4769442164027697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673996.3138004624, 673996.3138004618, 181033.9079162858]
[2019-03-27 12:27:42,358] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:27:42,361] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.9030286e-25 1.0000000e+00 7.3126016e-34 6.3999453e-29 2.0627735e-32], sampled 0.2456937921032314
[2019-03-27 12:28:01,180] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.04955615], dtype=float32), 0.045419443]
[2019-03-27 12:28:01,181] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.15, 77.5, 1.0, 2.0, 0.6279176661070672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 877491.2128339102, 877491.2128339102, 206054.379497607]
[2019-03-27 12:28:01,185] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:28:01,188] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.39557161e-25 1.00000000e+00 9.31749169e-34 1.35767115e-30
 2.01230086e-32], sampled 0.23213622099515963
[2019-03-27 12:28:05,900] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04955615], dtype=float32), 0.045419443]
[2019-03-27 12:28:05,900] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.77395739666667, 90.63580825166667, 1.0, 2.0, 0.4339481383014218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 639624.295272542, 639624.2952725427, 178112.1670949633]
[2019-03-27 12:28:05,901] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:28:05,903] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1807347e-25 1.0000000e+00 7.1426058e-34 7.5772536e-29 2.1323648e-32], sampled 0.4799636509344063
[2019-03-27 12:29:03,238] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04955615], dtype=float32), 0.045419443]
[2019-03-27 12:29:03,244] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.9, 65.66666666666667, 1.0, 2.0, 0.7468566260200566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1043785.595769323, 1043785.595769324, 231287.0132220485]
[2019-03-27 12:29:03,244] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:29:03,247] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5493728e-24 1.0000000e+00 9.9054538e-33 1.5529850e-28 2.2922634e-31], sampled 0.6211243546272565
[2019-03-27 12:29:20,963] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8893 2779197741.5739 933.0000
[2019-03-27 12:29:21,631] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1249 2927277438.0038 1338.0000
[2019-03-27 12:29:22,135] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 12:29:22,346] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.9159 3163634985.7536 1769.0000
[2019-03-27 12:29:22,378] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.3539 2842506384.9479 1131.0000
[2019-03-27 12:29:23,396] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 700000, evaluation results [700000.0, 7886.915881004086, 3163634985.7536316, 1769.0, 8255.124881931173, 2927277438.00382, 1338.0, 8659.889250812355, 2779197741.573899, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8496.353948261409, 2842506384.947873, 1131.0]
[2019-03-27 12:29:26,004] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.8404872e-09 9.3452400e-01 1.5716682e-15 6.5475985e-02 6.3353056e-16], sum to 1.0000
[2019-03-27 12:29:26,012] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6902
[2019-03-27 12:29:26,020] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2574615.417039727 W.
[2019-03-27 12:29:26,024] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.11666666666667, 60.83333333333334, 1.0, 2.0, 0.9204726093876229, 1.0, 2.0, 0.9204726093876229, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2574615.417039727, 2574615.417039727, 482785.205523545], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2391000.0000, 
sim time next is 2391600.0000, 
raw observation next is [33.13333333333334, 60.66666666666667, 1.0, 2.0, 0.5421024397659849, 1.0, 2.0, 0.5421024397659849, 1.0, 1.0, 0.9414526618571871, 6.9112, 6.9112, 170.5573041426782, 2274164.923355829, 2274164.923355829, 445482.1219405862], 
processed observation next is [1.0, 0.6956521739130435, 0.7693522906793052, 0.6066666666666667, 1.0, 1.0, 0.44831619248913845, 1.0, 1.0, 0.44831619248913845, 1.0, 0.5, 0.928600807142911, 0.0, 0.0, 0.8375144448122397, 0.6317124787099525, 0.6317124787099525, 0.6648986894635615], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.39387587], dtype=float32), -0.46258563]. 
=============================================
[2019-03-27 12:29:33,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2462125e-25 1.0000000e+00 1.7710248e-33 2.8405435e-29 1.1492330e-31], sum to 1.0000
[2019-03-27 12:29:33,039] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3700
[2019-03-27 12:29:33,042] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 95.0, 1.0, 2.0, 0.7298930664276032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1020066.420432293, 1020066.420432294, 227430.54580256], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2529000.0000, 
sim time next is 2529600.0000, 
raw observation next is [26.36666666666667, 94.66666666666666, 1.0, 2.0, 0.6889343645853419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 962798.393559916, 962798.393559916, 218475.4372980976], 
processed observation next is [1.0, 0.2608695652173913, 0.4486571879936811, 0.9466666666666665, 1.0, 1.0, 0.6252221260064359, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2674439982110878, 0.2674439982110878, 0.32608274223596656], 
reward next is 0.6739, 
noisyNet noise sample is [array([0.2864334], dtype=float32), 0.019329378]. 
=============================================
[2019-03-27 12:29:36,836] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9351074e-28 1.0000000e+00 3.0745961e-36 2.2168063e-35 1.2159945e-34], sum to 1.0000
[2019-03-27 12:29:36,852] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7603
[2019-03-27 12:29:36,859] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4749715298666664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663817.185979863, 663817.185979863, 179783.9465901326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2623200.0000, 
sim time next is 2623800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4754971946229017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664551.669029083, 664551.6690290824, 179862.3850760613], 
processed observation next is [0.0, 0.34782608695652173, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3680689091842189, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18459768584141195, 0.18459768584141178, 0.26845132100904673], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.77859807], dtype=float32), 0.3156541]. 
=============================================
[2019-03-27 12:29:37,273] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3099611e-15 1.0000000e+00 5.9423669e-24 3.4846357e-15 1.9207091e-23], sum to 1.0000
[2019-03-27 12:29:37,285] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5798
[2019-03-27 12:29:37,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2980575.456479793 W.
[2019-03-27 12:29:37,302] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.26666666666667, 69.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.875804604067183, 6.9112, 168.9075854707047, 2980575.456479793, 2296273.511130759, 474453.0074706973], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2554800.0000, 
sim time next is 2555400.0000, 
raw observation next is [30.38333333333333, 68.5, 1.0, 2.0, 0.6590568314010117, 1.0, 1.0, 0.6501184552147683, 1.0, 2.0, 1.03, 7.002971652036514, 6.9112, 170.5573041426782, 2727795.729153998, 2662055.961768113, 508797.0487902027], 
processed observation next is [1.0, 0.5652173913043478, 0.6390205371248023, 0.685, 1.0, 1.0, 0.589225098073508, 1.0, 0.5, 0.578455970138275, 1.0, 1.0, 1.0365853658536586, 0.009177165203651416, 0.0, 0.8375144448122397, 0.7577210358761106, 0.7394599893800314, 0.7593985802838846], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5821919], dtype=float32), -1.9713546]. 
=============================================
[2019-03-27 12:29:58,531] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7533492e-29 1.0000000e+00 4.2981591e-38 2.0159079e-36 5.0200592e-37], sum to 1.0000
[2019-03-27 12:29:58,542] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0198
[2019-03-27 12:29:58,549] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 92.66666666666666, 1.0, 2.0, 0.5411510038919033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832656.5604470108, 832656.5604470108, 199474.2511208097], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2886000.0000, 
sim time next is 2886600.0000, 
raw observation next is [22.25, 92.33333333333333, 1.0, 2.0, 0.5338712714333719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821272.4421623924, 821272.4421623924, 198104.5331663125], 
processed observation next is [1.0, 0.391304347826087, 0.2535545023696683, 0.9233333333333333, 1.0, 1.0, 0.4383991222088818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2281312339339979, 0.2281312339339979, 0.2956784077109142], 
reward next is 0.7043, 
noisyNet noise sample is [array([-0.4013542], dtype=float32), 0.45726326]. 
=============================================
[2019-03-27 12:30:00,687] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0642265e-30 1.0000000e+00 3.1111097e-37 5.7164716e-36 7.4721660e-37], sum to 1.0000
[2019-03-27 12:30:00,699] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9454
[2019-03-27 12:30:00,709] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 98.0, 1.0, 2.0, 0.3380407253222273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527771.1459022759, 527771.1459022759, 169022.2824738064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2917200.0000, 
sim time next is 2917800.0000, 
raw observation next is [21.0, 97.0, 1.0, 2.0, 0.3315803871043239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519262.2577181674, 519262.2577181674, 168389.7613645521], 
processed observation next is [1.0, 0.782608695652174, 0.19431279620853087, 0.97, 1.0, 1.0, 0.19467516518593242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14423951603282428, 0.14423951603282428, 0.25132800203664496], 
reward next is 0.7487, 
noisyNet noise sample is [array([2.521197], dtype=float32), -0.24435505]. 
=============================================
[2019-03-27 12:30:02,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.4465916e-31 1.0000000e+00 0.0000000e+00 1.3764285e-38 2.5965554e-37], sum to 1.0000
[2019-03-27 12:30:02,060] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5651
[2019-03-27 12:30:02,066] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 98.0, 1.0, 2.0, 0.3125757397116858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496358.7754493747, 496358.7754493747, 166791.2369796498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2922000.0000, 
sim time next is 2922600.0000, 
raw observation next is [20.16666666666667, 99.0, 1.0, 2.0, 0.3101219421506055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493050.0560429331, 493050.0560429325, 166556.303467885], 
processed observation next is [1.0, 0.8260869565217391, 0.15481832543443946, 0.99, 1.0, 1.0, 0.1688216170489223, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13695834890081474, 0.13695834890081457, 0.2485914977132612], 
reward next is 0.7514, 
noisyNet noise sample is [array([1.5789231], dtype=float32), -0.04729748]. 
=============================================
[2019-03-27 12:30:08,075] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.8516532e-29 1.0000000e+00 1.0545843e-37 2.3169747e-32 4.1170743e-36], sum to 1.0000
[2019-03-27 12:30:08,089] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0155
[2019-03-27 12:30:08,097] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.5296480638879271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787887.8744350799, 787887.8744350792, 194310.1199839892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3071400.0000, 
sim time next is 3072000.0000, 
raw observation next is [23.0, 96.0, 1.0, 2.0, 0.7583974758994021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1123210.192958733, 1123210.192958733, 242446.031017916], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.96, 1.0, 1.0, 0.7089126215655447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31200283137742585, 0.31200283137742585, 0.3618597477879343], 
reward next is 0.6381, 
noisyNet noise sample is [array([-0.7066771], dtype=float32), -1.0763901]. 
=============================================
[2019-03-27 12:30:08,118] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.089066]
 [73.16606 ]
 [72.379875]
 [72.23602 ]
 [71.94951 ]], R is [[71.4201889 ]
 [71.41596985]
 [71.42913055]
 [71.42137909]
 [71.41693878]].
[2019-03-27 12:30:22,818] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0022346e-29 1.0000000e+00 5.0924657e-37 4.1837778e-35 1.9829287e-36], sum to 1.0000
[2019-03-27 12:30:22,830] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8409
[2019-03-27 12:30:22,845] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5348318217707659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747361.497230726, 747361.497230726, 189234.8607281175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3232800.0000, 
sim time next is 3233400.0000, 
raw observation next is [29.16666666666667, 77.50000000000001, 1.0, 2.0, 0.5387596731589183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752852.1300982146, 752852.1300982139, 189892.6394897953], 
processed observation next is [0.0, 0.43478260869565216, 0.581358609794629, 0.7750000000000001, 1.0, 1.0, 0.4442887628420702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091255916939485, 0.20912559169394832, 0.2834218499847691], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.11886329], dtype=float32), -1.2270526]. 
=============================================
[2019-03-27 12:30:27,291] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4947680e-30 1.0000000e+00 2.1497222e-38 7.0358107e-35 3.2707438e-36], sum to 1.0000
[2019-03-27 12:30:27,298] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0574
[2019-03-27 12:30:27,305] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.5853798233546555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818023.2931631254, 818023.2931631254, 198054.0520738027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3347400.0000, 
sim time next is 3348000.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5853135828045638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 817930.6914463717, 817930.691446371, 198042.0032651483], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.79, 1.0, 1.0, 0.5003778106079082, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22720296984621438, 0.2272029698462142, 0.29558507950022134], 
reward next is 0.7044, 
noisyNet noise sample is [array([-0.51977754], dtype=float32), -0.44843683]. 
=============================================
[2019-03-27 12:30:27,320] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[73.47    ]
 [73.46297 ]
 [73.44196 ]
 [73.430725]
 [73.42485 ]], R is [[73.48020935]
 [73.44980621]
 [73.41963196]
 [73.389534  ]
 [73.35975647]].
[2019-03-27 12:30:29,387] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 12:30:29,391] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:30:29,392] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:30:29,394] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:30:29,395] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:30:29,396] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:30:29,398] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:30:29,401] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:30:29,401] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:30:29,402] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:30:29,404] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:30:29,418] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run30
[2019-03-27 12:30:29,419] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run30
[2019-03-27 12:30:29,435] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run30
[2019-03-27 12:30:29,471] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run30
[2019-03-27 12:30:29,485] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run30
[2019-03-27 12:30:56,867] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0510731], dtype=float32), 0.045298886]
[2019-03-27 12:30:56,868] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.41666666666667, 99.0, 1.0, 2.0, 0.4937005129002389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708056.9426142831, 708056.9426142831, 184933.0346114466]
[2019-03-27 12:30:56,870] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:30:56,873] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0837897e-29 1.0000000e+00 2.4073910e-38 1.0189147e-37 1.0451612e-36], sampled 0.7763307294691398
[2019-03-27 12:31:12,843] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0510731], dtype=float32), 0.045298886]
[2019-03-27 12:31:12,843] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.1, 68.0, 1.0, 2.0, 0.5618904472255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 785186.5636301049, 785186.5636301056, 193860.7653839294]
[2019-03-27 12:31:12,844] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:31:12,846] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.6197354e-29 1.0000000e+00 7.8122899e-38 2.3962260e-35 4.0881529e-36], sampled 0.4250242512002489
[2019-03-27 12:31:17,068] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0510731], dtype=float32), 0.045298886]
[2019-03-27 12:31:17,069] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.25, 72.83333333333333, 1.0, 2.0, 0.4967375519648041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694112.0623406153, 694112.0623406153, 183086.1705186634]
[2019-03-27 12:31:17,070] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:31:17,072] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.611769e-28 1.000000e+00 1.642890e-36 6.200871e-34 7.065720e-35], sampled 0.22741561917337005
[2019-03-27 12:31:19,356] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0510731], dtype=float32), 0.045298886]
[2019-03-27 12:31:19,359] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.26935776833333, 99.97407201833335, 1.0, 2.0, 0.3269025718109669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513495.8828508478, 513495.8828508484, 167978.1096088763]
[2019-03-27 12:31:19,362] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:31:19,369] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.9324352e-30 1.0000000e+00 1.4488929e-38 1.3473880e-38 6.2554820e-37], sampled 0.4309608042248013
[2019-03-27 12:31:21,066] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0510731], dtype=float32), 0.045298886]
[2019-03-27 12:31:21,068] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.26666666666667, 51.0, 1.0, 2.0, 1.020398944060312, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005994453115454, 6.9112, 168.9123159201472, 2323554.390501053, 2256304.197930169, 469646.7647113653]
[2019-03-27 12:31:21,070] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:31:21,074] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2447056e-23 1.0000000e+00 3.8564446e-32 1.3760820e-23 1.8919852e-30], sampled 0.4908144978075826
[2019-03-27 12:31:21,075] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2323554.390501053 W.
[2019-03-27 12:31:23,202] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0510731], dtype=float32), 0.045298886]
[2019-03-27 12:31:23,204] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.00000000000001, 1.0, 2.0, 0.5206777220900716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727576.1151952763, 727576.1151952758, 186899.1046222705]
[2019-03-27 12:31:23,205] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:31:23,213] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0320672e-28 1.0000000e+00 1.2560817e-37 1.8101684e-34 6.4898789e-36], sampled 0.5949551308525397
[2019-03-27 12:31:41,553] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0510731], dtype=float32), 0.045298886]
[2019-03-27 12:31:41,556] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.353910125, 72.468155575, 1.0, 2.0, 0.6054288200484669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846051.3830176631, 846051.3830176631, 201763.7082419811]
[2019-03-27 12:31:41,558] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:31:41,562] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.7637119e-29 1.0000000e+00 7.7075014e-38 2.8118668e-36 3.7283310e-36], sampled 0.038390318105165466
[2019-03-27 12:32:25,946] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0510731], dtype=float32), 0.045298886]
[2019-03-27 12:32:25,947] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.66666666666666, 77.16666666666667, 1.0, 2.0, 0.5720194025673438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799346.10711748, 799346.10711748, 195648.5161048248]
[2019-03-27 12:32:25,948] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:32:25,952] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.4918083e-28 1.0000000e+00 1.0544118e-36 7.5243540e-33 4.9699138e-35], sampled 0.023888782632266126
[2019-03-27 12:32:32,808] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0510731], dtype=float32), 0.045298886]
[2019-03-27 12:32:32,813] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.6, 79.0, 1.0, 2.0, 0.5740366034291992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802166.0298695418, 802166.0298695418, 196008.6931537037]
[2019-03-27 12:32:32,813] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:32:32,815] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3483380e-27 1.0000000e+00 3.8192510e-36 5.0757023e-32 1.7285535e-34], sampled 0.20592088467981617
[2019-03-27 12:32:36,855] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5883 2927327297.4404 1338.0000
[2019-03-27 12:32:37,424] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-03-27 12:32:37,528] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-27 12:32:37,805] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8604 2842435242.9853 1131.0000
[2019-03-27 12:32:37,844] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.5021 3164241877.9541 1777.0000
[2019-03-27 12:32:38,860] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 725000, evaluation results [725000.0, 7883.502076901241, 3164241877.95411, 1777.0, 8253.588267098503, 2927327297.440425, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8496.860364057107, 2842435242.9852624, 1131.0]
[2019-03-27 12:32:40,994] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.9081713e-28 1.0000000e+00 4.1703889e-37 3.3603462e-37 5.7466001e-36], sum to 1.0000
[2019-03-27 12:32:41,002] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2570
[2019-03-27 12:32:41,010] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 69.0, 1.0, 2.0, 0.5652948924024637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789945.7095237701, 789945.7095237701, 194459.1426914459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3324000.0000, 
sim time next is 3324600.0000, 
raw observation next is [31.5, 68.5, 1.0, 2.0, 0.5677779483641817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793416.8394003384, 793416.8394003378, 194897.1476478967], 
processed observation next is [0.0, 0.4782608695652174, 0.6919431279620853, 0.685, 1.0, 1.0, 0.47925054019780927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22039356650009398, 0.22039356650009384, 0.29089126514611446], 
reward next is 0.7091, 
noisyNet noise sample is [array([2.5577488], dtype=float32), -0.8620084]. 
=============================================
[2019-03-27 12:32:49,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5597327e-24 1.0000000e+00 3.5144954e-32 6.2306915e-29 3.6250062e-32], sum to 1.0000
[2019-03-27 12:32:49,258] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8853
[2019-03-27 12:32:49,264] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5394196110894326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753774.6415356724, 753774.641535673, 190004.9813102005], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3434400.0000, 
sim time next is 3435000.0000, 
raw observation next is [30.66666666666667, 70.66666666666667, 1.0, 2.0, 0.5429697960851061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758737.373915605, 758737.373915605, 190603.7892000535], 
processed observation next is [1.0, 0.782608695652174, 0.6524486571879939, 0.7066666666666667, 1.0, 1.0, 0.44936120010253744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2107603816432236, 0.2107603816432236, 0.2844832674627664], 
reward next is 0.7155, 
noisyNet noise sample is [array([0.27436844], dtype=float32), 0.005563946]. 
=============================================
[2019-03-27 12:32:49,287] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[62.36283 ]
 [61.455086]
 [60.033302]
 [58.391113]
 [54.980553]], R is [[63.1334343 ]
 [63.21851349]
 [63.30404282]
 [63.38985443]
 [63.47735596]].
[2019-03-27 12:32:56,020] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7564852e-25 1.0000000e+00 1.3232076e-32 2.1424818e-26 9.7441911e-32], sum to 1.0000
[2019-03-27 12:32:56,035] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3132
[2019-03-27 12:32:56,042] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5019401521394582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701384.2721856466, 701384.2721856473, 183900.7382228879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3631200.0000, 
sim time next is 3631800.0000, 
raw observation next is [27.16666666666666, 82.33333333333333, 1.0, 2.0, 0.5040955941213173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704397.1699210979, 704397.1699210979, 184240.4475573107], 
processed observation next is [1.0, 0.0, 0.4865718799368086, 0.8233333333333333, 1.0, 1.0, 0.4025248121943582, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19566588053363831, 0.19566588053363831, 0.2749857426228518], 
reward next is 0.7250, 
noisyNet noise sample is [array([-0.993709], dtype=float32), -0.15204674]. 
=============================================
[2019-03-27 12:33:09,551] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4793512e-26 1.0000000e+00 2.8369924e-36 5.4803549e-34 1.5098352e-34], sum to 1.0000
[2019-03-27 12:33:09,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7722
[2019-03-27 12:33:09,572] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.6258347803429243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 894527.8765292667, 894527.8765292667, 208245.7381931925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3726000.0000, 
sim time next is 3726600.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.7964846343115369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1138471.644683996, 1138471.644683996, 246585.3304880439], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.74, 1.0, 1.0, 0.7548007642307674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3162421235233322, 0.3162421235233322, 0.368037806698573], 
reward next is 0.6320, 
noisyNet noise sample is [array([1.5312927], dtype=float32), 0.8665458]. 
=============================================
[2019-03-27 12:33:12,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8103215e-30 1.0000000e+00 0.0000000e+00 7.2647397e-37 1.6807011e-37], sum to 1.0000
[2019-03-27 12:33:12,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2955
[2019-03-27 12:33:12,132] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.16666666666667, 62.33333333333334, 1.0, 2.0, 0.6188923233385047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864873.5020697841, 864873.5020697841, 204319.0439386505], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3846000.0000, 
sim time next is 3846600.0000, 
raw observation next is [34.25, 62.0, 1.0, 2.0, 0.6165147167353393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 861549.5588072789, 861549.5588072783, 203863.8159743474], 
processed observation next is [0.0, 0.5217391304347826, 0.8222748815165877, 0.62, 1.0, 1.0, 0.5379695382353485, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2393193218909108, 0.23931932189091062, 0.3042743522005185], 
reward next is 0.6957, 
noisyNet noise sample is [array([-2.0591903], dtype=float32), 0.3739605]. 
=============================================
[2019-03-27 12:33:14,429] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3523414e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 12:33:14,438] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0355
[2019-03-27 12:33:14,447] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.5, 1.0, 2.0, 0.554695264193918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775128.335297235, 775128.335297235, 192609.4052649781], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3828600.0000, 
sim time next is 3829200.0000, 
raw observation next is [29.33333333333334, 78.0, 1.0, 2.0, 0.5578162038926401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779491.1207615853, 779491.1207615853, 193150.4323246857], 
processed observation next is [0.0, 0.30434782608695654, 0.5892575039494474, 0.78, 1.0, 1.0, 0.4672484384248675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2165253113226626, 0.2165253113226626, 0.2882842273502772], 
reward next is 0.7117, 
noisyNet noise sample is [array([-0.89295644], dtype=float32), -1.1001139]. 
=============================================
[2019-03-27 12:33:16,290] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8872547e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8397652e-38], sum to 1.0000
[2019-03-27 12:33:16,300] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0290
[2019-03-27 12:33:16,309] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.41666666666666, 61.33333333333334, 1.0, 2.0, 0.616739349210046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861863.599213231, 861863.599213231, 203906.8101970024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3847800.0000, 
sim time next is 3848400.0000, 
raw observation next is [34.5, 61.0, 1.0, 2.0, 0.616858181621116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862029.7291866334, 862029.7291866334, 203929.5574139173], 
processed observation next is [0.0, 0.5652173913043478, 0.8341232227488152, 0.61, 1.0, 1.0, 0.5383833513507422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2394527025518426, 0.2394527025518426, 0.30437247375211535], 
reward next is 0.6956, 
noisyNet noise sample is [array([-0.2575425], dtype=float32), -0.121015474]. 
=============================================
[2019-03-27 12:33:19,506] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4240872e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3427596e-38], sum to 1.0000
[2019-03-27 12:33:19,521] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3105
[2019-03-27 12:33:19,528] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 56.0, 1.0, 2.0, 0.5386672034981668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752722.8690367057, 752722.8690367057, 189876.3394393971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3866400.0000, 
sim time next is 3867000.0000, 
raw observation next is [32.83333333333334, 57.83333333333334, 1.0, 2.0, 0.5499505337870324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768495.674717689, 768495.674717689, 191792.1702312551], 
processed observation next is [0.0, 0.782608695652174, 0.7551342812006324, 0.5783333333333335, 1.0, 1.0, 0.4577717274542559, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21347102075491362, 0.21347102075491362, 0.28625697049441057], 
reward next is 0.7137, 
noisyNet noise sample is [array([0.35057136], dtype=float32), 0.073182516]. 
=============================================
[2019-03-27 12:33:19,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.9699  ]
 [73.862564]
 [73.79463 ]
 [73.68514 ]
 [73.55584 ]], R is [[73.90877533]
 [73.8862915 ]
 [73.8624649 ]
 [73.83733368]
 [73.81096649]].
[2019-03-27 12:33:20,036] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3225687e-29 1.0000000e+00 1.8578055e-38 2.5691758e-36 7.8400887e-37], sum to 1.0000
[2019-03-27 12:33:20,044] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4050
[2019-03-27 12:33:20,051] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.638353093640639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 892080.4529525923, 892080.4529525923, 208107.0619841013], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3960600.0000, 
sim time next is 3961200.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.6120671115098942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855331.7425388523, 855331.7425388523, 203015.3604700686], 
processed observation next is [0.0, 0.8695652173913043, 0.6998420221169038, 0.7233333333333334, 1.0, 1.0, 0.5326109777227641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23759215070523673, 0.23759215070523673, 0.30300800070159495], 
reward next is 0.6970, 
noisyNet noise sample is [array([-0.3390705], dtype=float32), 0.6935591]. 
=============================================
[2019-03-27 12:33:36,053] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6632174e-26 1.0000000e+00 5.0779822e-34 1.8777424e-29 1.1588338e-32], sum to 1.0000
[2019-03-27 12:33:36,065] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4932
[2019-03-27 12:33:36,073] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.5, 69.0, 1.0, 2.0, 0.6105959413412285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 853275.0334308215, 853275.0334308215, 202739.5030187077], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4123800.0000, 
sim time next is 4124400.0000, 
raw observation next is [33.33333333333334, 69.66666666666666, 1.0, 2.0, 0.6139048750789351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857900.9581078772, 857900.9581078772, 203367.6507936875], 
processed observation next is [1.0, 0.7391304347826086, 0.7788309636650873, 0.6966666666666665, 1.0, 1.0, 0.5348251506975121, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23830582169663256, 0.23830582169663256, 0.3035338071547575], 
reward next is 0.6965, 
noisyNet noise sample is [array([-0.47121045], dtype=float32), -0.29275477]. 
=============================================
[2019-03-27 12:33:38,895] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8707705e-30 1.0000000e+00 0.0000000e+00 1.4763719e-37 2.0574478e-37], sum to 1.0000
[2019-03-27 12:33:38,906] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2067
[2019-03-27 12:33:38,912] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.6004596397817472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839104.5010695005, 839104.5010695005, 200831.6371857367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4231200.0000, 
sim time next is 4231800.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.6014617631809732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840505.459457621, 840505.459457621, 201018.551850352], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5198334496156304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23347373873822805, 0.23347373873822805, 0.3000276893288836], 
reward next is 0.7000, 
noisyNet noise sample is [array([0.4288506], dtype=float32), 0.5304524]. 
=============================================
[2019-03-27 12:33:40,719] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6487467e-23 1.0000000e+00 4.8853325e-33 7.0934967e-31 3.3944983e-31], sum to 1.0000
[2019-03-27 12:33:40,728] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1169
[2019-03-27 12:33:40,733] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 79.0, 1.0, 2.0, 0.9834529039382823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1374659.88324726, 1374659.883247259, 293927.35968535], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4257000.0000, 
sim time next is 4257600.0000, 
raw observation next is [29.66666666666667, 79.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.102012236591952, 6.9112, 168.9119325230154, 1589215.580946072, 1453847.633692573, 311353.500307062], 
processed observation next is [1.0, 0.2608695652173913, 0.6050552922590839, 0.79, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.019081223659195158, 0.0, 0.8294349169053086, 0.44144877248502, 0.40384656491460363, 0.46470671687621196], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6682703], dtype=float32), 0.3276191]. 
=============================================
[2019-03-27 12:33:40,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.7147632e-09 4.9188018e-01 1.6462738e-15 5.0811982e-01 4.1763349e-15], sum to 1.0000
[2019-03-27 12:33:40,753] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6132
[2019-03-27 12:33:40,758] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 61.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.295838996821846, 6.9112, 170.5573041426782, 3185183.240531588, 2909650.680994059, 551604.4989889193], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4380000.0000, 
sim time next is 4380600.0000, 
raw observation next is [33.5, 62.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.415350442778326, 6.9112, 170.5573041426782, 3270893.875509613, 2909750.405818656, 550925.8638281393], 
processed observation next is [1.0, 0.6956521739130435, 0.7867298578199052, 0.62, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.05041504427783261, 0.0, 0.8375144448122397, 0.9085816320860036, 0.8082640016162933, 0.8222774086987154], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.59232986], dtype=float32), -0.3127616]. 
=============================================
[2019-03-27 12:33:42,858] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2244557e-24 1.0000000e+00 3.1987747e-33 4.5457108e-28 1.0377466e-31], sum to 1.0000
[2019-03-27 12:33:42,868] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2121
[2019-03-27 12:33:42,876] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.6002922603166063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 838870.5064482886, 838870.5064482879, 200800.4467910177], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4233600.0000, 
sim time next is 4234200.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.601006346581114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839868.7914228167, 839868.7914228167, 200933.5713125359], 
processed observation next is [1.0, 0.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5192847549170048, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23329688650633798, 0.23329688650633798, 0.29990085270527744], 
reward next is 0.7001, 
noisyNet noise sample is [array([-0.8093946], dtype=float32), 1.0634091]. 
=============================================
[2019-03-27 12:33:44,992] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 12:33:44,994] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:33:44,995] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:33:44,997] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:33:44,998] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:33:44,998] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:33:45,000] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:33:45,000] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:33:44,999] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:33:44,999] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:33:45,005] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:33:45,029] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run31
[2019-03-27 12:33:45,030] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run31
[2019-03-27 12:33:45,047] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run31
[2019-03-27 12:33:45,087] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run31
[2019-03-27 12:33:45,107] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run31
[2019-03-27 12:33:48,025] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05241408], dtype=float32), 0.044264093]
[2019-03-27 12:33:48,025] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.45081278, 92.52179156, 1.0, 2.0, 0.3567811535800176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 555196.1106394809, 555196.1106394815, 171219.1761485162]
[2019-03-27 12:33:48,025] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:33:48,028] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7299242e-28 1.0000000e+00 4.0749010e-37 3.1674921e-35 2.8430281e-35], sampled 0.5183921507032885
[2019-03-27 12:34:02,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05241408], dtype=float32), 0.044264093]
[2019-03-27 12:34:02,318] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.8, 96.0, 1.0, 2.0, 0.3549255071666878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546063.4439535406, 546063.4439535406, 170298.3032891489]
[2019-03-27 12:34:02,321] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:34:02,325] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5429478e-28 1.0000000e+00 5.9663365e-37 5.7589144e-35 3.9920973e-35], sampled 0.46960773668925704
[2019-03-27 12:34:30,043] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05241408], dtype=float32), 0.044264093]
[2019-03-27 12:34:30,046] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.36666666666667, 67.66666666666667, 1.0, 2.0, 0.5708174584696438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 797665.866744857, 797665.8667448576, 195434.8479281227]
[2019-03-27 12:34:30,047] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:34:30,049] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5121520e-28 1.0000000e+00 5.4615422e-37 1.4697266e-34 3.9036988e-35], sampled 0.2494651381170251
[2019-03-27 12:34:38,885] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05241408], dtype=float32), 0.044264093]
[2019-03-27 12:34:38,886] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.93333333333334, 70.33333333333333, 1.0, 2.0, 0.5467541223346456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764027.4331875199, 764027.4331875199, 191244.7697638985]
[2019-03-27 12:34:38,889] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:34:38,893] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.0299633e-29 1.0000000e+00 2.4949310e-37 1.7032338e-36 1.9049654e-35], sampled 0.6162895330652104
[2019-03-27 12:35:01,127] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05241408], dtype=float32), 0.044264093]
[2019-03-27 12:35:01,127] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.31697883333333, 66.06706, 1.0, 2.0, 0.8722421121174591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1219121.383912581, 1219121.383912581, 262420.1318406202]
[2019-03-27 12:35:01,128] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:35:01,131] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3719541e-27 1.0000000e+00 4.4895465e-36 2.0966374e-35 2.3009946e-34], sampled 0.8727127527110133
[2019-03-27 12:35:28,112] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05241408], dtype=float32), 0.044264093]
[2019-03-27 12:35:28,114] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.6, 70.66666666666667, 1.0, 2.0, 0.5337046798877085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 745785.9025675897, 745785.9025675902, 189044.8030642572]
[2019-03-27 12:35:28,115] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:35:28,118] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3345206e-27 1.0000000e+00 4.9142028e-36 9.1349444e-33 3.1096551e-34], sampled 0.5838035091960561
[2019-03-27 12:35:34,118] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05241408], dtype=float32), 0.044264093]
[2019-03-27 12:35:34,118] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.7, 87.0, 1.0, 2.0, 1.032136021574284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1442754.863287279, 1442754.86328728, 308868.8246922487]
[2019-03-27 12:35:34,120] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:35:34,122] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.9554951e-25 1.0000000e+00 4.9290983e-33 3.1236906e-30 1.9846969e-31], sampled 0.05491550759158259
[2019-03-27 12:35:51,515] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.8717 2927447046.3306 1338.0000
[2019-03-27 12:35:52,705] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.7963 2779253700.5161 932.0000
[2019-03-27 12:35:52,919] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-03-27 12:35:52,939] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.1437 2842465573.2099 1131.0000
[2019-03-27 12:35:53,119] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7892.2028 3163389730.8703 1766.0000
[2019-03-27 12:35:54,138] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 750000, evaluation results [750000.0, 7892.2028395349125, 3163389730.8703213, 1766.0, 8254.871735174753, 2927447046.3306217, 1338.0, 8659.796268611322, 2779253700.5160947, 932.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8497.14369207633, 2842465573.209898, 1131.0]
[2019-03-27 12:36:01,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3522065e-17 1.0000000e+00 3.0128338e-25 3.0865401e-18 1.8629000e-24], sum to 1.0000
[2019-03-27 12:36:01,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5272
[2019-03-27 12:36:01,525] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2703669.180797368 W.
[2019-03-27 12:36:01,530] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.16666666666667, 57.5, 1.0, 2.0, 0.6475690501984809, 1.0, 2.0, 0.644374564613503, 1.0, 1.0, 1.03, 7.00509359838859, 6.9112, 170.5573041426782, 2703669.180797368, 2636409.376924233, 505280.7141107467], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4367400.0000, 
sim time next is 4368000.0000, 
raw observation next is [35.33333333333334, 61.00000000000001, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.588928509116456, 6.9112, 170.5573041426782, 3395379.757357657, 2909895.258044393, 549912.9545058233], 
processed observation next is [1.0, 0.5652173913043478, 0.8736176935229073, 0.6100000000000001, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.06777285091164557, 0.0, 0.8375144448122397, 0.9431610437104603, 0.8083042383456647, 0.8207656037400348], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.25210896], dtype=float32), 0.3742867]. 
=============================================
[2019-03-27 12:36:01,554] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[47.6425  ]
 [47.672295]
 [47.06212 ]
 [46.866413]
 [45.391285]], R is [[44.90447235]
 [44.45542908]
 [44.0108757 ]
 [43.57076645]
 [43.40541458]].
[2019-03-27 12:36:08,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.377084e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 12:36:08,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5213
[2019-03-27 12:36:08,820] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 86.0, 1.0, 2.0, 0.462441118112689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656257.5409933337, 656257.5409933343, 179218.7617788287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4653600.0000, 
sim time next is 4654200.0000, 
raw observation next is [24.66666666666666, 90.0, 1.0, 2.0, 0.4572043061861639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651777.7078058596, 651777.7078058602, 178824.3913709563], 
processed observation next is [1.0, 0.8695652173913043, 0.36808846761453373, 0.9, 1.0, 1.0, 0.3460292845616432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18104936327940546, 0.1810493632794056, 0.2669020766730691], 
reward next is 0.7331, 
noisyNet noise sample is [array([-2.1572099], dtype=float32), -1.348792]. 
=============================================
[2019-03-27 12:36:11,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8946844e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8066556e-38], sum to 1.0000
[2019-03-27 12:36:11,863] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1243
[2019-03-27 12:36:11,873] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5527835446855871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772455.9368315182, 772455.9368315176, 192278.7896705648], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4482600.0000, 
sim time next is 4483200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5501475598523213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 191825.7660821197], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4580091082558088, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2135475268641311, 0.21354752686413092, 0.2863071135554025], 
reward next is 0.7137, 
noisyNet noise sample is [array([-0.9132465], dtype=float32), -0.94072396]. 
=============================================
[2019-03-27 12:36:15,728] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7337623e-09 9.9995947e-01 3.6100819e-16 4.0533763e-05 8.0742600e-16], sum to 1.0000
[2019-03-27 12:36:15,735] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4087
[2019-03-27 12:36:15,750] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3247188.922389647 W.
[2019-03-27 12:36:15,757] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.38229721498926, 6.9112, 170.5573041426782, 3247188.922389647, 2909722.824283341, 551121.8857841747], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [35.0, 60.0, 1.0, 2.0, 0.9464728457256161, 1.0, 2.0, 0.7938264623770704, 1.0, 1.0, 1.03, 7.005117174363757, 6.9112, 170.5573041426782, 3331576.329758003, 3264299.637454846, 610491.1062907357], 
processed observation next is [1.0, 0.6086956521739131, 0.8578199052132701, 0.6, 1.0, 1.0, 0.9355094526814651, 1.0, 1.0, 0.7515981474422535, 1.0, 0.5, 1.0365853658536586, 0.009391717436375657, 0.0, 0.8375144448122397, 0.925437869377223, 0.9067498992930128, 0.9111807556578144], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5649188], dtype=float32), 1.1477288]. 
=============================================
[2019-03-27 12:36:18,117] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6926722e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1960625e-38], sum to 1.0000
[2019-03-27 12:36:18,130] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7410
[2019-03-27 12:36:18,140] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5221957909900145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729698.1380788796, 729698.1380788796, 187146.5020171929], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4567200.0000, 
sim time next is 4567800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5221095641334648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729577.6063064608, 729577.6063064614, 187132.431466196], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4242283905222467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2026604461962391, 0.20266044619623927, 0.27930213651671043], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.39640585], dtype=float32), -0.47604588]. 
=============================================
[2019-03-27 12:36:18,668] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1642690e-19 1.0000000e+00 1.7580256e-27 1.2687186e-21 2.3121358e-25], sum to 1.0000
[2019-03-27 12:36:18,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6402
[2019-03-27 12:36:18,682] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.930702569219262, 6.9112, 168.9125624514191, 1467600.16525355, 1453764.403700461, 311356.4047366949], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4608000.0000, 
sim time next is 4608600.0000, 
raw observation next is [30.33333333333333, 82.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.889763791167544, 6.9112, 168.9074864794835, 2148434.842363137, 1454230.490994172, 311356.9083444797], 
processed observation next is [1.0, 0.34782608695652173, 0.6366508688783569, 0.825, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.09785637911675442, 0.0, 0.8294130847961595, 0.5967874562119825, 0.40395291416504775, 0.4647118034992234], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.64747345], dtype=float32), 0.82501656]. 
=============================================
[2019-03-27 12:36:19,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4689225e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3737076e-36], sum to 1.0000
[2019-03-27 12:36:19,426] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4974
[2019-03-27 12:36:19,433] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5222476749156969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729770.6637668267, 729770.6637668267, 187154.9695321051], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4566600.0000, 
sim time next is 4567200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5221957909900145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729698.1380788796, 729698.1380788796, 187146.5020171929], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4243322783012222, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2026939272441332, 0.2026939272441332, 0.27932313733909386], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.74914926], dtype=float32), -0.23665261]. 
=============================================
[2019-03-27 12:36:20,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5995808e-08 5.5355495e-03 3.8846320e-15 9.9446434e-01 3.0549172e-14], sum to 1.0000
[2019-03-27 12:36:20,942] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6702
[2019-03-27 12:36:20,948] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.9670286867936845, 1.0, 2.0, 0.9670286867936845, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2704976.440132983, 2704976.440132983, 509390.5516428538], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4614600.0000, 
sim time next is 4615200.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.9427151703030063, 1.0, 2.0, 0.9427151703030063, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2636894.804793974, 2636894.804793974, 495342.9632039536], 
processed observation next is [1.0, 0.43478260869565216, 0.7156398104265403, 0.75, 1.0, 1.0, 0.9309821328951884, 1.0, 1.0, 0.9309821328951884, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7324707791094373, 0.7324707791094373, 0.7393178555282889], 
reward next is 0.2607, 
noisyNet noise sample is [array([2.0133843], dtype=float32), 0.11982134]. 
=============================================
[2019-03-27 12:36:22,852] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.8858938e-10 9.9999154e-01 6.6760298e-17 8.5195497e-06 7.3110722e-16], sum to 1.0000
[2019-03-27 12:36:22,861] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2958
[2019-03-27 12:36:22,874] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2241908.762191521 W.
[2019-03-27 12:36:22,882] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5344202907843443, 1.0, 2.0, 0.5344202907843443, 1.0, 2.0, 0.928111309601567, 6.911200000000001, 6.9112, 170.5573041426782, 2241908.762191521, 2241908.76219152, 439764.4618893093], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4719600.0000, 
sim time next is 4720200.0000, 
raw observation next is [30.33333333333333, 73.66666666666667, 1.0, 2.0, 0.9097653316212294, 1.0, 2.0, 0.9097653316212294, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2544636.041606901, 2544636.041606901, 476846.9162965777], 
processed observation next is [1.0, 0.6521739130434783, 0.6366508688783569, 0.7366666666666667, 1.0, 1.0, 0.8912835320737704, 1.0, 1.0, 0.8912835320737704, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7068433448908059, 0.7068433448908059, 0.7117118153680264], 
reward next is 0.2883, 
noisyNet noise sample is [array([1.5481752], dtype=float32), 1.2800912]. 
=============================================
[2019-03-27 12:36:28,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0902186e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1478596e-37], sum to 1.0000
[2019-03-27 12:36:28,380] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7066
[2019-03-27 12:36:28,388] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5163970000001702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721592.3580844634, 721592.3580844641, 186205.1566826971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4740600.0000, 
sim time next is 4741200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5156445368819844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720540.5398331, 720540.5398331007, 186083.7356781453], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4164392010626318, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2001501499536389, 0.20015014995363908, 0.27773691892260494], 
reward next is 0.7223, 
noisyNet noise sample is [array([-0.5606211], dtype=float32), 0.89417344]. 
=============================================
[2019-03-27 12:36:33,829] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3078387e-29 1.0000000e+00 6.0681313e-38 4.2482042e-35 1.7925830e-35], sum to 1.0000
[2019-03-27 12:36:33,842] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6284
[2019-03-27 12:36:33,848] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 82.33333333333334, 1.0, 2.0, 0.5009439659678925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699991.7963676837, 699991.7963676844, 183744.1116567507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5002800.0000, 
sim time next is 5003400.0000, 
raw observation next is [27.0, 83.16666666666666, 1.0, 2.0, 0.504204808114684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704549.8305291543, 704549.8305291549, 184257.5937168088], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.8316666666666666, 1.0, 1.0, 0.4026563953188964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1957082862580984, 0.19570828625809858, 0.2750113339056848], 
reward next is 0.7250, 
noisyNet noise sample is [array([0.11640669], dtype=float32), 1.0824504]. 
=============================================
[2019-03-27 12:36:39,793] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4401836e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7375261e-38], sum to 1.0000
[2019-03-27 12:36:39,805] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1948
[2019-03-27 12:36:39,813] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 76.5, 1.0, 2.0, 0.4890598876174903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 683380.2914506076, 683380.2914506069, 181899.5067941759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4912200.0000, 
sim time next is 4912800.0000, 
raw observation next is [27.33333333333334, 77.33333333333333, 1.0, 2.0, 0.4870278561144585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680539.9540463309, 680539.9540463302, 181588.3701370411], 
processed observation next is [1.0, 0.8695652173913043, 0.4944707740916275, 0.7733333333333333, 1.0, 1.0, 0.38196127242705846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1890388761239808, 0.1890388761239806, 0.27102741811498676], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.4870518], dtype=float32), 0.34544677]. 
=============================================
[2019-03-27 12:36:54,271] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3307334e-14 1.0000000e+00 1.6102228e-21 5.9153984e-12 4.8687508e-20], sum to 1.0000
[2019-03-27 12:36:54,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0440
[2019-03-27 12:36:54,285] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 77.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 11.84201667080206, 6.9112, 168.8667834107577, 4953018.195545754, 1455883.362242519, 298770.8172982074], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5300400.0000, 
sim time next is 5301000.0000, 
raw observation next is [31.5, 75.5, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 1.0, 1.0, 1.03, 7.579155665916922, 6.9112, 170.5573041426782, 4219336.573344289, 3740852.759148695, 703370.6322018681], 
processed observation next is [1.0, 0.34782608695652173, 0.6919431279620853, 0.755, 1.0, 1.0, 1.0481927710843375, 1.0, 0.5, 1.0481927710843375, 1.0, 0.5, 1.0365853658536586, 0.06679556659169217, 0.0, 0.8375144448122397, 1.1720379370400804, 1.0391257664301932, 1.0498069137341315], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11521753], dtype=float32), -2.0566783]. 
=============================================
[2019-03-27 12:36:54,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[41.592117]
 [50.60149 ]
 [50.215298]
 [49.38381 ]
 [50.515533]], R is [[34.62322998]
 [34.27699661]
 [33.93422699]
 [33.59488678]
 [33.77597809]].
[2019-03-27 12:37:00,055] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 12:37:00,056] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:37:00,056] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:37:00,057] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:37:00,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:37:00,059] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:37:00,058] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:37:00,060] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:37:00,061] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:37:00,064] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:37:00,064] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:37:00,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run32
[2019-03-27 12:37:00,085] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run32
[2019-03-27 12:37:00,123] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run32
[2019-03-27 12:37:00,124] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run32
[2019-03-27 12:37:00,164] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run32
[2019-03-27 12:37:09,042] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05290563], dtype=float32), 0.0440239]
[2019-03-27 12:37:09,043] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.469092255, 76.95519894, 1.0, 2.0, 0.2288261081758885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381231.2666951509, 381231.2666951509, 158377.5743974465]
[2019-03-27 12:37:09,044] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:37:09,049] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.5826821e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8265207e-38], sampled 0.15905997660084803
[2019-03-27 12:37:49,547] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05290563], dtype=float32), 0.0440239]
[2019-03-27 12:37:49,549] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.1, 89.16666666666667, 1.0, 2.0, 0.7279022803350071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1017282.852814553, 1017282.852814553, 226981.1011079634]
[2019-03-27 12:37:49,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:37:49,556] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.4108123e-29 1.0000000e+00 1.8856594e-37 4.7158525e-38 1.8079384e-35], sampled 0.9414410641275249
[2019-03-27 12:37:54,556] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05290563], dtype=float32), 0.0440239]
[2019-03-27 12:37:54,556] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.2, 57.0, 1.0, 2.0, 0.5305370913554744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741358.046981818, 741358.0469818175, 188518.318496429]
[2019-03-27 12:37:54,558] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:37:54,562] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.4249849e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1325699e-38], sampled 0.8655615498420959
[2019-03-27 12:37:58,342] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05290563], dtype=float32), 0.0440239]
[2019-03-27 12:37:58,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 68.66666666666667, 1.0, 2.0, 0.9901800981579266, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.998396282053287, 6.9112, 168.9123691000754, 2281257.368415002, 2219397.539985978, 460587.9853934082]
[2019-03-27 12:37:58,344] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:37:58,348] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.2104981e-18 1.0000000e+00 7.0301287e-26 4.7518753e-17 2.9999562e-24], sampled 0.26924006779046084
[2019-03-27 12:37:58,350] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2281257.368415002 W.
[2019-03-27 12:38:26,730] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05290563], dtype=float32), 0.0440239]
[2019-03-27 12:38:26,732] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.33333333333334, 78.33333333333334, 1.0, 2.0, 0.6295074414243633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879713.7838530749, 879713.7838530749, 206372.6021989454]
[2019-03-27 12:38:26,733] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:38:26,737] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4376471e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9190489118763698
[2019-03-27 12:38:53,320] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05290563], dtype=float32), 0.0440239]
[2019-03-27 12:38:53,321] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.45, 90.83333333333334, 1.0, 2.0, 0.5354245747002312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748190.088043633, 748190.0880436323, 189331.8068944654]
[2019-03-27 12:38:53,322] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:38:53,325] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.8517934e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6810474e-37], sampled 0.2873563586197059
[2019-03-27 12:39:07,538] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 12:39:08,153] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 12:39:08,299] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6703 2927337065.9000 1338.0000
[2019-03-27 12:39:08,381] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6673 3164067602.5339 1778.0000
[2019-03-27 12:39:08,443] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 12:39:09,459] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 775000, evaluation results [775000.0, 7882.66734019175, 3164067602.533886, 1778.0, 8253.670278353698, 2927337065.899996, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 12:39:17,417] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4560740e-08 9.5690334e-01 2.5528571e-15 4.3096513e-02 1.2951872e-14], sum to 1.0000
[2019-03-27 12:39:17,427] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3520
[2019-03-27 12:39:17,440] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3021420.572278593 W.
[2019-03-27 12:39:17,445] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.36666666666667, 51.33333333333334, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.067494923117207, 6.9112, 170.5573041426782, 3021420.572278593, 2909460.16117875, 552836.0740624482], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5311200.0000, 
sim time next is 5311800.0000, 
raw observation next is [36.34999999999999, 51.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.214748318954615, 6.9112, 170.5573041426782, 3127027.028217077, 2909583.019793726, 552051.7899311419], 
processed observation next is [1.0, 0.4782608695652174, 0.9218009478672979, 0.515, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.03035483189546149, 0.0, 0.8375144448122397, 0.868618618949188, 0.8082175054982572, 0.8239578954196147], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3534785], dtype=float32), -0.2169628]. 
=============================================
[2019-03-27 12:39:17,970] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4621440e-28 1.0000000e+00 5.2917317e-37 3.0883829e-37 7.4070706e-35], sum to 1.0000
[2019-03-27 12:39:17,979] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3566
[2019-03-27 12:39:17,985] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 86.33333333333334, 1.0, 2.0, 0.5755005646675427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804212.5623390183, 804212.5623390183, 196270.7866972732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5275200.0000, 
sim time next is 5275800.0000, 
raw observation next is [28.6, 86.5, 1.0, 2.0, 0.576573844478018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 805712.9474522794, 805712.9474522788, 196463.1186468823], 
processed observation next is [1.0, 0.043478260869565216, 0.5545023696682465, 0.865, 1.0, 1.0, 0.48984800539520235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22380915207007762, 0.22380915207007746, 0.2932285352938542], 
reward next is 0.7068, 
noisyNet noise sample is [array([1.105345], dtype=float32), -1.2934039]. 
=============================================
[2019-03-27 12:39:25,054] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0570527e-29 1.0000000e+00 1.2378917e-37 0.0000000e+00 3.9747572e-36], sum to 1.0000
[2019-03-27 12:39:25,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4234
[2019-03-27 12:39:25,073] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.9, 74.33333333333333, 1.0, 2.0, 0.560677956927181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783491.6029557209, 783491.6029557209, 193651.3025110843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5421000.0000, 
sim time next is 5421600.0000, 
raw observation next is [30.9, 75.0, 1.0, 2.0, 0.5725982609448191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800155.3147649188, 800155.3147649188, 195753.8019869578], 
processed observation next is [1.0, 0.782608695652174, 0.6635071090047393, 0.75, 1.0, 1.0, 0.4850581457166495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22226536521247744, 0.22226536521247744, 0.2921698537118773], 
reward next is 0.7078, 
noisyNet noise sample is [array([-0.67785585], dtype=float32), 0.7730431]. 
=============================================
[2019-03-27 12:39:28,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8427802e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 12:39:28,132] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2304
[2019-03-27 12:39:28,137] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.6, 74.66666666666667, 1.0, 2.0, 0.5730722300147552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800817.8942086841, 800817.8942086835, 195837.3381750356], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5514000.0000, 
sim time next is 5514600.0000, 
raw observation next is [30.35, 76.33333333333333, 1.0, 2.0, 0.576406859247662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805479.5113056763, 805479.5113056763, 196433.5117151019], 
processed observation next is [1.0, 0.8260869565217391, 0.637440758293839, 0.7633333333333333, 1.0, 1.0, 0.4896468183706771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2237443086960212, 0.2237443086960212, 0.29318434584343567], 
reward next is 0.7068, 
noisyNet noise sample is [array([-0.75505066], dtype=float32), -1.309241]. 
=============================================
[2019-03-27 12:39:31,032] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.86696053e-09 9.99888539e-01 5.91405074e-16 1.11407724e-04
 5.71343205e-15], sum to 1.0000
[2019-03-27 12:39:31,048] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9011
[2019-03-27 12:39:31,058] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2824011.548855186 W.
[2019-03-27 12:39:31,061] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.7, 51.0, 1.0, 2.0, 0.7048674891358243, 1.0, 2.0, 0.6730237840821748, 1.0, 2.0, 1.03, 7.005098115956102, 6.9112, 170.5573041426782, 2824011.548855186, 2756748.508864679, 522869.8856058843], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5504400.0000, 
sim time next is 5505000.0000, 
raw observation next is [34.45, 52.33333333333333, 1.0, 2.0, 0.2797999865742501, 1.0, 2.0, 0.2797999865742501, 1.0, 2.0, 0.4859200454099517, 6.911199999999999, 6.9112, 170.5573041426782, 1173186.728636569, 1173186.72863657, 296337.37531734], 
processed observation next is [1.0, 0.7391304347826086, 0.8317535545023698, 0.5233333333333333, 1.0, 1.0, 0.13228914045090373, 1.0, 1.0, 0.13228914045090373, 1.0, 1.0, 0.3730732261096972, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.325885202399047, 0.3258852023990472, 0.4422945900258806], 
reward next is 0.5577, 
noisyNet noise sample is [array([0.7674521], dtype=float32), 1.0543926]. 
=============================================
[2019-03-27 12:39:31,070] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[29.552456]
 [29.16814 ]
 [28.545391]
 [27.2545  ]
 [26.74598 ]], R is [[38.20075607]
 [37.81874847]
 [37.4405632 ]
 [37.06615829]
 [36.69549561]].
[2019-03-27 12:39:41,860] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.515377e-33 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 12:39:41,868] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7548
[2019-03-27 12:39:41,876] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 91.0, 1.0, 2.0, 0.5255056797706216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734324.8595977512, 734324.8595977512, 187688.4802586822], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5616000.0000, 
sim time next is 5616600.0000, 
raw observation next is [26.25, 91.16666666666667, 1.0, 2.0, 0.5242929619366326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732629.6620206813, 732629.6620206819, 187489.5288003953], 
processed observation next is [0.0, 0.0, 0.4431279620853081, 0.9116666666666667, 1.0, 1.0, 0.42685899028509944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20350823945018925, 0.20350823945018942, 0.27983511761253027], 
reward next is 0.7202, 
noisyNet noise sample is [array([0.00984397], dtype=float32), -0.5918945]. 
=============================================
[2019-03-27 12:39:47,259] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.9412742e-29 1.0000000e+00 2.8163418e-37 1.8495059e-36 6.0264358e-34], sum to 1.0000
[2019-03-27 12:39:47,272] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7725
[2019-03-27 12:39:47,281] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 88.66666666666666, 1.0, 2.0, 0.5385480237658156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752556.2706470994, 752556.2706471, 189855.7897335218], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5791200.0000, 
sim time next is 5791800.0000, 
raw observation next is [26.93333333333333, 88.83333333333334, 1.0, 2.0, 0.5379146463188768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 751670.888473976, 751670.8884739766, 189749.3929585985], 
processed observation next is [1.0, 0.0, 0.4755134281200631, 0.8883333333333334, 1.0, 1.0, 0.44327065821551415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2087974690205489, 0.20879746902054908, 0.28320804919193804], 
reward next is 0.7168, 
noisyNet noise sample is [array([-0.02615843], dtype=float32), 1.6187503]. 
=============================================
[2019-03-27 12:39:48,656] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1684286e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5107327e-38], sum to 1.0000
[2019-03-27 12:39:48,668] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0185
[2019-03-27 12:39:48,675] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 87.0, 1.0, 2.0, 0.5194312625950179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725833.7629470523, 725833.7629470523, 186695.802563423], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5704800.0000, 
sim time next is 5705400.0000, 
raw observation next is [26.5, 87.0, 1.0, 2.0, 0.5186722328170297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724772.7613659343, 724772.761365935, 186572.6404854767], 
processed observation next is [0.0, 0.0, 0.4549763033175356, 0.87, 1.0, 1.0, 0.42008702749039717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20132576704609284, 0.20132576704609303, 0.27846662759026375], 
reward next is 0.7215, 
noisyNet noise sample is [array([1.3745104], dtype=float32), 1.4096446]. 
=============================================
[2019-03-27 12:40:02,751] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.4314932e-12 9.9999988e-01 1.3929118e-18 1.0683511e-07 1.8383606e-17], sum to 1.0000
[2019-03-27 12:40:02,763] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0210
[2019-03-27 12:40:02,774] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1783314.044263167 W.
[2019-03-27 12:40:02,781] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 71.66666666666666, 1.0, 2.0, 0.6377824990612463, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.973271782006593, 6.9112, 168.9125460377471, 1783314.044263167, 1739278.292772666, 372674.8943411928], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6007200.0000, 
sim time next is 6007800.0000, 
raw observation next is [28.0, 72.83333333333334, 1.0, 2.0, 0.3937309067466404, 1.0, 1.0, 0.3937309067466404, 1.0, 2.0, 0.6655238052016573, 6.9112, 6.9112, 170.5573041426782, 1651261.078123809, 1651261.078123809, 346887.8113593315], 
processed observation next is [1.0, 0.5217391304347826, 0.5260663507109005, 0.7283333333333334, 1.0, 1.0, 0.2695553093333017, 1.0, 0.5, 0.2695553093333017, 1.0, 1.0, 0.5921022014654357, 0.0, 0.0, 0.8375144448122397, 0.4586836328121692, 0.4586836328121692, 0.5177430020288529], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.84786594], dtype=float32), 0.7044801]. 
=============================================
[2019-03-27 12:40:04,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4842050e-23 1.0000000e+00 1.9481154e-30 5.2753925e-27 1.6199453e-28], sum to 1.0000
[2019-03-27 12:40:04,377] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6225
[2019-03-27 12:40:04,385] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.33333333333334, 1.0, 2.0, 0.540706693735809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755573.8258657443, 755573.8258657443, 190219.5559314225], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6043200.0000, 
sim time next is 6043800.0000, 
raw observation next is [26.95, 89.66666666666666, 1.0, 2.0, 0.5406175873940464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755449.2659855877, 755449.2659855883, 190204.5194477715], 
processed observation next is [1.0, 0.9565217391304348, 0.476303317535545, 0.8966666666666666, 1.0, 1.0, 0.4465272137277667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20984701832932992, 0.2098470183293301, 0.2838873424593605], 
reward next is 0.7161, 
noisyNet noise sample is [array([2.067749], dtype=float32), -1.0302802]. 
=============================================
[2019-03-27 12:40:05,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5710343e-08 7.5752579e-02 1.6635144e-14 9.2424738e-01 9.5743983e-14], sum to 1.0000
[2019-03-27 12:40:05,224] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6411
[2019-03-27 12:40:05,229] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.1, 65.0, 1.0, 2.0, 0.8146563165980546, 1.0, 2.0, 0.8146563165980546, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2278371.268214217, 2278371.268214218, 427066.3844020094], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6098400.0000, 
sim time next is 6099000.0000, 
raw observation next is [31.05, 65.16666666666667, 1.0, 2.0, 0.8880446347856302, 1.0, 2.0, 0.8880446347856302, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2483822.33329758, 2483822.33329758, 465005.9271364259], 
processed observation next is [1.0, 0.6086956521739131, 0.6706161137440759, 0.6516666666666667, 1.0, 1.0, 0.8651140178140123, 1.0, 1.0, 0.8651140178140123, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6899506481382167, 0.6899506481382167, 0.6940386972185462], 
reward next is 0.3060, 
noisyNet noise sample is [array([1.6725413], dtype=float32), -0.55064774]. 
=============================================
[2019-03-27 12:40:05,247] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[42.723682]
 [43.145325]
 [44.2314  ]
 [45.05701 ]
 [47.56705 ]], R is [[41.45885849]
 [41.04426956]
 [40.63382721]
 [40.22748947]
 [39.82521439]].
[2019-03-27 12:40:07,475] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3445389e-22 1.0000000e+00 5.1005907e-30 4.6425051e-25 1.9199946e-27], sum to 1.0000
[2019-03-27 12:40:07,486] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4867
[2019-03-27 12:40:07,494] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333333, 84.33333333333334, 1.0, 2.0, 0.6946400925051264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970775.8972965084, 970775.8972965084, 219693.3994104363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6073800.0000, 
sim time next is 6074400.0000, 
raw observation next is [28.06666666666667, 83.66666666666667, 1.0, 2.0, 0.7124276490214955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995646.0795226158, 995646.0795226158, 223551.506195066], 
processed observation next is [1.0, 0.30434782608695654, 0.529225908372828, 0.8366666666666667, 1.0, 1.0, 0.6535272879777053, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27656835542294883, 0.27656835542294883, 0.3336589644702478], 
reward next is 0.6663, 
noisyNet noise sample is [array([-0.6010496], dtype=float32), 1.3469601]. 
=============================================
[2019-03-27 12:40:10,201] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.8126921e-23 1.0000000e+00 2.9465374e-31 6.0278141e-27 3.8636825e-28], sum to 1.0000
[2019-03-27 12:40:10,210] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6139
[2019-03-27 12:40:10,216] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.11666666666667, 93.0, 1.0, 2.0, 0.6510489065127901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 909830.0929953177, 909830.0929953177, 210633.3278343124], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6061800.0000, 
sim time next is 6062400.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.6464322751620479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 903375.6830844246, 903375.683084424, 209706.8720106509], 
processed observation next is [1.0, 0.17391304347826086, 0.4360189573459717, 0.93, 1.0, 1.0, 0.574014789351865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2509376897456735, 0.2509376897456733, 0.31299533135918045], 
reward next is 0.6870, 
noisyNet noise sample is [array([0.84494376], dtype=float32), 0.2652934]. 
=============================================
[2019-03-27 12:40:13,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9378577e-09 9.4290179e-01 2.7622748e-16 5.7098217e-02 3.5806420e-15], sum to 1.0000
[2019-03-27 12:40:13,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7188
[2019-03-27 12:40:13,207] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1927935.367984834 W.
[2019-03-27 12:40:13,211] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.76666666666667, 66.16666666666667, 1.0, 2.0, 0.4596425351522331, 1.0, 2.0, 0.4596425351522331, 1.0, 2.0, 0.7936449474943192, 6.9112, 6.9112, 170.5573041426782, 1927935.367984834, 1927935.367984834, 387604.517113168], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6102600.0000, 
sim time next is 6103200.0000, 
raw observation next is [30.73333333333333, 66.33333333333334, 1.0, 2.0, 0.4805879855294945, 1.0, 2.0, 0.4805879855294945, 1.0, 2.0, 0.8295584781786866, 6.9112, 6.9112, 170.5573041426782, 2015872.107969355, 2015872.107969355, 401106.7096641161], 
processed observation next is [1.0, 0.6521739130434783, 0.6556082148499209, 0.6633333333333334, 1.0, 1.0, 0.37420239220421025, 1.0, 1.0, 0.37420239220421025, 1.0, 1.0, 0.7921444855837639, 0.0, 0.0, 0.8375144448122397, 0.559964474435932, 0.559964474435932, 0.5986667308419643], 
reward next is 0.4013, 
noisyNet noise sample is [array([1.7262046], dtype=float32), -0.83906734]. 
=============================================
[2019-03-27 12:40:15,539] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 12:40:15,541] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:40:15,543] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:40:15,543] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:40:15,545] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:40:15,545] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:40:15,545] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:40:15,547] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:40:15,547] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:40:15,548] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:40:15,549] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:40:15,570] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run33
[2019-03-27 12:40:15,590] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run33
[2019-03-27 12:40:15,591] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run33
[2019-03-27 12:40:15,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run33
[2019-03-27 12:40:15,647] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run33
[2019-03-27 12:40:44,352] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05587186], dtype=float32), 0.045526113]
[2019-03-27 12:40:44,355] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.25, 94.00000000000001, 1.0, 2.0, 0.496998911905481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 694477.3908476771, 694477.3908476764, 183127.1937534992]
[2019-03-27 12:40:44,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:40:44,359] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.22709685e-27 1.00000000e+00 5.72506967e-36 1.00356824e-32
 3.76473776e-33], sampled 0.5289185343797648
[2019-03-27 12:40:46,894] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05587186], dtype=float32), 0.045526113]
[2019-03-27 12:40:46,896] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.37332887, 89.133415405, 1.0, 2.0, 0.5109081057038816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713919.8209602897, 713919.8209602897, 185320.720228282]
[2019-03-27 12:40:46,900] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:40:46,903] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3294400e-28 1.0000000e+00 4.1585696e-37 1.4849879e-34 3.4068042e-34], sampled 0.47975485131133244
[2019-03-27 12:40:53,599] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05587186], dtype=float32), 0.045526113]
[2019-03-27 12:40:53,600] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.01788992666666, 63.89347944333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.964058830902029, 6.9112, 168.9125330547986, 1491280.388277517, 1453780.608385214, 311359.8441928942]
[2019-03-27 12:40:53,601] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:40:53,603] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0072510e-22 1.0000000e+00 2.8613904e-30 1.1278303e-24 8.1953442e-28], sampled 0.19369585559837588
[2019-03-27 12:41:16,310] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05587186], dtype=float32), 0.045526113]
[2019-03-27 12:41:16,311] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.1, 50.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.156659314565713, 6.9112, 168.9114777194343, 2457974.023420427, 2283838.262138591, 475549.8922486813]
[2019-03-27 12:41:16,312] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:41:16,313] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7464707e-12 9.9999928e-01 2.3757827e-19 7.2342453e-07 6.2012006e-18], sampled 0.7606596845329807
[2019-03-27 12:41:16,316] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2457974.023420427 W.
[2019-03-27 12:42:04,652] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05587186], dtype=float32), 0.045526113]
[2019-03-27 12:42:04,654] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.7, 60.33333333333334, 1.0, 2.0, 0.8331220835401164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1164413.850451363, 1164413.850451363, 252205.4255806967]
[2019-03-27 12:42:04,654] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:42:04,657] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.8576236e-26 1.0000000e+00 2.5638133e-34 2.8655726e-31 1.4108872e-31], sampled 0.5731018623900273
[2019-03-27 12:42:08,735] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05587186], dtype=float32), 0.045526113]
[2019-03-27 12:42:08,737] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.7, 79.0, 1.0, 2.0, 0.6215545178544234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868595.3174719078, 868595.3174719078, 204827.3792776302]
[2019-03-27 12:42:08,739] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:42:08,743] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.8907453e-27 1.0000000e+00 2.8497820e-35 9.9227747e-33 2.0132683e-32], sampled 0.9182439760574951
[2019-03-27 12:42:16,773] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05587186], dtype=float32), 0.045526113]
[2019-03-27 12:42:16,775] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.04220651166667, 91.61579892333333, 1.0, 2.0, 0.3930542836122966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590427.721464842, 590427.721464842, 173772.538616983]
[2019-03-27 12:42:16,776] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:42:16,778] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0462300e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2953513e-37], sampled 0.3120969012334853
[2019-03-27 12:42:23,361] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.9058 2927350414.6102 1336.0000
[2019-03-27 12:42:23,505] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-03-27 12:42:23,804] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 12:42:23,937] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.6476 2779075539.2193 927.0000
[2019-03-27 12:42:24,126] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7891.5358 3163111877.2069 1763.0000
[2019-03-27 12:42:25,143] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 800000, evaluation results [800000.0, 7891.535823511164, 3163111877.2068624, 1763.0, 8255.905814281552, 2927350414.6102366, 1336.0, 8661.647623793686, 2779075539.219253, 927.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 12:42:30,780] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.2944401e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3476361e-36], sum to 1.0000
[2019-03-27 12:42:30,790] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5051
[2019-03-27 12:42:30,798] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.86666666666667, 62.66666666666666, 1.0, 2.0, 0.5167895115338861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722141.0243194941, 722141.0243194941, 186268.4968835686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6270000.0000, 
sim time next is 6270600.0000, 
raw observation next is [30.88333333333333, 62.33333333333334, 1.0, 2.0, 0.5146297733259568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719122.070858105, 719122.0708581056, 185920.072765458], 
processed observation next is [0.0, 0.5652173913043478, 0.6627172195892573, 0.6233333333333334, 1.0, 1.0, 0.4152165943686227, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19975613079391805, 0.1997561307939182, 0.277492645918594], 
reward next is 0.7225, 
noisyNet noise sample is [array([0.71096087], dtype=float32), -0.14990953]. 
=============================================
[2019-03-27 12:42:37,028] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.215105e-31 1.000000e+00 0.000000e+00 0.000000e+00 4.589371e-37], sum to 1.0000
[2019-03-27 12:42:37,044] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3325
[2019-03-27 12:42:37,053] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.76666666666667, 62.0, 1.0, 2.0, 0.5078006300671732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709576.1251180142, 709576.1251180142, 184827.4236625128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6273600.0000, 
sim time next is 6274200.0000, 
raw observation next is [30.73333333333333, 62.0, 1.0, 2.0, 0.5066521727252193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707970.7914016352, 707970.7914016346, 184645.0240866598], 
processed observation next is [0.0, 0.6086956521739131, 0.6556082148499209, 0.62, 1.0, 1.0, 0.4056050273797823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1966585531671209, 0.19665855316712072, 0.2755895881890445], 
reward next is 0.7244, 
noisyNet noise sample is [array([-0.71111166], dtype=float32), -0.6278996]. 
=============================================
[2019-03-27 12:42:41,536] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2101329e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.1005589e-37], sum to 1.0000
[2019-03-27 12:42:41,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6932
[2019-03-27 12:42:41,550] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 82.33333333333334, 1.0, 2.0, 0.5241016559454967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 732362.2451954585, 732362.245195458, 187458.3822954582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6331200.0000, 
sim time next is 6331800.0000, 
raw observation next is [27.7, 81.66666666666666, 1.0, 2.0, 0.5237736778688047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731903.7816338924, 731903.781633893, 187404.6838262411], 
processed observation next is [0.0, 0.2608695652173913, 0.5118483412322274, 0.8166666666666665, 1.0, 1.0, 0.4262333468298851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20330660600941458, 0.20330660600941472, 0.2797084833227479], 
reward next is 0.7203, 
noisyNet noise sample is [array([1.1084324], dtype=float32), 1.2269304]. 
=============================================
[2019-03-27 12:42:46,925] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8747282e-31 1.0000000e+00 0.0000000e+00 1.6052786e-37 8.6028390e-37], sum to 1.0000
[2019-03-27 12:42:46,939] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4430
[2019-03-27 12:42:46,943] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 82.0, 1.0, 2.0, 0.5193917096283556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725778.4742334208, 725778.4742334208, 186689.7429614227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6386400.0000, 
sim time next is 6387000.0000, 
raw observation next is [27.36666666666667, 82.00000000000001, 1.0, 2.0, 0.5182384788019105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724166.4434743027, 724166.4434743021, 186502.6230447753], 
processed observation next is [0.0, 0.9565217391304348, 0.49605055292259104, 0.8200000000000002, 1.0, 1.0, 0.41956443229145846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2011573454095285, 0.20115734540952834, 0.2783621239474258], 
reward next is 0.7216, 
noisyNet noise sample is [array([0.22680672], dtype=float32), 1.3492726]. 
=============================================
[2019-03-27 12:42:46,957] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[70.793  ]
 [70.72345]
 [70.72082]
 [70.71557]
 [70.71361]], R is [[70.82206726]
 [70.83520508]
 [70.84797668]
 [70.86034393]
 [70.87228394]].
[2019-03-27 12:42:47,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3492625e-30 1.0000000e+00 2.8966322e-38 0.0000000e+00 2.0042767e-35], sum to 1.0000
[2019-03-27 12:42:47,178] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9942
[2019-03-27 12:42:47,183] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 78.33333333333334, 1.0, 2.0, 0.4979545386699867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695813.1654802216, 695813.1654802216, 183276.4264311119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6556800.0000, 
sim time next is 6557400.0000, 
raw observation next is [27.6, 79.0, 1.0, 2.0, 0.5000774113909455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698780.5219603968, 698780.5219603961, 183608.4527207948], 
processed observation next is [1.0, 0.9130434782608695, 0.5071090047393366, 0.79, 1.0, 1.0, 0.39768362818186204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19410570054455467, 0.19410570054455448, 0.2740424667474549], 
reward next is 0.7260, 
noisyNet noise sample is [array([-0.6555927], dtype=float32), -1.251225]. 
=============================================
[2019-03-27 12:42:52,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2359212e-13 1.0000000e+00 3.8986984e-20 1.6343030e-08 1.8376798e-18], sum to 1.0000
[2019-03-27 12:42:52,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4838
[2019-03-27 12:42:52,872] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2209714.768608844 W.
[2019-03-27 12:42:52,877] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.1, 55.0, 1.0, 2.0, 0.5267528523802157, 1.0, 1.0, 0.5267528523802157, 1.0, 2.0, 0.8974980552110042, 6.911200000000001, 6.9112, 170.5573041426782, 2209714.768608844, 2209714.768608843, 430790.2735439556], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6530400.0000, 
sim time next is 6531000.0000, 
raw observation next is [31.98333333333333, 55.5, 1.0, 2.0, 0.5313144418300934, 1.0, 2.0, 0.5313144418300934, 1.0, 2.0, 0.9066416896339565, 6.9112, 6.9112, 170.5573041426782, 2228868.016801956, 2228868.016801956, 434337.9350754498], 
processed observation next is [1.0, 0.6086956521739131, 0.7148499210110584, 0.555, 1.0, 1.0, 0.4353186046145703, 1.0, 1.0, 0.4353186046145703, 1.0, 1.0, 0.8861484019926298, 0.0, 0.0, 0.8375144448122397, 0.61913000466721, 0.61913000466721, 0.6482655747394773], 
reward next is 0.3517, 
noisyNet noise sample is [array([-1.0099972], dtype=float32), 1.3590168]. 
=============================================
[2019-03-27 12:42:52,896] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[57.104755]
 [58.220074]
 [57.26986 ]
 [55.755836]
 [56.35004 ]], R is [[57.33974838]
 [57.12337875]
 [56.62283325]
 [56.05660629]
 [55.49604034]].
[2019-03-27 12:42:54,363] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.6352861e-09 9.9374545e-01 3.3716996e-15 6.2545463e-03 7.0193654e-14], sum to 1.0000
[2019-03-27 12:42:54,379] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3863
[2019-03-27 12:42:54,390] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2230979.127838785 W.
[2019-03-27 12:42:54,394] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.93333333333333, 69.83333333333333, 1.0, 2.0, 0.7977258551414182, 1.0, 1.0, 0.7977258551414182, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2230979.127838785, 2230979.127838785, 418761.3086189187], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6605400.0000, 
sim time next is 6606000.0000, 
raw observation next is [30.1, 69.0, 1.0, 2.0, 1.022141235821044, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.986466917971255, 6.9112, 168.9125088104736, 2325993.100722964, 2272596.300154146, 470908.804781801], 
processed observation next is [1.0, 0.4782608695652174, 0.6255924170616115, 0.69, 1.0, 1.0, 1.0266761877361976, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007526691797125462, 0.0, 0.8294377467405539, 0.6461091946452677, 0.6312767500428182, 0.702848962360897], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0605747], dtype=float32), 1.4093108]. 
=============================================
[2019-03-27 12:42:54,411] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[44.23795 ]
 [44.05364 ]
 [45.300518]
 [45.751446]
 [46.89046 ]], R is [[42.74586487]
 [42.31840515]
 [41.89522171]
 [41.8406868 ]
 [41.42227936]].
[2019-03-27 12:42:58,518] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.8844261e-27 1.0000000e+00 2.4183018e-35 5.9019891e-34 1.7408636e-32], sum to 1.0000
[2019-03-27 12:42:58,532] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0873
[2019-03-27 12:42:58,536] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 95.0, 1.0, 2.0, 0.5907893339468563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825585.6061579635, 825585.6061579635, 199035.3279170854], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6663600.0000, 
sim time next is 6664200.0000, 
raw observation next is [24.88333333333333, 95.0, 1.0, 2.0, 0.6139510287585741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857965.4815870208, 857965.4815870208, 203365.0439199614], 
processed observation next is [1.0, 0.13043478260869565, 0.3783570300157976, 0.95, 1.0, 1.0, 0.5348807575404507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23832374488528354, 0.23832374488528354, 0.30352991629844983], 
reward next is 0.6965, 
noisyNet noise sample is [array([-0.23237748], dtype=float32), -0.72209704]. 
=============================================
[2019-03-27 12:42:58,884] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.5469780e-27 1.0000000e+00 3.3349734e-36 2.2608910e-35 6.5214311e-33], sum to 1.0000
[2019-03-27 12:42:58,895] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2651
[2019-03-27 12:42:58,900] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 91.5, 1.0, 2.0, 0.754033857688654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1053821.267887807, 1053821.267887807, 232938.8551451454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6586200.0000, 
sim time next is 6586800.0000, 
raw observation next is [25.96666666666667, 91.0, 1.0, 2.0, 0.7025669105951161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 981858.9333790259, 981858.9333790265, 221398.4840133816], 
processed observation next is [1.0, 0.21739130434782608, 0.42969984202211703, 0.91, 1.0, 1.0, 0.6416468802350797, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.272738592605285, 0.27273859260528516, 0.3304454985274352], 
reward next is 0.6696, 
noisyNet noise sample is [array([-0.004387], dtype=float32), -1.3339835]. 
=============================================
[2019-03-27 12:43:01,631] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2097338e-29 1.0000000e+00 5.7265737e-37 1.4803774e-35 4.3060126e-34], sum to 1.0000
[2019-03-27 12:43:01,649] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9368
[2019-03-27 12:43:01,656] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666667, 87.0, 1.0, 2.0, 0.5100081957000538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712661.9057015249, 712661.9057015249, 185179.027469198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6646200.0000, 
sim time next is 6646800.0000, 
raw observation next is [26.53333333333334, 87.0, 1.0, 2.0, 0.5086050263659883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710700.5257481782, 710700.5257481782, 184955.3193013815], 
processed observation next is [1.0, 0.9565217391304348, 0.4565560821484995, 0.87, 1.0, 1.0, 0.4079578630915521, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1974168127078273, 0.1974168127078273, 0.27605271537519627], 
reward next is 0.7239, 
noisyNet noise sample is [array([0.19347508], dtype=float32), -1.6585472]. 
=============================================
[2019-03-27 12:43:11,673] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.1522798e-28 1.0000000e+00 1.3318036e-36 1.8198597e-36 5.1360374e-34], sum to 1.0000
[2019-03-27 12:43:11,687] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3780
[2019-03-27 12:43:11,694] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.88333333333333, 79.66666666666667, 1.0, 2.0, 0.3714902160695102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587046.9243801547, 587046.924380154, 174058.8901870213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6763800.0000, 
sim time next is 6764400.0000, 
raw observation next is [23.0, 79.0, 1.0, 2.0, 0.3478090090862956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549331.0909467237, 549331.0909467237, 170876.5057150823], 
processed observation next is [1.0, 0.30434782608695654, 0.28909952606635075, 0.79, 1.0, 1.0, 0.21422772179071756, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15259196970742325, 0.15259196970742325, 0.2550395607687796], 
reward next is 0.7450, 
noisyNet noise sample is [array([1.6060236], dtype=float32), 0.79816955]. 
=============================================
[2019-03-27 12:43:31,153] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 12:43:31,155] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:43:31,155] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:43:31,157] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:43:31,158] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:43:31,158] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:43:31,159] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:43:31,160] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:43:31,162] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:43:31,163] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:43:31,164] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:43:31,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run34
[2019-03-27 12:43:31,206] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run34
[2019-03-27 12:43:31,206] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run34
[2019-03-27 12:43:31,207] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run34
[2019-03-27 12:43:31,270] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run34
[2019-03-27 12:44:15,931] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05990269], dtype=float32), 0.045352362]
[2019-03-27 12:44:15,933] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.78318595333333, 92.70767881333333, 1.0, 2.0, 0.3450125434522149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536297.5910207671, 536297.5910207671, 169648.6170193763]
[2019-03-27 12:44:15,934] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:44:15,937] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.3243868e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5850356e-38], sampled 0.5070139119853909
[2019-03-27 12:44:18,042] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05990269], dtype=float32), 0.045352362]
[2019-03-27 12:44:18,045] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.85, 60.5, 1.0, 2.0, 0.5012642453016276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700439.4848034598, 700439.4848034598, 183794.4615224651]
[2019-03-27 12:44:18,046] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:44:18,048] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4600127e-29 1.0000000e+00 3.2216541e-38 5.2672016e-37 4.7826928e-35], sampled 0.5673267855424763
[2019-03-27 12:44:35,268] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05990269], dtype=float32), 0.045352362]
[2019-03-27 12:44:35,269] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.4, 47.0, 1.0, 2.0, 0.5849756824221436, 0.0, 2.0, 0.0, 1.0, 1.0, 1.003001779800796, 6.911200000000001, 6.9112, 168.9125732931978, 1635546.265063719, 1635546.265063718, 355309.906061211]
[2019-03-27 12:44:35,270] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:44:35,276] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.8133781e-20 1.0000000e+00 1.8052787e-27 3.0252451e-20 8.3162259e-25], sampled 0.44667349040310145
[2019-03-27 12:44:42,057] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05990269], dtype=float32), 0.045352362]
[2019-03-27 12:44:42,058] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 84.0, 1.0, 2.0, 0.7803543660497929, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005980301281884, 6.9112, 168.9123160116821, 1987578.954304762, 1920338.801457169, 403051.9729678099]
[2019-03-27 12:44:42,060] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:44:42,062] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6381362e-17 1.0000000e+00 7.7917759e-25 5.7563999e-16 1.6475460e-22], sampled 0.6810951409566481
[2019-03-27 12:44:42,066] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1987578.954304762 W.
[2019-03-27 12:45:37,178] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 12:45:38,734] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.1298 3007622042.5101 1764.0000
[2019-03-27 12:45:39,067] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 12:45:39,227] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 12:45:39,254] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.2051 3163556202.3285 1766.0000
[2019-03-27 12:45:40,270] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 825000, evaluation results [825000.0, 7890.205056329785, 3163556202.32854, 1766.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7999.129819288982, 3007622042.510076, 1764.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 12:45:50,965] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1237416e-29 1.0000000e+00 2.1219718e-38 8.8734462e-38 1.5283255e-35], sum to 1.0000
[2019-03-27 12:45:50,977] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3665
[2019-03-27 12:45:50,984] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 91.0, 1.0, 2.0, 0.3249681898428589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513389.5742666351, 513389.5742666351, 168030.9377689005], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7275600.0000, 
sim time next is 7276200.0000, 
raw observation next is [21.45, 90.66666666666667, 1.0, 2.0, 0.3474194645464314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548833.0656638375, 548833.0656638375, 170837.2225097524], 
processed observation next is [1.0, 0.21739130434782608, 0.2156398104265403, 0.9066666666666667, 1.0, 1.0, 0.21375839101979688, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15245362935106596, 0.15245362935106596, 0.2549809291190334], 
reward next is 0.7450, 
noisyNet noise sample is [array([0.6191291], dtype=float32), 0.2773719]. 
=============================================
[2019-03-27 12:45:57,186] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0531734e-29 1.0000000e+00 2.1480500e-38 4.9517628e-36 1.7277418e-35], sum to 1.0000
[2019-03-27 12:45:57,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1723
[2019-03-27 12:45:57,200] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.45, 91.5, 1.0, 2.0, 0.3612032419229793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 553953.6647652133, 553953.6647652133, 170909.5404931365], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7248600.0000, 
sim time next is 7249200.0000, 
raw observation next is [22.43333333333333, 91.33333333333334, 1.0, 2.0, 0.3594736064731612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 551913.1091430961, 551913.1091430968, 170755.0267024983], 
processed observation next is [1.0, 0.9130434782608695, 0.2622432859399683, 0.9133333333333334, 1.0, 1.0, 0.2282814535821219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15330919698419337, 0.15330919698419357, 0.25485824880969893], 
reward next is 0.7451, 
noisyNet noise sample is [array([0.2144159], dtype=float32), 0.37575498]. 
=============================================
[2019-03-27 12:45:58,779] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1217037e-24 1.0000000e+00 2.5367900e-32 1.5710881e-27 4.6451887e-29], sum to 1.0000
[2019-03-27 12:45:58,792] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5944
[2019-03-27 12:45:58,799] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 64.33333333333334, 1.0, 2.0, 0.7743874539833375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1179373.118487598, 1179373.118487598, 250502.7218039786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7299600.0000, 
sim time next is 7300200.0000, 
raw observation next is [26.81666666666667, 63.66666666666666, 1.0, 2.0, 0.7866281003577029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1199196.73302138, 1199196.733021379, 253857.3074249674], 
processed observation next is [1.0, 0.4782608695652174, 0.46998420221169057, 0.6366666666666666, 1.0, 1.0, 0.7429254221177144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33311020361704996, 0.33311020361704974, 0.37889150361935436], 
reward next is 0.6211, 
noisyNet noise sample is [array([-0.9541391], dtype=float32), 0.25597644]. 
=============================================
[2019-03-27 12:46:22,433] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.6031695e-21 1.0000000e+00 3.4023952e-30 5.3650623e-24 2.1954898e-27], sum to 1.0000
[2019-03-27 12:46:22,443] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6078
[2019-03-27 12:46:22,451] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 87.16666666666666, 1.0, 2.0, 0.5080128537097623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709872.7752950997, 709872.7752951003, 184860.9908938631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7858200.0000, 
sim time next is 7858800.0000, 
raw observation next is [26.4, 87.0, 1.0, 2.0, 0.5058889047323214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706903.8834219666, 706903.8834219672, 184523.7968878408], 
processed observation next is [1.0, 1.0, 0.45023696682464454, 0.87, 1.0, 1.0, 0.404685427388339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19636218983943515, 0.19636218983943532, 0.2754086520714042], 
reward next is 0.7246, 
noisyNet noise sample is [array([-1.3612971], dtype=float32), 0.61474884]. 
=============================================
[2019-03-27 12:46:25,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1617538e-19 1.0000000e+00 1.8534626e-27 1.0551699e-18 1.1982631e-24], sum to 1.0000
[2019-03-27 12:46:25,011] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4774
[2019-03-27 12:46:25,020] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 86.33333333333334, 1.0, 2.0, 0.5011073382071519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 700220.1590717006, 700220.1590717013, 183769.7325406553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7780800.0000, 
sim time next is 7781400.0000, 
raw observation next is [26.4, 86.0, 1.0, 2.0, 0.4989857216511807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697254.5544403295, 697254.5544403295, 183437.3057700585], 
processed observation next is [1.0, 0.043478260869565216, 0.45023696682464454, 0.86, 1.0, 1.0, 0.3963683393387719, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19368182067786932, 0.19368182067786932, 0.27378702353740075], 
reward next is 0.7262, 
noisyNet noise sample is [array([1.522325], dtype=float32), -0.6469924]. 
=============================================
[2019-03-27 12:46:29,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:29,863] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:29,906] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-03-27 12:46:35,329] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:35,329] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:35,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-03-27 12:46:37,288] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:37,289] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:37,318] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-03-27 12:46:37,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:37,628] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:37,656] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-03-27 12:46:37,815] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:37,816] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:37,846] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-03-27 12:46:38,310] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:38,311] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:38,331] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-03-27 12:46:40,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:40,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:40,204] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-03-27 12:46:40,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:40,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:40,962] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-03-27 12:46:41,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:41,145] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:41,175] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-03-27 12:46:41,365] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:41,365] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:41,367] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-03-27 12:46:42,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:42,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:42,409] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-03-27 12:46:42,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:42,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:42,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-03-27 12:46:42,528] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:42,530] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:42,574] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-03-27 12:46:42,597] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:42,597] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:42,637] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-03-27 12:46:42,824] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.0157449e-30 1.0000000e+00 1.1955699e-38 1.8225197e-37 4.1753414e-36], sum to 1.0000
[2019-03-27 12:46:42,828] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4080
[2019-03-27 12:46:42,831] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 90.0, 1.0, 2.0, 0.4188307082407541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 645170.0220620681, 645170.0220620674, 179164.0193579936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 97800.0000, 
sim time next is 98400.0000, 
raw observation next is [22.5, 90.0, 1.0, 2.0, 0.3892222281961041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599538.9229341301, 599538.9229341301, 174958.839227896], 
processed observation next is [1.0, 0.13043478260869565, 0.2654028436018958, 0.9, 1.0, 1.0, 0.2641231665013302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16653858970392502, 0.16653858970392502, 0.26113259586253135], 
reward next is 0.7389, 
noisyNet noise sample is [array([-0.8212494], dtype=float32), -1.0954485]. 
=============================================
[2019-03-27 12:46:42,998] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:42,999] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:43,022] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-03-27 12:46:43,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 12:46:43,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:43,269] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-03-27 12:46:43,821] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 12:46:43,826] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:46:43,827] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:43,828] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:46:43,830] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:43,831] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:46:43,832] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:43,832] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:46:43,833] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:43,833] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:46:43,835] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:46:43,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run35
[2019-03-27 12:46:43,865] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run35
[2019-03-27 12:46:43,882] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run35
[2019-03-27 12:46:43,882] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run35
[2019-03-27 12:46:43,895] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run35
[2019-03-27 12:46:55,543] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0623543], dtype=float32), 0.050617542]
[2019-03-27 12:46:55,544] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.46666666666667, 52.33333333333334, 1.0, 2.0, 0.2975112656410189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478917.6156306319, 478917.6156306312, 165584.6694777079]
[2019-03-27 12:46:55,546] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:46:55,548] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1720209e-30 1.0000000e+00 0.0000000e+00 6.4390236e-38 1.6705453e-36], sampled 0.47597158140781026
[2019-03-27 12:47:18,547] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0623543], dtype=float32), 0.050617542]
[2019-03-27 12:47:18,549] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.03333333333333, 90.66666666666667, 1.0, 2.0, 0.4836329369629926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676854.2710601749, 676854.2710601743, 181207.5337946926]
[2019-03-27 12:47:18,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:47:18,554] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.254619e-33 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.20447773566505356
[2019-03-27 12:48:08,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0623543], dtype=float32), 0.050617542]
[2019-03-27 12:48:08,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.76666666666667, 83.33333333333333, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 1.0, 2.0, 1.03, 7.75307774736229, 6.9112, 178.6582176852504, 4372500.459196918, 3740785.272215913, 704307.9417979727]
[2019-03-27 12:48:08,715] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:48:08,720] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.2911859e-10 2.3371587e-05 1.7174738e-16 9.9997663e-01 9.1781568e-16], sampled 0.8493985885042108
[2019-03-27 12:48:08,720] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 0, 0, 1, 0] has been changed to [0, 0, 0, 0, 1] for the demand 4372500.459196918 W.
[2019-03-27 12:48:14,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0623543], dtype=float32), 0.050617542]
[2019-03-27 12:48:14,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 83.0, 1.0, 2.0, 0.969679587527562, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991736998089, 6.9112, 168.9123159343589, 2252563.828884839, 2185315.56320813, 454104.6989882875]
[2019-03-27 12:48:14,316] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:48:14,318] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.3291770e-15 1.0000000e+00 1.2904830e-22 5.3613641e-11 1.8092493e-20], sampled 0.022192053019188362
[2019-03-27 12:48:14,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2252563.828884839 W.
[2019-03-27 12:48:17,580] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0623543], dtype=float32), 0.050617542]
[2019-03-27 12:48:17,581] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.02914139, 58.78765249, 1.0, 2.0, 0.5560219144307185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776982.8676158215, 776982.8676158215, 192839.3461380671]
[2019-03-27 12:48:17,583] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:48:17,588] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0732481e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2024633e-37], sampled 0.14393413622595375
[2019-03-27 12:48:38,387] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0623543], dtype=float32), 0.050617542]
[2019-03-27 12:48:38,388] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.85, 94.5, 1.0, 2.0, 0.5321593207823059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743625.6995673788, 743625.6995673788, 188787.4472062719]
[2019-03-27 12:48:38,389] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:48:38,393] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8920953e-28 1.0000000e+00 8.2763899e-38 1.6854355e-34 3.1496731e-34], sampled 0.4144080583104961
[2019-03-27 12:48:51,860] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 12:48:51,981] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.4063 3163711241.0718 1774.0000
[2019-03-27 12:48:52,163] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 12:48:52,349] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0344 2842500170.3949 1131.0000
[2019-03-27 12:48:52,443] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2300 2927457024.3359 1338.0000
[2019-03-27 12:48:53,459] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 850000, evaluation results [850000.0, 7886.406320872624, 3163711241.0718193, 1774.0, 8254.230042292205, 2927457024.3359056, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8496.034361333246, 2842500170.394918, 1131.0]
[2019-03-27 12:49:00,846] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9450283e-29 1.0000000e+00 0.0000000e+00 1.1643298e-37 6.9577908e-36], sum to 1.0000
[2019-03-27 12:49:00,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8085
[2019-03-27 12:49:00,859] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 90.5, 1.0, 2.0, 0.3692542726211967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 567152.388750782, 567152.3887507827, 172060.6127550784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 102600.0000, 
sim time next is 103200.0000, 
raw observation next is [22.56666666666667, 90.66666666666667, 1.0, 2.0, 0.3710589140826002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569293.6442332807, 569293.6442332807, 172229.5895008858], 
processed observation next is [1.0, 0.17391304347826086, 0.26856240126382325, 0.9066666666666667, 1.0, 1.0, 0.24223965552120508, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15813712339813352, 0.15813712339813352, 0.2570590888072922], 
reward next is 0.7429, 
noisyNet noise sample is [array([0.31762606], dtype=float32), -0.48875058]. 
=============================================
[2019-03-27 12:49:06,641] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.1534909e-32 1.0000000e+00 0.0000000e+00 8.7396760e-38 2.4169529e-37], sum to 1.0000
[2019-03-27 12:49:06,648] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4873
[2019-03-27 12:49:06,654] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 79.66666666666667, 1.0, 2.0, 0.297148701371844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 473710.9967790074, 473710.9967790081, 165182.6205214337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 321600.0000, 
sim time next is 322200.0000, 
raw observation next is [22.4, 80.0, 1.0, 2.0, 0.2965845280916127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473010.2810107688, 473010.2810107688, 165136.1381677796], 
processed observation next is [0.0, 0.7391304347826086, 0.2606635071090047, 0.8, 1.0, 1.0, 0.152511479628449, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13139174472521356, 0.13139174472521356, 0.24647184801161134], 
reward next is 0.7535, 
noisyNet noise sample is [array([0.62594795], dtype=float32), 0.10515038]. 
=============================================
[2019-03-27 12:49:11,078] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3059024e-30 1.0000000e+00 0.0000000e+00 4.3050228e-36 2.1634537e-37], sum to 1.0000
[2019-03-27 12:49:11,090] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1898
[2019-03-27 12:49:11,102] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.73333333333333, 86.0, 1.0, 2.0, 0.2699359480484261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 437581.6072774009, 437581.6072774003, 162773.8817263108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 341400.0000, 
sim time next is 342000.0000, 
raw observation next is [20.7, 86.0, 1.0, 2.0, 0.2691632611297922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 436573.3455994083, 436573.3455994083, 162706.0330891417], 
processed observation next is [0.0, 1.0, 0.18009478672985785, 0.86, 1.0, 1.0, 0.1194738085901111, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12127037377761342, 0.12127037377761342, 0.24284482550618167], 
reward next is 0.7572, 
noisyNet noise sample is [array([-1.1201757], dtype=float32), 0.18698166]. 
=============================================
[2019-03-27 12:49:11,140] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.530014]
 [80.505264]
 [80.4471  ]
 [80.39987 ]
 [80.39725 ]], R is [[80.58187866]
 [80.5331192 ]
 [80.48480988]
 [80.43652344]
 [80.38824463]].
[2019-03-27 12:49:13,037] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0333607e-30 1.0000000e+00 1.8798768e-38 5.1625669e-37 7.8095907e-35], sum to 1.0000
[2019-03-27 12:49:13,047] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2362
[2019-03-27 12:49:13,052] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 85.5, 1.0, 2.0, 0.2575769303980422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419634.9768516916, 419634.9768516916, 161603.3024957541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 369000.0000, 
sim time next is 369600.0000, 
raw observation next is [20.56666666666667, 85.0, 1.0, 2.0, 0.2574934045363385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419474.0532968437, 419474.0532968437, 161593.9726072189], 
processed observation next is [1.0, 0.2608695652173913, 0.17377567140600336, 0.85, 1.0, 1.0, 0.10541374040522711, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11652057036023436, 0.11652057036023436, 0.24118503374211775], 
reward next is 0.7588, 
noisyNet noise sample is [array([1.6000236], dtype=float32), 2.0622423]. 
=============================================
[2019-03-27 12:49:15,532] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.1649386e-31 1.0000000e+00 0.0000000e+00 3.3254204e-36 3.4145288e-36], sum to 1.0000
[2019-03-27 12:49:15,545] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7180
[2019-03-27 12:49:15,550] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 76.5, 1.0, 2.0, 0.2398221583705169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 396352.3710722703, 396352.3710722709, 159849.4220526468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 503400.0000, 
sim time next is 504000.0000, 
raw observation next is [20.3, 78.0, 1.0, 2.0, 0.2397404706644758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 396341.8945255243, 396341.8945255243, 159833.531529172], 
processed observation next is [1.0, 0.8695652173913043, 0.16113744075829392, 0.78, 1.0, 1.0, 0.08402466345117564, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11009497070153453, 0.11009497070153453, 0.23855750974503284], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.9567147], dtype=float32), 0.40717524]. 
=============================================
[2019-03-27 12:49:15,567] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.497086]
 [82.53667 ]
 [82.5671  ]
 [82.53225 ]
 [82.494225]], R is [[82.48651886]
 [82.42307281]
 [82.36026764]
 [82.29810333]
 [82.23651886]].
[2019-03-27 12:49:16,094] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4743391e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6156082e-36], sum to 1.0000
[2019-03-27 12:49:16,098] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9935
[2019-03-27 12:49:16,104] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 79.33333333333334, 1.0, 2.0, 0.298073151873832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 474982.3846607098, 474982.3846607105, 165269.4648284748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 321000.0000, 
sim time next is 321600.0000, 
raw observation next is [22.46666666666667, 79.66666666666667, 1.0, 2.0, 0.297148701371844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 473710.9967790074, 473710.9967790081, 165182.6205214337], 
processed observation next is [0.0, 0.7391304347826086, 0.2638230647709322, 0.7966666666666667, 1.0, 1.0, 0.15319120647210122, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13158638799416872, 0.1315863879941689, 0.24654122465885628], 
reward next is 0.7535, 
noisyNet noise sample is [array([0.7650831], dtype=float32), -0.2669993]. 
=============================================
[2019-03-27 12:49:18,473] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9084043e-29 1.0000000e+00 2.4790957e-38 1.7043795e-34 5.8246204e-35], sum to 1.0000
[2019-03-27 12:49:18,481] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1076
[2019-03-27 12:49:18,489] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 80.33333333333333, 1.0, 2.0, 0.2564244061805682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 420498.6523125551, 420498.6523125558, 161549.4921758973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 416400.0000, 
sim time next is 417000.0000, 
raw observation next is [20.58333333333334, 80.16666666666667, 1.0, 2.0, 0.2540036223956345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 417094.4037067279, 417094.4037067285, 161307.9435036959], 
processed observation next is [1.0, 0.8260869565217391, 0.17456556082148533, 0.8016666666666667, 1.0, 1.0, 0.1012091836091982, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1158595565852022, 0.11585955658520236, 0.24075812463238191], 
reward next is 0.7592, 
noisyNet noise sample is [array([1.0634705], dtype=float32), 1.9465095]. 
=============================================
[2019-03-27 12:49:18,518] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[82.695206]
 [82.68234 ]
 [82.62492 ]
 [82.52939 ]
 [82.46028 ]], R is [[82.62451172]
 [82.55715179]
 [82.49020386]
 [82.42357635]
 [82.35710907]].
[2019-03-27 12:49:27,424] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6690091e-25 1.0000000e+00 1.4742412e-34 1.9848201e-29 7.9938565e-32], sum to 1.0000
[2019-03-27 12:49:27,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1349
[2019-03-27 12:49:27,440] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.9, 88.0, 1.0, 2.0, 0.2134813270544272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 356140.5977505077, 356140.5977505077, 156860.3708652732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 606600.0000, 
sim time next is 607200.0000, 
raw observation next is [17.83333333333333, 88.33333333333333, 1.0, 2.0, 0.2123993107496134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 354389.3453061864, 354389.3453061858, 156746.184946145], 
processed observation next is [1.0, 0.0, 0.044233807266982464, 0.8833333333333333, 1.0, 1.0, 0.05108350692724506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09844148480727401, 0.09844148480727383, 0.23394952977036565], 
reward next is 0.7661, 
noisyNet noise sample is [array([-0.44692707], dtype=float32), 2.170089]. 
=============================================
[2019-03-27 12:49:39,379] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5671207e-24 1.0000000e+00 3.4967360e-32 2.6599833e-25 1.3114000e-30], sum to 1.0000
[2019-03-27 12:49:39,398] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0818
[2019-03-27 12:49:39,405] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 54.0, 1.0, 2.0, 0.6266273055150398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1028773.728638557, 1028773.728638557, 221450.6096715211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 666000.0000, 
sim time next is 666600.0000, 
raw observation next is [24.48333333333333, 55.0, 1.0, 2.0, 0.3202940534713662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525968.4570514825, 525968.4570514825, 168698.9697377339], 
processed observation next is [1.0, 0.7391304347826086, 0.3593996840442337, 0.55, 1.0, 1.0, 0.1810771728570677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14610234918096737, 0.14610234918096737, 0.2517895070712446], 
reward next is 0.7482, 
noisyNet noise sample is [array([0.978958], dtype=float32), 0.21939091]. 
=============================================
[2019-03-27 12:49:56,720] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.0844236e-24 1.0000000e+00 2.7581052e-33 1.8969457e-27 1.1951552e-30], sum to 1.0000
[2019-03-27 12:49:56,733] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9966
[2019-03-27 12:49:56,740] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 96.66666666666667, 1.0, 2.0, 0.3810725633409504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572713.346781979, 572713.346781979, 172186.7688893835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1041600.0000, 
sim time next is 1042200.0000, 
raw observation next is [22.5, 96.5, 1.0, 2.0, 0.3812903962043592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572786.0985696589, 572786.0985696589, 172185.1566277203], 
processed observation next is [1.0, 0.043478260869565216, 0.2654028436018958, 0.965, 1.0, 1.0, 0.25456674241489063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15910724960268305, 0.15910724960268305, 0.2569927710861497], 
reward next is 0.7430, 
noisyNet noise sample is [array([0.18735103], dtype=float32), -0.6615729]. 
=============================================
[2019-03-27 12:49:59,146] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 12:49:59,147] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:49:59,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:49:59,151] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:49:59,155] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:49:59,153] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:49:59,156] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:49:59,158] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:49:59,160] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:49:59,162] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:49:59,157] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:49:59,180] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run36
[2019-03-27 12:49:59,180] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run36
[2019-03-27 12:49:59,182] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run36
[2019-03-27 12:49:59,216] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run36
[2019-03-27 12:49:59,217] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run36
[2019-03-27 12:50:00,508] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06287514], dtype=float32), 0.05234093]
[2019-03-27 12:50:00,510] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.9, 75.66666666666666, 1.0, 2.0, 0.4234696596998004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 625016.9785953887, 625016.9785953894, 176709.4813979769]
[2019-03-27 12:50:00,511] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:50:00,516] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.6375920e-31 1.0000000e+00 0.0000000e+00 2.2126208e-38 1.3542945e-36], sampled 0.2150666603680158
[2019-03-27 12:50:00,568] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06287514], dtype=float32), 0.05234093]
[2019-03-27 12:50:00,569] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.75, 63.5, 1.0, 2.0, 0.6826183107324646, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.93705628845204, 6.9112, 168.9128025205277, 1897030.039036958, 1878686.714827063, 388403.6393296625]
[2019-03-27 12:50:00,572] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:50:00,575] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.88082470e-18 1.00000000e+00 7.44367301e-26 4.09993884e-16
 1.22936864e-23], sampled 0.8943292243309842
[2019-03-27 12:50:00,575] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1897030.039036958 W.
[2019-03-27 12:50:49,819] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06287514], dtype=float32), 0.05234093]
[2019-03-27 12:50:49,820] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.85, 77.5, 1.0, 2.0, 0.57980447369614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810229.199101254, 810229.1991012546, 197043.4416231819]
[2019-03-27 12:50:49,822] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:50:49,827] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.62216072e-26 1.00000000e+00 1.46608051e-35 7.18320050e-29
 1.23181346e-32], sampled 0.46052054541526666
[2019-03-27 12:50:50,121] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06287514], dtype=float32), 0.05234093]
[2019-03-27 12:50:50,123] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 76.33333333333333, 1.0, 2.0, 0.7484835145437521, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.991122687702148, 6.9112, 168.9124154259257, 1942976.562022723, 1886276.838268895, 396073.3323852273]
[2019-03-27 12:50:50,124] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:50:50,128] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.6717349e-20 1.0000000e+00 3.8906872e-29 7.9636590e-18 1.1737504e-26], sampled 0.019163766184620012
[2019-03-27 12:50:50,129] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1942976.562022723 W.
[2019-03-27 12:51:18,462] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06287514], dtype=float32), 0.05234093]
[2019-03-27 12:51:18,463] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.61666666666667, 48.66666666666666, 1.0, 2.0, 0.5181376382455087, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8998337268970882, 6.9112, 6.9112, 168.9126666399283, 1448544.804162294, 1448544.804162294, 318173.5668836086]
[2019-03-27 12:51:18,463] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:51:18,467] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7610691e-24 1.0000000e+00 2.4116159e-33 3.1736870e-26 1.7189436e-30], sampled 0.6631871715424327
[2019-03-27 12:51:35,711] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06287514], dtype=float32), 0.05234093]
[2019-03-27 12:51:35,712] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.70968910666667, 86.03270338, 1.0, 2.0, 0.5210264884340526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728063.6356191417, 728063.6356191417, 186955.364509825]
[2019-03-27 12:51:35,712] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:51:35,714] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6166319e-26 1.0000000e+00 1.4370002e-35 3.8827404e-29 1.3627236e-32], sampled 0.42682863102116664
[2019-03-27 12:52:07,041] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5565 3007659593.2782 1766.0000
[2019-03-27 12:52:07,277] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1249 2927277438.0038 1338.0000
[2019-03-27 12:52:07,375] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8604 2842435242.9853 1131.0000
[2019-03-27 12:52:07,543] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.6715 3163980542.2427 1775.0000
[2019-03-27 12:52:07,808] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 12:52:08,824] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 875000, evaluation results [875000.0, 7885.671463494937, 3163980542.2427187, 1775.0, 8255.124881931173, 2927277438.00382, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7997.556493918532, 3007659593.2782063, 1766.0, 8496.860364057107, 2842435242.9852624, 1131.0]
[2019-03-27 12:52:09,264] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7319276e-25 1.0000000e+00 3.2545423e-34 3.6275468e-27 1.0049407e-30], sum to 1.0000
[2019-03-27 12:52:09,281] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1028
[2019-03-27 12:52:09,286] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 97.5, 1.0, 2.0, 0.3643546219693376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 555207.2581828908, 555207.2581828914, 170909.2774059923], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1031400.0000, 
sim time next is 1032000.0000, 
raw observation next is [21.96666666666667, 97.66666666666666, 1.0, 2.0, 0.3647809916978575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555219.2460212935, 555219.2460212935, 170890.4417163264], 
processed observation next is [1.0, 0.9565217391304348, 0.24012638230647723, 0.9766666666666666, 1.0, 1.0, 0.2346758936118765, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1542275683392482, 0.1542275683392482, 0.25506036077063643], 
reward next is 0.7449, 
noisyNet noise sample is [array([1.3343774], dtype=float32), 0.6108804]. 
=============================================
[2019-03-27 12:52:09,299] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.33194 ]
 [78.27522 ]
 [78.231575]
 [78.18662 ]
 [78.08568 ]], R is [[78.35064697]
 [78.31204987]
 [78.27386475]
 [78.23622894]
 [78.19915771]].
[2019-03-27 12:52:09,845] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6109098e-28 1.0000000e+00 1.9501978e-37 2.1125980e-35 1.0470618e-34], sum to 1.0000
[2019-03-27 12:52:09,859] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1975
[2019-03-27 12:52:09,871] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 93.0, 1.0, 2.0, 0.3475140407416505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540193.0937402694, 540193.0937402694, 169965.9488089769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1224000.0000, 
sim time next is 1224600.0000, 
raw observation next is [21.76666666666667, 93.33333333333333, 1.0, 2.0, 0.3797506007495984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590201.4224331676, 590201.4224331676, 174223.4194236858], 
processed observation next is [1.0, 0.17391304347826086, 0.23064770932069528, 0.9333333333333332, 1.0, 1.0, 0.2527115671681908, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16394483956476877, 0.16394483956476877, 0.26003495436371016], 
reward next is 0.7400, 
noisyNet noise sample is [array([0.47965604], dtype=float32), 0.10273725]. 
=============================================
[2019-03-27 12:52:10,729] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.2349997e-28 1.0000000e+00 3.5708410e-36 4.7371017e-34 1.1647320e-33], sum to 1.0000
[2019-03-27 12:52:10,739] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2810
[2019-03-27 12:52:10,744] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.16666666666667, 1.0, 2.0, 0.3442852464206104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532953.7233094486, 532953.7233094486, 169317.6355052818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 960600.0000, 
sim time next is 961200.0000, 
raw observation next is [21.8, 94.0, 1.0, 2.0, 0.3433815576369807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531856.691001625, 531856.6910016245, 169237.5690268535], 
processed observation next is [1.0, 0.13043478260869565, 0.23222748815165886, 0.94, 1.0, 1.0, 0.2088934429361213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1477379697226736, 0.14773796972267345, 0.25259338660724406], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.6814475], dtype=float32), -0.41215682]. 
=============================================
[2019-03-27 12:52:11,028] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2856646e-29 1.0000000e+00 0.0000000e+00 1.6144112e-36 7.2900653e-35], sum to 1.0000
[2019-03-27 12:52:11,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2377
[2019-03-27 12:52:11,045] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.83333333333333, 1.0, 2.0, 0.3275763960988012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508178.4018330448, 508178.4018330454, 167391.3225322385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 971400.0000, 
sim time next is 972000.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3283808777197444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 509139.258063013, 509139.2580630137, 167456.4418894371], 
processed observation next is [1.0, 0.2608695652173913, 0.23696682464454974, 0.93, 1.0, 1.0, 0.19082033460210168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14142757168417028, 0.14142757168417047, 0.24993498789468221], 
reward next is 0.7501, 
noisyNet noise sample is [array([-1.7885613], dtype=float32), 0.48805076]. 
=============================================
[2019-03-27 12:52:11,056] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.66103 ]
 [73.6676  ]
 [73.6745  ]
 [73.684586]
 [73.67317 ]], R is [[73.70678711]
 [73.71987915]
 [73.73290253]
 [73.74585724]
 [73.75908661]].
[2019-03-27 12:52:11,671] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8128432e-26 1.0000000e+00 6.3260188e-34 1.7788089e-28 2.4965953e-31], sum to 1.0000
[2019-03-27 12:52:11,680] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8310
[2019-03-27 12:52:11,688] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 96.0, 1.0, 2.0, 0.3541366531440557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544850.3118745922, 544850.3118745928, 170197.4571940295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1022400.0000, 
sim time next is 1023000.0000, 
raw observation next is [21.81666666666667, 96.0, 1.0, 2.0, 0.3542167051187126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544791.930782759, 544791.930782759, 170187.3006609642], 
processed observation next is [1.0, 0.8695652173913043, 0.2330173775671408, 0.96, 1.0, 1.0, 0.22194783749242483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1513310918840997, 0.1513310918840997, 0.2540108965089018], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.9037574], dtype=float32), 0.9597067]. 
=============================================
[2019-03-27 12:52:11,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.00384 ]
 [76.951805]
 [76.951546]
 [76.93037 ]
 [76.9264  ]], R is [[76.93213654]
 [76.90879059]
 [76.88552856]
 [76.86231232]
 [76.83914948]].
[2019-03-27 12:52:25,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8212839e-28 1.0000000e+00 4.8210973e-38 4.7207735e-35 5.4320438e-35], sum to 1.0000
[2019-03-27 12:52:25,600] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0516
[2019-03-27 12:52:25,609] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333334, 93.33333333333334, 1.0, 2.0, 0.5404809540497971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780564.6154391714, 780564.6154391721, 193394.8741250193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1318800.0000, 
sim time next is 1319400.0000, 
raw observation next is [23.85, 93.5, 1.0, 2.0, 0.5543588314973338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802578.0055774802, 802578.0055774802, 196096.477506292], 
processed observation next is [1.0, 0.2608695652173913, 0.3293838862559243, 0.935, 1.0, 1.0, 0.46308292951485996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2229383348826334, 0.2229383348826334, 0.2926813097108836], 
reward next is 0.7073, 
noisyNet noise sample is [array([1.1465133], dtype=float32), -0.65473557]. 
=============================================
[2019-03-27 12:52:33,610] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3758653e-25 1.0000000e+00 9.5647977e-33 1.5319690e-26 3.7303503e-29], sum to 1.0000
[2019-03-27 12:52:33,625] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7717
[2019-03-27 12:52:33,635] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666666, 89.0, 1.0, 2.0, 0.6739470771927084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1062920.317937728, 1062920.317937729, 229780.7504728547], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1345200.0000, 
sim time next is 1345800.0000, 
raw observation next is [21.58333333333334, 88.5, 1.0, 2.0, 0.681101081227123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1077826.909542685, 1077826.909542686, 231799.145224729], 
processed observation next is [1.0, 0.5652173913043478, 0.22195892575039528, 0.885, 1.0, 1.0, 0.6157844352134012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2993963637618569, 0.29939636376185724, 0.34596887346974475], 
reward next is 0.6540, 
noisyNet noise sample is [array([-0.9145265], dtype=float32), -0.5323314]. 
=============================================
[2019-03-27 12:52:37,092] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1204747e-23 1.0000000e+00 3.1761375e-31 4.8964350e-25 7.8725213e-28], sum to 1.0000
[2019-03-27 12:52:37,100] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0225
[2019-03-27 12:52:37,105] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 92.33333333333334, 1.0, 2.0, 0.6524538832873579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1001912.57165928, 1001912.571659281, 222064.0386547018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1340400.0000, 
sim time next is 1341000.0000, 
raw observation next is [22.2, 92.0, 1.0, 2.0, 0.5936243180710554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 914529.9528314535, 914529.9528314535, 209841.8331625114], 
processed observation next is [1.0, 0.5217391304347826, 0.2511848341232228, 0.92, 1.0, 1.0, 0.5103907446639222, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2540360980087371, 0.2540360980087371, 0.31319676591419615], 
reward next is 0.6868, 
noisyNet noise sample is [array([-0.26905409], dtype=float32), -2.1118755]. 
=============================================
[2019-03-27 12:52:37,123] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.69322 ]
 [69.692894]
 [70.33446 ]
 [70.50378 ]
 [70.200966]], R is [[71.235672  ]
 [71.19187927]
 [71.07165527]
 [70.98838043]
 [70.92591858]].
[2019-03-27 12:52:40,968] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.2590754e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7820320e-38], sum to 1.0000
[2019-03-27 12:52:40,976] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1014
[2019-03-27 12:52:40,983] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.18333333333334, 94.66666666666667, 1.0, 2.0, 0.3240559951978547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509367.5124877075, 509367.5124877075, 167669.4512294972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1404600.0000, 
sim time next is 1405200.0000, 
raw observation next is [21.26666666666667, 94.33333333333334, 1.0, 2.0, 0.3258178604321098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 511585.9685040126, 511585.9685040131, 167826.8852016396], 
processed observation next is [0.0, 0.2608695652173913, 0.2069510268562403, 0.9433333333333335, 1.0, 1.0, 0.18773236196639737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14210721347333685, 0.14210721347333696, 0.25048788836065616], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.20375696], dtype=float32), -0.5099288]. 
=============================================
[2019-03-27 12:52:42,573] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2963435e-30 1.0000000e+00 1.4649770e-38 3.0409591e-34 6.5596482e-35], sum to 1.0000
[2019-03-27 12:52:42,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2644
[2019-03-27 12:52:42,594] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 94.5, 1.0, 2.0, 0.3758193335885419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 568786.0975120612, 568786.0975120618, 171964.8258838967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1459800.0000, 
sim time next is 1460400.0000, 
raw observation next is [22.43333333333333, 94.66666666666666, 1.0, 2.0, 0.3737201578384591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566450.1328268448, 566450.1328268448, 171786.3868790344], 
processed observation next is [0.0, 0.9130434782608695, 0.2622432859399683, 0.9466666666666665, 1.0, 1.0, 0.24544597329934828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.157347259118568, 0.157347259118568, 0.25639759235676773], 
reward next is 0.7436, 
noisyNet noise sample is [array([1.0865077], dtype=float32), -0.21574889]. 
=============================================
[2019-03-27 12:52:44,979] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.79891043e-32 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.20366175e-36], sum to 1.0000
[2019-03-27 12:52:44,990] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3746
[2019-03-27 12:52:45,004] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 51.00000000000001, 1.0, 2.0, 0.3512963936820009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537565.9046053564, 537565.9046053557, 169504.0934773705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1513200.0000, 
sim time next is 1513800.0000, 
raw observation next is [29.3, 51.0, 1.0, 2.0, 0.3526428651476576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538711.9606284414, 538711.9606284414, 169568.5944270689], 
processed observation next is [0.0, 0.5217391304347826, 0.5876777251184835, 0.51, 1.0, 1.0, 0.22005164475621394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14964221128567817, 0.14964221128567817, 0.25308745436875957], 
reward next is 0.7469, 
noisyNet noise sample is [array([-0.88401866], dtype=float32), -0.14235517]. 
=============================================
[2019-03-27 12:52:47,040] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5409919e-30 1.0000000e+00 1.9592268e-38 1.5281456e-34 3.9645378e-35], sum to 1.0000
[2019-03-27 12:52:47,052] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6713
[2019-03-27 12:52:47,060] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.35, 84.5, 1.0, 2.0, 0.3581996044590471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549699.2639673849, 549699.2639673844, 170561.4200263185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1542600.0000, 
sim time next is 1543200.0000, 
raw observation next is [23.26666666666667, 85.0, 1.0, 2.0, 0.3581687164955957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549893.545817771, 549893.5458177704, 170584.802836403], 
processed observation next is [0.0, 0.8695652173913043, 0.3017377567140602, 0.85, 1.0, 1.0, 0.2267092969826454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15274820717160306, 0.1527482071716029, 0.2546041833379149], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.40216804], dtype=float32), -0.7901263]. 
=============================================
[2019-03-27 12:53:02,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9512441e-13 9.9999762e-01 2.3964774e-19 2.3326929e-06 2.9077591e-18], sum to 1.0000
[2019-03-27 12:53:02,027] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2969
[2019-03-27 12:53:02,037] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1951389.299742888 W.
[2019-03-27 12:53:02,044] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.45, 77.33333333333334, 1.0, 2.0, 0.6978437122150589, 1.0, 2.0, 0.6978437122150589, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1951389.299742888, 1951389.299742888, 373198.9684441304], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1698600.0000, 
sim time next is 1699200.0000, 
raw observation next is [28.5, 77.0, 1.0, 2.0, 0.4546813713854433, 1.0, 2.0, 0.4546813713854433, 1.0, 1.0, 0.7801818278847791, 6.9112, 6.9112, 170.5573041426782, 1907107.626585217, 1907107.626585217, 383681.9272023166], 
processed observation next is [1.0, 0.6956521739130435, 0.5497630331753555, 0.77, 1.0, 1.0, 0.3429896040788473, 1.0, 1.0, 0.3429896040788473, 1.0, 0.5, 0.7319290583960719, 0.0, 0.0, 0.8375144448122397, 0.5297521184958937, 0.5297521184958937, 0.5726595928392785], 
reward next is 0.4273, 
noisyNet noise sample is [array([0.0166027], dtype=float32), -0.47240627]. 
=============================================
[2019-03-27 12:53:03,355] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.3375824e-30 1.0000000e+00 0.0000000e+00 6.2479190e-38 6.2375896e-36], sum to 1.0000
[2019-03-27 12:53:03,364] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6523
[2019-03-27 12:53:03,372] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 78.0, 1.0, 2.0, 0.5045633154050175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705050.9564022244, 705050.956402225, 184315.1357676582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1706400.0000, 
sim time next is 1707000.0000, 
raw observation next is [28.06666666666667, 78.66666666666667, 1.0, 2.0, 0.5064215132278067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707648.3718230554, 707648.371823056, 184609.1211600624], 
processed observation next is [1.0, 0.782608695652174, 0.529225908372828, 0.7866666666666667, 1.0, 1.0, 0.40532712437085144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19656899217307094, 0.1965689921730711, 0.27553600173143644], 
reward next is 0.7245, 
noisyNet noise sample is [array([0.5929962], dtype=float32), 0.48476943]. 
=============================================
[2019-03-27 12:53:03,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.25855]
 [72.59298]
 [72.97284]
 [72.81847]
 [72.18018]], R is [[73.52074432]
 [73.51043701]
 [73.50184631]
 [73.49550629]
 [73.49128723]].
[2019-03-27 12:53:07,408] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8485100e-30 1.0000000e+00 0.0000000e+00 3.5249764e-35 1.5664730e-36], sum to 1.0000
[2019-03-27 12:53:07,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1651
[2019-03-27 12:53:07,428] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.56666666666667, 95.66666666666666, 1.0, 2.0, 0.3450588332541842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535111.3019836913, 535111.3019836913, 169519.2272163641], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1812000.0000, 
sim time next is 1812600.0000, 
raw observation next is [21.6, 95.5, 1.0, 2.0, 0.3451039683524705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534982.1611068059, 534982.1611068066, 169503.3099992638], 
processed observation next is [1.0, 1.0, 0.22274881516587688, 0.955, 1.0, 1.0, 0.21096863656924153, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14860615586300163, 0.14860615586300183, 0.2529900149242743], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.15980448], dtype=float32), 0.676329]. 
=============================================
[2019-03-27 12:53:10,054] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9238461e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5373824e-38], sum to 1.0000
[2019-03-27 12:53:10,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4523
[2019-03-27 12:53:10,072] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.35, 92.0, 1.0, 2.0, 0.3197204806825953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504505.9850825375, 504505.9850825375, 167342.7604367631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1801800.0000, 
sim time next is 1802400.0000, 
raw observation next is [21.36666666666667, 92.33333333333334, 1.0, 2.0, 0.324855744863134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 511905.0107555073, 511905.0107555079, 167891.8686927303], 
processed observation next is [1.0, 0.8695652173913043, 0.21169036334913136, 0.9233333333333335, 1.0, 1.0, 0.18657318658208916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14219583632097424, 0.14219583632097443, 0.2505848786458661], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.57801676], dtype=float32), 0.032682817]. 
=============================================
[2019-03-27 12:53:11,883] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9524875e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 12:53:11,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9492
[2019-03-27 12:53:11,905] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 87.33333333333334, 1.0, 2.0, 0.4663137935741867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 657973.7384515434, 657973.7384515428, 179310.312292584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1887600.0000, 
sim time next is 1888200.0000, 
raw observation next is [25.25, 87.5, 1.0, 2.0, 0.4668237445662701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659459.1256455301, 659459.1256455301, 179484.1968688722], 
processed observation next is [1.0, 0.8695652173913043, 0.39573459715639814, 0.875, 1.0, 1.0, 0.357618969356952, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1831830904570917, 0.1831830904570917, 0.2678868609983167], 
reward next is 0.7321, 
noisyNet noise sample is [array([0.26757792], dtype=float32), 0.77690274]. 
=============================================
[2019-03-27 12:53:14,914] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 12:53:14,917] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:53:14,918] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:53:14,918] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:53:14,919] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:53:14,920] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:53:14,920] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:53:14,921] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:53:14,922] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:53:14,922] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:53:14,925] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:53:14,954] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run37
[2019-03-27 12:53:14,973] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run37
[2019-03-27 12:53:15,005] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run37
[2019-03-27 12:53:15,026] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run37
[2019-03-27 12:53:15,028] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run37
[2019-03-27 12:53:22,983] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06439663], dtype=float32), 0.05190986]
[2019-03-27 12:53:22,983] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.73333333333333, 92.0, 1.0, 2.0, 0.2898820390429611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463754.6694252208, 463754.6694252208, 164509.8184757568]
[2019-03-27 12:53:22,984] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:53:22,991] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4255992e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.3470912e-38], sampled 0.5233014042573413
[2019-03-27 12:53:57,504] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06439663], dtype=float32), 0.05190986]
[2019-03-27 12:53:57,505] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.46666666666667, 89.66666666666667, 1.0, 2.0, 0.4507839056681978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650073.3392051733, 650073.3392051739, 178828.4248826296]
[2019-03-27 12:53:57,505] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:53:57,508] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2459505e-29 1.0000000e+00 1.1908543e-38 7.9384117e-36 3.0464030e-35], sampled 0.0034239279154342217
[2019-03-27 12:54:42,893] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06439663], dtype=float32), 0.05190986]
[2019-03-27 12:54:42,895] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.01666666666667, 57.16666666666667, 1.0, 2.0, 0.6889362711818692, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00597459266293, 6.9112, 168.9123160451337, 1859646.836812311, 1792410.733826807, 382260.1379761883]
[2019-03-27 12:54:42,895] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:54:42,897] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.2050447e-23 1.0000000e+00 3.0631531e-32 4.4985786e-22 1.4715311e-29], sampled 0.5455131438374259
[2019-03-27 12:54:42,898] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1859646.836812311 W.
[2019-03-27 12:54:51,896] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06439663], dtype=float32), 0.05190986]
[2019-03-27 12:54:51,898] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.5, 75.0, 1.0, 2.0, 1.019508911052293, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005994404615503, 6.9112, 168.9123174056572, 2322308.59937272, 2255058.440617804, 469355.4078619841]
[2019-03-27 12:54:51,898] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:54:51,901] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.2418010e-23 1.0000000e+00 8.1571203e-32 6.7986005e-24 5.2022058e-29], sampled 0.5116810430483304
[2019-03-27 12:54:51,903] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2322308.59937272 W.
[2019-03-27 12:54:59,934] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06439663], dtype=float32), 0.05190986]
[2019-03-27 12:54:59,935] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.73287187166667, 92.34350884833334, 1.0, 2.0, 0.5139537066032548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718177.0442076884, 718177.044207689, 185810.7693798682]
[2019-03-27 12:54:59,937] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:54:59,938] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.3395573e-30 1.0000000e+00 0.0000000e+00 1.5455255e-37 1.4644707e-35], sampled 0.662098062921762
[2019-03-27 12:55:06,205] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06439663], dtype=float32), 0.05190986]
[2019-03-27 12:55:06,208] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.85, 84.33333333333333, 1.0, 2.0, 0.4236879052625635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619356.4395952794, 619356.4395952794, 176004.5588267967]
[2019-03-27 12:55:06,209] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:55:06,212] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1444192e-30 1.0000000e+00 1.3715331e-38 5.5536897e-37 2.7062833e-35], sampled 0.1387128594415662
[2019-03-27 12:55:15,046] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06439663], dtype=float32), 0.05190986]
[2019-03-27 12:55:15,048] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.83548847, 89.62626047500001, 1.0, 2.0, 0.3041994834001009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486776.1373093628, 486776.1373093628, 166135.6425294273]
[2019-03-27 12:55:15,051] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:55:15,053] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6541176e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.8378114502602212
[2019-03-27 12:55:22,582] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.2516 3164130202.9927 1777.0000
[2019-03-27 12:55:22,741] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-27 12:55:23,018] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 12:55:23,094] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5588 3007666311.2491 1766.0000
[2019-03-27 12:55:23,289] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8604 2842435242.9853 1131.0000
[2019-03-27 12:55:24,307] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 900000, evaluation results [900000.0, 7884.25162143558, 3164130202.992713, 1777.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7997.558800637214, 3007666311.249115, 1766.0, 8496.860364057107, 2842435242.9852624, 1131.0]
[2019-03-27 12:55:28,634] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4762683e-23 1.0000000e+00 4.1448309e-33 1.4825875e-27 2.7143145e-29], sum to 1.0000
[2019-03-27 12:55:28,647] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7683
[2019-03-27 12:55:28,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1698466.682908667 W.
[2019-03-27 12:55:28,666] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 80.66666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.213667205817211, 6.9112, 168.9109807955851, 1698466.682908667, 1483888.546857709, 316141.7367644325], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1935600.0000, 
sim time next is 1936200.0000, 
raw observation next is [26.05, 80.33333333333334, 1.0, 2.0, 0.5566220854301503, 1.0, 1.0, 0.5566220854301503, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1556202.706315943, 1556202.706315943, 318768.2174931149], 
processed observation next is [1.0, 0.391304347826087, 0.43364928909952616, 0.8033333333333335, 1.0, 1.0, 0.46580974148210885, 1.0, 0.5, 0.46580974148210885, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4322785295322064, 0.4322785295322064, 0.4757734589449476], 
reward next is 0.5242, 
noisyNet noise sample is [array([-0.4656598], dtype=float32), 0.325026]. 
=============================================
[2019-03-27 12:55:29,882] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6783558e-30 1.0000000e+00 0.0000000e+00 2.3642472e-35 3.9645378e-35], sum to 1.0000
[2019-03-27 12:55:29,889] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5493
[2019-03-27 12:55:29,899] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 89.66666666666667, 1.0, 2.0, 0.5391069384648353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753337.5638576915, 753337.5638576915, 189950.0373977612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [26.85, 90.0, 1.0, 2.0, 0.5382187451808397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752095.9804222521, 752095.9804222527, 189800.6613054311], 
processed observation next is [0.0, 0.9130434782608695, 0.4715639810426541, 0.9, 1.0, 1.0, 0.4436370423865538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20891555011729224, 0.2089155501172924, 0.28328456911258376], 
reward next is 0.7167, 
noisyNet noise sample is [array([0.16530922], dtype=float32), -0.27636397]. 
=============================================
[2019-03-27 12:55:29,916] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.06039 ]
 [76.04743 ]
 [76.00721 ]
 [75.908485]
 [75.889656]], R is [[76.05113983]
 [76.00711823]
 [75.963974  ]
 [75.92092896]
 [75.87788391]].
[2019-03-27 12:55:39,927] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3099187e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0912425e-38], sum to 1.0000
[2019-03-27 12:55:39,938] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8166
[2019-03-27 12:55:39,945] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.15, 81.5, 1.0, 2.0, 0.5281077366108463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737962.1537803897, 737962.1537803903, 188117.8881391904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2107800.0000, 
sim time next is 2108400.0000, 
raw observation next is [28.36666666666667, 80.66666666666667, 1.0, 2.0, 0.5318864316745767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743244.2379628149, 743244.2379628143, 188743.5019594662], 
processed observation next is [0.0, 0.391304347826087, 0.543443917851501, 0.8066666666666668, 1.0, 1.0, 0.4360077490055141, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20645673276744858, 0.20645673276744841, 0.28170671934248687], 
reward next is 0.7183, 
noisyNet noise sample is [array([-0.37940538], dtype=float32), -1.2562189]. 
=============================================
[2019-03-27 12:55:42,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8760241e-29 1.0000000e+00 4.5258241e-38 4.0243901e-35 2.1281279e-35], sum to 1.0000
[2019-03-27 12:55:42,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5825
[2019-03-27 12:55:42,537] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 72.66666666666667, 1.0, 2.0, 0.5543253982238353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 774611.2980177049, 774611.2980177054, 192546.7691664239], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2225400.0000, 
sim time next is 2226000.0000, 
raw observation next is [30.5, 73.33333333333334, 1.0, 2.0, 0.5552965038335744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775968.8113924285, 775968.8113924279, 192714.5463767926], 
processed observation next is [1.0, 0.782608695652174, 0.6445497630331753, 0.7333333333333334, 1.0, 1.0, 0.464212655221174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21554689205345234, 0.2155468920534522, 0.28763365130864565], 
reward next is 0.7124, 
noisyNet noise sample is [array([-1.4670314], dtype=float32), 0.21846609]. 
=============================================
[2019-03-27 12:55:42,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[70.861404]
 [70.50743 ]
 [70.07033 ]
 [69.07803 ]
 [68.31523 ]], R is [[70.92904663]
 [70.93237305]
 [70.93611908]
 [70.94090271]
 [70.94799042]].
[2019-03-27 12:55:46,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9568970e-08 3.5420343e-01 1.7159116e-14 6.4579654e-01 6.1518234e-14], sum to 1.0000
[2019-03-27 12:55:46,060] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6392
[2019-03-27 12:55:46,070] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.6, 65.33333333333334, 1.0, 2.0, 0.7366687460533244, 1.0, 2.0, 0.7366687460533244, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2060060.622269807, 2060060.622269807, 390219.3316231772], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2370000.0000, 
sim time next is 2370600.0000, 
raw observation next is [31.75, 65.0, 1.0, 2.0, 0.7432655675486565, 1.0, 2.0, 0.7432655675486565, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2078526.230306084, 2078526.230306084, 393198.6645118048], 
processed observation next is [1.0, 0.43478260869565216, 0.7037914691943128, 0.65, 1.0, 1.0, 0.6906814066851282, 1.0, 1.0, 0.6906814066851282, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5773683973072455, 0.5773683973072455, 0.5868636783758281], 
reward next is 0.4131, 
noisyNet noise sample is [array([1.1763558], dtype=float32), 1.2917702]. 
=============================================
[2019-03-27 12:55:47,727] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4747958e-17 1.0000000e+00 9.2798274e-25 1.0286573e-17 2.6470408e-22], sum to 1.0000
[2019-03-27 12:55:47,739] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1885
[2019-03-27 12:55:47,750] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1945297.335444182 W.
[2019-03-27 12:55:47,757] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.6, 66.66666666666667, 1.0, 2.0, 0.7501418672139923, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.004394790718656, 6.9112, 168.9124024865316, 1945297.335444182, 1879181.960246207, 395948.2630486401], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2207400.0000, 
sim time next is 2208000.0000, 
raw observation next is [31.7, 66.33333333333334, 1.0, 2.0, 0.566519127547339, 1.0, 1.0, 0.566519127547339, 1.0, 2.0, 0.9838563738113608, 6.9112, 6.9112, 170.5573041426782, 2376692.365895032, 2376692.365895032, 464192.6966209219], 
processed observation next is [1.0, 0.5652173913043478, 0.7014218009478673, 0.6633333333333334, 1.0, 1.0, 0.4777338886112518, 1.0, 0.5, 0.4777338886112518, 1.0, 1.0, 0.9803126509894644, 0.0, 0.0, 0.8375144448122397, 0.6601923238597311, 0.6601923238597311, 0.6928249203297342], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.32332847], dtype=float32), 0.2818686]. 
=============================================
[2019-03-27 12:55:47,776] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[49.600998]
 [49.123756]
 [50.00765 ]
 [50.255013]
 [50.107445]], R is [[47.35614777]
 [46.88258743]
 [46.41376114]
 [45.94962311]
 [45.49012756]].
[2019-03-27 12:55:48,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8656012e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2082868e-38], sum to 1.0000
[2019-03-27 12:55:48,284] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6418
[2019-03-27 12:55:48,288] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.81666666666667, 68.33333333333333, 1.0, 2.0, 0.5728583999402388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 800518.9729391882, 800518.9729391889, 195799.3403210805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2400600.0000, 
sim time next is 2401200.0000, 
raw observation next is [31.7, 69.0, 1.0, 2.0, 0.5759440905378115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804832.5864285645, 804832.5864285645, 196350.6354862268], 
processed observation next is [1.0, 0.8260869565217391, 0.7014218009478673, 0.69, 1.0, 1.0, 0.4890892657082066, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22356460734126793, 0.22356460734126793, 0.293060649979443], 
reward next is 0.7069, 
noisyNet noise sample is [array([0.77822405], dtype=float32), -1.1194603]. 
=============================================
[2019-03-27 12:55:54,978] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5259165e-24 1.0000000e+00 4.9344576e-34 2.2192426e-28 1.8529718e-30], sum to 1.0000
[2019-03-27 12:55:54,991] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7915
[2019-03-27 12:55:54,999] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.16666666666667, 65.33333333333334, 1.0, 2.0, 0.5351307402648946, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564312589, 747779.34598004, 747779.3459800393, 189286.575589165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2308800.0000, 
sim time next is 2309400.0000, 
raw observation next is [32.05, 66.0, 1.0, 2.0, 0.5282426375899637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104112, 738150.7259217168, 738150.7259217168, 188143.1625668645], 
processed observation next is [1.0, 0.7391304347826086, 0.7180094786729857, 0.66, 1.0, 1.0, 0.43161763565055866, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451522054, 0.205041868311588, 0.205041868311588, 0.28081069039830525], 
reward next is 0.7192, 
noisyNet noise sample is [array([-1.0877339], dtype=float32), 0.90019304]. 
=============================================
[2019-03-27 12:56:01,142] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.398962e-32 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 12:56:01,153] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3662
[2019-03-27 12:56:01,161] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.03333333333333, 73.16666666666667, 1.0, 2.0, 0.5830151288377925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814717.5470553037, 814717.547055303, 197624.7943381713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2404200.0000, 
sim time next is 2404800.0000, 
raw observation next is [30.9, 74.0, 1.0, 2.0, 0.5853945592538875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818043.8933791202, 818043.8933791202, 198056.8348316016], 
processed observation next is [1.0, 0.8695652173913043, 0.6635071090047393, 0.74, 1.0, 1.0, 0.5004753725950452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22723441482753337, 0.22723441482753337, 0.29560721616656954], 
reward next is 0.7044, 
noisyNet noise sample is [array([-0.731752], dtype=float32), 0.92921436]. 
=============================================
[2019-03-27 12:56:14,612] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4582385e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 12:56:14,624] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3051
[2019-03-27 12:56:14,627] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4852130322956492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678003.2320904683, 678003.2320904683, 181311.4334101371], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2646000.0000, 
sim time next is 2646600.0000, 
raw observation next is [26.83333333333333, 80.66666666666667, 1.0, 2.0, 0.485298489297006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678122.6819250647, 678122.6819250641, 181324.6432164886], 
processed observation next is [0.0, 0.6521739130434783, 0.470774091627172, 0.8066666666666668, 1.0, 1.0, 0.37987769794820003, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1883674116458513, 0.18836741164585113, 0.27063379584550534], 
reward next is 0.7294, 
noisyNet noise sample is [array([0.24113221], dtype=float32), 1.1788723]. 
=============================================
[2019-03-27 12:56:27,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3755856e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 12:56:27,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2866
[2019-03-27 12:56:27,927] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4098329272822472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603852.4830929274, 603852.4830929274, 174672.3354776844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2836200.0000, 
sim time next is 2836800.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4113506259202543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606088.3237429457, 606088.3237429451, 174881.5591191078], 
processed observation next is [1.0, 0.8695652173913043, 0.3364928909952607, 0.89, 1.0, 1.0, 0.29078388665090876, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1683578677063738, 0.16835786770637365, 0.2610172524165788], 
reward next is 0.7390, 
noisyNet noise sample is [array([0.7159157], dtype=float32), -1.253099]. 
=============================================
[2019-03-27 12:56:28,793] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.224793e-31 1.000000e+00 0.000000e+00 0.000000e+00 8.097975e-37], sum to 1.0000
[2019-03-27 12:56:28,803] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9981
[2019-03-27 12:56:28,807] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 91.0, 1.0, 2.0, 0.5540920708112779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861238.8819793045, 861238.8819793045, 202808.7056951553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2799000.0000, 
sim time next is 2799600.0000, 
raw observation next is [22.0, 90.0, 1.0, 2.0, 0.5349092040805988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834201.3221805632, 834201.3221805632, 199447.1048913579], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.9, 1.0, 1.0, 0.439649643470601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2317225894946009, 0.2317225894946009, 0.29768224610650434], 
reward next is 0.7023, 
noisyNet noise sample is [array([-1.0170664], dtype=float32), 0.14459564]. 
=============================================
[2019-03-27 12:56:30,274] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 12:56:30,274] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:56:30,274] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:56:30,275] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:56:30,276] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:56:30,277] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:56:30,277] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:56:30,279] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:56:30,282] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:56:30,283] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:56:30,287] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:56:30,315] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run38
[2019-03-27 12:56:30,335] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run38
[2019-03-27 12:56:30,337] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run38
[2019-03-27 12:56:30,356] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run38
[2019-03-27 12:56:30,357] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run38
[2019-03-27 12:56:34,907] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:56:34,909] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.463964885, 79.788199445, 1.0, 2.0, 0.3481527695119053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539739.9822542766, 539739.9822542772, 169891.3028233762]
[2019-03-27 12:56:34,910] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:56:34,913] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.895588e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.8237915984518944
[2019-03-27 12:56:40,310] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:56:40,316] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.802803925, 64.87100893666667, 1.0, 2.0, 0.3144708315436642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 500031.2845173705, 500031.2845173712, 167073.4608712133]
[2019-03-27 12:56:40,318] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:56:40,321] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.7969567e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.5737401433137505
[2019-03-27 12:56:42,997] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:56:42,998] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.2, 60.66666666666667, 1.0, 2.0, 0.4803175319889053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671160.4365752094, 671160.4365752094, 180569.6297423654]
[2019-03-27 12:56:42,999] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:56:43,003] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.132924e-32 1.000000e+00 0.000000e+00 0.000000e+00 9.485526e-38], sampled 0.21872508601820584
[2019-03-27 12:57:00,921] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:57:00,923] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.10446915, 100.0, 1.0, 2.0, 0.3814422665463806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575405.4480682525, 575405.4480682525, 172492.2253609912]
[2019-03-27 12:57:00,926] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:57:00,929] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.7647372e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9763418050164441
[2019-03-27 12:57:08,521] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:57:08,522] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.63333333333333, 80.0, 1.0, 2.0, 0.7301873107500682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1020477.840927375, 1020477.840927375, 227494.3686723046]
[2019-03-27 12:57:08,524] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:57:08,528] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.2129806e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1351129e-37], sampled 0.5660016204036357
[2019-03-27 12:57:15,331] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:57:15,332] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.6, 56.33333333333334, 1.0, 2.0, 0.5597269878539205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 782162.2291130882, 782162.2291130876, 193483.1623042578]
[2019-03-27 12:57:15,333] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:57:15,337] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.546225e-34 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.6746102885540634
[2019-03-27 12:57:27,178] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:57:27,179] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.794899855, 85.01733540999999, 1.0, 2.0, 0.6074343329091643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 848855.0899253059, 848855.0899253059, 202139.8985251857]
[2019-03-27 12:57:27,179] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 12:57:27,182] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1322140e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2209905e-38], sampled 0.9807712291625473
[2019-03-27 12:57:59,080] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:57:59,083] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.25, 52.5, 1.0, 2.0, 0.6113007485655533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104262, 854260.3597631737, 854260.3597631737, 202870.7588173557]
[2019-03-27 12:57:59,085] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 12:57:59,088] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.4816612e-29 1.0000000e+00 0.0000000e+00 8.7334661e-33 6.6010161e-35], sampled 0.5475345634737959
[2019-03-27 12:58:03,325] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:58:03,326] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.32146926666667, 69.03787986666667, 1.0, 2.0, 0.4864215385189593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684276.6234197166, 684276.6234197172, 182085.7884168462]
[2019-03-27 12:58:03,327] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 12:58:03,330] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.571448e-34 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.1097869589744549
[2019-03-27 12:58:13,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:58:13,023] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.78333333333333, 78.5, 1.0, 2.0, 0.5493502730903488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767656.5729951195, 767656.5729951201, 191688.9477361134]
[2019-03-27 12:58:13,025] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:58:13,026] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6595784e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6551255965506035
[2019-03-27 12:58:15,131] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0633961], dtype=float32), 0.05169283]
[2019-03-27 12:58:15,131] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.91666666666667, 86.5, 1.0, 2.0, 0.7749149412495336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1083019.096375648, 1083019.096375648, 237848.1120349048]
[2019-03-27 12:58:15,132] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 12:58:15,137] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.5997344e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1240544e-37], sampled 0.9259762503499949
[2019-03-27 12:58:37,985] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 12:58:38,074] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8604 2842435242.9853 1131.0000
[2019-03-27 12:58:38,639] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.7507 3164307242.1410 1777.0000
[2019-03-27 12:58:38,731] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6796 2779156607.2113 933.0000
[2019-03-27 12:58:38,778] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4791 3007658023.9129 1766.0000
[2019-03-27 12:58:39,794] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 925000, evaluation results [925000.0, 7882.750654250539, 3164307242.141041, 1777.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8660.679636567633, 2779156607.211253, 933.0, 7997.4790539186915, 3007658023.9129057, 1766.0, 8496.860364057107, 2842435242.9852624, 1131.0]
[2019-03-27 12:58:45,081] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0759179e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7054704e-36], sum to 1.0000
[2019-03-27 12:58:45,092] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6491
[2019-03-27 12:58:45,096] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 90.66666666666667, 1.0, 2.0, 0.621225282094197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 952801.9272920782, 952801.9272920782, 215142.7409349323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2889600.0000, 
sim time next is 2890200.0000, 
raw observation next is [22.63333333333333, 90.33333333333333, 1.0, 2.0, 0.6346173765704584, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 972623.6768112426, 972623.6768112426, 217936.2613286352], 
processed observation next is [1.0, 0.43478260869565216, 0.27172195892575024, 0.9033333333333333, 1.0, 1.0, 0.5597799717716365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2701732435586785, 0.2701732435586785, 0.32527800198303763], 
reward next is 0.6747, 
noisyNet noise sample is [array([-2.002188], dtype=float32), 0.48321223]. 
=============================================
[2019-03-27 12:58:47,598] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.4815525e-30 1.0000000e+00 0.0000000e+00 6.7696964e-38 3.8151610e-35], sum to 1.0000
[2019-03-27 12:58:47,608] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6097
[2019-03-27 12:58:47,613] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.8133363731871163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1213371.377180006, 1213371.377180006, 257710.3713101193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3064200.0000, 
sim time next is 3064800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.777874993083416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1160367.133498401, 1160367.1334984, 248408.0798749535], 
processed observation next is [1.0, 0.4782608695652174, 0.28909952606635075, 0.94, 1.0, 1.0, 0.7323795097390554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3223242037495558, 0.32232420374955556, 0.3707583281715724], 
reward next is 0.6292, 
noisyNet noise sample is [array([0.04078], dtype=float32), 1.804339]. 
=============================================
[2019-03-27 12:58:49,038] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.8011647e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5996763e-37], sum to 1.0000
[2019-03-27 12:58:49,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7348
[2019-03-27 12:58:49,057] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 96.0, 1.0, 2.0, 0.7583974758994021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1123210.192958733, 1123210.192958733, 242446.031017916], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3072000.0000, 
sim time next is 3072600.0000, 
raw observation next is [23.0, 97.0, 1.0, 2.0, 0.8339151105066627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1229316.07873723, 1229316.078737229, 261326.0559229381], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.97, 1.0, 1.0, 0.7998977235020032, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34147668853811947, 0.3414766885381192, 0.39003888943722104], 
reward next is 0.6100, 
noisyNet noise sample is [array([1.7344643], dtype=float32), -0.14446245]. 
=============================================
[2019-03-27 12:58:51,705] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1618267e-31 1.0000000e+00 0.0000000e+00 6.0978133e-37 4.6149166e-36], sum to 1.0000
[2019-03-27 12:58:51,721] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3561
[2019-03-27 12:58:51,729] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5609949487648926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 886911.2213742542, 886911.2213742542, 205547.0908245246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2989800.0000, 
sim time next is 2990400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5570100737043119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 880588.1501533274, 880588.1501533274, 204759.978187945], 
processed observation next is [1.0, 0.6086956521739131, 0.19431279620853087, 0.94, 1.0, 1.0, 0.46627719723411065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2446078194870354, 0.2446078194870354, 0.3056119077432015], 
reward next is 0.6944, 
noisyNet noise sample is [array([0.76083744], dtype=float32), -1.1394608]. 
=============================================
[2019-03-27 12:58:58,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1647999e-29 1.0000000e+00 6.8442718e-38 4.7805721e-37 2.2993726e-33], sum to 1.0000
[2019-03-27 12:58:58,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1400
[2019-03-27 12:58:58,973] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 89.0, 1.0, 2.0, 0.6572403156099959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1001733.16975557, 1001733.16975557, 222295.8885578048], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3142800.0000, 
sim time next is 3143400.0000, 
raw observation next is [23.16666666666667, 89.0, 1.0, 2.0, 0.6362739479298782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 965837.7640789308, 965837.7640789314, 217252.5311161857], 
processed observation next is [1.0, 0.391304347826087, 0.2969984202211693, 0.89, 1.0, 1.0, 0.5617758408793714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.268288267799703, 0.26828826779970316, 0.3242575091286354], 
reward next is 0.6757, 
noisyNet noise sample is [array([-0.09507333], dtype=float32), -0.2742188]. 
=============================================
[2019-03-27 12:59:00,954] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9412064e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1208157e-37], sum to 1.0000
[2019-03-27 12:59:00,967] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9730
[2019-03-27 12:59:00,973] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3843210724165375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578860.9898432888, 578860.9898432888, 172773.2748123322], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3105600.0000, 
sim time next is 3106200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3849212062235661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 579764.8643923954, 579764.8643923948, 172854.1836680601], 
processed observation next is [1.0, 0.9565217391304348, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2589412123175496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16104579566455426, 0.1610457956645541, 0.2579913189075524], 
reward next is 0.7420, 
noisyNet noise sample is [array([1.3601534], dtype=float32), 0.7389172]. 
=============================================
[2019-03-27 12:59:02,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4580286e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4551852e-37], sum to 1.0000
[2019-03-27 12:59:02,359] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9510
[2019-03-27 12:59:02,365] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5745439460520508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802875.2648342384, 802875.2648342384, 196099.9543997516], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3326400.0000, 
sim time next is 3327000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5880381520989124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821739.5401817085, 821739.5401817085, 198538.2724028351], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.5036604242155571, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22826098338380793, 0.22826098338380793, 0.29632577970572405], 
reward next is 0.7037, 
noisyNet noise sample is [array([0.4872755], dtype=float32), -1.0635691]. 
=============================================
[2019-03-27 12:59:02,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.31719 ]
 [75.19749 ]
 [75.1709  ]
 [75.14314 ]
 [75.119675]], R is [[75.24310303]
 [75.19798279]
 [75.15378571]
 [75.11065674]
 [75.06865692]].
[2019-03-27 12:59:03,998] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7744482e-16 1.0000000e+00 5.0676387e-23 3.0444462e-12 5.2335568e-21], sum to 1.0000
[2019-03-27 12:59:04,007] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7209
[2019-03-27 12:59:04,021] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2719181.54638753 W.
[2019-03-27 12:59:04,028] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.33333333333334, 62.0, 1.0, 2.0, 0.6549552404471332, 1.0, 2.0, 0.6480676597378291, 1.0, 1.0, 1.03, 7.005094180687435, 6.9112, 170.5573041426782, 2719181.54638753, 2651921.325390046, 507478.2753174159], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3426000.0000, 
sim time next is 3426600.0000, 
raw observation next is [33.16666666666666, 62.5, 1.0, 2.0, 0.9521678114625968, 1.0, 2.0, 0.9521678114625968, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2663363.232392109, 2663363.23239211, 500755.0214301713], 
processed observation next is [1.0, 0.6521739130434783, 0.7709320695102682, 0.625, 1.0, 1.0, 0.9423708571838515, 1.0, 1.0, 0.9423708571838515, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7398231201089192, 0.7398231201089194, 0.74739555437339], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6398686], dtype=float32), 0.90786004]. 
=============================================
[2019-03-27 12:59:07,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6052016e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 12:59:07,643] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9972
[2019-03-27 12:59:07,648] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 83.66666666666667, 1.0, 2.0, 0.4506962289047836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 642078.7491417435, 642078.7491417429, 177819.6168346419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3307200.0000, 
sim time next is 3307800.0000, 
raw observation next is [25.83333333333334, 83.83333333333333, 1.0, 2.0, 0.4595058990097754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 649386.2177416584, 649386.217741659, 178439.2593167208], 
processed observation next is [0.0, 0.2608695652173913, 0.42338072669826254, 0.8383333333333333, 1.0, 1.0, 0.34880228796358487, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18038506048379402, 0.18038506048379419, 0.2663272527115236], 
reward next is 0.7337, 
noisyNet noise sample is [array([0.88162637], dtype=float32), 0.6588312]. 
=============================================
[2019-03-27 12:59:12,517] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.1549159e-16 1.0000000e+00 2.1592054e-24 2.0585857e-15 6.4299237e-21], sum to 1.0000
[2019-03-27 12:59:12,533] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6919
[2019-03-27 12:59:12,541] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2206302.580758035 W.
[2019-03-27 12:59:12,546] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.525940290297026, 1.0, 2.0, 0.525940290297026, 1.0, 2.0, 0.9133843531341098, 6.9112, 6.9112, 170.5573041426782, 2206302.580758035, 2206302.580758035, 433549.3623111778], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3415800.0000, 
sim time next is 3416400.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.4894824110666747, 1.0, 2.0, 0.4894824110666747, 1.0, 2.0, 0.8500690737158899, 6.9112, 6.9112, 170.5573041426782, 2053216.376333814, 2053216.376333814, 407950.4527008252], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.67, 1.0, 1.0, 0.38491856755021053, 1.0, 1.0, 0.38491856755021053, 1.0, 1.0, 0.8171574069705974, 0.0, 0.0, 0.8375144448122397, 0.5703378823149484, 0.5703378823149484, 0.6088812726877988], 
reward next is 0.3911, 
noisyNet noise sample is [array([1.0527035], dtype=float32), -0.09386618]. 
=============================================
[2019-03-27 12:59:17,990] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.737974e-32 1.000000e+00 0.000000e+00 0.000000e+00 2.665462e-37], sum to 1.0000
[2019-03-27 12:59:17,999] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7187
[2019-03-27 12:59:18,009] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5489947093149598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 767159.5323086148, 767159.5323086142, 191628.2826581257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3355200.0000, 
sim time next is 3355800.0000, 
raw observation next is [28.0, 83.16666666666667, 1.0, 2.0, 0.5461071983525783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763123.1049005303, 763123.1049005303, 191135.0438016164], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8316666666666667, 1.0, 1.0, 0.4531412028344316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21197864025014732, 0.21197864025014732, 0.28527618477853195], 
reward next is 0.7147, 
noisyNet noise sample is [array([-1.019192], dtype=float32), 0.25997156]. 
=============================================
[2019-03-27 12:59:21,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6256926e-25 1.0000000e+00 3.6808119e-33 9.4749223e-32 7.6411623e-31], sum to 1.0000
[2019-03-27 12:59:21,543] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3002
[2019-03-27 12:59:21,551] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 87.33333333333334, 1.0, 2.0, 0.9440242841929092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1319512.732405236, 1319512.732405236, 282332.0423234189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3463800.0000, 
sim time next is 3464400.0000, 
raw observation next is [26.33333333333334, 85.66666666666667, 1.0, 2.0, 0.8807175721614448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1230974.290583841, 1230974.290583841, 264685.8386431032], 
processed observation next is [1.0, 0.08695652173913043, 0.44707740916271754, 0.8566666666666667, 1.0, 1.0, 0.856286231519813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34193730293995583, 0.34193730293995583, 0.3950534905120943], 
reward next is 0.6049, 
noisyNet noise sample is [array([-1.7500595], dtype=float32), 0.21029647]. 
=============================================
[2019-03-27 12:59:25,687] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.4595472e-13 1.0000000e+00 5.4564642e-21 2.5471396e-11 4.3923316e-19], sum to 1.0000
[2019-03-27 12:59:25,699] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8693
[2019-03-27 12:59:25,715] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2470964.589337432 W.
[2019-03-27 12:59:25,722] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.0, 59.66666666666667, 1.0, 2.0, 0.8834521276099749, 1.0, 2.0, 0.8834521276099749, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2470964.589337432, 2470964.589337432, 462544.1071905409], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3597000.0000, 
sim time next is 3597600.0000, 
raw observation next is [33.0, 60.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.538588128670135, 6.9112, 168.903699247931, 3439386.300529531, 2284924.425342406, 471955.2725638509], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.6033333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.16273881286701347, 0.0, 0.8293944877554346, 0.9553850834804253, 0.6347012292617795, 0.7044108545729119], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.837399], dtype=float32), 1.5565678]. 
=============================================
[2019-03-27 12:59:39,960] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7420288e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.4409879e-36], sum to 1.0000
[2019-03-27 12:59:39,974] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8276
[2019-03-27 12:59:39,979] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.834292388302565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1192657.540661651, 1192657.540661651, 256294.6185008968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3723000.0000, 
sim time next is 3723600.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.7511585591953072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1073596.744355148, 1073596.744355148, 235529.3833489165], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.74, 1.0, 1.0, 0.7001910351750689, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29822131787642997, 0.29822131787642997, 0.3515363930580844], 
reward next is 0.6485, 
noisyNet noise sample is [array([2.1759067], dtype=float32), -0.3907072]. 
=============================================
[2019-03-27 12:59:45,804] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 12:59:45,806] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 12:59:45,807] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 12:59:45,808] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 12:59:45,808] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 12:59:45,809] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:59:45,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:59:45,810] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:59:45,810] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:59:45,810] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 12:59:45,817] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 12:59:45,843] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run39
[2019-03-27 12:59:45,843] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run39
[2019-03-27 12:59:45,877] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run39
[2019-03-27 12:59:45,878] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run39
[2019-03-27 12:59:45,911] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run39
[2019-03-27 12:59:59,011] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06176079], dtype=float32), 0.048884228]
[2019-03-27 12:59:59,012] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.33333333333333, 82.50000000000001, 1.0, 2.0, 0.2573163386722432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 422445.1565407084, 422445.1565407078, 161640.7203193426]
[2019-03-27 12:59:59,013] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 12:59:59,017] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2701135e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.5953178205421054
[2019-03-27 13:00:19,942] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06176079], dtype=float32), 0.048884228]
[2019-03-27 13:00:19,943] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.390483685, 90.01024492666667, 1.0, 2.0, 0.4589454467868742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 662013.6219999206, 662013.6219999212, 180060.2359170092]
[2019-03-27 13:00:19,943] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:00:19,946] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4120010e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.2344495e-38], sampled 0.1682320689664918
[2019-03-27 13:00:28,626] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06176079], dtype=float32), 0.048884228]
[2019-03-27 13:00:28,628] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.42186805, 91.89672858, 1.0, 2.0, 0.5163882575663118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721580.137610436, 721580.1376104353, 186204.6008338905]
[2019-03-27 13:00:28,628] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:00:28,630] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.6558344e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.8368252098223518
[2019-03-27 13:01:03,114] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06176079], dtype=float32), 0.048884228]
[2019-03-27 13:01:03,117] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.250726393222018, 6.9112, 168.9109536598318, 2535846.027779817, 2294977.174918198, 475908.5720292011]
[2019-03-27 13:01:03,117] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:01:03,119] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3038567e-20 1.0000000e+00 1.1891626e-28 3.8364795e-22 3.4194882e-25], sampled 0.17892187031242768
[2019-03-27 13:01:03,121] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2535846.027779817 W.
[2019-03-27 13:01:23,603] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06176079], dtype=float32), 0.048884228]
[2019-03-27 13:01:23,606] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.4, 91.0, 1.0, 2.0, 0.5240565623480476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732299.2111800533, 732299.2111800539, 187451.1534132516]
[2019-03-27 13:01:23,606] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:01:23,613] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7779370e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7845285e-38], sampled 0.9193957103884234
[2019-03-27 13:01:53,678] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 13:01:53,836] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-03-27 13:01:54,397] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.3799 2842505467.1803 1131.0000
[2019-03-27 13:01:54,416] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 13:01:54,535] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-27 13:01:55,555] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 950000, evaluation results [950000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8495.379862364505, 2842505467.180306, 1131.0]
[2019-03-27 13:02:03,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7444515e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 13:02:03,263] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0240
[2019-03-27 13:02:03,269] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 81.5, 1.0, 2.0, 0.5868360361909778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820059.02419977, 820059.02419977, 198319.0129669711], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3915000.0000, 
sim time next is 3915600.0000, 
raw observation next is [29.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5873009691091451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820708.9838378612, 820708.9838378606, 198403.9046008279], 
processed observation next is [0.0, 0.30434782608695654, 0.6050552922590839, 0.8066666666666668, 1.0, 1.0, 0.502772251938729, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22797471773273922, 0.22797471773273906, 0.2961252307475043], 
reward next is 0.7039, 
noisyNet noise sample is [array([-0.22732553], dtype=float32), 1.5562414]. 
=============================================
[2019-03-27 13:02:09,304] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.337809e-33 1.000000e+00 0.000000e+00 0.000000e+00 5.437156e-38], sum to 1.0000
[2019-03-27 13:02:09,320] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9727
[2019-03-27 13:02:09,324] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.6126502051477495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 856146.9137832858, 856146.9137832851, 203126.3088945808], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3960000.0000, 
sim time next is 3960600.0000, 
raw observation next is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.638353093640639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 892080.4529525923, 892080.4529525923, 208107.0619841013], 
processed observation next is [0.0, 0.8695652173913043, 0.7077409162717223, 0.7166666666666667, 1.0, 1.0, 0.5642808357116132, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24780012582016453, 0.24780012582016453, 0.3106075552001512], 
reward next is 0.6894, 
noisyNet noise sample is [array([-1.2479577], dtype=float32), 2.4670892]. 
=============================================
[2019-03-27 13:02:10,917] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9487056e-08 3.6201831e-02 3.2140428e-15 9.6379817e-01 3.7269366e-13], sum to 1.0000
[2019-03-27 13:02:10,932] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9252
[2019-03-27 13:02:10,943] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5898837951365045, 1.0, 2.0, 0.5898837951365045, 1.0, 2.0, 1.02337900248667, 6.911200000000001, 6.9112, 170.5573041426782, 2474810.176058854, 2474810.176058854, 482636.8531100813], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4014000.0000, 
sim time next is 4014600.0000, 
raw observation next is [31.33333333333334, 65.5, 1.0, 2.0, 0.8312467763610768, 1.0, 2.0, 0.8312467763610768, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2324813.411441953, 2324813.411441953, 435368.5352022914], 
processed observation next is [1.0, 0.4782608695652174, 0.6840442338072673, 0.655, 1.0, 1.0, 0.7966828630856347, 1.0, 1.0, 0.7966828630856347, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6457815031783203, 0.6457815031783203, 0.649803783884017], 
reward next is 0.3502, 
noisyNet noise sample is [array([1.2265719], dtype=float32), 1.7284249]. 
=============================================
[2019-03-27 13:02:35,027] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7660193e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2735256e-36], sum to 1.0000
[2019-03-27 13:02:35,037] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4432
[2019-03-27 13:02:35,044] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6196018390965616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 865865.4218062824, 865865.421806283, 204454.7346157438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4324800.0000, 
sim time next is 4325400.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6186151948370953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864486.0701509388, 864486.0701509388, 204265.4046309429], 
processed observation next is [1.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5405002347434883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2401350194863719, 0.2401350194863719, 0.3048737382551387], 
reward next is 0.6951, 
noisyNet noise sample is [array([1.129775], dtype=float32), -0.07760372]. 
=============================================
[2019-03-27 13:02:37,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9011920e-24 1.0000000e+00 1.0131771e-32 3.6761729e-30 1.1335047e-27], sum to 1.0000
[2019-03-27 13:02:37,105] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9590
[2019-03-27 13:02:37,121] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1912413.103739659 W.
[2019-03-27 13:02:37,132] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.16666666666667, 88.16666666666667, 1.0, 2.0, 0.7266434689079914, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005977434116108, 6.9112, 168.9114489040951, 1912413.103739659, 1845175.330115809, 390537.3355099696], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4333800.0000, 
sim time next is 4334400.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.3551316507197109, 1.0, 1.0, 0.3551316507197109, 1.0, 2.0, 0.6167462334685978, 6.9112, 6.9112, 170.5573041426782, 1489267.869143901, 1489267.869143901, 329443.1604558317], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.89, 1.0, 1.0, 0.22305018159001316, 1.0, 0.5, 0.22305018159001316, 1.0, 1.0, 0.5326173578885339, 0.0, 0.0, 0.8375144448122397, 0.41368551920663915, 0.41368551920663915, 0.49170620963556966], 
reward next is 0.5083, 
noisyNet noise sample is [array([0.3955788], dtype=float32), 0.93514115]. 
=============================================
[2019-03-27 13:02:37,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1970328e-26 1.0000000e+00 1.2547283e-34 1.2520740e-35 1.0133559e-30], sum to 1.0000
[2019-03-27 13:02:37,967] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5311
[2019-03-27 13:02:37,976] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2602131.334153201 W.
[2019-03-27 13:02:37,979] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.16666666666666, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.528954585760246, 6.9112, 168.9040528137483, 2602131.334153201, 1454501.048982887, 310414.9103111032], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4345800.0000, 
sim time next is 4346400.0000, 
raw observation next is [30.33333333333334, 84.0, 1.0, 2.0, 0.5288076550731582, 1.0, 1.0, 0.5288076550731582, 1.0, 1.0, 0.9183640174982296, 6.911200000000001, 6.9112, 170.5573041426782, 2218342.695023546, 2218342.695023545, 435640.7172057435], 
processed observation next is [1.0, 0.30434782608695654, 0.6366508688783573, 0.84, 1.0, 1.0, 0.43229837960621464, 1.0, 0.5, 0.43229837960621464, 1.0, 0.5, 0.9004439237783286, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6162063041732072, 0.616206304173207, 0.6502100256802141], 
reward next is 0.3498, 
noisyNet noise sample is [array([0.63176876], dtype=float32), -0.12544832]. 
=============================================
[2019-03-27 13:02:47,916] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7464144e-13 1.0000000e+00 2.1407473e-21 8.6278268e-12 5.9211375e-18], sum to 1.0000
[2019-03-27 13:02:47,927] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6684
[2019-03-27 13:02:47,945] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2786497.768166468 W.
[2019-03-27 13:02:47,948] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.33333333333334, 69.66666666666667, 1.0, 2.0, 0.9961400833109855, 1.0, 2.0, 0.9961400833109855, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2786497.768166468, 2786497.768166468, 526662.6803720468], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4623000.0000, 
sim time next is 4623600.0000, 
raw observation next is [32.66666666666667, 68.33333333333334, 1.0, 2.0, 0.7875462267309841, 1.0, 2.0, 0.7875462267309841, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2202483.45047471, 2202483.45047471, 413865.7263402934], 
processed observation next is [1.0, 0.5217391304347826, 0.7472353870458138, 0.6833333333333335, 1.0, 1.0, 0.7440315984710651, 1.0, 1.0, 0.7440315984710651, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6118009584651973, 0.6118009584651973, 0.6177100393138708], 
reward next is 0.3823, 
noisyNet noise sample is [array([0.01483657], dtype=float32), -1.0969168]. 
=============================================
[2019-03-27 13:02:54,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2086069e-30 1.0000000e+00 3.8168293e-38 0.0000000e+00 2.7813495e-33], sum to 1.0000
[2019-03-27 13:02:54,259] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8193
[2019-03-27 13:02:54,265] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7650871540744784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1069276.89129477, 1069276.89129477, 235521.5928027905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4680000.0000, 
sim time next is 4680600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.835497732681488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1167736.004468091, 1167736.004468092, 252810.7880729979], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.801804497206612, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3243711123522475, 0.3243711123522478, 0.3773295344373103], 
reward next is 0.6227, 
noisyNet noise sample is [array([0.3786654], dtype=float32), 0.45231292]. 
=============================================
[2019-03-27 13:02:57,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6392712e-29 1.0000000e+00 1.8260967e-37 0.0000000e+00 8.0126897e-33], sum to 1.0000
[2019-03-27 13:02:57,861] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3393
[2019-03-27 13:02:57,865] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7526746236852463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051920.691221194, 1051920.691221194, 232622.368919132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4762800.0000, 
sim time next is 4763400.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.703777206773278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 983551.1428812349, 983551.1428812349, 221659.2422921011], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.6431050684015398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.273208650800343, 0.273208650800343, 0.3308346899882106], 
reward next is 0.6692, 
noisyNet noise sample is [array([1.8269519], dtype=float32), -1.0199165]. 
=============================================
[2019-03-27 13:02:57,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2519128e-19 1.0000000e+00 1.7438132e-26 6.4236229e-21 6.4029959e-22], sum to 1.0000
[2019-03-27 13:02:57,903] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2586
[2019-03-27 13:02:57,914] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2493988.193927696 W.
[2019-03-27 13:02:57,922] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [34.0, 63.0, 1.0, 2.0, 0.891675626187954, 1.0, 2.0, 0.891675626187954, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2493988.193927696, 2493988.193927696, 466981.0907478728], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4626000.0000, 
sim time next is 4626600.0000, 
raw observation next is [34.16666666666667, 62.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.175191385381599, 6.9112, 168.9058192394971, 3181186.361042983, 2284505.25911289, 473063.2080612063], 
processed observation next is [1.0, 0.5652173913043478, 0.8183254344391787, 0.625, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.12639913853815984, 0.0, 0.8294048978845605, 0.8836628780674952, 0.6345847941980249, 0.7060644896435915], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4278774], dtype=float32), 1.0427755]. 
=============================================
[2019-03-27 13:02:59,882] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5576183e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 13:02:59,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5236
[2019-03-27 13:02:59,901] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 82.0, 1.0, 2.0, 0.4678879828999695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661753.5244685997, 661753.5244685997, 179743.7983843949], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4653000.0000, 
sim time next is 4653600.0000, 
raw observation next is [25.33333333333334, 86.0, 1.0, 2.0, 0.462441118112689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656257.5409933337, 656257.5409933343, 179218.7617788287], 
processed observation next is [1.0, 0.8695652173913043, 0.3996840442338076, 0.86, 1.0, 1.0, 0.3523386965213121, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18229376138703712, 0.1822937613870373, 0.26749068922213237], 
reward next is 0.7325, 
noisyNet noise sample is [array([-1.2014729], dtype=float32), 0.95270866]. 
=============================================
[2019-03-27 13:03:01,658] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 13:03:01,660] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:03:01,661] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:03:01,661] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:03:01,662] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:03:01,664] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:03:01,668] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:03:01,668] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:03:01,670] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:03:01,669] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:03:01,671] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:03:01,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run40
[2019-03-27 13:03:01,722] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run40
[2019-03-27 13:03:01,723] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run40
[2019-03-27 13:03:01,760] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run40
[2019-03-27 13:03:01,776] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run40
[2019-03-27 13:03:22,146] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06150733], dtype=float32), 0.049438022]
[2019-03-27 13:03:22,147] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.46666666666667, 79.5, 1.0, 2.0, 0.2570464147997039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 423302.5578497166, 423302.5578497159, 161598.4690394585]
[2019-03-27 13:03:22,148] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:03:22,149] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.553441e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.697306979947866
[2019-03-27 13:03:29,001] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06150733], dtype=float32), 0.049438022]
[2019-03-27 13:03:29,001] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.73333333333333, 93.0, 1.0, 2.0, 0.6587727728078611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 960954.0477599311, 960954.0477599311, 217440.9243154122]
[2019-03-27 13:03:29,001] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:03:29,003] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1825275e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.1746721e-36], sampled 0.6860375610957044
[2019-03-27 13:03:42,675] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06150733], dtype=float32), 0.049438022]
[2019-03-27 13:03:42,677] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.719762005, 97.67157659, 1.0, 2.0, 0.7998790611722738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1117927.251016982, 1117927.251016983, 243889.1908926392]
[2019-03-27 13:03:42,679] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:03:42,682] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.9568835e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.4235535e-36], sampled 0.8563433898428342
[2019-03-27 13:03:49,610] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06150733], dtype=float32), 0.049438022]
[2019-03-27 13:03:49,615] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.99846298333333, 88.3523088, 1.0, 2.0, 0.256345666127895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418965.0694390898, 418965.0694390898, 161520.5089503075]
[2019-03-27 13:03:49,615] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:03:49,619] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2533875e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6755330630487362
[2019-03-27 13:03:49,670] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06150733], dtype=float32), 0.049438022]
[2019-03-27 13:03:49,670] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.7, 62.0, 1.0, 2.0, 0.7430082478632555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1038404.58219626, 1038404.58219626, 230400.4928567453]
[2019-03-27 13:03:49,671] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:03:49,674] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.8665705e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9453850e-35], sampled 0.5127933335176026
[2019-03-27 13:04:20,261] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06150733], dtype=float32), 0.049438022]
[2019-03-27 13:04:20,262] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.43333333333334, 68.33333333333333, 1.0, 2.0, 0.9387572525526051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1312146.173836022, 1312146.173836022, 280829.0786414462]
[2019-03-27 13:04:20,263] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:04:20,266] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0095040e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.7936316e-35], sampled 0.35228040459009047
[2019-03-27 13:05:08,656] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 13:05:09,450] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.7704 3164088996.2009 1777.0000
[2019-03-27 13:05:09,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8604 2842435242.9853 1131.0000
[2019-03-27 13:05:09,859] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2161 2927390796.4298 1338.0000
[2019-03-27 13:05:09,954] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8607 3007711637.9330 1766.0000
[2019-03-27 13:05:10,971] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 975000, evaluation results [975000.0, 7883.770448469881, 3164088996.2009315, 1777.0, 8254.216109707997, 2927390796.4297576, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.86073322157, 3007711637.9330444, 1766.0, 8496.860364057107, 2842435242.9852624, 1131.0]
[2019-03-27 13:05:17,710] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1652316e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 13:05:17,720] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0478
[2019-03-27 13:05:17,726] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4927319954189953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688513.1175253581, 688513.1175253581, 182464.9332078178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4826400.0000, 
sim time next is 4827000.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.493551619166988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689658.780671726, 689658.7806717253, 182591.6383510742], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.74, 1.0, 1.0, 0.38982122791203366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19157188351992388, 0.19157188351992369, 0.2725248333598122], 
reward next is 0.7275, 
noisyNet noise sample is [array([0.5002721], dtype=float32), 0.08387592]. 
=============================================
[2019-03-27 13:05:17,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.19045 ]
 [73.46984 ]
 [73.61603 ]
 [73.63164 ]
 [73.635254]], R is [[72.945755  ]
 [72.9439621 ]
 [72.94232941]
 [72.94078827]
 [72.93929291]].
[2019-03-27 13:05:17,918] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7140936e-29 1.0000000e+00 4.4031286e-37 2.4060792e-37 9.3127695e-33], sum to 1.0000
[2019-03-27 13:05:17,929] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7646
[2019-03-27 13:05:17,937] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.6177226191331104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863238.2300704888, 863238.2300704888, 204085.358952211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4850400.0000, 
sim time next is 4851000.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.6105353937534638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 853190.387444691, 853190.387444691, 202717.0632754363], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.815, 1.0, 1.0, 0.5307655346427275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2369973298457475, 0.2369973298457475, 0.30256278100811385], 
reward next is 0.6974, 
noisyNet noise sample is [array([0.84243864], dtype=float32), -0.7677046]. 
=============================================
[2019-03-27 13:05:17,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[62.849644]
 [62.454773]
 [61.437294]
 [61.538807]
 [61.540226]], R is [[63.05585861]
 [63.12069702]
 [63.17321014]
 [63.1928978 ]
 [63.21003723]].
[2019-03-27 13:05:21,282] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.1205168e-28 1.0000000e+00 1.5853925e-35 7.0677452e-34 1.3202543e-30], sum to 1.0000
[2019-03-27 13:05:21,293] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5420
[2019-03-27 13:05:21,298] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4853966873318575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678259.9409003592, 678259.9409003598, 181339.3974380624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4839600.0000, 
sim time next is 4840200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4846914292085788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677274.1475613822, 677274.1475613815, 181232.0632363257], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3791463002512997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18813170765593948, 0.18813170765593928, 0.2704956167706354], 
reward next is 0.7295, 
noisyNet noise sample is [array([0.41525042], dtype=float32), -1.7818699]. 
=============================================
[2019-03-27 13:05:21,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.4518177e-31 1.0000000e+00 3.4641710e-38 2.8262338e-36 1.2034897e-32], sum to 1.0000
[2019-03-27 13:05:21,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3242
[2019-03-27 13:05:21,923] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 75.5, 1.0, 2.0, 0.493124438284351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689061.6706909187, 689061.670690918, 182525.5695888306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4833000.0000, 
sim time next is 4833600.0000, 
raw observation next is [27.66666666666666, 76.0, 1.0, 2.0, 0.4923956981189029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688043.0443596254, 688043.0443596254, 182412.9919730041], 
processed observation next is [1.0, 0.9565217391304348, 0.5102685624012636, 0.76, 1.0, 1.0, 0.3884285519504854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19112306787767372, 0.19112306787767372, 0.272258196974633], 
reward next is 0.7277, 
noisyNet noise sample is [array([-1.1330256], dtype=float32), 2.1982229]. 
=============================================
[2019-03-27 13:05:22,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0299034e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1155588e-34], sum to 1.0000
[2019-03-27 13:05:22,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9775
[2019-03-27 13:05:22,137] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.4822587121336495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673873.7594510508, 673873.7594510501, 180864.0671064314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4903200.0000, 
sim time next is 4903800.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.4885937958356684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 682728.7959970834, 682728.7959970834, 181828.9288419744], 
processed observation next is [1.0, 0.782608695652174, 0.5734597156398105, 0.7, 1.0, 1.0, 0.3838479467899619, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18964688777696762, 0.18964688777696762, 0.27138646095817076], 
reward next is 0.7286, 
noisyNet noise sample is [array([-0.69093233], dtype=float32), 0.9535984]. 
=============================================
[2019-03-27 13:05:22,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.81558976e-27 1.00000000e+00 1.05736825e-36 8.68640964e-35
 1.23717376e-30], sum to 1.0000
[2019-03-27 13:05:22,372] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7679
[2019-03-27 13:05:22,379] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.8029879221334711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1122274.554489039, 1122274.554489039, 244644.3791342582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4867800.0000, 
sim time next is 4868400.0000, 
raw observation next is [28.33333333333334, 77.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.987081761574328, 6.9112, 168.9126192848853, 1507624.818489466, 1453791.792654888, 311349.3201276031], 
processed observation next is [1.0, 0.34782608695652173, 0.5418641390205374, 0.7733333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.007588176157432791, 0.0, 0.8294382892204908, 0.41878467180262946, 0.4038310535152467, 0.46470047780239265], 
reward next is 0.1559, 
noisyNet noise sample is [array([-2.0544605], dtype=float32), 0.2591315]. 
=============================================
[2019-03-27 13:05:26,739] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.8871691e-29 1.0000000e+00 1.3818578e-38 1.3648910e-35 1.1923904e-31], sum to 1.0000
[2019-03-27 13:05:26,752] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1335
[2019-03-27 13:05:26,760] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.508993293196418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711243.2530233152, 711243.2530233152, 185017.2070422437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4918800.0000, 
sim time next is 4919400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5094168162108175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711835.2624259198, 711835.2624259191, 185084.6938995246], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4089359231455632, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19773201734053328, 0.19773201734053308, 0.27624581179033525], 
reward next is 0.7238, 
noisyNet noise sample is [array([1.4242541], dtype=float32), -0.37656644]. 
=============================================
[2019-03-27 13:05:31,328] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.548201e-34 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 13:05:31,339] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3052
[2019-03-27 13:05:31,350] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4804699605607315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671499.0081530006, 671499.0081530006, 180608.3478709486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5119200.0000, 
sim time next is 5119800.0000, 
raw observation next is [26.16666666666667, 84.0, 1.0, 2.0, 0.4833168706571978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675352.8243392322, 675352.8243392328, 181023.1087109265], 
processed observation next is [0.0, 0.2608695652173913, 0.4391785150078992, 0.84, 1.0, 1.0, 0.3774902056110817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18759800676089783, 0.187598006760898, 0.2701837443446664], 
reward next is 0.7298, 
noisyNet noise sample is [array([0.0002954], dtype=float32), -0.8477966]. 
=============================================
[2019-03-27 13:05:38,597] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.553610e-32 1.000000e+00 0.000000e+00 0.000000e+00 1.894602e-35], sum to 1.0000
[2019-03-27 13:05:38,612] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1301
[2019-03-27 13:05:38,619] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5214569816534618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728665.3976527454, 728665.3976527454, 187026.0142254882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5083200.0000, 
sim time next is 5083800.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.5225655432050414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730214.9944851904, 730214.9944851898, 187206.8633915978], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.4247777628976403, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20283749846810845, 0.20283749846810828, 0.2794132289426833], 
reward next is 0.7206, 
noisyNet noise sample is [array([-2.671568], dtype=float32), 1.7727933]. 
=============================================
[2019-03-27 13:05:40,185] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8534690e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0178534e-37], sum to 1.0000
[2019-03-27 13:05:40,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0987
[2019-03-27 13:05:40,200] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4811471539177737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672445.3420356801, 672445.3420356801, 180710.5309765235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5116200.0000, 
sim time next is 5116800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4802510403429787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671192.9425618037, 671192.9425618037, 180575.3256395515], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3737964341481671, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18644248404494548, 0.18644248404494548, 0.26951541140231566], 
reward next is 0.7305, 
noisyNet noise sample is [array([-1.4795756], dtype=float32), 0.22581373]. 
=============================================
[2019-03-27 13:05:46,202] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1660820e-28 1.0000000e+00 1.5601398e-36 2.9796142e-36 1.0564361e-30], sum to 1.0000
[2019-03-27 13:05:46,213] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3108
[2019-03-27 13:05:46,220] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5164342776490943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721644.4660628254, 721644.4660628254, 186210.6323128443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5185800.0000, 
sim time next is 5186400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5171155593673222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722596.7855046365, 722596.7855046365, 186320.7193050761], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41821151731002676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20072132930684347, 0.20072132930684347, 0.2780906258284718], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.53301346], dtype=float32), 0.8409592]. 
=============================================
[2019-03-27 13:05:56,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4273102e-25 1.0000000e+00 5.3604728e-34 3.6295448e-32 3.2595719e-28], sum to 1.0000
[2019-03-27 13:05:56,221] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9153
[2019-03-27 13:05:56,229] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 63.33333333333334, 1.0, 2.0, 0.5148581257338996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719441.2689969313, 719441.2689969313, 185959.2389294299], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5593800.0000, 
sim time next is 5594400.0000, 
raw observation next is [31.4, 65.0, 1.0, 2.0, 0.5245087571855964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732931.3112031848, 732931.3112031848, 187527.1367840461], 
processed observation next is [1.0, 0.782608695652174, 0.6872037914691943, 0.65, 1.0, 1.0, 0.42711898456095954, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20359203088977354, 0.20359203088977354, 0.2798912489314121], 
reward next is 0.7201, 
noisyNet noise sample is [array([-0.36307678], dtype=float32), -0.416576]. 
=============================================
[2019-03-27 13:06:03,658] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5939859e-15 1.0000000e+00 3.4915292e-22 7.7576751e-16 7.6703887e-18], sum to 1.0000
[2019-03-27 13:06:03,670] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5303
[2019-03-27 13:06:03,680] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2219697.031123675 W.
[2019-03-27 13:06:03,686] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 49.0, 1.0, 2.0, 0.9461968505862722, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.982238397372287, 6.9112, 168.9124702717774, 2219697.031123675, 2169300.09214961, 448040.3632778021], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5581800.0000, 
sim time next is 5582400.0000, 
raw observation next is [33.93333333333334, 49.33333333333333, 1.0, 2.0, 0.5824280690400508, 1.0, 1.0, 0.5824280690400508, 1.0, 2.0, 0.995827263355929, 6.911199999999999, 6.9112, 170.5573041426782, 2443499.691080527, 2443499.691080528, 473450.0246742275], 
processed observation next is [1.0, 0.6086956521739131, 0.807266982622433, 0.4933333333333333, 1.0, 1.0, 0.4969012880000612, 1.0, 0.5, 0.4969012880000612, 1.0, 1.0, 0.994911296775523, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6787499141890353, 0.6787499141890355, 0.7066418278719814], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.273457], dtype=float32), -0.3994136]. 
=============================================
[2019-03-27 13:06:08,318] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.0930702e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7016085e-38], sum to 1.0000
[2019-03-27 13:06:08,324] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9275
[2019-03-27 13:06:08,329] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.45, 57.66666666666667, 1.0, 2.0, 0.5341408854567656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746395.6597459071, 746395.6597459076, 189120.6253001516], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5507400.0000, 
sim time next is 5508000.0000, 
raw observation next is [33.2, 59.0, 1.0, 2.0, 0.5421744223301029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757625.53488525, 757625.5348852506, 190469.7265458142], 
processed observation next is [1.0, 0.782608695652174, 0.7725118483412324, 0.59, 1.0, 1.0, 0.4484029184700035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.210451537468125, 0.21045153746812517, 0.2842831739489764], 
reward next is 0.7157, 
noisyNet noise sample is [array([-0.21800768], dtype=float32), -0.104228154]. 
=============================================
[2019-03-27 13:06:08,346] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[69.78851 ]
 [68.94353 ]
 [67.21291 ]
 [62.683495]
 [55.153584]], R is [[70.18452454]
 [70.20040894]
 [70.21746826]
 [70.23635101]
 [70.25331879]].
[2019-03-27 13:06:08,799] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 13:06:08,808] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6421
[2019-03-27 13:06:08,815] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333334, 72.66666666666667, 1.0, 2.0, 0.541256075815571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756341.7958374814, 756341.795837482, 190312.8282231646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5649600.0000, 
sim time next is 5650200.0000, 
raw observation next is [30.0, 72.0, 1.0, 2.0, 0.5419428748475508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757301.8589368007, 757301.8589368014, 190428.9813475051], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.72, 1.0, 1.0, 0.44812394559945873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21036162748244466, 0.21036162748244483, 0.28422236022015684], 
reward next is 0.7158, 
noisyNet noise sample is [array([1.2284744], dtype=float32), 2.0549831]. 
=============================================
[2019-03-27 13:06:09,156] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 13:06:09,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4071
[2019-03-27 13:06:09,171] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.35, 69.66666666666667, 1.0, 2.0, 0.5727498108578164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800367.1721246511, 800367.1721246506, 195779.4939248035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5512200.0000, 
sim time next is 5512800.0000, 
raw observation next is [31.1, 71.33333333333334, 1.0, 2.0, 0.5685703269645005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794524.5288780818, 794524.5288780824, 195037.5903009192], 
processed observation next is [1.0, 0.8260869565217391, 0.6729857819905214, 0.7133333333333334, 1.0, 1.0, 0.4802052132102415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22070125802168938, 0.22070125802168955, 0.29110088104614806], 
reward next is 0.7089, 
noisyNet noise sample is [array([-0.05995346], dtype=float32), -1.3004359]. 
=============================================
[2019-03-27 13:06:17,144] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 13:06:17,149] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:06:17,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:06:17,150] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:06:17,151] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:06:17,154] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:06:17,153] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:06:17,155] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:06:17,160] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:06:17,156] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:06:17,173] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:06:17,578] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run41
[2019-03-27 13:06:17,621] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run41
[2019-03-27 13:06:17,920] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run41
[2019-03-27 13:06:18,000] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run41
[2019-03-27 13:06:18,068] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run41
[2019-03-27 13:06:35,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06042419], dtype=float32), 0.04917233]
[2019-03-27 13:06:35,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.76666666666667, 95.0, 1.0, 2.0, 0.4490403868515986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693652.4545397687, 693652.4545397687, 183952.0066599745]
[2019-03-27 13:06:35,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:06:35,075] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.033458e-35 1.000000e+00 0.000000e+00 0.000000e+00 6.562294e-37], sampled 0.5791950431452251
[2019-03-27 13:06:35,979] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06042419], dtype=float32), 0.04917233]
[2019-03-27 13:06:35,980] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.8, 83.16666666666667, 1.0, 2.0, 0.3026255061786514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484424.6163609365, 484424.6163609365, 165967.935274268]
[2019-03-27 13:06:35,982] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:06:35,985] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6710751e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5635049e-37], sampled 0.9235391027958216
[2019-03-27 13:06:42,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06042419], dtype=float32), 0.04917233]
[2019-03-27 13:06:42,148] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.87657308, 76.6541486, 1.0, 2.0, 0.4948523784130203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691476.9734865043, 691476.9734865037, 182793.8124982288]
[2019-03-27 13:06:42,149] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:06:42,153] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4036392e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9357598388238044
[2019-03-27 13:06:46,472] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06042419], dtype=float32), 0.04917233]
[2019-03-27 13:06:46,473] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.93333333333333, 75.0, 1.0, 2.0, 0.5342987797163259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761089.8014656723, 761089.8014656723, 190993.8652952646]
[2019-03-27 13:06:46,474] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:06:46,478] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7311481e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6142025e-38], sampled 0.34080917715898196
[2019-03-27 13:07:14,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06042419], dtype=float32), 0.04917233]
[2019-03-27 13:07:14,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.6487600454747424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 906630.0822340903, 906630.0822340903, 210170.5499065353]
[2019-03-27 13:07:14,589] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:07:14,595] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1638119e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9352553e-36], sampled 0.09787099483652528
[2019-03-27 13:07:28,282] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06042419], dtype=float32), 0.04917233]
[2019-03-27 13:07:28,284] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.88333333333333, 67.5, 1.0, 2.0, 0.9020611021680933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1260823.742558258, 1260823.742558258, 270506.2785290526]
[2019-03-27 13:07:28,286] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:07:28,293] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.3556814e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2999771e-33], sampled 0.6449371359216036
[2019-03-27 13:08:14,243] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06042419], dtype=float32), 0.04917233]
[2019-03-27 13:08:14,245] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.60144419333334, 86.20693785499999, 1.0, 2.0, 0.568869878013608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794943.2804488974, 794943.2804488974, 195087.8202985117]
[2019-03-27 13:08:14,245] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:08:14,251] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0386839e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2256770e-37], sampled 0.501120786139509
[2019-03-27 13:08:21,318] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06042419], dtype=float32), 0.04917233]
[2019-03-27 13:08:21,318] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.4, 76.33333333333334, 1.0, 2.0, 0.4830342685713457, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564967285, 674957.8107127622, 674957.8107127622, 180982.6124664751]
[2019-03-27 13:08:21,319] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:08:21,324] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.24662564e-29 1.00000000e+00 0.00000000e+00 3.93213802e-36
 3.05373919e-31], sampled 0.14162603021840237
[2019-03-27 13:08:24,009] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06042419], dtype=float32), 0.04917233]
[2019-03-27 13:08:24,010] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.23333333333333, 68.0, 1.0, 2.0, 0.6005712776646637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839260.5696105291, 839260.5696105291, 200852.3259460085]
[2019-03-27 13:08:24,011] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:08:24,014] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.30544401025934165
[2019-03-27 13:08:25,818] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164139523.4469 1778.0000
[2019-03-27 13:08:25,952] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 13:08:26,209] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-27 13:08:26,622] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-03-27 13:08:26,707] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2178 3007719775.7786 1766.0000
[2019-03-27 13:08:27,725] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1000000, evaluation results [1000000.0, 7883.41881391125, 3164139523.4469013, 1778.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7998.217808978373, 3007719775.7785764, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 13:08:29,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.2948984e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 13:08:29,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5976
[2019-03-27 13:08:29,286] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.61666666666667, 91.16666666666667, 1.0, 2.0, 0.5019505234115789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701398.7692300805, 701398.7692300805, 183901.9238461858], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5629800.0000, 
sim time next is 5630400.0000, 
raw observation next is [25.6, 91.0, 1.0, 2.0, 0.4998949253814117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698525.4422616044, 698525.4422616044, 183579.3229989482], 
processed observation next is [0.0, 0.17391304347826086, 0.4123222748815167, 0.91, 1.0, 1.0, 0.3974637655197732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19403484507266788, 0.19403484507266788, 0.2739989895506689], 
reward next is 0.7260, 
noisyNet noise sample is [array([-0.79281306], dtype=float32), 0.9051514]. 
=============================================
[2019-03-27 13:08:30,213] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.212936e-37 1.000000e+00 0.000000e+00 0.000000e+00 5.803564e-38], sum to 1.0000
[2019-03-27 13:08:30,228] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0583
[2019-03-27 13:08:30,234] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.3, 61.33333333333334, 1.0, 2.0, 0.5501248672168346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768739.3747577169, 768739.3747577169, 191822.6140858433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5664000.0000, 
sim time next is 5664600.0000, 
raw observation next is [32.35, 61.0, 1.0, 2.0, 0.5479796405813133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765740.5744728966, 765740.574472896, 191455.4171123038], 
processed observation next is [0.0, 0.5652173913043478, 0.7322274881516588, 0.61, 1.0, 1.0, 0.4553971573268835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2127057151313602, 0.21270571513136002, 0.2857543538989609], 
reward next is 0.7142, 
noisyNet noise sample is [array([1.6861067], dtype=float32), -0.47274613]. 
=============================================
[2019-03-27 13:08:37,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5621441e-17 1.0000000e+00 1.1469428e-23 1.0043439e-13 9.4936273e-17], sum to 1.0000
[2019-03-27 13:08:37,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5435
[2019-03-27 13:08:37,055] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.8, 66.0, 1.0, 2.0, 0.4195141213241959, 1.0, 2.0, 0.4195141213241959, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1172666.914423534, 1172666.914423534, 277161.752796246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5850600.0000, 
sim time next is 5851200.0000, 
raw observation next is [31.6, 67.0, 1.0, 2.0, 0.5242928412597825, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104269, 732629.4933326861, 732629.4933326861, 187493.2213763452], 
processed observation next is [1.0, 0.7391304347826086, 0.6966824644549764, 0.67, 1.0, 1.0, 0.42685884489130416, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451522826, 0.20350819259241282, 0.20350819259241282, 0.27984062891991823], 
reward next is 0.7202, 
noisyNet noise sample is [array([0.33340818], dtype=float32), 0.18934692]. 
=============================================
[2019-03-27 13:08:40,519] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3062641e-20 1.0000000e+00 2.3068726e-28 4.4428583e-24 2.3073548e-22], sum to 1.0000
[2019-03-27 13:08:40,530] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9751
[2019-03-27 13:08:40,540] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 93.16666666666667, 1.0, 2.0, 0.9362294652417821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104223, 1308610.786365592, 1308610.786365591, 280097.1929322899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5803800.0000, 
sim time next is 5804400.0000, 
raw observation next is [26.1, 93.33333333333334, 1.0, 2.0, 0.8847047734088138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1236550.425911074, 1236550.425911074, 265764.2287484905], 
processed observation next is [1.0, 0.17391304347826086, 0.4360189573459717, 0.9333333333333335, 1.0, 1.0, 0.861090088444354, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3434862294197428, 0.3434862294197428, 0.39666302798282166], 
reward next is 0.6033, 
noisyNet noise sample is [array([-1.5718318], dtype=float32), -0.60239995]. 
=============================================
[2019-03-27 13:08:46,023] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9639097e-24 1.0000000e+00 5.0997074e-33 7.7541287e-30 7.1139777e-25], sum to 1.0000
[2019-03-27 13:08:46,031] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9367
[2019-03-27 13:08:46,037] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.81666666666667, 73.0, 1.0, 2.0, 0.5333841168605511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 745337.7984546522, 745337.7984546529, 188993.0165105355], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6027000.0000, 
sim time next is 6027600.0000, 
raw observation next is [29.63333333333334, 74.0, 1.0, 2.0, 0.5351201013424776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747764.4741624974, 747764.4741624967, 189282.4901317246], 
processed observation next is [1.0, 0.782608695652174, 0.6034755134281204, 0.74, 1.0, 1.0, 0.43990373655720183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20771235393402707, 0.20771235393402687, 0.2825111793010815], 
reward next is 0.7175, 
noisyNet noise sample is [array([0.77084917], dtype=float32), -1.2512002]. 
=============================================
[2019-03-27 13:08:50,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3537753e-29 1.0000000e+00 1.5305709e-36 1.6328450e-34 3.8893945e-30], sum to 1.0000
[2019-03-27 13:08:50,217] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3431
[2019-03-27 13:08:50,221] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 85.0, 1.0, 2.0, 0.5381012223055038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751931.6982087173, 751931.6982087173, 189781.4013220653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6035400.0000, 
sim time next is 6036000.0000, 
raw observation next is [27.7, 85.33333333333333, 1.0, 2.0, 0.5379872024702902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751772.3128294167, 751772.3128294161, 189762.2583688775], 
processed observation next is [1.0, 0.8695652173913043, 0.5118483412322274, 0.8533333333333333, 1.0, 1.0, 0.4433580752654098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20882564245261576, 0.2088256424526156, 0.28322725129683207], 
reward next is 0.7168, 
noisyNet noise sample is [array([-0.98835313], dtype=float32), -0.6141801]. 
=============================================
[2019-03-27 13:08:50,240] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[68.03463]
 [67.91572]
 [67.75322]
 [67.77585]
 [67.65142]], R is [[68.1888504 ]
 [68.22370911]
 [68.25816345]
 [68.29221344]
 [68.32590485]].
[2019-03-27 13:08:53,658] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4876341e-21 1.0000000e+00 6.6314959e-30 1.1394284e-24 9.7919689e-23], sum to 1.0000
[2019-03-27 13:08:53,670] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2222
[2019-03-27 13:08:53,679] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.6464322751620479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 903375.6830844246, 903375.683084424, 209706.8720106509], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6062400.0000, 
sim time next is 6063000.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.7593145975373807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1061205.202360641, 1061205.202360641, 234168.3392034768], 
processed observation next is [1.0, 0.17391304347826086, 0.4360189573459717, 0.93, 1.0, 1.0, 0.7100175873944347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2947792228779558, 0.2947792228779558, 0.34950498388578627], 
reward next is 0.6505, 
noisyNet noise sample is [array([-0.13743955], dtype=float32), -0.7366179]. 
=============================================
[2019-03-27 13:08:53,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[56.429058]
 [56.594986]
 [56.69113 ]
 [56.72564 ]
 [56.91864 ]], R is [[55.50845718]
 [55.64037704]
 [55.7695961 ]
 [55.89382553]
 [56.00892258]].
[2019-03-27 13:08:54,421] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7068901e-20 1.0000000e+00 2.4780954e-29 1.8151918e-23 2.6071471e-21], sum to 1.0000
[2019-03-27 13:08:54,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7462
[2019-03-27 13:08:54,447] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 84.0, 1.0, 2.0, 0.751059915969747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049662.897646974, 1049662.897646974, 232255.1746011922], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5990400.0000, 
sim time next is 5991000.0000, 
raw observation next is [28.86666666666667, 83.16666666666667, 1.0, 2.0, 0.8239101206838724, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1151531.760519927, 1151531.760519927, 249867.2128934293], 
processed observation next is [1.0, 0.34782608695652173, 0.567140600315956, 0.8316666666666667, 1.0, 1.0, 0.7878435188962317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3198699334777575, 0.3198699334777575, 0.3729361386469094], 
reward next is 0.6271, 
noisyNet noise sample is [array([0.11810243], dtype=float32), 0.30991238]. 
=============================================
[2019-03-27 13:08:54,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[55.85046 ]
 [55.708355]
 [55.71155 ]
 [55.587494]
 [55.63329 ]], R is [[55.47052002]
 [55.56916428]
 [55.66691589]
 [55.76641846]
 [55.86363983]].
[2019-03-27 13:08:55,631] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.90317184e-10 1.13097434e-04 4.14830250e-17 9.99886870e-01
 8.60264334e-13], sum to 1.0000
[2019-03-27 13:08:55,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9135
[2019-03-27 13:08:55,645] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.8676842727260184, 1.0, 2.0, 0.8676842727260184, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2426819.984026392, 2426819.984026392, 454162.6947561675], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6089400.0000, 
sim time next is 6090000.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.8250101709942864, 1.0, 2.0, 0.8250101709942864, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2307354.903203745, 2307354.903203745, 432226.8335294719], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.65, 1.0, 1.0, 0.7891688807160078, 1.0, 1.0, 0.7891688807160078, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6409319175565958, 0.6409319175565958, 0.6451146769096595], 
reward next is 0.3549, 
noisyNet noise sample is [array([0.13822123], dtype=float32), -0.34902722]. 
=============================================
[2019-03-27 13:08:55,657] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[53.960556]
 [54.235638]
 [54.074425]
 [55.0393  ]
 [54.436554]], R is [[54.44513702]
 [54.22282791]
 [53.99904251]
 [53.80242538]
 [53.7224884 ]].
[2019-03-27 13:08:56,875] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4315828e-10 9.9884295e-01 7.7375848e-18 1.1569787e-03 1.4165885e-11], sum to 1.0000
[2019-03-27 13:08:56,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9883
[2019-03-27 13:08:56,894] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2326114.652385924 W.
[2019-03-27 13:08:56,899] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.9, 65.66666666666667, 1.0, 2.0, 0.831711607905257, 1.0, 2.0, 0.831711607905257, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2326114.652385924, 2326114.652385924, 435601.0049667631], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6100800.0000, 
sim time next is 6101400.0000, 
raw observation next is [30.85, 65.83333333333333, 1.0, 2.0, 0.8051408101494677, 1.0, 2.0, 0.8051408101494677, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2251735.020685582, 2251735.020685582, 422378.3025316867], 
processed observation next is [1.0, 0.6086956521739131, 0.661137440758294, 0.6583333333333333, 1.0, 1.0, 0.7652298917463466, 1.0, 1.0, 0.7652298917463466, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6254819501904394, 0.6254819501904394, 0.6304153769129652], 
reward next is 0.3696, 
noisyNet noise sample is [array([0.46686786], dtype=float32), -0.88686097]. 
=============================================
[2019-03-27 13:08:58,733] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2059887e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1330831e-30], sum to 1.0000
[2019-03-27 13:08:58,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3087
[2019-03-27 13:08:58,748] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 75.66666666666667, 1.0, 2.0, 0.5220087075708716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729436.6244912896, 729436.6244912896, 187116.3963388392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6337200.0000, 
sim time next is 6337800.0000, 
raw observation next is [28.85, 75.0, 1.0, 2.0, 0.5235939093401797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731652.492605104, 731652.4926051034, 187375.4744279057], 
processed observation next is [0.0, 0.34782608695652173, 0.5663507109004741, 0.75, 1.0, 1.0, 0.4260167582411803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2032368035014178, 0.20323680350141762, 0.2796648872058294], 
reward next is 0.7203, 
noisyNet noise sample is [array([1.8807843], dtype=float32), -0.3807563]. 
=============================================
[2019-03-27 13:08:59,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.846279e-24 1.000000e+00 8.164218e-32 9.766432e-27 8.796595e-25], sum to 1.0000
[2019-03-27 13:08:59,438] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2550
[2019-03-27 13:08:59,442] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.06666666666667, 83.66666666666667, 1.0, 2.0, 0.7124276490214955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995646.0795226158, 995646.0795226158, 223551.506195066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6074400.0000, 
sim time next is 6075000.0000, 
raw observation next is [28.2, 83.0, 1.0, 2.0, 0.7095382681428593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 991606.1691697689, 991606.1691697689, 222918.5250526618], 
processed observation next is [1.0, 0.30434782608695654, 0.5355450236966824, 0.83, 1.0, 1.0, 0.650046106196216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2754461581027136, 0.2754461581027136, 0.3327142164965102], 
reward next is 0.6673, 
noisyNet noise sample is [array([-0.6752362], dtype=float32), -0.145323]. 
=============================================
[2019-03-27 13:08:59,457] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[59.61723 ]
 [59.602207]
 [59.02183 ]
 [58.27898 ]
 [58.735027]], R is [[59.82503128]
 [59.89311981]
 [59.96628952]
 [60.03194427]
 [60.07693863]].
[2019-03-27 13:09:11,729] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8439121e-10 8.8430703e-01 2.3762867e-17 1.1569296e-01 6.1491563e-12], sum to 1.0000
[2019-03-27 13:09:11,740] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6400
[2019-03-27 13:09:11,749] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2127833.310935671 W.
[2019-03-27 13:09:11,756] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 67.0, 1.0, 2.0, 0.7608799195686377, 1.0, 2.0, 0.7608799195686377, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2127833.310935671, 2127833.310935671, 401269.0253230425], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6453000.0000, 
sim time next is 6453600.0000, 
raw observation next is [30.0, 67.0, 1.0, 2.0, 0.7526808397266688, 1.0, 2.0, 0.7526808397266688, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2104881.705125514, 2104881.705125514, 397485.7698439703], 
processed observation next is [1.0, 0.6956521739130435, 0.6208530805687204, 0.67, 1.0, 1.0, 0.7020251081044202, 1.0, 1.0, 0.7020251081044202, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.584689362534865, 0.584689362534865, 0.5932623430507019], 
reward next is 0.4067, 
noisyNet noise sample is [array([0.00252275], dtype=float32), -1.0161229]. 
=============================================
[2019-03-27 13:09:13,573] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.9787229e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9998853e-33], sum to 1.0000
[2019-03-27 13:09:13,586] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9758
[2019-03-27 13:09:13,596] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 62.5, 1.0, 2.0, 0.5083801552382119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710386.1964374154, 710386.1964374154, 184919.5407924289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6276600.0000, 
sim time next is 6277200.0000, 
raw observation next is [30.63333333333333, 62.66666666666667, 1.0, 2.0, 0.5098059342026421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712379.1800531069, 712379.1800531069, 185146.7204130388], 
processed observation next is [0.0, 0.6521739130434783, 0.6508688783570299, 0.6266666666666667, 1.0, 1.0, 0.4094047400031832, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19788310557030747, 0.19788310557030747, 0.2763383886761773], 
reward next is 0.7237, 
noisyNet noise sample is [array([-2.144839], dtype=float32), 0.93953574]. 
=============================================
[2019-03-27 13:09:14,793] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4935338e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9870822e-32], sum to 1.0000
[2019-03-27 13:09:14,804] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6789
[2019-03-27 13:09:14,808] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.66666666666667, 1.0, 2.0, 0.5220100882952503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729438.5545294686, 729438.5545294686, 187116.4846361777], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6333600.0000, 
sim time next is 6334200.0000, 
raw observation next is [28.1, 79.0, 1.0, 2.0, 0.5217524332401644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729078.3928511643, 729078.3928511643, 187074.4467776832], 
processed observation next is [0.0, 0.30434782608695654, 0.5308056872037916, 0.79, 1.0, 1.0, 0.42379811233754744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2025217757919901, 0.2025217757919901, 0.2792155922054973], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.2122562], dtype=float32), 0.6316844]. 
=============================================
[2019-03-27 13:09:20,539] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.2477589e-21 1.0000000e+00 4.8649968e-29 9.5619597e-24 1.4724308e-20], sum to 1.0000
[2019-03-27 13:09:20,547] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3529
[2019-03-27 13:09:20,554] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.41666666666667, 83.33333333333334, 1.0, 2.0, 0.8167440714093376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1141510.802222194, 1141510.802222194, 248061.3240637312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6418200.0000, 
sim time next is 6418800.0000, 
raw observation next is [27.5, 83.0, 1.0, 2.0, 0.7978300314144624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1115061.981698445, 1115061.981698445, 243377.9230600449], 
processed observation next is [1.0, 0.30434782608695654, 0.5023696682464456, 0.83, 1.0, 1.0, 0.7564217245957379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30973943936067916, 0.30973943936067916, 0.36325063143290287], 
reward next is 0.6367, 
noisyNet noise sample is [array([-1.1673872], dtype=float32), -0.5028419]. 
=============================================
[2019-03-27 13:09:23,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6990790e-20 1.0000000e+00 7.0461459e-29 1.1031231e-21 9.3436787e-21], sum to 1.0000
[2019-03-27 13:09:23,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0907
[2019-03-27 13:09:23,308] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 89.33333333333334, 1.0, 2.0, 0.5304935333408877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741297.158962835, 741297.1589628356, 188511.6617082247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6482400.0000, 
sim time next is 6483000.0000, 
raw observation next is [26.73333333333333, 89.66666666666666, 1.0, 2.0, 0.53079302813172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741715.8109090619, 741715.8109090626, 188561.3232133546], 
processed observation next is [1.0, 0.0, 0.4660347551342811, 0.8966666666666666, 1.0, 1.0, 0.43469039533942166, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20603216969696164, 0.20603216969696184, 0.2814348107662009], 
reward next is 0.7186, 
noisyNet noise sample is [array([-0.49192914], dtype=float32), -0.926118]. 
=============================================
[2019-03-27 13:09:23,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[59.86509 ]
 [60.503326]
 [61.339935]
 [62.6539  ]
 [64.952995]], R is [[59.46154404]
 [59.58556747]
 [59.7085228 ]
 [59.83040619]
 [59.95117569]].
[2019-03-27 13:09:33,688] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 13:09:33,689] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:09:33,690] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:09:33,691] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:09:33,692] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:09:33,693] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:09:33,695] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:09:33,697] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:09:33,694] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:09:33,701] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:09:33,701] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:09:33,719] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run42
[2019-03-27 13:09:33,719] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run42
[2019-03-27 13:09:33,771] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run42
[2019-03-27 13:09:33,793] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run42
[2019-03-27 13:09:33,816] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run42
[2019-03-27 13:09:34,932] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06094614], dtype=float32), 0.052427202]
[2019-03-27 13:09:34,934] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.04883498666667, 86.65226605000001, 1.0, 2.0, 0.3057977723556052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490192.8047896203, 490192.8047896209, 166387.628946387]
[2019-03-27 13:09:34,935] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:09:34,940] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7661982e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5476270e-36], sampled 0.495381816832672
[2019-03-27 13:09:49,926] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06094614], dtype=float32), 0.052427202]
[2019-03-27 13:09:49,927] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.71666666666667, 81.0, 1.0, 2.0, 0.2991064564113938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482360.3084019152, 482360.3084019152, 165825.5134740438]
[2019-03-27 13:09:49,928] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:09:49,932] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1438374e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.0748428e-35], sampled 0.2105822971484771
[2019-03-27 13:10:05,363] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06094614], dtype=float32), 0.052427202]
[2019-03-27 13:10:05,365] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.40564069, 79.94615043333333, 1.0, 2.0, 0.4767912282689061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679904.2915853286, 679904.2915853292, 181791.3710167566]
[2019-03-27 13:10:05,366] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:10:05,369] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8062425e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4724380e-35], sampled 0.40925848040554946
[2019-03-27 13:10:28,871] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06094614], dtype=float32), 0.052427202]
[2019-03-27 13:10:28,872] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 75.0, 1.0, 2.0, 0.5964550567010019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833506.1528253664, 833506.1528253664, 200087.6766423961]
[2019-03-27 13:10:28,873] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:10:28,876] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0460044e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4483604e-33], sampled 0.4220878137997647
[2019-03-27 13:10:55,661] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06094614], dtype=float32), 0.052427202]
[2019-03-27 13:10:55,663] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.06387634, 53.63452415, 1.0, 2.0, 0.5803314279028743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810965.8555013625, 810965.8555013625, 197139.4881436988]
[2019-03-27 13:10:55,664] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:10:55,665] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.419016e-34 1.000000e+00 0.000000e+00 0.000000e+00 7.848389e-34], sampled 0.18760518835476336
[2019-03-27 13:11:03,501] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06094614], dtype=float32), 0.052427202]
[2019-03-27 13:11:03,502] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.15243416, 80.66271835333333, 1.0, 2.0, 0.5109779325684054, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8873996858868084, 6.9112, 6.9112, 160.1138559829176, 1428567.944973163, 1428567.944973163, 312839.7546469936]
[2019-03-27 13:11:03,502] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:11:03,504] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2907433e-23 1.0000000e+00 7.6818960e-33 7.5028561e-26 2.7602454e-23], sampled 0.5278387644022957
[2019-03-27 13:11:23,558] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06094614], dtype=float32), 0.052427202]
[2019-03-27 13:11:23,559] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.4, 86.66666666666667, 1.0, 2.0, 0.5436822908142365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759733.3587966483, 759733.3587966489, 190722.8177973247]
[2019-03-27 13:11:23,559] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:11:23,562] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7883091e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3315071e-33], sampled 0.6022626847899113
[2019-03-27 13:11:41,345] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2953 3007596012.4260 1766.0000
[2019-03-27 13:11:41,454] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 13:11:41,524] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06094614], dtype=float32), 0.052427202]
[2019-03-27 13:11:41,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.62536882, 69.42849333333334, 1.0, 2.0, 0.4429521789740697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 641305.1626110837, 641305.1626110837, 178003.5357801087]
[2019-03-27 13:11:41,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:11:41,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.2044446e-31 1.0000000e+00 0.0000000e+00 5.2645276e-38 1.1342815e-30], sampled 0.4994319042718103
[2019-03-27 13:11:41,687] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0344 2842563115.2351 1131.0000
[2019-03-27 13:11:41,708] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 13:11:42,054] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164027848.4855 1778.0000
[2019-03-27 13:11:43,072] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1025000, evaluation results [1025000.0, 7884.16835844559, 3164027848.485504, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.295259250187, 3007596012.4259605, 1766.0, 8496.034363079138, 2842563115.235065, 1131.0]
[2019-03-27 13:11:47,456] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7465756e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9382335e-31], sum to 1.0000
[2019-03-27 13:11:47,465] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4951
[2019-03-27 13:11:47,471] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 95.0, 1.0, 2.0, 0.54289478418599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 758632.516046979, 758632.5160469783, 190585.7856484646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6667200.0000, 
sim time next is 6667800.0000, 
raw observation next is [24.76666666666667, 95.0, 1.0, 2.0, 0.6289204452708832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 878893.1386424365, 878893.1386424365, 206247.6123008078], 
processed observation next is [1.0, 0.17391304347826086, 0.3728278041074251, 0.95, 1.0, 1.0, 0.552916199121546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24413698295623235, 0.24413698295623235, 0.3078322571653848], 
reward next is 0.6922, 
noisyNet noise sample is [array([0.8578461], dtype=float32), -2.648211]. 
=============================================
[2019-03-27 13:11:51,614] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.443671e-31 1.000000e+00 0.000000e+00 0.000000e+00 9.713314e-32], sum to 1.0000
[2019-03-27 13:11:51,621] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0776
[2019-03-27 13:11:51,627] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.51666666666667, 71.16666666666666, 1.0, 2.0, 0.3309378349317248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518444.9361335237, 518444.9361335237, 168330.5553564526], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6821400.0000, 
sim time next is 6822000.0000, 
raw observation next is [24.4, 72.0, 1.0, 2.0, 0.3309746863278688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518330.183853464, 518330.183853464, 168317.4379391805], 
processed observation next is [1.0, 1.0, 0.3554502369668246, 0.72, 1.0, 1.0, 0.19394540521429976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14398060662596224, 0.14398060662596224, 0.2512200566256425], 
reward next is 0.7488, 
noisyNet noise sample is [array([0.7369595], dtype=float32), -0.42178324]. 
=============================================
[2019-03-27 13:11:51,650] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[79.068275]
 [79.04435 ]
 [79.014496]
 [78.97858 ]
 [78.94153 ]], R is [[79.16676331]
 [79.12386322]
 [79.08137512]
 [79.03935242]
 [78.99782562]].
[2019-03-27 13:12:07,310] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7172189e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1792001e-31], sum to 1.0000
[2019-03-27 13:12:07,318] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5790
[2019-03-27 13:12:07,330] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333333, 74.83333333333334, 1.0, 2.0, 0.4031931511826403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598859.5032036089, 598859.5032036083, 174351.9116547568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6905400.0000, 
sim time next is 6906000.0000, 
raw observation next is [25.76666666666667, 75.66666666666667, 1.0, 2.0, 0.4053210920412618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 600730.3323681555, 600730.3323681548, 174486.7626591024], 
processed observation next is [0.0, 0.9565217391304348, 0.42022116903633505, 0.7566666666666667, 1.0, 1.0, 0.28351938800152027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16686953676893207, 0.16686953676893188, 0.26042800396880955], 
reward next is 0.7396, 
noisyNet noise sample is [array([-0.72645473], dtype=float32), -0.20539422]. 
=============================================
[2019-03-27 13:12:07,345] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.42861]
 [73.40504]
 [73.32839]
 [73.36561]
 [73.3897 ]], R is [[73.45500183]
 [73.46022797]
 [73.46565247]
 [73.47135925]
 [73.4773941 ]].
[2019-03-27 13:12:11,528] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1947565e-26 1.0000000e+00 4.6085129e-36 4.7177540e-33 4.0206777e-27], sum to 1.0000
[2019-03-27 13:12:11,529] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1855
[2019-03-27 13:12:11,536] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 81.5, 1.0, 2.0, 0.4642418070896867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700839.9378212957, 700839.9378212957, 184602.5237585252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7233000.0000, 
sim time next is 7233600.0000, 
raw observation next is [24.2, 82.0, 1.0, 2.0, 0.3709714109903834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560684.6458244983, 560684.6458244983, 171234.8478279128], 
processed observation next is [1.0, 0.7391304347826086, 0.3459715639810427, 0.82, 1.0, 1.0, 0.24213423010889565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15574573495124952, 0.15574573495124952, 0.25557439974315344], 
reward next is 0.7444, 
noisyNet noise sample is [array([-0.9522286], dtype=float32), 0.44730848]. 
=============================================
[2019-03-27 13:12:18,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7823297e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9412718e-34], sum to 1.0000
[2019-03-27 13:12:18,581] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9971
[2019-03-27 13:12:18,587] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 79.33333333333334, 1.0, 2.0, 0.4817471921539958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673158.771627048, 673158.771627048, 180785.7260450441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7066200.0000, 
sim time next is 7066800.0000, 
raw observation next is [26.9, 80.0, 1.0, 2.0, 0.4821338432490095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673699.2212934479, 673699.2212934486, 180844.1848919697], 
processed observation next is [1.0, 0.8260869565217391, 0.4739336492890995, 0.8, 1.0, 1.0, 0.3760648713843488, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18713867258151332, 0.1871386725815135, 0.2699166938686115], 
reward next is 0.7301, 
noisyNet noise sample is [array([0.4064226], dtype=float32), 0.8417825]. 
=============================================
[2019-03-27 13:12:28,950] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8189365e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.7868285e-34], sum to 1.0000
[2019-03-27 13:12:28,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5871
[2019-03-27 13:12:28,969] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 90.33333333333334, 1.0, 2.0, 0.347409159737246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540638.7765830187, 540638.7765830187, 170017.200224327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7366800.0000, 
sim time next is 7367400.0000, 
raw observation next is [21.85, 90.5, 1.0, 2.0, 0.3412572469574073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 533333.0381592724, 533333.0381592731, 169479.696774071], 
processed observation next is [1.0, 0.2608695652173913, 0.23459715639810438, 0.905, 1.0, 1.0, 0.20633403247880394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14814806615535345, 0.14814806615535364, 0.25295477130458355], 
reward next is 0.7470, 
noisyNet noise sample is [array([-1.1725197], dtype=float32), 0.020330189]. 
=============================================
[2019-03-27 13:12:29,606] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5947404e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.5835290e-34], sum to 1.0000
[2019-03-27 13:12:29,620] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4988
[2019-03-27 13:12:29,625] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 85.0, 1.0, 2.0, 0.3587679162030246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 565085.5175363016, 565085.5175363023, 172164.6353962878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7286400.0000, 
sim time next is 7287000.0000, 
raw observation next is [22.53333333333333, 83.83333333333334, 1.0, 2.0, 0.3973965204707379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625088.3231592949, 625088.3231592949, 177399.5895812583], 
processed observation next is [1.0, 0.34782608695652173, 0.26698262243285936, 0.8383333333333334, 1.0, 1.0, 0.2739717114105276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17363564532202636, 0.17363564532202636, 0.26477550683769896], 
reward next is 0.7352, 
noisyNet noise sample is [array([0.47595847], dtype=float32), -0.07434563]. 
=============================================
[2019-03-27 13:12:29,641] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.16537 ]
 [74.35684 ]
 [74.304924]
 [74.23972 ]
 [74.178734]], R is [[73.82883453]
 [73.83358002]
 [73.84565735]
 [73.85745239]
 [73.86869812]].
[2019-03-27 13:12:38,424] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7828600e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8346838e-35], sum to 1.0000
[2019-03-27 13:12:38,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3295
[2019-03-27 13:12:38,443] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 85.0, 1.0, 2.0, 0.4019174482743009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593008.380844887, 593008.3808448864, 173690.3280106127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7502400.0000, 
sim time next is 7503000.0000, 
raw observation next is [24.45, 85.33333333333334, 1.0, 2.0, 0.402064948367434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593435.8710656009, 593435.8710656002, 173736.6906591997], 
processed observation next is [0.0, 0.8695652173913043, 0.3578199052132702, 0.8533333333333334, 1.0, 1.0, 0.2795963233342578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16484329751822246, 0.16484329751822227, 0.25930849352119356], 
reward next is 0.7407, 
noisyNet noise sample is [array([1.8164752], dtype=float32), -0.36345878]. 
=============================================
[2019-03-27 13:12:38,460] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.9599 ]
 [81.74759]
 [81.64861]
 [81.53372]
 [81.42922]], R is [[82.00056458]
 [81.92131805]
 [81.84276581]
 [81.76489258]
 [81.68769836]].
[2019-03-27 13:12:45,199] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.944357e-34 1.000000e+00 0.000000e+00 0.000000e+00 9.437799e-33], sum to 1.0000
[2019-03-27 13:12:45,213] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6000
[2019-03-27 13:12:45,220] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 90.0, 1.0, 2.0, 0.3078431630587793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490075.2105830322, 490075.2105830322, 166347.9432973092], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7423200.0000, 
sim time next is 7423800.0000, 
raw observation next is [21.16666666666667, 90.83333333333333, 1.0, 2.0, 0.3089452171142616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491210.6600636904, 491210.6600636904, 166421.9512078323], 
processed observation next is [1.0, 0.9565217391304348, 0.2022116903633494, 0.9083333333333333, 1.0, 1.0, 0.16740387604127901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13644740557324733, 0.13644740557324733, 0.24839097195198848], 
reward next is 0.7516, 
noisyNet noise sample is [array([1.1478676], dtype=float32), 1.3798351]. 
=============================================
[2019-03-27 13:12:47,917] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.95057435e-30 1.00000000e+00 0.00000000e+00 1.19474644e-35
 2.56921462e-29], sum to 1.0000
[2019-03-27 13:12:47,929] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7055
[2019-03-27 13:12:47,935] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 92.5, 1.0, 2.0, 0.475874665225491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664950.3579151256, 664950.3579151256, 179902.1686926692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7693800.0000, 
sim time next is 7694400.0000, 
raw observation next is [24.8, 93.0, 1.0, 2.0, 0.4756899223566954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664692.13179326, 664692.1317932606, 179874.5957393567], 
processed observation next is [1.0, 0.043478260869565216, 0.3744075829383887, 0.93, 1.0, 1.0, 0.3683011112731269, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18463670327590556, 0.18463670327590573, 0.2684695458796369], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.54084224], dtype=float32), 1.0686569]. 
=============================================
[2019-03-27 13:12:49,097] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 13:12:49,100] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:12:49,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:12:49,101] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:12:49,104] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:12:49,105] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:12:49,103] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:12:49,109] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:12:49,105] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:12:49,110] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:12:49,112] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:12:49,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run43
[2019-03-27 13:12:49,160] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run43
[2019-03-27 13:12:49,161] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run43
[2019-03-27 13:12:49,200] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run43
[2019-03-27 13:12:49,219] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run43
[2019-03-27 13:12:59,316] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06177285], dtype=float32), 0.052605525]
[2019-03-27 13:12:59,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.9, 63.5, 1.0, 2.0, 0.3165983423573357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505410.526229367, 505410.5262293676, 167496.4043483977]
[2019-03-27 13:12:59,319] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:12:59,324] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.3700049e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0138874e-35], sampled 0.40353303250502925
[2019-03-27 13:13:30,915] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06177285], dtype=float32), 0.052605525]
[2019-03-27 13:13:30,915] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.13108911166666, 96.41565875, 1.0, 2.0, 0.3973337167880617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 598712.1025062033, 598712.1025062039, 174578.9033792949]
[2019-03-27 13:13:30,915] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:13:30,919] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.086771e-36 1.000000e+00 0.000000e+00 0.000000e+00 6.809330e-36], sampled 0.9343961967916513
[2019-03-27 13:13:54,131] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06177285], dtype=float32), 0.052605525]
[2019-03-27 13:13:54,132] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.53333333333333, 72.0, 1.0, 2.0, 0.5709101492202295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797795.4423448507, 797795.4423448507, 195451.2429383395]
[2019-03-27 13:13:54,136] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:13:54,139] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.2677270e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.2284244e-32], sampled 0.2387743881080927
[2019-03-27 13:14:45,592] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06177285], dtype=float32), 0.052605525]
[2019-03-27 13:14:45,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.33333333333334, 83.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.940461407551483, 6.9112, 168.9072293732682, 2184423.846554074, 1454255.137422598, 311347.0764316639]
[2019-03-27 13:14:45,597] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:14:45,600] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.0203004e-25 1.0000000e+00 7.7961109e-34 6.3744867e-28 9.6651140e-25], sampled 0.807770578550844
[2019-03-27 13:14:45,603] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2184423.846554074 W.
[2019-03-27 13:14:47,514] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06177285], dtype=float32), 0.052605525]
[2019-03-27 13:14:47,515] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.47345593333333, 82.52132745666665, 1.0, 2.0, 0.4992729788971771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 697656.0833540206, 697656.0833540199, 183481.2298864344]
[2019-03-27 13:14:47,516] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:14:47,519] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.1179328e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3134099e-34], sampled 0.3817004616911336
[2019-03-27 13:14:56,350] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06177285], dtype=float32), 0.052605525]
[2019-03-27 13:14:56,351] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.43333333333333, 83.0, 1.0, 2.0, 0.7642823916688315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1068151.599059242, 1068151.599059242, 235331.2969281832]
[2019-03-27 13:14:56,353] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:14:56,356] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0628495e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4727457e-33], sampled 0.35304502822619743
[2019-03-27 13:14:57,193] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2953 3007596012.4260 1766.0000
[2019-03-27 13:14:57,362] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-27 13:14:57,645] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 13:14:57,750] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-27 13:14:57,806] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6673 3164067602.5339 1778.0000
[2019-03-27 13:14:58,824] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1050000, evaluation results [1050000.0, 7882.66734019175, 3164067602.533886, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7998.295259250187, 3007596012.4259605, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-27 13:14:59,223] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.1702193e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4792288e-37], sum to 1.0000
[2019-03-27 13:14:59,233] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3281
[2019-03-27 13:14:59,237] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 85.0, 1.0, 2.0, 0.4019174482743009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593008.380844887, 593008.3808448864, 173690.3280106127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7502400.0000, 
sim time next is 7503000.0000, 
raw observation next is [24.45, 85.33333333333334, 1.0, 2.0, 0.402064948367434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593435.8710656009, 593435.8710656002, 173736.6906591997], 
processed observation next is [0.0, 0.8695652173913043, 0.3578199052132702, 0.8533333333333334, 1.0, 1.0, 0.2795963233342578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16484329751822246, 0.16484329751822227, 0.25930849352119356], 
reward next is 0.7407, 
noisyNet noise sample is [array([1.4887744], dtype=float32), 0.0879318]. 
=============================================
[2019-03-27 13:14:59,256] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[82.58155]
 [82.39503]
 [82.31734]
 [82.22822]
 [82.12795]], R is [[82.59281921]
 [82.50765228]
 [82.42324066]
 [82.33956146]
 [82.25662231]].
[2019-03-27 13:15:01,419] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.4487622e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1852448e-36], sum to 1.0000
[2019-03-27 13:15:01,427] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2305
[2019-03-27 13:15:01,435] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 90.5, 1.0, 2.0, 0.3755939519671112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566659.9541613702, 566659.9541613702, 171722.8529146654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7533000.0000, 
sim time next is 7533600.0000, 
raw observation next is [23.06666666666667, 90.33333333333334, 1.0, 2.0, 0.3731491645966507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563972.4160132934, 563972.4160132934, 171520.1497598521], 
processed observation next is [0.0, 0.17391304347826086, 0.29225908372827825, 0.9033333333333334, 1.0, 1.0, 0.24475802963451893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15665900444813707, 0.15665900444813707, 0.2560002235221673], 
reward next is 0.7440, 
noisyNet noise sample is [array([-2.6183379], dtype=float32), -1.0496304]. 
=============================================
[2019-03-27 13:15:04,024] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.10705419e-17 1.00000000e+00 5.66140327e-25 1.20194356e-17
 1.13710017e-18], sum to 1.0000
[2019-03-27 13:15:04,035] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0598
[2019-03-27 13:15:04,046] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2128895.342086634 W.
[2019-03-27 13:15:04,052] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.9, 71.0, 1.0, 2.0, 0.8813238192257509, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.987514617024613, 6.9112, 168.912440581972, 2128895.342086634, 2074755.291589069, 429610.452998361], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7725600.0000, 
sim time next is 7726200.0000, 
raw observation next is [30.1, 70.0, 1.0, 2.0, 0.7202196281742903, 1.0, 1.0, 0.7202196281742903, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2014018.1731268, 2014018.1731268, 382899.2532405896], 
processed observation next is [1.0, 0.43478260869565216, 0.6255924170616115, 0.7, 1.0, 1.0, 0.6629152146678197, 1.0, 0.5, 0.6629152146678197, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5594494925352222, 0.5594494925352222, 0.5714914227471486], 
reward next is 0.4285, 
noisyNet noise sample is [array([1.9659959], dtype=float32), -1.6227978]. 
=============================================
[2019-03-27 13:15:07,424] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.7696613e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8369858e-33], sum to 1.0000
[2019-03-27 13:15:07,433] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6050
[2019-03-27 13:15:07,442] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 92.66666666666666, 1.0, 2.0, 0.4939262784365839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712060.7058499899, 712060.7058499905, 185430.0023283002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7627200.0000, 
sim time next is 7627800.0000, 
raw observation next is [24.2, 92.33333333333333, 1.0, 2.0, 0.5029131377737938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723328.4567047626, 723328.4567047626, 186671.3806329673], 
processed observation next is [1.0, 0.2608695652173913, 0.3459715639810427, 0.9233333333333333, 1.0, 1.0, 0.4011001659925226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20092457130687852, 0.20092457130687852, 0.2786140009447273], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.04457459], dtype=float32), 0.17245232]. 
=============================================
[2019-03-27 13:15:09,948] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.9722139e-16 1.0000000e+00 1.4467996e-23 5.1239371e-14 1.9181972e-16], sum to 1.0000
[2019-03-27 13:15:09,960] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5924
[2019-03-27 13:15:09,969] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1736954.500810243 W.
[2019-03-27 13:15:09,974] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.2, 64.0, 1.0, 2.0, 0.6212209785956618, 1.0, 2.0, 0.6212209785956618, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1736954.500810243, 1736954.500810243, 342215.3884240976], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7657200.0000, 
sim time next is 7657800.0000, 
raw observation next is [30.08333333333334, 64.83333333333334, 1.0, 2.0, 0.5775473882203456, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9754471313399701, 6.911199999999999, 6.9112, 168.912956510431, 1614761.533240282, 1614761.533240283, 347444.8802913173], 
processed observation next is [1.0, 0.6521739130434783, 0.6248025276461299, 0.6483333333333334, 1.0, 1.0, 0.4910209496630669, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.9700574772438658, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4485448703445228, 0.448544870344523, 0.518574448195996], 
reward next is 0.4814, 
noisyNet noise sample is [array([0.89143974], dtype=float32), 1.3180107]. 
=============================================
[2019-03-27 13:15:12,494] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:12,494] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:12,581] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-03-27 13:15:16,073] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.3686997e-28 1.0000000e+00 3.7014307e-38 4.0476167e-38 3.3782767e-29], sum to 1.0000
[2019-03-27 13:15:16,083] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0515
[2019-03-27 13:15:16,089] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 85.0, 1.0, 2.0, 0.6255066348243193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874120.5012076673, 874120.5012076673, 205586.0067350522], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7886400.0000, 
sim time next is 7887000.0000, 
raw observation next is [26.93333333333333, 84.5, 1.0, 2.0, 0.624603450830829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872857.8190646672, 872857.8190646672, 205411.0861903034], 
processed observation next is [1.0, 0.2608695652173913, 0.4755134281200631, 0.845, 1.0, 1.0, 0.5477150010009988, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2424605052957409, 0.2424605052957409, 0.30658371073179613], 
reward next is 0.6934, 
noisyNet noise sample is [array([-1.593521], dtype=float32), -1.6744652]. 
=============================================
[2019-03-27 13:15:16,104] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[59.25332 ]
 [59.277447]
 [59.34783 ]
 [59.763798]
 [59.601566]], R is [[59.33227158]
 [59.4321022 ]
 [59.52856064]
 [59.62675476]
 [59.72993851]].
[2019-03-27 13:15:16,661] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:16,662] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:16,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-03-27 13:15:20,352] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:20,352] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:20,431] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-03-27 13:15:20,646] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:20,646] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:20,706] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-03-27 13:15:21,336] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:21,336] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:21,381] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-03-27 13:15:22,072] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:22,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:22,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-03-27 13:15:22,323] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1059222: loss 0.4631
[2019-03-27 13:15:22,325] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1059223: learning rate 0.0000
[2019-03-27 13:15:23,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:23,090] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:23,164] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-03-27 13:15:23,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2994225e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5934528e-32], sum to 1.0000
[2019-03-27 13:15:23,444] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2597
[2019-03-27 13:15:23,448] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666666, 96.0, 1.0, 2.0, 0.2967270955109366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475323.1578026265, 475323.1578026265, 165322.9709950197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 177000.0000, 
sim time next is 177600.0000, 
raw observation next is [20.13333333333333, 96.0, 1.0, 2.0, 0.2966127936639391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 475464.3975567197, 475464.3975567191, 165335.3061540372], 
processed observation next is [0.0, 0.043478260869565216, 0.15323854660347538, 0.96, 1.0, 1.0, 0.15254553453486638, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13207344376575547, 0.1320734437657553, 0.24676911366274207], 
reward next is 0.7532, 
noisyNet noise sample is [array([-0.9222789], dtype=float32), -1.370244]. 
=============================================
[2019-03-27 13:15:24,733] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:24,733] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:24,820] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-03-27 13:15:24,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:24,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:24,969] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-03-27 13:15:25,557] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1060688: loss 0.4841
[2019-03-27 13:15:25,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1060688: learning rate 0.0000
[2019-03-27 13:15:25,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:25,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:25,805] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-03-27 13:15:27,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:27,059] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:27,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-03-27 13:15:27,573] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:27,574] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:27,640] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-03-27 13:15:27,952] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:27,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:28,012] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-03-27 13:15:28,091] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:28,091] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:28,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-03-27 13:15:28,483] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1062087: loss 0.5595
[2019-03-27 13:15:28,486] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1062087: learning rate 0.0000
[2019-03-27 13:15:28,566] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:28,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:28,630] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-03-27 13:15:28,691] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:15:28,692] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:15:28,701] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1062200: loss 0.4096
[2019-03-27 13:15:28,705] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1062201: learning rate 0.0000
[2019-03-27 13:15:28,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-03-27 13:15:29,272] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1062530: loss 0.1617
[2019-03-27 13:15:29,273] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1062531: learning rate 0.0000
[2019-03-27 13:15:30,094] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1062921: loss 0.0872
[2019-03-27 13:15:30,099] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1062921: learning rate 0.0000
[2019-03-27 13:15:31,394] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1063411: loss 0.1748
[2019-03-27 13:15:31,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1063413: learning rate 0.0000
[2019-03-27 13:15:33,848] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1064353: loss 0.2108
[2019-03-27 13:15:33,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1064353: learning rate 0.0000
[2019-03-27 13:15:34,336] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1064537: loss 0.1605
[2019-03-27 13:15:34,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1064537: learning rate 0.0000
[2019-03-27 13:15:35,578] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1065011: loss 0.1029
[2019-03-27 13:15:35,579] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1065011: learning rate 0.0000
[2019-03-27 13:15:37,290] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5299982e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7030943e-32], sum to 1.0000
[2019-03-27 13:15:37,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4497
[2019-03-27 13:15:37,310] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.61666666666667, 91.00000000000001, 1.0, 2.0, 0.4082551616796137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 624520.2171965144, 624520.2171965144, 177155.8088613652], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 105000.0000, 
sim time next is 105600.0000, 
raw observation next is [22.63333333333334, 91.0, 1.0, 2.0, 0.3737406723380176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 571415.3344075446, 571415.3344075446, 172361.6639726915], 
processed observation next is [1.0, 0.21739130434782608, 0.27172195892575074, 0.91, 1.0, 1.0, 0.24547068956387663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1587264817798735, 0.1587264817798735, 0.25725621488461414], 
reward next is 0.7427, 
noisyNet noise sample is [array([-0.15056278], dtype=float32), -1.4936106]. 
=============================================
[2019-03-27 13:15:37,661] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1065801: loss 0.1746
[2019-03-27 13:15:37,666] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1065804: learning rate 0.0000
[2019-03-27 13:15:38,647] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1066167: loss 0.0420
[2019-03-27 13:15:38,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1066167: learning rate 0.0000
[2019-03-27 13:15:38,876] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1066254: loss 0.0439
[2019-03-27 13:15:38,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1066254: learning rate 0.0000
[2019-03-27 13:15:39,383] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1066446: loss 0.0126
[2019-03-27 13:15:39,387] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1066446: learning rate 0.0000
[2019-03-27 13:15:39,664] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1066551: loss 0.0032
[2019-03-27 13:15:39,667] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1066551: learning rate 0.0000
[2019-03-27 13:15:40,578] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1066898: loss 0.0479
[2019-03-27 13:15:40,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1066898: learning rate 0.0000
[2019-03-27 13:15:40,880] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1067000: loss 0.0102
[2019-03-27 13:15:40,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1067000: learning rate 0.0000
[2019-03-27 13:15:43,519] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1068004: loss 0.0165
[2019-03-27 13:15:43,521] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1068004: learning rate 0.0000
[2019-03-27 13:15:46,011] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4078412e-32 1.0000000e+00 0.0000000e+00 3.2383653e-36 5.9061195e-31], sum to 1.0000
[2019-03-27 13:15:46,022] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5452
[2019-03-27 13:15:46,027] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 53.16666666666667, 1.0, 2.0, 0.5712048465812091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 937928.0739887205, 937928.0739887205, 209564.4470087265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 490200.0000, 
sim time next is 490800.0000, 
raw observation next is [24.8, 53.33333333333334, 1.0, 2.0, 0.5701738353817984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 936418.0374867588, 936418.0374867594, 209354.1819427049], 
processed observation next is [1.0, 0.6956521739130435, 0.3744075829383887, 0.5333333333333334, 1.0, 1.0, 0.48213715106240773, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2601161215240997, 0.2601161215240998, 0.31246892827269385], 
reward next is 0.6875, 
noisyNet noise sample is [array([1.2386196], dtype=float32), -1.7354571]. 
=============================================
[2019-03-27 13:15:47,056] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.5424732e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8903362e-34], sum to 1.0000
[2019-03-27 13:15:47,068] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0564
[2019-03-27 13:15:47,080] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 92.0, 1.0, 2.0, 0.2853967690869965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458531.0327374417, 458531.0327374417, 164169.5043470557], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 264600.0000, 
sim time next is 265200.0000, 
raw observation next is [20.5, 92.0, 1.0, 2.0, 0.2851647242851123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458158.2345127119, 458158.2345127119, 164144.1567448862], 
processed observation next is [0.0, 0.043478260869565216, 0.1706161137440759, 0.92, 1.0, 1.0, 0.13875267986158107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12726617625353107, 0.12726617625353107, 0.24499127872371074], 
reward next is 0.7550, 
noisyNet noise sample is [array([-0.09945843], dtype=float32), -0.91157794]. 
=============================================
[2019-03-27 13:15:48,397] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1069852: loss 0.0316
[2019-03-27 13:15:48,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1069854: learning rate 0.0000
[2019-03-27 13:15:48,906] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1070047: loss 0.0099
[2019-03-27 13:15:48,910] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1070047: learning rate 0.0000
[2019-03-27 13:15:49,506] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4496470e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7368592e-34], sum to 1.0000
[2019-03-27 13:15:49,514] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9910
[2019-03-27 13:15:49,522] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.25, 79.0, 1.0, 2.0, 0.3317843451468782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541376.6389771572, 541376.6389771566, 170069.3708773235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 376200.0000, 
sim time next is 376800.0000, 
raw observation next is [21.33333333333333, 78.66666666666667, 1.0, 2.0, 0.3587676915180267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585056.6533695918, 585056.6533695924, 173609.863459545], 
processed observation next is [1.0, 0.34782608695652173, 0.21011058451816728, 0.7866666666666667, 1.0, 1.0, 0.2274309536361767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16251573704710884, 0.162515737047109, 0.25911919919335075], 
reward next is 0.7409, 
noisyNet noise sample is [array([0.87083334], dtype=float32), -0.7558288]. 
=============================================
[2019-03-27 13:15:50,274] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1070554: loss 0.0064
[2019-03-27 13:15:50,281] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1070555: learning rate 0.0000
[2019-03-27 13:15:51,100] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1070866: loss 0.0024
[2019-03-27 13:15:51,101] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1070866: learning rate 0.0000
[2019-03-27 13:15:52,185] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.9952458e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6910091e-33], sum to 1.0000
[2019-03-27 13:15:52,194] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6592
[2019-03-27 13:15:52,200] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 70.5, 1.0, 2.0, 0.2405443348468091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397198.3745515567, 397198.3745515567, 159938.2199016922], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 501000.0000, 
sim time next is 501600.0000, 
raw observation next is [21.23333333333333, 72.0, 1.0, 2.0, 0.2397650428501931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 395975.6637060771, 395975.6637060766, 159860.6397402199], 
processed observation next is [1.0, 0.8260869565217391, 0.2053712480252764, 0.72, 1.0, 1.0, 0.08405426849420854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10999323991835475, 0.10999323991835462, 0.23859796976152223], 
reward next is 0.7614, 
noisyNet noise sample is [array([-1.1816229], dtype=float32), -0.3924606]. 
=============================================
[2019-03-27 13:15:52,578] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1071435: loss 0.0025
[2019-03-27 13:15:52,583] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1071436: learning rate 0.0000
[2019-03-27 13:15:55,115] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1072394: loss 0.0050
[2019-03-27 13:15:55,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1072395: learning rate 0.0000
[2019-03-27 13:15:55,391] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1072499: loss 0.0026
[2019-03-27 13:15:55,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1072499: learning rate 0.0000
[2019-03-27 13:15:56,775] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1072998: loss 0.0133
[2019-03-27 13:15:56,778] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1072998: learning rate 0.0000
[2019-03-27 13:15:58,911] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1073802: loss 0.0018
[2019-03-27 13:15:58,915] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1073804: learning rate 0.0000
[2019-03-27 13:15:59,819] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1074140: loss 0.0155
[2019-03-27 13:15:59,820] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1074140: learning rate 0.0000
[2019-03-27 13:16:00,299] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1074316: loss 0.0960
[2019-03-27 13:16:00,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1074316: learning rate 0.0000
[2019-03-27 13:16:00,635] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1074444: loss 0.0029
[2019-03-27 13:16:00,639] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1074445: learning rate 0.0000
[2019-03-27 13:16:00,911] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1074548: loss 0.0021
[2019-03-27 13:16:00,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1074548: learning rate 0.0000
[2019-03-27 13:16:00,957] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7440907e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1520393e-35], sum to 1.0000
[2019-03-27 13:16:00,970] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5751
[2019-03-27 13:16:00,974] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.06666666666667, 87.0, 1.0, 2.0, 0.211391117839434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 352652.3475898298, 352652.3475898298, 156682.8825518083], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 625800.0000, 
sim time next is 626400.0000, 
raw observation next is [18.3, 86.0, 1.0, 2.0, 0.2135148031529875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 355961.443750149, 355961.4437501484, 156949.4386614617], 
processed observation next is [1.0, 0.2608695652173913, 0.06635071090047404, 0.86, 1.0, 1.0, 0.05242747367829816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09887817881948582, 0.09887817881948567, 0.23425289352456968], 
reward next is 0.7657, 
noisyNet noise sample is [array([-1.9528303], dtype=float32), 1.0364823]. 
=============================================
[2019-03-27 13:16:01,876] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1074914: loss 0.0127
[2019-03-27 13:16:01,882] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1074915: learning rate 0.0000
[2019-03-27 13:16:02,018] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1074970: loss 0.0106
[2019-03-27 13:16:02,022] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1074971: learning rate 0.0000
[2019-03-27 13:16:02,098] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 13:16:02,100] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:16:02,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:16:02,103] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:16:02,104] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:16:02,104] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:16:02,105] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:16:02,106] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:16:02,107] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:16:02,108] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:16:02,109] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:16:02,133] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run44
[2019-03-27 13:16:02,134] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run44
[2019-03-27 13:16:02,172] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run44
[2019-03-27 13:16:02,194] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run44
[2019-03-27 13:16:02,211] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run44
[2019-03-27 13:16:21,688] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06173533], dtype=float32), 0.059189912]
[2019-03-27 13:16:21,690] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.01287636, 91.13028815333334, 1.0, 2.0, 0.2304356694418875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381568.9888181655, 381568.9888181655, 158915.7303249729]
[2019-03-27 13:16:21,691] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:16:21,693] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.5560544e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7707104e-37], sampled 0.3849435772237846
[2019-03-27 13:16:37,083] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06173533], dtype=float32), 0.059189912]
[2019-03-27 13:16:37,084] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.1, 89.0, 1.0, 2.0, 0.4695110370955941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661882.6535466363, 661882.6535466369, 179708.3799031122]
[2019-03-27 13:16:37,084] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:16:37,089] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.3399498e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8638496e-36], sampled 0.3106640131840569
[2019-03-27 13:17:08,569] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06173533], dtype=float32), 0.059189912]
[2019-03-27 13:17:08,570] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.205612135, 96.34089401, 1.0, 2.0, 0.5781506269043221, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004056441598824, 6.9112, 6.9112, 168.9128793239793, 1616449.4098356, 1616449.4098356, 353811.1308909917]
[2019-03-27 13:17:08,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:17:08,575] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4865025e-27 1.0000000e+00 3.0113347e-36 9.8168057e-30 6.6113747e-27], sampled 0.4447758619387695
[2019-03-27 13:17:15,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06173533], dtype=float32), 0.059189912]
[2019-03-27 13:17:15,981] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.653206115, 62.25087354166666, 1.0, 2.0, 0.514997561623894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719636.1768927713, 719636.1768927713, 185978.7557154977]
[2019-03-27 13:17:15,985] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:17:15,988] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.4862184e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.4585804e-36], sampled 0.5183096319852146
[2019-03-27 13:18:09,490] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06173533], dtype=float32), 0.059189912]
[2019-03-27 13:18:09,495] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.53333333333333, 65.0, 1.0, 2.0, 0.7115610528041323, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.979912583764992, 6.9112, 168.9124897733965, 1891306.922935534, 1842559.985805, 388102.8777806655]
[2019-03-27 13:18:09,495] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:18:09,499] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.3415391e-27 1.0000000e+00 2.1434028e-36 6.0162787e-28 1.4241388e-26], sampled 0.6876536663758982
[2019-03-27 13:18:09,502] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1891306.922935534 W.
[2019-03-27 13:18:09,990] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-27 13:18:10,409] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 13:18:10,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4791 3007718786.5045 1766.0000
[2019-03-27 13:18:10,689] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 13:18:10,894] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-27 13:18:11,911] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1075000, evaluation results [1075000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7997.479055512932, 3007718786.504511, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-27 13:18:12,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 13:18:12,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8804
[2019-03-27 13:18:12,884] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 93.0, 1.0, 2.0, 0.2612251365279191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 426841.1356951171, 426841.1356951177, 162012.107858232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 794400.0000, 
sim time next is 795000.0000, 
raw observation next is [19.4, 93.0, 1.0, 2.0, 0.2613040230641719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 426970.1042571639, 426970.1042571633, 162020.1651532058], 
processed observation next is [0.0, 0.17391304347826086, 0.11848341232227487, 0.93, 1.0, 1.0, 0.11000484706526732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11860280673810109, 0.11860280673810092, 0.24182114201971014], 
reward next is 0.7582, 
noisyNet noise sample is [array([1.1320144], dtype=float32), -0.6730981]. 
=============================================
[2019-03-27 13:18:12,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[78.39571 ]
 [78.46259 ]
 [78.49844 ]
 [78.54555 ]
 [78.527084]], R is [[78.35681915]
 [78.33144379]
 [78.3062973 ]
 [78.28137207]
 [78.25670624]].
[2019-03-27 13:18:14,663] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1076046: loss 0.1649
[2019-03-27 13:18:14,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1076046: learning rate 0.0000
[2019-03-27 13:18:18,659] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.64078553e-35 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.47189865e-33], sum to 1.0000
[2019-03-27 13:18:18,670] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1693
[2019-03-27 13:18:18,674] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.58333333333334, 92.0, 1.0, 2.0, 0.2294932097526493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 382569.6950886355, 382569.6950886361, 158371.4658510725], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 706200.0000, 
sim time next is 706800.0000, 
raw observation next is [17.56666666666667, 92.0, 1.0, 2.0, 0.2219362550021812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 370005.9444047123, 370005.9444047123, 157681.2467350347], 
processed observation next is [1.0, 0.17391304347826086, 0.03159557661927352, 0.92, 1.0, 1.0, 0.06257380120744721, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10277942900130897, 0.10277942900130897, 0.23534514438064882], 
reward next is 0.7647, 
noisyNet noise sample is [array([-1.0015575], dtype=float32), 2.2763498]. 
=============================================
[2019-03-27 13:18:19,382] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1077841: loss 0.1615
[2019-03-27 13:18:19,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1077841: learning rate 0.0000
[2019-03-27 13:18:19,901] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1078022: loss 0.2459
[2019-03-27 13:18:19,908] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1078022: learning rate 0.0000
[2019-03-27 13:18:21,267] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1078544: loss 0.2286
[2019-03-27 13:18:21,271] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1078544: learning rate 0.0000
[2019-03-27 13:18:22,191] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1078899: loss 0.2653
[2019-03-27 13:18:22,194] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1078899: learning rate 0.0000
[2019-03-27 13:18:23,589] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1079423: loss 0.2873
[2019-03-27 13:18:23,592] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1079423: learning rate 0.0000
[2019-03-27 13:18:26,005] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1080333: loss 0.4202
[2019-03-27 13:18:26,007] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1080334: learning rate 0.0000
[2019-03-27 13:18:26,534] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1080534: loss 0.4700
[2019-03-27 13:18:26,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1080535: learning rate 0.0000
[2019-03-27 13:18:27,774] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1081008: loss 0.3347
[2019-03-27 13:18:27,776] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1081009: learning rate 0.0000
[2019-03-27 13:18:29,829] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1081790: loss 0.3375
[2019-03-27 13:18:29,835] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1081790: learning rate 0.0000
[2019-03-27 13:18:30,674] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1082107: loss 0.4558
[2019-03-27 13:18:30,676] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1082107: learning rate 0.0000
[2019-03-27 13:18:31,196] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1082297: loss 0.0035
[2019-03-27 13:18:31,200] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1082297: learning rate 0.0000
[2019-03-27 13:18:31,476] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2553143e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.9584450e-37], sum to 1.0000
[2019-03-27 13:18:31,486] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8652
[2019-03-27 13:18:31,490] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.56666666666667, 83.0, 1.0, 2.0, 0.2836067054072956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456236.4502784533, 456236.4502784533, 164016.7009061235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 886800.0000, 
sim time next is 887400.0000, 
raw observation next is [21.65, 82.5, 1.0, 2.0, 0.2830539234658758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 455198.6208543547, 455198.6208543541, 163945.9648696177], 
processed observation next is [0.0, 0.2608695652173913, 0.22511848341232227, 0.825, 1.0, 1.0, 0.1362095463444287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12644406134843186, 0.1264440613484317, 0.24469546995465327], 
reward next is 0.7553, 
noisyNet noise sample is [array([0.09956089], dtype=float32), 0.7216552]. 
=============================================
[2019-03-27 13:18:31,632] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1082459: loss 0.6198
[2019-03-27 13:18:31,634] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1082459: learning rate 0.0000
[2019-03-27 13:18:31,886] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1082565: loss 0.5602
[2019-03-27 13:18:31,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1082566: learning rate 0.0000
[2019-03-27 13:18:32,926] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1082956: loss 0.5054
[2019-03-27 13:18:32,928] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1082956: learning rate 0.0000
[2019-03-27 13:18:32,947] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1082963: loss 0.5757
[2019-03-27 13:18:32,949] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1082964: learning rate 0.0000
[2019-03-27 13:18:35,625] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8113469e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0072301e-35], sum to 1.0000
[2019-03-27 13:18:35,638] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1058
[2019-03-27 13:18:35,642] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 70.0, 1.0, 2.0, 0.2890485833937786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463072.8546722764, 463072.8546722764, 164469.9306947154], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 811800.0000, 
sim time next is 812400.0000, 
raw observation next is [23.83333333333334, 69.0, 1.0, 2.0, 0.2896023743382179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463720.3568616569, 463720.3568616569, 164512.0927808028], 
processed observation next is [0.0, 0.391304347826087, 0.32859399684044266, 0.69, 1.0, 1.0, 0.14409924619062395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12881121023934913, 0.12881121023934913, 0.24554043698627284], 
reward next is 0.7545, 
noisyNet noise sample is [array([1.2454805], dtype=float32), 0.054278236]. 
=============================================
[2019-03-27 13:18:35,815] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1084051: loss 0.0070
[2019-03-27 13:18:35,817] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1084052: learning rate 0.0000
[2019-03-27 13:18:36,058] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6699334e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.0359351e-35], sum to 1.0000
[2019-03-27 13:18:36,066] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6326
[2019-03-27 13:18:36,073] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.13333333333333, 61.33333333333334, 1.0, 2.0, 0.2899116500588187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463928.3928886835, 463928.3928886829, 164523.2570659499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 818400.0000, 
sim time next is 819000.0000, 
raw observation next is [25.1, 61.5, 1.0, 2.0, 0.2897228075243004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463660.3177618643, 463660.3177618649, 164505.1470210628], 
processed observation next is [0.0, 0.4782608695652174, 0.38862559241706174, 0.615, 1.0, 1.0, 0.14424434641481976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12879453271162897, 0.12879453271162913, 0.24553007018069073], 
reward next is 0.7545, 
noisyNet noise sample is [array([1.2879303], dtype=float32), -2.1567712]. 
=============================================
[2019-03-27 13:18:36,100] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[77.957016]
 [77.9266  ]
 [77.87755 ]
 [77.76413 ]
 [77.75313 ]], R is [[77.96161652]
 [77.93643951]
 [77.91148376]
 [77.88671875]
 [77.86209869]].
[2019-03-27 13:18:38,064] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1448277e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.7781763e-36], sum to 1.0000
[2019-03-27 13:18:38,076] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7559
[2019-03-27 13:18:38,080] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 89.0, 1.0, 2.0, 0.3087097295269451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489279.8158706811, 489279.8158706804, 166253.6816097139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 862200.0000, 
sim time next is 862800.0000, 
raw observation next is [21.46666666666667, 89.0, 1.0, 2.0, 0.3079426918569733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488430.3244859311, 488430.3244859317, 166198.4926851338], 
processed observation next is [0.0, 1.0, 0.21642969984202226, 0.89, 1.0, 1.0, 0.16619601428551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13567509013498086, 0.13567509013498102, 0.2480574517688564], 
reward next is 0.7519, 
noisyNet noise sample is [array([1.8246635], dtype=float32), 0.5189607]. 
=============================================
[2019-03-27 13:18:40,528] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1085836: loss 0.0460
[2019-03-27 13:18:40,531] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1085836: learning rate 0.0000
[2019-03-27 13:18:41,051] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1086033: loss 0.0100
[2019-03-27 13:18:41,055] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1086033: learning rate 0.0000
[2019-03-27 13:18:42,482] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1086576: loss 0.0016
[2019-03-27 13:18:42,483] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1086576: learning rate 0.0000
[2019-03-27 13:18:43,380] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1086912: loss 0.0052
[2019-03-27 13:18:43,383] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1086912: learning rate 0.0000
[2019-03-27 13:18:44,592] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1087379: loss 0.0032
[2019-03-27 13:18:44,595] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1087379: learning rate 0.0000
[2019-03-27 13:18:47,078] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1088335: loss 0.0040
[2019-03-27 13:18:47,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1088335: learning rate 0.0000
[2019-03-27 13:18:47,607] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1088530: loss 0.0153
[2019-03-27 13:18:47,611] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1088531: learning rate 0.0000
[2019-03-27 13:18:48,904] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1089013: loss 0.0121
[2019-03-27 13:18:48,908] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1089013: learning rate 0.0000
[2019-03-27 13:18:50,943] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1089778: loss 0.0054
[2019-03-27 13:18:50,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1089778: learning rate 0.0000
[2019-03-27 13:18:51,150] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9011694e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0936533e-36], sum to 1.0000
[2019-03-27 13:18:51,159] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7998
[2019-03-27 13:18:51,169] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 85.66666666666667, 1.0, 2.0, 0.3170925447827462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506436.7575140374, 506436.7575140367, 167575.0214745703], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1149600.0000, 
sim time next is 1150200.0000, 
raw observation next is [21.75, 85.0, 1.0, 2.0, 0.3143736639134423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501437.1411009541, 501437.1411009541, 167196.2687153479], 
processed observation next is [1.0, 0.30434782608695654, 0.2298578199052133, 0.85, 1.0, 1.0, 0.1739441733896895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13928809475026502, 0.13928809475026502, 0.24954666972439984], 
reward next is 0.7505, 
noisyNet noise sample is [array([-1.4270037], dtype=float32), -2.1661408]. 
=============================================
[2019-03-27 13:18:51,721] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1090071: loss 0.0316
[2019-03-27 13:18:51,726] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1090072: learning rate 0.0000
[2019-03-27 13:18:52,539] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1090384: loss 0.4144
[2019-03-27 13:18:52,546] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1090385: learning rate 0.0000
[2019-03-27 13:18:52,627] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1090422: loss 0.0070
[2019-03-27 13:18:52,630] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1090422: learning rate 0.0000
[2019-03-27 13:18:52,893] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1090522: loss 0.0011
[2019-03-27 13:18:52,897] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1090522: learning rate 0.0000
[2019-03-27 13:18:54,054] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1090960: loss 0.0011
[2019-03-27 13:18:54,056] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1090961: learning rate 0.0000
[2019-03-27 13:18:54,213] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1091015: loss 0.0043
[2019-03-27 13:18:54,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1091015: learning rate 0.0000
[2019-03-27 13:18:56,454] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3221753e-29 1.0000000e+00 0.0000000e+00 1.2662962e-36 5.4366802e-30], sum to 1.0000
[2019-03-27 13:18:56,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6087
[2019-03-27 13:18:56,465] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.61666666666667, 68.83333333333333, 1.0, 2.0, 0.6240442131922198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 959612.0107939396, 959612.0107939389, 216011.5901053671], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1087800.0000, 
sim time next is 1088400.0000, 
raw observation next is [25.63333333333334, 68.66666666666667, 1.0, 2.0, 0.6260929258838839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 963043.2271748716, 963043.2271748709, 216480.3294812276], 
processed observation next is [1.0, 0.6086956521739131, 0.4139020537124806, 0.6866666666666668, 1.0, 1.0, 0.5495095492576914, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26751200754857546, 0.26751200754857524, 0.3231049693749666], 
reward next is 0.6769, 
noisyNet noise sample is [array([-1.4900764], dtype=float32), 0.15087824]. 
=============================================
[2019-03-27 13:18:57,285] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1092185: loss 0.3349
[2019-03-27 13:18:57,287] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1092185: learning rate 0.0000
[2019-03-27 13:19:01,843] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1093910: loss 0.4000
[2019-03-27 13:19:01,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1093910: learning rate 0.0000
[2019-03-27 13:19:02,294] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1094080: loss 0.3447
[2019-03-27 13:19:02,299] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1094081: learning rate 0.0000
[2019-03-27 13:19:03,523] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1094538: loss 0.3007
[2019-03-27 13:19:03,527] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1094539: learning rate 0.0000
[2019-03-27 13:19:04,591] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1094946: loss 0.3335
[2019-03-27 13:19:04,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1094946: learning rate 0.0000
[2019-03-27 13:19:05,775] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1095394: loss 0.4080
[2019-03-27 13:19:05,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1095395: learning rate 0.0000
[2019-03-27 13:19:08,338] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1096381: loss 0.4699
[2019-03-27 13:19:08,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1096381: learning rate 0.0000
[2019-03-27 13:19:08,751] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1096536: loss 0.6572
[2019-03-27 13:19:08,755] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1096536: learning rate 0.0000
[2019-03-27 13:19:09,572] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1375076e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.4056963e-33], sum to 1.0000
[2019-03-27 13:19:09,581] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9159
[2019-03-27 13:19:09,594] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 71.33333333333334, 1.0, 2.0, 0.425355337139168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 615654.7900610234, 615654.7900610229, 175472.2527111693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1444800.0000, 
sim time next is 1445400.0000, 
raw observation next is [26.85, 72.5, 1.0, 2.0, 0.4235511067033637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614299.4433071408, 614299.4433071408, 175378.5844271042], 
processed observation next is [0.0, 0.7391304347826086, 0.4715639810426541, 0.725, 1.0, 1.0, 0.30548326108838997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17063873425198356, 0.17063873425198356, 0.2617590812344839], 
reward next is 0.7382, 
noisyNet noise sample is [array([1.3424327], dtype=float32), 0.7853596]. 
=============================================
[2019-03-27 13:19:10,182] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1097073: loss 0.4465
[2019-03-27 13:19:10,184] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1097074: learning rate 0.0000
[2019-03-27 13:19:12,153] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1097816: loss 0.3173
[2019-03-27 13:19:12,155] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1097816: learning rate 0.0000
[2019-03-27 13:19:12,776] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1098050: loss 0.3584
[2019-03-27 13:19:12,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1098052: learning rate 0.0000
[2019-03-27 13:19:13,424] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1098290: loss 0.0115
[2019-03-27 13:19:13,427] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1098291: learning rate 0.0000
[2019-03-27 13:19:13,695] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1098391: loss 0.3919
[2019-03-27 13:19:13,697] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1098392: learning rate 0.0000
[2019-03-27 13:19:14,022] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1098514: loss 0.3472
[2019-03-27 13:19:14,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1098516: learning rate 0.0000
[2019-03-27 13:19:15,144] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1098941: loss 0.3685
[2019-03-27 13:19:15,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1098941: learning rate 0.0000
[2019-03-27 13:19:15,430] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1099047: loss 0.2872
[2019-03-27 13:19:15,434] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1099048: learning rate 0.0000
[2019-03-27 13:19:17,909] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 13:19:17,912] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:19:17,913] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:19:17,913] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:19:17,914] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:19:17,917] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:19:17,918] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:19:17,921] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:19:17,922] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:19:17,924] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:19:17,925] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:19:17,952] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run45
[2019-03-27 13:19:17,974] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run45
[2019-03-27 13:19:17,992] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run45
[2019-03-27 13:19:18,011] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run45
[2019-03-27 13:19:18,012] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run45
[2019-03-27 13:19:39,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06013662], dtype=float32), 0.05958693]
[2019-03-27 13:19:39,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.37946933, 76.819461875, 1.0, 2.0, 0.9024599996079256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1261381.618433338, 1261381.618433338, 270619.2237965109]
[2019-03-27 13:19:39,103] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:19:39,106] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0498441e-29 1.0000000e+00 0.0000000e+00 1.3593061e-34 2.8208906e-29], sampled 0.6872475905401748
[2019-03-27 13:19:40,355] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06013662], dtype=float32), 0.05958693]
[2019-03-27 13:19:40,356] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.836731625, 95.91263086000001, 1.0, 2.0, 0.4662133937824632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696287.8947730745, 696287.8947730745, 184046.5783735113]
[2019-03-27 13:19:40,357] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:19:40,361] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.746073e-34 1.000000e+00 0.000000e+00 0.000000e+00 4.660941e-33], sampled 0.23037892821161743
[2019-03-27 13:19:52,286] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06013662], dtype=float32), 0.05958693]
[2019-03-27 13:19:52,287] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.21666666666667, 90.83333333333334, 1.0, 2.0, 0.4914706646750814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686750.0424878362, 686750.0424878356, 182269.7393594605]
[2019-03-27 13:19:52,288] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:19:52,292] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1735139e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.1067427e-32], sampled 0.7409623994992101
[2019-03-27 13:20:17,710] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06013662], dtype=float32), 0.05958693]
[2019-03-27 13:20:17,711] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.71666666666667, 74.83333333333333, 1.0, 2.0, 0.8669736365041073, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.995807320859271, 6.9112, 168.9123841254832, 2108809.744471867, 2048786.602492468, 425448.0652870184]
[2019-03-27 13:20:17,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:20:17,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7240013e-21 1.0000000e+00 7.4135634e-30 5.3067138e-21 4.0697900e-22], sampled 0.9077696598675129
[2019-03-27 13:20:17,717] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2108809.744471867 W.
[2019-03-27 13:20:35,604] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06013662], dtype=float32), 0.05958693]
[2019-03-27 13:20:35,605] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.36140310833333, 66.92869236666667, 1.0, 2.0, 0.8279250682229593, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00598327239036, 6.9112, 168.912315988471, 2054156.34847668, 1986914.087839708, 414867.6517415535]
[2019-03-27 13:20:35,605] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:20:35,609] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.0744016e-23 1.0000000e+00 7.1440299e-32 6.6130818e-22 2.1725416e-23], sampled 0.04018710668398218
[2019-03-27 13:20:35,611] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2054156.34847668 W.
[2019-03-27 13:20:35,639] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06013662], dtype=float32), 0.05958693]
[2019-03-27 13:20:35,640] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.39013835, 70.450271145, 1.0, 2.0, 0.6686731135005762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 934470.4666501568, 934470.4666501561, 214227.4270789115]
[2019-03-27 13:20:35,642] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:20:35,645] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.5768125e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5333231e-35], sampled 0.523875421464274
[2019-03-27 13:20:57,492] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06013662], dtype=float32), 0.05958693]
[2019-03-27 13:20:57,493] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.54240059333333, 97.80964503333334, 1.0, 2.0, 0.696797381459191, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.979786951698919, 6.9112, 168.9116977551895, 1870647.215063419, 1821989.633551085, 384890.1038118288]
[2019-03-27 13:20:57,494] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:20:57,496] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.4971483e-26 1.0000000e+00 6.3974845e-35 7.7984208e-29 4.7348627e-26], sampled 0.5721177844691783
[2019-03-27 13:20:57,496] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1870647.215063419 W.
[2019-03-27 13:21:06,690] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06013662], dtype=float32), 0.05958693]
[2019-03-27 13:21:06,691] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.86866602333333, 77.22798731, 1.0, 2.0, 0.5207811183717166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727720.646890451, 727720.6468904515, 186915.0019405165]
[2019-03-27 13:21:06,693] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:21:06,698] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4225250e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4368277e-31], sampled 0.5906667550055656
[2019-03-27 13:21:21,829] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06013662], dtype=float32), 0.05958693]
[2019-03-27 13:21:21,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.35, 89.0, 1.0, 2.0, 0.4800516621168904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670788.812251617, 670788.812251617, 180529.1795795794]
[2019-03-27 13:21:21,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:21:21,831] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4836077e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.2080352e-32], sampled 0.9732369184756584
[2019-03-27 13:21:26,007] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1701 3164000012.1395 1778.0000
[2019-03-27 13:21:26,009] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6296 2779265535.0700 933.0000
[2019-03-27 13:21:26,024] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-27 13:21:26,051] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 13:21:26,349] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9689 2927381913.5140 1338.0000
[2019-03-27 13:21:27,364] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1100000, evaluation results [1100000.0, 7884.170123390672, 3164000012.13951, 1778.0, 8254.968948333817, 2927381913.5140486, 1338.0, 8660.629559617946, 2779265535.0700107, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-27 13:21:27,542] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1100072: loss 0.3285
[2019-03-27 13:21:27,545] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1100072: learning rate 0.0000
[2019-03-27 13:21:32,132] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1101811: loss 0.0671
[2019-03-27 13:21:32,136] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1101812: learning rate 0.0000
[2019-03-27 13:21:32,756] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1102050: loss 0.1830
[2019-03-27 13:21:32,761] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1102050: learning rate 0.0000
[2019-03-27 13:21:33,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2126062e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.7857342e-35], sum to 1.0000
[2019-03-27 13:21:33,504] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2231
[2019-03-27 13:21:33,509] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.45, 55.5, 1.0, 2.0, 0.3641859483448473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 554235.9988482147, 554235.9988482147, 170804.1367881757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1524600.0000, 
sim time next is 1525200.0000, 
raw observation next is [28.3, 56.0, 1.0, 2.0, 0.3633649523660085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 553697.7564437406, 553697.7564437406, 170780.7044569965], 
processed observation next is [0.0, 0.6521739130434783, 0.5402843601895735, 0.56, 1.0, 1.0, 0.2329698221277211, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15380493234548348, 0.15380493234548348, 0.2548965738164127], 
reward next is 0.7451, 
noisyNet noise sample is [array([-1.6019839], dtype=float32), 0.16689448]. 
=============================================
[2019-03-27 13:21:33,898] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1102482: loss 0.2341
[2019-03-27 13:21:33,903] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1102484: learning rate 0.0000
[2019-03-27 13:21:34,985] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1102889: loss 0.2149
[2019-03-27 13:21:34,988] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1102890: learning rate 0.0000
[2019-03-27 13:21:36,127] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1103326: loss 0.0021
[2019-03-27 13:21:36,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1103326: learning rate 0.0000
[2019-03-27 13:21:38,806] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1104335: loss 0.1253
[2019-03-27 13:21:38,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1104335: learning rate 0.0000
[2019-03-27 13:21:39,126] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1104458: loss 0.0764
[2019-03-27 13:21:39,132] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1104458: learning rate 0.0000
[2019-03-27 13:21:40,662] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1105024: loss 0.1992
[2019-03-27 13:21:40,669] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1105025: learning rate 0.0000
[2019-03-27 13:21:42,658] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1105781: loss 0.1874
[2019-03-27 13:21:42,660] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1105781: learning rate 0.0000
[2019-03-27 13:21:43,380] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1106052: loss 0.7440
[2019-03-27 13:21:43,383] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1106053: learning rate 0.0000
[2019-03-27 13:21:44,238] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1106373: loss 0.6255
[2019-03-27 13:21:44,244] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1106374: learning rate 0.0000
[2019-03-27 13:21:44,252] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1106377: loss 24.9809
[2019-03-27 13:21:44,257] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1106377: learning rate 0.0000
[2019-03-27 13:21:44,367] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1106419: loss 0.5796
[2019-03-27 13:21:44,369] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1106419: learning rate 0.0000
[2019-03-27 13:21:45,791] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1106958: loss 0.0810
[2019-03-27 13:21:45,796] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1106960: learning rate 0.0000
[2019-03-27 13:21:45,845] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1106977: loss 0.0733
[2019-03-27 13:21:45,847] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1106977: learning rate 0.0000
[2019-03-27 13:21:47,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4912480e-28 1.0000000e+00 4.3604381e-37 9.4005330e-34 2.1274537e-28], sum to 1.0000
[2019-03-27 13:21:47,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6597
[2019-03-27 13:21:47,798] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 93.83333333333334, 1.0, 2.0, 0.3459735753827479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535856.5860672527, 535856.5860672527, 169561.2748346665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1818600.0000, 
sim time next is 1819200.0000, 
raw observation next is [21.86666666666667, 93.66666666666667, 1.0, 2.0, 0.3460386756776163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535768.6817740325, 535768.6817740325, 169548.8740903079], 
processed observation next is [1.0, 0.043478260869565216, 0.23538704581358633, 0.9366666666666668, 1.0, 1.0, 0.2120947899730317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14882463382612016, 0.14882463382612016, 0.2530580210303103], 
reward next is 0.7469, 
noisyNet noise sample is [array([1.3430146], dtype=float32), -0.013377249]. 
=============================================
[2019-03-27 13:21:48,866] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1108125: loss -149.2094
[2019-03-27 13:21:48,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1108126: learning rate 0.0000
[2019-03-27 13:21:53,635] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1109925: loss 10.8554
[2019-03-27 13:21:53,640] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1109925: learning rate 0.0000
[2019-03-27 13:21:54,301] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1110178: loss -11.5902
[2019-03-27 13:21:54,303] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1110178: learning rate 0.0000
[2019-03-27 13:21:55,331] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1110567: loss -75.4267
[2019-03-27 13:21:55,336] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1110568: learning rate 0.0000
[2019-03-27 13:21:56,382] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1110970: loss -77.0466
[2019-03-27 13:21:56,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1110972: learning rate 0.0000
[2019-03-27 13:21:57,047] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.1150850e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2284629e-32], sum to 1.0000
[2019-03-27 13:21:57,061] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0321
[2019-03-27 13:21:57,070] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 97.66666666666667, 1.0, 2.0, 0.4643511869416615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655091.546806363, 655091.546806363, 179005.5196459925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2004600.0000, 
sim time next is 2005200.0000, 
raw observation next is [23.9, 98.0, 1.0, 2.0, 0.4633728702384586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653961.8516538395, 653961.8516538395, 178893.4476831057], 
processed observation next is [0.0, 0.21739130434782608, 0.33175355450236965, 0.98, 1.0, 1.0, 0.353461289443926, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1816560699038443, 0.1816560699038443, 0.2670051457956802], 
reward next is 0.7330, 
noisyNet noise sample is [array([-0.24847579], dtype=float32), 0.772896]. 
=============================================
[2019-03-27 13:21:57,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.22854015e-18 1.00000000e+00 7.33781207e-25 1.17339445e-11
 6.74090367e-18], sum to 1.0000
[2019-03-27 13:21:57,554] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1930
[2019-03-27 13:21:57,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1709783.906237642 W.
[2019-03-27 13:21:57,572] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.03333333333333, 84.33333333333334, 1.0, 2.0, 0.6115111742065167, 1.0, 1.0, 0.6115111742065167, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1709783.906237642, 1709783.906237642, 338537.5533788351], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1867200.0000, 
sim time next is 1867800.0000, 
raw observation next is [27.01666666666667, 84.66666666666667, 1.0, 2.0, 0.5966630439058667, 1.0, 2.0, 0.5966630439058667, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1668236.252207693, 1668236.252207694, 333017.7468424011], 
processed observation next is [1.0, 0.6086956521739131, 0.4794628751974725, 0.8466666666666667, 1.0, 1.0, 0.5140518601275502, 1.0, 1.0, 0.5140518601275502, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4633989589465814, 0.4633989589465817, 0.4970414131976136], 
reward next is 0.5030, 
noisyNet noise sample is [array([-0.9915373], dtype=float32), -0.6907842]. 
=============================================
[2019-03-27 13:21:57,679] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1111461: loss -130.5119
[2019-03-27 13:21:57,681] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1111461: learning rate 0.0000
[2019-03-27 13:22:00,164] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1112400: loss -143.8109
[2019-03-27 13:22:00,166] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1112400: learning rate 0.0000
[2019-03-27 13:22:00,660] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1112577: loss -121.2695
[2019-03-27 13:22:00,662] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1112577: learning rate 0.0000
[2019-03-27 13:22:01,863] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1113038: loss -101.6620
[2019-03-27 13:22:01,866] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1113039: learning rate 0.0000
[2019-03-27 13:22:04,048] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1113863: loss -89.0068
[2019-03-27 13:22:04,049] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1113863: learning rate 0.0000
[2019-03-27 13:22:04,607] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1114071: loss -78.2969
[2019-03-27 13:22:04,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1114071: learning rate 0.0000
[2019-03-27 13:22:05,414] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1114380: loss 1.1433
[2019-03-27 13:22:05,418] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1114380: learning rate 0.0000
[2019-03-27 13:22:05,519] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1114416: loss -115.9596
[2019-03-27 13:22:05,521] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1114417: learning rate 0.0000
[2019-03-27 13:22:05,720] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1114492: loss -167.7047
[2019-03-27 13:22:05,723] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1114492: learning rate 0.0000
[2019-03-27 13:22:07,085] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1115014: loss -87.4456
[2019-03-27 13:22:07,086] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1115014: loss -141.9905
[2019-03-27 13:22:07,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1115017: learning rate 0.0000
[2019-03-27 13:22:07,097] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1115017: learning rate 0.0000
[2019-03-27 13:22:09,985] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1116099: loss 1.6575
[2019-03-27 13:22:09,991] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1116101: learning rate 0.0000
[2019-03-27 13:22:10,433] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2418940e-28 1.0000000e+00 5.3534785e-36 3.1844010e-30 8.4619499e-27], sum to 1.0000
[2019-03-27 13:22:10,444] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4545
[2019-03-27 13:22:10,450] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.65, 84.5, 1.0, 2.0, 0.7342333500924266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1026135.140452633, 1026135.140452633, 228408.1018651827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2187000.0000, 
sim time next is 2187600.0000, 
raw observation next is [27.86666666666667, 83.66666666666667, 1.0, 2.0, 0.7490072873415591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1046792.78035397, 1046792.780353971, 231779.3596278974], 
processed observation next is [1.0, 0.30434782608695654, 0.519747235387046, 0.8366666666666667, 1.0, 1.0, 0.6975991413753724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29077577232054724, 0.29077577232054747, 0.34593934272820503], 
reward next is 0.6541, 
noisyNet noise sample is [array([-1.3367823], dtype=float32), 0.122837655]. 
=============================================
[2019-03-27 13:22:14,921] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1117966: loss 0.9978
[2019-03-27 13:22:14,924] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1117966: learning rate 0.0000
[2019-03-27 13:22:15,557] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1118204: loss 0.9712
[2019-03-27 13:22:15,560] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1118204: learning rate 0.0000
[2019-03-27 13:22:16,557] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1118582: loss 0.8549
[2019-03-27 13:22:16,559] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1118582: learning rate 0.0000
[2019-03-27 13:22:16,610] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.1199035e-30 1.0000000e+00 1.0848442e-37 1.4336226e-34 5.1523970e-29], sum to 1.0000
[2019-03-27 13:22:16,622] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7220
[2019-03-27 13:22:16,630] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.33333333333334, 1.0, 2.0, 0.5588145818713958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780886.7647537526, 780886.7647537532, 193324.6083505794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2118000.0000, 
sim time next is 2118600.0000, 
raw observation next is [30.0, 75.5, 1.0, 2.0, 0.5597742409293968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782228.2848523678, 782228.2848523678, 193491.8349713187], 
processed observation next is [0.0, 0.5217391304347826, 0.6208530805687204, 0.755, 1.0, 1.0, 0.46960751919204424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21728563468121329, 0.21728563468121329, 0.28879378353928165], 
reward next is 0.7112, 
noisyNet noise sample is [array([0.26115632], dtype=float32), -0.9887989]. 
=============================================
[2019-03-27 13:22:17,462] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1118929: loss 1.0015
[2019-03-27 13:22:17,463] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1118929: learning rate 0.0000
[2019-03-27 13:22:18,597] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1119362: loss 0.8967
[2019-03-27 13:22:18,602] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1119362: learning rate 0.0000
[2019-03-27 13:22:21,312] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1120373: loss 0.9114
[2019-03-27 13:22:21,314] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1120374: learning rate 0.0000
[2019-03-27 13:22:21,835] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1120581: loss 0.8315
[2019-03-27 13:22:21,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1120581: learning rate 0.0000
[2019-03-27 13:22:22,586] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1120864: loss 0.7985
[2019-03-27 13:22:22,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1120865: learning rate 0.0000
[2019-03-27 13:22:23,886] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3233904e-14 1.0000000e+00 2.3331555e-22 2.3178726e-09 1.6164251e-15], sum to 1.0000
[2019-03-27 13:22:23,900] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5201
[2019-03-27 13:22:23,909] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1837447.008975521 W.
[2019-03-27 13:22:23,917] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.5, 63.5, 1.0, 2.0, 0.6571312885796176, 1.0, 1.0, 0.6571312885796176, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1837447.008975521, 1837447.008975521, 356312.8167947087], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2284200.0000, 
sim time next is 2284800.0000, 
raw observation next is [31.7, 63.0, 1.0, 2.0, 0.4524463352938021, 1.0, 2.0, 0.4524463352938021, 1.0, 1.0, 0.7830595204592492, 6.9112, 6.9112, 170.5573041426782, 1897724.727808365, 1897724.727808365, 383386.8979485778], 
processed observation next is [1.0, 0.43478260869565216, 0.7014218009478673, 0.63, 1.0, 1.0, 0.340296789510605, 1.0, 1.0, 0.340296789510605, 1.0, 0.5, 0.7354384395844503, 0.0, 0.0, 0.8375144448122397, 0.5271457577245459, 0.5271457577245459, 0.5722192506695192], 
reward next is 0.4278, 
noisyNet noise sample is [array([1.7567296], dtype=float32), -1.4110541]. 
=============================================
[2019-03-27 13:22:25,036] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1121779: loss 0.8145
[2019-03-27 13:22:25,038] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1121779: learning rate 0.0000
[2019-03-27 13:22:25,472] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1121937: loss 0.7263
[2019-03-27 13:22:25,477] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1121937: learning rate 0.0000
[2019-03-27 13:22:25,531] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7818364e-27 1.0000000e+00 8.6385892e-36 1.3739115e-29 2.9492790e-25], sum to 1.0000
[2019-03-27 13:22:25,541] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6809
[2019-03-27 13:22:25,548] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 84.66666666666667, 1.0, 2.0, 0.5300039202080183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740612.7484461255, 740612.7484461262, 188430.6489737317], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2241600.0000, 
sim time next is 2242200.0000, 
raw observation next is [27.38333333333333, 84.83333333333334, 1.0, 2.0, 0.5291336405919979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739396.2206433906, 739396.2206433906, 188286.5699530148], 
processed observation next is [1.0, 0.9565217391304348, 0.4968404423380725, 0.8483333333333334, 1.0, 1.0, 0.4326911332433709, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2053878390676085, 0.2053878390676085, 0.2810247312731564], 
reward next is 0.7190, 
noisyNet noise sample is [array([-1.4235624], dtype=float32), 1.4544275]. 
=============================================
[2019-03-27 13:22:25,998] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6859645e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3106650e-31], sum to 1.0000
[2019-03-27 13:22:26,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6613
[2019-03-27 13:22:26,016] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.2, 74.66666666666667, 1.0, 2.0, 0.554832280312203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775319.8708003875, 775319.8708003875, 192634.0995559795], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2227200.0000, 
sim time next is 2227800.0000, 
raw observation next is [30.05, 75.33333333333333, 1.0, 2.0, 0.5553783215004101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776083.1847961639, 776083.1847961632, 192728.4417279382], 
processed observation next is [1.0, 0.782608695652174, 0.6232227488151659, 0.7533333333333333, 1.0, 1.0, 0.46431123072338565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21557866244337887, 0.21557866244337867, 0.28765439063871373], 
reward next is 0.7123, 
noisyNet noise sample is [array([0.46694177], dtype=float32), -1.9674634]. 
=============================================
[2019-03-27 13:22:26,178] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1122203: loss 0.8440
[2019-03-27 13:22:26,181] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1122205: learning rate 0.0000
[2019-03-27 13:22:26,436] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1122300: loss 0.7959
[2019-03-27 13:22:26,441] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1122300: learning rate 0.0000
[2019-03-27 13:22:27,307] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1122637: loss -7.2326
[2019-03-27 13:22:27,308] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1122637: learning rate 0.0000
[2019-03-27 13:22:27,453] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6509985e-24 1.0000000e+00 2.3735676e-32 7.2019585e-24 2.0242292e-22], sum to 1.0000
[2019-03-27 13:22:27,471] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6103
[2019-03-27 13:22:27,479] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333334, 81.0, 1.0, 2.0, 0.5402690607811881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754962.0681075352, 754962.0681075352, 190145.7248019612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2334000.0000, 
sim time next is 2334600.0000, 
raw observation next is [28.2, 81.0, 1.0, 2.0, 0.5387324590898745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752814.0882196674, 752814.0882196674, 189886.9997333332], 
processed observation next is [1.0, 0.0, 0.5355450236966824, 0.81, 1.0, 1.0, 0.44425597480707774, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20911502450546318, 0.20911502450546318, 0.28341343243781075], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.6462016], dtype=float32), -0.88732904]. 
=============================================
[2019-03-27 13:22:27,874] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1122849: loss 0.7550
[2019-03-27 13:22:27,878] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1122850: learning rate 0.0000
[2019-03-27 13:22:27,923] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1122869: loss 0.7450
[2019-03-27 13:22:27,928] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1122870: learning rate 0.0000
[2019-03-27 13:22:30,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4585057e-16 1.0000000e+00 9.9005174e-23 4.7350769e-12 1.1312487e-14], sum to 1.0000
[2019-03-27 13:22:30,572] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9062
[2019-03-27 13:22:30,578] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.53333333333333, 63.33333333333334, 1.0, 2.0, 0.3832461493232555, 1.0, 2.0, 0.3832461493232555, 1.0, 1.0, 0.6655718199361909, 6.9112, 6.9112, 170.5573041426782, 1607256.266318974, 1607256.266318974, 343808.3595953895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2378400.0000, 
sim time next is 2379000.0000, 
raw observation next is [32.56666666666666, 63.16666666666666, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.404660924950383, 6.9112, 168.9107608795252, 1804068.311033993, 1453994.697783722, 311353.6130336943], 
processed observation next is [1.0, 0.5217391304347826, 0.7424960505529224, 0.6316666666666666, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.049346092495038275, 0.0, 0.8294291635993911, 0.5011300863983313, 0.4038874160510339, 0.4647068851249168], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.21187665], dtype=float32), 0.0053919256]. 
=============================================
[2019-03-27 13:22:30,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[55.615395]
 [56.079666]
 [55.76032 ]
 [54.13491 ]
 [53.96284 ]], R is [[55.40128708]
 [54.84727478]
 [54.80345154]
 [54.71581268]
 [54.54148102]].
[2019-03-27 13:22:31,184] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.16774205e-36 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.17321740e-34], sum to 1.0000
[2019-03-27 13:22:31,194] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9885
[2019-03-27 13:22:31,200] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 92.66666666666667, 1.0, 2.0, 0.4270046207340864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 623238.8685320641, 623238.8685320641, 176353.4121501376], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2612400.0000, 
sim time next is 2613000.0000, 
raw observation next is [23.73333333333333, 92.83333333333333, 1.0, 2.0, 0.4253946380073971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 621173.6743592846, 621173.6743592852, 176161.2105372751], 
processed observation next is [0.0, 0.21739130434782608, 0.3238546603475513, 0.9283333333333332, 1.0, 1.0, 0.3077043831414422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17254824287757906, 0.17254824287757922, 0.26292717990638076], 
reward next is 0.7371, 
noisyNet noise sample is [array([0.66995597], dtype=float32), -0.69911784]. 
=============================================
[2019-03-27 13:22:31,214] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.462555]
 [73.43616 ]
 [73.41322 ]
 [73.3947  ]
 [73.355515]], R is [[73.47963715]
 [73.48162842]
 [73.48323059]
 [73.48470306]
 [73.48644257]].
[2019-03-27 13:22:31,489] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1124203: loss -49.5670
[2019-03-27 13:22:31,490] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1124203: learning rate 0.0000
[2019-03-27 13:22:32,960] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7108329e-30 1.0000000e+00 0.0000000e+00 1.5716528e-33 4.6378781e-28], sum to 1.0000
[2019-03-27 13:22:32,973] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7286
[2019-03-27 13:22:32,979] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 80.0, 1.0, 2.0, 0.5613385714576048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 784415.086834244, 784415.086834244, 193764.4831830124], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2325600.0000, 
sim time next is 2326200.0000, 
raw observation next is [29.01666666666667, 80.16666666666667, 1.0, 2.0, 0.5599498469338116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 782473.7669935346, 782473.7669935353, 193521.8926222118], 
processed observation next is [1.0, 0.9565217391304348, 0.5742496050552924, 0.8016666666666667, 1.0, 1.0, 0.4698190926913392, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21735382416487073, 0.21735382416487092, 0.28883864570479373], 
reward next is 0.7112, 
noisyNet noise sample is [array([1.0779313], dtype=float32), 0.37626722]. 
=============================================
[2019-03-27 13:22:33,604] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 13:22:33,606] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:22:33,607] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:22:33,607] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:22:33,608] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:22:33,608] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:22:33,610] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:22:33,610] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:22:33,609] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:22:33,612] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:22:33,616] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:22:33,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run46
[2019-03-27 13:22:33,655] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run46
[2019-03-27 13:22:33,656] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run46
[2019-03-27 13:22:33,690] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run46
[2019-03-27 13:22:33,691] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run46
[2019-03-27 13:22:46,439] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:22:46,440] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.83333333333333, 54.0, 1.0, 2.0, 0.7978730043849921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1138497.273305682, 1138497.273305682, 246662.1287042089]
[2019-03-27 13:22:46,440] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:22:46,441] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7543257e-29 1.0000000e+00 5.3938556e-38 2.7757078e-35 1.8445190e-27], sampled 0.7461626392620251
[2019-03-27 13:23:12,276] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:23:12,279] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.93333333333333, 91.16666666666667, 1.0, 2.0, 0.3845640487152628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581718.774529762, 581718.7745297626, 173101.8427635663]
[2019-03-27 13:23:12,280] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:23:12,284] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.8472194e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3874726e-32], sampled 0.8959139014016526
[2019-03-27 13:23:12,812] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:23:12,812] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.64103576333333, 57.72314453666667, 1.0, 2.0, 0.6180157329730865, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.939731042690513, 6.9112, 168.9124114627235, 1727998.869373677, 1707758.031039595, 368706.1351156997]
[2019-03-27 13:23:12,813] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:23:12,818] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9317531e-23 1.0000000e+00 1.1391679e-31 3.7727754e-25 4.0285332e-22], sampled 0.42375209520528445
[2019-03-27 13:23:12,819] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1727998.869373677 W.
[2019-03-27 13:23:24,541] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:23:24,541] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.77967474666667, 90.37901510333333, 1.0, 2.0, 0.4290376568045718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 628152.9019403931, 628152.9019403937, 176882.5448007905]
[2019-03-27 13:23:24,541] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:23:24,544] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.0907393e-30 1.0000000e+00 0.0000000e+00 9.7309798e-37 3.1073642e-28], sampled 0.7432457694694192
[2019-03-27 13:23:28,839] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:23:28,840] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.81171218166667, 91.641498945, 1.0, 2.0, 0.6362158227340776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 897350.8941347363, 897350.8941347363, 208762.6014572277]
[2019-03-27 13:23:28,843] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:23:28,845] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4511384e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0284392e-29], sampled 0.2037058613197602
[2019-03-27 13:23:37,194] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:23:37,196] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.56666666666667, 70.33333333333334, 1.0, 2.0, 0.5324311992088461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744005.7485142675, 744005.7485142675, 188832.6622188912]
[2019-03-27 13:23:37,197] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:23:37,200] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4618315e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.3023087e-33], sampled 0.3525121346390232
[2019-03-27 13:23:55,532] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:23:55,533] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.4827398, 65.38345046, 1.0, 2.0, 0.5073015359120518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708878.48229933, 708878.48229933, 184746.8102319154]
[2019-03-27 13:23:55,534] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:23:55,540] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.1663942e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7171802e-32], sampled 0.5029757160401043
[2019-03-27 13:24:04,380] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:24:04,381] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.670895795, 97.368700785, 1.0, 2.0, 0.5600672473923519, 0.0, 2.0, 0.0, 1.0, 2.0, 0.939002531595072, 6.911200000000001, 6.9112, 168.9129564993273, 1582791.299480476, 1582791.299480475, 336752.6232613679]
[2019-03-27 13:24:04,382] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:24:04,383] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.8033072e-20 1.0000000e+00 3.7910484e-28 9.1964421e-19 6.9169908e-19], sampled 0.388994013445235
[2019-03-27 13:24:18,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:24:18,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.43941989, 83.20224076, 1.0, 2.0, 0.8107970210967835, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.975966921642221, 6.9112, 168.9125317323433, 2030184.371855377, 1984236.604085566, 411568.2127012734]
[2019-03-27 13:24:18,136] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:24:18,140] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4836202e-23 1.0000000e+00 1.3425956e-31 9.2419868e-26 4.1047465e-22], sampled 0.7850625422102243
[2019-03-27 13:24:18,142] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2030184.371855377 W.
[2019-03-27 13:24:41,574] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.2677 3164175508.8049 1777.0000
[2019-03-27 13:24:41,778] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 13:24:41,790] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05913338], dtype=float32), 0.05910775]
[2019-03-27 13:24:41,790] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.48487459, 75.38235292, 1.0, 2.0, 0.5232105546557609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731116.6213301197, 731116.6213301191, 187312.0251063599]
[2019-03-27 13:24:41,791] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:24:41,793] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.5916314e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0609988e-32], sampled 0.25785664453999546
[2019-03-27 13:24:41,891] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 13:24:42,009] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 13:24:42,128] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-27 13:24:43,143] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1125000, evaluation results [1125000.0, 7884.267691215359, 3164175508.804864, 1777.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 13:24:45,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9282017e-14 1.0000000e+00 1.6560217e-21 7.7033935e-10 2.7916287e-14], sum to 1.0000
[2019-03-27 13:24:45,244] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1009
[2019-03-27 13:24:45,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2399958.403977585 W.
[2019-03-27 13:24:45,265] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.83333333333333, 62.66666666666667, 1.0, 2.0, 0.5720596051243317, 1.0, 2.0, 0.5720596051243317, 1.0, 2.0, 0.9934783510986638, 6.9112, 6.9112, 170.5573041426782, 2399958.403977585, 2399958.403977585, 468552.9987544242], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2384400.0000, 
sim time next is 2385000.0000, 
raw observation next is [32.84999999999999, 62.5, 1.0, 2.0, 0.851792968664523, 1.0, 2.0, 0.851792968664523, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2382331.34101996, 2382331.34101996, 445877.3908005132], 
processed observation next is [1.0, 0.6086956521739131, 0.7559241706161132, 0.625, 1.0, 1.0, 0.8214373116440037, 1.0, 1.0, 0.8214373116440037, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6617587058388779, 0.6617587058388779, 0.6654886429858407], 
reward next is 0.3345, 
noisyNet noise sample is [array([-0.10755371], dtype=float32), -0.46028104]. 
=============================================
[2019-03-27 13:24:45,290] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[56.466854]
 [56.00393 ]
 [57.458588]
 [57.15013 ]
 [57.30143 ]], R is [[56.3231163 ]
 [56.06055069]
 [55.7937851 ]
 [55.63212967]
 [55.4438858 ]].
[2019-03-27 13:24:46,240] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1126166: loss 23.2016
[2019-03-27 13:24:46,244] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1126167: learning rate 0.0000
[2019-03-27 13:24:46,624] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1126314: loss -117.1282
[2019-03-27 13:24:46,627] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1126314: learning rate 0.0000
[2019-03-27 13:24:46,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2053333e-15 1.0000000e+00 4.9297628e-23 6.2236938e-11 4.4025397e-15], sum to 1.0000
[2019-03-27 13:24:46,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9498
[2019-03-27 13:24:46,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2109080.150750626 W.
[2019-03-27 13:24:46,913] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.4, 82.0, 1.0, 2.0, 0.8671668311280345, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.994264601350566, 6.9112, 168.9124619739814, 2109080.150750626, 2050151.436219179, 425555.5908953031], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2480400.0000, 
sim time next is 2481000.0000, 
raw observation next is [28.31666666666667, 82.50000000000001, 1.0, 2.0, 0.2388853606387698, 1.0, 1.0, 0.2388853606387698, 1.0, 2.0, 0.4126512219749989, 6.9112, 6.9112, 170.5573041426782, 1001553.716560897, 1001553.716560897, 281495.6368571309], 
processed observation next is [1.0, 0.7391304347826086, 0.5410742496050555, 0.8250000000000002, 1.0, 1.0, 0.08299441040815639, 1.0, 0.5, 0.08299441040815639, 1.0, 1.0, 0.28372100240853526, 0.0, 0.0, 0.8375144448122397, 0.2782093657113603, 0.2782093657113603, 0.42014274157780734], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02322114], dtype=float32), -1.1663206]. 
=============================================
[2019-03-27 13:24:46,925] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[55.926983]
 [55.414177]
 [55.003025]
 [54.887363]
 [54.496265]], R is [[58.84367371]
 [58.25523758]
 [58.06345749]
 [57.85941315]
 [57.28081894]].
[2019-03-27 13:24:47,439] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1126623: loss -70.5563
[2019-03-27 13:24:47,444] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1126624: learning rate 0.0000
[2019-03-27 13:24:48,288] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1126943: loss -150.3348
[2019-03-27 13:24:48,291] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1126944: learning rate 0.0000
[2019-03-27 13:24:49,425] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1127378: loss -120.5688
[2019-03-27 13:24:49,430] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1127379: learning rate 0.0000
[2019-03-27 13:24:52,144] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1128415: loss -171.8015
[2019-03-27 13:24:52,147] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1128416: learning rate 0.0000
[2019-03-27 13:24:52,684] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7697146e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8815813e-31], sum to 1.0000
[2019-03-27 13:24:52,693] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6819
[2019-03-27 13:24:52,700] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.83333333333334, 1.0, 2.0, 0.4894840688501706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683973.2053727263, 683973.2053727269, 181964.649621705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2645400.0000, 
sim time next is 2646000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4852130322956492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678003.2320904683, 678003.2320904683, 181311.4334101371], 
processed observation next is [0.0, 0.6521739130434783, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3797747377056015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1883342311362412, 0.1883342311362412, 0.27061407971662255], 
reward next is 0.7294, 
noisyNet noise sample is [array([-0.13464227], dtype=float32), -0.9053436]. 
=============================================
[2019-03-27 13:24:52,711] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.433235]
 [77.40316 ]
 [77.352264]
 [77.30515 ]
 [77.25891 ]], R is [[77.5139389 ]
 [77.46720886]
 [77.41995239]
 [77.37219238]
 [77.32407379]].
[2019-03-27 13:24:52,791] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1128659: loss -90.8862
[2019-03-27 13:24:52,791] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1128659: learning rate 0.0000
[2019-03-27 13:24:53,643] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1128977: loss -130.4698
[2019-03-27 13:24:53,647] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1128977: learning rate 0.0000
[2019-03-27 13:24:56,050] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1129881: loss -294.8245
[2019-03-27 13:24:56,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1129882: learning rate 0.0000
[2019-03-27 13:24:56,708] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1130127: loss -22.7596
[2019-03-27 13:24:56,711] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1130127: learning rate 0.0000
[2019-03-27 13:24:56,814] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1130173: loss 0.0393
[2019-03-27 13:24:56,816] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1130174: learning rate 0.0000
[2019-03-27 13:24:57,269] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1130349: loss -89.1037
[2019-03-27 13:24:57,275] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1130349: learning rate 0.0000
[2019-03-27 13:24:57,619] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1130482: loss -133.4907
[2019-03-27 13:24:57,623] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1130483: learning rate 0.0000
[2019-03-27 13:24:59,067] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1131032: loss -118.2293
[2019-03-27 13:24:59,070] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1131033: learning rate 0.0000
[2019-03-27 13:24:59,175] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1131071: loss -90.0549
[2019-03-27 13:24:59,178] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1131072: learning rate 0.0000
[2019-03-27 13:24:59,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7841822e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3249115e-33], sum to 1.0000
[2019-03-27 13:24:59,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8324
[2019-03-27 13:24:59,560] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4974254418994528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695073.5940608928, 695073.5940608921, 183193.8122244044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2652600.0000, 
sim time next is 2653200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4976160291611185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695339.9968591583, 695339.9968591583, 183223.5221411923], 
processed observation next is [0.0, 0.7391304347826086, 0.4312796208530806, 0.89, 1.0, 1.0, 0.3947181074230344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19314999912754396, 0.19314999912754396, 0.27346794349431686], 
reward next is 0.7265, 
noisyNet noise sample is [array([-0.2776248], dtype=float32), -1.041653]. 
=============================================
[2019-03-27 13:25:01,141] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1131822: loss 0.0293
[2019-03-27 13:25:01,145] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1131822: learning rate 0.0000
[2019-03-27 13:25:01,576] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5376552e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.6771836e-32], sum to 1.0000
[2019-03-27 13:25:01,585] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2677
[2019-03-27 13:25:01,591] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.3493758065039241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538680.3763129507, 538680.37631295, 169722.2388710988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2776200.0000, 
sim time next is 2776800.0000, 
raw observation next is [21.66666666666667, 96.0, 1.0, 2.0, 0.337116271885331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520451.6342583667, 520451.6342583673, 168272.3744305947], 
processed observation next is [1.0, 0.13043478260869565, 0.22590837282780438, 0.96, 1.0, 1.0, 0.20134490588594095, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14456989840510187, 0.14456989840510204, 0.251152797657604], 
reward next is 0.7488, 
noisyNet noise sample is [array([1.0263585], dtype=float32), 0.5730287]. 
=============================================
[2019-03-27 13:25:06,172] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.2560993e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9155145e-30], sum to 1.0000
[2019-03-27 13:25:06,185] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8695
[2019-03-27 13:25:06,190] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.4412583521901511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 633801.0473858996, 633801.047385899, 177121.2809928687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2688000.0000, 
sim time next is 2688600.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.4404975202183125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632708.5283133048, 632708.5283133048, 177012.2143808495], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 1.0, 1.0, 0.32590062676905124, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17575236897591798, 0.17575236897591798, 0.2641973348967903], 
reward next is 0.7358, 
noisyNet noise sample is [array([-1.8256282], dtype=float32), -1.9832939]. 
=============================================
[2019-03-27 13:25:06,861] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1133982: loss 0.7766
[2019-03-27 13:25:06,864] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1133983: learning rate 0.0000
[2019-03-27 13:25:07,083] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1134069: loss 0.6669
[2019-03-27 13:25:07,088] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1134070: learning rate 0.0000
[2019-03-27 13:25:07,128] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3611294e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4144088e-30], sum to 1.0000
[2019-03-27 13:25:07,143] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3717
[2019-03-27 13:25:07,149] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3519854848168404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541574.4567812396, 541574.4567812396, 169927.0531026155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2761200.0000, 
sim time next is 2761800.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.349903153279406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538978.7727720743, 538978.772772075, 169731.5328070277], 
processed observation next is [0.0, 1.0, 0.2417061611374408, 0.9400000000000002, 1.0, 1.0, 0.21675078708362172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14971632577002064, 0.14971632577002084, 0.2533306459806383], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.5613479], dtype=float32), 0.44038963]. 
=============================================
[2019-03-27 13:25:08,235] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0815053e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8415838e-29], sum to 1.0000
[2019-03-27 13:25:08,238] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1134508: loss 0.7492
[2019-03-27 13:25:08,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8337
[2019-03-27 13:25:08,243] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1134509: learning rate 0.0000
[2019-03-27 13:25:08,251] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3496208299396052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538583.736769365, 538583.7367693643, 169700.2904138055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2874600.0000, 
sim time next is 2875200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3482396457427301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536454.1334988913, 536454.1334988913, 169525.7583088381], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21474656113581939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14901503708302535, 0.14901503708302535, 0.25302351986393745], 
reward next is 0.7470, 
noisyNet noise sample is [array([-1.8026832], dtype=float32), -0.66248214]. 
=============================================
[2019-03-27 13:25:08,824] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1134730: loss 0.8773
[2019-03-27 13:25:08,827] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1134730: learning rate 0.0000
[2019-03-27 13:25:10,287] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1135281: loss 1.0854
[2019-03-27 13:25:10,292] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1135282: learning rate 0.0000
[2019-03-27 13:25:11,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2030603e-32 1.0000000e+00 0.0000000e+00 2.7892582e-38 1.4293662e-30], sum to 1.0000
[2019-03-27 13:25:11,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2710
[2019-03-27 13:25:11,943] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4113506259202543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606088.3237429457, 606088.3237429451, 174881.5591191078], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2836800.0000, 
sim time next is 2837400.0000, 
raw observation next is [23.83333333333333, 89.83333333333334, 1.0, 2.0, 0.40969293241241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604490.5301258386, 604490.5301258386, 174756.7883453504], 
processed observation next is [1.0, 0.8695652173913043, 0.32859399684044216, 0.8983333333333334, 1.0, 1.0, 0.2887866655571205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1679140361460663, 0.1679140361460663, 0.26083102738111996], 
reward next is 0.7392, 
noisyNet noise sample is [array([1.5054846], dtype=float32), 1.0229301]. 
=============================================
[2019-03-27 13:25:13,228] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1136406: loss 1.0936
[2019-03-27 13:25:13,230] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1136406: learning rate 0.0000
[2019-03-27 13:25:13,800] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1136620: loss 0.9642
[2019-03-27 13:25:13,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1136620: learning rate 0.0000
[2019-03-27 13:25:14,885] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1137026: loss 0.9375
[2019-03-27 13:25:14,890] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1137027: learning rate 0.0000
[2019-03-27 13:25:17,113] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1137866: loss 0.8256
[2019-03-27 13:25:17,117] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1137866: learning rate 0.0000
[2019-03-27 13:25:17,867] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1138145: loss 2.9809
[2019-03-27 13:25:17,868] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1138145: learning rate 0.0000
[2019-03-27 13:25:17,953] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1138174: loss 0.9721
[2019-03-27 13:25:17,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1138174: learning rate 0.0000
[2019-03-27 13:25:18,547] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1138402: loss 0.6316
[2019-03-27 13:25:18,550] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1138404: learning rate 0.0000
[2019-03-27 13:25:18,937] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1138548: loss 0.7147
[2019-03-27 13:25:18,941] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1138550: learning rate 0.0000
[2019-03-27 13:25:19,488] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2763989e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1602056e-33], sum to 1.0000
[2019-03-27 13:25:19,496] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0421
[2019-03-27 13:25:19,504] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3453979577601216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532077.8387668893, 532077.83876689, 169169.26886832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2868600.0000, 
sim time next is 2869200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3448394295254273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531217.893419224, 531217.8934192245, 169099.5435714672], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21064991509087624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14756052594978444, 0.14756052594978458, 0.2523873784648764], 
reward next is 0.7476, 
noisyNet noise sample is [array([-0.904998], dtype=float32), 1.0784436]. 
=============================================
[2019-03-27 13:25:20,244] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1139045: loss 0.6178
[2019-03-27 13:25:20,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1139045: learning rate 0.0000
[2019-03-27 13:25:20,513] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1139149: loss 0.6109
[2019-03-27 13:25:20,515] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1139149: learning rate 0.0000
[2019-03-27 13:25:22,225] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1139807: loss 2.9378
[2019-03-27 13:25:22,226] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1139807: learning rate 0.0000
[2019-03-27 13:25:22,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0151463e-24 1.0000000e+00 6.9832787e-32 6.2719804e-26 2.0826853e-22], sum to 1.0000
[2019-03-27 13:25:22,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9081
[2019-03-27 13:25:22,832] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9924922693589, 6.9112, 168.9123707921927, 1511465.756391976, 1453794.422625121, 311346.3173532994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3163800.0000, 
sim time next is 3164400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.111429885530697, 6.9112, 168.9116008852936, 1595901.043382397, 1453852.211240106, 311346.3237200585], 
processed observation next is [1.0, 0.6521739130434783, 0.4312796208530806, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0200229885530697, 0.0, 0.8294332884122703, 0.44330584538399914, 0.40384783645558503, 0.4646960055523261], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.48950505], dtype=float32), 0.57161856]. 
=============================================
[2019-03-27 13:25:28,192] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1142053: loss 3.4521
[2019-03-27 13:25:28,196] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1142054: learning rate 0.0000
[2019-03-27 13:25:28,229] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1142064: loss 3.6104
[2019-03-27 13:25:28,230] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1142064: learning rate 0.0000
[2019-03-27 13:25:29,490] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1142544: loss 3.6982
[2019-03-27 13:25:29,496] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1142545: learning rate 0.0000
[2019-03-27 13:25:30,072] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1142766: loss 3.4785
[2019-03-27 13:25:30,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1142766: learning rate 0.0000
[2019-03-27 13:25:31,272] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1143219: loss 3.9168
[2019-03-27 13:25:31,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1143220: learning rate 0.0000
[2019-03-27 13:25:34,396] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1144395: loss 3.9267
[2019-03-27 13:25:34,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1144395: learning rate 0.0000
[2019-03-27 13:25:34,904] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1144582: loss 4.3426
[2019-03-27 13:25:34,905] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1144582: learning rate 0.0000
[2019-03-27 13:25:36,034] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1145011: loss 4.0079
[2019-03-27 13:25:36,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1145013: learning rate 0.0000
[2019-03-27 13:25:36,916] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.5319710e-28 1.0000000e+00 1.5899453e-37 1.7718551e-34 2.2802251e-26], sum to 1.0000
[2019-03-27 13:25:36,918] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0436
[2019-03-27 13:25:36,928] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3909356051978644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 583336.3860204469, 583336.3860204476, 173009.5673319568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3114000.0000, 
sim time next is 3114600.0000, 
raw observation next is [23.0, 94.00000000000001, 1.0, 2.0, 0.3906318431671744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 582883.0890230265, 582883.0890230272, 172968.4333349663], 
processed observation next is [1.0, 0.043478260869565216, 0.28909952606635075, 0.9400000000000002, 1.0, 1.0, 0.2658214977917764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16191196917306291, 0.1619119691730631, 0.25816184079845717], 
reward next is 0.7418, 
noisyNet noise sample is [array([0.28257945], dtype=float32), -0.078901954]. 
=============================================
[2019-03-27 13:25:38,194] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1145828: loss 4.0644
[2019-03-27 13:25:38,198] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1145828: learning rate 0.0000
[2019-03-27 13:25:39,045] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1146148: loss 4.3357
[2019-03-27 13:25:39,047] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1146148: learning rate 0.0000
[2019-03-27 13:25:39,351] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1146263: loss 16.1550
[2019-03-27 13:25:39,356] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1146264: learning rate 0.0000
[2019-03-27 13:25:39,822] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1146443: loss 4.5737
[2019-03-27 13:25:39,824] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1146443: learning rate 0.0000
[2019-03-27 13:25:39,945] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1146489: loss 4.5514
[2019-03-27 13:25:39,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1146489: learning rate 0.0000
[2019-03-27 13:25:40,482] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.10147822e-28 1.00000000e+00 9.39152204e-37 3.39075991e-30
 1.19221604e-26], sum to 1.0000
[2019-03-27 13:25:40,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5644
[2019-03-27 13:25:40,499] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.0, 1.0, 2.0, 0.5015079647209113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700780.157443193, 700780.157443193, 183832.9873011234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3187800.0000, 
sim time next is 3188400.0000, 
raw observation next is [25.66666666666666, 94.0, 1.0, 2.0, 0.5068186383444803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708203.4797991891, 708203.4797991891, 184671.6829332463], 
processed observation next is [1.0, 0.9130434782608695, 0.4154818325434437, 0.94, 1.0, 1.0, 0.40580558836684366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1967231888331081, 0.1967231888331081, 0.2756293775123079], 
reward next is 0.7244, 
noisyNet noise sample is [array([-0.6845362], dtype=float32), -0.17887682]. 
=============================================
[2019-03-27 13:25:41,257] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1146979: loss 4.9146
[2019-03-27 13:25:41,259] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1146979: learning rate 0.0000
[2019-03-27 13:25:41,553] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1147093: loss 4.8416
[2019-03-27 13:25:41,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1147094: learning rate 0.0000
[2019-03-27 13:25:43,001] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3221835e-27 1.0000000e+00 4.6436457e-37 9.6436822e-32 2.9920674e-26], sum to 1.0000
[2019-03-27 13:25:43,010] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5773
[2019-03-27 13:25:43,016] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 83.16666666666667, 1.0, 2.0, 0.5495049870031599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767872.8468131485, 767872.846813149, 191715.3097223832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3269400.0000, 
sim time next is 3270000.0000, 
raw observation next is [28.0, 82.33333333333334, 1.0, 2.0, 0.5391602199958581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753412.044838922, 753412.0448389213, 189958.9602197447], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8233333333333335, 1.0, 1.0, 0.44477134939260005, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20928112356636722, 0.20928112356636702, 0.2835208361488727], 
reward next is 0.7165, 
noisyNet noise sample is [array([0.9414931], dtype=float32), -1.2877098]. 
=============================================
[2019-03-27 13:25:43,031] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.81597 ]
 [77.80695 ]
 [77.700096]
 [77.69526 ]
 [77.672844]], R is [[77.80854034]
 [77.7443161 ]
 [77.68180084]
 [77.61955261]
 [77.55754852]].
[2019-03-27 13:25:43,572] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1147858: loss 16.9198
[2019-03-27 13:25:43,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1147859: learning rate 0.0000
[2019-03-27 13:25:47,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6585564e-29 1.0000000e+00 1.5694621e-38 5.5961062e-33 8.1395084e-29], sum to 1.0000
[2019-03-27 13:25:47,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3665
[2019-03-27 13:25:47,811] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.595696175992508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832445.2520829113, 832445.2520829113, 199947.2292352715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3337200.0000, 
sim time next is 3337800.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5968682171159964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834083.7436432667, 834083.7436432667, 200164.2127618514], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5142990567662606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2316899287897963, 0.2316899287897963, 0.29875255636097225], 
reward next is 0.7012, 
noisyNet noise sample is [array([0.43391508], dtype=float32), -2.3156135]. 
=============================================
[2019-03-27 13:25:48,953] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6925975e-11 9.5615882e-01 8.2291453e-19 4.3841191e-02 2.1280139e-11], sum to 1.0000
[2019-03-27 13:25:48,960] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2820
[2019-03-27 13:25:48,969] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2830523.835579423 W.
[2019-03-27 13:25:48,982] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 61.0, 1.0, 2.0, 1.011861072348346, 1.0, 2.0, 1.011861072348346, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2830523.835579423, 2830523.835579423, 536198.8073304761], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3422400.0000, 
sim time next is 3423000.0000, 
raw observation next is [34.0, 60.5, 1.0, 2.0, 1.004356760158987, 1.0, 2.0, 1.004356760158987, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2809508.084190204, 2809508.084190205, 531627.7302773172], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.605, 1.0, 1.0, 1.0052491086252855, 1.0, 1.0, 1.0052491086252855, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7804189122750567, 0.780418912275057, 0.7934742242945033], 
reward next is 0.2065, 
noisyNet noise sample is [array([0.37917027], dtype=float32), 1.5410994]. 
=============================================
[2019-03-27 13:25:49,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[57.64012 ]
 [57.1175  ]
 [56.762283]
 [56.861916]
 [56.637703]], R is [[57.91212463]
 [57.53270721]
 [57.14719772]
 [56.77185059]
 [56.42893219]].
[2019-03-27 13:25:49,260] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 13:25:49,261] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:25:49,261] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:25:49,262] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:25:49,262] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:25:49,263] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:25:49,263] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:25:49,264] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:25:49,265] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:25:49,267] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:25:49,268] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:25:49,292] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run47
[2019-03-27 13:25:49,315] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run47
[2019-03-27 13:25:49,316] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run47
[2019-03-27 13:25:49,335] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run47
[2019-03-27 13:25:49,391] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run47
[2019-03-27 13:25:58,241] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06054215], dtype=float32), 0.059783332]
[2019-03-27 13:25:58,243] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.83225099666667, 69.75751737666667, 1.0, 2.0, 0.2382540014687682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 395772.4096010493, 395772.4096010499, 159490.9367369288]
[2019-03-27 13:25:58,244] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:25:58,248] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0011494e-29 1.0000000e+00 3.5443199e-38 6.6215448e-35 6.8575827e-28], sampled 0.9557485052885755
[2019-03-27 13:26:48,177] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06054215], dtype=float32), 0.059783332]
[2019-03-27 13:26:48,180] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.65, 75.0, 1.0, 2.0, 0.9567794339515955, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.995158727083734, 6.9112, 168.9123891646482, 2234508.468740348, 2174945.458172016, 450701.5925133033]
[2019-03-27 13:26:48,182] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:26:48,185] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2203034e-12 9.9989927e-01 7.6363308e-19 1.0067165e-04 7.2352142e-12], sampled 0.26530793153502985
[2019-03-27 13:26:48,185] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2234508.468740348 W.
[2019-03-27 13:27:04,006] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06054215], dtype=float32), 0.059783332]
[2019-03-27 13:27:04,007] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [38.4, 42.66666666666667, 1.0, 2.0, 0.8821655824426963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1232999.342062277, 1232999.342062277, 265083.6587820996]
[2019-03-27 13:27:04,010] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:27:04,013] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3556008e-23 1.0000000e+00 8.1193699e-32 5.3668842e-24 3.4135243e-22], sampled 0.36004226304012166
[2019-03-27 13:27:06,163] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06054215], dtype=float32), 0.059783332]
[2019-03-27 13:27:06,164] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.5211234920222549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728199.2313987339, 728199.2313987339, 186971.6801907987]
[2019-03-27 13:27:06,165] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:27:06,168] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.27766743e-27 1.00000000e+00 1.19337996e-35 3.09554329e-29
 2.12440759e-25], sampled 0.2382358213931689
[2019-03-27 13:27:27,963] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06054215], dtype=float32), 0.059783332]
[2019-03-27 13:27:27,966] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.03333333333333, 69.66666666666667, 1.0, 2.0, 0.5686029142691446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794570.0836596516, 794570.0836596516, 195042.3858208463]
[2019-03-27 13:27:27,966] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:27:27,971] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.24547050e-28 1.00000000e+00 1.39699655e-36 1.94315761e-30
 2.52596328e-26], sampled 0.40959907653315386
[2019-03-27 13:27:28,887] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06054215], dtype=float32), 0.059783332]
[2019-03-27 13:27:28,888] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.0, 63.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.149491762689783, 6.9112, 168.9115570175623, 1622921.68144186, 1453870.704268219, 311349.8795490685]
[2019-03-27 13:27:28,890] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:27:28,893] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1743708e-22 1.0000000e+00 1.0433400e-30 6.6595322e-23 2.4888677e-21], sampled 0.960980876797906
[2019-03-27 13:27:40,366] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06054215], dtype=float32), 0.059783332]
[2019-03-27 13:27:40,367] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.85766449, 70.77511383, 1.0, 2.0, 0.4734550066218506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666839.959524632, 666839.9595246325, 180220.9417565496]
[2019-03-27 13:27:40,369] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:27:40,374] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.8260744e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1823093e-29], sampled 0.7640310871226594
[2019-03-27 13:27:49,894] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06054215], dtype=float32), 0.059783332]
[2019-03-27 13:27:49,895] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.28333333333333, 95.0, 1.0, 2.0, 0.3253127911839967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509433.2232813542, 509433.2232813542, 167627.651623421]
[2019-03-27 13:27:49,895] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:27:49,901] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.8582821e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2367727e-31], sampled 0.45586977088535063
[2019-03-27 13:27:50,594] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06054215], dtype=float32), 0.059783332]
[2019-03-27 13:27:50,596] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.1, 85.16666666666667, 1.0, 2.0, 0.5264587266407332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735657.0780265077, 735657.0780265083, 187845.0008276712]
[2019-03-27 13:27:50,598] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:27:50,603] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9303566e-27 1.0000000e+00 1.3983889e-35 1.7519643e-30 1.7073026e-25], sampled 0.06252152745270612
[2019-03-27 13:27:57,103] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8267.9855 2926119582.1231 1310.0000
[2019-03-27 13:27:57,677] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8511.6191 2841033213.5276 1100.0000
[2019-03-27 13:27:57,713] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7929.5588 3159846905.6293 1680.0000
[2019-03-27 13:27:57,939] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8670.2375 2778463414.5906 912.0000
[2019-03-27 13:27:57,959] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8015.2590 3005725848.0139 1721.0000
[2019-03-27 13:27:58,978] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1150000, evaluation results [1150000.0, 7929.558818229476, 3159846905.6293254, 1680.0, 8267.985540890972, 2926119582.123097, 1310.0, 8670.2374839033, 2778463414.590644, 912.0, 8015.258955026751, 3005725848.0138764, 1721.0, 8511.61913203213, 2841033213.5276093, 1100.0]
[2019-03-27 13:27:59,145] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1688125e-27 1.0000000e+00 1.9152456e-36 1.2912318e-29 7.2677466e-26], sum to 1.0000
[2019-03-27 13:27:59,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6193
[2019-03-27 13:27:59,171] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5452878886973932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761977.801584096, 761977.801584096, 190995.9706930356], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3268800.0000, 
sim time next is 3269400.0000, 
raw observation next is [28.0, 83.16666666666667, 1.0, 2.0, 0.5495049870031599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767872.8468131485, 767872.846813149, 191715.3097223832], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8316666666666667, 1.0, 1.0, 0.45723492410019256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21329801300365236, 0.2132980130036525, 0.28614225331698984], 
reward next is 0.7139, 
noisyNet noise sample is [array([-0.30387273], dtype=float32), -1.4134666]. 
=============================================
[2019-03-27 13:27:59,250] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1150107: loss 17.7515
[2019-03-27 13:27:59,252] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1150107: learning rate 0.0000
[2019-03-27 13:27:59,455] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1150184: loss 17.9388
[2019-03-27 13:27:59,457] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1150184: learning rate 0.0000
[2019-03-27 13:28:00,388] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1150537: loss 17.8868
[2019-03-27 13:28:00,390] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1150537: learning rate 0.0000
[2019-03-27 13:28:00,988] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1150765: loss 18.1444
[2019-03-27 13:28:00,990] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1150765: learning rate 0.0000
[2019-03-27 13:28:02,018] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5689248e-24 1.0000000e+00 2.3418732e-32 5.6227428e-27 3.5542712e-23], sum to 1.0000
[2019-03-27 13:28:02,029] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2393
[2019-03-27 13:28:02,035] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7310351953082055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1021663.377300856, 1021663.377300855, 227686.0590640607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3387600.0000, 
sim time next is 3388200.0000, 
raw observation next is [26.16666666666667, 93.16666666666667, 1.0, 2.0, 0.8525567548986385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1191591.978612995, 1191591.978612995, 257219.8526596784], 
processed observation next is [1.0, 0.21739130434782608, 0.4391785150078992, 0.9316666666666668, 1.0, 1.0, 0.822357536022456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33099777183694307, 0.33099777183694307, 0.3839102278502663], 
reward next is 0.6161, 
noisyNet noise sample is [array([0.8121994], dtype=float32), 0.57389027]. 
=============================================
[2019-03-27 13:28:02,251] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1151249: loss 18.8092
[2019-03-27 13:28:02,252] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1151249: learning rate 0.0000
[2019-03-27 13:28:05,126] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1152335: loss 17.9941
[2019-03-27 13:28:05,127] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1152336: learning rate 0.0000
[2019-03-27 13:28:05,886] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1152622: loss 18.3903
[2019-03-27 13:28:05,891] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1152623: learning rate 0.0000
[2019-03-27 13:28:06,843] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1152990: loss 17.4070
[2019-03-27 13:28:06,845] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1152991: learning rate 0.0000
[2019-03-27 13:28:08,970] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1153779: loss 18.0231
[2019-03-27 13:28:08,972] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1153779: learning rate 0.0000
[2019-03-27 13:28:09,559] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1154005: loss 16.9427
[2019-03-27 13:28:09,566] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1154005: learning rate 0.0000
[2019-03-27 13:28:10,374] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1154315: loss 18.1958
[2019-03-27 13:28:10,376] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1154315: learning rate 0.0000
[2019-03-27 13:28:10,559] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1154382: loss 18.1949
[2019-03-27 13:28:10,561] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1154384: learning rate 0.0000
[2019-03-27 13:28:10,745] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1154456: loss -9.5720
[2019-03-27 13:28:10,748] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1154457: learning rate 0.0000
[2019-03-27 13:28:11,748] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1154832: loss 17.6365
[2019-03-27 13:28:11,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1154832: learning rate 0.0000
[2019-03-27 13:28:12,206] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1155014: loss 17.7322
[2019-03-27 13:28:12,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1155014: learning rate 0.0000
[2019-03-27 13:28:14,638] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1155933: loss -41.5361
[2019-03-27 13:28:14,640] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1155934: learning rate 0.0000
[2019-03-27 13:28:20,559] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1158167: loss -58.3049
[2019-03-27 13:28:20,563] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1158167: learning rate 0.0000
[2019-03-27 13:28:20,673] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1158209: loss -19.2589
[2019-03-27 13:28:20,676] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1158210: learning rate 0.0000
[2019-03-27 13:28:21,551] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1158541: loss -80.3723
[2019-03-27 13:28:21,557] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1158542: learning rate 0.0000
[2019-03-27 13:28:22,291] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1158828: loss 2.2155
[2019-03-27 13:28:22,294] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1158830: learning rate 0.0000
[2019-03-27 13:28:23,120] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2851177e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2932249e-34], sum to 1.0000
[2019-03-27 13:28:23,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0131
[2019-03-27 13:28:23,141] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.16666666666666, 66.33333333333333, 1.0, 2.0, 0.5573724294602583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778870.7639970582, 778870.7639970582, 193075.229872734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3606600.0000, 
sim time next is 3607200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5612244348728901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 784255.5333745743, 784255.5333745749, 193746.0851465776], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.67, 1.0, 1.0, 0.47135474081071094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21784875927071506, 0.21784875927071523, 0.28917326141280236], 
reward next is 0.7108, 
noisyNet noise sample is [array([0.45652387], dtype=float32), 0.06993252]. 
=============================================
[2019-03-27 13:28:23,635] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1159334: loss -54.7254
[2019-03-27 13:28:23,637] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1159334: learning rate 0.0000
[2019-03-27 13:28:26,492] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1160408: loss -115.5045
[2019-03-27 13:28:26,497] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1160409: learning rate 0.0000
[2019-03-27 13:28:27,034] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1160621: loss -44.3192
[2019-03-27 13:28:27,040] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1160622: learning rate 0.0000
[2019-03-27 13:28:28,214] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1161062: loss -57.1468
[2019-03-27 13:28:28,218] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1161063: learning rate 0.0000
[2019-03-27 13:28:30,134] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1161790: loss -40.9564
[2019-03-27 13:28:30,136] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1161790: learning rate 0.0000
[2019-03-27 13:28:31,094] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1162153: loss -62.7028
[2019-03-27 13:28:31,095] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1162153: learning rate 0.0000
[2019-03-27 13:28:31,353] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1162250: loss 0.0369
[2019-03-27 13:28:31,358] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1162251: learning rate 0.0000
[2019-03-27 13:28:31,790] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1162418: loss -52.3432
[2019-03-27 13:28:31,791] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1162418: learning rate 0.0000
[2019-03-27 13:28:32,006] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1162508: loss -43.7429
[2019-03-27 13:28:32,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1162508: learning rate 0.0000
[2019-03-27 13:28:33,248] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1162975: loss 8.7018
[2019-03-27 13:28:33,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1162978: learning rate 0.0000
[2019-03-27 13:28:33,758] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1163168: loss -81.1077
[2019-03-27 13:28:33,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1163168: learning rate 0.0000
[2019-03-27 13:28:35,359] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1163773: loss 0.0894
[2019-03-27 13:28:35,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1163773: learning rate 0.0000
[2019-03-27 13:28:36,456] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2531834e-30 1.0000000e+00 0.0000000e+00 6.2502342e-37 2.2932816e-28], sum to 1.0000
[2019-03-27 13:28:36,469] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3940
[2019-03-27 13:28:36,473] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 86.5, 1.0, 2.0, 0.5815642600583141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812689.2970028928, 812689.2970028921, 197361.2961121615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3889800.0000, 
sim time next is 3890400.0000, 
raw observation next is [28.33333333333333, 87.33333333333333, 1.0, 2.0, 0.5798409008873147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810280.1225297259, 810280.1225297266, 197050.0828706015], 
processed observation next is [0.0, 0.0, 0.541864139020537, 0.8733333333333333, 1.0, 1.0, 0.49378421793652366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22507781181381276, 0.22507781181381295, 0.29410460129940524], 
reward next is 0.7059, 
noisyNet noise sample is [array([0.3899182], dtype=float32), -1.4597334]. 
=============================================
[2019-03-27 13:28:39,514] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2864556e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.7129335e-34], sum to 1.0000
[2019-03-27 13:28:39,522] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2273
[2019-03-27 13:28:39,527] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 69.0, 1.0, 2.0, 0.5746693473168272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803050.5684547573, 803050.5684547573, 196122.4919113029], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3835200.0000, 
sim time next is 3835800.0000, 
raw observation next is [32.0, 68.5, 1.0, 2.0, 0.5806769320256622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811448.8537698874, 811448.8537698874, 197202.1880360653], 
processed observation next is [0.0, 0.391304347826087, 0.7156398104265403, 0.685, 1.0, 1.0, 0.49479148436826764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22540245938052428, 0.22540245938052428, 0.2943316239344258], 
reward next is 0.7057, 
noisyNet noise sample is [array([0.6787378], dtype=float32), 0.22160634]. 
=============================================
[2019-03-27 13:28:40,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1748757e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1047719e-32], sum to 1.0000
[2019-03-27 13:28:40,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6551
[2019-03-27 13:28:40,882] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 86.5, 1.0, 2.0, 0.5596684872221516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 782080.4502622526, 782080.4502622532, 193472.7470713609], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3911400.0000, 
sim time next is 3912000.0000, 
raw observation next is [28.33333333333334, 85.66666666666667, 1.0, 2.0, 0.565386746059028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 790074.113987782, 790074.1139877826, 194474.783560228], 
processed observation next is [0.0, 0.2608695652173913, 0.5418641390205374, 0.8566666666666667, 1.0, 1.0, 0.4763695735650939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21946503166327277, 0.21946503166327294, 0.2902608709854149], 
reward next is 0.7097, 
noisyNet noise sample is [array([-0.5912156], dtype=float32), 0.8826874]. 
=============================================
[2019-03-27 13:28:40,903] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.28571]
 [73.31528]
 [73.3394 ]
 [73.32619]
 [73.20057]], R is [[73.22693634]
 [73.2059021 ]
 [73.18655396]
 [73.16882324]
 [73.15197754]].
[2019-03-27 13:28:41,490] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1166091: loss 0.1263
[2019-03-27 13:28:41,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1166091: learning rate 0.0000
[2019-03-27 13:28:41,571] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1166118: loss 0.1035
[2019-03-27 13:28:41,573] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1166118: learning rate 0.0000
[2019-03-27 13:28:42,545] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1166497: loss 0.1386
[2019-03-27 13:28:42,548] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1166497: learning rate 0.0000
[2019-03-27 13:28:43,339] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1166794: loss 0.1093
[2019-03-27 13:28:43,342] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1166794: learning rate 0.0000
[2019-03-27 13:28:44,975] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1167419: loss 0.1212
[2019-03-27 13:28:44,980] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1167419: learning rate 0.0000
[2019-03-27 13:28:45,582] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4862361e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6738993e-31], sum to 1.0000
[2019-03-27 13:28:45,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0527
[2019-03-27 13:28:45,597] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.16666666666667, 61.83333333333333, 1.0, 2.0, 0.6104027810238561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 853004.9938681923, 853004.993868193, 202700.9115656583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3948600.0000, 
sim time next is 3949200.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.6139418457987807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857952.643682573, 857952.643682573, 203372.9679471235], 
processed observation next is [0.0, 0.7391304347826086, 0.8104265402843602, 0.63, 1.0, 1.0, 0.5348696937334707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2383201788007147, 0.2383201788007147, 0.303541743204662], 
reward next is 0.6965, 
noisyNet noise sample is [array([-0.11865691], dtype=float32), 1.3273516]. 
=============================================
[2019-03-27 13:28:47,541] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1168386: loss 0.1371
[2019-03-27 13:28:47,543] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1168387: learning rate 0.0000
[2019-03-27 13:28:47,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.4830908e-08 1.8801881e-03 8.5901008e-15 9.9811965e-01 2.6205766e-08], sum to 1.0000
[2019-03-27 13:28:47,704] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0610
[2019-03-27 13:28:47,710] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.66666666666666, 79.0, 1.0, 2.0, 0.9568549968388308, 1.0, 2.0, 0.9568549968388308, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2676488.071314696, 2676488.071314696, 503465.9012879105], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4095600.0000, 
sim time next is 4096200.0000, 
raw observation next is [30.83333333333334, 79.0, 1.0, 2.0, 0.9714190968216876, 1.0, 2.0, 0.9714190968216876, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2717270.667910184, 2717270.667910184, 511961.0116756849], 
processed observation next is [1.0, 0.391304347826087, 0.6603475513428123, 0.79, 1.0, 1.0, 0.9655651768935996, 1.0, 1.0, 0.9655651768935996, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7547974077528288, 0.7547974077528288, 0.7641209129487834], 
reward next is 0.2359, 
noisyNet noise sample is [array([-0.76269025], dtype=float32), -1.9240083]. 
=============================================
[2019-03-27 13:28:47,857] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.3647054e-19 1.0000000e+00 5.5691203e-27 1.5359645e-20 2.0255939e-17], sum to 1.0000
[2019-03-27 13:28:47,870] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9587
[2019-03-27 13:28:47,875] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.579546434008677, 0.0, 2.0, 0.0, 1.0, 1.0, 1.006480496938625, 6.911200000000001, 6.9112, 168.9125126976239, 1620354.923937473, 1620354.923937472, 354687.4612890676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3988800.0000, 
sim time next is 3989400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 9.321542237421134, 6.9112, 168.8990180853589, 3164670.11864987, 1454831.482619465, 308408.2508184198], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.24103422374211342, 0.0, 0.82937150110416, 0.8790750329582973, 0.40411985628318475, 0.4603108221170445], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.7417133], dtype=float32), 1.2010578]. 
=============================================
[2019-03-27 13:28:48,375] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1168697: loss 0.1653
[2019-03-27 13:28:48,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1168698: learning rate 0.0000
[2019-03-27 13:28:49,192] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1169006: loss 0.2596
[2019-03-27 13:28:49,197] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1169008: learning rate 0.0000
[2019-03-27 13:28:51,037] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1169703: loss 0.2307
[2019-03-27 13:28:51,042] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1169704: learning rate 0.0000
[2019-03-27 13:28:52,156] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1170132: loss 0.2127
[2019-03-27 13:28:52,163] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1170134: learning rate 0.0000
[2019-03-27 13:28:52,760] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1170355: loss 0.1299
[2019-03-27 13:28:52,765] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1170355: learning rate 0.0000
[2019-03-27 13:28:53,057] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1170470: loss -15.8907
[2019-03-27 13:28:53,062] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1170470: learning rate 0.0000
[2019-03-27 13:28:53,089] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1170481: loss 0.0537
[2019-03-27 13:28:53,092] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1170481: learning rate 0.0000
[2019-03-27 13:28:54,155] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1170885: loss 0.0824
[2019-03-27 13:28:54,159] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1170885: learning rate 0.0000
[2019-03-27 13:28:54,764] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1171121: loss 0.1466
[2019-03-27 13:28:54,766] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1171121: learning rate 0.0000
[2019-03-27 13:28:55,734] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4522432e-15 1.0000000e+00 5.3841966e-22 1.7066709e-10 6.2239011e-13], sum to 1.0000
[2019-03-27 13:28:55,747] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9502
[2019-03-27 13:28:55,761] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2740693.593811852 W.
[2019-03-27 13:28:55,773] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.0, 50.5, 1.0, 2.0, 0.6651980186698853, 1.0, 2.0, 0.6531890488492053, 1.0, 1.0, 1.03, 7.005094988212612, 6.9112, 170.5573041426782, 2740693.593811852, 2673432.794351235, 510561.679038465], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4207800.0000, 
sim time next is 4208400.0000, 
raw observation next is [36.0, 50.0, 1.0, 2.0, 0.7018977127849005, 1.0, 2.0, 0.6715388959067129, 1.0, 2.0, 1.03, 7.005097881789339, 6.9112, 170.5573041426782, 2817773.922704075, 2750511.050456746, 521929.0763246212], 
processed observation next is [1.0, 0.7391304347826086, 0.9052132701421801, 0.5, 1.0, 1.0, 0.6408406178131332, 1.0, 1.0, 0.6042637300080879, 1.0, 1.0, 1.0365853658536586, 0.009389788178933855, 0.0, 0.8375144448122397, 0.7827149785289097, 0.7640308473490961, 0.7789986213800316], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4123265], dtype=float32), 0.3384713]. 
=============================================
[2019-03-27 13:28:56,859] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1171905: loss 25.2454
[2019-03-27 13:28:56,862] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1171905: learning rate 0.0000
[2019-03-27 13:29:00,467] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7700110e-11 9.9999988e-01 4.7004342e-19 1.0570974e-07 3.1189593e-10], sum to 1.0000
[2019-03-27 13:29:00,478] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2669
[2019-03-27 13:29:00,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2661576.788987485 W.
[2019-03-27 13:29:00,493] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.66666666666667, 54.0, 1.0, 2.0, 0.9515298271474866, 1.0, 2.0, 0.9515298271474866, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2661576.788987485, 2661576.788987485, 500391.1063349234], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4189200.0000, 
sim time next is 4189800.0000, 
raw observation next is [35.83333333333333, 53.5, 1.0, 2.0, 0.9595144928766639, 1.0, 2.0, 0.9595144928766639, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2683935.129984501, 2683935.129984502, 505007.720963588], 
processed observation next is [1.0, 0.4782608695652174, 0.8973143759873615, 0.535, 1.0, 1.0, 0.9512222805742938, 1.0, 1.0, 0.9512222805742938, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7455375361068057, 0.7455375361068062, 0.7537428671098328], 
reward next is 0.2463, 
noisyNet noise sample is [array([0.29374897], dtype=float32), -1.4055115]. 
=============================================
[2019-03-27 13:29:02,780] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1174148: loss 35.8461
[2019-03-27 13:29:02,782] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1174149: learning rate 0.0000
[2019-03-27 13:29:02,792] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1174150: loss 32.9551
[2019-03-27 13:29:02,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1174151: learning rate 0.0000
[2019-03-27 13:29:03,872] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1174561: loss 25.0595
[2019-03-27 13:29:03,877] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1174563: learning rate 0.0000
[2019-03-27 13:29:04,395] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1174760: loss 27.1781
[2019-03-27 13:29:04,397] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1174760: learning rate 0.0000
[2019-03-27 13:29:05,025] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 13:29:05,028] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:29:05,029] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:29:05,033] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:29:05,035] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:29:05,035] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:29:05,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:29:05,035] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:29:05,041] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:29:05,042] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:29:05,037] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:29:05,055] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run48
[2019-03-27 13:29:05,079] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run48
[2019-03-27 13:29:05,100] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run48
[2019-03-27 13:29:05,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run48
[2019-03-27 13:29:05,140] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run48
[2019-03-27 13:29:16,207] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06097753], dtype=float32), 0.057916697]
[2019-03-27 13:29:16,210] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.15497479666667, 72.184649365, 1.0, 2.0, 0.2389637352411616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 394734.9179032752, 394734.9179032746, 159779.9514302865]
[2019-03-27 13:29:16,211] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:29:16,213] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.4972024e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2839892e-33], sampled 0.842470003254702
[2019-03-27 13:29:37,290] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06097753], dtype=float32), 0.057916697]
[2019-03-27 13:29:37,291] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.859034395, 91.12490695833334, 1.0, 2.0, 0.3465373452034228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540240.1391895899, 540240.1391895899, 170007.4700732159]
[2019-03-27 13:29:37,291] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:29:37,293] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.4320862e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4384477e-28], sampled 0.8494888143521827
[2019-03-27 13:29:53,000] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06097753], dtype=float32), 0.057916697]
[2019-03-27 13:29:53,002] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.95, 70.83333333333334, 1.0, 2.0, 0.5069236649047221, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708350.2876894749, 708350.2876894743, 184687.8163314035]
[2019-03-27 13:29:53,002] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:29:53,006] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.802893e-32 1.000000e+00 0.000000e+00 0.000000e+00 4.851415e-28], sampled 0.2779275733395915
[2019-03-27 13:29:57,004] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06097753], dtype=float32), 0.057916697]
[2019-03-27 13:29:57,006] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.03333333333333, 64.33333333333333, 1.0, 2.0, 0.5692186357673302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795430.8197846648, 795430.8197846648, 195151.3327711027]
[2019-03-27 13:29:57,006] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:29:57,011] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.9906884e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6638496e-27], sampled 0.9363227506413719
[2019-03-27 13:29:59,067] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06097753], dtype=float32), 0.057916697]
[2019-03-27 13:29:59,068] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.83907307, 80.40393479, 1.0, 2.0, 0.4969878397627452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 694461.914221721, 694461.9142217203, 183124.6987117255]
[2019-03-27 13:29:59,069] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:29:59,073] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.0157864e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.2357244e-32], sampled 0.5601383461222227
[2019-03-27 13:30:24,657] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06097753], dtype=float32), 0.057916697]
[2019-03-27 13:30:24,658] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.70511833, 84.21937756, 1.0, 2.0, 0.5360761691885799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749100.9323803002, 749100.9323803008, 189441.5557689936]
[2019-03-27 13:30:24,661] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:30:24,664] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2872622e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1072311e-26], sampled 0.055175369322697065
[2019-03-27 13:30:42,329] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06097753], dtype=float32), 0.057916697]
[2019-03-27 13:30:42,331] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.83333333333333, 90.33333333333334, 1.0, 2.0, 0.5390290820073553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753228.7301823727, 753228.7301823722, 189936.9319416075]
[2019-03-27 13:30:42,333] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:30:42,335] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1446833e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3628824e-28], sampled 0.4671081317276373
[2019-03-27 13:30:47,636] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06097753], dtype=float32), 0.057916697]
[2019-03-27 13:30:47,637] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.76666666666667, 81.5, 1.0, 2.0, 0.5296084936933778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740059.997938766, 740059.997938766, 188364.7580568799]
[2019-03-27 13:30:47,638] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:30:47,642] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3378107e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.7034881e-29], sampled 0.577645294140293
[2019-03-27 13:31:12,747] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 13:31:12,764] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.7751 3164042116.6984 1776.0000
[2019-03-27 13:31:13,158] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06097753], dtype=float32), 0.057916697]
[2019-03-27 13:31:13,159] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.094859, 64.48389455, 1.0, 2.0, 0.9481240097067953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1325246.712475108, 1325246.712475107, 283518.4392993143]
[2019-03-27 13:31:13,160] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:31:13,162] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.37365085e-27 1.00000000e+00 4.05361593e-36 1.64323564e-35
 9.13826254e-24], sampled 0.7293089331010574
[2019-03-27 13:31:13,203] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8113 3007673435.5440 1768.0000
[2019-03-27 13:31:13,309] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 13:31:13,362] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 13:31:14,379] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1175000, evaluation results [1175000.0, 7884.775092800427, 3164042116.6984243, 1776.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7997.811294675274, 3007673435.543974, 1768.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 13:31:15,438] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1175402: loss 24.4952
[2019-03-27 13:31:15,441] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1175402: learning rate 0.0000
[2019-03-27 13:31:15,474] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.5293140e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.2269577e-26], sum to 1.0000
[2019-03-27 13:31:15,485] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1135
[2019-03-27 13:31:15,494] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6188710837720963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864843.8086701048, 864843.8086701054, 204314.6587111072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4401600.0000, 
sim time next is 4402200.0000, 
raw observation next is [30.16666666666666, 83.16666666666666, 1.0, 2.0, 0.6178550359089731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863423.3514857844, 863423.3514857844, 204119.8234795695], 
processed observation next is [1.0, 0.9565217391304348, 0.6287519747235385, 0.8316666666666666, 1.0, 1.0, 0.5395843806132206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23983981985716232, 0.23983981985716232, 0.3046564529545813], 
reward next is 0.6953, 
noisyNet noise sample is [array([0.9998734], dtype=float32), 0.91818094]. 
=============================================
[2019-03-27 13:31:16,890] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7118945e-06 7.7459526e-01 2.9457171e-13 2.2539310e-01 9.8796863e-06], sum to 1.0000
[2019-03-27 13:31:16,897] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0259
[2019-03-27 13:31:16,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3599504.510191515 W.
[2019-03-27 13:31:16,914] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.33333333333334, 59.00000000000001, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.873551744603519, 6.9112, 170.5573041426782, 3599504.510191515, 2910132.809565882, 548136.8168213891], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4274400.0000, 
sim time next is 4275000.0000, 
raw observation next is [36.5, 58.5, 1.0, 2.0, 0.9220556242396909, 1.0, 2.0, 0.7816178516341079, 1.0, 1.0, 1.03, 7.005115247571641, 6.9112, 170.5573041426782, 3280271.234481911, 3212995.922418324, 600647.7759302304], 
processed observation next is [1.0, 0.4782608695652174, 0.9289099526066351, 0.585, 1.0, 1.0, 0.9060911135417963, 1.0, 1.0, 0.7368889778724191, 1.0, 0.5, 1.0365853658536586, 0.009391524757164139, 0.0, 0.8375144448122397, 0.9111864540227531, 0.8924988673384233, 0.896489217806314], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4716622], dtype=float32), 0.7601755]. 
=============================================
[2019-03-27 13:31:16,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[39.830482]
 [40.071754]
 [41.598434]
 [42.289883]
 [42.4182  ]], R is [[39.57010269]
 [39.17440033]
 [38.78265762]
 [38.39483261]
 [38.01088333]].
[2019-03-27 13:31:18,106] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1176421: loss 29.9390
[2019-03-27 13:31:18,111] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1176421: learning rate 0.0000
[2019-03-27 13:31:18,968] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1176749: loss 29.8631
[2019-03-27 13:31:18,971] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1176749: learning rate 0.0000
[2019-03-27 13:31:19,719] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1177032: loss 22.7907
[2019-03-27 13:31:19,722] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1177032: learning rate 0.0000
[2019-03-27 13:31:21,721] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1177781: loss 23.3591
[2019-03-27 13:31:21,729] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1177781: learning rate 0.0000
[2019-03-27 13:31:22,800] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1178190: loss 0.0372
[2019-03-27 13:31:22,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1178190: learning rate 0.0000
[2019-03-27 13:31:22,835] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1178206: loss 27.5289
[2019-03-27 13:31:22,837] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1178207: learning rate 0.0000
[2019-03-27 13:31:23,412] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1178418: loss 21.0219
[2019-03-27 13:31:23,415] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1178420: learning rate 0.0000
[2019-03-27 13:31:23,879] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1178598: loss 19.0778
[2019-03-27 13:31:23,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1178598: learning rate 0.0000
[2019-03-27 13:31:24,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7912938e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0785447e-27], sum to 1.0000
[2019-03-27 13:31:24,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3674
[2019-03-27 13:31:24,267] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5514555675697278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770599.5575740221, 770599.5575740221, 192050.3046828069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4484400.0000, 
sim time next is 4485000.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5512385033362721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770296.1237828465, 770296.1237828465, 192013.0073858922], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45932349799550853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21397114549523513, 0.21397114549523513, 0.28658657818789884], 
reward next is 0.7134, 
noisyNet noise sample is [array([-1.0871574], dtype=float32), -1.2698727]. 
=============================================
[2019-03-27 13:31:24,278] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.55365 ]
 [76.55275 ]
 [76.521545]
 [76.50021 ]
 [76.46627 ]], R is [[76.49714661]
 [76.44553375]
 [76.3946991 ]
 [76.34444427]
 [76.29402161]].
[2019-03-27 13:31:25,052] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1179045: loss 8.6800
[2019-03-27 13:31:25,056] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1179045: learning rate 0.0000
[2019-03-27 13:31:25,543] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1179225: loss 19.2970
[2019-03-27 13:31:25,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1179225: learning rate 0.0000
[2019-03-27 13:31:26,849] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1179709: loss 0.3161
[2019-03-27 13:31:26,850] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1179709: learning rate 0.0000
[2019-03-27 13:31:29,001] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.0304361e-28 1.0000000e+00 1.3748615e-35 4.0686124e-36 7.2738386e-21], sum to 1.0000
[2019-03-27 13:31:29,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6927
[2019-03-27 13:31:29,018] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.83333333333333, 1.0, 2.0, 0.5508464621512057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769748.0901731236, 769748.0901731236, 191946.0132456327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4579800.0000, 
sim time next is 4580400.0000, 
raw observation next is [28.0, 85.66666666666667, 1.0, 2.0, 0.5554147059510528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776134.0468403613, 776134.0468403606, 192733.7021832155], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.8566666666666667, 1.0, 1.0, 0.464355067410907, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21559279078898927, 0.21559279078898907, 0.28766224206450075], 
reward next is 0.7123, 
noisyNet noise sample is [array([0.903371], dtype=float32), 0.5782105]. 
=============================================
[2019-03-27 13:31:32,988] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1182037: loss 0.3354
[2019-03-27 13:31:32,991] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1182037: learning rate 0.0000
[2019-03-27 13:31:33,155] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1182101: loss 0.0382
[2019-03-27 13:31:33,158] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1182101: learning rate 0.0000
[2019-03-27 13:31:33,272] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2231622e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.3225966e-27], sum to 1.0000
[2019-03-27 13:31:33,280] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6423
[2019-03-27 13:31:33,284] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5284096911494777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738384.2427864469, 738384.2427864476, 188166.5361804091], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.529300894799023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739630.018258668, 739630.0182586674, 188313.6533800974], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.43289264433617225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20545278284963, 0.20545278284962984, 0.28106515429865286], 
reward next is 0.7189, 
noisyNet noise sample is [array([0.22467688], dtype=float32), 1.4168104]. 
=============================================
[2019-03-27 13:31:34,211] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1182497: loss 0.0724
[2019-03-27 13:31:34,213] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1182498: learning rate 0.0000
[2019-03-27 13:31:34,690] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1182683: loss 0.0653
[2019-03-27 13:31:34,693] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1182683: learning rate 0.0000
[2019-03-27 13:31:36,409] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1183329: loss 0.1261
[2019-03-27 13:31:36,411] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1183329: learning rate 0.0000
[2019-03-27 13:31:39,355] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1184437: loss 0.0258
[2019-03-27 13:31:39,358] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1184438: learning rate 0.0000
[2019-03-27 13:31:39,885] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1184641: loss 0.0306
[2019-03-27 13:31:39,890] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1184642: learning rate 0.0000
[2019-03-27 13:31:40,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0169899e-16 1.0000000e+00 6.7146539e-24 1.7809908e-18 1.7282235e-12], sum to 1.0000
[2019-03-27 13:31:40,180] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7023
[2019-03-27 13:31:40,190] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 84.83333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.026250125911212, 6.9112, 168.9124403132179, 1535431.092111308, 1453810.822502083, 311356.4029639142], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4607400.0000, 
sim time next is 4608000.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.983779399454038, 6.9112, 168.9121791676893, 1505280.277919357, 1453790.190916609, 311356.4013029789], 
processed observation next is [1.0, 0.34782608695652173, 0.6208530805687204, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.007257939945403802, 0.0, 0.8294361280435718, 0.4181334105331547, 0.40383060858794695, 0.46471104672086405], 
reward next is 0.1724, 
noisyNet noise sample is [array([-0.516641], dtype=float32), 0.9908833]. 
=============================================
[2019-03-27 13:31:40,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[44.73729 ]
 [43.841377]
 [44.939278]
 [49.416546]
 [50.147255]], R is [[46.30722046]
 [45.84415054]
 [45.38570786]
 [45.40916824]
 [44.95507812]].
[2019-03-27 13:31:40,810] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1184985: loss 0.0626
[2019-03-27 13:31:40,812] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1184985: learning rate 0.0000
[2019-03-27 13:31:42,693] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1185707: loss 0.0095
[2019-03-27 13:31:42,696] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1185708: learning rate 0.0000
[2019-03-27 13:31:43,592] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1186046: loss 0.0734
[2019-03-27 13:31:43,595] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1186048: learning rate 0.0000
[2019-03-27 13:31:44,420] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1186360: loss 0.0094
[2019-03-27 13:31:44,421] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1186360: learning rate 0.0000
[2019-03-27 13:31:44,543] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1186405: loss 64.2019
[2019-03-27 13:31:44,548] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1186406: learning rate 0.0000
[2019-03-27 13:31:44,868] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1186523: loss 0.2432
[2019-03-27 13:31:44,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1186523: learning rate 0.0000
[2019-03-27 13:31:45,221] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.19231885e-29 1.00000000e+00 0.00000000e+00 0.00000000e+00
 3.13694616e-22], sum to 1.0000
[2019-03-27 13:31:45,231] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2280
[2019-03-27 13:31:45,238] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4927319954189953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688513.1175253581, 688513.1175253581, 182464.9332078178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4826400.0000, 
sim time next is 4827000.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.493551619166988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689658.780671726, 689658.7806717253, 182591.6383510742], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.74, 1.0, 1.0, 0.38982122791203366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19157188351992388, 0.19157188351992369, 0.2725248333598122], 
reward next is 0.7275, 
noisyNet noise sample is [array([-1.2667804], dtype=float32), 0.773535]. 
=============================================
[2019-03-27 13:31:45,253] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.31521 ]
 [71.64268 ]
 [71.446785]
 [71.5118  ]
 [71.58779 ]], R is [[71.42263794]
 [71.4360733 ]
 [71.4495163 ]
 [71.46290588]
 [71.47618866]].
[2019-03-27 13:31:45,924] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1186919: loss 0.0978
[2019-03-27 13:31:45,926] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1186919: learning rate 0.0000
[2019-03-27 13:31:46,397] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3984318e-13 9.9999988e-01 6.1215217e-20 2.9900016e-11 1.2822770e-07], sum to 1.0000
[2019-03-27 13:31:46,408] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5500
[2019-03-27 13:31:46,414] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 64.5, 1.0, 2.0, 0.3830362499550641, 1.0, 1.0, 0.3830362499550641, 1.0, 2.0, 0.6629484229170346, 6.9112, 6.9112, 170.5573041426782, 1606375.331415386, 1606375.331415386, 343398.0103765382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4797000.0000, 
sim time next is 4797600.0000, 
raw observation next is [31.66666666666666, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.56830003420611, 6.9112, 168.9093133391052, 1920233.556471356, 1454074.231065358, 311351.7833758769], 
processed observation next is [1.0, 0.5217391304347826, 0.6998420221169034, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.06571000342061098, 0.0, 0.829422055513242, 0.5333982101309322, 0.4039095086292661, 0.46470415429235357], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6080626], dtype=float32), 0.22929132]. 
=============================================
[2019-03-27 13:31:46,492] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1187136: loss 0.0736
[2019-03-27 13:31:46,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1187136: learning rate 0.0000
[2019-03-27 13:31:48,450] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1187875: loss 43.7483
[2019-03-27 13:31:48,453] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1187875: learning rate 0.0000
[2019-03-27 13:31:54,316] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1190093: loss 54.1484
[2019-03-27 13:31:54,319] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1190095: learning rate 0.0000
[2019-03-27 13:31:54,383] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1190117: loss 59.3584
[2019-03-27 13:31:54,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1190118: learning rate 0.0000
[2019-03-27 13:31:55,386] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1190495: loss 44.2710
[2019-03-27 13:31:55,390] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1190495: learning rate 0.0000
[2019-03-27 13:31:55,890] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1190683: loss 59.0620
[2019-03-27 13:31:55,893] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1190684: learning rate 0.0000
[2019-03-27 13:31:57,458] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1191280: loss 52.8839
[2019-03-27 13:31:57,461] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1191281: learning rate 0.0000
[2019-03-27 13:31:58,927] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7331070e-29 1.0000000e+00 2.0919855e-37 0.0000000e+00 1.0868455e-21], sum to 1.0000
[2019-03-27 13:31:58,938] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1226
[2019-03-27 13:31:58,943] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5206245008709525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727501.7203321003, 727501.7203320996, 186890.6140350714], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4995600.0000, 
sim time next is 4996200.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5190113034278284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725246.7274278646, 725246.7274278653, 186628.4037743669], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.42049554629858843, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20145742428551794, 0.20145742428551813, 0.27854985637965207], 
reward next is 0.7215, 
noisyNet noise sample is [array([-0.19618623], dtype=float32), 0.07852299]. 
=============================================
[2019-03-27 13:31:59,873] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.5139313e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9578671e-23], sum to 1.0000
[2019-03-27 13:31:59,886] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8830
[2019-03-27 13:31:59,892] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4757018060878663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664837.0839761337, 664837.0839761337, 179892.8770539988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5018400.0000, 
sim time next is 5019000.0000, 
raw observation next is [26.0, 84.83333333333333, 1.0, 2.0, 0.4778245022237005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667675.7651618401, 667675.7651618401, 180194.3905076849], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.8483333333333333, 1.0, 1.0, 0.3708728942454223, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18546549032273338, 0.18546549032273338, 0.2689468515040073], 
reward next is 0.7311, 
noisyNet noise sample is [array([-1.5746428], dtype=float32), -0.091120735]. 
=============================================
[2019-03-27 13:31:59,905] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[75.787384]
 [75.3546  ]
 [75.237   ]
 [75.05985 ]
 [74.88341 ]], R is [[76.33532715]
 [76.30348206]
 [76.27188873]
 [76.24051666]
 [76.20934296]].
[2019-03-27 13:32:00,548] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1192438: loss 32.9486
[2019-03-27 13:32:00,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1192438: learning rate 0.0000
[2019-03-27 13:32:01,024] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1192616: loss 55.5665
[2019-03-27 13:32:01,027] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1192617: learning rate 0.0000
[2019-03-27 13:32:01,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9095612e-30 1.0000000e+00 3.1720941e-38 0.0000000e+00 1.4653905e-23], sum to 1.0000
[2019-03-27 13:32:01,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5436
[2019-03-27 13:32:01,886] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5510132275605634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769981.2111797953, 769981.2111797959, 191975.1303179837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5150400.0000, 
sim time next is 5151000.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5510141368343565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769982.482251813, 769982.4822518125, 191975.2864645808], 
processed observation next is [0.0, 0.6086956521739131, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45905317690886327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21388402284772584, 0.21388402284772567, 0.2865302783053445], 
reward next is 0.7135, 
noisyNet noise sample is [array([-0.8992878], dtype=float32), 0.91695607]. 
=============================================
[2019-03-27 13:32:01,901] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.76772 ]
 [74.77412 ]
 [74.76251 ]
 [74.724815]
 [74.73264 ]], R is [[74.7106781 ]
 [74.6770401 ]
 [74.64374542]
 [74.61038971]
 [74.57577515]].
[2019-03-27 13:32:02,014] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1192991: loss 35.4385
[2019-03-27 13:32:02,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1192991: learning rate 0.0000
[2019-03-27 13:32:03,909] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1193695: loss 58.9418
[2019-03-27 13:32:03,911] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1193695: learning rate 0.0000
[2019-03-27 13:32:04,813] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1194044: loss 33.8084
[2019-03-27 13:32:04,817] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1194045: learning rate 0.0000
[2019-03-27 13:32:05,505] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1194303: loss 0.7406
[2019-03-27 13:32:05,508] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1194303: learning rate 0.0000
[2019-03-27 13:32:05,684] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1194372: loss 58.7431
[2019-03-27 13:32:05,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1194372: learning rate 0.0000
[2019-03-27 13:32:06,241] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1194575: loss 55.6519
[2019-03-27 13:32:06,244] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1194575: learning rate 0.0000
[2019-03-27 13:32:07,195] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1194946: loss 54.1850
[2019-03-27 13:32:07,199] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1194947: learning rate 0.0000
[2019-03-27 13:32:08,011] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1195252: loss 50.1707
[2019-03-27 13:32:08,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1195253: learning rate 0.0000
[2019-03-27 13:32:09,415] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1195776: loss 0.0389
[2019-03-27 13:32:09,420] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1195776: learning rate 0.0000
[2019-03-27 13:32:10,987] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5071450e-28 1.0000000e+00 1.1120931e-36 2.6921123e-37 3.2916357e-20], sum to 1.0000
[2019-03-27 13:32:11,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6965
[2019-03-27 13:32:11,007] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5164354593231327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721646.1178474353, 721646.1178474353, 186210.8231642146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5095800.0000, 
sim time next is 5096400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5156883475053693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720601.7797672314, 720601.7797672321, 186090.2565777657], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.416491984946228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20016716104645318, 0.20016716104645338, 0.2777466516086055], 
reward next is 0.7223, 
noisyNet noise sample is [array([-0.92432296], dtype=float32), -1.5062997]. 
=============================================
[2019-03-27 13:32:15,334] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1198016: loss 0.0476
[2019-03-27 13:32:15,338] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1198017: learning rate 0.0000
[2019-03-27 13:32:15,498] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1198077: loss 0.0269
[2019-03-27 13:32:15,499] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1198077: learning rate 0.0000
[2019-03-27 13:32:16,388] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1198413: loss 0.0703
[2019-03-27 13:32:16,389] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1198413: learning rate 0.0000
[2019-03-27 13:32:16,967] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1198643: loss 0.0625
[2019-03-27 13:32:16,969] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1198643: learning rate 0.0000
[2019-03-27 13:32:18,335] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.2395373e-22 1.0000000e+00 2.0891232e-29 1.9469798e-29 2.0583020e-16], sum to 1.0000
[2019-03-27 13:32:18,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5401
[2019-03-27 13:32:18,355] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1681444.903759142 W.
[2019-03-27 13:32:18,362] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.41666666666667, 80.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.231930301401244, 6.9112, 168.9107479819956, 1681444.903759142, 1453910.764296407, 311355.9275346305], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5381400.0000, 
sim time next is 5382000.0000, 
raw observation next is [30.6, 79.0, 1.0, 2.0, 0.5339272733374691, 1.0, 1.0, 0.5339272733374691, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1492708.420244207, 1492708.420244207, 311143.9567452329], 
processed observation next is [1.0, 0.30434782608695654, 0.6492890995260664, 0.79, 1.0, 1.0, 0.4384665943824929, 1.0, 0.5, 0.4384665943824929, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.414641227845613, 0.414641227845613, 0.46439396529139243], 
reward next is 0.5356, 
noisyNet noise sample is [array([0.1634836], dtype=float32), 1.134953]. 
=============================================
[2019-03-27 13:32:18,399] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[48.980553]
 [49.66678 ]
 [50.37964 ]
 [51.038124]
 [52.53327 ]], R is [[46.95829391]
 [46.48871231]
 [46.0238266 ]
 [45.56358719]
 [45.65986633]].
[2019-03-27 13:32:18,694] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1199292: loss 0.4842
[2019-03-27 13:32:18,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1199293: learning rate 0.0000
[2019-03-27 13:32:20,570] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 13:32:20,571] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:32:20,572] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:32:20,573] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:32:20,574] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:32:20,574] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:32:20,575] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:32:20,576] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:32:20,576] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:32:20,577] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:32:20,578] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:32:20,608] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run49
[2019-03-27 13:32:20,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run49
[2019-03-27 13:32:20,629] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run49
[2019-03-27 13:32:20,676] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run49
[2019-03-27 13:32:20,695] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run49
[2019-03-27 13:33:04,765] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06334163], dtype=float32), 0.060596686]
[2019-03-27 13:33:04,767] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.41666666666667, 77.83333333333334, 1.0, 2.0, 0.5077014171903753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709437.4435315451, 709437.4435315451, 184810.5005000518]
[2019-03-27 13:33:04,768] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:33:04,771] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7083014e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5935698e-24], sampled 0.3142629318216451
[2019-03-27 13:33:07,958] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06334163], dtype=float32), 0.060596686]
[2019-03-27 13:33:07,958] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3511670205972222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 540963.3487664761, 540963.3487664767, 169895.9395900932]
[2019-03-27 13:33:07,958] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:33:07,960] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.022443e-34 1.000000e+00 0.000000e+00 0.000000e+00 4.637878e-28], sampled 0.48720783501529397
[2019-03-27 13:33:11,728] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06334163], dtype=float32), 0.060596686]
[2019-03-27 13:33:11,730] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.77175190666667, 96.11600237333334, 1.0, 2.0, 0.3093911428054031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490390.1077904542, 490390.1077904536, 166335.900852136]
[2019-03-27 13:33:11,731] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:33:11,735] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.1060191e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.7367886e-29], sampled 0.24283194251921503
[2019-03-27 13:33:12,749] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06334163], dtype=float32), 0.060596686]
[2019-03-27 13:33:12,750] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.90000000000001, 52.0, 1.0, 2.0, 0.8959582230677271, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005987522158039, 6.9112, 168.9123159652221, 2149379.151006536, 2082133.875458795, 432935.3536150305]
[2019-03-27 13:33:12,751] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:33:12,754] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.0799714e-18 1.0000000e+00 2.6459652e-26 6.7586405e-19 6.7675527e-12], sampled 0.2563848321842004
[2019-03-27 13:33:12,755] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2149379.151006536 W.
[2019-03-27 13:33:49,287] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06334163], dtype=float32), 0.060596686]
[2019-03-27 13:33:49,289] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.7, 74.33333333333334, 1.0, 2.0, 0.558444275389493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780369.1088745949, 780369.1088745949, 193258.9642051567]
[2019-03-27 13:33:49,290] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:33:49,295] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0166481e-28 1.0000000e+00 1.4659000e-37 4.2276502e-38 3.0484226e-22], sampled 0.1439981905835941
[2019-03-27 13:33:50,508] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06334163], dtype=float32), 0.060596686]
[2019-03-27 13:33:50,509] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.20995286, 51.59198354, 1.0, 2.0, 0.9587094135363573, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564503125, 1340051.869924022, 1340051.869924021, 286594.0858764286]
[2019-03-27 13:33:50,510] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:33:50,512] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3125726e-21 1.0000000e+00 7.9803977e-30 1.5426722e-25 1.7185829e-15], sampled 0.7937968985711757
[2019-03-27 13:34:28,380] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.7334 2927247875.2881 1338.0000
[2019-03-27 13:34:29,126] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.3542 2842639586.0143 1131.0000
[2019-03-27 13:34:29,372] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8004.0763 3007185531.9148 1758.0000
[2019-03-27 13:34:29,400] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.3488 2779170744.1757 930.0000
[2019-03-27 13:34:29,406] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7891.9079 3162945657.7806 1753.0000
[2019-03-27 13:34:30,420] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1200000, evaluation results [1200000.0, 7891.907938777589, 3162945657.7805514, 1753.0, 8254.73344550888, 2927247875.288058, 1338.0, 8662.348763295671, 2779170744.1757083, 930.0, 8004.07630231024, 3007185531.914777, 1758.0, 8496.354188084728, 2842639586.0142555, 1131.0]
[2019-03-27 13:34:31,346] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9049308e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7016319e-27], sum to 1.0000
[2019-03-27 13:34:31,361] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9425
[2019-03-27 13:34:31,371] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 78.0, 1.0, 2.0, 0.6284275537215386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 878204.056099985, 878204.0560999856, 206162.5132255311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5342400.0000, 
sim time next is 5343000.0000, 
raw observation next is [31.41666666666666, 78.16666666666667, 1.0, 2.0, 0.630550140738688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 881171.5227346177, 881171.5227346184, 206576.2042180133], 
processed observation next is [1.0, 0.8695652173913043, 0.6879936808846759, 0.7816666666666667, 1.0, 1.0, 0.5548796876369735, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2447698674262827, 0.2447698674262829, 0.3083226928627064], 
reward next is 0.6917, 
noisyNet noise sample is [array([0.4350566], dtype=float32), -1.5795716]. 
=============================================
[2019-03-27 13:34:31,389] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.61524 ]
 [68.49397 ]
 [68.94686 ]
 [69.151886]
 [69.24781 ]], R is [[68.23847198]
 [68.24838257]
 [68.2587738 ]
 [68.26959991]
 [68.28059387]].
[2019-03-27 13:34:31,754] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1200506: loss 0.0926
[2019-03-27 13:34:31,759] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1200507: learning rate 0.0000
[2019-03-27 13:34:32,166] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1200668: loss 0.2642
[2019-03-27 13:34:32,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1200668: learning rate 0.0000
[2019-03-27 13:34:33,351] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1201110: loss 0.0491
[2019-03-27 13:34:33,357] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1201111: learning rate 0.0000
[2019-03-27 13:34:33,963] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5471375e-20 1.0000000e+00 3.3943579e-30 1.5850223e-29 3.5816394e-17], sum to 1.0000
[2019-03-27 13:34:33,978] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1266
[2019-03-27 13:34:33,989] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.68333333333334, 84.83333333333334, 1.0, 2.0, 0.9271896082850597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1295967.652439482, 1295967.652439482, 277531.9163570415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5379000.0000, 
sim time next is 5379600.0000, 
raw observation next is [29.86666666666667, 83.66666666666667, 1.0, 2.0, 1.00421400093909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1403698.707146698, 1403698.707146698, 300218.0830243568], 
processed observation next is [1.0, 0.2608695652173913, 0.6145339652448659, 0.8366666666666667, 1.0, 1.0, 1.0050771095651687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3899163075407494, 0.3899163075407494, 0.4480866910811296], 
reward next is 0.5519, 
noisyNet noise sample is [array([-0.42680243], dtype=float32), -0.45430252]. 
=============================================
[2019-03-27 13:34:35,099] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1201770: loss 0.8387
[2019-03-27 13:34:35,102] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1201770: learning rate 0.0000
[2019-03-27 13:34:35,506] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5454656e-07 5.2577849e-07 2.9081592e-14 9.9990046e-01 9.8838878e-05], sum to 1.0000
[2019-03-27 13:34:35,520] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5682
[2019-03-27 13:34:35,526] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.13333333333333, 66.66666666666667, 1.0, 2.0, 0.9624738769241316, 1.0, 2.0, 0.9624738769241316, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2692221.980233966, 2692221.980233966, 506729.7243900104], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5304000.0000, 
sim time next is 5304600.0000, 
raw observation next is [33.45, 65.0, 1.0, 2.0, 0.9614983163131616, 1.0, 2.0, 0.9614983163131616, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2689490.214928734, 2689490.214928734, 506161.8778847878], 
processed observation next is [1.0, 0.391304347826087, 0.7843601895734599, 0.65, 1.0, 1.0, 0.9536124292929659, 1.0, 1.0, 0.9536124292929659, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7470806152579818, 0.7470806152579818, 0.7554654893802802], 
reward next is 0.2445, 
noisyNet noise sample is [array([-1.8184959], dtype=float32), -0.9619452]. 
=============================================
[2019-03-27 13:34:35,776] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1202025: loss 0.0859
[2019-03-27 13:34:35,777] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1202025: learning rate 0.0000
[2019-03-27 13:34:36,696] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1202377: loss 0.2456
[2019-03-27 13:34:36,700] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1202377: learning rate 0.0000
[2019-03-27 13:34:36,827] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1202421: loss 0.1488
[2019-03-27 13:34:36,830] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1202422: learning rate 0.0000
[2019-03-27 13:34:37,310] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1202612: loss 0.0779
[2019-03-27 13:34:37,316] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1202613: learning rate 0.0000
[2019-03-27 13:34:38,200] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1202937: loss 0.0703
[2019-03-27 13:34:38,208] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1202938: learning rate 0.0000
[2019-03-27 13:34:39,000] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1203238: loss 0.0084
[2019-03-27 13:34:39,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1203239: learning rate 0.0000
[2019-03-27 13:34:40,513] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1203807: loss 0.3005
[2019-03-27 13:34:40,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1203807: learning rate 0.0000
[2019-03-27 13:34:44,069] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7699583e-22 1.0000000e+00 6.7628579e-30 6.0341315e-30 3.4041869e-15], sum to 1.0000
[2019-03-27 13:34:44,081] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6145
[2019-03-27 13:34:44,087] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.15, 82.5, 1.0, 2.0, 0.615268035246576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 859806.676429168, 859806.6764291673, 203625.032445763], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5351400.0000, 
sim time next is 5352000.0000, 
raw observation next is [30.06666666666666, 83.0, 1.0, 2.0, 0.6145502430742251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 858803.1918004336, 858803.191800433, 203488.1768498952], 
processed observation next is [1.0, 0.9565217391304348, 0.6240126382306473, 0.83, 1.0, 1.0, 0.5356027024990664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2385564421667871, 0.23855644216678693, 0.30371369679088833], 
reward next is 0.6963, 
noisyNet noise sample is [array([0.47134933], dtype=float32), -0.1531293]. 
=============================================
[2019-03-27 13:34:44,107] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[61.902184]
 [61.869484]
 [61.822643]
 [61.8454  ]
 [62.07501 ]], R is [[62.00071335]
 [62.07678986]
 [62.15189743]
 [62.22643661]
 [62.30069733]].
[2019-03-27 13:34:46,374] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2419594e-22 1.0000000e+00 9.0113237e-31 9.3650365e-32 1.3780403e-17], sum to 1.0000
[2019-03-27 13:34:46,382] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6884
[2019-03-27 13:34:46,386] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 85.16666666666667, 1.0, 2.0, 1.003157796075264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1402221.360106829, 1402221.36010683, 299893.1620900814], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5465400.0000, 
sim time next is 5466000.0000, 
raw observation next is [29.1, 84.33333333333334, 1.0, 2.0, 1.014097425310018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1417523.064023287, 1417523.064023288, 303254.4921827132], 
processed observation next is [1.0, 0.2608695652173913, 0.5781990521327015, 0.8433333333333334, 1.0, 1.0, 1.0169848497711058, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3937564066731353, 0.39375640667313555, 0.45261864504882565], 
reward next is 0.5474, 
noisyNet noise sample is [array([-0.27176303], dtype=float32), 1.1644131]. 
=============================================
[2019-03-27 13:34:46,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[44.246056]
 [45.53682 ]
 [45.623253]
 [45.55195 ]
 [44.985497]], R is [[43.77049255]
 [43.88518524]
 [44.08148575]
 [44.27656937]
 [44.47237778]].
[2019-03-27 13:34:46,406] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1206044: loss 0.6100
[2019-03-27 13:34:46,410] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1206045: learning rate 0.0000
[2019-03-27 13:34:46,565] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1206105: loss 0.5233
[2019-03-27 13:34:46,570] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1206106: learning rate 0.0000
[2019-03-27 13:34:47,322] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1206394: loss 0.2735
[2019-03-27 13:34:47,328] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1206394: learning rate 0.0000
[2019-03-27 13:34:47,849] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1206593: loss 0.4780
[2019-03-27 13:34:47,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1206594: learning rate 0.0000
[2019-03-27 13:34:49,653] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1207270: loss 0.2679
[2019-03-27 13:34:49,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1207271: learning rate 0.0000
[2019-03-27 13:34:50,838] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0026226e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2231085e-26], sum to 1.0000
[2019-03-27 13:34:50,852] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4264
[2019-03-27 13:34:50,857] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333334, 88.33333333333334, 1.0, 2.0, 0.5678499471285307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793517.4885544695, 793517.4885544688, 194909.1621618885], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5602800.0000, 
sim time next is 5603400.0000, 
raw observation next is [27.85, 88.5, 1.0, 2.0, 0.5651462421135254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789737.9076967178, 789737.9076967178, 194432.3052078924], 
processed observation next is [1.0, 0.8695652173913043, 0.5189573459715641, 0.885, 1.0, 1.0, 0.47607980977533176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21937164102686604, 0.21937164102686604, 0.2901974704595409], 
reward next is 0.7098, 
noisyNet noise sample is [array([1.7786669], dtype=float32), -0.0152940005]. 
=============================================
[2019-03-27 13:34:51,275] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6223324e-28 1.0000000e+00 1.0720383e-36 0.0000000e+00 8.5560790e-20], sum to 1.0000
[2019-03-27 13:34:51,285] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9333
[2019-03-27 13:34:51,296] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 90.0, 1.0, 2.0, 0.5464752681517986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763637.6257394119, 763637.6257394119, 191197.798214859], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5609400.0000, 
sim time next is 5610000.0000, 
raw observation next is [26.93333333333334, 90.0, 1.0, 2.0, 0.5440535270929651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760252.3043683963, 760252.3043683963, 190785.8175681132], 
processed observation next is [1.0, 0.9565217391304348, 0.4755134281200636, 0.9, 1.0, 1.0, 0.45066690011200605, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21118119565788787, 0.21118119565788787, 0.28475495159419884], 
reward next is 0.7152, 
noisyNet noise sample is [array([-1.5163662], dtype=float32), -0.36825517]. 
=============================================
[2019-03-27 13:34:51,316] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[69.62878 ]
 [69.62752 ]
 [69.56678 ]
 [69.87846 ]
 [70.044876]], R is [[69.67974091]
 [69.6975708 ]
 [69.71452332]
 [69.7305603 ]
 [69.74588013]].
[2019-03-27 13:34:52,851] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1208478: loss 0.3523
[2019-03-27 13:34:52,852] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1208478: learning rate 0.0000
[2019-03-27 13:34:53,326] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1208657: loss 0.5759
[2019-03-27 13:34:53,327] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1208657: learning rate 0.0000
[2019-03-27 13:34:54,364] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0821658e-22 1.0000000e+00 4.6247799e-31 3.4829743e-30 2.3359335e-15], sum to 1.0000
[2019-03-27 13:34:54,375] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7525
[2019-03-27 13:34:54,381] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.45, 52.33333333333333, 1.0, 2.0, 0.2761975372754034, 1.0, 2.0, 0.2761975372754034, 1.0, 1.0, 0.4796637823260429, 6.9112, 6.9112, 170.5573041426782, 1158073.689185883, 1158073.689185883, 294951.1143247102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5505000.0000, 
sim time next is 5505600.0000, 
raw observation next is [34.2, 53.66666666666667, 1.0, 2.0, 0.5269756644878862, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736379.6813046848, 736379.6813046848, 187933.1506878773], 
processed observation next is [1.0, 0.7391304347826086, 0.8199052132701423, 0.5366666666666667, 1.0, 1.0, 0.4300911620335979, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20454991147352355, 0.20454991147352355, 0.2804972398326527], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.90686154], dtype=float32), 0.17786716]. 
=============================================
[2019-03-27 13:34:54,636] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1209151: loss 0.5542
[2019-03-27 13:34:54,640] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1209152: learning rate 0.0000
[2019-03-27 13:34:55,775] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7154940e-27 1.0000000e+00 4.9998989e-36 1.0832527e-36 2.6503704e-18], sum to 1.0000
[2019-03-27 13:34:55,787] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7909
[2019-03-27 13:34:55,793] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 87.0, 1.0, 2.0, 0.550308261473757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768995.7409539418, 768995.7409539425, 191853.4611998173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5526000.0000, 
sim time next is 5526600.0000, 
raw observation next is [27.53333333333334, 87.5, 1.0, 2.0, 0.5495289017941459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767906.2771858142, 767906.2771858149, 191719.8930627896], 
processed observation next is [1.0, 1.0, 0.5039494470774094, 0.875, 1.0, 1.0, 0.45726373710138063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21330729921828173, 0.21330729921828193, 0.2861490941235666], 
reward next is 0.7139, 
noisyNet noise sample is [array([-0.6450282], dtype=float32), 1.1984872]. 
=============================================
[2019-03-27 13:34:56,037] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.3584042e-25 1.0000000e+00 1.2714294e-32 4.5534023e-34 6.8055008e-17], sum to 1.0000
[2019-03-27 13:34:56,046] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6855
[2019-03-27 13:34:56,053] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.11666666666667, 90.33333333333333, 1.0, 2.0, 0.5486145661652951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766628.1325044517, 766628.1325044523, 191563.3728262976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5530200.0000, 
sim time next is 5530800.0000, 
raw observation next is [27.03333333333333, 90.66666666666667, 1.0, 2.0, 0.5475336966681766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765117.1930401626, 765117.193040162, 191378.6003513641], 
processed observation next is [1.0, 0.0, 0.48025276461295413, 0.9066666666666667, 1.0, 1.0, 0.4548598755038272, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2125325536222674, 0.21253255362226722, 0.2856397020169613], 
reward next is 0.7144, 
noisyNet noise sample is [array([-0.42211723], dtype=float32), -2.1006572]. 
=============================================
[2019-03-27 13:34:56,412] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1209826: loss 0.2962
[2019-03-27 13:34:56,420] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1209827: learning rate 0.0000
[2019-03-27 13:34:56,891] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1210003: loss 0.6033
[2019-03-27 13:34:56,895] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1210003: learning rate 0.0000
[2019-03-27 13:34:57,491] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1210238: loss 0.2495
[2019-03-27 13:34:57,500] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1210238: learning rate 0.0000
[2019-03-27 13:34:57,759] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1210336: loss 0.5889
[2019-03-27 13:34:57,763] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1210336: learning rate 0.0000
[2019-03-27 13:34:57,974] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1067316e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6554401e-27], sum to 1.0000
[2019-03-27 13:34:57,988] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9392
[2019-03-27 13:34:57,994] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 91.0, 1.0, 2.0, 0.5106868069915107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713610.4843042409, 713610.4843042409, 185286.9209277059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5718600.0000, 
sim time next is 5719200.0000, 
raw observation next is [25.76666666666667, 91.33333333333333, 1.0, 2.0, 0.5113428308367751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714527.4904560291, 714527.4904560298, 185391.8352907108], 
processed observation next is [0.0, 0.17391304347826086, 0.42022116903633505, 0.9133333333333333, 1.0, 1.0, 0.41125642269490975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19847985846000807, 0.19847985846000826, 0.2767042317771803], 
reward next is 0.7233, 
noisyNet noise sample is [array([-0.29412854], dtype=float32), 0.0005015642]. 
=============================================
[2019-03-27 13:34:58,647] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1210677: loss 0.8356
[2019-03-27 13:34:58,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1210678: learning rate 0.0000
[2019-03-27 13:34:59,369] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1210943: loss 0.5524
[2019-03-27 13:34:59,372] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1210943: learning rate 0.0000
[2019-03-27 13:35:00,173] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1211247: loss 0.3892
[2019-03-27 13:35:00,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1211248: learning rate 0.0000
[2019-03-27 13:35:00,196] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2433817e-28 1.0000000e+00 1.2035056e-36 0.0000000e+00 2.5343295e-20], sum to 1.0000
[2019-03-27 13:35:00,208] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4954
[2019-03-27 13:35:00,212] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333334, 89.33333333333334, 1.0, 2.0, 0.5565317298699469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777695.5425356231, 777695.5425356238, 192927.0943395259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5606400.0000, 
sim time next is 5607000.0000, 
raw observation next is [27.35, 89.5, 1.0, 2.0, 0.5553076913136273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775984.4504366948, 775984.4504366942, 192714.954714195], 
processed observation next is [1.0, 0.9130434782608695, 0.4952606635071091, 0.895, 1.0, 1.0, 0.464226134112804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2155512362324152, 0.21555123623241507, 0.2876342607674552], 
reward next is 0.7124, 
noisyNet noise sample is [array([1.653129], dtype=float32), 0.046969157]. 
=============================================
[2019-03-27 13:35:00,232] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.957245]
 [72.8991  ]
 [72.707596]
 [72.50749 ]
 [72.389366]], R is [[73.24497986]
 [73.22457886]
 [73.20394135]
 [73.18309021]
 [73.16204834]].
[2019-03-27 13:35:01,324] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1211679: loss 0.5101
[2019-03-27 13:35:01,325] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1211679: learning rate 0.0000
[2019-03-27 13:35:04,520] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2937114e-22 1.0000000e+00 3.6522828e-31 1.0826978e-31 7.4162748e-16], sum to 1.0000
[2019-03-27 13:35:04,536] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7430
[2019-03-27 13:35:04,540] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 86.33333333333334, 1.0, 2.0, 0.8800687012622707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1230066.84180666, 1230066.841806659, 264513.9030617427], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5811600.0000, 
sim time next is 5812200.0000, 
raw observation next is [27.55, 85.5, 1.0, 2.0, 0.956055345539492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1336339.767816107, 1336339.767816107, 285822.5644311356], 
processed observation next is [1.0, 0.2608695652173913, 0.504739336492891, 0.855, 1.0, 1.0, 0.9470546331801108, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3712054910600297, 0.3712054910600297, 0.4266008424345307], 
reward next is 0.5734, 
noisyNet noise sample is [array([-0.38162944], dtype=float32), 1.0180323]. 
=============================================
[2019-03-27 13:35:05,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2180251e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0061368e-24], sum to 1.0000
[2019-03-27 13:35:05,693] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0092
[2019-03-27 13:35:05,700] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5164094392170678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721609.7460489165, 721609.7460489159, 186207.1827764414], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5643000.0000, 
sim time next is 5643600.0000, 
raw observation next is [28.16666666666667, 78.33333333333333, 1.0, 2.0, 0.51893795364967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725144.1962427127, 725144.1962427133, 186616.5906831961], 
processed observation next is [0.0, 0.30434782608695654, 0.5339652448657191, 0.7833333333333333, 1.0, 1.0, 0.4204071730718915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2014289434007535, 0.20142894340075368, 0.27853222490029267], 
reward next is 0.7215, 
noisyNet noise sample is [array([-0.9570094], dtype=float32), 1.5951133]. 
=============================================
[2019-03-27 13:35:07,410] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1213998: loss 4.3779
[2019-03-27 13:35:07,413] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1213998: learning rate 0.0000
[2019-03-27 13:35:07,625] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1214080: loss 1.6018
[2019-03-27 13:35:07,627] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1214081: learning rate 0.0000
[2019-03-27 13:35:08,089] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1214257: loss 0.0290
[2019-03-27 13:35:08,094] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1214257: learning rate 0.0000
[2019-03-27 13:35:08,679] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1214475: loss 2.6373
[2019-03-27 13:35:08,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1214475: learning rate 0.0000
[2019-03-27 13:35:10,695] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1215234: loss 2.2510
[2019-03-27 13:35:10,701] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1215234: learning rate 0.0000
[2019-03-27 13:35:11,595] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.6785558e-16 1.0000000e+00 1.3540961e-24 3.1252766e-22 1.1748333e-09], sum to 1.0000
[2019-03-27 13:35:11,610] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6460
[2019-03-27 13:35:11,615] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.36666666666667, 80.5, 1.0, 2.0, 0.9795041304955399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1369136.770955952, 1369136.770955951, 292746.1710574201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5904600.0000, 
sim time next is 5905200.0000, 
raw observation next is [29.53333333333333, 80.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.987940680208649, 6.9112, 168.8954401013694, 3637628.799626759, 1455109.420327928, 306342.9021688391], 
processed observation next is [1.0, 0.34782608695652173, 0.598736176935229, 0.8, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.30767406802086483, 0.0, 0.8293539315647634, 1.0104524443407663, 0.4041970612022022, 0.4572282121922972], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2574068], dtype=float32), -0.48199454]. 
=============================================
[2019-03-27 13:35:14,338] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1216607: loss 0.7295
[2019-03-27 13:35:14,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1216608: learning rate 0.0000
[2019-03-27 13:35:14,513] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1216675: loss 5.7864
[2019-03-27 13:35:14,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1216675: learning rate 0.0000
[2019-03-27 13:35:16,029] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1217250: loss 0.0645
[2019-03-27 13:35:16,033] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1217250: learning rate 0.0000
[2019-03-27 13:35:17,582] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1217841: loss 0.0639
[2019-03-27 13:35:17,585] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1217841: learning rate 0.0000
[2019-03-27 13:35:18,172] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1218066: loss 0.0597
[2019-03-27 13:35:18,174] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1218066: learning rate 0.0000
[2019-03-27 13:35:18,852] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1218320: loss 0.6977
[2019-03-27 13:35:18,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1218320: learning rate 0.0000
[2019-03-27 13:35:19,031] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1218385: loss 0.0298
[2019-03-27 13:35:19,039] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1218387: learning rate 0.0000
[2019-03-27 13:35:19,932] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1218721: loss 0.0456
[2019-03-27 13:35:19,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1218723: learning rate 0.0000
[2019-03-27 13:35:20,515] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1218945: loss 1.1481
[2019-03-27 13:35:20,519] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1218947: learning rate 0.0000
[2019-03-27 13:35:21,323] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1219246: loss 0.0283
[2019-03-27 13:35:21,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1219246: learning rate 0.0000
[2019-03-27 13:35:21,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5297612e-19 1.0000000e+00 3.1006993e-27 9.6148083e-25 3.1132122e-10], sum to 1.0000
[2019-03-27 13:35:21,395] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8222
[2019-03-27 13:35:21,401] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 89.66666666666667, 1.0, 2.0, 0.5362423797057536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749333.27318802, 749333.2731880193, 189469.5242792782], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6136800.0000, 
sim time next is 6137400.0000, 
raw observation next is [26.93333333333333, 89.83333333333333, 1.0, 2.0, 0.5360785903220822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749104.3168114576, 749104.3168114576, 189442.0911609878], 
processed observation next is [1.0, 0.0, 0.4755134281200631, 0.8983333333333333, 1.0, 1.0, 0.44105854255672544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20808453244762712, 0.20808453244762712, 0.2827493897925191], 
reward next is 0.7173, 
noisyNet noise sample is [array([0.14129743], dtype=float32), 0.8741519]. 
=============================================
[2019-03-27 13:35:22,608] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9132623e-17 9.9999964e-01 8.2908828e-25 2.4256645e-20 4.1162505e-07], sum to 1.0000
[2019-03-27 13:35:22,619] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8913
[2019-03-27 13:35:22,626] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 91.66666666666666, 1.0, 2.0, 0.5318134151143034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743142.1708332984, 743142.1708332978, 188730.7215990936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6050400.0000, 
sim time next is 6051000.0000, 
raw observation next is [26.45, 91.83333333333333, 1.0, 2.0, 0.5309558338746615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741943.3906999601, 741943.3906999595, 188588.3629352796], 
processed observation next is [1.0, 0.0, 0.45260663507109006, 0.9183333333333333, 1.0, 1.0, 0.43488654683694156, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20609538630554447, 0.2060953863055443, 0.28147516856011884], 
reward next is 0.7185, 
noisyNet noise sample is [array([0.63217205], dtype=float32), -1.5574548]. 
=============================================
[2019-03-27 13:35:22,646] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[55.418327]
 [56.27212 ]
 [57.363716]
 [59.266655]
 [62.640614]], R is [[54.84594345]
 [55.01579666]
 [55.18359756]
 [55.34932709]
 [55.51301956]].
[2019-03-27 13:35:22,734] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1219783: loss 1.0659
[2019-03-27 13:35:22,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1219783: learning rate 0.0000
[2019-03-27 13:35:23,816] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8091282e-24 1.0000000e+00 3.6546204e-33 8.7445631e-34 3.9819176e-15], sum to 1.0000
[2019-03-27 13:35:23,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3204
[2019-03-27 13:35:23,826] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 86.66666666666667, 1.0, 2.0, 0.540030127717527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754628.068830808, 754628.068830808, 190105.7250444031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6038400.0000, 
sim time next is 6039000.0000, 
raw observation next is [27.4, 87.0, 1.0, 2.0, 0.5402887407433934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754989.5782987179, 754989.5782987179, 190149.2567552365], 
processed observation next is [1.0, 0.9130434782608695, 0.4976303317535545, 0.87, 1.0, 1.0, 0.44613101294384744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2097193273051994, 0.2097193273051994, 0.2838048608287112], 
reward next is 0.7162, 
noisyNet noise sample is [array([0.37388682], dtype=float32), 1.6144154]. 
=============================================
[2019-03-27 13:35:23,844] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[70.27388 ]
 [70.551285]
 [70.69871 ]
 [70.92259 ]
 [71.14799 ]], R is [[70.42221069]
 [70.43424988]
 [70.44633484]
 [70.45846558]
 [70.47058105]].
[2019-03-27 13:35:28,550] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1221992: loss 1.0204
[2019-03-27 13:35:28,551] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1221992: learning rate 0.0000
[2019-03-27 13:35:28,984] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1222155: loss 0.6965
[2019-03-27 13:35:28,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1222155: learning rate 0.0000
[2019-03-27 13:35:29,254] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1222258: loss 1.2705
[2019-03-27 13:35:29,256] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1222259: learning rate 0.0000
[2019-03-27 13:35:30,019] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1222544: loss 0.4456
[2019-03-27 13:35:30,022] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1222546: learning rate 0.0000
[2019-03-27 13:35:31,912] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1223258: loss 0.5022
[2019-03-27 13:35:31,916] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1223260: learning rate 0.0000
[2019-03-27 13:35:35,403] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1224578: loss 0.4619
[2019-03-27 13:35:35,407] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1224578: learning rate 0.0000
[2019-03-27 13:35:35,550] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1224631: loss 1.0042
[2019-03-27 13:35:35,552] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1224631: learning rate 0.0000
[2019-03-27 13:35:36,524] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 13:35:36,524] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:35:36,525] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:35:36,526] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:35:36,526] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:35:36,526] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:35:36,527] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:35:36,525] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:35:36,528] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:35:36,528] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:35:36,532] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:35:36,559] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run50
[2019-03-27 13:35:36,581] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run50
[2019-03-27 13:35:36,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run50
[2019-03-27 13:35:36,626] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run50
[2019-03-27 13:35:36,647] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run50
[2019-03-27 13:35:51,578] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06483342], dtype=float32), 0.057128668]
[2019-03-27 13:35:51,579] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.9, 92.0, 1.0, 2.0, 0.3451618729521336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 536849.4822663242, 536849.4822663248, 169701.6554634356]
[2019-03-27 13:35:51,579] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:35:51,581] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6692119e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8717694e-23], sampled 0.7612964669832911
[2019-03-27 13:36:54,662] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06483342], dtype=float32), 0.057128668]
[2019-03-27 13:36:54,663] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.46360235333334, 75.34933763000001, 1.0, 2.0, 0.4529631684104401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654053.9972390925, 654053.9972390919, 179254.6160432828]
[2019-03-27 13:36:54,665] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:36:54,669] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.75846841e-27 1.00000000e+00 1.11426725e-36 4.56650884e-38
 9.52634993e-17], sampled 0.877514276302046
[2019-03-27 13:37:06,035] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06483342], dtype=float32), 0.057128668]
[2019-03-27 13:37:06,037] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.45, 73.5, 1.0, 2.0, 0.7086100181887341, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.003939077342926, 6.9112, 168.912330510543, 1887177.327660647, 1821385.278190404, 386598.1264597398]
[2019-03-27 13:37:06,039] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:37:06,043] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1002443e-23 1.0000000e+00 8.7378646e-33 1.8373160e-32 1.0952530e-13], sampled 0.915223470724277
[2019-03-27 13:37:06,043] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1887177.327660647 W.
[2019-03-27 13:37:32,874] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06483342], dtype=float32), 0.057128668]
[2019-03-27 13:37:32,875] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.23330580166667, 81.90190444166666, 1.0, 2.0, 0.6684599377805802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 934172.4226249078, 934172.4226249078, 214185.2988731463]
[2019-03-27 13:37:32,876] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:37:32,881] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.8271362e-26 1.0000000e+00 1.0703395e-34 2.3650668e-36 5.0147151e-16], sampled 0.035929895287190594
[2019-03-27 13:37:36,892] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06483342], dtype=float32), 0.057128668]
[2019-03-27 13:37:36,893] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.787204555, 92.30342181166667, 1.0, 2.0, 0.2768391961609865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 450171.0596612198, 450171.0596612205, 163572.6271496727]
[2019-03-27 13:37:36,895] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:37:36,898] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4634298e-29 1.0000000e+00 2.4744661e-38 0.0000000e+00 7.9474190e-19], sampled 0.8736242631711386
[2019-03-27 13:37:44,723] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8675.7506 2779031419.4678 893.0000
[2019-03-27 13:37:45,051] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7899.9320 3163396506.0534 1769.0000
[2019-03-27 13:37:45,187] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8504.2415 2842027536.5692 1122.0000
[2019-03-27 13:37:45,190] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8013.6199 3007025294.2789 1713.0000
[2019-03-27 13:37:45,321] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8273.3129 2926552106.7220 1300.0000
[2019-03-27 13:37:46,337] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1225000, evaluation results [1225000.0, 7899.931998604529, 3163396506.0533977, 1769.0, 8273.312888654884, 2926552106.72195, 1300.0, 8675.75055610898, 2779031419.4678173, 893.0, 8013.619932975473, 3007025294.278942, 1713.0, 8504.24151714093, 2842027536.5692387, 1122.0]
[2019-03-27 13:37:47,038] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1225275: loss 1.0463
[2019-03-27 13:37:47,039] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1225275: learning rate 0.0000
[2019-03-27 13:37:48,551] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1225852: loss 0.7068
[2019-03-27 13:37:48,554] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1225854: learning rate 0.0000
[2019-03-27 13:37:48,801] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1225945: loss 0.8850
[2019-03-27 13:37:48,808] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1225946: learning rate 0.0000
[2019-03-27 13:37:49,280] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1226124: loss 296.0000
[2019-03-27 13:37:49,285] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1226126: learning rate 0.0000
[2019-03-27 13:37:49,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7439736e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8685761e-20], sum to 1.0000
[2019-03-27 13:37:49,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4894
[2019-03-27 13:37:49,600] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 80.0, 1.0, 2.0, 0.5251802323519601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733869.9326706474, 733869.9326706467, 187635.8053387474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6116400.0000, 
sim time next is 6117000.0000, 
raw observation next is [28.03333333333333, 81.0, 1.0, 2.0, 0.525454711778383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 734253.6139373227, 734253.613937322, 187680.8262562323], 
processed observation next is [1.0, 0.8260869565217391, 0.5276461295418641, 0.81, 1.0, 1.0, 0.42825868888961804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20395933720481188, 0.20395933720481169, 0.2801206362033318], 
reward next is 0.7199, 
noisyNet noise sample is [array([2.623986], dtype=float32), 0.09549877]. 
=============================================
[2019-03-27 13:37:49,624] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.271034]
 [72.31083 ]
 [72.03579 ]
 [71.85254 ]
 [71.9529  ]], R is [[72.13600159]
 [72.13459015]
 [72.13313293]
 [72.13161469]
 [72.13023376]].
[2019-03-27 13:37:50,095] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1226434: loss 0.2475
[2019-03-27 13:37:50,102] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1226436: learning rate 0.0000
[2019-03-27 13:37:51,021] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1226784: loss 0.9573
[2019-03-27 13:37:51,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1226784: learning rate 0.0000
[2019-03-27 13:37:51,582] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1227000: loss 0.5264
[2019-03-27 13:37:51,584] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1227001: learning rate 0.0000
[2019-03-27 13:37:52,320] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1227282: loss 0.5883
[2019-03-27 13:37:52,322] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1227282: learning rate 0.0000
[2019-03-27 13:37:53,323] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4688812e-29 1.0000000e+00 1.7819920e-38 0.0000000e+00 9.2101383e-18], sum to 1.0000
[2019-03-27 13:37:53,337] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4255
[2019-03-27 13:37:53,341] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.65, 64.5, 1.0, 2.0, 0.535164047575894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747825.9052312032, 747825.9052312026, 189288.0137192084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6365400.0000, 
sim time next is 6366000.0000, 
raw observation next is [30.3, 66.0, 1.0, 2.0, 0.5212844232852776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728424.188064545, 728424.188064545, 186998.0201035529], 
processed observation next is [0.0, 0.6956521739130435, 0.6350710900473934, 0.66, 1.0, 1.0, 0.42323424492202116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2023400522401514, 0.2023400522401514, 0.2791015225426163], 
reward next is 0.7209, 
noisyNet noise sample is [array([-0.77937734], dtype=float32), -0.18465294]. 
=============================================
[2019-03-27 13:37:53,366] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[76.76779 ]
 [76.79381 ]
 [76.6731  ]
 [76.67877 ]
 [76.666794]], R is [[76.79542542]
 [76.74495697]
 [76.69747925]
 [76.64989471]
 [76.60224915]].
[2019-03-27 13:37:53,433] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1227697: loss 114.6021
[2019-03-27 13:37:53,434] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1227697: learning rate 0.0000
[2019-03-27 13:37:56,692] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1495380e-19 1.0000000e+00 9.3185860e-28 2.7867014e-26 4.3248627e-11], sum to 1.0000
[2019-03-27 13:37:56,701] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2601
[2019-03-27 13:37:56,709] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.63333333333333, 86.66666666666667, 1.0, 2.0, 0.8158653122606526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1140281.957457591, 1140281.957457591, 247840.6586379914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6410400.0000, 
sim time next is 6411000.0000, 
raw observation next is [26.61666666666667, 86.83333333333333, 1.0, 2.0, 0.8100175178758163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1132104.518057822, 1132104.518057821, 246382.8546300093], 
processed observation next is [1.0, 0.17391304347826086, 0.4605055292259086, 0.8683333333333333, 1.0, 1.0, 0.7711054432238751, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3144734772382839, 0.3144734772382836, 0.367735603925387], 
reward next is 0.6323, 
noisyNet noise sample is [array([2.1095548], dtype=float32), 1.1473083]. 
=============================================
[2019-03-27 13:37:56,723] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[54.174503]
 [54.204327]
 [53.997723]
 [54.096355]
 [53.94646 ]], R is [[54.18427658]
 [54.27252197]
 [54.36233521]
 [54.44419479]
 [54.50334549]].
[2019-03-27 13:37:59,354] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1229926: loss 254.3378
[2019-03-27 13:37:59,359] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1229928: learning rate 0.0000
[2019-03-27 13:37:59,829] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1230103: loss 18.3928
[2019-03-27 13:37:59,831] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1230103: learning rate 0.0000
[2019-03-27 13:38:00,082] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1230198: loss 11.2809
[2019-03-27 13:38:00,088] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1230198: learning rate 0.0000
[2019-03-27 13:38:01,017] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1230550: loss 27.7527
[2019-03-27 13:38:01,019] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1230550: learning rate 0.0000
[2019-03-27 13:38:02,827] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1231242: loss 10.9361
[2019-03-27 13:38:02,828] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1231242: learning rate 0.0000
[2019-03-27 13:38:04,761] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2163186e-20 1.0000000e+00 3.3760351e-29 2.8261469e-29 1.0740429e-08], sum to 1.0000
[2019-03-27 13:38:04,772] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7012
[2019-03-27 13:38:04,780] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.68333333333334, 94.83333333333334, 1.0, 2.0, 0.5503152292301223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769005.48115764, 769005.4811576394, 191850.0694632815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6671400.0000, 
sim time next is 6672000.0000, 
raw observation next is [24.76666666666667, 94.66666666666667, 1.0, 2.0, 0.514986512681467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719620.7323303864, 719620.7323303871, 185975.4179810555], 
processed observation next is [1.0, 0.21739130434782608, 0.3728278041074251, 0.9466666666666668, 1.0, 1.0, 0.41564640082104454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19989464786955177, 0.19989464786955197, 0.27757525071799327], 
reward next is 0.7224, 
noisyNet noise sample is [array([1.5238665], dtype=float32), 1.4035168]. 
=============================================
[2019-03-27 13:38:04,800] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[60.362488]
 [60.127045]
 [60.244415]
 [60.187683]
 [60.130257]], R is [[60.5286026 ]
 [60.63697433]
 [60.74052048]
 [60.84701538]
 [60.94975281]].
[2019-03-27 13:38:06,142] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1232479: loss 15.3710
[2019-03-27 13:38:06,146] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1232480: learning rate 0.0000
[2019-03-27 13:38:06,660] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1232681: loss 9.7583
[2019-03-27 13:38:06,668] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1232681: learning rate 0.0000
[2019-03-27 13:38:07,054] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2708038e-12 7.1743548e-01 1.0893915e-20 2.6813914e-13 2.8256452e-01], sum to 1.0000
[2019-03-27 13:38:07,066] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7351
[2019-03-27 13:38:07,074] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.9, 69.0, 1.0, 2.0, 0.3974318310425401, 1.0, 2.0, 0.3974318310425401, 1.0, 2.0, 0.6820191938280878, 6.9112, 6.9112, 170.5573041426782, 1666794.389438845, 1666794.389438845, 350330.3401588306], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6439200.0000, 
sim time next is 6439800.0000, 
raw observation next is [29.9, 69.0, 1.0, 2.0, 0.38275496427293, 1.0, 2.0, 0.38275496427293, 1.0, 2.0, 0.6563686987376776, 6.911200000000001, 6.9112, 170.5573041426782, 1605194.793622035, 1605194.793622034, 342434.4178864899], 
processed observation next is [1.0, 0.5217391304347826, 0.6161137440758293, 0.69, 1.0, 1.0, 0.25633128225654217, 1.0, 1.0, 0.25633128225654217, 1.0, 1.0, 0.5809374374849726, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4458874426727875, 0.4458874426727872, 0.5110961460992386], 
reward next is 0.4889, 
noisyNet noise sample is [array([-0.87862754], dtype=float32), -1.221567]. 
=============================================
[2019-03-27 13:38:08,266] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1233292: loss 15.6442
[2019-03-27 13:38:08,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1233292: learning rate 0.0000
[2019-03-27 13:38:09,837] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1233893: loss 5.1354
[2019-03-27 13:38:09,841] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1233893: learning rate 0.0000
[2019-03-27 13:38:09,983] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1233947: loss 8.4668
[2019-03-27 13:38:09,985] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1233948: learning rate 0.0000
[2019-03-27 13:38:10,668] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1234203: loss 0.1602
[2019-03-27 13:38:10,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1234204: learning rate 0.0000
[2019-03-27 13:38:11,401] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1234477: loss 2.7907
[2019-03-27 13:38:11,403] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1234477: learning rate 0.0000
[2019-03-27 13:38:12,125] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1234766: loss 10.5214
[2019-03-27 13:38:12,129] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1234767: learning rate 0.0000
[2019-03-27 13:38:12,894] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1235050: loss 5.7507
[2019-03-27 13:38:12,896] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1235051: learning rate 0.0000
[2019-03-27 13:38:13,585] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1235311: loss 0.8019
[2019-03-27 13:38:13,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1235311: learning rate 0.0000
[2019-03-27 13:38:14,695] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1235728: loss 0.2012
[2019-03-27 13:38:14,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1235728: learning rate 0.0000
[2019-03-27 13:38:20,542] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1237941: loss 0.0847
[2019-03-27 13:38:20,547] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1237941: learning rate 0.0000
[2019-03-27 13:38:20,963] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1238100: loss 0.0340
[2019-03-27 13:38:20,971] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1238103: learning rate 0.0000
[2019-03-27 13:38:21,291] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1238222: loss 0.0195
[2019-03-27 13:38:21,292] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1238223: learning rate 0.0000
[2019-03-27 13:38:22,090] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1238535: loss 0.0140
[2019-03-27 13:38:22,095] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1238535: learning rate 0.0000
[2019-03-27 13:38:23,744] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4609996e-28 1.0000000e+00 1.1931381e-36 0.0000000e+00 8.6531891e-16], sum to 1.0000
[2019-03-27 13:38:23,753] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6836
[2019-03-27 13:38:23,761] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 68.66666666666667, 1.0, 2.0, 0.4346868807780826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628596.8167288657, 628596.8167288657, 176719.6224060748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6942000.0000, 
sim time next is 6942600.0000, 
raw observation next is [27.75, 68.0, 1.0, 2.0, 0.4358092757034699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 629424.2024752607, 629424.2024752607, 176779.7439311932], 
processed observation next is [0.0, 0.34782608695652173, 0.514218009478673, 0.68, 1.0, 1.0, 0.320252139401771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17484005624312798, 0.17484005624312798, 0.2638503640764078], 
reward next is 0.7361, 
noisyNet noise sample is [array([-0.22524089], dtype=float32), 1.4615551]. 
=============================================
[2019-03-27 13:38:23,947] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1239237: loss 0.0261
[2019-03-27 13:38:23,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1239237: learning rate 0.0000
[2019-03-27 13:38:27,089] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1240433: loss 0.0310
[2019-03-27 13:38:27,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1240434: learning rate 0.0000
[2019-03-27 13:38:27,598] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1240624: loss 0.0207
[2019-03-27 13:38:27,599] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1240624: learning rate 0.0000
[2019-03-27 13:38:29,328] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1241274: loss 0.0102
[2019-03-27 13:38:29,330] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1241276: learning rate 0.0000
[2019-03-27 13:38:30,492] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4740023e-24 1.0000000e+00 5.4407357e-34 1.0466465e-34 1.4418875e-12], sum to 1.0000
[2019-03-27 13:38:30,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3765
[2019-03-27 13:38:30,508] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 56.5, 1.0, 2.0, 0.3590881788372061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 550403.7658441856, 550403.765844185, 170601.182203602], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6892200.0000, 
sim time next is 6892800.0000, 
raw observation next is [27.86666666666667, 57.33333333333333, 1.0, 2.0, 0.3606794020708926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 552367.4256874664, 552367.4256874664, 170752.5849879545], 
processed observation next is [0.0, 0.782608695652174, 0.519747235387046, 0.5733333333333333, 1.0, 1.0, 0.2297342193625212, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15343539602429623, 0.15343539602429623, 0.2548546044596336], 
reward next is 0.7451, 
noisyNet noise sample is [array([0.22484559], dtype=float32), 1.3616494]. 
=============================================
[2019-03-27 13:38:30,807] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2390877e-21 1.0000000e+00 1.8841777e-30 3.4132109e-30 7.1966608e-12], sum to 1.0000
[2019-03-27 13:38:30,818] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4906
[2019-03-27 13:38:30,827] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 83.33333333333334, 1.0, 2.0, 0.405761655036166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642075.9602737251, 642075.9602737251, 178947.0463925825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6747600.0000, 
sim time next is 6748200.0000, 
raw observation next is [22.25, 83.5, 1.0, 2.0, 0.3659360682601785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 579438.8657669369, 579438.8657669376, 173408.5297280036], 
processed observation next is [1.0, 0.08695652173913043, 0.2535545023696683, 0.835, 1.0, 1.0, 0.23606755212069697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1609552404908158, 0.160955240490816, 0.25881870108657257], 
reward next is 0.7412, 
noisyNet noise sample is [array([1.2865295], dtype=float32), 0.5509661]. 
=============================================
[2019-03-27 13:38:31,020] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1241917: loss 0.0107
[2019-03-27 13:38:31,021] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1241917: learning rate 0.0000
[2019-03-27 13:38:31,349] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1242035: loss 0.0084
[2019-03-27 13:38:31,352] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1242035: learning rate 0.0000
[2019-03-27 13:38:31,870] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1242240: loss 0.5522
[2019-03-27 13:38:31,872] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1242241: learning rate 0.0000
[2019-03-27 13:38:32,542] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1242499: loss 0.0314
[2019-03-27 13:38:32,543] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1242499: learning rate 0.0000
[2019-03-27 13:38:33,372] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1242811: loss 0.0250
[2019-03-27 13:38:33,376] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1242811: learning rate 0.0000
[2019-03-27 13:38:33,902] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1243007: loss 0.0167
[2019-03-27 13:38:33,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1243008: learning rate 0.0000
[2019-03-27 13:38:34,839] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1243362: loss 0.0263
[2019-03-27 13:38:34,842] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1243363: learning rate 0.0000
[2019-03-27 13:38:35,795] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1243725: loss 0.7422
[2019-03-27 13:38:35,799] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1243727: learning rate 0.0000
[2019-03-27 13:38:41,517] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1245909: loss 0.0470
[2019-03-27 13:38:41,520] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1245909: learning rate 0.0000
[2019-03-27 13:38:41,921] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1246056: loss 0.0343
[2019-03-27 13:38:41,926] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1246056: learning rate 0.0000
[2019-03-27 13:38:42,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7226785e-25 1.0000000e+00 8.2086706e-36 8.5574091e-37 9.8092114e-14], sum to 1.0000
[2019-03-27 13:38:42,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2946
[2019-03-27 13:38:42,113] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.28333333333334, 66.5, 1.0, 2.0, 0.3689589507035062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 562859.2528873349, 562859.2528873342, 171584.1420206068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6858600.0000, 
sim time next is 6859200.0000, 
raw observation next is [26.46666666666667, 65.0, 1.0, 2.0, 0.3663699001749949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560082.745709959, 560082.745709959, 171379.7278704616], 
processed observation next is [0.0, 0.391304347826087, 0.45339652448657203, 0.65, 1.0, 1.0, 0.23659024117469263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1555785404749886, 0.1555785404749886, 0.25579063861262924], 
reward next is 0.7442, 
noisyNet noise sample is [array([-0.2195334], dtype=float32), -0.14729486]. 
=============================================
[2019-03-27 13:38:42,357] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1246229: loss 0.0333
[2019-03-27 13:38:42,361] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1246230: learning rate 0.0000
[2019-03-27 13:38:42,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7088347e-24 1.0000000e+00 7.9573047e-35 4.5601073e-36 2.6345440e-11], sum to 1.0000
[2019-03-27 13:38:42,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6833
[2019-03-27 13:38:42,382] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 73.5, 1.0, 2.0, 0.4313252869929471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 625638.6753910359, 625638.6753910353, 176481.0930227634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6939000.0000, 
sim time next is 6939600.0000, 
raw observation next is [26.9, 72.33333333333333, 1.0, 2.0, 0.4313948059465861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 625497.9356080208, 625497.9356080202, 176460.6867298727], 
processed observation next is [0.0, 0.30434782608695654, 0.4739336492890995, 0.7233333333333333, 1.0, 1.0, 0.31493350114046514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17374942655778355, 0.1737494265577834, 0.2633741592983174], 
reward next is 0.7366, 
noisyNet noise sample is [array([-0.26984164], dtype=float32), -1.1955633]. 
=============================================
[2019-03-27 13:38:43,079] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1246497: loss 0.0274
[2019-03-27 13:38:43,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1246497: learning rate 0.0000
[2019-03-27 13:38:44,804] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0664498e-26 1.0000000e+00 5.5169306e-36 0.0000000e+00 4.4963324e-14], sum to 1.0000
[2019-03-27 13:38:44,817] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3082
[2019-03-27 13:38:44,825] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 90.66666666666667, 1.0, 2.0, 0.4169826035801141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 613833.7400076356, 613833.7400076349, 175596.5747421358], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6927600.0000, 
sim time next is 6928200.0000, 
raw observation next is [23.75, 91.0, 1.0, 2.0, 0.4167690655225633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613665.0841539429, 613665.0841539429, 175584.6354077567], 
processed observation next is [0.0, 0.17391304347826086, 0.3246445497630332, 0.91, 1.0, 1.0, 0.29731212713561844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17046252337609524, 0.17046252337609524, 0.26206662001157716], 
reward next is 0.7379, 
noisyNet noise sample is [array([-0.31536135], dtype=float32), -0.82686186]. 
=============================================
[2019-03-27 13:38:45,019] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1247216: loss 0.0314
[2019-03-27 13:38:45,020] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1247216: learning rate 0.0000
[2019-03-27 13:38:47,997] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1248346: loss 0.0452
[2019-03-27 13:38:47,999] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1248346: learning rate 0.0000
[2019-03-27 13:38:48,682] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1248604: loss 0.0332
[2019-03-27 13:38:48,689] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1248605: learning rate 0.0000
[2019-03-27 13:38:50,621] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1249342: loss 0.0402
[2019-03-27 13:38:50,623] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1249342: learning rate 0.0000
[2019-03-27 13:38:52,181] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1249938: loss 0.2059
[2019-03-27 13:38:52,186] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1249940: learning rate 0.0000
[2019-03-27 13:38:52,336] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 13:38:52,338] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:38:52,339] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:38:52,340] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:38:52,339] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:38:52,344] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:38:52,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:38:52,347] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:38:52,347] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:38:52,341] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:38:52,354] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:38:52,380] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run51
[2019-03-27 13:38:52,402] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run51
[2019-03-27 13:38:52,421] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run51
[2019-03-27 13:38:52,441] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run51
[2019-03-27 13:38:52,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run51
[2019-03-27 13:39:17,631] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06266666], dtype=float32), 0.05693306]
[2019-03-27 13:39:17,631] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.732897495, 65.303204445, 1.0, 2.0, 0.4629629512606956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 656336.0540307141, 656336.0540307135, 179211.5399929832]
[2019-03-27 13:39:17,632] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:39:17,634] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0585451e-24 1.0000000e+00 5.4623634e-34 3.7714736e-36 1.8022787e-12], sampled 0.8775883767335274
[2019-03-27 13:39:27,046] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06266666], dtype=float32), 0.05693306]
[2019-03-27 13:39:27,047] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.82678863333334, 92.22237474666667, 1.0, 2.0, 0.4185880379091796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614126.6117944198, 614126.6117944198, 175566.6458712416]
[2019-03-27 13:39:27,048] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:39:27,051] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4565104e-26 1.0000000e+00 8.0955386e-36 0.0000000e+00 1.7655712e-14], sampled 0.4265252514144824
[2019-03-27 13:39:41,867] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06266666], dtype=float32), 0.05693306]
[2019-03-27 13:39:41,869] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 1.0, 2.0, 0.3041065107829917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484272.064722271, 484272.064722271, 165928.8890831842]
[2019-03-27 13:39:41,870] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:39:41,873] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8365677e-23 1.0000000e+00 8.5869870e-33 1.0015446e-33 6.4151587e-11], sampled 0.7281905200933201
[2019-03-27 13:40:02,402] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06266666], dtype=float32), 0.05693306]
[2019-03-27 13:40:02,403] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.63333333333333, 52.0, 1.0, 2.0, 0.5298170912867826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740351.5879236187, 740351.587923618, 188399.8010171927]
[2019-03-27 13:40:02,404] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:40:02,406] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2219110e-20 9.9999976e-01 1.7158713e-29 2.3444707e-27 2.3656064e-07], sampled 0.0584712678500553
[2019-03-27 13:40:53,362] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06266666], dtype=float32), 0.05693306]
[2019-03-27 13:40:53,364] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.3, 93.66666666666667, 1.0, 2.0, 0.3208500107730792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504366.6218006241, 504366.6218006241, 167289.2720421492]
[2019-03-27 13:40:53,367] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:40:53,370] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.9602087e-28 1.0000000e+00 4.0476342e-37 0.0000000e+00 9.1167818e-16], sampled 0.9257342505357463
[2019-03-27 13:41:00,740] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8514.3685 3000650391.7543 539.0000
[2019-03-27 13:41:01,059] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8580.1701 2933216460.2640 637.0000
[2019-03-27 13:41:01,216] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8841.3636 2789297865.0567 489.0000
[2019-03-27 13:41:01,223] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8227.2372 3156749926.9122 936.0000
[2019-03-27 13:41:01,238] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8764.3477 2846282519.6094 449.0000
[2019-03-27 13:41:02,255] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1250000, evaluation results [1250000.0, 8227.237231798612, 3156749926.9122176, 936.0, 8580.170109465287, 2933216460.2640076, 637.0, 8841.363579520872, 2789297865.0567317, 489.0, 8514.368496258761, 3000650391.754259, 539.0, 8764.347660259482, 2846282519.60943, 449.0]
[2019-03-27 13:41:02,441] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1250077: loss 0.4911
[2019-03-27 13:41:02,444] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1250078: learning rate 0.0000
[2019-03-27 13:41:02,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8021006e-11 1.1413987e-03 9.1978669e-19 2.6726960e-08 9.9885857e-01], sum to 1.0000
[2019-03-27 13:41:02,842] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2206
[2019-03-27 13:41:02,850] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.13333333333333, 69.66666666666666, 1.0, 2.0, 0.3967260195862393, 1.0, 2.0, 0.3967260195862393, 1.0, 2.0, 0.6662155072607951, 6.911200000000001, 6.9112, 170.5573041426782, 1663831.979914188, 1663831.979914187, 347840.6756135362], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7126800.0000, 
sim time next is 7127400.0000, 
raw observation next is [28.01666666666667, 70.33333333333334, 1.0, 2.0, 0.401368012860522, 1.0, 2.0, 0.401368012860522, 1.0, 2.0, 0.674080474782744, 6.9112, 6.9112, 170.5573041426782, 1683315.357473907, 1683315.357473907, 350331.8984067608], 
processed observation next is [1.0, 0.4782608695652174, 0.5268562401263824, 0.7033333333333335, 1.0, 1.0, 0.2787566420006289, 1.0, 1.0, 0.2787566420006289, 1.0, 1.0, 0.6025371643692, 0.0, 0.0, 0.8375144448122397, 0.4675875992983075, 0.4675875992983075, 0.522883430457852], 
reward next is 0.4771, 
noisyNet noise sample is [array([-0.46720812], dtype=float32), 1.252036]. 
=============================================
[2019-03-27 13:41:02,894] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1250250: loss 0.0117
[2019-03-27 13:41:02,898] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1250250: learning rate 0.0000
[2019-03-27 13:41:03,664] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1250539: loss 0.8424
[2019-03-27 13:41:03,665] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1250539: learning rate 0.0000
[2019-03-27 13:41:04,290] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1250775: loss 0.7520
[2019-03-27 13:41:04,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1250775: learning rate 0.0000
[2019-03-27 13:41:04,869] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1250994: loss 0.5510
[2019-03-27 13:41:04,870] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1250994: learning rate 0.0000
[2019-03-27 13:41:05,879] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1251369: loss 0.8134
[2019-03-27 13:41:05,881] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1251371: learning rate 0.0000
[2019-03-27 13:41:06,836] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1251740: loss 0.1625
[2019-03-27 13:41:06,844] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1251741: learning rate 0.0000
[2019-03-27 13:41:09,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6454532e-22 1.0000000e+00 8.0443385e-30 9.7158534e-28 8.6349683e-09], sum to 1.0000
[2019-03-27 13:41:09,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0059
[2019-03-27 13:41:09,939] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 84.0, 1.0, 2.0, 0.4793260540578593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670566.7363261781, 670566.7363261781, 180522.1158581421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7161000.0000, 
sim time next is 7161600.0000, 
raw observation next is [25.93333333333334, 84.0, 1.0, 2.0, 0.4804312696594108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673085.5141042876, 673085.5141042876, 180814.3123274779], 
processed observation next is [1.0, 0.9130434782608695, 0.42812006319115364, 0.84, 1.0, 1.0, 0.3740135779029046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1869681983623021, 0.1869681983623021, 0.26987210795145955], 
reward next is 0.7301, 
noisyNet noise sample is [array([0.26691556], dtype=float32), -0.14614008]. 
=============================================
[2019-03-27 13:41:12,595] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1253931: loss 0.2225
[2019-03-27 13:41:12,597] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1253931: learning rate 0.0000
[2019-03-27 13:41:12,871] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1254037: loss 0.1646
[2019-03-27 13:41:12,878] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1254039: learning rate 0.0000
[2019-03-27 13:41:13,404] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1254236: loss 0.0471
[2019-03-27 13:41:13,408] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1254237: learning rate 0.0000
[2019-03-27 13:41:13,984] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1254455: loss 0.1152
[2019-03-27 13:41:13,986] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1254455: learning rate 0.0000
[2019-03-27 13:41:16,066] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1255226: loss 0.1402
[2019-03-27 13:41:16,071] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1255228: learning rate 0.0000
[2019-03-27 13:41:16,246] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5085951e-22 1.0000000e+00 5.5932951e-31 1.2521392e-30 3.7161849e-09], sum to 1.0000
[2019-03-27 13:41:16,254] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2629
[2019-03-27 13:41:16,262] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 87.5, 1.0, 2.0, 0.2810551699175988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 455297.3336452136, 455297.3336452136, 163942.1032784081], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7407000.0000, 
sim time next is 7407600.0000, 
raw observation next is [20.66666666666667, 87.0, 1.0, 2.0, 0.2817976931909142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456471.5194258331, 456471.5194258331, 164021.1520503433], 
processed observation next is [1.0, 0.7391304347826086, 0.17851500789889443, 0.87, 1.0, 1.0, 0.1346960158926677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12679764428495363, 0.12679764428495363, 0.2448076896273781], 
reward next is 0.7552, 
noisyNet noise sample is [array([1.2731638], dtype=float32), -0.16367508]. 
=============================================
[2019-03-27 13:41:16,529] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5006276e-13 1.0882189e-01 1.5117018e-21 8.7270228e-15 8.9117813e-01], sum to 1.0000
[2019-03-27 13:41:16,543] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8295
[2019-03-27 13:41:16,549] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.96666666666667, 57.5, 1.0, 2.0, 0.343332644376631, 1.0, 2.0, 0.343332644376631, 1.0, 2.0, 0.5949710387715134, 6.9112, 6.9112, 170.5573041426782, 1527269.034202492, 1527269.034202492, 331208.1869896022], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7308600.0000, 
sim time next is 7309200.0000, 
raw observation next is [27.93333333333333, 58.00000000000001, 1.0, 2.0, 0.3565457140894802, 1.0, 2.0, 0.3565457140894802, 1.0, 2.0, 0.6163927081369425, 6.9112, 6.9112, 170.5573041426782, 1580430.291705416, 1580430.291705416, 337150.5023389044], 
processed observation next is [1.0, 0.6086956521739131, 0.522906793048973, 0.5800000000000001, 1.0, 1.0, 0.22475387239696412, 1.0, 1.0, 0.22475387239696412, 1.0, 1.0, 0.5321862294352957, 0.0, 0.0, 0.8375144448122397, 0.43900841436261556, 0.43900841436261556, 0.5032097049834394], 
reward next is 0.4968, 
noisyNet noise sample is [array([-0.84280133], dtype=float32), 1.4259752]. 
=============================================
[2019-03-27 13:41:19,092] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1256377: loss 0.3203
[2019-03-27 13:41:19,097] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1256377: learning rate 0.0000
[2019-03-27 13:41:19,799] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1256638: loss 0.2583
[2019-03-27 13:41:19,801] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1256638: learning rate 0.0000
[2019-03-27 13:41:20,099] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.6367037e-28 1.0000000e+00 5.0969963e-36 0.0000000e+00 6.8200838e-16], sum to 1.0000
[2019-03-27 13:41:20,108] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7935
[2019-03-27 13:41:20,114] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.21666666666667, 95.0, 1.0, 2.0, 0.3230113846010633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506686.1879506215, 506686.1879506215, 167439.300956639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7449000.0000, 
sim time next is 7449600.0000, 
raw observation next is [21.23333333333333, 95.0, 1.0, 2.0, 0.3239423236257969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507934.9275484583, 507934.9275484583, 167529.4040636521], 
processed observation next is [0.0, 0.21739130434782608, 0.2053712480252764, 0.95, 1.0, 1.0, 0.18547267906722517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1410930354301273, 0.1410930354301273, 0.2500438866621673], 
reward next is 0.7500, 
noisyNet noise sample is [array([-1.1941957], dtype=float32), -1.4059628]. 
=============================================
[2019-03-27 13:41:20,141] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.9191824e-25 1.0000000e+00 2.0730578e-34 4.9701247e-34 2.0384288e-11], sum to 1.0000
[2019-03-27 13:41:20,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7964
[2019-03-27 13:41:20,165] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 74.0, 1.0, 2.0, 0.3807836591491494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575343.7028946707, 575343.7028946707, 172514.4012383298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7333200.0000, 
sim time next is 7333800.0000, 
raw observation next is [25.35, 74.33333333333334, 1.0, 2.0, 0.3809585040310223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575809.4812119086, 575809.4812119086, 172561.7600756117], 
processed observation next is [1.0, 0.9130434782608695, 0.4004739336492892, 0.7433333333333334, 1.0, 1.0, 0.2541668723265329, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15994707811441905, 0.15994707811441905, 0.2575548657844951], 
reward next is 0.7424, 
noisyNet noise sample is [array([-1.1694388], dtype=float32), 0.41831294]. 
=============================================
[2019-03-27 13:41:20,805] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8443843e-20 1.0000000e+00 4.6137034e-28 3.6183269e-28 1.7539900e-08], sum to 1.0000
[2019-03-27 13:41:20,815] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9157
[2019-03-27 13:41:20,821] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333334, 92.83333333333333, 1.0, 2.0, 0.5834743065123338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 929380.7455296658, 929380.7455296658, 210651.7969317116], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7379400.0000, 
sim time next is 7380000.0000, 
raw observation next is [20.9, 93.0, 1.0, 2.0, 0.5517688611380813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 877217.9051937595, 877217.9051937595, 204146.8271148559], 
processed observation next is [1.0, 0.43478260869565216, 0.1895734597156398, 0.93, 1.0, 1.0, 0.4599624832988931, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24367164033159985, 0.24367164033159985, 0.30469675688784464], 
reward next is 0.6953, 
noisyNet noise sample is [array([-0.5702035], dtype=float32), 0.4591178]. 
=============================================
[2019-03-27 13:41:20,842] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[68.504906]
 [68.51304 ]
 [68.44814 ]
 [68.59171 ]
 [68.69986 ]], R is [[68.87572479]
 [68.87255859]
 [68.86946106]
 [68.86102295]
 [68.85796356]].
[2019-03-27 13:41:21,681] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1257352: loss 0.2236
[2019-03-27 13:41:21,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1257352: learning rate 0.0000
[2019-03-27 13:41:21,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8936951e-24 1.0000000e+00 9.7513951e-34 3.9770797e-35 4.0152388e-11], sum to 1.0000
[2019-03-27 13:41:21,762] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0185
[2019-03-27 13:41:21,769] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.35, 91.5, 1.0, 2.0, 0.3145421837505583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496828.6231944223, 496828.6231944217, 166777.0656819067], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7435800.0000, 
sim time next is 7436400.0000, 
raw observation next is [21.36666666666667, 91.33333333333334, 1.0, 2.0, 0.3141037069787677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496181.5054580989, 496181.5054580989, 166729.8703255484], 
processed observation next is [0.0, 0.043478260869565216, 0.21169036334913136, 0.9133333333333334, 1.0, 1.0, 0.17361892407080448, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13782819596058302, 0.13782819596058302, 0.2488505527246991], 
reward next is 0.7511, 
noisyNet noise sample is [array([-0.04713254], dtype=float32), 0.52146876]. 
=============================================
[2019-03-27 13:41:23,313] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1257976: loss 0.3565
[2019-03-27 13:41:23,317] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1257977: learning rate 0.0000
[2019-03-27 13:41:23,443] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1258025: loss 0.4492
[2019-03-27 13:41:23,446] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1258025: learning rate 0.0000
[2019-03-27 13:41:24,046] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1258252: loss 1.3444
[2019-03-27 13:41:24,048] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1258252: learning rate 0.0000
[2019-03-27 13:41:24,746] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1258517: loss 0.4854
[2019-03-27 13:41:24,749] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1258518: learning rate 0.0000
[2019-03-27 13:41:25,392] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1258759: loss 0.3336
[2019-03-27 13:41:25,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1258759: learning rate 0.0000
[2019-03-27 13:41:25,607] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2361252e-24 1.0000000e+00 4.2165176e-32 8.2508095e-34 4.2417139e-11], sum to 1.0000
[2019-03-27 13:41:25,619] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7598
[2019-03-27 13:41:25,630] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 75.16666666666666, 1.0, 2.0, 0.4242490707016482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613440.8641981283, 613440.8641981283, 175240.5397031254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7491000.0000, 
sim time next is 7491600.0000, 
raw observation next is [26.6, 75.0, 1.0, 2.0, 0.425838547850772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 614799.5149246486, 614799.5149246479, 175343.7455768385], 
processed observation next is [0.0, 0.7391304347826086, 0.4597156398104266, 0.75, 1.0, 1.0, 0.3082392142780385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17077764303462462, 0.17077764303462442, 0.2617070829505052], 
reward next is 0.7383, 
noisyNet noise sample is [array([1.1714894], dtype=float32), 1.6054084]. 
=============================================
[2019-03-27 13:41:25,989] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1258978: loss 0.3257
[2019-03-27 13:41:25,990] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1258978: learning rate 0.0000
[2019-03-27 13:41:27,027] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1259374: loss 0.3895
[2019-03-27 13:41:27,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1259375: learning rate 0.0000
[2019-03-27 13:41:28,021] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1259751: loss 0.0120
[2019-03-27 13:41:28,022] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1259751: learning rate 0.0000
[2019-03-27 13:41:32,880] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5236197e-23 1.0000000e+00 5.0579755e-33 2.1502303e-31 7.9904928e-12], sum to 1.0000
[2019-03-27 13:41:32,892] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0748
[2019-03-27 13:41:32,898] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.63333333333333, 91.33333333333334, 1.0, 2.0, 0.5852324403612141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831151.5802689774, 831151.5802689774, 199743.1143115084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7630800.0000, 
sim time next is 7631400.0000, 
raw observation next is [24.71666666666667, 91.16666666666667, 1.0, 2.0, 0.6029276688474788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 854008.1614414913, 854008.1614414913, 202772.713782027], 
processed observation next is [1.0, 0.30434782608695654, 0.3704581358609796, 0.9116666666666667, 1.0, 1.0, 0.5215996010210588, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23722448928930315, 0.23722448928930315, 0.30264584146571194], 
reward next is 0.6974, 
noisyNet noise sample is [array([0.16256829], dtype=float32), 0.7841868]. 
=============================================
[2019-03-27 13:41:33,743] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1261905: loss 0.3005
[2019-03-27 13:41:33,751] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1261906: learning rate 0.0000
[2019-03-27 13:41:34,067] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1262027: loss 0.9241
[2019-03-27 13:41:34,069] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1262027: learning rate 0.0000
[2019-03-27 13:41:34,720] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1262272: loss 0.2559
[2019-03-27 13:41:34,724] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1262273: learning rate 0.0000
[2019-03-27 13:41:35,230] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1262463: loss 0.3144
[2019-03-27 13:41:35,233] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1262463: learning rate 0.0000
[2019-03-27 13:41:37,073] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1263159: loss 0.0522
[2019-03-27 13:41:37,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1263159: learning rate 0.0000
[2019-03-27 13:41:37,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0885701e-14 6.6513178e-07 1.6186828e-22 1.1757481e-11 9.9999928e-01], sum to 1.0000
[2019-03-27 13:41:37,664] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2863
[2019-03-27 13:41:37,671] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333333, 74.0, 1.0, 2.0, 0.4278102934013561, 1.0, 2.0, 0.4278102934013561, 1.0, 2.0, 0.7323248724704979, 6.9112, 6.9112, 170.5573041426782, 1794305.673270725, 1794305.673270725, 367196.7942286705], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7663800.0000, 
sim time next is 7664400.0000, 
raw observation next is [28.7, 75.0, 1.0, 2.0, 0.4220027048875234, 1.0, 2.0, 0.4220027048875234, 1.0, 2.0, 0.722666308009716, 6.9112, 6.9112, 170.5573041426782, 1769927.589345896, 1769927.589345896, 363872.3780084965], 
processed observation next is [1.0, 0.7391304347826086, 0.5592417061611374, 0.75, 1.0, 1.0, 0.3036177167319559, 1.0, 1.0, 0.3036177167319559, 1.0, 1.0, 0.6617881804996537, 0.0, 0.0, 0.8375144448122397, 0.4916465525960822, 0.4916465525960822, 0.5430931015052186], 
reward next is 0.4569, 
noisyNet noise sample is [array([-0.3097386], dtype=float32), 1.3530289]. 
=============================================
[2019-03-27 13:41:40,410] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1264422: loss 1.1007
[2019-03-27 13:41:40,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1264422: learning rate 0.0000
[2019-03-27 13:41:41,017] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1264645: loss 0.2243
[2019-03-27 13:41:41,020] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1264645: learning rate 0.0000
[2019-03-27 13:41:42,862] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1265354: loss 0.0762
[2019-03-27 13:41:42,866] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1265356: learning rate 0.0000
[2019-03-27 13:41:44,550] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1265993: loss 0.9128
[2019-03-27 13:41:44,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1265993: learning rate 0.0000
[2019-03-27 13:41:44,743] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1266066: loss 1.1395
[2019-03-27 13:41:44,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1266066: learning rate 0.0000
[2019-03-27 13:41:45,699] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.9196386e-11 8.0036007e-05 1.2035946e-18 8.5827363e-09 9.9991989e-01], sum to 1.0000
[2019-03-27 13:41:45,710] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2520
[2019-03-27 13:41:45,715] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 75.0, 1.0, 2.0, 0.4141926150403256, 1.0, 2.0, 0.4141926150403256, 1.0, 2.0, 0.7120575566306423, 6.9112, 6.9112, 170.5573041426782, 1737144.628377806, 1737144.628377806, 359824.0382589143], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7812000.0000, 
sim time next is 7812600.0000, 
raw observation next is [29.1, 74.5, 1.0, 2.0, 0.4554515556624176, 1.0, 2.0, 0.4554515556624176, 1.0, 2.0, 0.7837730848874046, 6.9112, 6.9112, 170.5573041426782, 1910340.954085025, 1910340.954085025, 384534.9403046201], 
processed observation next is [1.0, 0.43478260869565216, 0.5781990521327015, 0.745, 1.0, 1.0, 0.34391753694267174, 1.0, 1.0, 0.34391753694267174, 1.0, 1.0, 0.7363086401065909, 0.0, 0.0, 0.8375144448122397, 0.530650265023618, 0.530650265023618, 0.5739327467233135], 
reward next is 0.4261, 
noisyNet noise sample is [array([1.2684171], dtype=float32), -1.0819823]. 
=============================================
[2019-03-27 13:41:45,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:41:45,870] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:41:45,940] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1266512: loss 1.7608
[2019-03-27 13:41:45,947] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1266513: learning rate 0.0000
[2019-03-27 13:41:45,986] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-03-27 13:41:46,418] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1266705: loss 1.6684
[2019-03-27 13:41:46,422] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1266706: learning rate 0.0000
[2019-03-27 13:41:46,861] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1266909: loss 2.5421
[2019-03-27 13:41:46,868] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1266909: learning rate 0.0000
[2019-03-27 13:41:47,849] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1267314: loss 1.3797
[2019-03-27 13:41:47,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1267314: learning rate 0.0000
[2019-03-27 13:41:49,314] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:41:49,314] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:41:49,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-03-27 13:41:49,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.9889276e-17 9.9996686e-01 1.4146780e-23 9.4816038e-21 3.3111221e-05], sum to 1.0000
[2019-03-27 13:41:49,661] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6141
[2019-03-27 13:41:49,666] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.76666666666667, 88.0, 1.0, 2.0, 0.4875094117492169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681213.063239944, 681213.063239944, 181662.0310324607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7681200.0000, 
sim time next is 7681800.0000, 
raw observation next is [25.73333333333333, 88.0, 1.0, 2.0, 0.4874824152639923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 681175.3280614469, 681175.3280614462, 181657.8265918322], 
processed observation next is [1.0, 0.9130434782608695, 0.41864139020537117, 0.88, 1.0, 1.0, 0.38250893405300274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18921536890595747, 0.18921536890595728, 0.2711310844654212], 
reward next is 0.7289, 
noisyNet noise sample is [array([-0.86140263], dtype=float32), -1.5410666]. 
=============================================
[2019-03-27 13:41:49,886] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9815063e-23 1.0000000e+00 2.3945314e-31 1.2644850e-30 6.5053039e-11], sum to 1.0000
[2019-03-27 13:41:49,900] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9107
[2019-03-27 13:41:49,911] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 86.66666666666666, 1.0, 2.0, 0.5408653677065502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755795.6329067571, 755795.6329067578, 190246.9048509759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7846800.0000, 
sim time next is 7847400.0000, 
raw observation next is [27.45, 87.83333333333334, 1.0, 2.0, 0.5415887712110053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756806.8641954357, 756806.8641954362, 190369.0900165024], 
processed observation next is [1.0, 0.8260869565217391, 0.5, 0.8783333333333334, 1.0, 1.0, 0.4476973147120546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21022412894317657, 0.21022412894317674, 0.28413297017388417], 
reward next is 0.7159, 
noisyNet noise sample is [array([-2.2209144], dtype=float32), -0.54804003]. 
=============================================
[2019-03-27 13:41:51,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7814183e-19 1.0000000e+00 1.9769041e-26 7.1640286e-26 1.0870760e-09], sum to 1.0000
[2019-03-27 13:41:51,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6725
[2019-03-27 13:41:51,642] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 88.0, 1.0, 2.0, 0.6431744756247157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 898821.0494038481, 898821.0494038475, 209054.8110721068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7788600.0000, 
sim time next is 7789200.0000, 
raw observation next is [25.83333333333334, 88.33333333333333, 1.0, 2.0, 0.6392658256620077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893356.5066685327, 893356.5066685327, 208279.128575393], 
processed observation next is [1.0, 0.13043478260869565, 0.42338072669826254, 0.8833333333333333, 1.0, 1.0, 0.5653805128457924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24815458518570355, 0.24815458518570355, 0.31086437100804926], 
reward next is 0.6891, 
noisyNet noise sample is [array([0.5389999], dtype=float32), 0.92340267]. 
=============================================
[2019-03-27 13:41:53,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8554409e-22 1.0000000e+00 1.0933275e-30 9.0628484e-34 7.9228984e-12], sum to 1.0000
[2019-03-27 13:41:53,572] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8605
[2019-03-27 13:41:53,578] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 90.0, 1.0, 2.0, 0.379467015531183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 584533.8555543392, 584533.8555543387, 173620.7069622091], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 100800.0000, 
sim time next is 101400.0000, 
raw observation next is [22.51666666666667, 90.16666666666667, 1.0, 2.0, 0.378453223783551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 582541.3871756506, 582541.38717565, 173434.9363647899], 
processed observation next is [1.0, 0.17391304347826086, 0.2661927330173777, 0.9016666666666667, 1.0, 1.0, 0.25114846238982047, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16181705199323626, 0.1618170519932361, 0.25885811397729835], 
reward next is 0.7411, 
noisyNet noise sample is [array([-0.3698517], dtype=float32), 1.442001]. 
=============================================
[2019-03-27 13:41:54,562] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:41:54,563] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:41:54,668] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-03-27 13:41:55,131] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:41:55,132] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:41:55,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-03-27 13:41:55,513] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:41:55,514] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:41:55,579] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-03-27 13:41:56,002] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0869543e-10 1.4063102e-04 1.1365375e-17 2.6281086e-09 9.9985933e-01], sum to 1.0000
[2019-03-27 13:41:56,006] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9850
[2019-03-27 13:41:56,010] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.2, 74.0, 1.0, 2.0, 0.4555159328132403, 1.0, 2.0, 0.4555159328132403, 1.0, 2.0, 0.7842584373518454, 6.9112, 6.9112, 170.5573041426782, 1910611.217655284, 1910611.217655284, 384636.6336429788], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7900200.0000, 
sim time next is 7900800.0000, 
raw observation next is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.4270773332023082, 1.0, 2.0, 0.4270773332023082, 1.0, 2.0, 0.7352417567525253, 6.911200000000001, 6.9112, 170.5573041426782, 1791228.949978638, 1791228.949978638, 367408.565106563], 
processed observation next is [1.0, 0.43478260869565216, 0.5892575039494474, 0.7333333333333334, 1.0, 1.0, 0.3097317267497689, 1.0, 1.0, 0.3097317267497689, 1.0, 1.0, 0.6771240936006407, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.49756359721628834, 0.49756359721628834, 0.5483709926963627], 
reward next is 0.4516, 
noisyNet noise sample is [array([0.75907815], dtype=float32), 0.26110455]. 
=============================================
[2019-03-27 13:41:56,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:41:56,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:41:56,123] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-03-27 13:41:56,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:41:56,972] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:41:57,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-03-27 13:41:59,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:41:59,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:41:59,548] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-03-27 13:42:00,084] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:42:00,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:00,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-03-27 13:42:01,646] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:42:01,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:01,739] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-03-27 13:42:02,899] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:42:02,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:02,980] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:42:02,981] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:02,982] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-03-27 13:42:03,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-03-27 13:42:03,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5586456e-12 9.1598850e-01 4.9099981e-20 1.0989541e-14 8.4011473e-02], sum to 1.0000
[2019-03-27 13:42:03,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9375
[2019-03-27 13:42:03,419] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.7, 65.33333333333334, 1.0, 2.0, 1.021612509034909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1550388.656170097, 1550388.656170097, 324055.4728581236], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 42600.0000, 
sim time next is 43200.0000, 
raw observation next is [26.8, 65.0, 1.0, 2.0, 0.3316200004625058, 1.0, 1.0, 0.3316200004625058, 1.0, 1.0, 0.5784366915606127, 6.9112, 6.9112, 170.5573041426782, 1489716.147997992, 1489716.147997992, 327193.6272946036], 
processed observation next is [1.0, 0.5217391304347826, 0.4691943127962086, 0.65, 1.0, 1.0, 0.19472289212350094, 1.0, 0.5, 0.19472289212350094, 1.0, 0.5, 0.4858984043422106, 0.0, 0.0, 0.8375144448122397, 0.41381004111055336, 0.41381004111055336, 0.4883486974546323], 
reward next is 0.5117, 
noisyNet noise sample is [array([0.40753555], dtype=float32), 0.35794994]. 
=============================================
[2019-03-27 13:42:04,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:42:04,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:04,148] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-03-27 13:42:04,277] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:42:04,277] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:04,336] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-03-27 13:42:04,736] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:42:04,736] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:04,799] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-03-27 13:42:05,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 13:42:05,258] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:05,312] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-03-27 13:42:05,507] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5041374e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0315412e-18], sum to 1.0000
[2019-03-27 13:42:05,514] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5937
[2019-03-27 13:42:05,519] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2866967877716635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461796.446713019, 461796.446713019, 164395.6499334722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 184800.0000, 
sim time next is 185400.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.2863924508741391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461306.5985648271, 461306.5985648271, 164362.226335434], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.14023186852305916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1281407218235631, 0.1281407218235631, 0.24531675572452838], 
reward next is 0.7547, 
noisyNet noise sample is [array([0.87371624], dtype=float32), -1.084566]. 
=============================================
[2019-03-27 13:42:05,699] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 13:42:05,701] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:42:05,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:05,703] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:42:05,704] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:05,704] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:42:05,704] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:42:05,705] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:05,705] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:42:05,706] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:05,707] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:42:05,730] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run52
[2019-03-27 13:42:05,731] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run52
[2019-03-27 13:42:05,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run52
[2019-03-27 13:42:05,779] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run52
[2019-03-27 13:42:05,779] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run52
[2019-03-27 13:42:42,873] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06176555], dtype=float32), 0.06359182]
[2019-03-27 13:42:42,873] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.73785263, 90.65258373666667, 1.0, 2.0, 0.5047696404023139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705339.3601128841, 705339.3601128848, 184346.3679552857]
[2019-03-27 13:42:42,873] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:42:42,876] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.16275008e-20 1.00000000e+00 2.34728899e-28 1.27056056e-26
 1.94580512e-08], sampled 0.08484052678498322
[2019-03-27 13:42:56,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06176555], dtype=float32), 0.06359182]
[2019-03-27 13:42:56,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.68751803333333, 97.61292999, 1.0, 2.0, 0.4901842931535852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684951.9687249085, 684951.9687249085, 182072.5923057899]
[2019-03-27 13:42:56,246] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:42:56,248] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3189395e-23 1.0000000e+00 2.7965113e-32 2.6204335e-32 2.8365685e-11], sampled 0.7840720178704834
[2019-03-27 13:44:07,115] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06176555], dtype=float32), 0.06359182]
[2019-03-27 13:44:07,116] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.1, 85.33333333333334, 1.0, 2.0, 0.5301383339862067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740800.6400195063, 740800.6400195058, 188452.2032967345]
[2019-03-27 13:44:07,117] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:44:07,120] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.91323679e-22 1.00000000e+00 7.17131064e-31 1.18342320e-30
 1.12550455e-10], sampled 0.7283743258182611
[2019-03-27 13:44:11,876] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06176555], dtype=float32), 0.06359182]
[2019-03-27 13:44:11,876] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.83333333333333, 65.16666666666667, 1.0, 2.0, 0.4385641038687605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681584.7494318925, 681584.7494318931, 182745.5988264776]
[2019-03-27 13:44:11,877] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:44:11,879] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2630744e-27 1.0000000e+00 2.0882631e-36 0.0000000e+00 3.7662214e-16], sampled 0.8528376753394241
[2019-03-27 13:44:11,882] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06176555], dtype=float32), 0.06359182]
[2019-03-27 13:44:11,882] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.59240234833334, 67.14006774166667, 1.0, 2.0, 0.5483544702542774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766264.5461605393, 766264.5461605393, 191515.8028221366]
[2019-03-27 13:44:11,883] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:44:11,885] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4376402e-22 1.0000000e+00 1.9356621e-31 1.1980531e-31 3.1730455e-11], sampled 0.4933973439897711
[2019-03-27 13:44:12,643] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06176555], dtype=float32), 0.06359182]
[2019-03-27 13:44:12,644] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.35211972, 87.48189165333334, 1.0, 2.0, 0.4038938927766195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 609689.8502613517, 609689.8502613524, 175611.1078728895]
[2019-03-27 13:44:12,646] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:44:12,650] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2857688e-22 1.0000000e+00 2.4740315e-31 2.2226462e-30 4.9494486e-10], sampled 0.5564298835074712
[2019-03-27 13:44:14,010] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8778.8988 2783122906.6350 640.0000
[2019-03-27 13:44:14,114] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8392.2485 2997659666.9414 816.0000
[2019-03-27 13:44:14,158] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8155.3191 3155389672.7300 1143.0000
[2019-03-27 13:44:14,251] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8699.5680 2840663119.3586 626.0000
[2019-03-27 13:44:14,435] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8477.8413 2927718762.5201 844.0000
[2019-03-27 13:44:15,452] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1275000, evaluation results [1275000.0, 8155.319055063866, 3155389672.7300467, 1143.0, 8477.841282472007, 2927718762.52012, 844.0, 8778.898806203712, 2783122906.6349616, 640.0, 8392.24850084998, 2997659666.9414425, 816.0, 8699.568007634083, 2840663119.3586493, 626.0]
[2019-03-27 13:44:17,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2227084e-28 1.0000000e+00 2.9293142e-37 0.0000000e+00 4.8981570e-16], sum to 1.0000
[2019-03-27 13:44:17,160] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8174
[2019-03-27 13:44:17,165] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.13333333333333, 94.33333333333334, 1.0, 2.0, 0.2890022745360856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465186.9534925816, 465186.9534925816, 164627.7355783828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 196800.0000, 
sim time next is 197400.0000, 
raw observation next is [20.16666666666666, 94.16666666666667, 1.0, 2.0, 0.2890798779014609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 465186.8719295451, 465186.8719295457, 164627.5537880551], 
processed observation next is [0.0, 0.2608695652173913, 0.15481832543443896, 0.9416666666666668, 1.0, 1.0, 0.14346973241139865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12921857553598473, 0.12921857553598493, 0.24571276684784346], 
reward next is 0.7543, 
noisyNet noise sample is [array([-1.9051678], dtype=float32), 0.10601317]. 
=============================================
[2019-03-27 13:44:21,223] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4114265e-22 1.0000000e+00 2.2197725e-31 1.8293531e-29 6.9402057e-09], sum to 1.0000
[2019-03-27 13:44:21,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7772
[2019-03-27 13:44:21,238] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.45, 88.0, 1.0, 2.0, 0.3473432723090848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539420.6966323056, 539420.6966323056, 169890.0225572612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 81000.0000, 
sim time next is 81600.0000, 
raw observation next is [22.36666666666667, 88.33333333333333, 1.0, 2.0, 0.3456443531509074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537359.5098161114, 537359.5098161114, 169736.950705746], 
processed observation next is [1.0, 0.9565217391304348, 0.2590837282780413, 0.8833333333333333, 1.0, 1.0, 0.2116197025914547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1492665305044754, 0.1492665305044754, 0.25333873239663585], 
reward next is 0.7467, 
noisyNet noise sample is [array([0.31399864], dtype=float32), 0.5692296]. 
=============================================
[2019-03-27 13:44:24,147] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6596828e-22 1.0000000e+00 8.2617514e-31 9.6806237e-30 7.5766332e-10], sum to 1.0000
[2019-03-27 13:44:24,160] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4196
[2019-03-27 13:44:24,166] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 96.0, 1.0, 2.0, 0.3462904811820275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538335.8262998111, 538335.8262998117, 169815.5839444151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 164400.0000, 
sim time next is 165000.0000, 
raw observation next is [21.3, 96.0, 1.0, 2.0, 0.3422260071858832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 533454.9806005908, 533454.9806005908, 169456.5331503068], 
processed observation next is [1.0, 0.9130434782608695, 0.2085308056872039, 0.96, 1.0, 1.0, 0.2075012134769677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14818193905571966, 0.14818193905571966, 0.25292019873180116], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.6634804], dtype=float32), -0.038373973]. 
=============================================
[2019-03-27 13:44:24,176] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.102295]
 [81.97106 ]
 [81.76407 ]
 [81.60789 ]
 [81.39812 ]], R is [[82.09509277]
 [82.02068329]
 [81.94647217]
 [81.87250519]
 [81.79884338]].
[2019-03-27 13:44:25,577] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8711501e-18 9.9999440e-01 3.0369783e-26 1.1592096e-21 5.5892715e-06], sum to 1.0000
[2019-03-27 13:44:25,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8402
[2019-03-27 13:44:25,607] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 96.0, 1.0, 2.0, 0.3810339500190703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 573499.0209116923, 573499.0209116916, 172282.963680865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 150600.0000, 
sim time next is 151200.0000, 
raw observation next is [22.5, 96.0, 1.0, 2.0, 0.3826110538410947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575872.5079116046, 575872.5079116046, 172493.9529415882], 
processed observation next is [1.0, 0.782608695652174, 0.2654028436018958, 0.96, 1.0, 1.0, 0.25615789619408996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15996458553100126, 0.15996458553100126, 0.2574536611068481], 
reward next is 0.7425, 
noisyNet noise sample is [array([-1.0911185], dtype=float32), -0.6152681]. 
=============================================
[2019-03-27 13:44:27,426] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2538340e-24 1.0000000e+00 5.9966242e-34 3.3372708e-32 5.0877150e-12], sum to 1.0000
[2019-03-27 13:44:27,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6921
[2019-03-27 13:44:27,440] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 96.0, 1.0, 2.0, 0.3176503916619723, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 503813.350235659, 503813.3502356585, 167337.6746773538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 169200.0000, 
sim time next is 169800.0000, 
raw observation next is [20.56666666666667, 96.0, 1.0, 2.0, 0.3148618225490976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 500013.017037639, 500013.0170376384, 167063.0082945245], 
processed observation next is [1.0, 1.0, 0.17377567140600336, 0.96, 1.0, 1.0, 0.17453231632421398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1388925047326775, 0.13889250473267734, 0.2493477735739172], 
reward next is 0.7507, 
noisyNet noise sample is [array([-0.3668294], dtype=float32), 0.098068]. 
=============================================
[2019-03-27 13:44:35,520] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8667344e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2279107e-16], sum to 1.0000
[2019-03-27 13:44:35,538] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0529
[2019-03-27 13:44:35,545] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.23333333333333, 82.33333333333334, 1.0, 2.0, 0.2981145243526388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 474091.621026023, 474091.6210260236, 165190.2972076315], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 291000.0000, 
sim time next is 291600.0000, 
raw observation next is [22.3, 82.0, 1.0, 2.0, 0.2994262534653076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475955.9677280707, 475955.9677280707, 165318.6499212074], 
processed observation next is [0.0, 0.391304347826087, 0.25592417061611383, 0.82, 1.0, 1.0, 0.15593524513892482, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13220999103557518, 0.13220999103557518, 0.2467442536137424], 
reward next is 0.7533, 
noisyNet noise sample is [array([-0.27894422], dtype=float32), 1.1709625]. 
=============================================
[2019-03-27 13:44:37,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6363772e-23 1.0000000e+00 1.0156968e-31 2.6488268e-31 2.4833904e-10], sum to 1.0000
[2019-03-27 13:44:37,874] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8548
[2019-03-27 13:44:37,878] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.03333333333333, 87.33333333333333, 1.0, 2.0, 0.2152333624153343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 358951.9704273189, 358951.9704273189, 157053.0264604676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 605400.0000, 
sim time next is 606000.0000, 
raw observation next is [17.96666666666667, 87.66666666666667, 1.0, 2.0, 0.2144727358452151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 357739.3476943186, 357739.3476943193, 156966.718099004], 
processed observation next is [1.0, 0.0, 0.050552922590837435, 0.8766666666666667, 1.0, 1.0, 0.05358160945206636, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09937204102619962, 0.09937204102619981, 0.23427868372985672], 
reward next is 0.7657, 
noisyNet noise sample is [array([0.02277553], dtype=float32), 1.4260303]. 
=============================================
[2019-03-27 13:44:37,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.21733]
 [82.94846]
 [86.83313]
 [86.80522]
 [86.76459]], R is [[78.76065826]
 [78.73864746]
 [78.71673584]
 [78.69493866]
 [78.67327881]].
[2019-03-27 13:44:40,897] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2070056e-24 1.0000000e+00 1.6487447e-32 1.2263613e-31 2.2060763e-12], sum to 1.0000
[2019-03-27 13:44:40,909] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1960
[2019-03-27 13:44:40,914] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 77.0, 1.0, 2.0, 0.4975196734632705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 798472.6572228409, 798472.6572228416, 194600.2014099483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 400800.0000, 
sim time next is 401400.0000, 
raw observation next is [22.45, 77.5, 1.0, 2.0, 0.5017450538570146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 804951.2767451936, 804951.2767451929, 195335.7752347875], 
processed observation next is [1.0, 0.6521739130434783, 0.26303317535545023, 0.775, 1.0, 1.0, 0.39969283597230676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2235975768736649, 0.22359757687366472, 0.29154593318625], 
reward next is 0.7085, 
noisyNet noise sample is [array([2.0674493], dtype=float32), 0.50117517]. 
=============================================
[2019-03-27 13:44:44,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4605713e-27 1.0000000e+00 7.6590571e-37 0.0000000e+00 2.6336229e-16], sum to 1.0000
[2019-03-27 13:44:44,289] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6770
[2019-03-27 13:44:44,295] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 82.16666666666667, 1.0, 2.0, 0.2283710658019763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377793.6542760348, 377793.6542760348, 158757.3940653998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 449400.0000, 
sim time next is 450000.0000, 
raw observation next is [19.7, 82.0, 1.0, 2.0, 0.2276214682274569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 376631.9645493028, 376631.9645493035, 158682.8392897372], 
processed observation next is [1.0, 0.21739130434782608, 0.1327014218009479, 0.82, 1.0, 1.0, 0.0694234556957312, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10461999015258412, 0.10461999015258432, 0.2368400586413988], 
reward next is 0.7632, 
noisyNet noise sample is [array([1.0353088], dtype=float32), -0.6226378]. 
=============================================
[2019-03-27 13:44:44,311] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.95123 ]
 [74.9632  ]
 [75.074005]
 [75.195335]
 [75.25313 ]], R is [[75.08219147]
 [75.09441376]
 [75.10642242]
 [75.11830139]
 [75.13015747]].
[2019-03-27 13:44:55,661] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6031086e-18 9.9998391e-01 4.3767405e-26 1.6839719e-23 1.6085585e-05], sum to 1.0000
[2019-03-27 13:44:55,677] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9537
[2019-03-27 13:44:55,683] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333334, 60.33333333333333, 1.0, 2.0, 0.7576476266142503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1239955.634815671, 1239955.63481567, 253664.6309543703], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 571200.0000, 
sim time next is 571800.0000, 
raw observation next is [23.81666666666667, 60.66666666666666, 1.0, 2.0, 0.7596594214233054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1242601.624228484, 1242601.624228484, 254190.4673776444], 
processed observation next is [1.0, 0.6086956521739131, 0.3278041074249607, 0.6066666666666666, 1.0, 1.0, 0.7104330378594041, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34516711784124554, 0.34516711784124554, 0.37938875728006627], 
reward next is 0.6206, 
noisyNet noise sample is [array([-1.3153323], dtype=float32), 2.4047642]. 
=============================================
[2019-03-27 13:44:57,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0270050e-24 1.0000000e+00 1.5267305e-33 1.6977217e-34 6.8559085e-13], sum to 1.0000
[2019-03-27 13:44:57,788] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4538
[2019-03-27 13:44:57,794] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.73333333333333, 87.0, 1.0, 2.0, 0.2569188624737023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 421992.8074784896, 421992.807478489, 161600.1278023405], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 772800.0000, 
sim time next is 773400.0000, 
raw observation next is [19.66666666666667, 87.5, 1.0, 2.0, 0.2563921792308387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 421159.7838542316, 421159.7838542322, 161546.9231726904], 
processed observation next is [1.0, 0.9565217391304348, 0.1311216429699845, 0.875, 1.0, 1.0, 0.10408696292872129, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11698882884839766, 0.11698882884839784, 0.24111481070550808], 
reward next is 0.7589, 
noisyNet noise sample is [array([0.00140021], dtype=float32), 1.1264104]. 
=============================================
[2019-03-27 13:44:57,825] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9735893e-28 1.0000000e+00 6.8980942e-35 6.3861857e-38 1.9112737e-15], sum to 1.0000
[2019-03-27 13:44:57,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7961
[2019-03-27 13:44:57,849] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.58333333333334, 87.0, 1.0, 2.0, 0.221775458374074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 368721.5013603071, 368721.5013603077, 157924.3054245826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 712200.0000, 
sim time next is 712800.0000, 
raw observation next is [18.8, 86.0, 1.0, 2.0, 0.2235569183009453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 371396.0065617808, 371396.0065617808, 158134.560378769], 
processed observation next is [1.0, 0.2608695652173913, 0.09004739336492901, 0.86, 1.0, 1.0, 0.06452640759150034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10316555737827245, 0.10316555737827245, 0.23602173190861045], 
reward next is 0.7640, 
noisyNet noise sample is [array([0.50595415], dtype=float32), 0.27784237]. 
=============================================
[2019-03-27 13:45:04,793] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.426316e-22 1.000000e+00 1.737090e-31 1.631411e-29 7.786657e-12], sum to 1.0000
[2019-03-27 13:45:04,802] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3045
[2019-03-27 13:45:04,809] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 96.33333333333334, 1.0, 2.0, 0.3815997228179991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572996.761448401, 572996.761448401, 172195.7943425744], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1042800.0000, 
sim time next is 1043400.0000, 
raw observation next is [22.56666666666667, 96.16666666666666, 1.0, 2.0, 0.3827724543523893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 574503.0928234248, 574503.0928234248, 172321.7144258202], 
processed observation next is [1.0, 0.043478260869565216, 0.26856240126382325, 0.9616666666666666, 1.0, 1.0, 0.2563523546414329, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15958419245095135, 0.15958419245095135, 0.257196588695254], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.86124355], dtype=float32), -0.115060985]. 
=============================================
[2019-03-27 13:45:12,726] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6423048e-24 1.0000000e+00 3.1529326e-33 5.1555887e-34 6.6184715e-14], sum to 1.0000
[2019-03-27 13:45:12,734] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0801
[2019-03-27 13:45:12,741] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 80.33333333333334, 1.0, 2.0, 0.297552861713451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474741.94150616, 474741.94150616, 165261.1024417164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 848400.0000, 
sim time next is 849000.0000, 
raw observation next is [22.26666666666667, 80.66666666666666, 1.0, 2.0, 0.2966932051623078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 473573.2531905661, 473573.2531905668, 165181.3424400975], 
processed observation next is [0.0, 0.8260869565217391, 0.2543443917851502, 0.8066666666666665, 1.0, 1.0, 0.15264241585820215, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13154812588626835, 0.13154812588626855, 0.2465393170747724], 
reward next is 0.7535, 
noisyNet noise sample is [array([0.37409586], dtype=float32), 1.1593295]. 
=============================================
[2019-03-27 13:45:12,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.15324 ]
 [77.16158 ]
 [77.15843 ]
 [77.161995]
 [77.151985]], R is [[77.11430359]
 [77.09650421]
 [77.07875824]
 [77.06109619]
 [77.04355621]].
[2019-03-27 13:45:14,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.409668e-28 1.000000e+00 3.384569e-38 0.000000e+00 7.153357e-17], sum to 1.0000
[2019-03-27 13:45:14,604] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7196
[2019-03-27 13:45:14,610] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333333, 75.66666666666666, 1.0, 2.0, 0.2966799430323602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 473057.4905222168, 473057.4905222162, 165137.9243036443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 906000.0000, 
sim time next is 906600.0000, 
raw observation next is [23.16666666666667, 74.83333333333334, 1.0, 2.0, 0.296635130069516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472871.0608527992, 472871.0608527992, 165123.0257339152], 
processed observation next is [0.0, 0.4782608695652174, 0.2969984202211693, 0.7483333333333334, 1.0, 1.0, 0.1525724458668867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1313530724591109, 0.1313530724591109, 0.2464522772147988], 
reward next is 0.7535, 
noisyNet noise sample is [array([-0.35747513], dtype=float32), 1.0645111]. 
=============================================
[2019-03-27 13:45:21,542] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 13:45:21,544] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:45:21,544] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:45:21,546] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:45:21,547] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:45:21,547] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:45:21,550] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:45:21,550] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:45:21,552] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:45:21,554] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:45:21,557] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:45:21,581] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run53
[2019-03-27 13:45:21,601] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run53
[2019-03-27 13:45:21,602] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run53
[2019-03-27 13:45:21,642] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run53
[2019-03-27 13:45:21,666] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run53
[2019-03-27 13:45:28,641] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06058685], dtype=float32), 0.064877115]
[2019-03-27 13:45:28,642] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.36666666666667, 90.33333333333334, 1.0, 2.0, 0.2791471317299539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458313.9486108707, 458313.9486108707, 163928.7712787949]
[2019-03-27 13:45:28,643] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:45:28,646] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0378024e-26 1.0000000e+00 8.0700442e-35 6.2481365e-37 6.5600148e-15], sampled 0.5895875361892238
[2019-03-27 13:45:43,013] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06058685], dtype=float32), 0.064877115]
[2019-03-27 13:45:43,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.96666666666667, 69.33333333333334, 1.0, 2.0, 0.2328675965586186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 386943.7186074242, 386943.7186074235, 158967.9818614163]
[2019-03-27 13:45:43,018] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:45:43,022] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.9121127e-26 1.0000000e+00 2.9654430e-34 1.1631895e-36 5.7991835e-15], sampled 0.9048241826992725
[2019-03-27 13:45:58,159] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06058685], dtype=float32), 0.064877115]
[2019-03-27 13:45:58,160] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.39675723, 92.863666335, 1.0, 2.0, 0.5935147055608718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 829395.6039677893, 829395.6039677886, 199542.2820972099]
[2019-03-27 13:45:58,161] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:45:58,165] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3749133e-21 1.0000000e+00 1.5256208e-30 2.1108091e-28 7.1826811e-10], sampled 0.8872533586208571
[2019-03-27 13:46:19,592] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06058685], dtype=float32), 0.064877115]
[2019-03-27 13:46:19,594] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.16666666666666, 78.16666666666667, 1.0, 2.0, 0.485625832047042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678580.234230278, 678580.2342302772, 181374.384125974]
[2019-03-27 13:46:19,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:46:19,596] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.4652151e-23 1.0000000e+00 6.6038215e-32 1.0408469e-31 4.8030434e-12], sampled 0.36442507108403266
[2019-03-27 13:46:48,480] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06058685], dtype=float32), 0.064877115]
[2019-03-27 13:46:48,482] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.30026076666667, 97.30605725, 1.0, 2.0, 0.8580683055501075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1199299.651567069, 1199299.651567068, 258667.6361252645]
[2019-03-27 13:46:48,482] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:46:48,485] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2964646e-20 1.0000000e+00 1.9669267e-29 1.9774160e-27 1.0921907e-09], sampled 0.12759514624169732
[2019-03-27 13:46:56,943] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06058685], dtype=float32), 0.064877115]
[2019-03-27 13:46:56,944] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.00472176666667, 84.72485611666667, 1.0, 2.0, 0.6356103000395874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 888245.8734002295, 888245.8734002295, 207559.3051874815]
[2019-03-27 13:46:56,945] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:46:56,948] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1219392e-23 1.0000000e+00 1.5528515e-32 1.2668301e-32 1.5407359e-12], sampled 0.7832099033402352
[2019-03-27 13:47:29,655] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8659.5696 2839134719.5296 743.0000
[2019-03-27 13:47:29,928] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06058685], dtype=float32), 0.064877115]
[2019-03-27 13:47:29,929] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.67783477333333, 65.11338505, 1.0, 2.0, 0.6180188203617786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 910003.0511495487, 910003.0511495481, 210052.0628253363]
[2019-03-27 13:47:29,930] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:47:29,932] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5044122e-21 1.0000000e+00 2.7225158e-30 9.6677494e-29 2.3307056e-10], sampled 0.3316900329078065
[2019-03-27 13:47:30,011] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8392.3431 2925766833.0800 1025.0000
[2019-03-27 13:47:30,190] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8100.1714 3156003705.1288 1294.0000
[2019-03-27 13:47:30,225] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8738.0800 2779165414.1968 739.0000
[2019-03-27 13:47:30,406] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8319.9635 2997572358.7201 981.0000
[2019-03-27 13:47:31,422] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1300000, evaluation results [1300000.0, 8100.171366055923, 3156003705.1287847, 1294.0, 8392.343118794248, 2925766833.0800457, 1025.0, 8738.080039342049, 2779165414.1967688, 739.0, 8319.963519357565, 2997572358.72011, 981.0, 8659.56959916869, 2839134719.5296097, 743.0]
[2019-03-27 13:47:32,852] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.5039449e-29 1.0000000e+00 7.8103831e-38 0.0000000e+00 3.2410412e-17], sum to 1.0000
[2019-03-27 13:47:32,860] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8223
[2019-03-27 13:47:32,864] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 93.5, 1.0, 2.0, 0.2755818247337372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445192.6284017785, 445192.6284017785, 163280.6944681586], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1143000.0000, 
sim time next is 1143600.0000, 
raw observation next is [20.2, 92.66666666666666, 1.0, 2.0, 0.2773614965207933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447636.3536289852, 447636.3536289852, 163443.0537337247], 
processed observation next is [1.0, 0.21739130434782608, 0.15639810426540288, 0.9266666666666665, 1.0, 1.0, 0.12935120062746178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.124343431563607, 0.124343431563607, 0.24394485631899207], 
reward next is 0.7561, 
noisyNet noise sample is [array([-0.7814472], dtype=float32), 0.4951349]. 
=============================================
[2019-03-27 13:47:44,963] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6693777e-23 1.0000000e+00 1.5308493e-31 1.5729619e-31 7.4344567e-13], sum to 1.0000
[2019-03-27 13:47:44,973] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9215
[2019-03-27 13:47:44,979] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 78.33333333333334, 1.0, 2.0, 0.9348104143229191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1353717.688544176, 1353717.688544176, 286729.6534344393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1243200.0000, 
sim time next is 1243800.0000, 
raw observation next is [26.2, 77.5, 1.0, 2.0, 0.9357744376790966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1351334.99541321, 1351334.995413211, 286461.4781470044], 
processed observation next is [1.0, 0.391304347826087, 0.44075829383886256, 0.775, 1.0, 1.0, 0.9226198044326466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.375370832059225, 0.3753708320592253, 0.4275544449955289], 
reward next is 0.5724, 
noisyNet noise sample is [array([-1.1800133], dtype=float32), 0.8119251]. 
=============================================
[2019-03-27 13:47:51,514] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2325681e-28 1.0000000e+00 3.1203092e-37 0.0000000e+00 9.2440649e-19], sum to 1.0000
[2019-03-27 13:47:51,524] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1683
[2019-03-27 13:47:51,530] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.25, 98.0, 1.0, 2.0, 0.3048391428512721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485239.5302921806, 485239.53029218, 165995.854233575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1391400.0000, 
sim time next is 1392000.0000, 
raw observation next is [20.26666666666667, 98.0, 1.0, 2.0, 0.3050233257361261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485352.8525933396, 485352.8525933389, 166001.3273036943], 
processed observation next is [0.0, 0.08695652173913043, 0.15955766192733034, 0.98, 1.0, 1.0, 0.16267870570617604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13482023683148323, 0.13482023683148303, 0.24776317508014076], 
reward next is 0.7522, 
noisyNet noise sample is [array([-0.91841155], dtype=float32), -0.1580123]. 
=============================================
[2019-03-27 13:47:51,550] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.44814 ]
 [75.857574]
 [76.21829 ]
 [75.95555 ]
 [75.87204 ]], R is [[75.26493835]
 [75.264534  ]
 [75.2641449 ]
 [75.26379395]
 [75.26343536]].
[2019-03-27 13:48:01,753] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5578167e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.2406633e-23], sum to 1.0000
[2019-03-27 13:48:01,764] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3725
[2019-03-27 13:48:01,771] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333334, 93.66666666666667, 1.0, 2.0, 0.3272875263374947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 512773.1299449203, 512773.1299449197, 167891.275647502], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1406400.0000, 
sim time next is 1407000.0000, 
raw observation next is [21.51666666666667, 93.33333333333333, 1.0, 2.0, 0.3283729307895555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513909.2603748629, 513909.2603748622, 167964.9492477559], 
processed observation next is [0.0, 0.2608695652173913, 0.21879936808846778, 0.9333333333333332, 1.0, 1.0, 0.19081075998741623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1427525723263508, 0.1427525723263506, 0.2506939541011282], 
reward next is 0.7493, 
noisyNet noise sample is [array([-1.1934551], dtype=float32), 0.77539146]. 
=============================================
[2019-03-27 13:48:01,788] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.18767 ]
 [76.191765]
 [76.194466]
 [76.21061 ]
 [76.210396]], R is [[76.16042328]
 [76.14823151]
 [76.13617706]
 [76.12432861]
 [76.11283112]].
[2019-03-27 13:48:03,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8330616e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.0925809e-22], sum to 1.0000
[2019-03-27 13:48:03,191] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7830
[2019-03-27 13:48:03,195] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.73333333333333, 97.66666666666666, 1.0, 2.0, 0.3204859474045841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504587.1865091437, 504587.1865091444, 167324.4294676684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1482000.0000, 
sim time next is 1482600.0000, 
raw observation next is [20.66666666666667, 97.83333333333334, 1.0, 2.0, 0.3187335954136062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502412.6620609707, 502412.6620609707, 167173.1726237352], 
processed observation next is [0.0, 0.13043478260869565, 0.17851500789889443, 0.9783333333333334, 1.0, 1.0, 0.17919710290795926, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13955907279471408, 0.13955907279471408, 0.24951219794587345], 
reward next is 0.7505, 
noisyNet noise sample is [array([0.8306913], dtype=float32), -0.26632386]. 
=============================================
[2019-03-27 13:48:05,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7964823e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.2807165e-23], sum to 1.0000
[2019-03-27 13:48:05,993] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5499
[2019-03-27 13:48:06,003] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 90.0, 1.0, 2.0, 0.3504139499905664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 537146.5565886591, 537146.5565886598, 169499.8816408128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1494000.0000, 
sim time next is 1494600.0000, 
raw observation next is [22.91666666666667, 89.16666666666667, 1.0, 2.0, 0.3533220127885744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539921.864594008, 539921.864594008, 169674.4484729247], 
processed observation next is [0.0, 0.30434782608695654, 0.28515007898894185, 0.8916666666666667, 1.0, 1.0, 0.22086989492599324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14997829572055776, 0.14997829572055776, 0.2532454454819772], 
reward next is 0.7468, 
noisyNet noise sample is [array([0.5516228], dtype=float32), -1.0949016]. 
=============================================
[2019-03-27 13:48:10,660] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4059393e-26 1.0000000e+00 1.9145984e-33 4.7780402e-36 3.2890443e-16], sum to 1.0000
[2019-03-27 13:48:10,671] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4886
[2019-03-27 13:48:10,679] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 86.66666666666667, 1.0, 2.0, 0.6317802615002365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 997703.614510872, 997703.614510872, 220280.7405242832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1779600.0000, 
sim time next is 1780200.0000, 
raw observation next is [21.7, 88.0, 1.0, 2.0, 0.6384595995319078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1009697.859320773, 1009697.859320773, 221894.1809250204], 
processed observation next is [1.0, 0.6086956521739131, 0.2274881516587678, 0.88, 1.0, 1.0, 0.5644091560625395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2804716275891036, 0.2804716275891036, 0.33118534466420957], 
reward next is 0.6688, 
noisyNet noise sample is [array([-0.5459855], dtype=float32), -0.42370895]. 
=============================================
[2019-03-27 13:48:13,917] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9544473e-30 1.0000000e+00 1.2501201e-38 0.0000000e+00 7.3030322e-21], sum to 1.0000
[2019-03-27 13:48:13,928] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1385
[2019-03-27 13:48:13,934] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 93.66666666666667, 1.0, 2.0, 0.4933639726387568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701216.7262283522, 701216.7262283515, 184076.0281955756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1747200.0000, 
sim time next is 1747800.0000, 
raw observation next is [24.35, 93.5, 1.0, 2.0, 0.4873468549343948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691831.4654513524, 691831.465451353, 183027.3932835453], 
processed observation next is [1.0, 0.21739130434782608, 0.35308056872037924, 0.935, 1.0, 1.0, 0.3823456083546925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19217540706982011, 0.19217540706982028, 0.27317521385603777], 
reward next is 0.7268, 
noisyNet noise sample is [array([-1.1897362], dtype=float32), -1.3529731]. 
=============================================
[2019-03-27 13:48:15,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2868188e-25 1.0000000e+00 9.7611462e-34 1.6608814e-35 9.8348248e-17], sum to 1.0000
[2019-03-27 13:48:15,157] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9716
[2019-03-27 13:48:15,166] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 85.0, 1.0, 2.0, 0.808003420546072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1225805.69447403, 1225805.694474031, 258861.4250405442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1593600.0000, 
sim time next is 1594200.0000, 
raw observation next is [23.68333333333333, 85.0, 1.0, 2.0, 0.8268311027735775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1253689.908224907, 1253689.908224907, 263925.0723206759], 
processed observation next is [1.0, 0.43478260869565216, 0.32148499210110576, 0.85, 1.0, 1.0, 0.791362774425997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34824719672914084, 0.34824719672914084, 0.3939180183890685], 
reward next is 0.6061, 
noisyNet noise sample is [array([0.6214737], dtype=float32), -0.75410837]. 
=============================================
[2019-03-27 13:48:17,386] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.0403238e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4304456e-20], sum to 1.0000
[2019-03-27 13:48:17,398] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4479
[2019-03-27 13:48:17,403] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 96.0, 1.0, 2.0, 0.4160993969157197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610950.8839793513, 610950.8839793506, 175278.2734516194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1630800.0000, 
sim time next is 1631400.0000, 
raw observation next is [23.18333333333333, 96.16666666666666, 1.0, 2.0, 0.4160070435122561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610601.1554898728, 610601.1554898728, 175238.9585569047], 
processed observation next is [1.0, 0.9130434782608695, 0.29778830963665076, 0.9616666666666666, 1.0, 1.0, 0.29639402832801937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1696114320805202, 0.1696114320805202, 0.2615506844132906], 
reward next is 0.7384, 
noisyNet noise sample is [array([1.4649377], dtype=float32), -1.1419897]. 
=============================================
[2019-03-27 13:48:23,706] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3648802e-28 1.0000000e+00 1.2669380e-37 0.0000000e+00 3.4722599e-20], sum to 1.0000
[2019-03-27 13:48:23,716] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9014
[2019-03-27 13:48:23,727] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.81666666666667, 85.33333333333334, 1.0, 2.0, 0.5083002362419704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710274.4841059294, 710274.4841059294, 184906.9153788787], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1713000.0000, 
sim time next is 1713600.0000, 
raw observation next is [26.7, 86.0, 1.0, 2.0, 0.5067137639541339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708056.8846347855, 708056.8846347849, 184654.8703867525], 
processed observation next is [1.0, 0.8695652173913043, 0.46445497630331756, 0.86, 1.0, 1.0, 0.4056792336796794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19668246795410707, 0.1966824679541069, 0.2756042841593321], 
reward next is 0.7244, 
noisyNet noise sample is [array([-0.01779549], dtype=float32), 0.3805016]. 
=============================================
[2019-03-27 13:48:28,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6148914e-29 1.0000000e+00 1.2827375e-38 0.0000000e+00 1.8392207e-20], sum to 1.0000
[2019-03-27 13:48:28,996] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4125
[2019-03-27 13:48:29,002] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.46666666666667, 95.5, 1.0, 2.0, 0.4784877433853632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668602.8197707537, 668602.8197707537, 180293.7844910112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2094600.0000, 
sim time next is 2095200.0000, 
raw observation next is [24.6, 95.0, 1.0, 2.0, 0.4780982346258191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668058.3783569079, 668058.3783569079, 180235.4446810426], 
processed observation next is [0.0, 0.2608695652173913, 0.36492890995260674, 0.95, 1.0, 1.0, 0.371202692320264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18557177176580775, 0.18557177176580775, 0.2690081263896158], 
reward next is 0.7310, 
noisyNet noise sample is [array([-0.4471206], dtype=float32), -1.0723976]. 
=============================================
[2019-03-27 13:48:30,420] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5687796e-28 1.0000000e+00 1.1450472e-35 3.7095725e-38 3.5040803e-17], sum to 1.0000
[2019-03-27 13:48:30,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0568575e-26 1.0000000e+00 1.4397719e-34 2.0718767e-37 1.5008957e-17], sum to 1.0000
[2019-03-27 13:48:30,436] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9742
[2019-03-27 13:48:30,448] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5164
[2019-03-27 13:48:30,449] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 85.5, 1.0, 2.0, 0.6861817415830092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1055454.805219974, 1055454.805219974, 229954.8168320121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1773000.0000, 
sim time next is 1773600.0000, 
raw observation next is [23.03333333333333, 85.33333333333334, 1.0, 2.0, 0.5787107684086624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 892389.9307546557, 892389.9307546552, 206922.1475052646], 
processed observation next is [1.0, 0.5217391304347826, 0.29067930489731436, 0.8533333333333334, 1.0, 1.0, 0.49242261254055714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24788609187629326, 0.2478860918762931, 0.3088390261272606], 
reward next is 0.6912, 
noisyNet noise sample is [array([-0.46426842], dtype=float32), 0.19780666]. 
=============================================
[2019-03-27 13:48:30,459] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 85.5, 1.0, 2.0, 0.6861817415830092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1055454.805219974, 1055454.805219974, 229954.8168320121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1773000.0000, 
sim time next is 1773600.0000, 
raw observation next is [23.03333333333333, 85.33333333333334, 1.0, 2.0, 0.5787107684086624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 892389.9307546557, 892389.9307546552, 206922.1475052646], 
processed observation next is [1.0, 0.5217391304347826, 0.29067930489731436, 0.8533333333333334, 1.0, 1.0, 0.49242261254055714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24788609187629326, 0.2478860918762931, 0.3088390261272606], 
reward next is 0.6912, 
noisyNet noise sample is [array([0.63752794], dtype=float32), 1.1497865]. 
=============================================
[2019-03-27 13:48:34,830] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.1340454e-29 1.0000000e+00 1.3257109e-36 0.0000000e+00 2.1364990e-19], sum to 1.0000
[2019-03-27 13:48:34,840] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3752
[2019-03-27 13:48:34,847] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 94.5, 1.0, 2.0, 0.562948585415229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786665.7550940972, 786665.7550940979, 194041.5687225549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2179800.0000, 
sim time next is 2180400.0000, 
raw observation next is [25.36666666666667, 93.66666666666666, 1.0, 2.0, 0.5759891990431278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804895.6455888246, 804895.6455888246, 196352.5197220974], 
processed observation next is [1.0, 0.21739130434782608, 0.40126382306477115, 0.9366666666666665, 1.0, 1.0, 0.4891436133049733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2235821237746735, 0.2235821237746735, 0.2930634622717872], 
reward next is 0.7069, 
noisyNet noise sample is [array([0.6966567], dtype=float32), -0.94829834]. 
=============================================
[2019-03-27 13:48:37,582] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 13:48:37,586] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:48:37,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:48:37,590] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:48:37,591] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:48:37,592] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:48:37,593] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:48:37,593] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:48:37,595] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:48:37,596] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:48:37,600] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:48:37,617] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run54
[2019-03-27 13:48:37,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run54
[2019-03-27 13:48:37,672] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run54
[2019-03-27 13:48:37,673] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run54
[2019-03-27 13:48:37,707] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run54
[2019-03-27 13:48:44,661] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:48:44,662] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.11292766, 86.22857749, 1.0, 2.0, 0.48421606102819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 702285.1770190044, 702285.177019005, 184414.5637921969]
[2019-03-27 13:48:44,663] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:48:44,665] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.9485002e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1551898e-23], sampled 0.8617827429400785
[2019-03-27 13:48:45,583] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:48:45,584] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.5, 91.0, 1.0, 2.0, 0.3181981512888502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 501792.8571815927, 501792.8571815921, 167131.4805827679]
[2019-03-27 13:48:45,587] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:48:45,593] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.131883e-33 1.000000e+00 0.000000e+00 0.000000e+00 3.056349e-24], sampled 0.6533674615151785
[2019-03-27 13:49:01,838] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:49:01,839] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.537512945, 65.416154815, 1.0, 2.0, 0.5008365553523287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699841.6571864391, 699841.6571864397, 183726.5984347813]
[2019-03-27 13:49:01,839] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:49:01,842] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.9729070e-29 1.0000000e+00 6.2884720e-38 0.0000000e+00 1.2254334e-19], sampled 0.2206097932747082
[2019-03-27 13:49:36,958] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:49:36,959] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.51666666666667, 75.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.209820268620939, 6.9112, 168.9116294464501, 1665749.847920324, 1453900.016054626, 311351.466639994]
[2019-03-27 13:49:36,960] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:49:36,962] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9251431e-26 1.0000000e+00 1.3970619e-34 2.2458974e-37 2.7267640e-17], sampled 0.17603446119319432
[2019-03-27 13:49:36,966] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1665749.847920324 W.
[2019-03-27 13:49:39,000] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:49:39,004] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.66666666666667, 69.0, 1.0, 2.0, 0.5395623729842705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753974.2050768668, 753974.2050768662, 190027.7037936371]
[2019-03-27 13:49:39,007] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:49:39,011] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.5717073e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.3982762e-26], sampled 0.5672313189545634
[2019-03-27 13:49:40,214] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:49:40,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.587651755, 60.74652306833333, 1.0, 2.0, 0.573675200963071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801660.810896749, 801660.8108967496, 195943.1462914314]
[2019-03-27 13:49:40,217] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:49:40,220] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3684521e-27 1.0000000e+00 2.3389758e-36 0.0000000e+00 5.6065764e-18], sampled 0.052553335618167885
[2019-03-27 13:49:55,196] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:49:55,197] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.16666666666667, 65.5, 1.0, 2.0, 0.8017993443641086, 1.0, 1.0, 0.8017993443641086, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2242381.56916736, 2242381.56916736, 420746.5056416474]
[2019-03-27 13:49:55,200] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:49:55,202] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3845212e-15 9.9999881e-01 1.7499846e-23 3.8613990e-18 1.2078114e-06], sampled 0.41785226016118315
[2019-03-27 13:49:55,204] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2242381.56916736 W.
[2019-03-27 13:49:55,993] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:49:55,995] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.26891793, 80.90529624999999, 1.0, 2.0, 0.5986499222109051, 0.0, 2.0, 0.0, 1.0, 2.0, 1.023968526112407, 6.911200000000001, 6.9112, 168.9129337052726, 1741569.30999185, 1741569.30999185, 368435.7366492172]
[2019-03-27 13:49:55,995] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:49:56,000] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.6211669e-23 1.0000000e+00 4.9789944e-31 2.9009981e-32 1.4029321e-14], sampled 0.6548918478540111
[2019-03-27 13:49:56,000] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1741569.30999185 W.
[2019-03-27 13:50:34,709] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:50:34,710] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.76666666666667, 56.66666666666666, 1.0, 2.0, 0.5983600933092571, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.921586413640911, 6.9112, 168.9128330787708, 1672997.524183107, 1665629.050257927, 364293.2856780995]
[2019-03-27 13:50:34,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:50:34,717] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4103207e-18 1.0000000e+00 1.1263759e-26 3.9610075e-24 7.8599904e-10], sampled 0.2705352913094613
[2019-03-27 13:50:34,717] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1672997.524183107 W.
[2019-03-27 13:50:38,505] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05939181], dtype=float32), 0.06364737]
[2019-03-27 13:50:38,506] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.41866171666667, 93.79314779666666, 1.0, 2.0, 0.367301130515175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560493.492630713, 560493.492630713, 171385.6019471215]
[2019-03-27 13:50:38,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:50:38,511] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.7495878e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0129434e-21], sampled 0.810125804317026
[2019-03-27 13:50:44,822] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.1917 2927303906.6071 1342.0000
[2019-03-27 13:50:45,084] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.5008 3164122230.1016 1783.0000
[2019-03-27 13:50:45,314] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6493 2779214162.7158 935.0000
[2019-03-27 13:50:45,385] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5461 3007746331.4965 1766.0000
[2019-03-27 13:50:45,489] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8727 2842416607.5499 1133.0000
[2019-03-27 13:50:46,501] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1325000, evaluation results [1325000.0, 7881.500821707205, 3164122230.1015577, 1783.0, 8254.191748350075, 2927303906.6070843, 1342.0, 8660.649316191608, 2779214162.7158475, 935.0, 7997.546052179389, 3007746331.496539, 1766.0, 8496.872744206477, 2842416607.549879, 1133.0]
[2019-03-27 13:51:11,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.0484582e-29 1.0000000e+00 6.1967436e-38 0.0000000e+00 5.8710396e-21], sum to 1.0000
[2019-03-27 13:51:11,673] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3327
[2019-03-27 13:51:11,683] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.71666666666667, 84.16666666666667, 1.0, 2.0, 0.5362253578679649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749309.478857817, 749309.4788578164, 189466.5266259519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2239800.0000, 
sim time next is 2240400.0000, 
raw observation next is [27.63333333333333, 84.33333333333334, 1.0, 2.0, 0.533385824450514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745340.1854358703, 745340.1854358697, 188992.3759554533], 
processed observation next is [1.0, 0.9565217391304348, 0.5086887835703, 0.8433333333333334, 1.0, 1.0, 0.4378142463259204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20703894039885287, 0.2070389403988527, 0.2820781730678407], 
reward next is 0.7179, 
noisyNet noise sample is [array([-1.7582105], dtype=float32), 1.8171427]. 
=============================================
[2019-03-27 13:51:12,710] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0280875e-28 1.0000000e+00 6.4537847e-36 0.0000000e+00 1.0692208e-20], sum to 1.0000
[2019-03-27 13:51:12,722] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0128
[2019-03-27 13:51:12,729] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 82.0, 1.0, 2.0, 0.670485267407717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 937004.0683421745, 937004.0683421738, 214601.8070888437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2347200.0000, 
sim time next is 2347800.0000, 
raw observation next is [27.26666666666667, 82.00000000000001, 1.0, 2.0, 0.6952553961221722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 971636.1919683645, 971636.1919683645, 219822.6598649816], 
processed observation next is [1.0, 0.17391304347826086, 0.4913112164297, 0.8200000000000002, 1.0, 1.0, 0.6328378266532195, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2698989422134346, 0.2698989422134346, 0.3280935221865397], 
reward next is 0.6719, 
noisyNet noise sample is [array([-0.993136], dtype=float32), 0.44851196]. 
=============================================
[2019-03-27 13:51:15,862] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4187035e-29 1.0000000e+00 3.0360515e-37 0.0000000e+00 1.1383310e-21], sum to 1.0000
[2019-03-27 13:51:15,874] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8657
[2019-03-27 13:51:15,881] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 81.0, 1.0, 2.0, 0.5220923383277316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729553.5273032588, 729553.5273032588, 187129.9057731547], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2340000.0000, 
sim time next is 2340600.0000, 
raw observation next is [27.76666666666667, 81.16666666666667, 1.0, 2.0, 1.036008902548683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1448172.200901085, 1448172.200901085, 310088.4172271824], 
processed observation next is [1.0, 0.08695652173913043, 0.515007898894155, 0.8116666666666668, 1.0, 1.0, 1.0433842199381722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.40227005580585695, 0.40227005580585695, 0.4628185331748991], 
reward next is 0.5372, 
noisyNet noise sample is [array([0.7088981], dtype=float32), -1.2262204]. 
=============================================
[2019-03-27 13:51:23,057] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.9516000e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1103062e-25], sum to 1.0000
[2019-03-27 13:51:23,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4557
[2019-03-27 13:51:23,073] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.9, 74.0, 1.0, 2.0, 0.5853945592538875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818043.8933791202, 818043.8933791202, 198056.8348316016], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2404800.0000, 
sim time next is 2405400.0000, 
raw observation next is [30.81666666666667, 74.33333333333334, 1.0, 2.0, 0.5856821183125834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818445.8898779831, 818445.8898779831, 198109.0939281125], 
processed observation next is [1.0, 0.8695652173913043, 0.6595576619273303, 0.7433333333333334, 1.0, 1.0, 0.5008218292922691, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22734608052166197, 0.22734608052166197, 0.29568521481807836], 
reward next is 0.7043, 
noisyNet noise sample is [array([0.418789], dtype=float32), -0.35173413]. 
=============================================
[2019-03-27 13:51:24,492] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7392049e-13 9.9999833e-01 3.8989449e-20 9.5249530e-17 1.7040807e-06], sum to 1.0000
[2019-03-27 13:51:24,505] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5795
[2019-03-27 13:51:24,515] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1803405.729477914 W.
[2019-03-27 13:51:24,521] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.7, 89.0, 1.0, 2.0, 0.4299781650222115, 1.0, 2.0, 0.4299781650222115, 1.0, 2.0, 0.7278582770961838, 6.9112, 6.9112, 170.5573041426782, 1803405.729477914, 1803405.729477914, 367179.5605323915], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2458800.0000, 
sim time next is 2459400.0000, 
raw observation next is [25.73333333333333, 89.0, 1.0, 2.0, 0.6113900390954723, 0.0, 1.0, 0.0, 1.0, 2.0, 1.027627013265348, 6.9112, 6.9112, 168.9129205615612, 1709458.21629816, 1709458.21629816, 367334.1231520423], 
processed observation next is [1.0, 0.4782608695652174, 0.41864139020537117, 0.89, 1.0, 1.0, 0.5317952278258702, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.033691479591888, 0.0, 0.0, 0.8294397686268945, 0.47484950452726665, 0.47484950452726665, 0.5482598853015557], 
reward next is 0.4517, 
noisyNet noise sample is [array([-0.5526134], dtype=float32), 1.3959621]. 
=============================================
[2019-03-27 13:51:26,791] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1705402e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.8213434e-24], sum to 1.0000
[2019-03-27 13:51:26,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0683
[2019-03-27 13:51:26,810] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.41666666666667, 89.66666666666667, 1.0, 2.0, 0.5473637432064795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764879.6165503991, 764879.6165503991, 191350.2349033937], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2488200.0000, 
sim time next is 2488800.0000, 
raw observation next is [27.33333333333334, 90.33333333333334, 1.0, 2.0, 0.5500832803391397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768681.2406141062, 768681.2406141069, 191815.441126939], 
processed observation next is [1.0, 0.8260869565217391, 0.4944707740916275, 0.9033333333333334, 1.0, 1.0, 0.4579316630592044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2135225668372517, 0.2135225668372519, 0.28629170317453584], 
reward next is 0.7137, 
noisyNet noise sample is [array([0.17835762], dtype=float32), 0.112282835]. 
=============================================
[2019-03-27 13:51:36,259] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1004659e-30 1.0000000e+00 1.9414306e-38 0.0000000e+00 1.7480394e-22], sum to 1.0000
[2019-03-27 13:51:36,268] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9607
[2019-03-27 13:51:36,276] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3474518561810484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535240.9313666883, 535240.9313666878, 169426.6608317343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2771400.0000, 
sim time next is 2772000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3473481168878181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535081.3018466077, 535081.3018466084, 169413.6413674864], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.94, 1.0, 1.0, 0.213672429985323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14863369495739104, 0.14863369495739123, 0.25285618114550207], 
reward next is 0.7471, 
noisyNet noise sample is [array([0.81808287], dtype=float32), 0.27720603]. 
=============================================
[2019-03-27 13:51:36,292] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[69.80514 ]
 [69.807076]
 [69.96276 ]
 [70.06095 ]
 [70.09699 ]], R is [[70.38757324]
 [70.43082428]
 [70.47358704]
 [70.51583099]
 [70.55758667]].
[2019-03-27 13:51:46,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5646622e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6403550e-27], sum to 1.0000
[2019-03-27 13:51:46,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6276
[2019-03-27 13:51:46,835] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3859984006418039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581386.5162420993, 581386.5162421, 172999.6175667803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2728800.0000, 
sim time next is 2729400.0000, 
raw observation next is [22.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3877364187725297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 583346.5155750271, 583346.5155750271, 173156.6542452343], 
processed observation next is [0.0, 0.6086956521739131, 0.24960505529225935, 0.9900000000000001, 1.0, 1.0, 0.2623330346656984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16204069877084085, 0.16204069877084085, 0.25844276753020046], 
reward next is 0.7416, 
noisyNet noise sample is [array([0.18544605], dtype=float32), 1.6598362]. 
=============================================
[2019-03-27 13:51:51,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3829237e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6140268e-26], sum to 1.0000
[2019-03-27 13:51:51,295] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5725
[2019-03-27 13:51:51,302] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.6897703147367458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1062221.276120694, 1062221.276120694, 230933.8717244141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2909400.0000, 
sim time next is 2910000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6798689586545278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1046995.521519257, 1046995.521519256, 228620.2719340856], 
processed observation next is [1.0, 0.6956521739130435, 0.2417061611374408, 0.94, 1.0, 1.0, 0.614299950186178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2908320893109047, 0.2908320893109045, 0.3412242864687845], 
reward next is 0.6588, 
noisyNet noise sample is [array([0.6851232], dtype=float32), -1.4665059]. 
=============================================
[2019-03-27 13:51:51,319] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.75129]
 [76.77524]
 [76.48333]
 [76.38897]
 [76.36358]], R is [[76.74433136]
 [76.63220978]
 [76.52848816]
 [76.41980743]
 [76.31025696]].
[2019-03-27 13:51:52,281] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5252372e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.8244972e-31], sum to 1.0000
[2019-03-27 13:51:52,288] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7730
[2019-03-27 13:51:52,292] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3368404410383452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523209.2688497411, 523209.2688497411, 168588.2257081255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2963400.0000, 
sim time next is 2964000.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3355473044568527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521168.9336773183, 521168.9336773177, 168425.7389128151], 
processed observation next is [1.0, 0.30434782608695654, 0.19431279620853087, 1.0, 1.0, 1.0, 0.19945458368295507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14476914824369952, 0.14476914824369935, 0.2513816998698733], 
reward next is 0.7486, 
noisyNet noise sample is [array([-0.56986237], dtype=float32), -1.1362756]. 
=============================================
[2019-03-27 13:51:52,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.826416]
 [77.738525]
 [77.59077 ]
 [77.56088 ]
 [77.52301 ]], R is [[77.85640717]
 [77.82621765]
 [77.79716492]
 [77.76930237]
 [77.74227905]].
[2019-03-27 13:51:52,528] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 13:51:52,532] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:51:52,532] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:51:52,534] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:51:52,536] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:51:52,534] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:51:52,536] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:51:52,537] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:51:52,542] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:51:52,542] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:51:52,543] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:51:52,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run55
[2019-03-27 13:51:52,588] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run55
[2019-03-27 13:51:52,609] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run55
[2019-03-27 13:51:52,629] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run55
[2019-03-27 13:51:52,647] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run55
[2019-03-27 13:52:07,676] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05977271], dtype=float32), 0.063539624]
[2019-03-27 13:52:07,677] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.76831286, 77.05925774, 1.0, 2.0, 0.2559658699413042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 421058.6032006906, 421058.6032006906, 161498.147970799]
[2019-03-27 13:52:07,679] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:52:07,681] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7247819e-31], sampled 0.0769411289404176
[2019-03-27 13:52:15,008] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05977271], dtype=float32), 0.063539624]
[2019-03-27 13:52:15,013] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.9, 49.5, 1.0, 2.0, 0.2944043799572356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481875.5915226829, 481875.5915226835, 165611.5393560499]
[2019-03-27 13:52:15,014] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:52:15,016] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.994381e-38 1.000000e+00 0.000000e+00 0.000000e+00 5.775035e-29], sampled 0.7465510355398797
[2019-03-27 13:52:50,252] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05977271], dtype=float32), 0.063539624]
[2019-03-27 13:52:50,254] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.23946293, 70.08486431, 1.0, 2.0, 0.4998303032723063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565103561, 698435.1132249398, 698435.1132249391, 183570.084733775]
[2019-03-27 13:52:50,258] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:52:50,261] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.1463669e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6536755e-24], sampled 0.21736623223632312
[2019-03-27 13:53:12,191] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05977271], dtype=float32), 0.063539624]
[2019-03-27 13:53:12,193] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.83333333333334, 80.66666666666667, 1.0, 2.0, 0.6673898951587246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 932676.3819837064, 932676.381983707, 213962.9556299086]
[2019-03-27 13:53:12,193] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:53:12,194] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1021463e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6290321e-26], sampled 0.7505394631392573
[2019-03-27 13:53:29,022] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05977271], dtype=float32), 0.063539624]
[2019-03-27 13:53:29,022] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.6, 91.66666666666666, 1.0, 2.0, 0.5143044615738008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718667.3400806441, 718667.3400806441, 185866.6458937382]
[2019-03-27 13:53:29,023] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 13:53:29,025] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.9235311e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1277967e-26], sampled 0.395764850341065
[2019-03-27 13:53:33,160] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05977271], dtype=float32), 0.063539624]
[2019-03-27 13:53:33,161] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.14472472, 75.50284358, 1.0, 2.0, 0.8814483947699223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1240403.262371025, 1240403.262371025, 266120.928931339]
[2019-03-27 13:53:33,161] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:53:33,165] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.6090394e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5358843e-23], sampled 0.9918462669258463
[2019-03-27 13:54:00,528] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.8362 3007797477.9977 1766.0000
[2019-03-27 13:54:00,551] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 13:54:00,814] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.9007 2779180218.3546 935.0000
[2019-03-27 13:54:00,936] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1340 3163966899.9238 1778.0000
[2019-03-27 13:54:00,946] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.6972 2842565415.8015 1132.0000
[2019-03-27 13:54:01,964] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1350000, evaluation results [1350000.0, 7884.134034255627, 3163966899.923814, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8660.900744119015, 2779180218.3546224, 935.0, 7996.836170632875, 3007797477.997744, 1766.0, 8495.697223249183, 2842565415.8014503, 1132.0]
[2019-03-27 13:54:05,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3346980e-30 1.0000000e+00 1.5398107e-38 0.0000000e+00 4.0056547e-22], sum to 1.0000
[2019-03-27 13:54:06,005] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6211
[2019-03-27 13:54:06,012] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.8321429260622062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1163044.580593564, 1163044.580593563, 251949.5857546731], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3157800.0000, 
sim time next is 3158400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 1.032079575851926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1442675.907919105, 1442675.907919105, 308848.5206368698], 
processed observation next is [1.0, 0.5652173913043478, 0.4312796208530806, 0.84, 1.0, 1.0, 1.0386500913878625, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.4007433077553069, 0.4007433077553069, 0.46096794124905943], 
reward next is 0.5390, 
noisyNet noise sample is [array([-0.16208175], dtype=float32), 0.8993519]. 
=============================================
[2019-03-27 13:54:07,838] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.3733285e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7443704e-23], sum to 1.0000
[2019-03-27 13:54:07,848] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2114
[2019-03-27 13:54:07,857] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 89.33333333333333, 1.0, 2.0, 0.7267118807483757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1109168.379321267, 1109168.379321268, 238730.1906129474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2893200.0000, 
sim time next is 2893800.0000, 
raw observation next is [22.95, 89.16666666666667, 1.0, 2.0, 0.7309011927871052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1114585.836322638, 1114585.836322639, 239648.4685344286], 
processed observation next is [1.0, 0.4782608695652174, 0.28672985781990523, 0.8916666666666667, 1.0, 1.0, 0.6757845696230182, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30960717675628835, 0.30960717675628857, 0.35768428139466957], 
reward next is 0.6423, 
noisyNet noise sample is [array([-0.05937864], dtype=float32), -1.5485799]. 
=============================================
[2019-03-27 13:54:09,495] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0500277e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3252310e-28], sum to 1.0000
[2019-03-27 13:54:09,507] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9979
[2019-03-27 13:54:09,513] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.4068317331916693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 603477.5571900024, 603477.557190003, 174755.8674228257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3095400.0000, 
sim time next is 3096000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.4033197702163072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 600927.3924241631, 600927.3924241625, 174596.4139606935], 
processed observation next is [1.0, 0.8695652173913043, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2811081568871171, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1669242756733786, 0.16692427567337847, 0.26059166262790073], 
reward next is 0.7394, 
noisyNet noise sample is [array([1.1582718], dtype=float32), 1.8116466]. 
=============================================
[2019-03-27 13:54:09,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.541405]
 [83.49826 ]
 [83.44381 ]
 [83.342186]
 [83.2488  ]], R is [[83.65859222]
 [83.56118011]
 [83.46430969]
 [83.36761475]
 [83.2713089 ]].
[2019-03-27 13:54:17,337] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4000877e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.5062373e-25], sum to 1.0000
[2019-03-27 13:54:17,345] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0803
[2019-03-27 13:54:17,351] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 95.0, 1.0, 2.0, 0.3061661194374781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484879.9304059562, 484879.9304059562, 165925.5327052435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3037800.0000, 
sim time next is 3038400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3072393866141007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 486032.5584057404, 486032.558405741, 165997.7728645652], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.94, 1.0, 1.0, 0.16534865857120568, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13500904400159455, 0.13500904400159472, 0.24775786994711224], 
reward next is 0.7522, 
noisyNet noise sample is [array([-1.3274345], dtype=float32), 1.4517338]. 
=============================================
[2019-03-27 13:54:36,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7570599e-18 1.0000000e+00 1.2624356e-24 1.5710914e-23 2.6133951e-10], sum to 1.0000
[2019-03-27 13:54:36,428] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7397
[2019-03-27 13:54:36,441] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2068317.66410907 W.
[2019-03-27 13:54:36,452] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.7396185807149555, 1.0, 1.0, 0.7396185807149555, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2068317.66410907, 2068317.66410907, 391537.1160899383], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3486600.0000, 
sim time next is 3487200.0000, 
raw observation next is [29.33333333333334, 68.66666666666667, 1.0, 2.0, 0.704598951313661, 1.0, 2.0, 0.704598951313661, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1970296.43106178, 1970296.43106178, 376091.6652362996], 
processed observation next is [1.0, 0.34782608695652173, 0.5892575039494474, 0.6866666666666668, 1.0, 1.0, 0.6440951220646518, 1.0, 1.0, 0.6440951220646518, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5473045641838278, 0.5473045641838278, 0.5613308436362681], 
reward next is 0.4387, 
noisyNet noise sample is [array([-1.5662189], dtype=float32), -1.236066]. 
=============================================
[2019-03-27 13:54:46,021] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.5115796e-27 1.0000000e+00 2.5830740e-34 2.7027729e-37 7.6932454e-18], sum to 1.0000
[2019-03-27 13:54:46,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9189
[2019-03-27 13:54:46,042] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5516599039562212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770885.1992047711, 770885.1992047705, 192085.9950600215], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3531600.0000, 
sim time next is 3532200.0000, 
raw observation next is [28.83333333333334, 79.00000000000001, 1.0, 2.0, 0.5485434540216025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766528.7252994437, 766528.7252994437, 191551.2232536246], 
processed observation next is [1.0, 0.9130434782608695, 0.5655608214849924, 0.7900000000000001, 1.0, 1.0, 0.45607645062843677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21292464591651214, 0.21292464591651214, 0.2858973481397382], 
reward next is 0.7141, 
noisyNet noise sample is [array([0.60680693], dtype=float32), -1.0372727]. 
=============================================
[2019-03-27 13:54:55,755] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1944488e-12 9.9941909e-01 8.3196363e-19 2.2826904e-14 5.8095157e-04], sum to 1.0000
[2019-03-27 13:54:55,764] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3732
[2019-03-27 13:54:55,776] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2246397.908507392 W.
[2019-03-27 13:54:55,783] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 68.0, 1.0, 2.0, 0.965274196480853, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.997648232576195, 6.9112, 168.9124424173252, 2246397.908507392, 2185068.743657547, 453120.036332368], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [31.0, 68.66666666666667, 1.0, 2.0, 0.9897301520539943, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.998394774654183, 6.9112, 168.9123678642249, 2280627.594215718, 2218768.835636371, 460450.966546005], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.6866666666666668, 1.0, 1.0, 0.9876266892216798, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00871947746541828, 0.0, 0.8294370546299473, 0.6335076650599217, 0.6163246765656586, 0.6872402485761269], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.101783], dtype=float32), -1.1341175]. 
=============================================
[2019-03-27 13:55:00,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5368424e-29 1.0000000e+00 1.5139119e-35 0.0000000e+00 8.9490273e-20], sum to 1.0000
[2019-03-27 13:55:00,222] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1475
[2019-03-27 13:55:00,228] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7736879900000767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1081303.439445454, 1081303.439445454, 237551.2278678445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3641400.0000, 
sim time next is 3642000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7243636465149337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1012335.063248648, 1012335.063248648, 226188.5617082594], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.6679080078493177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28120418423573557, 0.28120418423573557, 0.3375948682212827], 
reward next is 0.6624, 
noisyNet noise sample is [array([0.6471469], dtype=float32), 0.3386328]. 
=============================================
[2019-03-27 13:55:00,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[61.543655]
 [61.719894]
 [61.835205]
 [61.526817]
 [61.34474 ]], R is [[61.76910019]
 [61.79685593]
 [61.82919312]
 [61.85642624]
 [61.87138367]].
[2019-03-27 13:55:02,813] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8639400e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4914837e-25], sum to 1.0000
[2019-03-27 13:55:02,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8408
[2019-03-27 13:55:02,830] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.25, 62.0, 1.0, 2.0, 0.6165147167353393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 861549.5588072789, 861549.5588072783, 203863.8159743474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3846600.0000, 
sim time next is 3847200.0000, 
raw observation next is [34.33333333333334, 61.66666666666667, 1.0, 2.0, 0.6165615271090111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861615.000578552, 861615.000578552, 203872.7903916272], 
processed observation next is [0.0, 0.5217391304347826, 0.8262243285939973, 0.6166666666666667, 1.0, 1.0, 0.538025936275917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2393375001607089, 0.2393375001607089, 0.30428774685317495], 
reward next is 0.6957, 
noisyNet noise sample is [array([-0.74781734], dtype=float32), 1.6598979]. 
=============================================
[2019-03-27 13:55:08,036] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 13:55:08,038] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:55:08,038] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:55:08,039] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:55:08,040] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:55:08,041] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:55:08,043] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:55:08,044] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:55:08,044] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:55:08,046] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:55:08,046] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:55:08,069] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run56
[2019-03-27 13:55:08,091] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run56
[2019-03-27 13:55:08,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run56
[2019-03-27 13:55:08,129] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run56
[2019-03-27 13:55:08,146] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run56
[2019-03-27 13:55:13,959] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05919428], dtype=float32), 0.060915638]
[2019-03-27 13:55:13,961] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.53333333333333, 47.33333333333333, 1.0, 2.0, 0.2276353711234594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 378512.9448147927, 378512.9448147933, 158440.3449063933]
[2019-03-27 13:55:13,965] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:55:13,966] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6342443e-32], sampled 0.4032268262765937
[2019-03-27 13:55:25,228] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05919428], dtype=float32), 0.060915638]
[2019-03-27 13:55:25,231] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.02440970666667, 94.56873660333335, 1.0, 2.0, 0.2700190544560364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 444795.2371382, 444795.2371382007, 162935.960367523]
[2019-03-27 13:55:25,231] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:55:25,236] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4485281e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9767020e-31], sampled 0.262838518727771
[2019-03-27 13:56:06,782] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05919428], dtype=float32), 0.060915638]
[2019-03-27 13:56:06,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.53500323333333, 70.16946605666666, 1.0, 2.0, 0.4858671768219588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678917.5806337581, 678917.5806337575, 181411.2553876721]
[2019-03-27 13:56:06,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 13:56:06,790] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7358600e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.0506634e-29], sampled 0.46911985442734516
[2019-03-27 13:56:19,235] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05919428], dtype=float32), 0.060915638]
[2019-03-27 13:56:19,236] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.23333333333333, 62.33333333333334, 1.0, 2.0, 0.5647424163638277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789173.3901388351, 789173.3901388351, 194360.979104057]
[2019-03-27 13:56:19,237] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:56:19,239] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3126128e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0471094e-27], sampled 0.2328390543792639
[2019-03-27 13:56:49,421] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05919428], dtype=float32), 0.060915638]
[2019-03-27 13:56:49,423] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.81576256, 72.86749165, 1.0, 2.0, 0.5609492334473352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783870.8247518877, 783870.824751887, 193695.2432589452]
[2019-03-27 13:56:49,427] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 13:56:49,432] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.893753e-37 1.000000e+00 0.000000e+00 0.000000e+00 7.704064e-30], sampled 0.707572924970417
[2019-03-27 13:57:15,453] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9042 3164046086.8482 1776.0000
[2019-03-27 13:57:16,081] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2178 3007719775.7786 1766.0000
[2019-03-27 13:57:16,209] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5752 2842445703.7631 1131.0000
[2019-03-27 13:57:16,243] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4033 2927329517.0614 1341.0000
[2019-03-27 13:57:16,333] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.9007 2779180218.3546 935.0000
[2019-03-27 13:57:17,351] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1375000, evaluation results [1375000.0, 7884.904175899575, 3164046086.8482485, 1776.0, 8254.403345880972, 2927329517.0613775, 1341.0, 8660.900744119015, 2779180218.3546224, 935.0, 7998.217808978373, 3007719775.7785764, 1766.0, 8497.575230404103, 2842445703.7631187, 1131.0]
[2019-03-27 13:57:18,982] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0602531e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4851436e-30], sum to 1.0000
[2019-03-27 13:57:18,993] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0354
[2019-03-27 13:57:18,999] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5352077536567074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747887.0006016005, 747887.0006016005, 189296.364715986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3815400.0000, 
sim time next is 3816000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5369645573922603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750342.7847877458, 750342.7847877452, 189590.289968943], 
processed observation next is [0.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4421259727617594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20842855132992938, 0.2084285513299292, 0.28297058204319847], 
reward next is 0.7170, 
noisyNet noise sample is [array([-0.18364063], dtype=float32), 1.9462974]. 
=============================================
[2019-03-27 13:57:19,013] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.07901]
 [68.96578]
 [68.96183]
 [68.83987]
 [68.66841]], R is [[69.24905396]
 [69.27403259]
 [69.29885864]
 [69.32344055]
 [69.34777069]].
[2019-03-27 13:57:30,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.4164421e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5693397e-27], sum to 1.0000
[2019-03-27 13:57:30,991] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7951
[2019-03-27 13:57:30,996] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 63.0, 1.0, 2.0, 0.6139418457987807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857952.643682573, 857952.643682573, 203372.9679471235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3949200.0000, 
sim time next is 3949800.0000, 
raw observation next is [34.0, 62.5, 1.0, 2.0, 0.626238150094815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 875143.1856992028, 875143.1856992021, 205735.8527295734], 
processed observation next is [0.0, 0.7391304347826086, 0.8104265402843602, 0.625, 1.0, 1.0, 0.5496845181865241, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24309532936088968, 0.24309532936088948, 0.30706843690981106], 
reward next is 0.6929, 
noisyNet noise sample is [array([0.71220845], dtype=float32), -0.26650167]. 
=============================================
[2019-03-27 13:57:34,981] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.2245490e-06 2.7409431e-03 2.3113155e-13 1.8243317e-06 9.9725097e-01], sum to 1.0000
[2019-03-27 13:57:34,995] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5246
[2019-03-27 13:57:35,001] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5967994098180212, 1.0, 2.0, 0.5967994098180212, 1.0, 2.0, 1.03, 6.91307319885579, 6.9112, 170.5573041426782, 2503853.152711113, 2502511.304173852, 487044.3224940763], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4012200.0000, 
sim time next is 4012800.0000, 
raw observation next is [30.33333333333333, 68.66666666666667, 1.0, 2.0, 0.6059313962340803, 1.0, 2.0, 0.6059313962340803, 1.0, 2.0, 1.03, 6.932440782459663, 6.9112, 170.5573041426782, 2542205.090055557, 2526989.453559911, 490299.6478378236], 
processed observation next is [1.0, 0.43478260869565216, 0.6366508688783569, 0.6866666666666668, 1.0, 1.0, 0.5252185496796148, 1.0, 1.0, 0.5252185496796148, 1.0, 1.0, 1.0365853658536586, 0.0021240782459662987, 0.0, 0.8375144448122397, 0.706168080570988, 0.701941514877753, 0.7317905191609307], 
reward next is 0.1620, 
noisyNet noise sample is [array([0.10369427], dtype=float32), 0.33628172]. 
=============================================
[2019-03-27 13:57:39,601] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2254436e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.9272854e-25], sum to 1.0000
[2019-03-27 13:57:39,612] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3279
[2019-03-27 13:57:39,618] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333333, 82.5, 1.0, 2.0, 0.5867467770759431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 819934.2431858237, 819934.2431858242, 198302.7192295657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4135800.0000, 
sim time next is 4136400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5822518676951293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813650.5414273574, 813650.5414273574, 197485.9854785991], 
processed observation next is [1.0, 0.9130434782608695, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49668899722304727, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22601403928537706, 0.22601403928537706, 0.2947552022068643], 
reward next is 0.7052, 
noisyNet noise sample is [array([1.4229814], dtype=float32), 0.1296222]. 
=============================================
[2019-03-27 13:57:47,405] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0627649e-06 1.4521857e-01 1.3477484e-13 1.7976434e-07 8.5478026e-01], sum to 1.0000
[2019-03-27 13:57:47,417] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3356
[2019-03-27 13:57:47,425] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.66666666666666, 61.83333333333333, 1.0, 2.0, 0.7081509532369177, 1.0, 2.0, 0.6746655161327215, 1.0, 2.0, 1.03, 7.005098374859854, 6.9112, 170.5573041426782, 2830908.071284234, 2763644.845830438, 523917.2725798198], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4182600.0000, 
sim time next is 4183200.0000, 
raw observation next is [35.0, 60.0, 1.0, 2.0, 0.7118649102531702, 1.0, 2.0, 0.6765224946408478, 1.0, 2.0, 1.03, 7.005098667711756, 6.9112, 170.5573041426782, 2838708.838325421, 2771445.403089894, 525104.078865546], 
processed observation next is [1.0, 0.43478260869565216, 0.8578199052132701, 0.6, 1.0, 1.0, 0.6528492894616509, 1.0, 1.0, 0.6102680658323467, 1.0, 1.0, 1.0365853658536586, 0.009389866771175637, 0.0, 0.8375144448122397, 0.7885302328681725, 0.7698459453027483, 0.783737431142606], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7248921], dtype=float32), 0.051789343]. 
=============================================
[2019-03-27 13:57:48,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7094164e-08 7.7887988e-01 3.7402851e-17 3.9724279e-12 2.2112007e-01], sum to 1.0000
[2019-03-27 13:57:48,937] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5939
[2019-03-27 13:57:48,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3077540.293341603 W.
[2019-03-27 13:57:48,957] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.66666666666667, 46.0, 1.0, 2.0, 0.8255617031401917, 1.0, 2.0, 0.7333708910843585, 1.0, 2.0, 1.03, 7.005107634668881, 6.9112, 170.5573041426782, 3077540.293341603, 3010270.434709936, 563920.8410730404], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4292400.0000, 
sim time next is 4293000.0000, 
raw observation next is [37.5, 46.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.3098405551293, 6.9112, 170.5573041426782, 3195224.809364693, 2909662.364064557, 551525.7896615312], 
processed observation next is [1.0, 0.6956521739130435, 0.976303317535545, 0.465, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.03986405551293002, 0.0, 0.8375144448122397, 0.8875624470457479, 0.8082395455734881, 0.8231728203903451], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.22472234], dtype=float32), 0.08999914]. 
=============================================
[2019-03-27 13:57:48,968] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[54.036823]
 [53.89436 ]
 [53.48955 ]
 [52.707478]
 [51.432095]], R is [[52.94486237]
 [52.4154129 ]
 [51.89125824]
 [51.37234497]
 [50.8586235 ]].
[2019-03-27 13:57:49,761] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2282833e-13 9.9999821e-01 3.0964834e-21 1.9006162e-18 1.7341329e-06], sum to 1.0000
[2019-03-27 13:57:49,772] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9235
[2019-03-27 13:57:49,784] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2862509.708254311 W.
[2019-03-27 13:57:49,789] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.66666666666666, 55.0, 1.0, 2.0, 1.023282357157434, 1.0, 2.0, 1.023282357157434, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2862509.708254311, 2862509.708254311, 543232.2628756819], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4279200.0000, 
sim time next is 4279800.0000, 
raw observation next is [37.83333333333334, 54.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.007047764039147, 6.9112, 170.5573041426782, 2978069.38324379, 2909409.731024785, 553161.9326883622], 
processed observation next is [1.0, 0.5217391304347826, 0.9921011058451821, 0.545, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.009584776403914664, 0.0, 0.8375144448122397, 0.8272414953454972, 0.8081693697291069, 0.8256148249080033], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2803188], dtype=float32), -1.7129431]. 
=============================================
[2019-03-27 13:57:52,629] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.276276e-31], sum to 1.0000
[2019-03-27 13:57:52,645] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0395
[2019-03-27 13:57:52,652] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 78.16666666666667, 1.0, 2.0, 0.5108555554023693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713846.3648783503, 713846.3648783503, 185314.5741124338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4528200.0000, 
sim time next is 4528800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5150205109398079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719668.2561756148, 719668.2561756148, 185983.1662604543], 
processed observation next is [0.0, 0.43478260869565216, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4156873625780818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19990784893767077, 0.19990784893767077, 0.2775868153141109], 
reward next is 0.7224, 
noisyNet noise sample is [array([-0.16633543], dtype=float32), -0.16944847]. 
=============================================
[2019-03-27 13:57:54,280] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.855322e-38 1.000000e+00 0.000000e+00 0.000000e+00 9.621172e-29], sum to 1.0000
[2019-03-27 13:57:54,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2191
[2019-03-27 13:57:54,302] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5247837973173619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733315.7759126496, 733315.7759126503, 187570.242284233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4392000.0000, 
sim time next is 4392600.0000, 
raw observation next is [31.0, 65.66666666666667, 1.0, 2.0, 0.5321880424947747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 743665.8485963726, 743665.8485963732, 188793.6441412979], 
processed observation next is [1.0, 0.8695652173913043, 0.6682464454976303, 0.6566666666666667, 1.0, 1.0, 0.4363711355358731, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20657384683232574, 0.2065738468323259, 0.28178155841984764], 
reward next is 0.7182, 
noisyNet noise sample is [array([-0.9872492], dtype=float32), 1.4798017]. 
=============================================
[2019-03-27 13:57:58,051] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.3221313e-23 1.0000000e+00 9.0807832e-32 4.8197417e-36 6.2670789e-18], sum to 1.0000
[2019-03-27 13:57:58,065] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9056
[2019-03-27 13:57:58,079] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1661994.054681021 W.
[2019-03-27 13:57:58,086] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.5944276982483069, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.91383736512031, 6.9112, 168.9125222770994, 1661994.054681021, 1660123.021903545, 363558.5591765073], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4336200.0000, 
sim time next is 4336800.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.3391316897633156, 1.0, 1.0, 0.3391316897633156, 1.0, 2.0, 0.588959592555283, 6.911199999999999, 6.9112, 170.5573041426782, 1422126.471924457, 1422126.471924458, 321755.2566304881], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.89, 1.0, 1.0, 0.20377312019676577, 1.0, 0.5, 0.20377312019676577, 1.0, 1.0, 0.4987312104332719, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.39503513109012695, 0.3950351310901272, 0.48023172631416133], 
reward next is 0.5198, 
noisyNet noise sample is [array([-0.5852951], dtype=float32), 0.4921421]. 
=============================================
[2019-03-27 13:57:58,440] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6139978e-22 1.0000000e+00 2.7627954e-30 1.9339164e-35 2.8399879e-16], sum to 1.0000
[2019-03-27 13:57:58,448] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2359
[2019-03-27 13:57:58,460] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1785460.236575166 W.
[2019-03-27 13:57:58,468] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.37845113368264, 6.9112, 168.9100189500361, 1785460.236575166, 1453981.965627171, 311355.8631751942], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4341600.0000, 
sim time next is 4342200.0000, 
raw observation next is [29.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5265749847248506, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9144866074585863, 6.911199999999999, 6.9112, 168.9125760087979, 1472149.245811736, 1472149.245811737, 322952.6521363257], 
processed observation next is [1.0, 0.2608695652173913, 0.581358609794629, 0.8816666666666667, 1.0, 1.0, 0.42960841533114535, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.8957153749494954, -8.881784197001253e-17, 0.0, 0.8294380767150898, 0.40893034605881556, 0.4089303460588158, 0.4820188837855608], 
reward next is 0.5180, 
noisyNet noise sample is [array([0.27493063], dtype=float32), -1.1425834]. 
=============================================
[2019-03-27 13:57:58,651] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8994295e-22 1.0000000e+00 1.8820227e-30 7.8259328e-35 2.4591177e-16], sum to 1.0000
[2019-03-27 13:57:58,660] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7052
[2019-03-27 13:57:58,668] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.9721585919427448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1358862.711680835, 1358862.711680835, 290563.1431599421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4338000.0000, 
sim time next is 4338600.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.754425147401236, 6.9112, 168.9086912352809, 2052362.533200363, 1454164.697730641, 311356.0814260076], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.89, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.08432251474012356, 0.0, 0.8294190006986644, 0.5701007036667675, 0.40393463825851145, 0.46471056929254867], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0583966], dtype=float32), -0.9697431]. 
=============================================
[2019-03-27 13:58:14,688] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.6132056e-30 1.0000000e+00 1.3780991e-38 0.0000000e+00 1.9428561e-20], sum to 1.0000
[2019-03-27 13:58:14,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1924
[2019-03-27 13:58:14,708] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.08333333333333, 78.66666666666667, 1.0, 2.0, 0.4882767536349127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 682285.6398334254, 682285.6398334247, 181779.3175101378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4837800.0000, 
sim time next is 4838400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4874479715313773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681127.1832627802, 681127.1832627802, 181652.4226032182], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.79, 1.0, 1.0, 0.38246743557997265, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18920199535077228, 0.18920199535077228, 0.27112301881077344], 
reward next is 0.7289, 
noisyNet noise sample is [array([1.6485734], dtype=float32), 0.27079]. 
=============================================
[2019-03-27 13:58:19,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9335378e-09 1.4192497e-03 2.3925733e-17 2.0364339e-10 9.9858081e-01], sum to 1.0000
[2019-03-27 13:58:19,157] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0570
[2019-03-27 13:58:19,165] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.33333333333334, 62.00000000000001, 1.0, 2.0, 0.8184539306564175, 1.0, 1.0, 0.7298170048424714, 1.0, 2.0, 1.03, 7.005107073997742, 6.9112, 170.5573041426782, 3062608.369911549, 2995338.912911425, 561353.6626657997], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4627200.0000, 
sim time next is 4627800.0000, 
raw observation next is [34.5, 61.5, 1.0, 2.0, 0.7847728418023227, 1.0, 2.0, 0.7129764604154241, 1.0, 2.0, 1.03, 7.005104417368003, 6.9112, 170.5573041426782, 2991853.968368638, 2924586.41442046, 549441.9966125308], 
processed observation next is [1.0, 0.5652173913043478, 0.8341232227488152, 0.615, 1.0, 1.0, 0.7406901708461718, 1.0, 1.0, 0.6541885065246072, 1.0, 1.0, 1.0365853658536586, 0.009390441736800259, 0.0, 0.8375144448122397, 0.8310705467690661, 0.8123851151167945, 0.82006268151124], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.15478627], dtype=float32), -0.1928634]. 
=============================================
[2019-03-27 13:58:23,524] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 13:58:23,526] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 13:58:23,527] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:58:23,528] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 13:58:23,528] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:58:23,528] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 13:58:23,531] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 13:58:23,530] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 13:58:23,534] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:58:23,535] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:58:23,533] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 13:58:23,569] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run57
[2019-03-27 13:58:23,569] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run57
[2019-03-27 13:58:23,608] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run57
[2019-03-27 13:58:23,608] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run57
[2019-03-27 13:58:23,656] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run57
[2019-03-27 13:58:26,505] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06147877], dtype=float32), 0.06325998]
[2019-03-27 13:58:26,506] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.73333333333333, 96.0, 1.0, 2.0, 0.8486763137904157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1266470.075735952, 1266470.075735952, 267414.8147117951]
[2019-03-27 13:58:26,507] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:58:26,511] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2981389e-25 1.0000000e+00 1.6988305e-33 1.4851208e-38 3.2375939e-17], sampled 0.715329603968955
[2019-03-27 13:58:37,007] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06147877], dtype=float32), 0.06325998]
[2019-03-27 13:58:37,010] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.05, 74.66666666666666, 1.0, 2.0, 0.4140498178784816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610615.9952189941, 610615.9952189948, 175323.085821854]
[2019-03-27 13:58:37,010] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 13:58:37,013] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.9950496e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7123078e-22], sampled 0.9606144300691308
[2019-03-27 13:59:52,696] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06147877], dtype=float32), 0.06325998]
[2019-03-27 13:59:52,697] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.35, 66.5, 1.0, 2.0, 0.8079531095266129, 1.0, 2.0, 0.7245665942775691, 1.0, 2.0, 1.03, 7.005106245702646, 6.9112, 170.5573041426782, 3040548.697148766, 2973279.833490112, 557595.5334344301]
[2019-03-27 13:59:52,698] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 13:59:52,701] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3831539e-09 2.6610558e-05 1.9390916e-16 8.7642951e-09 9.9997342e-01], sampled 0.2098492787649221
[2019-03-27 14:00:14,395] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06147877], dtype=float32), 0.06325998]
[2019-03-27 14:00:14,398] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.16666666666667, 80.33333333333334, 1.0, 2.0, 0.3308119758133395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 518109.4306393077, 518109.4306393083, 168301.0305760404]
[2019-03-27 14:00:14,401] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:00:14,404] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.3587795e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.5456097e-27], sampled 0.10049146104635753
[2019-03-27 14:00:31,234] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8514.7345 2841340706.9393 1107.0000
[2019-03-27 14:00:31,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8670.0664 2779163901.9558 912.0000
[2019-03-27 14:00:31,927] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.4747 2927960949.1447 1350.0000
[2019-03-27 14:00:31,929] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7911.5739 3162319930.9158 1804.0000
[2019-03-27 14:00:32,136] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8050.9983 3005133747.7588 1621.0000
[2019-03-27 14:00:33,153] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1400000, evaluation results [1400000.0, 7911.573938140582, 3162319930.9157677, 1804.0, 8255.4746864397, 2927960949.144687, 1350.0, 8670.066373618161, 2779163901.9557986, 912.0, 8050.998294145805, 3005133747.7588153, 1621.0, 8514.734483964376, 2841340706.9392614, 1107.0]
[2019-03-27 14:00:33,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0159634e-25 1.0000000e+00 3.3381190e-35 0.0000000e+00 1.3708932e-18], sum to 1.0000
[2019-03-27 14:00:33,559] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9480
[2019-03-27 14:00:33,567] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.6655394859862604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 930089.3055936471, 930089.3055936464, 213581.1324064603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4860000.0000, 
sim time next is 4860600.0000, 
raw observation next is [27.0, 84.83333333333333, 1.0, 2.0, 0.6571396177117861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 918345.4412641564, 918345.441264157, 211864.4852656514], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.8483333333333333, 1.0, 1.0, 0.5869152020623929, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2550959559067101, 0.25509595590671025, 0.31621564965022597], 
reward next is 0.6838, 
noisyNet noise sample is [array([0.00286945], dtype=float32), -2.3489666]. 
=============================================
[2019-03-27 14:00:40,141] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2930092e-11 9.9209565e-01 1.8893826e-19 2.9173443e-15 7.9043852e-03], sum to 1.0000
[2019-03-27 14:00:40,146] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0715
[2019-03-27 14:00:40,152] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1713467.877956364 W.
[2019-03-27 14:00:40,157] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.3, 65.0, 1.0, 2.0, 0.4085518060895899, 1.0, 1.0, 0.4085518060895899, 1.0, 2.0, 0.6950405416474361, 6.9112, 6.9112, 170.5573041426782, 1713467.877956364, 1713467.877956364, 355570.258101146], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4969800.0000, 
sim time next is 4970400.0000, 
raw observation next is [30.33333333333334, 65.0, 1.0, 2.0, 0.652548073689806, 1.0, 2.0, 0.652548073689806, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1824620.675767711, 1824620.675767711, 354466.6167029301], 
processed observation next is [1.0, 0.5217391304347826, 0.6366508688783573, 0.65, 1.0, 1.0, 0.5813832213130193, 1.0, 1.0, 0.5813832213130193, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5068390766021419, 0.5068390766021419, 0.5290546517954181], 
reward next is 0.4709, 
noisyNet noise sample is [array([0.267911], dtype=float32), -2.0507264]. 
=============================================
[2019-03-27 14:00:43,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0982192e-28 1.0000000e+00 4.2225755e-37 0.0000000e+00 1.6853657e-18], sum to 1.0000
[2019-03-27 14:00:43,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4206
[2019-03-27 14:00:43,907] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5082969055604767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710269.8284146391, 710269.8284146397, 184906.3555936836], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5004600.0000, 
sim time next is 5005200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5082536061360354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710209.303639924, 710209.3036399233, 184899.468040855], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.40753446522413905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19728036212220112, 0.19728036212220093, 0.2759693552848582], 
reward next is 0.7240, 
noisyNet noise sample is [array([-1.792411], dtype=float32), -0.33833176]. 
=============================================
[2019-03-27 14:00:45,487] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5065876e-20 1.0000000e+00 5.6134815e-28 6.3150370e-30 2.5406704e-12], sum to 1.0000
[2019-03-27 14:00:45,501] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5538
[2019-03-27 14:00:45,509] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1904861.754345987 W.
[2019-03-27 14:00:45,515] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.546645800660943, 6.9112, 168.9095570668036, 1904861.754345987, 1454063.705681666, 311348.321691749], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5193600.0000, 
sim time next is 5194200.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.3930068483624073, 1.0, 1.0, 0.3930068483624073, 1.0, 1.0, 0.6692740694397744, 6.9112, 6.9112, 170.5573041426782, 1648222.127327668, 1648222.127327668, 347214.5480382026], 
processed observation next is [1.0, 0.08695652173913043, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.26868294983422564, 1.0, 0.5, 0.26868294983422564, 1.0, 0.5, 0.5966756944387492, 0.0, 0.0, 0.8375144448122397, 0.45783947981324113, 0.45783947981324113, 0.5182306687137352], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0431026], dtype=float32), 0.8421518]. 
=============================================
[2019-03-27 14:00:55,414] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4687605e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5592564e-28], sum to 1.0000
[2019-03-27 14:00:55,425] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1910
[2019-03-27 14:00:55,431] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 77.33333333333333, 1.0, 2.0, 0.5201964524523757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726903.3764013159, 726903.3764013159, 186820.9851021372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5125200.0000, 
sim time next is 5125800.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5220508323514502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729495.5083918261, 729495.5083918266, 187123.1724870216], 
processed observation next is [0.0, 0.30434782608695654, 0.5576619273301741, 0.7566666666666667, 1.0, 1.0, 0.4241576293390966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20263764121995168, 0.20263764121995184, 0.27928831714480834], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.928589], dtype=float32), 0.45522732]. 
=============================================
[2019-03-27 14:01:30,348] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6458459e-11 9.9547750e-01 2.7032810e-18 2.3478835e-16 4.5225522e-03], sum to 1.0000
[2019-03-27 14:01:30,364] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8991
[2019-03-27 14:01:30,373] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2917824.926123546 W.
[2019-03-27 14:01:30,380] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.61666666666667, 48.83333333333334, 1.0, 2.0, 0.7495308891868521, 1.0, 2.0, 0.6953554841076885, 1.0, 2.0, 1.03, 7.005101637945574, 6.9112, 170.5573041426782, 2917824.926123546, 2850559.363188715, 537428.8282871025], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5500200.0000, 
sim time next is 5500800.0000, 
raw observation next is [35.5, 49.0, 1.0, 2.0, 1.020205603966002, 1.0, 2.0, 1.020205603966002, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2853893.028254834, 2853893.028254834, 541311.6549475522], 
processed observation next is [1.0, 0.6956521739130435, 0.8815165876777251, 0.49, 1.0, 1.0, 1.0243441011638577, 1.0, 1.0, 1.0243441011638577, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7927480634041205, 0.7927480634041205, 0.8079278432053019], 
reward next is 0.1921, 
noisyNet noise sample is [array([1.8700567], dtype=float32), -0.3494796]. 
=============================================
[2019-03-27 14:01:39,299] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 14:01:39,301] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:01:39,303] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:01:39,304] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:01:39,305] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:01:39,308] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:01:39,308] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:01:39,309] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:01:39,307] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:01:39,313] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:01:39,309] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:01:39,341] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run58
[2019-03-27 14:01:39,361] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run58
[2019-03-27 14:01:39,383] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run58
[2019-03-27 14:01:39,399] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run58
[2019-03-27 14:01:39,400] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run58
[2019-03-27 14:02:49,126] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06408616], dtype=float32), 0.06383436]
[2019-03-27 14:02:49,129] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [38.0, 50.0, 1.0, 2.0, 0.8889057652913162, 1.0, 2.0, 0.7650429221599205, 1.0, 2.0, 1.03, 7.005112631929873, 6.9112, 170.5573041426782, 3210620.627215636, 3143347.188842644, 587638.5156566929]
[2019-03-27 14:02:49,132] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:02:49,139] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2817976e-10 3.7081033e-02 6.2200309e-18 3.2028866e-13 9.6291894e-01], sampled 0.2878145546682548
[2019-03-27 14:03:26,884] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06408616], dtype=float32), 0.06383436]
[2019-03-27 14:03:26,884] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.0, 93.0, 1.0, 2.0, 0.4995668485091918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698066.8552069102, 698066.8552069102, 183527.1942010921]
[2019-03-27 14:03:26,887] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:03:26,890] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.230172e-33 1.000000e+00 0.000000e+00 0.000000e+00 3.720041e-25], sampled 0.9366407448132011
[2019-03-27 14:03:47,295] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8667.0282 2779277539.6188 918.0000
[2019-03-27 14:03:47,369] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8247.3379 2928025174.8368 1364.0000
[2019-03-27 14:03:47,371] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.9376 2842048231.6063 1139.0000
[2019-03-27 14:03:47,479] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8019.7133 3006597612.8135 1706.0000
[2019-03-27 14:03:47,547] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7903.4304 3162550820.0075 1784.0000
[2019-03-27 14:03:48,565] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1425000, evaluation results [1425000.0, 7903.430366731033, 3162550820.007478, 1784.0, 8247.337894238995, 2928025174.836844, 1364.0, 8667.028226482002, 2779277539.6187973, 918.0, 8019.713330957723, 3006597612.8134766, 1706.0, 8500.937622167035, 2842048231.6063085, 1139.0]
[2019-03-27 14:03:49,148] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5863836e-09 3.8656536e-02 3.9186468e-16 5.8922546e-12 9.6134347e-01], sum to 1.0000
[2019-03-27 14:03:49,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6766
[2019-03-27 14:03:49,162] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.6, 72.0, 1.0, 2.0, 0.5522728659927477, 1.0, 2.0, 0.5522728659927477, 1.0, 2.0, 0.9591153288017249, 6.9112, 6.9112, 170.5573041426782, 2316870.258435529, 2316870.258435529, 453176.8865112567], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5913000.0000, 
sim time next is 5913600.0000, 
raw observation next is [31.76666666666667, 71.33333333333333, 1.0, 2.0, 0.566895241172343, 1.0, 2.0, 0.566895241172343, 1.0, 2.0, 0.9845095587953163, 6.911199999999999, 6.9112, 170.5573041426782, 2378271.760066396, 2378271.760066396, 464488.3660931656], 
processed observation next is [1.0, 0.43478260869565216, 0.7045813586097948, 0.7133333333333333, 1.0, 1.0, 0.4781870375570398, 1.0, 1.0, 0.4781870375570398, 1.0, 1.0, 0.9811092180430687, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6606310444628878, 0.6606310444628878, 0.6932662180495008], 
reward next is 0.3067, 
noisyNet noise sample is [array([0.6084701], dtype=float32), 1.1301913]. 
=============================================
[2019-03-27 14:04:02,356] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3147885e-21 1.0000000e+00 1.8431103e-29 4.9755498e-34 5.0274482e-14], sum to 1.0000
[2019-03-27 14:04:02,365] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8261
[2019-03-27 14:04:02,370] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 92.0, 1.0, 2.0, 0.7062129601582325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 986956.7678764942, 986956.7678764935, 222192.2251188254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6148200.0000, 
sim time next is 6148800.0000, 
raw observation next is [26.6, 92.0, 1.0, 2.0, 0.6984058532447021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 976041.0550635345, 976041.0550635351, 220501.738111406], 
processed observation next is [1.0, 0.17391304347826086, 0.4597156398104266, 0.92, 1.0, 1.0, 0.6366335581261471, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27112251529542625, 0.2711225152954264, 0.3291070718080687], 
reward next is 0.6709, 
noisyNet noise sample is [array([-0.474089], dtype=float32), -0.891641]. 
=============================================
[2019-03-27 14:04:04,998] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.0844392e-23 1.0000000e+00 7.5207211e-30 1.0993739e-32 2.9002419e-12], sum to 1.0000
[2019-03-27 14:04:05,010] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8257
[2019-03-27 14:04:05,016] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 88.33333333333334, 1.0, 2.0, 0.5317020573634238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742986.5080006435, 742986.5080006435, 188712.1889039932], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5872200.0000, 
sim time next is 5872800.0000, 
raw observation next is [26.9, 88.66666666666667, 1.0, 2.0, 0.5308549561676368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741802.3776865877, 741802.3776865872, 188571.6623203371], 
processed observation next is [1.0, 1.0, 0.4739336492890995, 0.8866666666666667, 1.0, 1.0, 0.43476500743088764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20605621602405216, 0.206056216024052, 0.2814502422691599], 
reward next is 0.7185, 
noisyNet noise sample is [array([0.6204342], dtype=float32), -0.023740875]. 
=============================================
[2019-03-27 14:04:07,722] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5840777e-24 1.0000000e+00 2.2767165e-31 1.6925801e-35 1.0358977e-12], sum to 1.0000
[2019-03-27 14:04:07,733] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0917
[2019-03-27 14:04:07,738] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 89.83333333333333, 1.0, 2.0, 0.520294911051758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727041.005907795, 727041.0059077957, 186837.2215072147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6223800.0000, 
sim time next is 6224400.0000, 
raw observation next is [26.5, 90.0, 1.0, 2.0, 0.5200222195557138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726659.8264886823, 726659.8264886816, 186792.8575244056], 
processed observation next is [0.0, 0.043478260869565216, 0.4549763033175356, 0.9, 1.0, 1.0, 0.4217135175370046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20184995180241175, 0.20184995180241155, 0.27879530973791883], 
reward next is 0.7212, 
noisyNet noise sample is [array([1.1581975], dtype=float32), -0.13407601]. 
=============================================
[2019-03-27 14:04:08,926] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8690707e-18 1.0000000e+00 1.6688858e-25 4.6210306e-28 8.9670305e-10], sum to 1.0000
[2019-03-27 14:04:08,937] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3769
[2019-03-27 14:04:08,945] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.7133067839727418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 996875.2823695963, 996875.2823695963, 223742.9300283698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6064800.0000, 
sim time next is 6065400.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.6993744570760846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 977395.3280066288, 977395.3280066282, 220709.5056729922], 
processed observation next is [1.0, 0.17391304347826086, 0.4360189573459717, 0.93, 1.0, 1.0, 0.6378005506940778, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2714987022240635, 0.2714987022240634, 0.32941717264625703], 
reward next is 0.6706, 
noisyNet noise sample is [array([-2.0980256], dtype=float32), -0.64820504]. 
=============================================
[2019-03-27 14:04:22,773] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.6914546e-29 1.0000000e+00 9.7196595e-38 0.0000000e+00 1.0580593e-19], sum to 1.0000
[2019-03-27 14:04:22,781] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2683
[2019-03-27 14:04:22,786] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.63333333333333, 68.33333333333334, 1.0, 2.0, 0.5423444730103086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757863.2456525544, 757863.2456525537, 190496.8040392407], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6261600.0000, 
sim time next is 6262200.0000, 
raw observation next is [30.65, 68.0, 1.0, 2.0, 0.5401834855089805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754842.4443038256, 754842.444303825, 190131.8280194779], 
processed observation next is [0.0, 0.4782608695652174, 0.6516587677725119, 0.68, 1.0, 1.0, 0.44600419940841024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20967845675106267, 0.2096784567510625, 0.2837788477902655], 
reward next is 0.7162, 
noisyNet noise sample is [array([1.4190102], dtype=float32), -1.4294388]. 
=============================================
[2019-03-27 14:04:38,877] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2908520e-12 3.3922859e-08 3.9250218e-20 1.5367259e-13 1.0000000e+00], sum to 1.0000
[2019-03-27 14:04:38,885] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0440
[2019-03-27 14:04:38,891] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.75, 56.5, 1.0, 2.0, 0.4385644081241497, 1.0, 2.0, 0.4385644081241497, 1.0, 2.0, 0.7468761848954569, 6.911199999999999, 6.9112, 170.5573041426782, 1839448.888144799, 1839448.8881448, 372937.1400469553], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6532200.0000, 
sim time next is 6532800.0000, 
raw observation next is [31.63333333333333, 57.0, 1.0, 2.0, 0.4777642860218745, 1.0, 2.0, 0.4777642860218745, 1.0, 2.0, 0.8138579703890835, 6.911200000000001, 6.9112, 170.5573041426782, 2004016.753131374, 2004016.753131374, 397356.1056890357], 
processed observation next is [1.0, 0.6086956521739131, 0.6982622432859398, 0.57, 1.0, 1.0, 0.37080034460466804, 1.0, 1.0, 0.37080034460466804, 1.0, 1.0, 0.7729975248647358, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5566713203142706, 0.5566713203142706, 0.5930688144612474], 
reward next is 0.4069, 
noisyNet noise sample is [array([-0.7811108], dtype=float32), -0.1128031]. 
=============================================
[2019-03-27 14:04:51,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0823459e-25 1.0000000e+00 7.1749699e-33 0.0000000e+00 2.1516004e-16], sum to 1.0000
[2019-03-27 14:04:51,169] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3596
[2019-03-27 14:04:51,178] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 91.0, 1.0, 2.0, 0.6327125986573268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 884194.7391060111, 884194.7391060111, 206990.0732604964], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6577200.0000, 
sim time next is 6577800.0000, 
raw observation next is [25.98333333333333, 91.16666666666667, 1.0, 2.0, 0.7469134029996835, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1043864.984662786, 1043864.984662787, 231295.574925382], 
processed observation next is [1.0, 0.13043478260869565, 0.43048973143759867, 0.9116666666666667, 1.0, 1.0, 0.6950763891562451, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28996249573966276, 0.2899624957396631, 0.34521727600803287], 
reward next is 0.6548, 
noisyNet noise sample is [array([-0.1342975], dtype=float32), 1.9713569]. 
=============================================
[2019-03-27 14:04:54,691] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 14:04:54,695] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:04:54,696] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:04:54,698] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:04:54,698] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:04:54,699] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:04:54,700] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:04:54,702] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:04:54,703] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:04:54,700] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:04:54,704] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:04:54,730] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run59
[2019-03-27 14:04:54,748] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run59
[2019-03-27 14:04:54,750] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run59
[2019-03-27 14:04:54,796] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run59
[2019-03-27 14:04:54,823] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run59
[2019-03-27 14:05:14,619] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06615058], dtype=float32), 0.06847393]
[2019-03-27 14:05:14,621] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.06666666666667, 90.33333333333333, 1.0, 2.0, 0.4521071560793938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704014.5907130953, 704014.5907130947, 185005.6770500243]
[2019-03-27 14:05:14,621] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:05:14,626] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7901468e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5903488e-23], sampled 0.0402165804543233
[2019-03-27 14:05:21,069] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06615058], dtype=float32), 0.06847393]
[2019-03-27 14:05:21,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.01666666666667, 96.0, 1.0, 2.0, 0.475461973881675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 702676.0166899071, 702676.0166899076, 184636.998916795]
[2019-03-27 14:05:21,074] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:05:21,078] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.4287750e-26 1.0000000e+00 5.8225128e-34 2.9245898e-38 3.1091988e-15], sampled 0.7984128892472343
[2019-03-27 14:05:47,106] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06615058], dtype=float32), 0.06847393]
[2019-03-27 14:05:47,106] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 79.0, 1.0, 2.0, 0.5348318217707659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747361.497230726, 747361.497230726, 189234.8607281175]
[2019-03-27 14:05:47,106] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:05:47,110] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7322575e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1267060e-23], sampled 0.7326284665009241
[2019-03-27 14:05:50,885] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06615058], dtype=float32), 0.06847393]
[2019-03-27 14:05:50,886] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.6191672080767281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865257.7972965139, 865257.7972965139, 204361.709972923]
[2019-03-27 14:05:50,888] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:05:50,892] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7644821e-30 1.0000000e+00 1.9837134e-38 0.0000000e+00 3.1785068e-20], sampled 0.6587926617781574
[2019-03-27 14:06:01,782] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06615058], dtype=float32), 0.06847393]
[2019-03-27 14:06:01,783] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.1, 72.33333333333333, 1.0, 2.0, 0.5551032861513411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775698.7112926081, 775698.7112926086, 192679.6764513418]
[2019-03-27 14:06:01,784] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:06:01,787] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.380103e-29 1.000000e+00 1.024727e-36 0.000000e+00 7.833767e-18], sampled 0.8784383276986105
[2019-03-27 14:06:03,140] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06615058], dtype=float32), 0.06847393]
[2019-03-27 14:06:03,142] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.86666666666667, 63.00000000000001, 1.0, 2.0, 0.5909399199350022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825796.1210506543, 825796.1210506543, 199069.8130626706]
[2019-03-27 14:06:03,143] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:06:03,148] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.3520606e-28 1.0000000e+00 1.2186454e-35 0.0000000e+00 4.8308904e-16], sampled 0.8904933669904351
[2019-03-27 14:06:15,510] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06615058], dtype=float32), 0.06847393]
[2019-03-27 14:06:15,511] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.36666666666667, 58.66666666666666, 1.0, 2.0, 0.4919346548681197, 1.0, 2.0, 0.4919346548681197, 1.0, 2.0, 0.8543278102295824, 6.911199999999999, 6.9112, 169.0403247858759, 2063530.468535922, 2063530.468535922, 409315.6947699411]
[2019-03-27 14:06:15,513] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:06:15,517] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7042238e-13 1.5216183e-02 6.2802678e-21 1.0483488e-15 9.8478377e-01], sampled 0.6489852404201663
[2019-03-27 14:06:29,950] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06615058], dtype=float32), 0.06847393]
[2019-03-27 14:06:29,952] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.75, 95.0, 1.0, 2.0, 0.5301204689614931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740775.6672235276, 740775.667223527, 188449.2246914557]
[2019-03-27 14:06:29,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:06:29,959] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3612405e-28 1.0000000e+00 7.2940078e-37 0.0000000e+00 3.0208543e-18], sampled 0.15133667400595596
[2019-03-27 14:06:51,485] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06615058], dtype=float32), 0.06847393]
[2019-03-27 14:06:51,487] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.36202951, 76.7123702, 1.0, 2.0, 0.9264606757965986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1294948.17482842, 1294948.17482842, 277316.5175561906]
[2019-03-27 14:06:51,488] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:06:51,490] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5680609e-24 1.0000000e+00 3.6253656e-32 7.4775022e-36 3.3386004e-14], sampled 0.958252198447693
[2019-03-27 14:07:02,619] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8702.3575 2778720737.8244 831.0000
[2019-03-27 14:07:02,939] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8246.1950 2997215570.7291 1144.0000
[2019-03-27 14:07:03,065] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8593.9292 2838041583.1806 903.0000
[2019-03-27 14:07:03,137] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8053.5017 3157157725.7349 1453.0000
[2019-03-27 14:07:03,168] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8318.7911 2926528066.1814 1192.0000
[2019-03-27 14:07:04,186] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1450000, evaluation results [1450000.0, 8053.5016933821535, 3157157725.73489, 1453.0, 8318.791108858677, 2926528066.1813765, 1192.0, 8702.357543274042, 2778720737.8244476, 831.0, 8246.195043495347, 2997215570.7291255, 1144.0, 8593.92924049392, 2838041583.1806445, 903.0]
[2019-03-27 14:07:07,516] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1522807e-27 1.0000000e+00 1.7677021e-35 0.0000000e+00 4.3352247e-16], sum to 1.0000
[2019-03-27 14:07:07,525] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4355
[2019-03-27 14:07:07,529] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 52.0, 1.0, 2.0, 0.326015373759971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508090.75159728, 508090.7515972807, 167456.6544804861], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6804000.0000, 
sim time next is 6804600.0000, 
raw observation next is [28.06666666666667, 52.33333333333334, 1.0, 2.0, 0.3271270502280216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 510597.1251775273, 510597.125177528, 167672.0984466339], 
processed observation next is [1.0, 0.782608695652174, 0.529225908372828, 0.5233333333333334, 1.0, 1.0, 0.1893096990699055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14183253477153537, 0.14183253477153557, 0.2502568633531849], 
reward next is 0.7497, 
noisyNet noise sample is [array([0.6069184], dtype=float32), 0.4436605]. 
=============================================
[2019-03-27 14:07:15,036] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.7233619e-29 1.0000000e+00 5.1635298e-38 0.0000000e+00 4.5463256e-17], sum to 1.0000
[2019-03-27 14:07:15,043] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1626
[2019-03-27 14:07:15,048] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 77.0, 1.0, 2.0, 0.446562342492944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 640099.1347765512, 640099.1347765506, 177719.6724194971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6996600.0000, 
sim time next is 6997200.0000, 
raw observation next is [26.4, 77.33333333333334, 1.0, 2.0, 0.447679540859505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 641649.3312446683, 641649.3312446683, 177875.1561538553], 
processed observation next is [0.0, 1.0, 0.45023696682464454, 0.7733333333333334, 1.0, 1.0, 0.3345536636861506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1782359253457412, 0.1782359253457412, 0.2654853076923213], 
reward next is 0.7345, 
noisyNet noise sample is [array([-0.19114923], dtype=float32), -0.47068867]. 
=============================================
[2019-03-27 14:07:16,414] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2243911e-27 1.0000000e+00 2.3554510e-36 0.0000000e+00 4.2414280e-16], sum to 1.0000
[2019-03-27 14:07:16,423] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0655
[2019-03-27 14:07:16,436] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.55, 74.0, 1.0, 2.0, 0.3480497775111033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538613.2762694726, 538613.2762694726, 169773.31635188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6737400.0000, 
sim time next is 6738000.0000, 
raw observation next is [24.4, 74.66666666666667, 1.0, 2.0, 0.3468501450369242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 537429.8862201667, 537429.8862201673, 169695.2257226839], 
processed observation next is [1.0, 1.0, 0.3554502369668246, 0.7466666666666667, 1.0, 1.0, 0.21307246389990867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14928607950560185, 0.14928607950560202, 0.25327645630251333], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.6887202], dtype=float32), 1.408621]. 
=============================================
[2019-03-27 14:07:16,452] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.84723 ]
 [78.776146]
 [78.698616]
 [78.600296]
 [78.39301 ]], R is [[78.85816956]
 [78.81619263]
 [78.77445221]
 [78.73279572]
 [78.69108582]].
[2019-03-27 14:07:16,960] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9457467e-21 1.0000000e+00 3.8493976e-28 3.8366104e-30 1.9531523e-12], sum to 1.0000
[2019-03-27 14:07:16,969] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5398
[2019-03-27 14:07:16,978] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 74.0, 1.0, 2.0, 0.7689866624988516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1100967.114886573, 1100967.114886574, 240056.2035162981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7024200.0000, 
sim time next is 7024800.0000, 
raw observation next is [27.1, 73.0, 1.0, 2.0, 0.7580408675962303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1085756.157478967, 1085756.157478967, 237480.9454318413], 
processed observation next is [1.0, 0.30434782608695654, 0.4834123222748816, 0.73, 1.0, 1.0, 0.7084829730075064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3015989326330464, 0.3015989326330464, 0.3544491722863303], 
reward next is 0.6456, 
noisyNet noise sample is [array([0.6429903], dtype=float32), 0.9328642]. 
=============================================
[2019-03-27 14:07:22,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.6461995e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.9280694e-24], sum to 1.0000
[2019-03-27 14:07:22,934] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7550
[2019-03-27 14:07:22,941] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 59.0, 1.0, 2.0, 0.3515686018820105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542502.2869446904, 542502.286944691, 170049.3311366432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6861600.0000, 
sim time next is 6862200.0000, 
raw observation next is [27.4, 57.66666666666667, 1.0, 2.0, 0.3490455866151417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539549.9280122808, 539549.9280122814, 169833.1768647651], 
processed observation next is [0.0, 0.43478260869565216, 0.4976303317535545, 0.5766666666666667, 1.0, 1.0, 0.21571757423511045, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14987498000341135, 0.1498749800034115, 0.25348235352950016], 
reward next is 0.7465, 
noisyNet noise sample is [array([0.06615796], dtype=float32), 1.0999366]. 
=============================================
[2019-03-27 14:07:29,618] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4575912e-11 1.6768728e-06 3.5234416e-18 1.1028028e-11 9.9999833e-01], sum to 1.0000
[2019-03-27 14:07:29,629] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1814
[2019-03-27 14:07:29,635] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 49.33333333333333, 1.0, 2.0, 0.4531673381948052, 1.0, 2.0, 0.4531673381948052, 1.0, 2.0, 0.7542070150484175, 6.9112, 6.9112, 170.5573041426782, 1900751.559096053, 1900751.559096053, 378770.9812381563], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7051200.0000, 
sim time next is 7051800.0000, 
raw observation next is [31.11666666666667, 50.66666666666667, 1.0, 2.0, 0.4527598805428369, 1.0, 2.0, 0.4527598805428369, 1.0, 2.0, 0.7542975074664169, 6.9112, 6.9112, 170.5573041426782, 1899041.016287891, 1899041.016287891, 378661.5508017055], 
processed observation next is [1.0, 0.6086956521739131, 0.6737756714060034, 0.5066666666666667, 1.0, 1.0, 0.34067455487088777, 1.0, 1.0, 0.34067455487088777, 1.0, 1.0, 0.7003628139834353, 0.0, 0.0, 0.8375144448122397, 0.527511393413303, 0.527511393413303, 0.5651664937338888], 
reward next is 0.4348, 
noisyNet noise sample is [array([0.3625421], dtype=float32), 0.5816053]. 
=============================================
[2019-03-27 14:07:30,666] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.5999966e-26 1.0000000e+00 7.0169179e-34 0.0000000e+00 2.4930443e-17], sum to 1.0000
[2019-03-27 14:07:30,674] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9013
[2019-03-27 14:07:30,681] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 86.83333333333334, 1.0, 2.0, 0.5273177890111737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750799.7312878647, 750799.7312878647, 189768.2273237901], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7107000.0000, 
sim time next is 7107600.0000, 
raw observation next is [25.33333333333334, 85.66666666666667, 1.0, 2.0, 0.5065954625839889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721175.2415038793, 721175.2415038793, 186333.5788862473], 
processed observation next is [1.0, 0.2608695652173913, 0.3996840442338076, 0.8566666666666667, 1.0, 1.0, 0.40553670190842034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20032645597329982, 0.20032645597329982, 0.27810981923320494], 
reward next is 0.7219, 
noisyNet noise sample is [array([0.00996628], dtype=float32), -0.035423733]. 
=============================================
[2019-03-27 14:07:36,509] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5780868e-14 9.9996722e-01 9.6809043e-21 8.3090029e-19 3.2775701e-05], sum to 1.0000
[2019-03-27 14:07:36,520] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6177
[2019-03-27 14:07:36,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1701876.913099301 W.
[2019-03-27 14:07:36,531] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.45, 59.66666666666667, 1.0, 2.0, 0.5995307569473799, 0.0, 1.0, 0.0, 1.0, 1.0, 1.008262036874249, 6.9112, 6.9112, 168.9129565036114, 1701876.913099301, 1701876.913099301, 361875.0624137789], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7033800.0000, 
sim time next is 7034400.0000, 
raw observation next is [29.6, 59.0, 1.0, 2.0, 0.4373748289587177, 1.0, 1.0, 0.4373748289587177, 1.0, 2.0, 0.7298115921056394, 6.911199999999999, 6.9112, 170.5573041426782, 1834455.224288004, 1834455.224288005, 369761.6789739763], 
processed observation next is [1.0, 0.43478260869565216, 0.6018957345971565, 0.59, 1.0, 1.0, 0.3221383481430333, 1.0, 0.5, 0.3221383481430333, 1.0, 1.0, 0.6705019415922432, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5095708956355567, 0.5095708956355569, 0.5518831029462332], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.18674819], dtype=float32), 1.348419]. 
=============================================
[2019-03-27 14:07:37,757] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4633725e-25 1.0000000e+00 8.0938881e-32 9.3801056e-36 2.9854243e-16], sum to 1.0000
[2019-03-27 14:07:37,767] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9905
[2019-03-27 14:07:37,775] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.23333333333333, 65.0, 1.0, 2.0, 0.3692330817980358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552508.9387727159, 552508.9387727166, 170337.5364096352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7320000.0000, 
sim time next is 7320600.0000, 
raw observation next is [27.15, 65.5, 1.0, 2.0, 0.3666095040643219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548563.7826171903, 548563.7826171903, 169998.3087747669], 
processed observation next is [1.0, 0.7391304347826086, 0.485781990521327, 0.655, 1.0, 1.0, 0.236878920559424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15237882850477508, 0.15237882850477508, 0.25372881906681627], 
reward next is 0.7463, 
noisyNet noise sample is [array([-0.02812998], dtype=float32), -0.35064122]. 
=============================================
[2019-03-27 14:07:48,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7091187e-31 1.0000000e+00 2.7721588e-38 0.0000000e+00 1.1689080e-24], sum to 1.0000
[2019-03-27 14:07:48,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0552
[2019-03-27 14:07:48,056] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 88.33333333333333, 1.0, 2.0, 0.6130224763090107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 856667.3532408532, 856667.3532408539, 203188.4528057531], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7180800.0000, 
sim time next is 7181400.0000, 
raw observation next is [25.8, 88.66666666666667, 1.0, 2.0, 0.5825227407387057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814029.2099487165, 814029.2099487165, 197528.584119129], 
processed observation next is [1.0, 0.08695652173913043, 0.42180094786729866, 0.8866666666666667, 1.0, 1.0, 0.4970153502875972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22611922498575457, 0.22611922498575457, 0.2948187822673567], 
reward next is 0.7052, 
noisyNet noise sample is [array([-1.8151833], dtype=float32), -1.070676]. 
=============================================
[2019-03-27 14:07:50,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0290342e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1809276e-23], sum to 1.0000
[2019-03-27 14:07:50,562] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1744
[2019-03-27 14:07:50,570] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.91666666666667, 92.0, 1.0, 2.0, 0.6932037179371177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1103666.675103436, 1103666.675103435, 235280.3307514945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7393800.0000, 
sim time next is 7394400.0000, 
raw observation next is [20.9, 92.0, 1.0, 2.0, 0.6590100484177079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1049587.856301528, 1049587.856301527, 227189.844411565], 
processed observation next is [1.0, 0.6086956521739131, 0.1895734597156398, 0.92, 1.0, 1.0, 0.589168733033383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29155218230597996, 0.29155218230597973, 0.33908932001726116], 
reward next is 0.6609, 
noisyNet noise sample is [array([-1.6018114], dtype=float32), -0.024709465]. 
=============================================
[2019-03-27 14:07:53,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2196105e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1450929e-24], sum to 1.0000
[2019-03-27 14:07:53,254] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1510
[2019-03-27 14:07:53,263] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 74.0, 1.0, 2.0, 0.7306945428776342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1131166.859759534, 1131166.859759534, 241497.8793449228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7292400.0000, 
sim time next is 7293000.0000, 
raw observation next is [24.78333333333333, 73.0, 1.0, 2.0, 0.7518275171007635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161936.169716239, 1161936.169716239, 246643.2278884322], 
processed observation next is [1.0, 0.391304347826087, 0.37361769352290675, 0.73, 1.0, 1.0, 0.7009970085551367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32276004714339973, 0.32276004714339973, 0.36812422072900325], 
reward next is 0.6319, 
noisyNet noise sample is [array([-0.8560351], dtype=float32), -0.87583905]. 
=============================================
[2019-03-27 14:07:53,279] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.41936]
 [72.60197]
 [72.71666]
 [72.86651]
 [72.84947]], R is [[72.25655365]
 [72.17354584]
 [72.10649109]
 [72.05062103]
 [72.00756836]].
[2019-03-27 14:07:59,472] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2321446e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2018088e-24], sum to 1.0000
[2019-03-27 14:07:59,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0048
[2019-03-27 14:07:59,489] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 75.33333333333334, 1.0, 2.0, 0.3653064065813205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 557927.7927616836, 557927.7927616829, 171179.8382326069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7345200.0000, 
sim time next is 7345800.0000, 
raw observation next is [24.85, 75.0, 1.0, 2.0, 0.3646007499554474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557398.0143201961, 557398.0143201961, 171150.8276421347], 
processed observation next is [1.0, 0.0, 0.37677725118483424, 0.75, 1.0, 1.0, 0.23445873488608118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15483278175561002, 0.15483278175561002, 0.25544899648079805], 
reward next is 0.7446, 
noisyNet noise sample is [array([2.0662596], dtype=float32), 0.51498383]. 
=============================================
[2019-03-27 14:07:59,686] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.17955764e-35 1.00000000e+00 0.00000000e+00 0.00000000e+00
 6.00563637e-29], sum to 1.0000
[2019-03-27 14:07:59,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7907
[2019-03-27 14:07:59,700] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 92.5, 1.0, 2.0, 0.4017426776871507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594625.5350755848, 594625.5350755848, 173898.9743302296], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7525800.0000, 
sim time next is 7526400.0000, 
raw observation next is [23.36666666666667, 92.33333333333333, 1.0, 2.0, 0.3984910285097117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590974.3716383805, 590974.3716383805, 173596.6020012604], 
processed observation next is [0.0, 0.08695652173913043, 0.30647709320695127, 0.9233333333333333, 1.0, 1.0, 0.27529039579483333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1641595476773279, 0.1641595476773279, 0.25909940597203046], 
reward next is 0.7409, 
noisyNet noise sample is [array([0.57898545], dtype=float32), 0.19261271]. 
=============================================
[2019-03-27 14:08:01,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1769825e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.7404946e-26], sum to 1.0000
[2019-03-27 14:08:01,381] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9245
[2019-03-27 14:08:01,386] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666666, 93.0, 1.0, 2.0, 0.6971100003796143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1101304.569605126, 1101304.569605127, 235486.3110821837], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7382400.0000, 
sim time next is 7383000.0000, 
raw observation next is [21.23333333333333, 93.0, 1.0, 2.0, 0.6994920625006624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1103318.787937988, 1103318.787937988, 235905.5648011823], 
processed observation next is [1.0, 0.43478260869565216, 0.2053712480252764, 0.93, 1.0, 1.0, 0.6379422439767017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3064774410938856, 0.3064774410938856, 0.3520978579122124], 
reward next is 0.6479, 
noisyNet noise sample is [array([-1.3059243], dtype=float32), -2.3049972]. 
=============================================
[2019-03-27 14:08:01,419] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.23961 ]
 [72.24254 ]
 [72.35698 ]
 [72.59183 ]
 [72.753815]], R is [[72.18943787]
 [72.11607361]
 [72.04367065]
 [71.98114777]
 [71.94052887]].
[2019-03-27 14:08:09,907] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 14:08:09,909] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:08:09,910] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:08:09,911] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:08:09,912] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:08:09,912] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:08:09,915] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:08:09,914] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:08:09,920] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:08:09,922] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:08:09,924] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:08:09,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run60
[2019-03-27 14:08:09,964] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run60
[2019-03-27 14:08:09,984] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run60
[2019-03-27 14:08:10,003] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run60
[2019-03-27 14:08:10,003] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run60
[2019-03-27 14:08:22,194] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:08:22,194] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.5, 74.0, 1.0, 2.0, 0.306050880997029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485894.6111842604, 485894.6111842604, 166022.4276916357]
[2019-03-27 14:08:22,196] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:08:22,200] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 8.96112e-35], sampled 0.6915283679196963
[2019-03-27 14:08:22,285] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:08:22,286] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.55, 74.0, 1.0, 2.0, 0.3566956692972057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 565664.3120874428, 565664.3120874423, 172246.979481505]
[2019-03-27 14:08:22,286] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:08:22,289] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.285842e-33], sampled 0.70683736922119
[2019-03-27 14:08:51,575] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:08:51,575] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.7770382996748189, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.987423626034889, 6.9112, 168.9124396110559, 1982938.120707829, 1928862.622463703, 402932.4211312347]
[2019-03-27 14:08:51,576] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:08:51,578] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.0981078e-17 1.0000000e+00 2.5996067e-23 8.3432441e-23 5.3931153e-08], sampled 0.3195697571718722
[2019-03-27 14:08:51,579] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1982938.120707829 W.
[2019-03-27 14:08:53,813] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:08:53,814] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.4, 88.0, 1.0, 2.0, 0.4908769868477258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686928.6840628021, 686928.6840628014, 182308.0766452939]
[2019-03-27 14:08:53,815] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:08:53,817] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2482967e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2693543e-26], sampled 0.5871315160512276
[2019-03-27 14:09:18,680] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:09:18,681] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.04933364666667, 68.16750335333333, 1.0, 2.0, 0.6884935315717595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 962182.0422663459, 962182.0422663466, 218393.1181497739]
[2019-03-27 14:09:18,681] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:09:18,685] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9464539e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4843260e-27], sampled 0.4149742964987718
[2019-03-27 14:09:21,003] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:09:21,004] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.90338177666666, 86.18038311166667, 1.0, 2.0, 0.8725932022929337, 1.0, 2.0, 0.8725932022929337, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2440531.361767929, 2440531.361767929, 457249.9400246353]
[2019-03-27 14:09:21,004] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:09:21,008] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2754471e-24 1.0000000e+00 1.6586433e-31 9.0288445e-37 4.8645069e-17], sampled 0.3884599262051247
[2019-03-27 14:09:21,009] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2440531.361767929 W.
[2019-03-27 14:09:21,167] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:09:21,172] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.9, 66.5, 1.0, 2.0, 0.5405369536737208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755336.549817244, 755336.5498172446, 190191.3312608803]
[2019-03-27 14:09:21,172] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:09:21,177] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2547417e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.8657549e-29], sampled 0.507596777758908
[2019-03-27 14:09:27,961] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:09:27,964] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.00000000000001, 1.0, 2.0, 0.9806421946334349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1370728.567619118, 1370728.567619118, 293079.7629098151]
[2019-03-27 14:09:27,965] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:09:27,970] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.8172736e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5448807e-27], sampled 0.3368720802657885
[2019-03-27 14:09:36,376] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:09:36,376] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.89402229, 72.10698769, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.182924067481553, 6.9112, 168.9057782135851, 2356523.318491946, 1454356.836670093, 311137.0672579724]
[2019-03-27 14:09:36,377] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:09:36,380] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.2890559e-29 1.0000000e+00 3.4632231e-36 0.0000000e+00 7.1253143e-22], sampled 0.29625078429272644
[2019-03-27 14:09:36,382] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2356523.318491946 W.
[2019-03-27 14:09:53,382] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06217666], dtype=float32), 0.065976895]
[2019-03-27 14:09:53,386] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.42553981333333, 60.60655243999999, 1.0, 2.0, 0.543196328328072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759054.0395801944, 759054.0395801937, 190641.6460859455]
[2019-03-27 14:09:53,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:09:53,392] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.291242e-38 1.000000e+00 0.000000e+00 0.000000e+00 6.831645e-31], sampled 0.4386199182579874
[2019-03-27 14:10:17,589] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.2240 2927300355.1475 1343.0000
[2019-03-27 14:10:17,797] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7880.9350 3164132470.8112 1794.0000
[2019-03-27 14:10:17,929] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.0694 3007810615.5123 1771.0000
[2019-03-27 14:10:18,100] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.3370 2842489488.6110 1142.0000
[2019-03-27 14:10:18,218] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.5643 2779408808.8481 937.0000
[2019-03-27 14:10:19,235] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1475000, evaluation results [1475000.0, 7880.935009336218, 3164132470.811223, 1794.0, 8253.223980709216, 2927300355.147502, 1343.0, 8657.564303819094, 2779408808.848113, 937.0, 7997.069419993469, 3007810615.512334, 1771.0, 8495.337010127836, 2842489488.610977, 1142.0]
[2019-03-27 14:10:29,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:29,309] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:29,388] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run8
[2019-03-27 14:10:30,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5773174e-21 1.0000000e+00 1.1477677e-27 3.4354288e-31 7.1559735e-14], sum to 1.0000
[2019-03-27 14:10:30,351] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0891
[2019-03-27 14:10:30,359] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1904991.697341284 W.
[2019-03-27 14:10:30,364] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.88333333333333, 75.66666666666666, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.546827954045475, 6.9112, 168.9097940231034, 1904991.697341284, 1454063.792810063, 311350.2242664805], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7811400.0000, 
sim time next is 7812000.0000, 
raw observation next is [29.0, 75.0, 1.0, 2.0, 0.4355779075499691, 1.0, 1.0, 0.4355779075499691, 1.0, 1.0, 0.748583922399319, 6.9112, 6.9112, 170.5573041426782, 1826912.078562264, 1826912.078562264, 372237.7780283099], 
processed observation next is [1.0, 0.43478260869565216, 0.5734597156398105, 0.75, 1.0, 1.0, 0.31997338259032426, 1.0, 0.5, 0.31997338259032426, 1.0, 0.5, 0.6933950273162426, 0.0, 0.0, 0.8375144448122397, 0.5074755773784067, 0.5074755773784067, 0.555578773176582], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02164444], dtype=float32), 1.4579064]. 
=============================================
[2019-03-27 14:10:30,380] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[54.878994]
 [54.21796 ]
 [53.041286]
 [52.630367]
 [54.629387]], R is [[51.56560898]
 [51.04995346]
 [51.04986191]
 [51.00567627]
 [50.96650314]].
[2019-03-27 14:10:31,771] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:31,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:31,806] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4166286e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2301828e-27], sum to 1.0000
[2019-03-27 14:10:31,817] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5501
[2019-03-27 14:10:31,823] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.35, 89.0, 1.0, 2.0, 0.4800516621168904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670788.812251617, 670788.812251617, 180529.1795795794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7687800.0000, 
sim time next is 7688400.0000, 
raw observation next is [25.3, 89.33333333333334, 1.0, 2.0, 0.4808545151576401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 671911.0147086182, 671911.0147086189, 180650.2653881644], 
processed observation next is [1.0, 1.0, 0.39810426540284366, 0.8933333333333334, 1.0, 1.0, 0.3745235122381206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18664194853017171, 0.1866419485301719, 0.2696272617733797], 
reward next is 0.7304, 
noisyNet noise sample is [array([-1.4882267], dtype=float32), 0.2582317]. 
=============================================
[2019-03-27 14:10:31,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run8
[2019-03-27 14:10:35,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:35,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:36,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run8
[2019-03-27 14:10:37,599] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:37,601] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:37,617] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:37,618] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:37,675] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run8
[2019-03-27 14:10:37,713] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run8
[2019-03-27 14:10:38,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:38,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:38,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run8
[2019-03-27 14:10:39,715] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:39,715] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:39,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run8
[2019-03-27 14:10:40,482] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:40,482] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:40,550] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-03-27 14:10:42,609] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:42,610] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:42,684] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run8
[2019-03-27 14:10:44,316] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:44,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:44,405] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run8
[2019-03-27 14:10:45,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:45,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:45,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run8
[2019-03-27 14:10:45,280] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0357618e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2382503e-30], sum to 1.0000
[2019-03-27 14:10:45,288] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7983
[2019-03-27 14:10:45,290] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.51666666666667, 90.16666666666667, 1.0, 2.0, 0.378453223783551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 582541.3871756506, 582541.38717565, 173434.9363647899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 101400.0000, 
sim time next is 102000.0000, 
raw observation next is [22.53333333333333, 90.33333333333334, 1.0, 2.0, 0.3699564899225927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 568852.5160799492, 568852.5160799492, 172223.4007100372], 
processed observation next is [1.0, 0.17391304347826086, 0.26698262243285936, 0.9033333333333334, 1.0, 1.0, 0.24091143364167797, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1580145877999859, 0.1580145877999859, 0.2570498518060257], 
reward next is 0.7430, 
noisyNet noise sample is [array([-1.5046517], dtype=float32), -0.6700063]. 
=============================================
[2019-03-27 14:10:45,299] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.900276]
 [71.8159  ]
 [71.727585]
 [71.70036 ]
 [71.71991 ]], R is [[71.84671021]
 [71.86938477]
 [71.89155579]
 [71.91318512]
 [71.93404388]].
[2019-03-27 14:10:45,884] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:45,885] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:45,972] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run8
[2019-03-27 14:10:47,306] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:47,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:47,402] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run8
[2019-03-27 14:10:47,492] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:47,492] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:47,561] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run8
[2019-03-27 14:10:47,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:47,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:47,664] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run8
[2019-03-27 14:10:47,839] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:10:47,839] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:10:47,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run8
[2019-03-27 14:10:47,980] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.7142904e-32], sum to 1.0000
[2019-03-27 14:10:47,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9457
[2019-03-27 14:10:47,991] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.15, 87.33333333333334, 1.0, 2.0, 0.2842399948985962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 456185.0715017262, 456185.0715017269, 164006.8604565128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 285000.0000, 
sim time next is 285600.0000, 
raw observation next is [21.3, 86.66666666666667, 1.0, 2.0, 0.2858559580874585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458183.616533574, 458183.616533574, 164137.3242651715], 
processed observation next is [0.0, 0.30434782608695654, 0.2085308056872039, 0.8666666666666667, 1.0, 1.0, 0.13958549167163675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12727322681488168, 0.12727322681488168, 0.2449810809927933], 
reward next is 0.7550, 
noisyNet noise sample is [array([0.52996194], dtype=float32), -0.3345581]. 
=============================================
[2019-03-27 14:10:50,805] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6550162e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1329076e-28], sum to 1.0000
[2019-03-27 14:10:50,815] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8222
[2019-03-27 14:10:50,822] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.65, 91.0, 1.0, 2.0, 0.3855563486761903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589193.3865070618, 589193.3865070618, 173929.1667299773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 106200.0000, 
sim time next is 106800.0000, 
raw observation next is [22.66666666666667, 91.0, 1.0, 2.0, 0.3864269377587324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590224.8193248805, 590224.8193248805, 174014.7813205443], 
processed observation next is [1.0, 0.21739130434782608, 0.27330173775671435, 0.91, 1.0, 1.0, 0.2607553466972679, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1639513387013557, 0.1639513387013557, 0.2597235542097676], 
reward next is 0.7403, 
noisyNet noise sample is [array([1.2369728], dtype=float32), 0.03036915]. 
=============================================
[2019-03-27 14:10:57,115] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.9558949e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4373383e-30], sum to 1.0000
[2019-03-27 14:10:57,126] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7281
[2019-03-27 14:10:57,133] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 80.66666666666666, 1.0, 2.0, 0.2954624317198164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471619.8540841072, 471619.8540841072, 165043.9769706901], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 323400.0000, 
sim time next is 324000.0000, 
raw observation next is [22.2, 81.0, 1.0, 2.0, 0.2944966427733454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470279.2579332673, 470279.2579332667, 164952.6699789232], 
processed observation next is [0.0, 0.782608695652174, 0.2511848341232228, 0.81, 1.0, 1.0, 0.14999595514860892, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13063312720368536, 0.1306331272036852, 0.2461980148939152], 
reward next is 0.7538, 
noisyNet noise sample is [array([-0.6755898], dtype=float32), -0.30399472]. 
=============================================
[2019-03-27 14:10:57,146] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[79.09691 ]
 [79.075874]
 [79.04109 ]
 [78.994156]
 [78.939445]], R is [[79.21521759]
 [79.17672729]
 [79.13853455]
 [79.10067749]
 [79.06312561]].
[2019-03-27 14:11:01,171] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8252620e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7049447e-27], sum to 1.0000
[2019-03-27 14:11:01,182] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4394
[2019-03-27 14:11:01,187] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.06666666666667, 74.0, 1.0, 2.0, 0.4619755834979364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752241.4872019907, 752241.4872019907, 189065.6488971295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 382800.0000, 
sim time next is 383400.0000, 
raw observation next is [22.15, 73.5, 1.0, 2.0, 0.4780898799845202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778347.4763162672, 778347.4763162679, 191802.4131019234], 
processed observation next is [1.0, 0.43478260869565216, 0.24881516587677724, 0.735, 1.0, 1.0, 0.3711926264873738, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2162076323100742, 0.2162076323100744, 0.28627225836107967], 
reward next is 0.7137, 
noisyNet noise sample is [array([0.30596945], dtype=float32), 0.9102816]. 
=============================================
[2019-03-27 14:11:02,825] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5431132e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2522271e-27], sum to 1.0000
[2019-03-27 14:11:02,835] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2451
[2019-03-27 14:11:02,840] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 75.0, 1.0, 2.0, 0.4174778319579757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686437.3715234355, 686437.3715234349, 182086.0896672615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 550800.0000, 
sim time next is 551400.0000, 
raw observation next is [21.4, 73.5, 1.0, 2.0, 0.4935744429945579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811601.5395526467, 811601.5395526467, 194680.2085834122], 
processed observation next is [1.0, 0.391304347826087, 0.21327014218009477, 0.735, 1.0, 1.0, 0.3898487264994673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2254448720979574, 0.2254448720979574, 0.2905674754976301], 
reward next is 0.7094, 
noisyNet noise sample is [array([0.32082197], dtype=float32), 0.3716844]. 
=============================================
[2019-03-27 14:11:07,547] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3138682e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.2566998e-31], sum to 1.0000
[2019-03-27 14:11:07,559] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9372
[2019-03-27 14:11:07,563] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 77.0, 1.0, 2.0, 0.3150392842453921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496097.0192038848, 496097.0192038848, 166688.1536447444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 313200.0000, 
sim time next is 313800.0000, 
raw observation next is [23.33333333333333, 77.16666666666667, 1.0, 2.0, 0.3136728975625669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 494428.568492173, 494428.5684921724, 166575.4995984131], 
processed observation next is [0.0, 0.6521739130434783, 0.30489731437598716, 0.7716666666666667, 1.0, 1.0, 0.17309987658140588, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13734126902560362, 0.13734126902560345, 0.2486201486543479], 
reward next is 0.7514, 
noisyNet noise sample is [array([-0.15377209], dtype=float32), -0.7136305]. 
=============================================
[2019-03-27 14:11:14,273] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.492618e-33], sum to 1.0000
[2019-03-27 14:11:14,295] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7689
[2019-03-27 14:11:14,302] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 75.83333333333333, 1.0, 2.0, 0.2436993758467897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401287.8421324504, 401287.8421324504, 160282.7574208353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 546600.0000, 
sim time next is 547200.0000, 
raw observation next is [21.2, 75.0, 1.0, 2.0, 0.2456393008848934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 404081.025471935, 404081.0254719344, 160478.3772219267], 
processed observation next is [1.0, 0.34782608695652173, 0.20379146919431282, 0.75, 1.0, 1.0, 0.09113168781312458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11224472929775972, 0.11224472929775955, 0.23951996600287567], 
reward next is 0.7605, 
noisyNet noise sample is [array([-0.8616165], dtype=float32), 2.9566226]. 
=============================================
[2019-03-27 14:11:15,911] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.6197314e-33], sum to 1.0000
[2019-03-27 14:11:15,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2752
[2019-03-27 14:11:15,932] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 79.16666666666667, 1.0, 2.0, 0.2379483029790189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 393328.7558126719, 393328.7558126719, 159667.2780906805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 544200.0000, 
sim time next is 544800.0000, 
raw observation next is [20.4, 78.33333333333334, 1.0, 2.0, 0.2407276706654041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397550.4897433306, 397550.4897433306, 159953.1145831218], 
processed observation next is [1.0, 0.30434782608695654, 0.16587677725118483, 0.7833333333333334, 1.0, 1.0, 0.08521406104265553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1104306915953696, 0.1104306915953696, 0.23873599191510714], 
reward next is 0.7613, 
noisyNet noise sample is [array([1.3868834], dtype=float32), 2.187547]. 
=============================================
[2019-03-27 14:11:22,404] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 14:11:22,412] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:11:22,413] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:11:22,414] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:11:22,416] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:11:22,416] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:11:22,420] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:11:22,422] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:11:22,424] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:11:22,417] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:11:22,447] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:11:23,504] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run61
[2019-03-27 14:11:23,514] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run61
[2019-03-27 14:11:24,015] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run61
[2019-03-27 14:11:24,214] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run61
[2019-03-27 14:11:24,230] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run61
[2019-03-27 14:11:26,192] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:11:26,192] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.41666666666666, 67.0, 1.0, 2.0, 1.029616093215731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9127667897571, 1439230.034864057, 1439230.034864057, 308076.7752449627]
[2019-03-27 14:11:26,192] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:11:26,194] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7474250e-23 1.0000000e+00 1.4540592e-30 8.6015812e-32 3.2238218e-13], sampled 0.6974028823478685
[2019-03-27 14:11:36,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:11:36,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.3, 68.33333333333333, 1.0, 2.0, 0.3451758447547085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536357.2389285953, 536357.2389285953, 169648.5584591416]
[2019-03-27 14:11:36,347] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:11:36,349] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.349492e-36 1.000000e+00 0.000000e+00 0.000000e+00 4.197717e-28], sampled 0.14811895078243276
[2019-03-27 14:11:39,784] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:11:39,787] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.83333333333334, 86.33333333333334, 1.0, 2.0, 0.2850796513709253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461102.7688939435, 461102.7688939441, 164340.6630516481]
[2019-03-27 14:11:39,789] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:11:39,793] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7211723e-33], sampled 0.6953438273380554
[2019-03-27 14:11:46,798] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:11:46,799] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.98398520666667, 68.42503600166667, 1.0, 2.0, 0.5239696377404663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732177.7037624327, 732177.7037624333, 187436.9026922825]
[2019-03-27 14:11:46,801] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:11:46,803] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.3863064e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3126764e-25], sampled 0.2550736481160252
[2019-03-27 14:11:48,855] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:11:48,858] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.85, 57.33333333333333, 1.0, 2.0, 0.3572748588014932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546906.9123869502, 546906.9123869502, 170285.8858871213]
[2019-03-27 14:11:48,859] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:11:48,863] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5079227e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6686995e-28], sampled 0.19517300348326594
[2019-03-27 14:11:57,238] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:11:57,239] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.86180565, 92.77031379, 1.0, 2.0, 0.395035367633272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 596844.1084203549, 596844.1084203543, 174452.6092312787]
[2019-03-27 14:11:57,240] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:11:57,243] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4225747e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6570832e-26], sampled 0.468674013937186
[2019-03-27 14:12:12,846] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:12:12,846] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.5, 58.0, 1.0, 2.0, 0.6788062877506931, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00435943817754, 6.9112, 168.9123258536074, 1845471.705925707, 1779381.440939016, 380176.6333261277]
[2019-03-27 14:12:12,847] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:12:12,848] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.3414903e-17 9.9998879e-01 6.5494845e-24 9.5664270e-21 1.1152792e-05], sampled 0.28189506988540913
[2019-03-27 14:12:12,849] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1845471.705925707 W.
[2019-03-27 14:12:27,249] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:12:27,251] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.048080465, 77.42069425, 1.0, 2.0, 0.5508969467255119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769818.6624454455, 769818.6624454461, 191954.3686657559]
[2019-03-27 14:12:27,252] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:12:27,257] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4731666e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0542664e-27], sampled 0.9348675505069642
[2019-03-27 14:12:45,919] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:12:45,922] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.88756863, 59.6501564, 1.0, 2.0, 0.7016327036446569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 980552.7476077966, 980552.7476077966, 221195.5560047596]
[2019-03-27 14:12:45,922] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:12:45,924] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.00718306e-28 1.00000000e+00 2.24676402e-36 0.00000000e+00
 3.06819022e-19], sampled 0.5956795009623778
[2019-03-27 14:12:52,662] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:12:52,663] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.04935265333333, 82.66651133333333, 1.0, 2.0, 0.5193925594812484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725779.6621916441, 725779.6621916448, 186689.3245203519]
[2019-03-27 14:12:52,664] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:12:52,667] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.86038928e-32 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.01344506e-22], sampled 0.5789364575799107
[2019-03-27 14:12:55,710] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:12:55,712] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.3151804, 62.01511749, 1.0, 2.0, 0.9148255702240794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1278675.566291285, 1278675.566291286, 274045.61053306]
[2019-03-27 14:12:55,713] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:12:55,714] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2409186e-28 1.0000000e+00 3.8134277e-36 0.0000000e+00 2.4824318e-19], sampled 0.5071680380170634
[2019-03-27 14:13:13,826] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06421135], dtype=float32), 0.07347087]
[2019-03-27 14:13:13,826] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.39251978666666, 78.45314296833334, 1.0, 2.0, 0.3267423637429718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513978.4126046205, 513978.4126046211, 168032.7079948346]
[2019-03-27 14:13:13,827] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:13:13,829] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6524035e-33], sampled 0.7149654229169298
[2019-03-27 14:13:28,626] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.0329 2927910763.9312 1348.0000
[2019-03-27 14:13:30,744] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8515.2441 2841459054.3661 1107.0000
[2019-03-27 14:13:30,973] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7913.2227 3162162951.6369 1807.0000
[2019-03-27 14:13:31,048] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8045.0195 3005307417.4815 1638.0000
[2019-03-27 14:13:31,394] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8668.3932 2779151625.7958 916.0000
[2019-03-27 14:13:32,412] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1500000, evaluation results [1500000.0, 7913.222690110423, 3162162951.6368747, 1807.0, 8256.032861118485, 2927910763.931169, 1348.0, 8668.393228333081, 2779151625.795753, 916.0, 8045.019450583847, 3005307417.4815383, 1638.0, 8515.244063352731, 2841459054.366088, 1107.0]
[2019-03-27 14:13:53,574] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.062874e-32 1.000000e+00 0.000000e+00 0.000000e+00 8.188857e-25], sum to 1.0000
[2019-03-27 14:13:53,588] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1774
[2019-03-27 14:13:53,595] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.76666666666667, 96.33333333333334, 1.0, 2.0, 0.3560687070328912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547699.3542263196, 547699.3542263196, 170431.1093911203], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1017600.0000, 
sim time next is 1018200.0000, 
raw observation next is [21.78333333333333, 96.16666666666666, 1.0, 2.0, 0.3549598945735055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546046.257277699, 546046.2572776984, 170294.8354384893], 
processed observation next is [1.0, 0.782608695652174, 0.2314375987361769, 0.9616666666666666, 1.0, 1.0, 0.22284324647410303, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15167951591047193, 0.15167951591047177, 0.2541713961768497], 
reward next is 0.7458, 
noisyNet noise sample is [array([-0.8774752], dtype=float32), -0.3764282]. 
=============================================
[2019-03-27 14:13:57,190] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4972155e-33], sum to 1.0000
[2019-03-27 14:13:57,197] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3310
[2019-03-27 14:13:57,202] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 93.33333333333334, 1.0, 2.0, 0.2942293455128591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474785.2745179156, 474785.2745179156, 165290.5602383536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1135200.0000, 
sim time next is 1135800.0000, 
raw observation next is [20.05, 93.5, 1.0, 2.0, 0.2889763351293487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466575.7557576234, 466575.7557576234, 164720.1972861002], 
processed observation next is [1.0, 0.13043478260869565, 0.14928909952606645, 0.935, 1.0, 1.0, 0.14334498208355267, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12960437659933985, 0.12960437659933985, 0.24585104072552272], 
reward next is 0.7541, 
noisyNet noise sample is [array([-0.8561886], dtype=float32), -0.05571806]. 
=============================================
[2019-03-27 14:14:16,229] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0129703e-38], sum to 1.0000
[2019-03-27 14:14:16,241] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6584
[2019-03-27 14:14:16,245] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 97.0, 1.0, 2.0, 0.316605561680278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499321.9557494573, 499321.9557494573, 166946.7863432244], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1401600.0000, 
sim time next is 1402200.0000, 
raw observation next is [20.85, 96.5, 1.0, 2.0, 0.3181855972912074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501507.0583683559, 501507.0583683565, 167104.0824172663], 
processed observation next is [0.0, 0.21739130434782608, 0.18720379146919444, 0.965, 1.0, 1.0, 0.17853686420627393, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1393075162134322, 0.13930751621343235, 0.24940907823472583], 
reward next is 0.7506, 
noisyNet noise sample is [array([0.1298906], dtype=float32), 0.107557744]. 
=============================================
[2019-03-27 14:14:24,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5607977e-32], sum to 1.0000
[2019-03-27 14:14:24,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3247
[2019-03-27 14:14:24,137] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.35, 89.16666666666667, 1.0, 2.0, 0.3445147210194953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534367.0638527004, 534367.0638527004, 169461.5855176417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1213800.0000, 
sim time next is 1214400.0000, 
raw observation next is [22.3, 89.33333333333334, 1.0, 2.0, 0.3432941827803873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532884.6881421336, 532884.6881421336, 169352.8989846993], 
processed observation next is [1.0, 0.043478260869565216, 0.25592417061611383, 0.8933333333333334, 1.0, 1.0, 0.208788172024563, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.148023524483926, 0.148023524483926, 0.25276552087268556], 
reward next is 0.7472, 
noisyNet noise sample is [array([-0.05120152], dtype=float32), 0.86410874]. 
=============================================
[2019-03-27 14:14:24,793] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9653465e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0966059e-31], sum to 1.0000
[2019-03-27 14:14:24,803] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8644
[2019-03-27 14:14:24,809] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 94.5, 1.0, 2.0, 0.3758193335885419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 568786.0975120612, 568786.0975120618, 171964.8258838967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1459800.0000, 
sim time next is 1460400.0000, 
raw observation next is [22.43333333333333, 94.66666666666666, 1.0, 2.0, 0.3737201578384591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566450.1328268448, 566450.1328268448, 171786.3868790344], 
processed observation next is [0.0, 0.9130434782608695, 0.2622432859399683, 0.9466666666666665, 1.0, 1.0, 0.24544597329934828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.157347259118568, 0.157347259118568, 0.25639759235676773], 
reward next is 0.7436, 
noisyNet noise sample is [array([-0.7426829], dtype=float32), 0.41444594]. 
=============================================
[2019-03-27 14:14:32,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5650077e-33], sum to 1.0000
[2019-03-27 14:14:32,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3316
[2019-03-27 14:14:32,465] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.45, 97.0, 1.0, 2.0, 0.3113004068474109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 494451.5192349766, 494451.5192349772, 166652.224787149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1384200.0000, 
sim time next is 1384800.0000, 
raw observation next is [20.43333333333333, 97.0, 1.0, 2.0, 0.312230594196674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 496114.8118131755, 496114.8118131761, 166777.9154338325], 
processed observation next is [0.0, 0.0, 0.1674565560821484, 0.97, 1.0, 1.0, 0.17136216168273974, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1378096699481043, 0.13780966994810448, 0.24892226184154104], 
reward next is 0.7511, 
noisyNet noise sample is [array([-0.9519615], dtype=float32), -1.4311491]. 
=============================================
[2019-03-27 14:14:38,501] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 14:14:38,503] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:14:38,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:14:38,504] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:14:38,506] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:14:38,507] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:14:38,504] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:14:38,509] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:14:38,508] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:14:38,509] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:14:38,510] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:14:38,550] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run62
[2019-03-27 14:14:38,570] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run62
[2019-03-27 14:14:38,594] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run62
[2019-03-27 14:14:38,618] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run62
[2019-03-27 14:14:38,639] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run62
[2019-03-27 14:14:42,171] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06029452], dtype=float32), 0.07197196]
[2019-03-27 14:14:42,176] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.66666666666666, 93.0, 1.0, 2.0, 0.2972232293653841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 474875.4220113505, 474875.4220113511, 165279.0807824007]
[2019-03-27 14:14:42,177] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:14:42,180] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.9314044e-36], sampled 0.05592376843010538
[2019-03-27 14:15:18,024] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06029452], dtype=float32), 0.07197196]
[2019-03-27 14:15:18,025] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.15379401, 56.404227125, 1.0, 2.0, 0.942987342651164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9128618820812, 1318062.445034348, 1318062.445034347, 282034.5908239344]
[2019-03-27 14:15:18,026] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:15:18,030] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.2650571e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8777303e-25], sampled 0.9813389255969397
[2019-03-27 14:15:28,396] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06029452], dtype=float32), 0.07197196]
[2019-03-27 14:15:28,396] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.66666666666667, 96.0, 1.0, 2.0, 0.3975217528705691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 594620.1196329258, 594620.1196329253, 174085.4609062469]
[2019-03-27 14:15:28,396] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:15:28,401] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4066117e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4534478e-31], sampled 0.3243762984824482
[2019-03-27 14:15:50,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06029452], dtype=float32), 0.07197196]
[2019-03-27 14:15:50,649] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 84.0, 1.0, 2.0, 0.5505290632624137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769304.3990912419, 769304.3990912419, 191891.2043528809]
[2019-03-27 14:15:50,649] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:15:50,654] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1059036e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.7187709e-29], sampled 0.6966877960030523
[2019-03-27 14:16:31,984] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06029452], dtype=float32), 0.07197196]
[2019-03-27 14:16:31,985] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.91666666666666, 74.83333333333333, 1.0, 2.0, 0.7168347689111441, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976334641037, 6.9112, 168.9123160395413, 1898686.858304625, 1831449.519506809, 388339.4064861836]
[2019-03-27 14:16:31,987] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:16:31,991] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0393672e-23 1.0000000e+00 6.4462968e-31 1.4311700e-33 4.9831339e-17], sampled 0.25856074451451105
[2019-03-27 14:16:31,992] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1898686.858304625 W.
[2019-03-27 14:16:44,554] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06029452], dtype=float32), 0.07197196]
[2019-03-27 14:16:44,554] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.2, 64.0, 1.0, 2.0, 0.51026688915263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789620.7519310823, 789620.7519310823, 194329.3141978598]
[2019-03-27 14:16:44,557] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:16:44,560] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4020992e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.4848740e-33], sampled 0.24836634767454302
[2019-03-27 14:16:46,685] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9447 3007732427.2347 1767.0000
[2019-03-27 14:16:46,744] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.2316 3164157316.7116 1779.0000
[2019-03-27 14:16:46,901] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6233 2927325603.3535 1340.0000
[2019-03-27 14:16:46,966] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.8549 2779239624.6829 934.0000
[2019-03-27 14:16:47,201] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1242 2842479692.9772 1131.0000
[2019-03-27 14:16:48,218] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1525000, evaluation results [1525000.0, 7883.231620325446, 3164157316.711623, 1779.0, 8254.623312201229, 2927325603.3534656, 1340.0, 8660.854935399948, 2779239624.6828623, 934.0, 7997.944701448608, 3007732427.2347236, 1767.0, 8496.124165032095, 2842479692.9772096, 1131.0]
[2019-03-27 14:16:50,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4779335e-36], sum to 1.0000
[2019-03-27 14:16:50,766] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7606
[2019-03-27 14:16:50,772] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 85.0, 1.0, 2.0, 0.3788689335744938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 569426.0801235304, 569426.0801235298, 171896.7726663494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1497600.0000, 
sim time next is 1498200.0000, 
raw observation next is [24.26666666666667, 82.83333333333334, 1.0, 2.0, 0.3788202469692281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 569188.2520316965, 569188.2520316971, 171870.3749591663], 
processed observation next is [0.0, 0.34782608695652173, 0.34913112164297017, 0.8283333333333335, 1.0, 1.0, 0.25159065899907, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15810784778658235, 0.15810784778658252, 0.2565229477002482], 
reward next is 0.7435, 
noisyNet noise sample is [array([-0.7878884], dtype=float32), 1.3094406]. 
=============================================
[2019-03-27 14:16:53,211] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:16:53,219] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4138
[2019-03-27 14:16:53,228] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 98.66666666666667, 1.0, 2.0, 0.3104452244773311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492129.4430516197, 492129.4430516197, 166464.6233860319], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1485600.0000, 
sim time next is 1486200.0000, 
raw observation next is [20.26666666666667, 98.83333333333333, 1.0, 2.0, 0.3089550858985022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490307.2586242497, 490307.2586242491, 166340.6201551674], 
processed observation next is [0.0, 0.17391304347826086, 0.15955766192733034, 0.9883333333333333, 1.0, 1.0, 0.1674157661427737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13619646072895825, 0.13619646072895808, 0.24826958232114535], 
reward next is 0.7517, 
noisyNet noise sample is [array([0.5766227], dtype=float32), -0.3203927]. 
=============================================
[2019-03-27 14:16:57,773] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2560962e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8806701e-26], sum to 1.0000
[2019-03-27 14:16:57,781] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5212
[2019-03-27 14:16:57,785] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.91666666666667, 85.00000000000001, 1.0, 2.0, 0.7486606651701452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1126843.687314722, 1126843.687314722, 242303.078706332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1599000.0000, 
sim time next is 1599600.0000, 
raw observation next is [23.93333333333333, 85.0, 1.0, 2.0, 0.6382923479185131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 960205.2470163631, 960205.2470163625, 216681.1652413676], 
processed observation next is [1.0, 0.5217391304347826, 0.3333333333333332, 0.85, 1.0, 1.0, 0.564207648094594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2667236797267675, 0.26672367972676736, 0.32340472424084715], 
reward next is 0.6766, 
noisyNet noise sample is [array([1.1981759], dtype=float32), 0.085652426]. 
=============================================
[2019-03-27 14:16:59,277] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0348472e-37], sum to 1.0000
[2019-03-27 14:16:59,285] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8815
[2019-03-27 14:16:59,291] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 89.0, 1.0, 2.0, 0.3058931738368764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483763.8062439788, 483763.8062439788, 165829.6271591172], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1573200.0000, 
sim time next is 1573800.0000, 
raw observation next is [21.7, 88.83333333333334, 1.0, 2.0, 0.3336543243877199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527088.794799645, 527088.794799645, 169094.8504072993], 
processed observation next is [1.0, 0.21739130434782608, 0.2274881516587678, 0.8883333333333334, 1.0, 1.0, 0.1971738848044818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14641355411101248, 0.14641355411101248, 0.25238037374223776], 
reward next is 0.7476, 
noisyNet noise sample is [array([-0.34403187], dtype=float32), -1.2625977]. 
=============================================
[2019-03-27 14:17:01,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1301068e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.5072033e-27], sum to 1.0000
[2019-03-27 14:17:01,242] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4143
[2019-03-27 14:17:01,250] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 85.0, 1.0, 2.0, 0.6123591816638826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 920756.4851794781, 920756.4851794788, 211193.4751485365], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1600200.0000, 
sim time next is 1600800.0000, 
raw observation next is [23.96666666666667, 85.0, 1.0, 2.0, 0.5557216071884554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835169.4369399251, 835169.4369399251, 200024.7237217609], 
processed observation next is [1.0, 0.5217391304347826, 0.33491311216429714, 0.85, 1.0, 1.0, 0.4647248279378981, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2319915102610903, 0.2319915102610903, 0.29854436376382226], 
reward next is 0.7015, 
noisyNet noise sample is [array([1.1627926], dtype=float32), 0.5445876]. 
=============================================
[2019-03-27 14:17:03,888] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9683276e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.2617422e-28], sum to 1.0000
[2019-03-27 14:17:03,900] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1002
[2019-03-27 14:17:03,907] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 98.0, 1.0, 2.0, 0.4204909772655581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 613788.3591621993, 613788.3591621986, 175444.6769027057], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1638600.0000, 
sim time next is 1639200.0000, 
raw observation next is [23.1, 98.0, 1.0, 2.0, 0.4208772223101874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614343.8698365949, 614343.8698365949, 175497.5859488033], 
processed observation next is [1.0, 1.0, 0.2938388625592418, 0.98, 1.0, 1.0, 0.3022617136267318, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1706510749546097, 0.1706510749546097, 0.26193669544597503], 
reward next is 0.7381, 
noisyNet noise sample is [array([2.507934], dtype=float32), 1.1612363]. 
=============================================
[2019-03-27 14:17:06,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.249418e-33], sum to 1.0000
[2019-03-27 14:17:06,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7016
[2019-03-27 14:17:06,203] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.56666666666667, 98.33333333333334, 1.0, 2.0, 0.4781503930941028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683549.6838074777, 683549.6838074777, 182215.226587118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1665600.0000, 
sim time next is 1666200.0000, 
raw observation next is [23.58333333333333, 98.16666666666667, 1.0, 2.0, 0.4773386055273855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682501.2686907761, 682501.2686907767, 182104.4983785113], 
processed observation next is [1.0, 0.2608695652173913, 0.31674565560821466, 0.9816666666666667, 1.0, 1.0, 0.3702874765390186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1895836857474378, 0.18958368574743797, 0.2717977587738975], 
reward next is 0.7282, 
noisyNet noise sample is [array([1.5391517], dtype=float32), 1.5631262]. 
=============================================
[2019-03-27 14:17:06,896] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.3262882e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.9611292e-33], sum to 1.0000
[2019-03-27 14:17:06,908] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2410
[2019-03-27 14:17:06,917] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 94.66666666666667, 1.0, 2.0, 0.4337006350218194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 630318.5950626275, 630318.5950626275, 176973.4965908111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1916400.0000, 
sim time next is 1917000.0000, 
raw observation next is [23.55, 94.5, 1.0, 2.0, 0.4325988030863935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 630478.0488462491, 630478.0488462498, 177035.4536174381], 
processed observation next is [1.0, 0.17391304347826086, 0.3151658767772513, 0.945, 1.0, 1.0, 0.3163841001040886, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17513279134618032, 0.1751327913461805, 0.26423202032453447], 
reward next is 0.7358, 
noisyNet noise sample is [array([0.31265247], dtype=float32), 1.7838663]. 
=============================================
[2019-03-27 14:17:06,931] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[69.986465]
 [70.012215]
 [70.09486 ]
 [70.103195]
 [70.073074]], R is [[70.01903534]
 [70.05471039]
 [70.08564758]
 [70.12275696]
 [70.15926361]].
[2019-03-27 14:17:10,942] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6104754e-32], sum to 1.0000
[2019-03-27 14:17:10,953] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0218
[2019-03-27 14:17:10,961] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 87.0, 1.0, 2.0, 0.4928728000745627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688709.9329947385, 688709.9329947378, 182486.8250618643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1881600.0000, 
sim time next is 1882200.0000, 
raw observation next is [25.98333333333333, 87.0, 1.0, 2.0, 0.4892254776866197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683611.7506314571, 683611.7506314571, 181925.0582542246], 
processed observation next is [1.0, 0.782608695652174, 0.43048973143759867, 0.87, 1.0, 1.0, 0.38460900926098757, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1898921529531825, 0.1898921529531825, 0.27152993769287254], 
reward next is 0.7285, 
noisyNet noise sample is [array([-0.01970612], dtype=float32), -0.7872812]. 
=============================================
[2019-03-27 14:17:11,740] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.1005171e-24 1.0000000e+00 6.3448069e-31 8.2166366e-34 6.3764925e-17], sum to 1.0000
[2019-03-27 14:17:11,748] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6181
[2019-03-27 14:17:11,754] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 79.66666666666667, 1.0, 2.0, 0.5863283081831752, 1.0, 2.0, 0.5863283081831752, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1639318.812844872, 1639318.812844872, 329243.011873895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1956000.0000, 
sim time next is 1956600.0000, 
raw observation next is [26.1, 80.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.89414560013482, 6.9112, 168.9074643451342, 2165420.982202053, 1468108.216788543, 313576.4952546674], 
processed observation next is [1.0, 0.6521739130434783, 0.4360189573459717, 0.805, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.098294560013482, 0.0, 0.8294129761063712, 0.6015058283894592, 0.40780783799681747, 0.4680246197830857], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4497265], dtype=float32), -1.4558796]. 
=============================================
[2019-03-27 14:17:14,475] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0520767e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2443911e-26], sum to 1.0000
[2019-03-27 14:17:14,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1005
[2019-03-27 14:17:14,495] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 85.16666666666667, 1.0, 2.0, 0.886336154583685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1238831.934994465, 1238831.934994465, 266203.9479851646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1860600.0000, 
sim time next is 1861200.0000, 
raw observation next is [26.5, 85.0, 1.0, 2.0, 0.7470040219412213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1043991.693378904, 1043991.693378904, 231315.5883645196], 
processed observation next is [1.0, 0.5652173913043478, 0.4549763033175356, 0.85, 1.0, 1.0, 0.695185568603881, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2899976926052511, 0.2899976926052511, 0.3452471468127158], 
reward next is 0.6548, 
noisyNet noise sample is [array([0.99111134], dtype=float32), -0.06756564]. 
=============================================
[2019-03-27 14:17:19,480] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0548616e-15 1.0000000e+00 1.2476999e-23 2.0320290e-21 1.5223379e-09], sum to 1.0000
[2019-03-27 14:17:19,491] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3358
[2019-03-27 14:17:19,501] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1840330.622589397 W.
[2019-03-27 14:17:19,507] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 85.83333333333334, 1.0, 2.0, 0.4387744523079474, 1.0, 2.0, 0.4387744523079474, 1.0, 2.0, 0.7515775054771439, 6.9112, 6.9112, 170.5573041426782, 1840330.622589397, 1840330.622589397, 373756.1613179129], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1871400.0000, 
sim time next is 1872000.0000, 
raw observation next is [27.0, 86.0, 1.0, 2.0, 0.4196032361297646, 1.0, 2.0, 0.4196032361297646, 1.0, 2.0, 0.7187024558420707, 6.9112, 6.9112, 170.5573041426782, 1759855.679064514, 1759855.679064514, 362515.8708868382], 
processed observation next is [1.0, 0.6956521739130435, 0.4786729857819906, 0.86, 1.0, 1.0, 0.30072679051778867, 1.0, 1.0, 0.30072679051778867, 1.0, 1.0, 0.6569542144415497, 0.0, 0.0, 0.8375144448122397, 0.48884879974014284, 0.48884879974014284, 0.5410684640102063], 
reward next is 0.4589, 
noisyNet noise sample is [array([-0.27544534], dtype=float32), 0.33639738]. 
=============================================
[2019-03-27 14:17:19,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[59.508137]
 [59.38355 ]
 [61.247093]
 [60.608536]
 [59.246174]], R is [[60.72557449]
 [60.5604744 ]
 [59.95487213]
 [59.35532379]
 [58.76177216]].
[2019-03-27 14:17:23,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1052139e-33], sum to 1.0000
[2019-03-27 14:17:23,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4246
[2019-03-27 14:17:23,726] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.0, 1.0, 2.0, 0.5033578552556454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703365.9504080118, 703365.9504080118, 184124.0527852743], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2025600.0000, 
sim time next is 2026200.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5029984965547285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 702863.6352198946, 702863.6352198952, 184067.4316723697], 
processed observation next is [0.0, 0.43478260869565216, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4012030078972632, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19523989867219294, 0.1952398986721931, 0.2747275099587608], 
reward next is 0.7253, 
noisyNet noise sample is [array([-1.8529638], dtype=float32), 0.166284]. 
=============================================
[2019-03-27 14:17:27,496] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.1144643e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1903596e-26], sum to 1.0000
[2019-03-27 14:17:27,507] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8006
[2019-03-27 14:17:27,512] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333333, 93.83333333333334, 1.0, 2.0, 0.5110489068622259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714116.6362485503, 714116.6362485496, 185345.2931599961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2163000.0000, 
sim time next is 2163600.0000, 
raw observation next is [25.6, 94.0, 1.0, 2.0, 0.5105656148420609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713441.0790447135, 713441.0790447135, 185268.0269526198], 
processed observation next is [1.0, 0.043478260869565216, 0.4123222748815167, 0.94, 1.0, 1.0, 0.410320017882001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19817807751242042, 0.19817807751242042, 0.27651944321286537], 
reward next is 0.7235, 
noisyNet noise sample is [array([-1.1335598], dtype=float32), -1.0721363]. 
=============================================
[2019-03-27 14:17:28,186] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8648305e-35], sum to 1.0000
[2019-03-27 14:17:28,195] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8933
[2019-03-27 14:17:28,204] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333334, 97.83333333333334, 1.0, 2.0, 0.4552583366142032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644983.9256269227, 644983.9256269227, 178025.2266158016], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2091000.0000, 
sim time next is 2091600.0000, 
raw observation next is [23.8, 98.0, 1.0, 2.0, 0.4545531477589509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 644345.0983357441, 644345.0983357447, 177968.8180014287], 
processed observation next is [0.0, 0.21739130434782608, 0.3270142180094788, 0.98, 1.0, 1.0, 0.3428351177818686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1789847495377067, 0.17898474953770685, 0.2656251014946697], 
reward next is 0.7344, 
noisyNet noise sample is [array([-0.07046439], dtype=float32), 0.16754751]. 
=============================================
[2019-03-27 14:17:30,644] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.92784547e-33 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.20211686e-26], sum to 1.0000
[2019-03-27 14:17:30,653] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8109
[2019-03-27 14:17:30,660] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 94.33333333333334, 1.0, 2.0, 1.001253187031836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1399557.328591492, 1399557.328591492, 299305.3428096813], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2168400.0000, 
sim time next is 2169000.0000, 
raw observation next is [25.15, 94.5, 1.0, 2.0, 0.8989701569902202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1256500.92769338, 1256500.92769338, 269651.8516837089], 
processed observation next is [1.0, 0.08695652173913043, 0.3909952606635071, 0.945, 1.0, 1.0, 0.8782772975785785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34902803547038336, 0.34902803547038336, 0.4024654502741924], 
reward next is 0.5975, 
noisyNet noise sample is [array([0.79579186], dtype=float32), 1.026333]. 
=============================================
[2019-03-27 14:17:30,681] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[67.64918 ]
 [67.91398 ]
 [69.24808 ]
 [68.70334 ]
 [68.786804]], R is [[67.37478638]
 [67.25431824]
 [67.12162018]
 [67.17481995]
 [67.22756195]].
[2019-03-27 14:17:45,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6886938e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5772964e-25], sum to 1.0000
[2019-03-27 14:17:45,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1635
[2019-03-27 14:17:45,581] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.05, 81.0, 1.0, 2.0, 0.5323853369376412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 743941.6393067535, 743941.6393067541, 188825.6811382481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2337000.0000, 
sim time next is 2337600.0000, 
raw observation next is [28.0, 81.0, 1.0, 2.0, 0.5300839181191508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740724.5742801601, 740724.5742801601, 188443.8136541813], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.81, 1.0, 1.0, 0.4338360459266877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20575682618893337, 0.20575682618893337, 0.2812594233644497], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.10312746], dtype=float32), -1.042285]. 
=============================================
[2019-03-27 14:17:49,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5034535e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2395071e-23], sum to 1.0000
[2019-03-27 14:17:49,290] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8467
[2019-03-27 14:17:49,295] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333334, 81.0, 1.0, 2.0, 0.5402690607811881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754962.0681075352, 754962.0681075352, 190145.7248019612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2334000.0000, 
sim time next is 2334600.0000, 
raw observation next is [28.2, 81.0, 1.0, 2.0, 0.5387324590898745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752814.0882196674, 752814.0882196674, 189886.9997333332], 
processed observation next is [1.0, 0.0, 0.5355450236966824, 0.81, 1.0, 1.0, 0.44425597480707774, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20911502450546318, 0.20911502450546318, 0.28341343243781075], 
reward next is 0.7166, 
noisyNet noise sample is [array([1.6624824], dtype=float32), 3.054427]. 
=============================================
[2019-03-27 14:17:54,320] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 14:17:54,322] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:17:54,325] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:17:54,325] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:17:54,326] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:17:54,330] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:17:54,331] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:17:54,332] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:17:54,333] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:17:54,329] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:17:54,335] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:17:54,371] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run63
[2019-03-27 14:17:54,394] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run63
[2019-03-27 14:17:54,395] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run63
[2019-03-27 14:17:54,396] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run63
[2019-03-27 14:17:54,468] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run63
[2019-03-27 14:18:01,482] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06011126], dtype=float32), 0.07252945]
[2019-03-27 14:18:01,484] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.9, 81.33333333333333, 1.0, 2.0, 0.2632798901781312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 429699.1839963064, 429699.1839963064, 162208.926143423]
[2019-03-27 14:18:01,484] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:18:01,489] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2719204e-38], sampled 0.50326304805572
[2019-03-27 14:18:15,210] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06011126], dtype=float32), 0.07252945]
[2019-03-27 14:18:15,213] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.85, 92.5, 1.0, 2.0, 0.3702202436027319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575654.3116356224, 575654.311635623, 172952.9782742802]
[2019-03-27 14:18:15,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:18:15,219] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4574978e-36], sampled 0.2546236842651185
[2019-03-27 14:19:09,639] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06011126], dtype=float32), 0.07252945]
[2019-03-27 14:19:09,640] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.58333333333333, 46.16666666666666, 1.0, 2.0, 0.5246076150363802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733069.4995840222, 733069.4995840222, 187540.685275473]
[2019-03-27 14:19:09,645] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:19:09,649] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.0559423e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4208858e-29], sampled 0.39475369724243947
[2019-03-27 14:19:12,550] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06011126], dtype=float32), 0.07252945]
[2019-03-27 14:19:12,551] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.78351781, 70.17194065333334, 1.0, 2.0, 0.5779978976635963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 807703.6973859334, 807703.6973859327, 196719.9038064089]
[2019-03-27 14:19:12,553] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:19:12,557] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2031982e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1815550e-28], sampled 0.14140963450637167
[2019-03-27 14:19:55,927] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06011126], dtype=float32), 0.07252945]
[2019-03-27 14:19:55,928] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.3525815, 94.90745888999999, 1.0, 2.0, 0.4570371248568965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 647951.7958155609, 647951.7958155609, 178342.3509532796]
[2019-03-27 14:19:55,931] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:19:55,933] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.3412350e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8855932e-31], sampled 0.6550581920397186
[2019-03-27 14:20:02,966] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.6979 2842514172.0868 1153.0000
[2019-03-27 14:20:03,037] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.6964 2927493388.2390 1350.0000
[2019-03-27 14:20:03,115] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.0890 3007579297.7484 1766.0000
[2019-03-27 14:20:03,175] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.4214 3164180174.0061 1807.0000
[2019-03-27 14:20:03,461] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.3865 2779524983.6856 937.0000
[2019-03-27 14:20:04,478] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1550000, evaluation results [1550000.0, 7881.421449720981, 3164180174.0060863, 1807.0, 8251.69642916078, 2927493388.2389956, 1350.0, 8657.38647672303, 2779524983.685566, 937.0, 8001.089038581124, 3007579297.748389, 1766.0, 8493.697888547767, 2842514172.0868425, 1153.0]
[2019-03-27 14:20:05,992] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7781822e-09 3.8950842e-02 6.2995169e-16 3.1307394e-09 9.6104914e-01], sum to 1.0000
[2019-03-27 14:20:06,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4783
[2019-03-27 14:20:06,011] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 68.0, 1.0, 2.0, 0.5582659957015773, 1.0, 2.0, 0.5582659957015773, 1.0, 2.0, 0.9665960158019332, 6.9112, 6.9112, 170.5573041426782, 2342035.925709513, 2342035.925709513, 457179.5580782956], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2556000.0000, 
sim time next is 2556600.0000, 
raw observation next is [30.41666666666667, 68.33333333333334, 1.0, 2.0, 0.4384495105044696, 1.0, 2.0, 0.4384495105044696, 1.0, 2.0, 0.7574566184506514, 6.9112, 6.9112, 170.5573041426782, 1838966.565223231, 1838966.565223231, 374574.0804577553], 
processed observation next is [1.0, 0.6086956521739131, 0.6406003159557664, 0.6833333333333335, 1.0, 1.0, 0.323433145186108, 1.0, 1.0, 0.323433145186108, 1.0, 1.0, 0.7042153883544527, 0.0, 0.0, 0.8375144448122397, 0.5108240458953419, 0.5108240458953419, 0.559065791727993], 
reward next is 0.4409, 
noisyNet noise sample is [array([-0.16464268], dtype=float32), 1.311437]. 
=============================================
[2019-03-27 14:20:07,821] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8331418e-34], sum to 1.0000
[2019-03-27 14:20:07,830] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0938
[2019-03-27 14:20:07,841] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.47695265418527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666584.9123931888, 666584.9123931894, 180079.941457943], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2630400.0000, 
sim time next is 2631000.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4760064844815962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665262.5215583926, 665262.5215583926, 179938.3610314093], 
processed observation next is [0.0, 0.43478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.36868251142360986, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18479514487733126, 0.18479514487733126, 0.2685647179573273], 
reward next is 0.7314, 
noisyNet noise sample is [array([-0.9298287], dtype=float32), 0.5810579]. 
=============================================
[2019-03-27 14:20:07,868] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.916794]
 [77.89449 ]
 [77.8786  ]
 [77.84894 ]
 [77.79747 ]], R is [[77.86281586]
 [77.81541443]
 [77.76846313]
 [77.72203064]
 [77.67622375]].
[2019-03-27 14:20:19,411] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2519047e-31], sum to 1.0000
[2019-03-27 14:20:19,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4178
[2019-03-27 14:20:19,432] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 82.33333333333334, 1.0, 2.0, 0.495270425329538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692061.3174084985, 692061.3174084985, 182858.6997948849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2637600.0000, 
sim time next is 2638200.0000, 
raw observation next is [27.0, 83.16666666666666, 1.0, 2.0, 0.499595863403963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698107.4123252076, 698107.4123252076, 183533.1453478362], 
processed observation next is [0.0, 0.5217391304347826, 0.4786729857819906, 0.8316666666666666, 1.0, 1.0, 0.39710344988429275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.193918725645891, 0.193918725645891, 0.2739300676833376], 
reward next is 0.7261, 
noisyNet noise sample is [array([-0.5342485], dtype=float32), 0.7071759]. 
=============================================
[2019-03-27 14:20:25,263] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4262323e-33], sum to 1.0000
[2019-03-27 14:20:25,272] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8540
[2019-03-27 14:20:25,278] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666667, 89.0, 1.0, 2.0, 0.4486449415494924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642721.1019697809, 642721.1019697809, 177975.758107507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2658000.0000, 
sim time next is 2658600.0000, 
raw observation next is [24.5, 89.0, 1.0, 2.0, 0.4406684354096254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635592.2341938505, 635592.2341938505, 177369.741876003], 
processed observation next is [0.0, 0.782608695652174, 0.3601895734597157, 0.89, 1.0, 1.0, 0.3261065486862956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1765533983871807, 0.1765533983871807, 0.2647309580238851], 
reward next is 0.7353, 
noisyNet noise sample is [array([0.44912878], dtype=float32), -0.26168165]. 
=============================================
[2019-03-27 14:20:29,821] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5347561e-34], sum to 1.0000
[2019-03-27 14:20:29,830] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2445
[2019-03-27 14:20:29,834] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 96.0, 1.0, 2.0, 0.3134946486893489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 496952.0609642697, 496952.0609642704, 166820.8496323185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2928000.0000, 
sim time next is 2928600.0000, 
raw observation next is [20.75, 95.5, 1.0, 2.0, 0.3147153254597316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498600.5081672747, 498600.5081672741, 166938.3623574729], 
processed observation next is [1.0, 0.9130434782608695, 0.18246445497630337, 0.955, 1.0, 1.0, 0.1743558138069055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1385001411575763, 0.13850014115757614, 0.24916173486189988], 
reward next is 0.7508, 
noisyNet noise sample is [array([-0.16917595], dtype=float32), -0.41950703]. 
=============================================
[2019-03-27 14:20:53,133] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1162377e-38], sum to 1.0000
[2019-03-27 14:20:53,145] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2957
[2019-03-27 14:20:53,153] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3329359902532384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 517112.201823252, 517112.201823252, 168106.1988458252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3043200.0000, 
sim time next is 3043800.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.331634081581932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515089.818378539, 515089.818378539, 167947.7659570396], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 1.0, 1.0, 1.0, 0.19473985732762894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14308050510514972, 0.14308050510514972, 0.2506683073985666], 
reward next is 0.7493, 
noisyNet noise sample is [array([-0.39134762], dtype=float32), -0.2622287]. 
=============================================
[2019-03-27 14:21:04,619] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.4326302e-11 3.2364279e-01 5.4439320e-18 6.0606195e-12 6.7635727e-01], sum to 1.0000
[2019-03-27 14:21:04,631] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2280
[2019-03-27 14:21:04,645] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2439758.168643538 W.
[2019-03-27 14:21:04,653] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.00000000000001, 1.0, 2.0, 0.581537116448517, 1.0, 2.0, 0.581537116448517, 1.0, 2.0, 1.009937654007884, 6.9112, 6.9112, 170.5573041426782, 2439758.168643538, 2439758.168643538, 476109.2002229158], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3507600.0000, 
sim time next is 3508200.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.8782843190843511, 1.0, 2.0, 0.8782843190843511, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2456496.319660246, 2456496.319660246, 459787.3161821532], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.63, 1.0, 1.0, 0.8533546013064471, 1.0, 1.0, 0.8533546013064471, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6823600887945127, 0.6823600887945127, 0.6862497256450047], 
reward next is 0.3138, 
noisyNet noise sample is [array([-1.8441356], dtype=float32), 2.2412004]. 
=============================================
[2019-03-27 14:21:06,054] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5599116e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6986162e-23], sum to 1.0000
[2019-03-27 14:21:06,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4321
[2019-03-27 14:21:06,077] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 89.0, 1.0, 2.0, 0.8138384672348753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1137447.650540901, 1137447.650540901, 247337.8339284521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3393600.0000, 
sim time next is 3394200.0000, 
raw observation next is [27.83333333333334, 89.0, 1.0, 2.0, 0.8402982449068611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1174449.165960978, 1174449.165960978, 254045.2440379111], 
processed observation next is [1.0, 0.2608695652173913, 0.5181674565560824, 0.89, 1.0, 1.0, 0.8075882468757363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.326235879433605, 0.326235879433605, 0.37917200602673296], 
reward next is 0.6208, 
noisyNet noise sample is [array([-0.18782191], dtype=float32), 1.0286012]. 
=============================================
[2019-03-27 14:21:10,275] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 14:21:10,276] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:21:10,277] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:21:10,278] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:21:10,279] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:21:10,280] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:21:10,282] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:21:10,283] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:21:10,284] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:21:10,285] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:21:10,286] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:21:10,316] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run64
[2019-03-27 14:21:10,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run64
[2019-03-27 14:21:10,362] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run64
[2019-03-27 14:21:10,363] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run64
[2019-03-27 14:21:10,401] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run64
[2019-03-27 14:21:18,688] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:21:18,692] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.63333333333333, 80.33333333333334, 1.0, 2.0, 0.434466677901905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 631023.4700198655, 631023.4700198655, 177032.2860272246]
[2019-03-27 14:21:18,693] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:21:18,697] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.604366e-33], sampled 0.3391330731947494
[2019-03-27 14:21:21,360] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:21:21,361] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.26666666666667, 69.0, 1.0, 2.0, 0.3479180036303062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539398.8269698924, 539398.826969893, 169864.38041023]
[2019-03-27 14:21:21,364] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:21:21,369] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1862136e-34], sampled 0.6774344692223184
[2019-03-27 14:21:22,376] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:21:22,377] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.94235694, 92.26466529000001, 1.0, 2.0, 0.2431474287534471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403487.0033266261, 403487.0033266261, 160015.3885939264]
[2019-03-27 14:21:22,379] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:21:22,382] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0488523e-33], sampled 0.26435124088989004
[2019-03-27 14:21:24,236] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:21:24,236] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.53333333333333, 89.33333333333334, 1.0, 2.0, 0.4514127596668465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 649877.0614350522, 649877.0614350528, 178782.2246448968]
[2019-03-27 14:21:24,239] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:21:24,243] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.3149847e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2364944e-30], sampled 0.30963153175116565
[2019-03-27 14:21:31,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:21:31,317] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.8, 76.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.991351862770625, 6.9112, 168.9013010618802, 2930323.300774876, 1454693.805456217, 309296.5358793141]
[2019-03-27 14:21:31,320] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:21:31,325] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.2374941e-22 1.0000000e+00 7.2900007e-30 8.1181125e-29 9.6055369e-13], sampled 0.5413573447829259
[2019-03-27 14:21:31,326] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2930323.300774876 W.
[2019-03-27 14:21:36,144] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:21:36,144] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.25, 78.5, 1.0, 2.0, 0.3277190167609265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515491.8813197218, 515491.8813197225, 168148.8278307004]
[2019-03-27 14:21:36,146] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:21:36,151] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.7180797e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5535997e-30], sampled 0.5277836433161275
[2019-03-27 14:22:02,495] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:22:02,496] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 89.0, 1.0, 2.0, 0.4577248384834284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 648075.29979201, 648075.2997920093, 178333.5326944034]
[2019-03-27 14:22:02,497] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:22:02,500] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6609916e-33], sampled 0.7111919095067937
[2019-03-27 14:22:28,912] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:22:28,912] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.39928922, 73.893556485, 1.0, 2.0, 0.4283551581089671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 627429.2617895713, 627429.2617895707, 176819.781422221]
[2019-03-27 14:22:28,913] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:22:28,917] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.051436e-36 1.000000e+00 0.000000e+00 0.000000e+00 6.103706e-28], sampled 0.48259453767879756
[2019-03-27 14:22:39,060] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:22:39,061] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.07534053, 89.31151931, 1.0, 2.0, 0.6429141537873155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 898457.1019450418, 898457.1019450411, 209014.4902766231]
[2019-03-27 14:22:39,061] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:22:39,064] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3528972e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3734390e-26], sampled 0.8313235563324404
[2019-03-27 14:22:56,436] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05911728], dtype=float32), 0.073882505]
[2019-03-27 14:22:56,438] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.16666666666666, 84.16666666666667, 1.0, 2.0, 0.5627186753932463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 786344.3593344021, 786344.3593344014, 194005.2614561505]
[2019-03-27 14:22:56,440] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:22:56,445] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8574124e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3232835e-26], sampled 0.12750470192759011
[2019-03-27 14:23:18,049] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7891.8411 3163438074.1704 1832.0000
[2019-03-27 14:23:18,475] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8248.4067 2928125997.1199 1367.0000
[2019-03-27 14:23:18,539] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8017.2731 3006815041.0880 1721.0000
[2019-03-27 14:23:18,622] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.1387 2842242806.7834 1145.0000
[2019-03-27 14:23:18,709] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8666.0646 2779272405.9767 926.0000
[2019-03-27 14:23:19,725] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1575000, evaluation results [1575000.0, 7891.841083800223, 3163438074.1704, 1832.0, 8248.406696430375, 2928125997.1199265, 1367.0, 8666.064571185949, 2779272405.9767203, 926.0, 8017.273083645838, 3006815041.08795, 1721.0, 8498.138668109556, 2842242806.7833886, 1145.0]
[2019-03-27 14:23:21,850] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0694636e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3433938e-25], sum to 1.0000
[2019-03-27 14:23:21,863] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3736
[2019-03-27 14:23:21,867] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7268589847660603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1015824.093397069, 1015824.093397069, 226746.1007951709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3553200.0000, 
sim time next is 3553800.0000, 
raw observation next is [26.91666666666666, 79.33333333333334, 1.0, 2.0, 0.7438852858019134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1039630.902383026, 1039630.902383027, 230599.472486983], 
processed observation next is [1.0, 0.13043478260869565, 0.4747235387045811, 0.7933333333333334, 1.0, 1.0, 0.6914280551830282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28878636177306277, 0.28878636177306305, 0.3441783171447507], 
reward next is 0.6558, 
noisyNet noise sample is [array([-1.494054], dtype=float32), 0.2473541]. 
=============================================
[2019-03-27 14:23:22,388] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.1744359e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9133027e-28], sum to 1.0000
[2019-03-27 14:23:22,403] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2833
[2019-03-27 14:23:22,410] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.573834901793472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801884.0630500738, 801884.0630500738, 195973.3259548839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3328800.0000, 
sim time next is 3329400.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5740616382238679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 802201.0270199444, 802201.0270199438, 196013.8027636713], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.48682125087212996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2228336186166512, 0.22283361861665105, 0.29255791457264374], 
reward next is 0.7074, 
noisyNet noise sample is [array([0.46811423], dtype=float32), -2.077296]. 
=============================================
[2019-03-27 14:23:24,043] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0011910e-08 5.1964834e-02 2.2944869e-15 6.2583880e-09 9.4803512e-01], sum to 1.0000
[2019-03-27 14:23:24,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3197
[2019-03-27 14:23:24,059] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 72.5, 1.0, 2.0, 0.569133891319692, 1.0, 2.0, 0.569133891319692, 1.0, 2.0, 0.9883973537683448, 6.9112, 6.9112, 170.5573041426782, 2387672.447233525, 2387672.447233525, 466244.9892708113], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3403800.0000, 
sim time next is 3404400.0000, 
raw observation next is [30.66666666666666, 71.66666666666667, 1.0, 2.0, 0.5165709133792777, 1.0, 2.0, 0.5165709133792777, 1.0, 2.0, 0.8971128439282756, 6.911199999999999, 6.9112, 170.5573041426782, 2166958.588864124, 2166958.588864125, 426794.3517357994], 
processed observation next is [1.0, 0.391304347826087, 0.6524486571879934, 0.7166666666666667, 1.0, 1.0, 0.41755531732443096, 1.0, 1.0, 0.41755531732443096, 1.0, 1.0, 0.8745278584491166, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6019329413511455, 0.6019329413511458, 0.6370064951280588], 
reward next is 0.3630, 
noisyNet noise sample is [array([-0.34906185], dtype=float32), -1.4081337]. 
=============================================
[2019-03-27 14:23:24,748] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0953494e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6116568e-28], sum to 1.0000
[2019-03-27 14:23:24,758] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6760
[2019-03-27 14:23:24,762] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5373605047599435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750896.2687323533, 750896.2687323539, 189656.6609110289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3365400.0000, 
sim time next is 3366000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5371847715771958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750650.6161097618, 750650.6161097625, 189627.1977992605], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 1.0, 1.0, 0.44239129105686237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2085140600304894, 0.20851406003048958, 0.28302566835710524], 
reward next is 0.7170, 
noisyNet noise sample is [array([-1.6473038], dtype=float32), 2.0294247]. 
=============================================
[2019-03-27 14:23:24,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.56984 ]
 [77.53197 ]
 [77.49904 ]
 [77.456154]
 [77.39809 ]], R is [[77.69400024]
 [77.63398743]
 [77.57447815]
 [77.51544189]
 [77.45703888]].
[2019-03-27 14:23:25,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1906463e-35], sum to 1.0000
[2019-03-27 14:23:25,051] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9319081e-15 9.9999988e-01 6.4958903e-23 5.0956011e-19 1.4245273e-07], sum to 1.0000
[2019-03-27 14:23:25,054] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7517
[2019-03-27 14:23:25,061] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 72.66666666666666, 1.0, 2.0, 0.5334975255055133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745496.3285065243, 745496.3285065236, 189011.3577720588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3436800.0000, 
sim time next is 3437400.0000, 
raw observation next is [29.33333333333333, 73.33333333333334, 1.0, 2.0, 0.5277161059006845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737414.7106371127, 737414.7106371133, 188052.9137473982], 
processed observation next is [1.0, 0.782608695652174, 0.5892575039494469, 0.7333333333333334, 1.0, 1.0, 0.4309832601213066, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2048374196214202, 0.20483741962142035, 0.2806759906677585], 
reward next is 0.7193, 
noisyNet noise sample is [array([-1.1758649], dtype=float32), 0.6738143]. 
=============================================
[2019-03-27 14:23:25,064] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9375
[2019-03-27 14:23:25,075] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1972482.77240912 W.
[2019-03-27 14:23:25,084] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7053800911338828, 1.0, 1.0, 0.7053800911338828, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1972482.77240912, 1972482.77240912, 376436.0382467785], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3578400.0000, 
sim time next is 3579000.0000, 
raw observation next is [31.0, 66.66666666666667, 1.0, 2.0, 0.4477829113966598, 1.0, 2.0, 0.4477829113966598, 1.0, 1.0, 0.7753509793459025, 6.9112, 6.9112, 170.5573041426782, 1878147.485549668, 1878147.485549668, 380550.0747325812], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.6666666666666667, 1.0, 1.0, 0.33467820650199975, 1.0, 1.0, 0.33467820650199975, 1.0, 0.5, 0.7260377796901248, 0.0, 0.0, 0.8375144448122397, 0.5217076348749078, 0.5217076348749078, 0.5679851861680316], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2515152], dtype=float32), 1.538197]. 
=============================================
[2019-03-27 14:23:25,108] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[54.828144]
 [54.91536 ]
 [54.07344 ]
 [52.98462 ]
 [52.172173]], R is [[53.80885696]
 [53.70892334]
 [53.17415237]
 [52.64241028]
 [52.11598587]].
[2019-03-27 14:23:32,466] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0185444e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2894440e-32], sum to 1.0000
[2019-03-27 14:23:32,479] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6278
[2019-03-27 14:23:32,484] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6191672080767281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865257.7972965139, 865257.7972965139, 204361.709972923], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3473400.0000, 
sim time next is 3474000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6176968887537886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863202.2584671223, 863202.2584671223, 204079.8605584736], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5393938418720344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2397784051297562, 0.2397784051297562, 0.30459680680369194], 
reward next is 0.6954, 
noisyNet noise sample is [array([0.68342435], dtype=float32), 0.091578774]. 
=============================================
[2019-03-27 14:23:32,498] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[62.743095]
 [62.66852 ]
 [62.66566 ]
 [62.723026]
 [62.652363]], R is [[62.771492  ]
 [62.83876038]
 [62.90303802]
 [62.96212387]
 [63.02024841]].
[2019-03-27 14:23:38,791] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.37332188e-33 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.00154334e-28], sum to 1.0000
[2019-03-27 14:23:38,803] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2624
[2019-03-27 14:23:38,808] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 79.00000000000001, 1.0, 2.0, 0.7646196907920382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1068623.241328951, 1068623.241328951, 235408.5105545231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3564600.0000, 
sim time next is 3565200.0000, 
raw observation next is [27.33333333333334, 79.0, 1.0, 2.0, 0.7723456710232763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1079426.465963131, 1079426.465963132, 237233.2042361728], 
processed observation next is [1.0, 0.2608695652173913, 0.4944707740916275, 0.79, 1.0, 1.0, 0.7257176759316583, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29984068498975863, 0.2998406849897589, 0.3540794093077206], 
reward next is 0.6459, 
noisyNet noise sample is [array([1.0520872], dtype=float32), 0.51486355]. 
=============================================
[2019-03-27 14:23:40,358] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3866424e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6761618e-28], sum to 1.0000
[2019-03-27 14:23:40,371] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1329
[2019-03-27 14:23:40,379] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6748815702130709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 943150.6355299543, 943150.6355299538, 215513.4039092174], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3643800.0000, 
sim time next is 3644400.0000, 
raw observation next is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6450945540044076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 901505.4521294803, 901505.4521294803, 209436.7986959239], 
processed observation next is [1.0, 0.17391304347826086, 0.4628751974723541, 0.8066666666666668, 1.0, 1.0, 0.5724030771137442, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25041818114707787, 0.25041818114707787, 0.3125922368595879], 
reward next is 0.6874, 
noisyNet noise sample is [array([-1.0195122], dtype=float32), -2.001283]. 
=============================================
[2019-03-27 14:23:45,599] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8556336e-33], sum to 1.0000
[2019-03-27 14:23:45,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8761
[2019-03-27 14:23:45,615] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 76.5, 1.0, 2.0, 0.5105657294396888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713441.2392319824, 713441.2392319817, 185267.6695008594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3706200.0000, 
sim time next is 3706800.0000, 
raw observation next is [28.0, 75.66666666666666, 1.0, 2.0, 0.5054199822890488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706248.4168026005, 706248.4168026012, 184449.2617156579], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.7566666666666666, 1.0, 1.0, 0.4041204605892154, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19618011577850014, 0.19618011577850034, 0.27529740554575804], 
reward next is 0.7247, 
noisyNet noise sample is [array([0.8739672], dtype=float32), -0.47409293]. 
=============================================
[2019-03-27 14:23:46,875] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6993564e-35], sum to 1.0000
[2019-03-27 14:23:46,884] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1916
[2019-03-27 14:23:46,896] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 66.16666666666666, 1.0, 2.0, 0.5418233254587831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757134.7430933043, 757134.7430933043, 190408.9722613195], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3783000.0000, 
sim time next is 3783600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5365075531008595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749703.9514058627, 749703.9514058627, 189514.2663557047], 
processed observation next is [1.0, 0.8260869565217391, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44157536518175844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20825109761273963, 0.20825109761273963, 0.28285711396373836], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.07282353], dtype=float32), 0.17727114]. 
=============================================
[2019-03-27 14:23:55,961] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.674591e-36], sum to 1.0000
[2019-03-27 14:23:55,973] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5480
[2019-03-27 14:23:55,979] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 55.0, 1.0, 2.0, 0.5874360757035686, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820897.8581129654, 820897.8581129654, 198429.3402831675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3859200.0000, 
sim time next is 3859800.0000, 
raw observation next is [35.0, 54.66666666666667, 1.0, 2.0, 0.603812365442395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843791.5849227291, 843791.5849227291, 201457.7535669962], 
processed observation next is [0.0, 0.6956521739130435, 0.8578199052132701, 0.5466666666666667, 1.0, 1.0, 0.5226655005330061, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23438655136742473, 0.23438655136742473, 0.3006832142790988], 
reward next is 0.6993, 
noisyNet noise sample is [array([-0.75425065], dtype=float32), -0.039104406]. 
=============================================
[2019-03-27 14:23:58,389] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5512360e-07 9.8305124e-01 6.7543800e-13 2.9633649e-09 1.6948337e-02], sum to 1.0000
[2019-03-27 14:23:58,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9797
[2019-03-27 14:23:58,411] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2039484.422065501 W.
[2019-03-27 14:23:58,419] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.33333333333333, 82.33333333333333, 1.0, 2.0, 0.4862118556632458, 1.0, 2.0, 0.4862118556632458, 1.0, 1.0, 0.830639349132422, 6.911199999999999, 6.9112, 170.5573041426782, 2039484.422065501, 2039484.422065502, 403313.5576749169], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4007400.0000, 
sim time next is 4008000.0000, 
raw observation next is [27.66666666666667, 80.66666666666667, 1.0, 2.0, 0.8120309425618201, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.979240183339744, 6.9112, 168.9125510560989, 2031911.31826348, 1983641.386306068, 411775.5492118177], 
processed observation next is [1.0, 0.391304347826087, 0.5102685624012641, 0.8066666666666668, 1.0, 1.0, 0.7735312560985784, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006804018333974415, 0.0, 0.8294379541859143, 0.5644198106287445, 0.55101149619613, 0.6145903719579369], 
reward next is 0.0452, 
noisyNet noise sample is [array([-1.6802807], dtype=float32), -1.1337638]. 
=============================================
[2019-03-27 14:23:58,441] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[38.483406]
 [39.23678 ]
 [38.29851 ]
 [39.816544]
 [41.95269 ]], R is [[38.70339584]
 [38.31636047]
 [38.39408112]
 [38.01013947]
 [37.63003922]].
[2019-03-27 14:23:59,079] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1145988e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.4691292e-32], sum to 1.0000
[2019-03-27 14:23:59,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6980
[2019-03-27 14:23:59,094] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.5428048308066511, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129565103939, 758506.7717488018, 758506.7717488012, 190577.8625517101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4036800.0000, 
sim time next is 4037400.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5409512605071201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755915.7006841992, 755915.7006841986, 190264.5678668763], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.79, 1.0, 1.0, 0.44692922952665065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20997658352338866, 0.20997658352338852, 0.283976966965487], 
reward next is 0.7160, 
noisyNet noise sample is [array([0.504263], dtype=float32), 1.7280676]. 
=============================================
[2019-03-27 14:24:04,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2146790e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.4367117e-28], sum to 1.0000
[2019-03-27 14:24:04,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2869
[2019-03-27 14:24:04,086] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5883725691910666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822207.0440493142, 822207.0440493142, 198599.2459760719], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3979800.0000, 
sim time next is 3980400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5875621490323545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821074.1042301796, 821074.1042301796, 198451.2058671792], 
processed observation next is [1.0, 0.043478260869565216, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5030869265450054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22807614006393878, 0.22807614006393878, 0.2961958296525063], 
reward next is 0.7038, 
noisyNet noise sample is [array([-0.7607757], dtype=float32), -0.044773206]. 
=============================================
[2019-03-27 14:24:12,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.13449934e-35 1.00000000e+00 0.00000000e+00 0.00000000e+00
 4.21988272e-31], sum to 1.0000
[2019-03-27 14:24:12,088] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1067
[2019-03-27 14:24:12,094] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 76.33333333333334, 1.0, 2.0, 0.6212109711911795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868115.0297971586, 868115.0297971586, 204764.2958587647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4224000.0000, 
sim time next is 4224600.0000, 
raw observation next is [31.5, 75.0, 1.0, 2.0, 0.6176027634820951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863070.6693976154, 863070.6693976154, 204071.4122473456], 
processed observation next is [1.0, 0.9130434782608695, 0.6919431279620853, 0.75, 1.0, 1.0, 0.5392804379302351, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23974185261044872, 0.23974185261044872, 0.3045841973840979], 
reward next is 0.6954, 
noisyNet noise sample is [array([0.18696806], dtype=float32), -0.8938013]. 
=============================================
[2019-03-27 14:24:21,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1283706e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.5317228e-29], sum to 1.0000
[2019-03-27 14:24:21,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4068
[2019-03-27 14:24:21,267] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 88.16666666666667, 1.0, 2.0, 0.8064759847043237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1127152.139103602, 1127152.139103602, 245509.9468453412], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4169400.0000, 
sim time next is 4170000.0000, 
raw observation next is [28.66666666666667, 87.33333333333334, 1.0, 2.0, 0.7760527835367242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1084610.153922436, 1084610.153922437, 238121.4827969765], 
processed observation next is [1.0, 0.2608695652173913, 0.5576619273301741, 0.8733333333333334, 1.0, 1.0, 0.7301840765502702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3012805983117878, 0.30128059831178805, 0.3554051982044425], 
reward next is 0.6446, 
noisyNet noise sample is [array([0.27937084], dtype=float32), -1.17629]. 
=============================================
[2019-03-27 14:24:21,288] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[57.291344]
 [57.165825]
 [57.567123]
 [57.305527]
 [57.26178 ]], R is [[57.57788467]
 [57.63567352]
 [57.66287231]
 [57.6911087 ]
 [57.71102905]].
[2019-03-27 14:24:21,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0400486e-23 1.0000000e+00 4.0495792e-30 2.1036162e-34 7.7098147e-18], sum to 1.0000
[2019-03-27 14:24:21,679] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1768
[2019-03-27 14:24:21,694] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1681733.197753472 W.
[2019-03-27 14:24:21,699] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.232334929058812, 6.9112, 168.9115256850071, 1681733.197753472, 1453910.956317576, 311352.7066286844], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4244400.0000, 
sim time next is 4245000.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.3890524403959132, 1.0, 1.0, 0.3890524403959132, 1.0, 1.0, 0.6756554273595977, 6.911199999999999, 6.9112, 170.5573041426782, 1631625.204474399, 1631625.2044744, 346911.1163449569], 
processed observation next is [1.0, 0.13043478260869565, 0.6208530805687204, 0.75, 1.0, 1.0, 0.2639186028866424, 1.0, 0.5, 0.2639186028866424, 1.0, 0.5, 0.6044578382434118, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.45322922346511085, 0.45322922346511113, 0.517777785589488], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6284378], dtype=float32), -1.1892147]. 
=============================================
[2019-03-27 14:24:21,724] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[49.530483]
 [46.916447]
 [44.334137]
 [46.81944 ]
 [45.435314]], R is [[44.86294556]
 [44.41431808]
 [43.9701767 ]
 [43.53047562]
 [43.09517288]].
[2019-03-27 14:24:25,711] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 14:24:25,712] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:24:25,714] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:24:25,714] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:24:25,716] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:24:25,717] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:24:25,717] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:24:25,718] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:24:25,717] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:24:25,720] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:24:25,722] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:24:25,756] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run65
[2019-03-27 14:24:25,780] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run65
[2019-03-27 14:24:25,803] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run65
[2019-03-27 14:24:25,823] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run65
[2019-03-27 14:24:25,823] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run65
[2019-03-27 14:25:26,607] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0574919], dtype=float32), 0.070759244]
[2019-03-27 14:25:26,608] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.33333333333333, 64.33333333333334, 1.0, 2.0, 0.8845525506659954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1236337.540554896, 1236337.540554896, 265724.4053127404]
[2019-03-27 14:25:26,609] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:25:26,613] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3039534e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0983186e-30], sampled 0.15746630646491022
[2019-03-27 14:26:09,957] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0574919], dtype=float32), 0.070759244]
[2019-03-27 14:26:09,960] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.60316678833333, 75.12755054833333, 1.0, 2.0, 0.5184351338532024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724441.3353561546, 724441.3353561546, 186534.8158540601]
[2019-03-27 14:26:09,961] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:26:09,965] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.379976e-38], sampled 0.6425063936030758
[2019-03-27 14:26:28,029] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0574919], dtype=float32), 0.070759244]
[2019-03-27 14:26:28,031] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.42087920833333, 72.19347516666667, 1.0, 2.0, 0.4138099090860345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610221.40544307, 610221.4054430694, 175284.6461372056]
[2019-03-27 14:26:28,035] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:26:28,038] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.047212818628729725
[2019-03-27 14:26:31,943] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7172 2842486185.3035 1134.0000
[2019-03-27 14:26:33,636] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.5082 3164156124.2468 1798.0000
[2019-03-27 14:26:33,878] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6338 3007781730.7027 1766.0000
[2019-03-27 14:26:34,041] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0144 2927297980.4957 1343.0000
[2019-03-27 14:26:34,076] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.0224 2779271766.9493 938.0000
[2019-03-27 14:26:35,092] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1600000, evaluation results [1600000.0, 7878.5081671547905, 3164156124.246764, 1798.0, 8254.014408503073, 2927297980.4957156, 1343.0, 8660.022428690709, 2779271766.949257, 938.0, 7997.633781483245, 3007781730.702652, 1766.0, 8496.717237092873, 2842486185.30348, 1134.0]
[2019-03-27 14:26:37,323] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5537965e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6319072e-31], sum to 1.0000
[2019-03-27 14:26:37,335] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8178
[2019-03-27 14:26:37,341] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6172275254812768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862546.0791018306, 862546.0791018312, 203999.608627941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4406400.0000, 
sim time next is 4407000.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6167573076257186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 861888.7054186263, 861888.705418627, 203909.6718430076], 
processed observation next is [0.0, 0.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5382618164165284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23941352928295176, 0.23941352928295195, 0.30434279379553375], 
reward next is 0.6957, 
noisyNet noise sample is [array([-0.70786256], dtype=float32), -0.54822]. 
=============================================
[2019-03-27 14:26:37,362] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[69.42125 ]
 [72.337425]
 [72.34025 ]
 [72.3373  ]
 [72.315025]], R is [[68.2448349 ]
 [68.25791168]
 [68.2707901 ]
 [68.28354645]
 [68.29624939]].
[2019-03-27 14:26:38,659] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.7274457e-11 9.9953353e-01 4.1171565e-17 9.6423608e-14 4.6651132e-04], sum to 1.0000
[2019-03-27 14:26:38,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3331
[2019-03-27 14:26:38,684] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3516932.799596167 W.
[2019-03-27 14:26:38,694] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 49.00000000000001, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.758417123387868, 6.9112, 170.5573041426782, 3516932.799596167, 2910036.711524258, 548866.7370325739], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4288800.0000, 
sim time next is 4289400.0000, 
raw observation next is [38.0, 48.0, 1.0, 2.0, 1.018553983804695, 1.0, 2.0, 0.8298670314166099, 1.0, 1.0, 1.03, 7.00512286329562, 6.9112, 170.5573041426782, 3483044.421476895, 3415763.653960434, 640853.4175024435], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.48, 1.0, 1.0, 1.0223541973550543, 1.0, 1.0, 0.7950205197790481, 1.0, 0.5, 1.0365853658536586, 0.009392286329561994, 0.0, 0.8375144448122397, 0.9675123392991375, 0.9488232372112316, 0.9564976380633485], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.748313], dtype=float32), 1.2240398]. 
=============================================
[2019-03-27 14:26:40,256] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0657459e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.8779048e-32], sum to 1.0000
[2019-03-27 14:26:40,266] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4074
[2019-03-27 14:26:40,273] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5079805750720062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709827.6556061554, 709827.655606156, 184855.410145665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4493400.0000, 
sim time next is 4494000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5075207329956939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709184.8798829295, 709184.8798829295, 184782.3339952629], 
processed observation next is [0.0, 0.0, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40665148553698055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19699579996748043, 0.19699579996748043, 0.27579452835113866], 
reward next is 0.7242, 
noisyNet noise sample is [array([-0.8191783], dtype=float32), 0.56895477]. 
=============================================
[2019-03-27 14:26:40,286] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.62963]
 [72.21806]
 [75.46853]
 [75.45101]
 [75.41943]], R is [[70.58108521]
 [70.59937286]
 [70.61728668]
 [70.63459778]
 [70.65129852]].
[2019-03-27 14:26:49,412] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7135885e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7133829e-33], sum to 1.0000
[2019-03-27 14:26:49,424] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3411
[2019-03-27 14:26:49,431] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5713417938296217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798398.853453197, 798398.853453197, 195523.5098540154], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4774800.0000, 
sim time next is 4775400.0000, 
raw observation next is [27.5, 81.5, 1.0, 2.0, 0.5771051952313765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 806455.7468549297, 806455.7468549297, 196553.5174873156], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.815, 1.0, 1.0, 0.4904881870257548, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22401548523748047, 0.22401548523748047, 0.29336345893629195], 
reward next is 0.7066, 
noisyNet noise sample is [array([-0.57350713], dtype=float32), -0.42031962]. 
=============================================
[2019-03-27 14:26:50,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.0683902e-11 9.9998307e-01 7.1357348e-18 3.0597291e-15 1.6907832e-05], sum to 1.0000
[2019-03-27 14:26:50,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2585
[2019-03-27 14:26:50,786] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2626670.03139032 W.
[2019-03-27 14:26:50,790] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 71.66666666666667, 1.0, 2.0, 0.6260423695477817, 1.0, 2.0, 0.6260423695477817, 1.0, 1.0, 1.03, 6.975536380003819, 6.9112, 170.5573041426782, 2626670.03139032, 2580583.265356968, 497615.3890791895], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4618200.0000, 
sim time next is 4618800.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.8700451983670623, 1.0, 2.0, 0.8700451983670623, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2433429.66974798, 2433429.66974798, 455421.2733104546], 
processed observation next is [1.0, 0.4782608695652174, 0.7156398104265403, 0.71, 1.0, 1.0, 0.843427949839834, 1.0, 1.0, 0.843427949839834, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6759526860411055, 0.6759526860411055, 0.6797332437469472], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.4669282], dtype=float32), -0.75097275]. 
=============================================
[2019-03-27 14:26:56,109] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4815122e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.8992524e-31], sum to 1.0000
[2019-03-27 14:26:56,119] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8793
[2019-03-27 14:26:56,123] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6546175374521693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 914819.3436515267, 914819.3436515274, 211351.0746198205], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4764600.0000, 
sim time next is 4765200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6945860471020369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 970700.3330538451, 970700.3330538458, 219677.5756208749], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.6320313820506468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26963898140384585, 0.26963898140384607, 0.32787697853861925], 
reward next is 0.6721, 
noisyNet noise sample is [array([1.3628802], dtype=float32), 0.45757404]. 
=============================================
[2019-03-27 14:26:58,187] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8318154e-11 9.9968040e-01 7.3038050e-17 3.8392944e-14 3.1962505e-04], sum to 1.0000
[2019-03-27 14:26:58,194] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5551
[2019-03-27 14:26:58,204] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2167266.974414206 W.
[2019-03-27 14:26:58,208] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.75, 64.0, 1.0, 2.0, 0.5166443536420755, 1.0, 2.0, 0.5166443536420755, 1.0, 1.0, 0.8972403853776892, 6.911199999999999, 6.9112, 170.5573041426782, 2167266.974414206, 2167266.974414207, 426846.4422119858], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4890600.0000, 
sim time next is 4891200.0000, 
raw observation next is [31.66666666666667, 64.33333333333333, 1.0, 2.0, 0.5417579225221706, 1.0, 2.0, 0.5417579225221706, 1.0, 2.0, 0.940854349338275, 6.9112, 6.9112, 170.5573041426782, 2272718.330644937, 2272718.330644937, 445223.2693568346], 
processed observation next is [1.0, 0.6086956521739131, 0.6998420221169038, 0.6433333333333333, 1.0, 1.0, 0.4479011114724947, 1.0, 1.0, 0.4479011114724947, 1.0, 1.0, 0.9278711577296037, 0.0, 0.0, 0.8375144448122397, 0.6313106474013714, 0.6313106474013714, 0.6645123423236338], 
reward next is 0.3355, 
noisyNet noise sample is [array([-0.7185704], dtype=float32), 0.25457868]. 
=============================================
[2019-03-27 14:27:01,451] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9191256e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2798760e-27], sum to 1.0000
[2019-03-27 14:27:01,462] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4556
[2019-03-27 14:27:01,470] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 76.5, 1.0, 2.0, 0.4897629725854023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684363.0528513591, 684363.0528513585, 182007.4195761309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4753800.0000, 
sim time next is 4754400.0000, 
raw observation next is [27.33333333333334, 77.33333333333333, 1.0, 2.0, 0.4898574627576148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684495.1298363131, 684495.1298363131, 182021.8596418193], 
processed observation next is [1.0, 0.0, 0.4944707740916275, 0.7733333333333333, 1.0, 1.0, 0.3853704370573673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19013753606564252, 0.19013753606564252, 0.2716744173758497], 
reward next is 0.7283, 
noisyNet noise sample is [array([-0.62489945], dtype=float32), 1.2720952]. 
=============================================
[2019-03-27 14:27:03,085] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8743843e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.9008859e-27], sum to 1.0000
[2019-03-27 14:27:03,096] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5329
[2019-03-27 14:27:03,104] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7674757770387476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1072616.888923591, 1072616.888923591, 236084.0565103992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4682400.0000, 
sim time next is 4683000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7580254610233658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1059402.628320965, 1059402.628320965, 233868.611976253], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.708464410871525, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2942785078669347, 0.2942785078669347, 0.349057629815303], 
reward next is 0.6509, 
noisyNet noise sample is [array([-0.89299345], dtype=float32), -0.6319084]. 
=============================================
[2019-03-27 14:27:03,116] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[56.734203]
 [56.89902 ]
 [57.03085 ]
 [57.20429 ]
 [57.747288]], R is [[56.84721756]
 [56.92638397]
 [57.00037766]
 [57.07123947]
 [57.12319565]].
[2019-03-27 14:27:05,864] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5143599e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4268271e-31], sum to 1.0000
[2019-03-27 14:27:05,874] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1615
[2019-03-27 14:27:05,880] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5158180632273026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720783.1007017494, 720783.10070175, 186112.0459839604], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4735200.0000, 
sim time next is 4735800.0000, 
raw observation next is [28.5, 76.5, 1.0, 2.0, 0.5161334735872127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721223.9918837352, 721223.9918837352, 186162.8632412496], 
processed observation next is [1.0, 0.8260869565217391, 0.5497630331753555, 0.765, 1.0, 1.0, 0.41702828143037673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.200339997745482, 0.200339997745482, 0.2778550197630591], 
reward next is 0.7221, 
noisyNet noise sample is [array([1.4817489], dtype=float32), -1.2918766]. 
=============================================
[2019-03-27 14:27:07,049] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3299374e-10 1.1878172e-03 1.1260313e-16 5.8067204e-11 9.9881220e-01], sum to 1.0000
[2019-03-27 14:27:07,056] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9791
[2019-03-27 14:27:07,064] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.66666666666667, 68.33333333333333, 1.0, 2.0, 0.631256238834656, 1.0, 2.0, 0.631256238834656, 1.0, 2.0, 1.03, 6.985716406193537, 6.9112, 170.5573041426782, 2648568.917261376, 2595189.78437134, 499589.8497377385], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4722600.0000, 
sim time next is 4723200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.6726223307429049, 1.0, 2.0, 0.6569012048857151, 1.0, 2.0, 1.03, 7.005095573551547, 6.9112, 170.5573041426782, 2756286.484897425, 2689025.26613472, 512820.657765069], 
processed observation next is [1.0, 0.6956521739130435, 0.7156398104265403, 0.67, 1.0, 1.0, 0.6055690731842227, 1.0, 1.0, 0.5866279576936326, 1.0, 1.0, 1.0365853658536586, 0.009389557355154742, 0.0, 0.8375144448122397, 0.7656351346937291, 0.7469514628152, 0.7654039668135358], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.28142518], dtype=float32), -0.44086793]. 
=============================================
[2019-03-27 14:27:10,188] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2737790e-10 8.5063984e-05 4.9695713e-18 2.8585756e-09 9.9991488e-01], sum to 1.0000
[2019-03-27 14:27:10,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5124
[2019-03-27 14:27:10,207] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.16666666666666, 65.66666666666667, 1.0, 2.0, 0.6260336266413957, 1.0, 2.0, 0.6260336266413957, 1.0, 2.0, 1.03, 6.975519309718673, 6.9112, 170.5573041426782, 2626633.310441279, 2580558.772547552, 497610.1033519421], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4894800.0000, 
sim time next is 4895400.0000, 
raw observation next is [31.08333333333334, 65.83333333333333, 1.0, 2.0, 0.6174277147787884, 1.0, 2.0, 0.6174277147787884, 1.0, 2.0, 1.03, 6.958716745680162, 6.9112, 170.5573041426782, 2590488.273400113, 2556450.095611778, 494383.1869588727], 
processed observation next is [1.0, 0.6521739130434783, 0.6721958925750398, 0.6583333333333333, 1.0, 1.0, 0.5390695358780584, 1.0, 1.0, 0.5390695358780584, 1.0, 1.0, 1.0365853658536586, 0.004751674568016196, 0.0, 0.8375144448122397, 0.7195800759444758, 0.7101250265588273, 0.7378853536699592], 
reward next is 0.0245, 
noisyNet noise sample is [array([0.3105461], dtype=float32), -0.784133]. 
=============================================
[2019-03-27 14:27:25,485] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.2805564e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1068923e-27], sum to 1.0000
[2019-03-27 14:27:25,493] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0605
[2019-03-27 14:27:25,501] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.6910034200028303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965691.251060816, 965691.251060816, 218913.5188449852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4949400.0000, 
sim time next is 4950000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.7016581958525254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 980588.3901806338, 980588.3901806338, 221201.8509231201], 
processed observation next is [1.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.6405520431958137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27238566393906494, 0.27238566393906494, 0.3301520163031643], 
reward next is 0.6698, 
noisyNet noise sample is [array([-0.3527268], dtype=float32), 1.6100866]. 
=============================================
[2019-03-27 14:27:25,513] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[61.45943 ]
 [61.473103]
 [61.485268]
 [61.622765]
 [61.73568 ]], R is [[61.59711456]
 [61.6544075 ]
 [61.7102356 ]
 [61.76080322]
 [61.81624222]].
[2019-03-27 14:27:25,707] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5178032e-35], sum to 1.0000
[2019-03-27 14:27:25,716] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9776
[2019-03-27 14:27:25,721] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 86.5, 1.0, 2.0, 0.4841421065163933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676506.317761109, 676506.3177611083, 181148.8198019235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5020200.0000, 
sim time next is 5020800.0000, 
raw observation next is [26.0, 87.33333333333333, 1.0, 2.0, 0.4878876570466119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681741.7675323692, 681741.7675323692, 181720.2063402289], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.8733333333333333, 1.0, 1.0, 0.3829971771645927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1893727132034359, 0.1893727132034359, 0.27122418856750585], 
reward next is 0.7288, 
noisyNet noise sample is [array([1.1095159], dtype=float32), 0.76114047]. 
=============================================
[2019-03-27 14:27:34,240] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2449334e-12 9.9999857e-01 1.4711549e-19 5.8602553e-18 1.4685895e-06], sum to 1.0000
[2019-03-27 14:27:34,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0378
[2019-03-27 14:27:34,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2643177.711156523 W.
[2019-03-27 14:27:34,267] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.9449589960397379, 1.0, 2.0, 0.9449589960397379, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2643177.711156523, 2643177.711156523, 496615.0449737287], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5238600.0000, 
sim time next is 5239200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.6171019932792395, 1.0, 2.0, 0.6171019932792395, 1.0, 1.0, 1.03, 6.958080802114463, 6.9112, 170.5573041426782, 2589120.255796233, 2555537.630252538, 494263.0307804568], 
processed observation next is [1.0, 0.6521739130434783, 0.7156398104265403, 0.67, 1.0, 1.0, 0.5386771003364331, 1.0, 1.0, 0.5386771003364331, 1.0, 0.5, 1.0365853658536586, 0.004688080211446266, 0.0, 0.8375144448122397, 0.7192000710545092, 0.7098715639590383, 0.737706016090234], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.92009664], dtype=float32), -0.52593255]. 
=============================================
[2019-03-27 14:27:37,620] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:27:37,628] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1097
[2019-03-27 14:27:37,635] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5104630775478667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713297.7500051999, 713297.7500052005, 185251.6245727143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5131800.0000, 
sim time next is 5132400.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5103689724725529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713166.2077364186, 713166.2077364192, 185236.5954961804], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.4100830993645215, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1981017243712274, 0.19810172437122756, 0.27647253059131405], 
reward next is 0.7235, 
noisyNet noise sample is [array([1.1246945], dtype=float32), 0.99289125]. 
=============================================
[2019-03-27 14:27:40,602] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 14:27:40,604] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:27:40,606] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:27:40,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:27:40,607] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:27:40,608] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:27:40,609] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:27:40,609] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:27:40,610] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:27:40,610] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:27:40,611] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:27:40,644] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run66
[2019-03-27 14:27:40,669] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run66
[2019-03-27 14:27:40,690] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run66
[2019-03-27 14:27:40,691] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run66
[2019-03-27 14:27:40,731] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run66
[2019-03-27 14:27:45,515] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05946669], dtype=float32), 0.072096184]
[2019-03-27 14:27:45,517] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.16666666666667, 71.66666666666667, 1.0, 2.0, 0.2430283995994861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 401928.3524741675, 401928.3524741668, 160137.4609100876]
[2019-03-27 14:27:45,521] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:27:45,523] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9490871717624775
[2019-03-27 14:28:08,585] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05946669], dtype=float32), 0.072096184]
[2019-03-27 14:28:08,586] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.23078082166667, 78.95143151166667, 1.0, 2.0, 0.8850687860211544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1237059.502147276, 1237059.502147276, 265865.3814694223]
[2019-03-27 14:28:08,587] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:28:08,591] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3734952e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.7094944e-27], sampled 0.49825098863842077
[2019-03-27 14:28:19,343] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05946669], dtype=float32), 0.072096184]
[2019-03-27 14:28:19,347] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.5, 69.83333333333333, 1.0, 2.0, 0.3384496303785491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 533035.8297673026, 533035.8297673032, 169538.9828780717]
[2019-03-27 14:28:19,350] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:28:19,355] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9603966395512603
[2019-03-27 14:28:32,867] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05946669], dtype=float32), 0.072096184]
[2019-03-27 14:28:32,868] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.4, 68.0, 1.0, 2.0, 0.574292467221249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 802523.7122508698, 802523.7122508704, 196054.0192877743]
[2019-03-27 14:28:32,868] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:28:32,874] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10082886774477662
[2019-03-27 14:28:41,333] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05946669], dtype=float32), 0.072096184]
[2019-03-27 14:28:41,335] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.7, 61.0, 1.0, 2.0, 0.7721843630398215, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005979791104314, 6.9112, 168.9123159155093, 1976145.078227641, 1908905.287354486, 401083.2497606193]
[2019-03-27 14:28:41,337] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:28:41,339] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.68472622e-23 1.00000000e+00 1.68106706e-29 9.14414851e-33
 1.17616265e-17], sampled 0.8078423759784535
[2019-03-27 14:28:41,340] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1976145.078227641 W.
[2019-03-27 14:29:47,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05946669], dtype=float32), 0.072096184]
[2019-03-27 14:29:47,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.4, 71.0, 1.0, 2.0, 0.3212023672479577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504840.6142313058, 504840.6142313058, 167323.3100058327]
[2019-03-27 14:29:47,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:29:47,955] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14694887280258895
[2019-03-27 14:29:48,041] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.4056 2927427185.2523 1343.0000
[2019-03-27 14:29:48,555] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.1382 3164263527.9767 1803.0000
[2019-03-27 14:29:48,684] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.9570 2779392122.5824 937.0000
[2019-03-27 14:29:48,722] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1480 3007781423.5699 1769.0000
[2019-03-27 14:29:48,732] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.0621 2842609336.2899 1141.0000
[2019-03-27 14:29:49,751] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1625000, evaluation results [1625000.0, 7878.138173398535, 3164263527.9767103, 1803.0, 8252.405588883943, 2927427185.252342, 1343.0, 8658.956997395346, 2779392122.5823827, 937.0, 7998.1479601310075, 3007781423.5698953, 1769.0, 8494.062140826145, 2842609336.289869, 1141.0]
[2019-03-27 14:29:50,149] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.242469e-36], sum to 1.0000
[2019-03-27 14:29:50,160] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9393
[2019-03-27 14:29:50,167] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5234912123965862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731508.9379428031, 731508.9379428037, 187358.1546805001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5173800.0000, 
sim time next is 5174400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5225737880560548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730226.5195145251, 730226.5195145257, 187208.2098061929], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42478769645307807, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20284069986514586, 0.20284069986514602, 0.2794152385167058], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.5049051], dtype=float32), 0.20454592]. 
=============================================
[2019-03-27 14:29:59,495] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2778987e-21 1.0000000e+00 1.3390371e-29 1.0396033e-34 2.5627501e-17], sum to 1.0000
[2019-03-27 14:29:59,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5162
[2019-03-27 14:29:59,513] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.45, 78.5, 1.0, 2.0, 0.9403748797023306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565028221, 1314408.608791878, 1314408.608791878, 281290.0940149715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5470200.0000, 
sim time next is 5470800.0000, 
raw observation next is [30.63333333333333, 77.66666666666667, 1.0, 2.0, 1.039472898513964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129385126699, 1453017.61879139, 1453017.61879139, 311188.5989980483], 
processed observation next is [1.0, 0.30434782608695654, 0.6508688783570299, 0.7766666666666667, 1.0, 1.0, 1.0475577090529689, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294398567750563, 0.40361600521983054, 0.40361600521983054, 0.46446059551947505], 
reward next is 0.5355, 
noisyNet noise sample is [array([-0.965344], dtype=float32), 0.11905692]. 
=============================================
[2019-03-27 14:30:01,383] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5906625e-19 1.0000000e+00 1.1732093e-25 3.3653552e-26 3.7554044e-12], sum to 1.0000
[2019-03-27 14:30:01,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6141
[2019-03-27 14:30:01,396] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.65, 55.16666666666666, 1.0, 2.0, 0.3078660433690864, 1.0, 2.0, 0.3078660433690864, 1.0, 2.0, 0.5346615044757692, 6.911200000000001, 6.9112, 170.5573041426782, 1290937.090043375, 1290937.090043374, 307754.8314484487], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5332200.0000, 
sim time next is 5332800.0000, 
raw observation next is [35.4, 56.33333333333334, 1.0, 2.0, 0.5836962351495041, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564928147, 815669.7047350331, 815669.7047350331, 197750.5812698498], 
processed observation next is [1.0, 0.7391304347826086, 0.8767772511848341, 0.5633333333333335, 1.0, 1.0, 0.4984291989753061, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399450657985, 0.22657491798195364, 0.22657491798195364, 0.2951501212982833], 
reward next is 0.7048, 
noisyNet noise sample is [array([-0.1938563], dtype=float32), 1.2268063]. 
=============================================
[2019-03-27 14:30:06,314] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7396164e-16 1.0000000e+00 1.5187570e-23 2.7499023e-24 1.0237858e-09], sum to 1.0000
[2019-03-27 14:30:06,325] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2537
[2019-03-27 14:30:06,332] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 76.0, 1.0, 2.0, 0.5530087284291393, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9603932785003108, 6.911200000000001, 6.9112, 168.9127069892774, 1546104.062654078, 1546104.062654078, 338408.6421658547], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5472000.0000, 
sim time next is 5472600.0000, 
raw observation next is [31.21666666666667, 75.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.274298604456446, 6.9112, 168.9108517438647, 1711522.787558335, 1453931.350466198, 311355.9389236551], 
processed observation next is [1.0, 0.34782608695652173, 0.6785150078988943, 0.7533333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.03630986044564457, 0.0, 0.8294296097848959, 0.47542299654398196, 0.4038698195739439, 0.4647103566024703], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4184821], dtype=float32), -0.5470655]. 
=============================================
[2019-03-27 14:30:07,903] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5278097e-22 1.0000000e+00 8.3482127e-30 6.2330772e-34 6.4025206e-18], sum to 1.0000
[2019-03-27 14:30:07,919] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4712
[2019-03-27 14:30:07,927] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1800026.595570599 W.
[2019-03-27 14:30:07,931] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.7, 81.83333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.398970201970895, 6.9112, 168.9098758912226, 1800026.595570599, 1453991.937601569, 311354.7977091381], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5467800.0000, 
sim time next is 5468400.0000, 
raw observation next is [29.9, 81.0, 1.0, 2.0, 0.3685519900043862, 1.0, 1.0, 0.3685519900043862, 1.0, 1.0, 0.6400529246320582, 6.9112, 6.9112, 170.5573041426782, 1545587.520801519, 1545587.520801519, 336164.1681070608], 
processed observation next is [1.0, 0.30434782608695654, 0.6161137440758293, 0.81, 1.0, 1.0, 0.23921926506552554, 1.0, 0.5, 0.23921926506552554, 1.0, 0.5, 0.5610401519903149, 0.0, 0.0, 0.8375144448122397, 0.42932986688931085, 0.42932986688931085, 0.5017375643388967], 
reward next is 0.4983, 
noisyNet noise sample is [array([0.0087585], dtype=float32), -0.07863721]. 
=============================================
[2019-03-27 14:30:07,956] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2652180e-09 9.6794230e-01 8.2071992e-18 5.9999227e-15 3.2057650e-02], sum to 1.0000
[2019-03-27 14:30:07,970] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6248
[2019-03-27 14:30:07,989] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2867131.818601898 W.
[2019-03-27 14:30:07,994] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.73333333333333, 46.66666666666667, 1.0, 2.0, 0.725396938055624, 1.0, 2.0, 0.6832885085420745, 1.0, 2.0, 1.03, 7.005099734766365, 6.9112, 170.5573041426782, 2867131.818601898, 2799867.61899175, 529470.428863393], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5494800.0000, 
sim time next is 5495400.0000, 
raw observation next is [36.6, 47.0, 1.0, 2.0, 0.7227591964998923, 1.0, 2.0, 0.6819696377642086, 1.0, 2.0, 1.03, 7.005099526766069, 6.9112, 170.5573041426782, 2861591.39524494, 2794327.344633866, 528613.6506654155], 
processed observation next is [1.0, 0.6086956521739131, 0.9336492890995262, 0.47, 1.0, 1.0, 0.6659749355420389, 1.0, 1.0, 0.6168308888725405, 1.0, 1.0, 1.0365853658536586, 0.009389952676606939, 0.0, 0.8375144448122397, 0.7948864986791501, 0.776202040176074, 0.7889755980080828], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.74272823], dtype=float32), -0.31109846]. 
=============================================
[2019-03-27 14:30:08,699] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.78183364e-19 1.00000000e+00 1.18363325e-26 6.81632300e-28
 6.33137683e-12], sum to 1.0000
[2019-03-27 14:30:08,708] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2774
[2019-03-27 14:30:08,721] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2357759.851747002 W.
[2019-03-27 14:30:08,726] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.41666666666666, 51.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.015528757739146, 6.9112, 168.9119627885146, 2357759.851747002, 2283745.875717975, 475842.1616013565], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5490600.0000, 
sim time next is 5491200.0000, 
raw observation next is [36.53333333333333, 50.0, 1.0, 2.0, 0.9766393729104899, 1.0, 1.0, 0.9766393729104899, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2731888.882192284, 2731888.882192284, 515033.8027820223], 
processed observation next is [1.0, 0.5652173913043478, 0.9304897314375986, 0.5, 1.0, 1.0, 0.9718546661572167, 1.0, 0.5, 0.9718546661572167, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.75885802283119, 0.75885802283119, 0.7687071683313765], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.26037654], dtype=float32), 1.1522484]. 
=============================================
[2019-03-27 14:30:19,907] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2582512e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3879906e-32], sum to 1.0000
[2019-03-27 14:30:19,923] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4397
[2019-03-27 14:30:19,932] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.5, 60.0, 1.0, 2.0, 0.547191515452865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764638.8609016353, 764638.8609016358, 191320.7237545327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5666400.0000, 
sim time next is 5667000.0000, 
raw observation next is [32.46666666666667, 60.0, 1.0, 2.0, 0.551524761842415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770696.2841701536, 770696.284170153, 192062.6559332326], 
processed observation next is [0.0, 0.6086956521739131, 0.7377567140600317, 0.6, 1.0, 1.0, 0.4596683877619457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21408230115837598, 0.21408230115837582, 0.2866606804973621], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.39942053], dtype=float32), -0.057128582]. 
=============================================
[2019-03-27 14:30:19,952] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.715675]
 [75.5688  ]
 [75.55825 ]
 [75.53544 ]
 [75.501595]], R is [[75.690979  ]
 [75.64851379]
 [75.60639191]
 [75.56463623]
 [75.52323914]].
[2019-03-27 14:30:20,109] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.1441691e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0679856e-28], sum to 1.0000
[2019-03-27 14:30:20,121] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5320
[2019-03-27 14:30:20,130] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.68333333333333, 90.0, 1.0, 2.0, 0.5361237086903345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749167.3864504865, 749167.3864504859, 189449.0325888569], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5611800.0000, 
sim time next is 5612400.0000, 
raw observation next is [26.6, 90.0, 1.0, 2.0, 0.5340010684298273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746200.2141612999, 746200.2141613004, 189094.3196509706], 
processed observation next is [1.0, 1.0, 0.4597156398104266, 0.9, 1.0, 1.0, 0.438555504132322, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20727783726702775, 0.2072778372670279, 0.28223032783726953], 
reward next is 0.7178, 
noisyNet noise sample is [array([-1.0920192], dtype=float32), -0.3902397]. 
=============================================
[2019-03-27 14:30:22,977] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.578196e-32], sum to 1.0000
[2019-03-27 14:30:22,987] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7481
[2019-03-27 14:30:22,994] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 53.0, 1.0, 2.0, 0.5359444086305802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748916.7481963548, 748916.7481963541, 189420.552626323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5756400.0000, 
sim time next is 5757000.0000, 
raw observation next is [33.81666666666667, 53.83333333333334, 1.0, 2.0, 0.5579740119941156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779711.722389029, 779711.722389029, 193177.2254882831], 
processed observation next is [0.0, 0.6521739130434783, 0.8017377567140602, 0.5383333333333334, 1.0, 1.0, 0.4674385686676091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21658658955250806, 0.21658658955250806, 0.28832421714669115], 
reward next is 0.7117, 
noisyNet noise sample is [array([-0.2572277], dtype=float32), -0.7913435]. 
=============================================
[2019-03-27 14:30:23,016] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.87774 ]
 [78.77444 ]
 [78.799446]
 [78.8116  ]
 [78.8129  ]], R is [[78.72911072]
 [78.65910339]
 [78.59111786]
 [78.5249176 ]
 [78.46063232]].
[2019-03-27 14:30:28,279] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1544357e-23 1.0000000e+00 1.6054131e-30 1.1262854e-34 1.7651116e-16], sum to 1.0000
[2019-03-27 14:30:28,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1419
[2019-03-27 14:30:28,294] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 93.0, 1.0, 2.0, 0.9961360363984695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1392399.855754101, 1392399.8557541, 297750.2141445098], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6056400.0000, 
sim time next is 6057000.0000, 
raw observation next is [26.2, 93.0, 1.0, 2.0, 0.9038350009678966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1263304.621457548, 1263304.621457549, 270994.1707909416], 
processed observation next is [1.0, 0.08695652173913043, 0.44075829383886256, 0.93, 1.0, 1.0, 0.8841385553830079, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3509179504048745, 0.3509179504048747, 0.4044689116282711], 
reward next is 0.5955, 
noisyNet noise sample is [array([0.81648904], dtype=float32), 0.8281162]. 
=============================================
[2019-03-27 14:30:28,311] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[56.689747]
 [57.12406 ]
 [59.612072]
 [59.038006]
 [59.196472]], R is [[56.80804825]
 [56.79556274]
 [56.76711273]
 [56.91898727]
 [57.06916428]].
[2019-03-27 14:30:30,121] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.3649467e-08 1.3683601e-01 5.8519025e-15 1.4577449e-09 8.6316389e-01], sum to 1.0000
[2019-03-27 14:30:30,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5640
[2019-03-27 14:30:30,139] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.93333333333333, 72.0, 1.0, 2.0, 0.4833672453053287, 1.0, 2.0, 0.4833672453053287, 1.0, 2.0, 0.8372992058444373, 6.911199999999999, 6.9112, 170.5573041426782, 2027541.009883724, 2027541.009883725, 403461.4860238184], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6082800.0000, 
sim time next is 6083400.0000, 
raw observation next is [30.06666666666667, 71.0, 1.0, 2.0, 0.4888454089862926, 1.0, 2.0, 0.4888454089862926, 1.0, 2.0, 0.8464260316307537, 6.911199999999999, 6.9112, 170.5573041426782, 2050541.806573765, 2050541.806573766, 407072.0167650325], 
processed observation next is [1.0, 0.391304347826087, 0.6240126382306479, 0.71, 1.0, 1.0, 0.3841510951642079, 1.0, 1.0, 0.3841510951642079, 1.0, 1.0, 0.8127146727204313, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5695949462704902, 0.5695949462704906, 0.6075701742761679], 
reward next is 0.3924, 
noisyNet noise sample is [array([-1.8906732], dtype=float32), 0.51583314]. 
=============================================
[2019-03-27 14:30:30,233] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.9726790e-08 2.0390225e-07 1.4512495e-14 2.3330349e-06 9.9999738e-01], sum to 1.0000
[2019-03-27 14:30:30,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8545
[2019-03-27 14:30:30,252] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.65, 76.0, 1.0, 2.0, 0.5913946341503558, 1.0, 2.0, 0.5913946341503558, 1.0, 2.0, 1.02705690232506, 6.911199999999999, 6.9112, 170.5573041426782, 2481155.073177639, 2481155.07317764, 484099.2689876346], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5909400.0000, 
sim time next is 5910000.0000, 
raw observation next is [30.8, 75.33333333333333, 1.0, 2.0, 0.5684174472003647, 1.0, 2.0, 0.5684174472003647, 1.0, 2.0, 0.987153127264764, 6.9112, 6.9112, 170.5573041426782, 2384663.900198073, 2384663.900198073, 465682.5518242753], 
processed observation next is [1.0, 0.391304347826087, 0.6587677725118484, 0.7533333333333333, 1.0, 1.0, 0.4800210207233309, 1.0, 1.0, 0.4800210207233309, 1.0, 1.0, 0.9843330820302, 0.0, 0.0, 0.8375144448122397, 0.6624066389439092, 0.6624066389439092, 0.6950485848123512], 
reward next is 0.3050, 
noisyNet noise sample is [array([-0.07841879], dtype=float32), -1.3261695]. 
=============================================
[2019-03-27 14:30:30,270] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[51.618202]
 [50.902336]
 [50.783924]
 [50.656776]
 [50.222603]], R is [[52.41333008]
 [52.16666031]
 [51.93317032]
 [51.74738312]
 [51.59438324]].
[2019-03-27 14:30:30,793] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2594671e-10 9.5295346e-01 4.8693037e-18 9.2038473e-15 4.7046568e-02], sum to 1.0000
[2019-03-27 14:30:30,803] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2192
[2019-03-27 14:30:30,814] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2531833.195332192 W.
[2019-03-27 14:30:30,820] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.56666666666667, 71.33333333333334, 1.0, 2.0, 0.603461770244164, 1.0, 2.0, 0.603461770244164, 1.0, 2.0, 1.03, 6.931450049522403, 6.9112, 170.5573041426782, 2531833.195332192, 2517327.261139946, 489237.5400131237], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5917200.0000, 
sim time next is 5917800.0000, 
raw observation next is [31.43333333333333, 71.66666666666666, 1.0, 2.0, 0.8792296494936155, 1.0, 2.0, 0.8792296494936155, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2459142.940665908, 2459142.940665909, 460292.1204999851], 
processed observation next is [1.0, 0.4782608695652174, 0.6887835703001578, 0.7166666666666666, 1.0, 1.0, 0.8544935536067656, 1.0, 1.0, 0.8544935536067656, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6830952612960856, 0.6830952612960859, 0.687003164925351], 
reward next is 0.3130, 
noisyNet noise sample is [array([-0.49840254], dtype=float32), -0.5901093]. 
=============================================
[2019-03-27 14:30:33,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8982772e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7890109e-27], sum to 1.0000
[2019-03-27 14:30:33,254] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9166
[2019-03-27 14:30:33,261] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 87.5, 1.0, 2.0, 0.5386528356464708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752702.7845720698, 752702.7845720698, 189873.4473849325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5787000.0000, 
sim time next is 5787600.0000, 
raw observation next is [27.13333333333333, 87.66666666666667, 1.0, 2.0, 0.5386799251324969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752740.6522951062, 752740.6522951062, 189878.0200447449], 
processed observation next is [0.0, 1.0, 0.484992101105845, 0.8766666666666667, 1.0, 1.0, 0.4441926808825263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2090946256375295, 0.2090946256375295, 0.28340002991752966], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.8441036], dtype=float32), -0.30215952]. 
=============================================
[2019-03-27 14:30:43,475] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4903658e-29 1.0000000e+00 1.2772310e-36 0.0000000e+00 1.8983195e-21], sum to 1.0000
[2019-03-27 14:30:43,484] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0010
[2019-03-27 14:30:43,489] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 86.0, 1.0, 2.0, 0.5255676779563936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734411.5238320093, 734411.5238320099, 187699.1656411612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6121800.0000, 
sim time next is 6122400.0000, 
raw observation next is [27.2, 86.0, 1.0, 2.0, 0.5253805049346134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734149.8838328974, 734149.8838328974, 187668.4385286001], 
processed observation next is [1.0, 0.8695652173913043, 0.4881516587677725, 0.86, 1.0, 1.0, 0.42816928305375107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20393052328691594, 0.20393052328691594, 0.28010214705761205], 
reward next is 0.7199, 
noisyNet noise sample is [array([-0.93229514], dtype=float32), -0.26269263]. 
=============================================
[2019-03-27 14:30:43,578] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4866766e-27 1.0000000e+00 6.8950196e-34 5.6387380e-37 7.2040380e-19], sum to 1.0000
[2019-03-27 14:30:43,584] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5212
[2019-03-27 14:30:43,590] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 85.5, 1.0, 2.0, 0.5575061115300578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779057.6394755057, 779057.6394755057, 193096.6139462574], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5950200.0000, 
sim time next is 5950800.0000, 
raw observation next is [28.1, 86.0, 1.0, 2.0, 0.5581219703799527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779918.55503559, 779918.55503559, 193203.6357014503], 
processed observation next is [1.0, 0.9130434782608695, 0.5308056872037916, 0.86, 1.0, 1.0, 0.4676168317830755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21664404306544166, 0.21664404306544166, 0.2883636353752989], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.5837379], dtype=float32), 0.059417345]. 
=============================================
[2019-03-27 14:30:48,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5016433e-10 8.1150909e-05 3.7583340e-18 7.8367703e-12 9.9991882e-01], sum to 1.0000
[2019-03-27 14:30:48,632] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9448
[2019-03-27 14:30:48,638] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 68.16666666666667, 1.0, 2.0, 0.6099949508267876, 1.0, 2.0, 0.6099949508267876, 1.0, 2.0, 1.03, 6.944205078467771, 6.9112, 170.5573041426782, 2559271.327364173, 2535628.447096955, 491631.4241135793], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6005400.0000, 
sim time next is 6006000.0000, 
raw observation next is [31.0, 69.33333333333334, 1.0, 2.0, 0.500492017898791, 1.0, 2.0, 0.500492017898791, 1.0, 2.0, 0.8691891198506598, 6.9112, 6.9112, 170.5573041426782, 2099443.279677471, 2099443.279677471, 415486.3171898398], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.6933333333333335, 1.0, 1.0, 0.39818315409492894, 1.0, 1.0, 0.39818315409492894, 1.0, 1.0, 0.8404745364032437, 0.0, 0.0, 0.8375144448122397, 0.5831786887992976, 0.5831786887992976, 0.6201288316266266], 
reward next is 0.3799, 
noisyNet noise sample is [array([-0.6271363], dtype=float32), 1.0875864]. 
=============================================
[2019-03-27 14:30:48,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[60.96166 ]
 [60.5492  ]
 [59.91219 ]
 [59.354477]
 [58.532547]], R is [[61.28430176]
 [60.77265549]
 [60.44353104]
 [59.95306015]
 [59.41619873]].
[2019-03-27 14:30:51,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4163314e-19 1.0000000e+00 6.0970332e-26 1.3358911e-26 3.4192696e-12], sum to 1.0000
[2019-03-27 14:30:51,499] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4861
[2019-03-27 14:30:51,504] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 88.0, 1.0, 2.0, 0.7225304708573849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1009771.888934351, 1009771.888934351, 225784.9448147022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6161400.0000, 
sim time next is 6162000.0000, 
raw observation next is [27.66666666666666, 87.66666666666667, 1.0, 2.0, 0.8856103043301344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1237816.822793957, 1237816.822793957, 266011.4167593589], 
processed observation next is [1.0, 0.30434782608695654, 0.5102685624012636, 0.8766666666666667, 1.0, 1.0, 0.8621810895543788, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34383800633165473, 0.34383800633165473, 0.397031965312476], 
reward next is 0.6030, 
noisyNet noise sample is [array([-0.58681923], dtype=float32), 1.3033433]. 
=============================================
[2019-03-27 14:30:51,519] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[56.579044]
 [56.74737 ]
 [56.704647]
 [56.493633]
 [55.94166 ]], R is [[56.29227066]
 [56.39235306]
 [56.49557495]
 [56.60816956]
 [56.71624756]].
[2019-03-27 14:30:55,431] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 14:30:55,434] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:30:55,434] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:30:55,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:30:55,436] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:30:55,436] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:30:55,439] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:30:55,439] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:30:55,438] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:30:55,445] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:30:55,445] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:30:55,477] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run67
[2019-03-27 14:30:55,498] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run67
[2019-03-27 14:30:55,522] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run67
[2019-03-27 14:30:55,543] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run67
[2019-03-27 14:30:55,562] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run67
[2019-03-27 14:31:22,251] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05928102], dtype=float32), 0.07652195]
[2019-03-27 14:31:22,254] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.38333333333333, 84.33333333333334, 1.0, 2.0, 0.5090808439829277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756851.843747224, 756851.8437472247, 190685.0966113087]
[2019-03-27 14:31:22,255] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:31:22,259] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6080889e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9409862e-26], sampled 0.8664109340464015
[2019-03-27 14:31:45,555] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05928102], dtype=float32), 0.07652195]
[2019-03-27 14:31:45,558] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.04993531333333, 88.09893955, 1.0, 2.0, 0.3250573879085423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520220.1947025566, 520220.1947025572, 168624.3697377282]
[2019-03-27 14:31:45,559] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:31:45,563] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8251115e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5804417e-26], sampled 0.31219199280575294
[2019-03-27 14:31:46,307] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05928102], dtype=float32), 0.07652195]
[2019-03-27 14:31:46,308] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.13333333333333, 53.33333333333334, 1.0, 2.0, 0.4627829252061056, 1.0, 2.0, 0.4627829252061056, 1.0, 2.0, 0.8037008964309803, 6.9112, 6.9112, 169.0403247858759, 1941135.206675653, 1941135.206675653, 390067.1437863575]
[2019-03-27 14:31:46,310] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:31:46,313] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3369744e-10 2.6949303e-04 7.0208701e-17 2.4865265e-10 9.9973053e-01], sampled 0.9935339946853929
[2019-03-27 14:32:44,467] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05928102], dtype=float32), 0.07652195]
[2019-03-27 14:32:44,467] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.01197574, 59.4717382, 1.0, 2.0, 0.9436411715809484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1331799.289720348, 1331799.289720348, 284173.6501822245]
[2019-03-27 14:32:44,469] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:32:44,473] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.6202833e-17 9.9999988e-01 4.2145263e-23 1.1400953e-21 1.1827295e-07], sampled 0.3718050469295414
[2019-03-27 14:32:46,398] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05928102], dtype=float32), 0.07652195]
[2019-03-27 14:32:46,400] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.93333333333333, 90.0, 1.0, 2.0, 0.4347866979369849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 638596.3101569397, 638596.3101569397, 177957.5765295447]
[2019-03-27 14:32:46,400] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:32:46,403] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3161782e-27 1.0000000e+00 1.8786906e-34 0.0000000e+00 1.3643243e-19], sampled 0.81966930679598
[2019-03-27 14:33:02,824] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8433.6636 2927026858.8117 965.0000
[2019-03-27 14:33:03,726] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8787.1176 2779733701.0476 644.0000
[2019-03-27 14:33:03,972] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8693.2619 2836637764.5098 687.0000
[2019-03-27 14:33:04,160] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8344.7911 2994817595.2658 929.0000
[2019-03-27 14:33:04,305] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8122.1702 3153797313.3084 1289.0000
[2019-03-27 14:33:05,321] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1650000, evaluation results [1650000.0, 8122.170221844991, 3153797313.308386, 1289.0, 8433.663622459344, 2927026858.811663, 965.0, 8787.117559738612, 2779733701.04756, 644.0, 8344.791097046793, 2994817595.2658434, 929.0, 8693.261889282476, 2836637764.509849, 687.0]
[2019-03-27 14:33:05,328] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4651882e-17 1.0000000e+00 5.0773111e-24 7.0340518e-24 5.0967819e-10], sum to 1.0000
[2019-03-27 14:33:05,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9315
[2019-03-27 14:33:05,338] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.23333333333333, 89.33333333333333, 1.0, 2.0, 0.6827572529635202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 954161.8889304552, 954161.8889304558, 217168.3687943614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6158400.0000, 
sim time next is 6159000.0000, 
raw observation next is [27.31666666666666, 89.16666666666667, 1.0, 2.0, 0.6866506204013002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 959605.3773132082, 959605.3773132089, 217991.2683076609], 
processed observation next is [1.0, 0.2608695652173913, 0.493680884676145, 0.8916666666666667, 1.0, 1.0, 0.6224706269895183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26655704925366897, 0.26655704925366913, 0.3253601019517327], 
reward next is 0.6746, 
noisyNet noise sample is [array([1.460061], dtype=float32), -0.6565562]. 
=============================================
[2019-03-27 14:33:05,369] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[53.688343]
 [52.859196]
 [52.453453]
 [52.793484]
 [53.0563  ]], R is [[54.39495468]
 [54.52687454]
 [54.65777588]
 [54.78749466]
 [54.91345215]].
[2019-03-27 14:33:12,803] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.21933599e-23 1.00000000e+00 1.01414144e-29 1.73074017e-31
 1.74483298e-13], sum to 1.0000
[2019-03-27 14:33:12,808] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9475
[2019-03-27 14:33:12,814] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 85.0, 1.0, 2.0, 0.5310323353892278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742050.3291968158, 742050.3291968158, 188601.2464162806], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6206400.0000, 
sim time next is 6207000.0000, 
raw observation next is [27.46666666666667, 85.16666666666667, 1.0, 2.0, 0.5308337023828295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741772.6678518901, 741772.6678518901, 188568.29408851], 
processed observation next is [1.0, 0.8695652173913043, 0.500789889415482, 0.8516666666666667, 1.0, 1.0, 0.43473940046124027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2060479632921917, 0.2060479632921917, 0.2814452150574776], 
reward next is 0.7186, 
noisyNet noise sample is [array([0.8463163], dtype=float32), -0.6311437]. 
=============================================
[2019-03-27 14:33:12,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.1476 ]
 [72.02782]
 [72.15185]
 [72.13697]
 [72.33023]], R is [[72.04678345]
 [72.04482269]
 [72.04305267]
 [72.04146576]
 [72.03990936]].
[2019-03-27 14:33:16,669] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.8975190e-19 1.0000000e+00 9.8676939e-27 2.1233775e-26 2.4016145e-10], sum to 1.0000
[2019-03-27 14:33:16,682] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0422
[2019-03-27 14:33:16,693] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 92.66666666666667, 1.0, 2.0, 0.655766349758343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 916425.4863003239, 916425.4863003232, 211586.4064488214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6496800.0000, 
sim time next is 6497400.0000, 
raw observation next is [26.11666666666667, 92.83333333333333, 1.0, 2.0, 0.6530760839191702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 912664.2571650674, 912664.257165068, 211042.0464341715], 
processed observation next is [1.0, 0.17391304347826086, 0.43680884676145365, 0.9283333333333332, 1.0, 1.0, 0.5820193782158677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2535178492125187, 0.25351784921251885, 0.31498812900622614], 
reward next is 0.6850, 
noisyNet noise sample is [array([0.34434497], dtype=float32), 0.27707258]. 
=============================================
[2019-03-27 14:33:17,636] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6190096e-14 9.9992061e-01 7.3535771e-22 5.0650072e-18 7.9338271e-05], sum to 1.0000
[2019-03-27 14:33:17,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3342
[2019-03-27 14:33:17,650] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.08333333333334, 65.0, 1.0, 2.0, 0.5852770309060226, 0.0, 2.0, 0.0, 1.0, 1.0, 1.0010787667969, 6.911200000000001, 6.9112, 168.9125862043714, 1636389.461296305, 1636389.461296304, 354960.3633621943], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6613800.0000, 
sim time next is 6614400.0000, 
raw observation next is [31.16666666666667, 64.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 9.848341007791397, 6.9112, 168.8958639156804, 3538549.255137323, 1455051.189734302, 306801.0885041529], 
processed observation next is [1.0, 0.5652173913043478, 0.6761453396524489, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.29371410077913973, 0.0, 0.8293560126870535, 0.9829303486492563, 0.4041808860373061, 0.4579120723942581], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.68963265], dtype=float32), 0.3533004]. 
=============================================
[2019-03-27 14:33:28,021] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.9088652e-16 9.9999547e-01 8.0907437e-22 1.0563078e-20 4.5717825e-06], sum to 1.0000
[2019-03-27 14:33:28,032] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9286
[2019-03-27 14:33:28,037] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 80.0, 1.0, 2.0, 0.7369009601951348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029865.093325775, 1029865.093325775, 229010.8452252608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6422400.0000, 
sim time next is 6423000.0000, 
raw observation next is [28.0, 79.5, 1.0, 2.0, 0.8306915036831023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161014.891009867, 1161014.891009867, 251582.8259627409], 
processed observation next is [1.0, 0.34782608695652173, 0.5260663507109005, 0.795, 1.0, 1.0, 0.7960138598591594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3225041363916297, 0.3225041363916297, 0.37549675516827], 
reward next is 0.6245, 
noisyNet noise sample is [array([-0.26654142], dtype=float32), 1.0860562]. 
=============================================
[2019-03-27 14:33:28,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[60.270233]
 [60.10944 ]
 [59.966595]
 [59.607273]
 [59.298214]], R is [[60.26083755]
 [60.31642151]
 [60.37474442]
 [60.429039  ]
 [60.47029495]].
[2019-03-27 14:33:35,883] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6807523e-10 4.3764252e-02 1.4499581e-17 3.7540436e-11 9.5623577e-01], sum to 1.0000
[2019-03-27 14:33:35,896] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7213
[2019-03-27 14:33:35,902] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.86666666666667, 74.0, 1.0, 2.0, 0.420850245462413, 1.0, 2.0, 0.420850245462413, 1.0, 2.0, 0.7206422038137834, 6.9112, 6.9112, 170.5573041426782, 1765090.061769981, 1765090.061769981, 363201.6983051518], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6691200.0000, 
sim time next is 6691800.0000, 
raw observation next is [29.03333333333333, 73.0, 1.0, 2.0, 0.4255033350777748, 1.0, 2.0, 0.4255033350777748, 1.0, 2.0, 0.7286641885279987, 6.911200000000001, 6.9112, 170.5573041426782, 1784621.858877669, 1784621.858877669, 365897.8994225314], 
processed observation next is [1.0, 0.43478260869565216, 0.5750394944707741, 0.73, 1.0, 1.0, 0.30783534346719854, 1.0, 1.0, 0.30783534346719854, 1.0, 1.0, 0.6691026689365839, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4957282941326859, 0.4957282941326859, 0.546116267794823], 
reward next is 0.4539, 
noisyNet noise sample is [array([1.0209647], dtype=float32), -0.1815528]. 
=============================================
[2019-03-27 14:33:36,947] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.2184268e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2391442e-25], sum to 1.0000
[2019-03-27 14:33:36,956] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2729
[2019-03-27 14:33:36,964] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.06666666666667, 67.0, 1.0, 2.0, 0.4625701752838414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646354.0396361843, 646354.0396361843, 177940.2774703909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6716400.0000, 
sim time next is 6717000.0000, 
raw observation next is [28.93333333333333, 67.0, 1.0, 2.0, 0.4654666963528185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650402.617155248, 650402.617155248, 178362.7854083443], 
processed observation next is [1.0, 0.7391304347826086, 0.5703001579778829, 0.67, 1.0, 1.0, 0.3559839715094199, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18066739365423554, 0.18066739365423554, 0.2662131125497676], 
reward next is 0.7338, 
noisyNet noise sample is [array([-1.500724], dtype=float32), -0.08704503]. 
=============================================
[2019-03-27 14:33:36,989] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.93589]
 [78.61052]
 [78.20995]
 [78.1563 ]
 [76.94798]], R is [[78.61115265]
 [78.5594635 ]
 [78.50977325]
 [77.72467804]
 [76.94743347]].
[2019-03-27 14:33:47,711] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.856179e-33 1.000000e+00 0.000000e+00 0.000000e+00 4.724576e-27], sum to 1.0000
[2019-03-27 14:33:47,720] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9396
[2019-03-27 14:33:47,728] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 70.66666666666666, 1.0, 2.0, 0.7568114523384156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1181391.360127434, 1181391.360127434, 249189.4448756497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6770400.0000, 
sim time next is 6771000.0000, 
raw observation next is [24.96666666666667, 69.33333333333334, 1.0, 2.0, 0.7703379065677078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1201873.870929913, 1201873.870929913, 252678.3682327296], 
processed observation next is [1.0, 0.34782608695652173, 0.3823064770932071, 0.6933333333333335, 1.0, 1.0, 0.7232986826116962, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3338538530360869, 0.3338538530360869, 0.3771318928846711], 
reward next is 0.6229, 
noisyNet noise sample is [array([1.013819], dtype=float32), -0.28198633]. 
=============================================
[2019-03-27 14:33:47,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[66.37903 ]
 [66.79493 ]
 [67.73739 ]
 [69.261696]
 [69.41786 ]], R is [[65.91802979]
 [65.88692474]
 [65.85662079]
 [65.86219025]
 [65.93572998]].
[2019-03-27 14:34:05,177] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.332504e-37], sum to 1.0000
[2019-03-27 14:34:05,189] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3774
[2019-03-27 14:34:05,200] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333334, 73.66666666666667, 1.0, 2.0, 0.4386159301385224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632901.8878519399, 632901.8878519406, 177109.0428965779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6993600.0000, 
sim time next is 6994200.0000, 
raw observation next is [26.71666666666667, 74.83333333333333, 1.0, 2.0, 0.4421174907599544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 636186.8733373553, 636186.8733373553, 177390.3630573587], 
processed observation next is [0.0, 0.9565217391304348, 0.46524486571879947, 0.7483333333333333, 1.0, 1.0, 0.32785239850596914, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17671857592704315, 0.17671857592704315, 0.26476173590650554], 
reward next is 0.7352, 
noisyNet noise sample is [array([1.9268943], dtype=float32), 1.3880856]. 
=============================================
[2019-03-27 14:34:09,055] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.5843391e-14 9.9999487e-01 3.6122551e-20 4.1384586e-18 5.1801203e-06], sum to 1.0000
[2019-03-27 14:34:09,065] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9686
[2019-03-27 14:34:09,081] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1921469.258410015 W.
[2019-03-27 14:34:09,089] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.25, 76.0, 1.0, 2.0, 0.4581023194892126, 1.0, 2.0, 0.4581023194892126, 1.0, 2.0, 0.7926846993607962, 6.9112, 6.9112, 170.5573041426782, 1921469.258410015, 1921469.258410015, 386910.8365014987], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7212600.0000, 
sim time next is 7213200.0000, 
raw observation next is [29.16666666666666, 77.0, 1.0, 2.0, 0.7711845340866766, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.994355464066912, 6.9112, 168.9124615816669, 1974745.8297639, 1915752.654419967, 401285.7214345585], 
processed observation next is [1.0, 0.4782608695652174, 0.5813586097946285, 0.77, 1.0, 1.0, 0.7243187157670803, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008315546406691165, 0.0, 0.8294375148254916, 0.54854050826775, 0.5321535151166575, 0.5989339125888933], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7624788], dtype=float32), -0.8268508]. 
=============================================
[2019-03-27 14:34:11,370] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 14:34:11,373] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:34:11,373] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:34:11,374] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:34:11,374] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:34:11,376] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:34:11,377] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:34:11,375] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:34:11,379] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:34:11,378] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:34:11,380] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:34:11,410] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run68
[2019-03-27 14:34:11,434] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run68
[2019-03-27 14:34:11,457] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run68
[2019-03-27 14:34:11,458] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run68
[2019-03-27 14:34:11,498] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run68
[2019-03-27 14:34:15,576] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05711272], dtype=float32), 0.07580967]
[2019-03-27 14:34:15,579] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.4, 93.0, 1.0, 2.0, 0.2903677292345507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 466384.9573950135, 466384.9573950141, 164707.2559364908]
[2019-03-27 14:34:15,579] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:34:15,580] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.34470410988737477
[2019-03-27 14:34:16,438] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05711272], dtype=float32), 0.07580967]
[2019-03-27 14:34:16,439] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.34787222, 90.12892260000001, 1.0, 2.0, 0.3270790941934179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509708.3742143783, 509708.3742143783, 167580.873636141]
[2019-03-27 14:34:16,439] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:34:16,440] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7593534e-36], sampled 0.659344857346403
[2019-03-27 14:34:22,991] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05711272], dtype=float32), 0.07580967]
[2019-03-27 14:34:22,993] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.79403462, 52.04095515, 1.0, 2.0, 0.484050027069894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777926.680377644, 777926.680377644, 192301.7642147008]
[2019-03-27 14:34:22,994] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:34:22,997] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.578027e-35], sampled 0.9344464808106021
[2019-03-27 14:34:38,793] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05711272], dtype=float32), 0.07580967]
[2019-03-27 14:34:38,793] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.40968711, 75.79603656, 1.0, 2.0, 0.6133959606686582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 857189.4890202854, 857189.489020286, 203263.6150110571]
[2019-03-27 14:34:38,797] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:34:38,799] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.713738e-38 1.000000e+00 0.000000e+00 0.000000e+00 7.570914e-32], sampled 0.1849582276873637
[2019-03-27 14:34:54,154] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05711272], dtype=float32), 0.07580967]
[2019-03-27 14:34:54,157] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.78824344, 84.12307983333334, 1.0, 2.0, 0.5128820750944253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 716679.0867772667, 716679.0867772674, 185638.4288122678]
[2019-03-27 14:34:54,158] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:34:54,164] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9154229e-34], sampled 0.9683914558840678
[2019-03-27 14:35:05,339] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05711272], dtype=float32), 0.07580967]
[2019-03-27 14:35:05,340] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.23333333333333, 57.0, 1.0, 2.0, 0.562241867139418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785677.8199709582, 785677.8199709582, 193921.9480306831]
[2019-03-27 14:35:05,341] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:35:05,346] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.9235584e-33], sampled 0.5400954017218936
[2019-03-27 14:35:12,618] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05711272], dtype=float32), 0.07580967]
[2019-03-27 14:35:12,618] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.36041484, 77.39291736999999, 1.0, 2.0, 0.4342942801015436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 643593.8841986973, 643593.8841986967, 178580.9295064092]
[2019-03-27 14:35:12,620] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:35:12,622] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1969882e-32], sampled 0.1720798866646397
[2019-03-27 14:35:51,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05711272], dtype=float32), 0.07580967]
[2019-03-27 14:35:51,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.6, 73.0, 1.0, 2.0, 0.9769369253622813, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988669529240839, 6.9112, 168.9124306943434, 2262721.484490249, 2207762.105237219, 456932.6411731181]
[2019-03-27 14:35:51,831] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:35:51,834] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.6988868e-14 9.9995267e-01 2.6633771e-20 2.1514892e-17 4.7334794e-05], sampled 0.37204682516075704
[2019-03-27 14:35:51,835] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2262721.484490249 W.
[2019-03-27 14:36:10,878] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05711272], dtype=float32), 0.07580967]
[2019-03-27 14:36:10,881] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.35, 86.5, 1.0, 2.0, 0.5438435793176702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759958.8215217746, 759958.8215217746, 190749.9766033631]
[2019-03-27 14:36:10,883] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:36:10,886] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.525982e-38 1.000000e+00 0.000000e+00 0.000000e+00 2.391446e-31], sampled 0.4617910022849736
[2019-03-27 14:36:19,032] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.9339 2927631855.6521 1355.0000
[2019-03-27 14:36:19,197] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.7683 2779461651.7900 936.0000
[2019-03-27 14:36:19,254] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2216 3007769090.0527 1770.0000
[2019-03-27 14:36:19,536] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.4257 2842466641.1927 1145.0000
[2019-03-27 14:36:19,565] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.8249 3164170227.4008 1826.0000
[2019-03-27 14:36:20,583] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1675000, evaluation results [1675000.0, 7875.824867272314, 3164170227.400794, 1826.0, 8249.933945747602, 2927631855.6520705, 1355.0, 8658.768303271105, 2779461651.7899995, 936.0, 7998.221597952647, 3007769090.052668, 1770.0, 8495.425717214737, 2842466641.192746, 1145.0]
[2019-03-27 14:36:27,548] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4113903e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0661568e-28], sum to 1.0000
[2019-03-27 14:36:27,560] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8165
[2019-03-27 14:36:27,568] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.58333333333334, 59.5, 1.0, 2.0, 0.5314612537988196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 810713.4058482694, 810713.4058482688, 196925.4549290281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7305000.0000, 
sim time next is 7305600.0000, 
raw observation next is [27.66666666666667, 59.00000000000001, 1.0, 2.0, 0.8192553128984903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1250146.897308022, 1250146.897308022, 262814.2048696524], 
processed observation next is [1.0, 0.5652173913043478, 0.5102685624012641, 0.5900000000000001, 1.0, 1.0, 0.782235316745169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3472630270300061, 0.3472630270300061, 0.3922600072681379], 
reward next is 0.6077, 
noisyNet noise sample is [array([0.91817415], dtype=float32), 0.71197736]. 
=============================================
[2019-03-27 14:36:32,120] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 9.183172e-38], sum to 1.0000
[2019-03-27 14:36:32,128] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4400
[2019-03-27 14:36:32,135] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 83.0, 1.0, 2.0, 0.2872973960283477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462798.8579883144, 462798.8579883144, 164464.1637426504], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7414200.0000, 
sim time next is 7414800.0000, 
raw observation next is [21.56666666666667, 82.66666666666667, 1.0, 2.0, 0.2874157617010465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462797.0075401866, 462797.0075401873, 164463.9532688753], 
processed observation next is [1.0, 0.8260869565217391, 0.22116903633491333, 0.8266666666666667, 1.0, 1.0, 0.14146477313379094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1285547243167185, 0.1285547243167187, 0.2454685869684706], 
reward next is 0.7545, 
noisyNet noise sample is [array([-0.55332315], dtype=float32), -1.1831068]. 
=============================================
[2019-03-27 14:36:32,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6905744e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.3771601e-31], sum to 1.0000
[2019-03-27 14:36:32,636] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9507
[2019-03-27 14:36:32,644] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.46666666666667, 89.33333333333333, 1.0, 2.0, 0.5318350685422644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859660.5847458986, 859660.5847458986, 201295.2674595128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7404000.0000, 
sim time next is 7404600.0000, 
raw observation next is [20.43333333333333, 89.16666666666667, 1.0, 2.0, 0.537329516017936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869358.8287553352, 869358.8287553352, 202392.1847829527], 
processed observation next is [1.0, 0.6956521739130435, 0.1674565560821484, 0.8916666666666667, 1.0, 1.0, 0.44256568194932044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24148856354314865, 0.24148856354314865, 0.30207788773575034], 
reward next is 0.6979, 
noisyNet noise sample is [array([0.9318045], dtype=float32), 0.11051971]. 
=============================================
[2019-03-27 14:36:37,485] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 9.473891e-36], sum to 1.0000
[2019-03-27 14:36:37,497] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5671
[2019-03-27 14:36:37,505] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.45, 90.66666666666667, 1.0, 2.0, 0.3474194645464314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548833.0656638375, 548833.0656638375, 170837.2225097524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7276200.0000, 
sim time next is 7276800.0000, 
raw observation next is [21.5, 90.33333333333334, 1.0, 2.0, 0.3234665662315713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 510886.6264841023, 510886.6264841029, 167836.885513008], 
processed observation next is [1.0, 0.21739130434782608, 0.21800947867298584, 0.9033333333333334, 1.0, 1.0, 0.18489947738743526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14191295180113953, 0.1419129518011397, 0.2505028141985194], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.91823393], dtype=float32), -1.8831912]. 
=============================================
[2019-03-27 14:36:43,751] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.680902e-37], sum to 1.0000
[2019-03-27 14:36:43,764] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8838
[2019-03-27 14:36:43,769] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333334, 91.66666666666667, 1.0, 2.0, 0.3149976963032695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497502.8806715659, 497502.8806715665, 166826.3545428564], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7435200.0000, 
sim time next is 7435800.0000, 
raw observation next is [21.35, 91.5, 1.0, 2.0, 0.3145421837505583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496828.6231944223, 496828.6231944217, 166777.0656819067], 
processed observation next is [0.0, 0.043478260869565216, 0.2109004739336494, 0.915, 1.0, 1.0, 0.17414720933802205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13800795088733953, 0.13800795088733936, 0.24892099355508465], 
reward next is 0.7511, 
noisyNet noise sample is [array([-0.11625833], dtype=float32), -1.6703986]. 
=============================================
[2019-03-27 14:36:45,640] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4138741e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.2665289e-32], sum to 1.0000
[2019-03-27 14:36:45,650] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4420
[2019-03-27 14:36:45,659] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 91.5, 1.0, 2.0, 0.4819867045926162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673493.5551222712, 673493.5551222712, 180821.801901838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7597800.0000, 
sim time next is 7598400.0000, 
raw observation next is [25.13333333333333, 92.0, 1.0, 2.0, 0.4824504968940798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674141.8307629039, 674141.8307629032, 180891.9996816422], 
processed observation next is [0.0, 0.9565217391304348, 0.3902053712480251, 0.92, 1.0, 1.0, 0.37644638180009615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1872616196563622, 0.187261619656362, 0.2699880592263316], 
reward next is 0.7300, 
noisyNet noise sample is [array([-2.2457564], dtype=float32), 0.5215587]. 
=============================================
[2019-03-27 14:36:48,706] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0313487e-38], sum to 1.0000
[2019-03-27 14:36:48,717] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8986
[2019-03-27 14:36:48,725] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 95.0, 1.0, 2.0, 0.3283968108683529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512782.1031462479, 512782.1031462479, 167846.8008654031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7453800.0000, 
sim time next is 7454400.0000, 
raw observation next is [21.43333333333333, 95.0, 1.0, 2.0, 0.3293756249311166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513858.8195547291, 513858.8195547291, 167917.9562271235], 
processed observation next is [0.0, 0.2608695652173913, 0.21484992101105835, 0.95, 1.0, 1.0, 0.19201882521821279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14273856098742474, 0.14273856098742474, 0.25062381526436345], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.8985727], dtype=float32), 0.26338604]. 
=============================================
[2019-03-27 14:36:56,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.634704e-36], sum to 1.0000
[2019-03-27 14:36:56,575] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2948
[2019-03-27 14:36:56,582] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 60.0, 1.0, 2.0, 0.4268624074058939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616395.5155018282, 616395.5155018289, 175501.8653789267], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7568400.0000, 
sim time next is 7569000.0000, 
raw observation next is [29.5, 60.5, 1.0, 2.0, 0.4363663616191771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 624022.2027819117, 624022.2027819117, 176073.0586350189], 
processed observation next is [0.0, 0.6086956521739131, 0.5971563981042655, 0.605, 1.0, 1.0, 0.32092332725202055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17333950077275326, 0.17333950077275326, 0.2627956099030133], 
reward next is 0.7372, 
noisyNet noise sample is [array([-1.7668977], dtype=float32), -0.9302696]. 
=============================================
[2019-03-27 14:36:56,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.79437 ]
 [75.83539 ]
 [75.8396  ]
 [75.70624 ]
 [75.706436]], R is [[75.73678589]
 [75.71747589]
 [75.69914246]
 [75.68135071]
 [75.66297913]].
[2019-03-27 14:37:00,737] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1250739e-20 1.0000000e+00 6.1719129e-27 3.7694879e-29 6.9797971e-15], sum to 1.0000
[2019-03-27 14:37:00,743] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0667
[2019-03-27 14:37:00,752] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1695739.304878218 W.
[2019-03-27 14:37:00,759] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.3, 79.0, 1.0, 2.0, 0.6064920365474394, 1.0, 2.0, 0.6064920365474394, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1695739.304878218, 1695739.304878217, 336660.1311590239], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7808400.0000, 
sim time next is 7809000.0000, 
raw observation next is [28.41666666666667, 78.33333333333334, 1.0, 2.0, 0.403592529386316, 1.0, 2.0, 0.403592529386316, 1.0, 1.0, 0.6931192277055901, 6.9112, 6.9112, 170.5573041426782, 1692652.225410782, 1692652.225410782, 353781.8479856129], 
processed observation next is [1.0, 0.391304347826087, 0.5458135860979465, 0.7833333333333334, 1.0, 1.0, 0.28143678239315184, 1.0, 1.0, 0.28143678239315184, 1.0, 0.5, 0.6257551557385244, 0.0, 0.0, 0.8375144448122397, 0.4701811737252172, 0.4701811737252172, 0.5280326089337506], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2789406], dtype=float32), -0.63724434]. 
=============================================
[2019-03-27 14:37:00,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[56.10785 ]
 [55.950874]
 [59.295265]
 [62.249535]
 [66.20688 ]], R is [[53.13769531]
 [52.60631943]
 [52.53573227]
 [52.45471573]
 [51.93016815]].
[2019-03-27 14:37:02,497] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:02,497] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:02,575] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run9
[2019-03-27 14:37:03,057] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6421869e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0143264e-34], sum to 1.0000
[2019-03-27 14:37:03,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3469
[2019-03-27 14:37:03,071] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 88.66666666666667, 1.0, 2.0, 0.4791948013832009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669591.1208661617, 669591.1208661622, 180400.1578883383], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7687200.0000, 
sim time next is 7687800.0000, 
raw observation next is [25.35, 89.0, 1.0, 2.0, 0.4800516621168904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670788.812251617, 670788.812251617, 180529.1795795794], 
processed observation next is [1.0, 1.0, 0.4004739336492892, 0.89, 1.0, 1.0, 0.37355621941794026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18633022562544915, 0.18633022562544915, 0.2694465366859394], 
reward next is 0.7306, 
noisyNet noise sample is [array([1.2462723], dtype=float32), -0.90701526]. 
=============================================
[2019-03-27 14:37:05,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:05,366] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:05,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run9
[2019-03-27 14:37:08,243] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.9811889e-13 9.9675351e-01 2.9205231e-18 1.1456287e-16 3.2464711e-03], sum to 1.0000
[2019-03-27 14:37:08,252] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7296
[2019-03-27 14:37:08,260] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2157827.303625551 W.
[2019-03-27 14:37:08,267] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.6, 62.0, 1.0, 2.0, 0.5143963402116002, 1.0, 2.0, 0.5143963402116002, 1.0, 2.0, 0.8776215589888762, 6.9112, 6.9112, 170.5573041426782, 2157827.303625551, 2157827.303625551, 422284.2193234096], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7749000.0000, 
sim time next is 7749600.0000, 
raw observation next is [30.5, 62.33333333333334, 1.0, 2.0, 0.8072014129532347, 1.0, 2.0, 0.8072014129532347, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2257503.107586105, 2257503.107586105, 423382.3071143011], 
processed observation next is [1.0, 0.6956521739130435, 0.6445497630331753, 0.6233333333333334, 1.0, 1.0, 0.7677125457267887, 1.0, 1.0, 0.7677125457267887, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6270841965516958, 0.6270841965516958, 0.6319138912153748], 
reward next is 0.3681, 
noisyNet noise sample is [array([0.24908927], dtype=float32), 0.28730905]. 
=============================================
[2019-03-27 14:37:08,613] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:08,613] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:08,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run9
[2019-03-27 14:37:09,128] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3852068e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8135865e-30], sum to 1.0000
[2019-03-27 14:37:09,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6191
[2019-03-27 14:37:09,140] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 89.0, 1.0, 2.0, 0.5296271582216264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740086.0883108559, 740086.0883108565, 188368.1372474113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7942800.0000, 
sim time next is 7943400.0000, 
raw observation next is [26.7, 89.5, 1.0, 2.0, 0.5296601283048128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740132.1758280952, 740132.1758280959, 188373.6006343171], 
processed observation next is [1.0, 0.9565217391304348, 0.46445497630331756, 0.895, 1.0, 1.0, 0.43332545578893106, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20559227106335978, 0.20559227106335998, 0.28115462781241357], 
reward next is 0.7188, 
noisyNet noise sample is [array([-1.2244135], dtype=float32), 0.64090693]. 
=============================================
[2019-03-27 14:37:09,908] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:09,908] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:09,994] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run9
[2019-03-27 14:37:10,448] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:10,449] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:10,522] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run9
[2019-03-27 14:37:10,697] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:10,698] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:10,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run9
[2019-03-27 14:37:11,925] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:11,925] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:11,986] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run9
[2019-03-27 14:37:12,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:12,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:12,244] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-03-27 14:37:12,288] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2160504e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.8594385e-34], sum to 1.0000
[2019-03-27 14:37:12,294] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3179
[2019-03-27 14:37:12,301] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 96.0, 1.0, 2.0, 0.3383558982507733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 528818.2053455696, 528818.2053455702, 169119.0769720839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 165600.0000, 
sim time next is 166200.0000, 
raw observation next is [21.1, 96.0, 1.0, 2.0, 0.334621014927417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524335.8812284133, 524335.8812284126, 168795.4524456441], 
processed observation next is [1.0, 0.9565217391304348, 0.1990521327014219, 0.96, 1.0, 1.0, 0.19833857220170725, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14564885589678148, 0.14564885589678128, 0.25193351111290163], 
reward next is 0.7481, 
noisyNet noise sample is [array([2.216137], dtype=float32), -1.1035155]. 
=============================================
[2019-03-27 14:37:14,411] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:14,412] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:14,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run9
[2019-03-27 14:37:17,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:17,524] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:17,608] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run9
[2019-03-27 14:37:17,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:17,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:17,902] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run9
[2019-03-27 14:37:19,152] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:19,152] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:19,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run9
[2019-03-27 14:37:20,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:20,010] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:20,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run9
[2019-03-27 14:37:20,140] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:20,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:20,192] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run9
[2019-03-27 14:37:20,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:20,946] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:20,967] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:37:20,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4953
[2019-03-27 14:37:20,978] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 92.0, 1.0, 2.0, 0.2706050916224741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 438197.227455016, 438197.227455016, 162818.2025584676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 280800.0000, 
sim time next is 281400.0000, 
raw observation next is [20.25, 91.33333333333334, 1.0, 2.0, 0.2725676136235184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440822.8523189685, 440822.8523189685, 162992.3844759037], 
processed observation next is [0.0, 0.2608695652173913, 0.1587677725118484, 0.9133333333333334, 1.0, 1.0, 0.12357543810062456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12245079231082459, 0.12245079231082459, 0.24327221563567716], 
reward next is 0.7567, 
noisyNet noise sample is [array([1.1200145], dtype=float32), 1.4414864]. 
=============================================
[2019-03-27 14:37:21,008] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run9
[2019-03-27 14:37:21,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 14:37:21,111] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:21,160] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run9
[2019-03-27 14:37:23,440] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 14:37:23,444] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:37:23,445] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:23,447] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:37:23,448] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:37:23,451] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:37:23,451] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:23,451] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:23,455] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:37:23,452] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:23,459] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:37:23,483] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run69
[2019-03-27 14:37:23,506] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run69
[2019-03-27 14:37:23,542] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run69
[2019-03-27 14:37:23,562] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run69
[2019-03-27 14:37:23,582] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run69
[2019-03-27 14:37:27,788] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05863501], dtype=float32), 0.080674365]
[2019-03-27 14:37:27,790] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.5, 52.0, 1.0, 2.0, 0.3157451395306572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500055.9411278322, 500055.9411278322, 167043.8471587906]
[2019-03-27 14:37:27,791] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:37:27,793] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8133710570392393
[2019-03-27 14:37:38,511] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05863501], dtype=float32), 0.080674365]
[2019-03-27 14:37:38,511] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.15, 90.33333333333333, 1.0, 2.0, 0.3427558536610344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532549.5758201008, 532549.5758201008, 169339.3548983001]
[2019-03-27 14:37:38,513] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:37:38,517] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05928165238480654
[2019-03-27 14:37:45,452] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05863501], dtype=float32), 0.080674365]
[2019-03-27 14:37:45,452] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.43333333333333, 73.83333333333333, 1.0, 2.0, 0.2576204383790148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 423485.9721452699, 423485.9721452699, 161668.7780846427]
[2019-03-27 14:37:45,452] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:37:45,455] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6751654833542617
[2019-03-27 14:39:01,832] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05863501], dtype=float32), 0.080674365]
[2019-03-27 14:39:01,833] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.65, 91.16666666666667, 1.0, 2.0, 0.5363461140211148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749478.2803711395, 749478.2803711395, 189486.5991619451]
[2019-03-27 14:39:01,834] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:39:01,837] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.4415757e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9654762e-27], sampled 0.6557662298523722
[2019-03-27 14:39:27,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05863501], dtype=float32), 0.080674365]
[2019-03-27 14:39:27,357] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.9, 93.33333333333334, 1.0, 2.0, 0.4493217893935383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650747.1850006422, 650747.1850006428, 178962.3134321063]
[2019-03-27 14:39:27,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:39:27,360] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 7.87086e-38], sampled 0.41614116036587756
[2019-03-27 14:39:31,106] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.5303 2779495564.6640 935.0000
[2019-03-27 14:39:31,777] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.6997 3007741977.1228 1765.0000
[2019-03-27 14:39:31,840] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.3693 2927577331.1704 1361.0000
[2019-03-27 14:39:31,975] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7877.6595 3163802639.9511 1824.0000
[2019-03-27 14:39:32,086] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.5189 2842317223.7170 1151.0000
[2019-03-27 14:39:33,104] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1700000, evaluation results [1700000.0, 7877.659487513501, 3163802639.9511185, 1824.0, 8249.369337052149, 2927577331.17044, 1361.0, 8662.530332814973, 2779495564.663955, 935.0, 8000.699688698461, 3007741977.122799, 1765.0, 8494.51886774087, 2842317223.716976, 1151.0]
[2019-03-27 14:39:34,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7032780e-30 1.0000000e+00 2.8567231e-37 0.0000000e+00 2.4662663e-24], sum to 1.0000
[2019-03-27 14:39:34,484] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2275
[2019-03-27 14:39:34,489] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 67.0, 1.0, 2.0, 0.7945696492078416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1213461.36399597, 1213461.36399597, 256228.012275726], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 39600.0000, 
sim time next is 40200.0000, 
raw observation next is [26.3, 66.66666666666667, 1.0, 2.0, 0.8659858944834563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1320632.66499609, 1320632.66499609, 275943.9148476038], 
processed observation next is [1.0, 0.4782608695652174, 0.4454976303317536, 0.6666666666666667, 1.0, 1.0, 0.8385372222692244, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.36684240694335835, 0.36684240694335835, 0.4118565893247818], 
reward next is 0.5881, 
noisyNet noise sample is [array([-0.6058439], dtype=float32), 0.69673055]. 
=============================================
[2019-03-27 14:39:36,072] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0259740e-30 1.0000000e+00 8.3156319e-38 0.0000000e+00 1.5118032e-23], sum to 1.0000
[2019-03-27 14:39:36,083] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1626
[2019-03-27 14:39:36,089] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.78333333333333, 96.0, 1.0, 2.0, 0.8757011245039606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1304635.5303332, 1304635.5303332, 274777.8038964959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [22.76666666666667, 96.0, 1.0, 2.0, 0.9044035746117289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1348117.53132895, 1348117.53132895, 283243.743922193], 
processed observation next is [1.0, 0.6086956521739131, 0.2780410742496052, 0.96, 1.0, 1.0, 0.884823583869553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37447709203581947, 0.37447709203581947, 0.4227518566002881], 
reward next is 0.5772, 
noisyNet noise sample is [array([-0.48929235], dtype=float32), 1.3777504]. 
=============================================
[2019-03-27 14:39:36,106] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.52879 ]
 [69.12445 ]
 [68.526955]
 [68.64167 ]
 [68.876495]], R is [[68.08362579]
 [67.99267578]
 [67.94616699]
 [67.87859344]
 [67.8139267 ]].
[2019-03-27 14:39:51,377] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.455907e-38], sum to 1.0000
[2019-03-27 14:39:51,389] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7298
[2019-03-27 14:39:51,395] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.5, 87.66666666666667, 1.0, 2.0, 0.2270708616949041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377424.1968030478, 377424.1968030478, 158417.41702413], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 526800.0000, 
sim time next is 527400.0000, 
raw observation next is [18.45, 88.0, 1.0, 2.0, 0.2204669521490859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 366481.11293434, 366481.1129343406, 157820.9812425564], 
processed observation next is [1.0, 0.08695652173913043, 0.07345971563981045, 0.88, 1.0, 1.0, 0.06080355680612758, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10180030914842779, 0.10180030914842794, 0.23555370334709907], 
reward next is 0.7644, 
noisyNet noise sample is [array([-1.4597774], dtype=float32), -1.2041526]. 
=============================================
[2019-03-27 14:40:00,972] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.251623e-35], sum to 1.0000
[2019-03-27 14:40:00,986] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0339
[2019-03-27 14:40:01,000] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 80.33333333333333, 1.0, 2.0, 0.2564244061805682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 420498.6523125551, 420498.6523125558, 161549.4921758973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 416400.0000, 
sim time next is 417000.0000, 
raw observation next is [20.58333333333334, 80.16666666666667, 1.0, 2.0, 0.2540036223956345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 417094.4037067279, 417094.4037067285, 161307.9435036959], 
processed observation next is [1.0, 0.8260869565217391, 0.17456556082148533, 0.8016666666666667, 1.0, 1.0, 0.1012091836091982, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1158595565852022, 0.11585955658520236, 0.24075812463238191], 
reward next is 0.7592, 
noisyNet noise sample is [array([0.69360846], dtype=float32), 0.17467691]. 
=============================================
[2019-03-27 14:40:01,021] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[82.0497  ]
 [82.130424]
 [82.18305 ]
 [82.28533 ]
 [82.41242 ]], R is [[81.93271637]
 [81.87226868]
 [81.81216431]
 [81.75231171]
 [81.69255829]].
[2019-03-27 14:40:17,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:40:17,640] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6769
[2019-03-27 14:40:17,647] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 63.0, 1.0, 2.0, 0.2904307138657254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 465128.1484360214, 465128.1484360207, 164610.1315868421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 824400.0000, 
sim time next is 825000.0000, 
raw observation next is [24.76666666666667, 63.0, 1.0, 2.0, 0.2898091305429463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464338.8374976719, 464338.8374976725, 164557.6143666503], 
processed observation next is [0.0, 0.5652173913043478, 0.3728278041074251, 0.63, 1.0, 1.0, 0.14434835005174254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12898301041601998, 0.12898301041602014, 0.24560837965171686], 
reward next is 0.7544, 
noisyNet noise sample is [array([-0.73845327], dtype=float32), 0.68848604]. 
=============================================
[2019-03-27 14:40:17,664] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.598114]
 [80.43673 ]
 [80.41859 ]
 [80.38651 ]
 [80.34937 ]], R is [[80.59875488]
 [80.54708099]
 [80.49610138]
 [80.44577026]
 [80.39593506]].
[2019-03-27 14:40:22,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.321016e-38], sum to 1.0000
[2019-03-27 14:40:22,739] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6005
[2019-03-27 14:40:22,745] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.5, 1.0, 2.0, 0.3263890627998525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506904.7506743953, 506904.7506743953, 167311.231182964], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 970200.0000, 
sim time next is 970800.0000, 
raw observation next is [21.9, 92.66666666666667, 1.0, 2.0, 0.3269795633378633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 507537.7101245375, 507537.7101245369, 167350.9904420424], 
processed observation next is [1.0, 0.21739130434782608, 0.23696682464454974, 0.9266666666666667, 1.0, 1.0, 0.18913200402152208, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14098269725681598, 0.1409826972568158, 0.24977759767469018], 
reward next is 0.7502, 
noisyNet noise sample is [array([0.2517294], dtype=float32), 0.7569436]. 
=============================================
[2019-03-27 14:40:39,168] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 14:40:39,170] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:40:39,171] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:40:39,171] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:40:39,172] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:40:39,174] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:40:39,176] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:40:39,175] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:40:39,177] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:40:39,178] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:40:39,180] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:40:39,212] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run70
[2019-03-27 14:40:39,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run70
[2019-03-27 14:40:39,256] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run70
[2019-03-27 14:40:39,279] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run70
[2019-03-27 14:40:39,311] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run70
[2019-03-27 14:40:59,606] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05688533], dtype=float32), 0.0821054]
[2019-03-27 14:40:59,608] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.16666666666666, 85.66666666666667, 1.0, 2.0, 0.2391946282972784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 396034.7839114111, 396034.7839114104, 159735.3711313462]
[2019-03-27 14:40:59,608] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:40:59,612] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5420486e-37], sampled 0.34053141022391287
[2019-03-27 14:41:13,521] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05688533], dtype=float32), 0.0821054]
[2019-03-27 14:41:13,522] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.95, 93.5, 1.0, 2.0, 0.4942752407426005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 690670.2538200644, 690670.2538200651, 182703.2787902619]
[2019-03-27 14:41:13,523] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:41:13,527] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1052525e-37], sampled 0.07340586905139945
[2019-03-27 14:41:46,454] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05688533], dtype=float32), 0.0821054]
[2019-03-27 14:41:46,455] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.70826766666667, 73.67306379666667, 1.0, 2.0, 0.5585694718270416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780544.1224502703, 780544.1224502698, 193280.4739853834]
[2019-03-27 14:41:46,455] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:41:46,459] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.7367155e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.6917403e-31], sampled 0.4056942898789807
[2019-03-27 14:41:52,609] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05688533], dtype=float32), 0.0821054]
[2019-03-27 14:41:52,609] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.16666666666666, 45.0, 1.0, 2.0, 0.6882621572246888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 961858.5460430144, 961858.546043015, 218335.6791458556]
[2019-03-27 14:41:52,612] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:41:52,615] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.658651e-37 1.000000e+00 0.000000e+00 0.000000e+00 2.270420e-31], sampled 0.8388627551519441
[2019-03-27 14:41:54,017] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05688533], dtype=float32), 0.0821054]
[2019-03-27 14:41:54,017] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.11562380333334, 73.98926732333334, 1.0, 2.0, 0.6214593235006167, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.967106749429154, 6.9112, 168.9125245515387, 1737635.179997737, 1697973.109218433, 368619.6704167726]
[2019-03-27 14:41:54,017] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:41:54,021] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.76879618e-27 1.00000000e+00 2.97813996e-34 1.09802205e-36
 1.54029043e-18], sampled 0.4098545247235177
[2019-03-27 14:41:54,022] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1737635.179997737 W.
[2019-03-27 14:42:16,137] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05688533], dtype=float32), 0.0821054]
[2019-03-27 14:42:16,138] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.87046488666667, 86.23587213333334, 1.0, 2.0, 0.7472336395484719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044312.758245179, 1044312.758245179, 231372.2849615634]
[2019-03-27 14:42:16,138] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:42:16,143] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2456087e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6339450e-30], sampled 0.17028491291934078
[2019-03-27 14:42:21,935] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05688533], dtype=float32), 0.0821054]
[2019-03-27 14:42:21,936] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.75, 62.16666666666667, 1.0, 2.0, 0.5132764499090148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717230.3550217845, 717230.355021784, 185702.1646817968]
[2019-03-27 14:42:21,937] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:42:21,941] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9684904e-34], sampled 0.08490614101022276
[2019-03-27 14:42:47,629] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.3375 2927428162.8444 1346.0000
[2019-03-27 14:42:47,732] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7452 3007652442.0292 1773.0000
[2019-03-27 14:42:47,744] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1310 2842387210.3754 1138.0000
[2019-03-27 14:42:47,745] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.9886 3163992464.5181 1810.0000
[2019-03-27 14:42:47,812] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.3816 2779363399.5364 936.0000
[2019-03-27 14:42:48,832] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1725000, evaluation results [1725000.0, 7879.988564613993, 3163992464.5180597, 1810.0, 8253.337487830455, 2927428162.8444457, 1346.0, 8658.381625709813, 2779363399.5363774, 936.0, 7998.745203006758, 3007652442.0291696, 1773.0, 8496.130969709453, 2842387210.375433, 1138.0]
[2019-03-27 14:42:50,673] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:42:50,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5844
[2019-03-27 14:42:50,695] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 95.33333333333333, 1.0, 2.0, 0.2901553888209243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465414.2904711285, 465414.2904711285, 164636.3037502427], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1057200.0000, 
sim time next is 1057800.0000, 
raw observation next is [20.25, 95.16666666666667, 1.0, 2.0, 0.2912758945374515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466920.9517367519, 466920.9517367513, 164738.2797270654], 
processed observation next is [1.0, 0.21739130434782608, 0.1587677725118484, 0.9516666666666667, 1.0, 1.0, 0.14611553558729096, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12970026437131996, 0.12970026437131982, 0.24587802944338122], 
reward next is 0.7541, 
noisyNet noise sample is [array([0.09822031], dtype=float32), -0.15539701]. 
=============================================
[2019-03-27 14:42:51,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:42:51,295] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4546
[2019-03-27 14:42:51,301] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 95.0, 1.0, 2.0, 0.3114368452079352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492449.6841941559, 492449.6841941559, 166463.6412705227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1065600.0000, 
sim time next is 1066200.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3304369664384131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522451.8271187349, 522451.8271187343, 168738.7956749833], 
processed observation next is [1.0, 0.34782608695652173, 0.19431279620853087, 0.94, 1.0, 1.0, 0.1932975499257989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14512550753298192, 0.14512550753298176, 0.2518489487686318], 
reward next is 0.7482, 
noisyNet noise sample is [array([0.91876036], dtype=float32), -0.7373429]. 
=============================================
[2019-03-27 14:42:55,490] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2308164e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6461150e-28], sum to 1.0000
[2019-03-27 14:42:55,505] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3068
[2019-03-27 14:42:55,510] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.459051136245247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651035.2717632691, 651035.2717632685, 178666.2408256308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1297200.0000, 
sim time next is 1297800.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.4594574191080348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651614.1455646659, 651614.1455646659, 178726.2787576435], 
processed observation next is [1.0, 0.0, 0.3507109004739337, 0.94, 1.0, 1.0, 0.3487438784434154, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1810039293235183, 0.1810039293235183, 0.26675563993678136], 
reward next is 0.7332, 
noisyNet noise sample is [array([-1.7711353], dtype=float32), 0.6879837]. 
=============================================
[2019-03-27 14:43:01,226] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:43:01,235] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3700
[2019-03-27 14:43:01,243] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 93.5, 1.0, 2.0, 0.2889763351293487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466575.7557576234, 466575.7557576234, 164720.1972861002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1135800.0000, 
sim time next is 1136400.0000, 
raw observation next is [20.0, 93.66666666666667, 1.0, 2.0, 0.2836976881972058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 458312.5188377349, 458312.5188377356, 164155.6497084287], 
processed observation next is [1.0, 0.13043478260869565, 0.1469194312796209, 0.9366666666666668, 1.0, 1.0, 0.1369851665026576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1273090330104819, 0.1273090330104821, 0.24500843240063988], 
reward next is 0.7550, 
noisyNet noise sample is [array([-2.491871], dtype=float32), 0.91549605]. 
=============================================
[2019-03-27 14:43:13,258] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.12324275e-35 1.00000000e+00 0.00000000e+00 0.00000000e+00
 3.13873046e-29], sum to 1.0000
[2019-03-27 14:43:13,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9835
[2019-03-27 14:43:13,277] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.45, 91.5, 1.0, 2.0, 0.767624055597594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1139953.630798365, 1139953.630798365, 245145.2787010064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1612200.0000, 
sim time next is 1612800.0000, 
raw observation next is [23.4, 92.0, 1.0, 2.0, 0.7373990066418296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1094497.444836996, 1094497.444836996, 237604.2578892988], 
processed observation next is [1.0, 0.6956521739130435, 0.30805687203791465, 0.92, 1.0, 1.0, 0.6836132610142525, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3040270680102767, 0.3040270680102767, 0.35463322073029674], 
reward next is 0.6454, 
noisyNet noise sample is [array([0.8271104], dtype=float32), 1.0092412]. 
=============================================
[2019-03-27 14:43:16,020] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:43:16,035] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8270
[2019-03-27 14:43:16,044] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 90.0, 1.0, 2.0, 0.3265599017474729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 514528.6383058255, 514528.6383058249, 168092.3358277449], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1568400.0000, 
sim time next is 1569000.0000, 
raw observation next is [21.61666666666667, 90.0, 1.0, 2.0, 0.3226006728800868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508495.0533680868, 508495.0533680868, 167634.0193741018], 
processed observation next is [1.0, 0.13043478260869565, 0.22353870458135885, 0.9, 1.0, 1.0, 0.18385623238564675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14124862593557966, 0.14124862593557966, 0.25020002891656984], 
reward next is 0.7498, 
noisyNet noise sample is [array([-0.9389565], dtype=float32), -0.2765667]. 
=============================================
[2019-03-27 14:43:16,062] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.41547 ]
 [73.432724]
 [73.5761  ]
 [73.4976  ]
 [73.588615]], R is [[73.38665009]
 [73.40190125]
 [73.41629791]
 [73.43120575]
 [73.43686676]].
[2019-03-27 14:43:20,703] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0103464e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.1801762e-29], sum to 1.0000
[2019-03-27 14:43:20,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4696
[2019-03-27 14:43:20,721] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333333, 85.33333333333334, 1.0, 2.0, 0.5787107684086624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 892389.9307546557, 892389.9307546552, 206922.1475052646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1773600.0000, 
sim time next is 1774200.0000, 
raw observation next is [22.96666666666667, 85.16666666666667, 1.0, 2.0, 0.5477983218924735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846861.7074088061, 846861.7074088061, 201136.9342255705], 
processed observation next is [1.0, 0.5217391304347826, 0.2875197472353872, 0.8516666666666667, 1.0, 1.0, 0.4551787010752692, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2352393631691128, 0.2352393631691128, 0.30020437944114997], 
reward next is 0.6998, 
noisyNet noise sample is [array([0.6087904], dtype=float32), 0.21912996]. 
=============================================
[2019-03-27 14:43:54,905] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 14:43:54,906] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:43:54,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:43:54,907] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:43:54,909] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:43:54,910] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:43:54,911] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:43:54,908] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:43:54,915] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:43:54,918] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:43:54,912] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:43:54,948] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run71
[2019-03-27 14:43:54,972] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run71
[2019-03-27 14:43:54,973] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run71
[2019-03-27 14:43:54,974] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run71
[2019-03-27 14:43:55,049] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run71
[2019-03-27 14:44:00,634] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05686799], dtype=float32), 0.081611805]
[2019-03-27 14:44:00,635] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.33333333333333, 77.16666666666667, 1.0, 2.0, 0.3136728975625669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 494428.568492173, 494428.5684921724, 166575.4995984131]
[2019-03-27 14:44:00,637] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:44:00,640] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0531413230793325
[2019-03-27 14:44:19,859] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05686799], dtype=float32), 0.081611805]
[2019-03-27 14:44:19,860] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.65, 51.5, 1.0, 2.0, 0.3685639424325112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 556260.8394629584, 556260.8394629578, 170827.1007411704]
[2019-03-27 14:44:19,861] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:44:19,863] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.871132008650178
[2019-03-27 14:44:29,309] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05686799], dtype=float32), 0.081611805]
[2019-03-27 14:44:29,310] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.9, 86.5, 1.0, 2.0, 0.5107083068985722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713640.5373868372, 713640.5373868379, 185291.4615950815]
[2019-03-27 14:44:29,312] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:44:29,313] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1210978e-38], sampled 0.5865783053254304
[2019-03-27 14:44:49,444] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05686799], dtype=float32), 0.081611805]
[2019-03-27 14:44:49,446] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.66739591666667, 84.32783873666668, 1.0, 2.0, 0.4725174717762538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 671703.7899311447, 671703.7899311453, 180871.6928732885]
[2019-03-27 14:44:49,446] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:44:49,447] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 4.86133e-34], sampled 0.46829268397385515
[2019-03-27 14:45:31,590] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05686799], dtype=float32), 0.081611805]
[2019-03-27 14:45:31,590] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 77.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.315529881561647, 6.9112, 168.910505668163, 1740792.759322322, 1453951.387366752, 311351.6391609623]
[2019-03-27 14:45:31,591] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:45:31,595] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0511160e-26 1.0000000e+00 5.0994130e-34 2.4318241e-37 3.1488065e-20], sampled 0.21216368862342594
[2019-03-27 14:45:31,597] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1740792.759322322 W.
[2019-03-27 14:46:02,132] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2068 2779339346.4641 935.0000
[2019-03-27 14:46:02,218] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.2377 3007604634.4066 1766.0000
[2019-03-27 14:46:02,471] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9159 2927434154.2291 1345.0000
[2019-03-27 14:46:02,564] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.0193 2842488153.1303 1136.0000
[2019-03-27 14:46:02,720] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.9003 3164038337.1036 1779.0000
[2019-03-27 14:46:03,736] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1750000, evaluation results [1750000.0, 7883.900312283644, 3164038337.1035805, 1779.0, 8252.915870093115, 2927434154.229101, 1345.0, 8659.206848420383, 2779339346.4641194, 935.0, 7999.237681330799, 3007604634.4066215, 1766.0, 8495.019330847295, 2842488153.1303353, 1136.0]
[2019-03-27 14:46:07,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4135657e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4350995e-32], sum to 1.0000
[2019-03-27 14:46:07,282] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4624
[2019-03-27 14:46:07,287] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.85, 90.0, 1.0, 2.0, 0.5382187451808397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752095.9804222521, 752095.9804222527, 189800.6613054311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2151000.0000, 
sim time next is 2151600.0000, 
raw observation next is [26.76666666666667, 90.33333333333334, 1.0, 2.0, 0.5363365448326775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749464.9038800414, 749464.9038800414, 189484.9939448163], 
processed observation next is [0.0, 0.9130434782608695, 0.46761453396524505, 0.9033333333333334, 1.0, 1.0, 0.44136933112370774, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20818469552223373, 0.20818469552223373, 0.28281342379823327], 
reward next is 0.7172, 
noisyNet noise sample is [array([-1.2902037], dtype=float32), -0.23668833]. 
=============================================
[2019-03-27 14:46:11,454] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:46:11,464] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4487
[2019-03-27 14:46:11,470] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 97.33333333333334, 1.0, 2.0, 0.4669774741565006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 656237.0641911187, 656237.064191118, 179064.9579631308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2006400.0000, 
sim time next is 2007000.0000, 
raw observation next is [24.2, 97.0, 1.0, 2.0, 0.4695583185565638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658112.1932833813, 658112.1932833806, 179221.2954939674], 
processed observation next is [0.0, 0.21739130434782608, 0.3459715639810427, 0.97, 1.0, 1.0, 0.3609136368151371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18280894257871702, 0.18280894257871683, 0.2674944708865185], 
reward next is 0.7325, 
noisyNet noise sample is [array([0.20245332], dtype=float32), -0.6000792]. 
=============================================
[2019-03-27 14:46:11,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.825035]
 [75.83229 ]
 [75.81719 ]
 [75.6759  ]
 [75.657265]], R is [[75.77370453]
 [75.74871063]
 [75.72414398]
 [75.69989777]
 [75.67572784]].
[2019-03-27 14:46:27,248] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.400518e-33 1.000000e+00 0.000000e+00 0.000000e+00 6.963100e-29], sum to 1.0000
[2019-03-27 14:46:27,258] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4622
[2019-03-27 14:46:27,264] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 85.5, 1.0, 2.0, 0.5158807715841106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720870.7565319655, 720870.7565319655, 186121.5643318131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2248200.0000, 
sim time next is 2248800.0000, 
raw observation next is [26.86666666666667, 85.66666666666666, 1.0, 2.0, 0.5158277893766025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720796.6962367847, 720796.6962367853, 186112.9971079416], 
processed observation next is [1.0, 0.0, 0.4723538704581361, 0.8566666666666666, 1.0, 1.0, 0.41665998720072583, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20022130451021797, 0.20022130451021813, 0.2777805926984203], 
reward next is 0.7222, 
noisyNet noise sample is [array([-0.98398286], dtype=float32), -0.3984709]. 
=============================================
[2019-03-27 14:46:38,969] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.1685399e-19 1.0000000e+00 3.1829015e-26 2.2581368e-24 4.8239901e-10], sum to 1.0000
[2019-03-27 14:46:38,986] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5202
[2019-03-27 14:46:38,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2184794.999780844 W.
[2019-03-27 14:46:39,002] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.43333333333334, 74.0, 1.0, 2.0, 0.7812277774866719, 1.0, 2.0, 0.7812277774866719, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2184794.999780844, 2184794.999780844, 410833.1620994578], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2564400.0000, 
sim time next is 2565000.0000, 
raw observation next is [29.35, 74.5, 1.0, 2.0, 0.4992343079232169, 1.0, 2.0, 0.4992343079232169, 1.0, 1.0, 0.8626790326343383, 6.9112, 6.9112, 170.5573041426782, 2094162.332816554, 2094162.332816554, 413837.2471019274], 
processed observation next is [1.0, 0.6956521739130435, 0.590047393364929, 0.745, 1.0, 1.0, 0.3966678408713456, 1.0, 1.0, 0.3966678408713456, 1.0, 0.5, 0.8325354056516321, 0.0, 0.0, 0.8375144448122397, 0.5817117591157095, 0.5817117591157095, 0.6176675329879514], 
reward next is 0.3823, 
noisyNet noise sample is [array([1.334461], dtype=float32), 0.10361279]. 
=============================================
[2019-03-27 14:46:39,016] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[67.35046 ]
 [66.83016 ]
 [66.338844]
 [66.1101  ]
 [65.530075]], R is [[67.00906372]
 [66.72579193]
 [66.05853271]
 [65.39794922]
 [65.09234619]].
[2019-03-27 14:46:44,482] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.3898896e-38], sum to 1.0000
[2019-03-27 14:46:44,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8687
[2019-03-27 14:46:44,497] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5036098937985236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703718.2525470251, 703718.2525470251, 184164.0102108445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2638800.0000, 
sim time next is 2639400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5050180542036596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705686.5961941202, 705686.5961941202, 184386.3563279679], 
processed observation next is [0.0, 0.5652173913043478, 0.4786729857819906, 0.84, 1.0, 1.0, 0.40363620988392723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19602405449836674, 0.19602405449836674, 0.2752035169074148], 
reward next is 0.7248, 
noisyNet noise sample is [array([-1.4634176], dtype=float32), 0.13367489]. 
=============================================
[2019-03-27 14:46:57,323] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.050577e-37], sum to 1.0000
[2019-03-27 14:46:57,332] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9630
[2019-03-27 14:46:57,339] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3971945146211823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592670.2684337221, 592670.2684337215, 173863.0354437851], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2665200.0000, 
sim time next is 2665800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3969508427242628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592308.4251204082, 592308.4251204075, 173829.7584087286], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27343475027019615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16453011808900228, 0.1645301180890021, 0.2594474006100427], 
reward next is 0.7406, 
noisyNet noise sample is [array([-0.8269199], dtype=float32), 0.250094]. 
=============================================
[2019-03-27 14:47:04,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 2.428423e-36], sum to 1.0000
[2019-03-27 14:47:04,213] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4048
[2019-03-27 14:47:04,223] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.34953729368586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538453.7168682074, 538453.7168682081, 169689.5798246755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2763600.0000, 
sim time next is 2764200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3498789882288919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538979.1598952005, 538979.1598952012, 169732.6897413997], 
processed observation next is [0.0, 1.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21672167256493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14971643330422235, 0.14971643330422255, 0.25333237274835774], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.8341059], dtype=float32), 0.14005613]. 
=============================================
[2019-03-27 14:47:06,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0911646e-37], sum to 1.0000
[2019-03-27 14:47:06,778] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1850
[2019-03-27 14:47:06,785] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 99.0, 1.0, 2.0, 0.3911575441158831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587864.5402136683, 587864.5402136683, 173547.3584021127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3099000.0000, 
sim time next is 3099600.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3898993276311051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586936.0112060449, 586936.0112060449, 173490.883454552], 
processed observation next is [1.0, 0.9130434782608695, 0.2417061611374408, 1.0, 1.0, 1.0, 0.26493894895313863, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16303778089056803, 0.16303778089056803, 0.2589416170963463], 
reward next is 0.7411, 
noisyNet noise sample is [array([-0.45152128], dtype=float32), -0.49464074]. 
=============================================
[2019-03-27 14:47:06,908] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6594257e-37], sum to 1.0000
[2019-03-27 14:47:06,919] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3811
[2019-03-27 14:47:06,923] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3853117113113794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 580352.2812850083, 580352.2812850077, 172906.8068618597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3101400.0000, 
sim time next is 3102000.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3840270094326337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578417.5145680577, 578417.5145680577, 172733.6035332546], 
processed observation next is [1.0, 0.9130434782608695, 0.2417061611374408, 1.0, 1.0, 1.0, 0.25786386678630563, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16067153182446048, 0.16067153182446048, 0.2578113485570964], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.45152128], dtype=float32), -0.49464074]. 
=============================================
[2019-03-27 14:47:06,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.12779]
 [78.0909 ]
 [78.05354]
 [77.99486]
 [77.83781]], R is [[78.08478546]
 [78.04586792]
 [78.00727844]
 [77.96920013]
 [77.93056488]].
[2019-03-27 14:47:06,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:47:06,993] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9567
[2019-03-27 14:47:07,005] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.3875935231170022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597063.342476695, 597063.342476695, 174739.5927327472], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2794200.0000, 
sim time next is 2794800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.5208599807107414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802332.5437939562, 802332.5437939562, 195847.4851315653], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4227228683261944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22287015105387672, 0.22287015105387672, 0.2923096793008437], 
reward next is 0.7077, 
noisyNet noise sample is [array([1.2208301], dtype=float32), 0.6018694]. 
=============================================
[2019-03-27 14:47:07,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.064666e-36], sum to 1.0000
[2019-03-27 14:47:07,995] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5905
[2019-03-27 14:47:08,002] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.6031809713445242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 947686.2622197444, 947686.2622197444, 213650.3070876373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2974800.0000, 
sim time next is 2975400.0000, 
raw observation next is [22.0, 88.0, 1.0, 2.0, 0.6098847685019685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958222.5644618089, 958222.5644618089, 215071.2350403916], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.88, 1.0, 1.0, 0.5299816487975524, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2661729345727247, 0.2661729345727247, 0.32100184334386805], 
reward next is 0.6790, 
noisyNet noise sample is [array([-0.2872006], dtype=float32), 0.6046406]. 
=============================================
[2019-03-27 14:47:09,931] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 14:47:09,934] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:47:09,936] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:47:09,936] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:47:09,936] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:47:09,937] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:47:09,937] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:47:09,939] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:47:09,941] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:47:09,941] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:47:09,943] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:47:09,963] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run72
[2019-03-27 14:47:09,987] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run72
[2019-03-27 14:47:10,013] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run72
[2019-03-27 14:47:10,036] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run72
[2019-03-27 14:47:10,037] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run72
[2019-03-27 14:47:50,423] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05624814], dtype=float32), 0.08194972]
[2019-03-27 14:47:50,424] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.13333333333334, 72.0, 1.0, 2.0, 0.5562586931709723, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801144.7430608455, 801144.7430608461, 195915.4579400001]
[2019-03-27 14:47:50,425] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:47:50,430] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3276056e-36], sampled 0.3196819323071436
[2019-03-27 14:47:51,885] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05624814], dtype=float32), 0.08194972]
[2019-03-27 14:47:51,886] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.70874554666666, 93.36288443333333, 1.0, 2.0, 0.606928229249979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 848147.5563296513, 848147.5563296513, 202042.2567118985]
[2019-03-27 14:47:51,887] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:47:51,891] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2133514e-34], sampled 0.9372461243512206
[2019-03-27 14:47:57,057] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05624814], dtype=float32), 0.08194972]
[2019-03-27 14:47:57,058] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.08308541, 99.43946980666666, 1.0, 2.0, 0.4638177229050643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648097.7824413648, 648097.7824413654, 178121.7479635436]
[2019-03-27 14:47:57,060] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:47:57,062] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3602632035491894
[2019-03-27 14:47:58,008] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05624814], dtype=float32), 0.08194972]
[2019-03-27 14:47:58,010] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.56666666666667, 71.83333333333333, 1.0, 2.0, 0.5015924683604013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 700898.277223034, 700898.2772230345, 183845.6661811807]
[2019-03-27 14:47:58,011] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:47:58,013] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.77689504e-38 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.17593384e-32], sampled 0.25154818209676677
[2019-03-27 14:48:07,076] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05624814], dtype=float32), 0.08194972]
[2019-03-27 14:48:07,124] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.7, 49.66666666666667, 1.0, 2.0, 0.5367311666674893, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9202699765243669, 6.911199999999999, 6.9112, 168.9127108075784, 1500563.020944513, 1500563.020944514, 326438.5467165364]
[2019-03-27 14:48:07,125] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:48:07,127] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3390180e-27 1.0000000e+00 1.2919824e-34 2.4480645e-38 8.3223999e-21], sampled 0.003772606676737511
[2019-03-27 14:48:25,057] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05624814], dtype=float32), 0.08194972]
[2019-03-27 14:48:25,061] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.66666666666667, 81.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 8.580008593772568, 6.9112, 168.9026305707337, 3468608.356101095, 2284770.478190713, 471841.4844066477]
[2019-03-27 14:48:25,062] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:48:25,063] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.5557984e-17 1.0000000e+00 4.0660482e-23 9.4467479e-22 3.0173728e-09], sampled 0.07119773458041412
[2019-03-27 14:48:25,064] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3468608.356101095 W.
[2019-03-27 14:48:36,799] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05624814], dtype=float32), 0.08194972]
[2019-03-27 14:48:36,800] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.86721855, 82.2047961, 1.0, 2.0, 0.7225995792470148, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005977074341009, 6.9112, 168.9116398884979, 1906754.085035426, 1839516.490622176, 389704.9617061394]
[2019-03-27 14:48:36,801] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:48:36,804] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.6403088e-26 1.0000000e+00 2.9151628e-33 1.0538891e-36 1.1724881e-19], sampled 0.554675444127198
[2019-03-27 14:48:36,805] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1906754.085035426 W.
[2019-03-27 14:49:06,106] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05624814], dtype=float32), 0.08194972]
[2019-03-27 14:49:06,108] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.6, 85.0, 1.0, 2.0, 0.8124062583576549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1135444.877645451, 1135444.877645451, 246981.6364521313]
[2019-03-27 14:49:06,110] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:49:06,115] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1571439e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8538830e-31], sampled 0.38943143872153163
[2019-03-27 14:49:17,854] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.0609 2842602060.9998 1150.0000
[2019-03-27 14:49:18,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8250.5403 2927569972.1394 1352.0000
[2019-03-27 14:49:18,204] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.9936 3007692497.1904 1769.0000
[2019-03-27 14:49:18,378] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.8734 3163814291.3359 1825.0000
[2019-03-27 14:49:18,524] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1310 2779424528.4542 936.0000
[2019-03-27 14:49:19,544] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1775000, evaluation results [1775000.0, 7878.8733900283205, 3163814291.3358965, 1825.0, 8250.540274524466, 2927569972.1394224, 1352.0, 8659.130957902744, 2779424528.4542313, 936.0, 8000.993595798196, 3007692497.1903896, 1769.0, 8495.06094413163, 2842602060.9998035, 1150.0]
[2019-03-27 14:49:26,267] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.69147248e-36 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.17848575e-30], sum to 1.0000
[2019-03-27 14:49:26,278] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6976
[2019-03-27 14:49:26,285] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 80.33333333333334, 1.0, 2.0, 0.8477329814321128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1229115.629818144, 1229115.629818144, 262270.1081799117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3152400.0000, 
sim time next is 3153000.0000, 
raw observation next is [25.83333333333334, 79.66666666666667, 1.0, 2.0, 0.8513808529999223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1231236.254246626, 1231236.254246626, 262814.8963577074], 
processed observation next is [1.0, 0.4782608695652174, 0.42338072669826254, 0.7966666666666667, 1.0, 1.0, 0.8209407867468943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3420100706240628, 0.3420100706240628, 0.3922610393398618], 
reward next is 0.6077, 
noisyNet noise sample is [array([-1.6029403], dtype=float32), -0.70595694]. 
=============================================
[2019-03-27 14:49:26,307] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.29681]
 [73.2916 ]
 [73.57424]
 [73.9222 ]
 [74.10084]], R is [[73.2209549 ]
 [73.09729767]
 [72.9673233 ]
 [72.85290527]
 [72.76315308]].
[2019-03-27 14:49:33,982] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.4547028e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.4529196e-28], sum to 1.0000
[2019-03-27 14:49:33,990] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6836
[2019-03-27 14:49:33,996] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.5986154564799745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 836526.3562738885, 836526.3562738892, 200488.5665501464], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3261000.0000, 
sim time next is 3261600.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.597699948860203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835246.4905922757, 835246.4905922757, 200318.4407519266], 
processed observation next is [0.0, 0.782608695652174, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5153011432050638, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23201291405340993, 0.23201291405340993, 0.2989827473909352], 
reward next is 0.7010, 
noisyNet noise sample is [array([-0.04466867], dtype=float32), 0.10516265]. 
=============================================
[2019-03-27 14:49:34,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:49:34,530] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2558
[2019-03-27 14:49:34,539] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 98.0, 1.0, 2.0, 0.3852086289591706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583957.1103113506, 583957.1103113499, 173337.6641927693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3122400.0000, 
sim time next is 3123000.0000, 
raw observation next is [22.0, 97.0, 1.0, 2.0, 0.3820660893101436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581410.119889785, 581410.119889785, 173170.2989534034], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.97, 1.0, 1.0, 0.25550131242185975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16150281108049583, 0.16150281108049583, 0.25846313276627375], 
reward next is 0.7415, 
noisyNet noise sample is [array([0.23687704], dtype=float32), 1.0091344]. 
=============================================
[2019-03-27 14:49:34,558] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.15473 ]
 [73.10543 ]
 [73.37681 ]
 [73.210304]
 [73.16262 ]], R is [[73.19857788]
 [73.20787811]
 [73.20838165]
 [73.21936798]
 [73.22973633]].
[2019-03-27 14:49:35,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 14:49:35,227] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2623
[2019-03-27 14:49:35,235] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 98.0, 1.0, 2.0, 0.3852086289591706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583957.1103113506, 583957.1103113499, 173337.6641927693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3122400.0000, 
sim time next is 3123000.0000, 
raw observation next is [22.0, 97.0, 1.0, 2.0, 0.3820660893101436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581410.119889785, 581410.119889785, 173170.2989534034], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.97, 1.0, 1.0, 0.25550131242185975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16150281108049583, 0.16150281108049583, 0.25846313276627375], 
reward next is 0.7415, 
noisyNet noise sample is [array([-0.9089835], dtype=float32), 0.34378585]. 
=============================================
[2019-03-27 14:49:35,251] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[74.12234 ]
 [74.072586]
 [74.348694]
 [74.17653 ]
 [74.13045 ]], R is [[74.14168549]
 [74.14155579]
 [74.13272095]
 [74.13446808]
 [74.13568878]].
[2019-03-27 14:49:42,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8914936e-34], sum to 1.0000
[2019-03-27 14:49:42,519] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1721
[2019-03-27 14:49:42,527] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 73.0, 1.0, 2.0, 0.5341717959314043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746438.8684780421, 746438.8684780427, 189123.865751679], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3235200.0000, 
sim time next is 3235800.0000, 
raw observation next is [29.83333333333333, 71.5, 1.0, 2.0, 0.5325943095267229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744233.7545501533, 744233.7545501539, 188860.9576867862], 
processed observation next is [0.0, 0.43478260869565216, 0.6129541864139019, 0.715, 1.0, 1.0, 0.436860613887618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2067315984861537, 0.20673159848615386, 0.2818820263981883], 
reward next is 0.7181, 
noisyNet noise sample is [array([0.7583529], dtype=float32), 0.13801067]. 
=============================================
[2019-03-27 14:49:47,912] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2980483e-10 4.3296728e-01 2.7046215e-18 2.0513391e-12 5.6703275e-01], sum to 1.0000
[2019-03-27 14:49:47,924] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6339
[2019-03-27 14:49:47,929] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.16666666666667, 66.33333333333334, 1.0, 2.0, 0.4521459685756126, 1.0, 2.0, 0.4521459685756126, 1.0, 2.0, 0.7852280204591253, 6.911199999999999, 6.9112, 170.5573041426782, 1896463.765390722, 1896463.765390723, 383632.4420068584], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3589800.0000, 
sim time next is 3590400.0000, 
raw observation next is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.5308799095148509, 1.0, 2.0, 0.5308799095148509, 1.0, 2.0, 0.9219628381584339, 6.911200000000001, 6.9112, 170.5573041426782, 2227043.526406548, 2227043.526406548, 437157.5846259375], 
processed observation next is [1.0, 0.5652173913043478, 0.7314375987361774, 0.6566666666666667, 1.0, 1.0, 0.43479507170463955, 1.0, 1.0, 0.43479507170463955, 1.0, 1.0, 0.9048327294615048, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6186232017795966, 0.6186232017795966, 0.6524740069043844], 
reward next is 0.3475, 
noisyNet noise sample is [array([0.28692916], dtype=float32), -1.4017265]. 
=============================================
[2019-03-27 14:49:56,178] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.375086e-36], sum to 1.0000
[2019-03-27 14:49:56,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4857
[2019-03-27 14:49:56,198] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 79.0, 1.0, 2.0, 0.7951577808035653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1111325.240513955, 1111325.240513955, 242723.3068325832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3479400.0000, 
sim time next is 3480000.0000, 
raw observation next is [27.66666666666666, 79.0, 1.0, 2.0, 0.8064513657963535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1127117.712805864, 1127117.712805865, 245498.5105834061], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012636, 0.79, 1.0, 1.0, 0.7668088744534379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31308825355718445, 0.3130882535571847, 0.3664156874379196], 
reward next is 0.6336, 
noisyNet noise sample is [array([-0.48646432], dtype=float32), 0.9455223]. 
=============================================
[2019-03-27 14:49:56,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[63.40688 ]
 [63.972324]
 [64.85734 ]
 [64.76877 ]
 [64.73242 ]], R is [[63.05022812]
 [63.05745316]
 [63.08094025]
 [63.13435364]
 [63.19488144]].
[2019-03-27 14:50:00,383] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1471992e-35], sum to 1.0000
[2019-03-27 14:50:00,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5280
[2019-03-27 14:50:00,400] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5157695029439897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720715.2215249649, 720715.2215249649, 186104.0597786985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3440400.0000, 
sim time next is 3441000.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.516028016286732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721076.5801110404, 721076.5801110397, 186145.6822240083], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.41690122444184574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20029905003084456, 0.20029905003084436, 0.2778293764537437], 
reward next is 0.7222, 
noisyNet noise sample is [array([-0.35009778], dtype=float32), 0.33735144]. 
=============================================
[2019-03-27 14:50:00,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.99443]
 [69.84323]
 [70.12668]
 [70.43308]
 [70.84215]], R is [[69.41726685]
 [69.44532013]
 [69.47288513]
 [69.49990082]
 [69.52628326]].
[2019-03-27 14:50:01,778] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.12005086e-20 1.00000000e+00 5.27283508e-28 1.13606736e-27
 9.19389651e-12], sum to 1.0000
[2019-03-27 14:50:01,787] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9250
[2019-03-27 14:50:01,798] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2550402.490237236 W.
[2019-03-27 14:50:01,805] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 68.83333333333333, 1.0, 2.0, 0.9118248663703852, 1.0, 1.0, 0.9118248663703852, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2550402.490237236, 2550402.490237236, 477984.5379657315], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3430200.0000, 
sim time next is 3430800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5487944064407315, 1.0, 2.0, 0.5487944064407315, 1.0, 1.0, 0.953074394904025, 6.911199999999999, 6.9112, 170.5573041426782, 2302264.139925688, 2302264.139925688, 450528.3915992212], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.45637880294064037, 1.0, 1.0, 0.45637880294064037, 1.0, 0.5, 0.9427736523219817, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6395178166460245, 0.6395178166460245, 0.6724304352227183], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0776384], dtype=float32), -1.1187416]. 
=============================================
[2019-03-27 14:50:05,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0814252e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1273672e-29], sum to 1.0000
[2019-03-27 14:50:05,093] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2895
[2019-03-27 14:50:05,102] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6887043417611763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 962476.78678245, 962476.78678245, 218422.5750313775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3643200.0000, 
sim time next is 3643800.0000, 
raw observation next is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6748815702130709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 943150.6355299543, 943150.6355299538, 215513.4039092174], 
processed observation next is [1.0, 0.17391304347826086, 0.470774091627172, 0.7983333333333335, 1.0, 1.0, 0.608291048449483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26198628764720955, 0.2619862876472094, 0.3216617968794289], 
reward next is 0.6783, 
noisyNet noise sample is [array([-1.7375537], dtype=float32), 0.6254519]. 
=============================================
[2019-03-27 14:50:07,907] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.139972e-37], sum to 1.0000
[2019-03-27 14:50:07,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6798
[2019-03-27 14:50:07,924] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5357155380887247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748596.8169163256, 748596.8169163256, 189381.1172780632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3877200.0000, 
sim time next is 3877800.0000, 
raw observation next is [29.83333333333333, 71.5, 1.0, 2.0, 0.5427197607385906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758387.8537620524, 758387.8537620524, 190559.7242899053], 
processed observation next is [0.0, 0.9130434782608695, 0.6129541864139019, 0.715, 1.0, 1.0, 0.44905995269709714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21066329271168122, 0.21066329271168122, 0.28441749894015717], 
reward next is 0.7156, 
noisyNet noise sample is [array([-0.26374012], dtype=float32), 0.2759769]. 
=============================================
[2019-03-27 14:50:18,157] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.764629e-35], sum to 1.0000
[2019-03-27 14:50:18,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1562
[2019-03-27 14:50:18,175] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5414278334244371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756581.892399559, 756581.8923995583, 190341.0291017367], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3873600.0000, 
sim time next is 3874200.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5433314895713639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759242.9792850231, 759242.9792850225, 190663.0707257695], 
processed observation next is [0.0, 0.8695652173913043, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4497969753871854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21090082757917308, 0.21090082757917292, 0.2845717473518948], 
reward next is 0.7154, 
noisyNet noise sample is [array([0.43097082], dtype=float32), -0.8217644]. 
=============================================
[2019-03-27 14:50:20,499] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5856843e-18 1.0000000e+00 2.2630080e-25 1.5365826e-24 8.4624550e-11], sum to 1.0000
[2019-03-27 14:50:20,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6221
[2019-03-27 14:50:20,520] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2847743.995244639 W.
[2019-03-27 14:50:20,527] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 1.018009959151973, 1.0, 2.0, 1.018009959151973, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2847743.995244639, 2847743.99524464, 539964.6964577293], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3773400.0000, 
sim time next is 3774000.0000, 
raw observation next is [33.0, 63.00000000000001, 1.0, 2.0, 0.719055798809357, 1.0, 2.0, 0.680117938918941, 1.0, 1.0, 1.03, 7.005099234736087, 6.9112, 170.5573041426782, 2853812.664371373, 2786548.822953254, 527415.3388620315], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.6300000000000001, 1.0, 1.0, 0.6615130106136831, 1.0, 1.0, 0.6145999264083627, 1.0, 0.5, 1.0365853658536586, 0.009389923473608697, 0.0, 0.8375144448122397, 0.7927257401031592, 0.7740413397092372, 0.7871870729284052], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6333291], dtype=float32), -2.7702765]. 
=============================================
[2019-03-27 14:50:20,543] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[55.197884]
 [54.439445]
 [54.552902]
 [54.43856 ]
 [54.10388 ]], R is [[54.58195496]
 [54.03613663]
 [53.49577713]
 [53.16240311]
 [52.63077927]].
[2019-03-27 14:50:23,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8687795e-37], sum to 1.0000
[2019-03-27 14:50:23,371] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0541
[2019-03-27 14:50:23,376] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.543206161525321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759067.7852456463, 759067.785245647, 190642.5440530445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4054200.0000, 
sim time next is 4054800.0000, 
raw observation next is [27.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5417132703311206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756980.8991221943, 756980.8991221943, 190389.8431550976], 
processed observation next is [1.0, 0.9565217391304348, 0.5102685624012641, 0.8566666666666667, 1.0, 1.0, 0.4478473136519525, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2102724719783873, 0.2102724719783873, 0.28416394500760833], 
reward next is 0.7158, 
noisyNet noise sample is [array([-2.5846732], dtype=float32), -1.3393931]. 
=============================================
[2019-03-27 14:50:25,335] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 14:50:25,336] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:50:25,338] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:50:25,339] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:50:25,340] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:50:25,340] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:50:25,341] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:50:25,342] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:50:25,344] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:50:25,345] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:50:25,346] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:50:25,377] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run73
[2019-03-27 14:50:25,402] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run73
[2019-03-27 14:50:25,403] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run73
[2019-03-27 14:50:25,403] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run73
[2019-03-27 14:50:25,442] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run73
[2019-03-27 14:50:41,959] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.05631877], dtype=float32), 0.0810589]
[2019-03-27 14:50:41,959] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.65, 84.16666666666667, 1.0, 2.0, 0.2918258972063348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 467445.3099390939, 467445.3099390932, 164771.5348766488]
[2019-03-27 14:50:41,960] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:50:41,965] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24383022208644678
[2019-03-27 14:51:17,241] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05631877], dtype=float32), 0.0810589]
[2019-03-27 14:51:17,242] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.49658032333333, 77.76043475666667, 1.0, 2.0, 0.4925195700086353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708063.763648675, 708063.763648675, 184959.1013673086]
[2019-03-27 14:51:17,245] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:51:17,248] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6176280104923894
[2019-03-27 14:51:34,592] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05631877], dtype=float32), 0.0810589]
[2019-03-27 14:51:34,594] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.66666666666667, 79.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.102012236591952, 6.9112, 168.9119325230154, 1589215.580946072, 1453847.633692573, 311353.500307062]
[2019-03-27 14:51:34,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:51:34,597] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.8064718e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0837914e-28], sampled 0.12535596531553816
[2019-03-27 14:52:09,041] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.05631877], dtype=float32), 0.0810589]
[2019-03-27 14:52:09,042] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 73.0, 1.0, 2.0, 0.5341015860017703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746340.7243375909, 746340.7243375916, 189110.9739723515]
[2019-03-27 14:52:09,044] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 14:52:09,047] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.9676977e-36], sampled 0.7401742390164652
[2019-03-27 14:52:10,425] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05631877], dtype=float32), 0.0810589]
[2019-03-27 14:52:10,426] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.29206466, 68.938862725, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.501384873210311, 6.9112, 168.9096350116504, 1872730.937325042, 1454041.708926968, 311348.3308505103]
[2019-03-27 14:52:10,427] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:52:10,429] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.09989736e-26 1.00000000e+00 1.26925288e-33 1.57178156e-37
 6.19757723e-21], sampled 0.592657160260029
[2019-03-27 14:52:10,431] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1872730.937325042 W.
[2019-03-27 14:52:27,263] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.05631877], dtype=float32), 0.0810589]
[2019-03-27 14:52:27,263] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.63076259, 84.26106518, 1.0, 2.0, 0.368697710662288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551994.008022008, 551994.008022008, 170304.6225021328]
[2019-03-27 14:52:27,264] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:52:27,269] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5118729848256383
[2019-03-27 14:52:32,428] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8250.2629 2927560972.9740 1354.0000
[2019-03-27 14:52:33,203] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.4793 3007756294.7233 1769.0000
[2019-03-27 14:52:33,266] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.1839 3163977399.3232 1822.0000
[2019-03-27 14:52:33,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.3922 2842445509.2972 1144.0000
[2019-03-27 14:52:33,603] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.0924 2779469392.2286 937.0000
[2019-03-27 14:52:34,622] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1800000, evaluation results [1800000.0, 7874.183946756341, 3163977399.323203, 1822.0, 8250.262890825219, 2927560972.9740314, 1354.0, 8658.092424480858, 2779469392.2286196, 937.0, 7998.479290413541, 3007756294.723305, 1769.0, 8494.392177637086, 2842445509.297248, 1144.0]
[2019-03-27 14:52:36,043] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.23814985e-13 9.99999762e-01 1.04582755e-20 5.29028582e-19
 2.42930838e-07], sum to 1.0000
[2019-03-27 14:52:36,052] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1692
[2019-03-27 14:52:36,066] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3426386.541155975 W.
[2019-03-27 14:52:36,072] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.66666666666667, 65.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 8.511918301715237, 6.9112, 168.9035945033453, 3426386.541155975, 2290844.8263786, 472347.374266626], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4015200.0000, 
sim time next is 4015800.0000, 
raw observation next is [32.0, 64.5, 1.0, 2.0, 0.7183875392900863, 1.0, 1.0, 0.6797838091593058, 1.0, 2.0, 1.03, 7.005099182041132, 6.9112, 170.5573041426782, 2852409.036599541, 2785145.232928961, 527198.9310262403], 
processed observation next is [1.0, 0.4782608695652174, 0.7156398104265403, 0.645, 1.0, 1.0, 0.6607078786627545, 1.0, 0.5, 0.6141973604328985, 1.0, 1.0, 1.0365853658536586, 0.00938991820411319, 0.0, 0.8375144448122397, 0.7923358434998725, 0.7736514535913781, 0.7868640761585677], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6535679], dtype=float32), -0.3180932]. 
=============================================
[2019-03-27 14:52:41,936] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7342451e-21 1.0000000e+00 6.9747439e-29 1.9405629e-32 2.8651885e-17], sum to 1.0000
[2019-03-27 14:52:41,947] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9241
[2019-03-27 14:52:41,953] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.131769907161198, 6.9112, 168.9119720352745, 1610341.062969428, 1453862.091410439, 311354.7706473112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4002000.0000, 
sim time next is 4002600.0000, 
raw observation next is [29.16666666666667, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.234358553009545, 6.9112, 168.9107546241868, 1683168.755349099, 1453911.944130268, 311354.2363388008], 
processed observation next is [1.0, 0.30434782608695654, 0.581358609794629, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.03231585530095451, 0.0, 0.8294291328828158, 0.4675468764858608, 0.4038644289250744, 0.464707815431046], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.43950978], dtype=float32), 0.1526328]. 
=============================================
[2019-03-27 14:52:52,927] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9158760e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0775974e-31], sum to 1.0000
[2019-03-27 14:52:52,935] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3654
[2019-03-27 14:52:52,940] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5952278981837773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831790.6091750228, 831790.6091750228, 199860.3863433832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4238400.0000, 
sim time next is 4239000.0000, 
raw observation next is [30.5, 77.0, 1.0, 2.0, 0.593500489155033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 829375.7297710113, 829375.7297710113, 199541.4305315053], 
processed observation next is [1.0, 0.043478260869565216, 0.6445497630331753, 0.77, 1.0, 1.0, 0.5102415531988349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23038214715861424, 0.23038214715861424, 0.29782303064403776], 
reward next is 0.7022, 
noisyNet noise sample is [array([-0.14361268], dtype=float32), -0.18107265]. 
=============================================
[2019-03-27 14:52:52,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[58.05136 ]
 [58.170277]
 [58.37906 ]
 [58.66153 ]
 [59.313786]], R is [[57.98234177]
 [58.10422134]
 [58.22393036]
 [58.34260559]
 [58.46001816]].
[2019-03-27 14:52:54,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5936897e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8534889e-29], sum to 1.0000
[2019-03-27 14:52:54,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9109
[2019-03-27 14:52:54,238] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 79.00000000000001, 1.0, 2.0, 1.003181903310714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1402255.079634105, 1402255.079634106, 299898.6610264403], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4255800.0000, 
sim time next is 4256400.0000, 
raw observation next is [29.33333333333334, 79.0, 1.0, 2.0, 0.9187025758693804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1284097.835336189, 1284097.835336189, 275136.2849042831], 
processed observation next is [1.0, 0.2608695652173913, 0.5892575039494474, 0.79, 1.0, 1.0, 0.9020512962281692, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35669384314894137, 0.35669384314894137, 0.41065117149893005], 
reward next is 0.5893, 
noisyNet noise sample is [array([0.21399651], dtype=float32), -0.52706444]. 
=============================================
[2019-03-27 14:52:55,477] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5974181e-20 1.0000000e+00 5.1574389e-26 2.0033818e-28 3.8858500e-15], sum to 1.0000
[2019-03-27 14:52:55,482] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9805
[2019-03-27 14:52:55,488] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5367848488544159, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9299384181376464, 6.911199999999999, 6.9112, 168.912956413896, 1500713.207155931, 1500713.207155932, 328364.3741684543], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4245600.0000, 
sim time next is 4246200.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.112147662151455, 6.9112, 168.9117142504512, 1596410.698479155, 1453852.55930694, 311352.5950440553], 
processed observation next is [1.0, 0.13043478260869565, 0.6208530805687204, 0.75, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.020094766215145478, 0.0, 0.8294338450870928, 0.44344741624420975, 0.4038479331408167, 0.464705365737396], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3108853], dtype=float32), -1.6532619]. 
=============================================
[2019-03-27 14:53:04,805] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7337177e-34], sum to 1.0000
[2019-03-27 14:53:04,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8683
[2019-03-27 14:53:04,822] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.5, 53.0, 1.0, 2.0, 0.552596270508102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104266, 772194.1460556478, 772194.1460556483, 192250.3639880457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4210200.0000, 
sim time next is 4210800.0000, 
raw observation next is [35.33333333333334, 54.0, 1.0, 2.0, 0.5594066550731059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781714.4315185265, 781714.4315185265, 193430.1404196138], 
processed observation next is [1.0, 0.7391304347826086, 0.8736176935229073, 0.54, 1.0, 1.0, 0.4691646446663927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21714289764403513, 0.21714289764403513, 0.28870170211882656], 
reward next is 0.7113, 
noisyNet noise sample is [array([-1.402138], dtype=float32), -0.50960857]. 
=============================================
[2019-03-27 14:53:19,176] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.4812651e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2046983e-31], sum to 1.0000
[2019-03-27 14:53:19,188] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5714
[2019-03-27 14:53:19,199] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 79.83333333333334, 1.0, 2.0, 0.6225070610073073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 869927.0001083423, 869927.0001083417, 205014.3152212159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4399800.0000, 
sim time next is 4400400.0000, 
raw observation next is [30.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6216299483060438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868700.7713449328, 868700.7713449328, 204845.1600819521], 
processed observation next is [1.0, 0.9565217391304348, 0.6524486571879939, 0.8066666666666668, 1.0, 1.0, 0.5441324678386069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2413057698180369, 0.2413057698180369, 0.305739044898436], 
reward next is 0.6943, 
noisyNet noise sample is [array([0.4748741], dtype=float32), -0.54231656]. 
=============================================
[2019-03-27 14:53:26,355] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1211597e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0947189e-27], sum to 1.0000
[2019-03-27 14:53:26,371] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0235
[2019-03-27 14:53:26,378] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.7316039499709531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1022458.62703462, 1022458.62703462, 227813.60955987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4867200.0000, 
sim time next is 4867800.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.8029879221334711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1122274.554489039, 1122274.554489039, 244644.3791342582], 
processed observation next is [1.0, 0.34782608695652173, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.7626360507632183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3117429318025108, 0.3117429318025108, 0.36514086437948984], 
reward next is 0.6349, 
noisyNet noise sample is [array([-0.94175404], dtype=float32), 2.0709994]. 
=============================================
[2019-03-27 14:53:28,892] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.0530153e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3014381e-30], sum to 1.0000
[2019-03-27 14:53:28,903] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6473
[2019-03-27 14:53:28,908] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 52.0, 1.0, 2.0, 0.5316923754061694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 742972.973956, 742972.9739560005, 188711.4153926059], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4545600.0000, 
sim time next is 4546200.0000, 
raw observation next is [34.0, 52.5, 1.0, 2.0, 0.5339334248901505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746105.6575136452, 746105.657513646, 189084.4687387619], 
processed observation next is [0.0, 0.6086956521739131, 0.8104265402843602, 0.525, 1.0, 1.0, 0.43847400589174756, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20725157153156812, 0.2072515715315683, 0.2822156249832267], 
reward next is 0.7178, 
noisyNet noise sample is [array([0.66904974], dtype=float32), 1.8690981]. 
=============================================
[2019-03-27 14:53:33,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.69229936e-28 1.00000000e+00 1.13183655e-35 0.00000000e+00
 4.74213667e-23], sum to 1.0000
[2019-03-27 14:53:33,891] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6711
[2019-03-27 14:53:33,899] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.835497732681488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1167736.004468091, 1167736.004468092, 252810.7880729979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4680600.0000, 
sim time next is 4681200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7864823679441934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1099194.069068189, 1099194.069068189, 240619.8717489485], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.7427498408966184, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3053316858522747, 0.3053316858522747, 0.3591341369387291], 
reward next is 0.6409, 
noisyNet noise sample is [array([-1.1244985], dtype=float32), 0.43086255]. 
=============================================
[2019-03-27 14:53:40,040] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 14:53:40,041] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:53:40,042] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:53:40,043] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:53:40,043] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:53:40,044] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:53:40,047] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:53:40,047] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:53:40,052] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:53:40,044] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:53:40,053] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:53:40,078] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run74
[2019-03-27 14:53:40,100] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run74
[2019-03-27 14:53:40,125] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run74
[2019-03-27 14:53:40,126] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run74
[2019-03-27 14:53:40,173] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run74
[2019-03-27 14:54:12,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.05995422], dtype=float32), 0.08320025]
[2019-03-27 14:54:12,345] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.6, 74.0, 1.0, 2.0, 0.660258199125461, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.958542740470024, 6.9112, 168.912636492982, 1819517.404863219, 1785930.898994522, 377731.5119265246]
[2019-03-27 14:54:12,346] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:54:12,351] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.8415597e-14 9.9998796e-01 1.4753592e-20 2.4443473e-17 1.2098717e-05], sampled 0.6554720339380207
[2019-03-27 14:54:12,352] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1819517.404863219 W.
[2019-03-27 14:55:21,622] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.05995422], dtype=float32), 0.08320025]
[2019-03-27 14:55:21,623] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.22140423, 83.44268088, 1.0, 2.0, 0.4413536785300091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 640759.9715195325, 640759.9715195319, 177992.4781561731]
[2019-03-27 14:55:21,623] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:55:21,627] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7428668e-36], sampled 0.5319280615320742
[2019-03-27 14:55:47,567] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8678.0816 2778967158.9375 894.0000
[2019-03-27 14:55:47,988] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8540.6692 2840114707.5038 1038.0000
[2019-03-27 14:55:48,139] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8107.8099 3001885612.4227 1476.0000
[2019-03-27 14:55:48,315] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7952.6787 3160664143.8575 1717.0000
[2019-03-27 14:55:48,435] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8279.6937 2926804758.8839 1281.0000
[2019-03-27 14:55:49,453] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1825000, evaluation results [1825000.0, 7952.67866828323, 3160664143.857465, 1717.0, 8279.693702276869, 2926804758.883914, 1281.0, 8678.081550068919, 2778967158.9375086, 894.0, 8107.809881283486, 3001885612.422675, 1476.0, 8540.669249967075, 2840114707.5037894, 1038.0]
[2019-03-27 14:55:54,164] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7465185e-37], sum to 1.0000
[2019-03-27 14:55:54,175] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4147
[2019-03-27 14:55:54,188] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 71.33333333333334, 1.0, 2.0, 0.5221001399677294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729564.4327851983, 729564.4327851989, 187131.0297538437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5127600.0000, 
sim time next is 5128200.0000, 
raw observation next is [29.5, 70.0, 1.0, 2.0, 0.5205520509824388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727400.4468327963, 727400.4468327957, 186878.614573153], 
processed observation next is [0.0, 0.34782608695652173, 0.5971563981042655, 0.7, 1.0, 1.0, 0.42235186865354074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20205567967577676, 0.2020556796757766, 0.2789233053330642], 
reward next is 0.7211, 
noisyNet noise sample is [array([0.31764403], dtype=float32), 0.34806377]. 
=============================================
[2019-03-27 14:56:08,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2068424e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.7671613e-34], sum to 1.0000
[2019-03-27 14:56:08,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2963
[2019-03-27 14:56:08,582] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5516185973084917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770827.45667694, 770827.4566769394, 192079.1430813605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5155200.0000, 
sim time next is 5155800.0000, 
raw observation next is [31.83333333333334, 63.5, 1.0, 2.0, 0.5556764701865975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 776499.9687244205, 776499.9687244212, 192778.9960582308], 
processed observation next is [0.0, 0.6956521739130435, 0.7077409162717223, 0.635, 1.0, 1.0, 0.46467044600794877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2156944357567835, 0.21569443575678368, 0.28772984486303105], 
reward next is 0.7123, 
noisyNet noise sample is [array([-0.27769646], dtype=float32), 0.8045669]. 
=============================================
[2019-03-27 14:56:10,641] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1868848e-23 1.0000000e+00 6.0940445e-30 3.7387762e-33 8.5321968e-18], sum to 1.0000
[2019-03-27 14:56:10,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1628
[2019-03-27 14:56:10,663] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1895544.802368958 W.
[2019-03-27 14:56:10,669] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.53352129452583, 6.9112, 168.9096396740877, 1895544.802368958, 1454057.326765505, 311348.3186205905], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5193600.0000, 
sim time next is 5194200.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.588782952637381, 1.0, 1.0, 0.588782952637381, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1646187.03897598, 1646187.03897598, 330138.769288555], 
processed observation next is [1.0, 0.08695652173913043, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.5045577742619048, 1.0, 0.5, 0.5045577742619048, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.45727417749332777, 0.45727417749332777, 0.4927444317739627], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2673838], dtype=float32), -0.16055705]. 
=============================================
[2019-03-27 14:56:25,437] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2217904e-06 1.4126178e-06 2.7726080e-14 7.0187447e-08 9.9999726e-01], sum to 1.0000
[2019-03-27 14:56:25,446] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2194
[2019-03-27 14:56:25,450] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.76666666666667, 70.66666666666667, 1.0, 2.0, 0.6635936870087659, 1.0, 2.0, 0.6523868830186454, 1.0, 2.0, 1.03, 7.005094861727689, 6.9112, 170.5573041426782, 2737324.126209341, 2670063.41735502, 510077.3388945927], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5476800.0000, 
sim time next is 5477400.0000, 
raw observation next is [33.0, 70.0, 1.0, 2.0, 0.7039717122669862, 1.0, 2.0, 0.6725758956477558, 1.0, 2.0, 1.03, 7.00509804532388, 6.9112, 170.5573041426782, 2822130.083618509, 2754867.094224736, 522587.8422185783], 
processed observation next is [1.0, 0.391304347826087, 0.7630331753554502, 0.7, 1.0, 1.0, 0.6433394123698628, 1.0, 1.0, 0.6055131272864527, 1.0, 1.0, 1.0365853658536586, 0.009389804532388002, 0.0, 0.8375144448122397, 0.7839250232273637, 0.7652408595068712, 0.7799818540575796], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.600434], dtype=float32), 1.0630834]. 
=============================================
[2019-03-27 14:56:27,783] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2695770e-26 1.0000000e+00 4.7321169e-34 0.0000000e+00 4.2465955e-22], sum to 1.0000
[2019-03-27 14:56:27,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7181
[2019-03-27 14:56:27,798] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 87.33333333333334, 1.0, 2.0, 0.9813925244868335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9128296438562, 1371778.046959113, 1371778.046959113, 293311.3582079693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5282400.0000, 
sim time next is 5283000.0000, 
raw observation next is [28.6, 87.5, 1.0, 2.0, 1.005528955069026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564787521, 1405537.977815509, 1405537.977815509, 300618.8163370342], 
processed observation next is [1.0, 0.13043478260869565, 0.5545023696682465, 0.875, 1.0, 1.0, 1.006661391649429, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399449967448, 0.39042721605986364, 0.39042721605986364, 0.44868480050303616], 
reward next is 0.5513, 
noisyNet noise sample is [array([0.87889147], dtype=float32), 1.3361216]. 
=============================================
[2019-03-27 14:56:27,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[49.477386]
 [49.012768]
 [48.768204]
 [46.531467]
 [43.848198]], R is [[49.09010315]
 [49.16142273]
 [48.66980743]
 [48.75423431]
 [48.82694626]].
[2019-03-27 14:56:30,630] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9978460e-26 1.0000000e+00 1.2392780e-34 0.0000000e+00 7.1503956e-21], sum to 1.0000
[2019-03-27 14:56:30,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9693
[2019-03-27 14:56:30,645] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 87.33333333333333, 1.0, 2.0, 0.8827643314146273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1233836.697391139, 1233836.697391138, 265246.1361687371], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5377800.0000, 
sim time next is 5378400.0000, 
raw observation next is [29.5, 86.0, 1.0, 2.0, 0.8926081449586144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1247603.445734231, 1247603.445734231, 267918.0238272005], 
processed observation next is [1.0, 0.2608695652173913, 0.5971563981042655, 0.86, 1.0, 1.0, 0.8706122228417041, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34655651270395305, 0.34655651270395305, 0.39987764750328436], 
reward next is 0.6001, 
noisyNet noise sample is [array([1.5783814], dtype=float32), 0.3226756]. 
=============================================
[2019-03-27 14:56:39,468] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1540241e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9242194e-32], sum to 1.0000
[2019-03-27 14:56:39,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4295
[2019-03-27 14:56:39,483] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.08333333333334, 92.33333333333333, 1.0, 2.0, 0.5916375406498293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826771.375346926, 826771.375346926, 199198.4885205289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5449800.0000, 
sim time next is 5450400.0000, 
raw observation next is [28.0, 93.0, 1.0, 2.0, 0.5919339212080333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827185.707369549, 827185.707369549, 199252.9924921208], 
processed observation next is [1.0, 0.08695652173913043, 0.5260663507109005, 0.93, 1.0, 1.0, 0.5083541219373895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22977380760265248, 0.22977380760265248, 0.297392526107643], 
reward next is 0.7026, 
noisyNet noise sample is [array([1.358741], dtype=float32), 1.5300485]. 
=============================================
[2019-03-27 14:56:42,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1378104e-35], sum to 1.0000
[2019-03-27 14:56:42,048] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8325
[2019-03-27 14:56:42,053] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 82.0, 1.0, 2.0, 0.5464315738809508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763576.5459721037, 763576.5459721037, 191190.3587485276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5774400.0000, 
sim time next is 5775000.0000, 
raw observation next is [28.15, 82.33333333333334, 1.0, 2.0, 0.5469741334697272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764334.9846339363, 764334.9846339363, 191282.8422239294], 
processed observation next is [0.0, 0.8695652173913043, 0.533175355450237, 0.8233333333333335, 1.0, 1.0, 0.4541857029755749, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21231527350942675, 0.21231527350942675, 0.28549677943870055], 
reward next is 0.7145, 
noisyNet noise sample is [array([-1.4445618], dtype=float32), 0.5301983]. 
=============================================
[2019-03-27 14:56:42,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.19291 ]
 [76.05065 ]
 [76.0631  ]
 [76.06646 ]
 [76.062225]], R is [[76.16972351]
 [76.12266541]
 [76.075737  ]
 [76.02902222]
 [75.98258209]].
[2019-03-27 14:56:48,830] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9248556e-09 7.7385053e-02 4.7744457e-16 1.9663974e-11 9.2261493e-01], sum to 1.0000
[2019-03-27 14:56:48,840] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3537
[2019-03-27 14:56:48,845] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.93333333333334, 49.33333333333333, 1.0, 2.0, 0.5701380476500114, 1.0, 2.0, 0.5701380476500114, 1.0, 2.0, 0.9794128280766915, 6.911199999999999, 6.9112, 170.5573041426782, 2391889.189618608, 2391889.189618608, 464784.7949155278], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5582400.0000, 
sim time next is 5583000.0000, 
raw observation next is [33.86666666666666, 49.66666666666667, 1.0, 2.0, 0.5861199077053515, 1.0, 2.0, 0.5861199077053515, 1.0, 2.0, 1.007679419676299, 6.9112, 6.9112, 170.5573041426782, 2459003.550324644, 2459003.550324644, 477598.4104087886], 
processed observation next is [1.0, 0.6086956521739131, 0.8041074249605052, 0.4966666666666667, 1.0, 1.0, 0.5013492863919897, 1.0, 1.0, 0.5013492863919897, 1.0, 1.0, 1.0093651459467061, 0.0, 0.0, 0.8375144448122397, 0.6830565417568455, 0.6830565417568455, 0.7128334483713262], 
reward next is 0.2872, 
noisyNet noise sample is [array([-1.60719], dtype=float32), -0.63432646]. 
=============================================
[2019-03-27 14:56:48,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[53.322002]
 [53.363255]
 [52.804394]
 [51.92389 ]
 [51.27975 ]], R is [[53.49441147]
 [53.26575851]
 [53.09718323]
 [52.91280746]
 [52.69156265]].
[2019-03-27 14:56:52,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8353906e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2150165e-33], sum to 1.0000
[2019-03-27 14:56:52,660] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6890
[2019-03-27 14:56:52,669] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.46666666666667, 76.66666666666667, 1.0, 2.0, 0.5231095193629334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730975.3894732237, 730975.3894732231, 187295.9904514712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5730000.0000, 
sim time next is 5730600.0000, 
raw observation next is [28.63333333333333, 75.83333333333333, 1.0, 2.0, 0.5236745060107719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731765.1544703929, 731765.1544703934, 187388.4692998539], 
processed observation next is [0.0, 0.30434782608695654, 0.55608214849921, 0.7583333333333333, 1.0, 1.0, 0.4261138626635806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20326809846399801, 0.20326809846399818, 0.2796842825370954], 
reward next is 0.7203, 
noisyNet noise sample is [array([-0.96500844], dtype=float32), 0.52315503]. 
=============================================
[2019-03-27 14:56:55,139] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 14:56:55,143] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 14:56:55,147] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:56:55,148] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 14:56:55,148] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:56:55,152] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 14:56:55,152] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 14:56:55,153] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:56:55,156] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:56:55,154] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 14:56:55,162] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 14:56:55,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run75
[2019-03-27 14:56:55,206] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run75
[2019-03-27 14:56:55,228] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run75
[2019-03-27 14:56:55,246] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run75
[2019-03-27 14:56:55,248] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run75
[2019-03-27 14:57:06,942] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06122404], dtype=float32), 0.083626464]
[2019-03-27 14:57:06,943] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.83333333333333, 92.33333333333334, 1.0, 2.0, 0.22213393700398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 369577.2077384248, 369577.2077384248, 157902.5166785965]
[2019-03-27 14:57:06,943] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 14:57:06,946] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.380904e-38], sampled 0.8604589462402636
[2019-03-27 14:57:10,018] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06122404], dtype=float32), 0.083626464]
[2019-03-27 14:57:10,018] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.73814346, 92.19201049, 1.0, 2.0, 0.2904513624961729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470626.7417708941, 470626.7417708935, 164982.1132070706]
[2019-03-27 14:57:10,018] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:57:10,022] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5889482e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3541785e-34], sampled 0.483312759947769
[2019-03-27 14:57:14,628] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06122404], dtype=float32), 0.083626464]
[2019-03-27 14:57:14,629] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.560710585, 83.62529611, 1.0, 2.0, 0.4205886286058153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 600851.5677379111, 600851.5677379104, 173802.7062855262]
[2019-03-27 14:57:14,631] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:57:14,637] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9298951355387175
[2019-03-27 14:57:30,865] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06122404], dtype=float32), 0.083626464]
[2019-03-27 14:57:30,865] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.37651612333334, 99.84540123666667, 1.0, 2.0, 0.5352532559120549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747950.6067922228, 747950.6067922235, 189303.5010662293]
[2019-03-27 14:57:30,865] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 14:57:30,868] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5318018e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.7413294e-26], sampled 0.8531832694792152
[2019-03-27 14:57:50,574] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06122404], dtype=float32), 0.083626464]
[2019-03-27 14:57:50,577] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.36671973833333, 89.639899155, 1.0, 2.0, 0.6400914305644734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 894510.7528047313, 894510.7528047313, 208444.225536728]
[2019-03-27 14:57:50,577] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 14:57:50,579] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5015428e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9991230e-27], sampled 0.18886467074440372
[2019-03-27 14:58:25,809] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06122404], dtype=float32), 0.083626464]
[2019-03-27 14:58:25,810] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.6, 80.0, 1.0, 2.0, 0.5901201396264585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824650.0921949331, 824650.0921949331, 198919.0405142204]
[2019-03-27 14:58:25,812] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 14:58:25,816] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1857860e-30 1.0000000e+00 1.3976836e-37 0.0000000e+00 1.6561519e-24], sampled 0.19643244329313114
[2019-03-27 14:59:02,024] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8350.7113 2925607969.3427 1129.0000
[2019-03-27 14:59:03,370] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8603.5280 2837815644.7879 890.0000
[2019-03-27 14:59:03,824] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8219.6513 2997292535.7891 1197.0000
[2019-03-27 14:59:03,851] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8710.9646 2778361532.3675 810.0000
[2019-03-27 14:59:03,900] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8026.8435 3157011233.8521 1517.0000
[2019-03-27 14:59:04,916] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1850000, evaluation results [1850000.0, 8026.8435019210765, 3157011233.8520646, 1517.0, 8350.711275133262, 2925607969.3427057, 1129.0, 8710.964555525492, 2778361532.367525, 810.0, 8219.651314868912, 2997292535.789088, 1197.0, 8603.527991306692, 2837815644.787868, 890.0]
[2019-03-27 14:59:11,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1011509e-29 1.0000000e+00 9.3743414e-37 0.0000000e+00 2.4335279e-25], sum to 1.0000
[2019-03-27 14:59:11,808] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9814
[2019-03-27 14:59:11,818] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 93.0, 1.0, 2.0, 0.5268298874518801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736175.90634333, 736175.90634333, 187906.6062386272], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6055200.0000, 
sim time next is 6055800.0000, 
raw observation next is [26.2, 93.0, 1.0, 2.0, 1.030843541314846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 169.2856565914384, 1440944.811024248, 1440944.811024248, 308531.7359730095], 
processed observation next is [1.0, 0.08695652173913043, 0.44075829383886256, 0.93, 1.0, 1.0, 1.0371608931504168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8312700731728844, 0.4002624475067355, 0.4002624475067355, 0.4604951283179246], 
reward next is 0.5395, 
noisyNet noise sample is [array([-0.7667638], dtype=float32), 0.7122171]. 
=============================================
[2019-03-27 14:59:19,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9609780e-29 1.0000000e+00 2.8140865e-37 0.0000000e+00 1.1193475e-25], sum to 1.0000
[2019-03-27 14:59:19,456] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2574
[2019-03-27 14:59:19,464] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 87.33333333333334, 1.0, 2.0, 0.5401113560650096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754741.616140158, 754741.6161401586, 190119.3144044394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6039600.0000, 
sim time next is 6040200.0000, 
raw observation next is [27.26666666666667, 87.66666666666667, 1.0, 2.0, 0.5404300852562047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755187.160740262, 755187.160740262, 190172.9857932953], 
processed observation next is [1.0, 0.9130434782608695, 0.4913112164297, 0.8766666666666667, 1.0, 1.0, 0.44630130753759606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20977421131673946, 0.20977421131673946, 0.2838402773034258], 
reward next is 0.7162, 
noisyNet noise sample is [array([-0.10085249], dtype=float32), -1.7752402]. 
=============================================
[2019-03-27 14:59:20,955] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.391233e-30 1.000000e+00 7.551516e-36 0.000000e+00 3.445010e-23], sum to 1.0000
[2019-03-27 14:59:20,964] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2013
[2019-03-27 14:59:20,968] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 87.0, 1.0, 2.0, 0.536514041933107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749713.0219583681, 749713.0219583681, 189514.8672336705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6127200.0000, 
sim time next is 6127800.0000, 
raw observation next is [27.3, 87.0, 1.0, 2.0, 0.5361114870589861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749150.3021913682, 749150.3021913688, 189447.4903778952], 
processed observation next is [1.0, 0.9565217391304348, 0.4928909952606636, 0.87, 1.0, 1.0, 0.44109817717950134, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20809730616426894, 0.2080973061642691, 0.28275744832521676], 
reward next is 0.7172, 
noisyNet noise sample is [array([0.34093413], dtype=float32), 0.23564933]. 
=============================================
[2019-03-27 14:59:21,514] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.9581575e-14 9.9999809e-01 1.3544159e-21 4.4419630e-19 1.8603941e-06], sum to 1.0000
[2019-03-27 14:59:21,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3540
[2019-03-27 14:59:21,536] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2184865.620119947 W.
[2019-03-27 14:59:21,547] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.63333333333334, 66.83333333333333, 1.0, 2.0, 0.520835335877913, 1.0, 2.0, 0.520835335877913, 1.0, 2.0, 0.8998730817899283, 6.911199999999999, 6.9112, 170.5573041426782, 2184865.620119947, 2184865.620119948, 428975.1225118136], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6105000.0000, 
sim time next is 6105600.0000, 
raw observation next is [30.6, 67.0, 1.0, 2.0, 0.9530052164385835, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.989850005211304, 6.9112, 168.912488255067, 2229226.033501036, 2173429.167558671, 449779.8376234797], 
processed observation next is [1.0, 0.6956521739130435, 0.6492890995260664, 0.67, 1.0, 1.0, 0.9433797788416669, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007865000521130395, 0.0, 0.8294376458040972, 0.6192294537502878, 0.603730324321853, 0.6713131904828056], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9406565], dtype=float32), 1.1825444]. 
=============================================
[2019-03-27 14:59:21,925] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0429455e-08 1.7033422e-08 8.9720133e-16 2.4062899e-07 9.9999976e-01], sum to 1.0000
[2019-03-27 14:59:21,934] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6201
[2019-03-27 14:59:21,949] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 76.66666666666667, 1.0, 2.0, 0.5825468666183545, 1.0, 2.0, 0.5825468666183545, 1.0, 2.0, 1.011691256123408, 6.9112, 6.9112, 170.5573041426782, 2443998.577960798, 2443998.577960798, 476921.5730642774], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5908800.0000, 
sim time next is 5909400.0000, 
raw observation next is [30.65, 76.0, 1.0, 2.0, 0.5913946341503558, 1.0, 2.0, 0.5913946341503558, 1.0, 2.0, 1.02705690232506, 6.911199999999999, 6.9112, 170.5573041426782, 2481155.073177639, 2481155.07317764, 484099.2689875587], 
processed observation next is [1.0, 0.391304347826087, 0.6516587677725119, 0.76, 1.0, 1.0, 0.5077043784944045, 1.0, 1.0, 0.5077043784944045, 1.0, 1.0, 1.032996222347634, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6892097425493442, 0.6892097425493444, 0.7225362223694907], 
reward next is 0.2775, 
noisyNet noise sample is [array([-0.2614394], dtype=float32), -0.8137558]. 
=============================================
[2019-03-27 14:59:26,828] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0148514e-14 9.9999940e-01 4.0446394e-21 3.9391339e-20 6.0926072e-07], sum to 1.0000
[2019-03-27 14:59:26,839] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1966
[2019-03-27 14:59:26,853] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2099962.375537966 W.
[2019-03-27 14:59:26,859] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.03333333333333, 82.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.821479272219213, 6.9112, 168.9081532308424, 2099962.375537966, 1454197.294199544, 311353.450260544], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5991600.0000, 
sim time next is 5992200.0000, 
raw observation next is [29.2, 81.5, 1.0, 2.0, 0.8138482944838253, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.00545983078665, 6.9112, 168.9116550448551, 2034454.826056434, 1967584.173162265, 411295.5807374633], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.815, 1.0, 1.0, 0.7757208367275004, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009425983078665023, 0.0, 0.8294335543605116, 0.5651263405712317, 0.5465511592117402, 0.6138740011006916], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.12707625], dtype=float32), 0.056985352]. 
=============================================
[2019-03-27 14:59:29,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7234279e-24 1.0000000e+00 4.6152430e-32 2.6038583e-35 8.8741415e-19], sum to 1.0000
[2019-03-27 14:59:29,063] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0472
[2019-03-27 14:59:29,068] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.98333333333333, 89.83333333333333, 1.0, 2.0, 0.6892076481477727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 963180.4855339851, 963180.4855339851, 218533.4147144271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6156600.0000, 
sim time next is 6157200.0000, 
raw observation next is [27.06666666666667, 89.66666666666667, 1.0, 2.0, 0.6813980085189678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 952261.4760156533, 952261.4760156539, 216881.8717542713], 
processed observation next is [1.0, 0.2608695652173913, 0.48183254344391807, 0.8966666666666667, 1.0, 1.0, 0.6161421789385154, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2645170766710148, 0.264517076671015, 0.3237042862004049], 
reward next is 0.6763, 
noisyNet noise sample is [array([0.5632676], dtype=float32), -0.29789203]. 
=============================================
[2019-03-27 14:59:33,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2748787e-23 1.0000000e+00 9.7764774e-31 2.3488875e-32 2.9827151e-16], sum to 1.0000
[2019-03-27 14:59:33,317] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9461
[2019-03-27 14:59:33,321] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.46666666666667, 81.66666666666666, 1.0, 2.0, 0.7067526511077058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 987711.3551835127, 987711.3551835127, 222310.6945217957], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6076200.0000, 
sim time next is 6076800.0000, 
raw observation next is [28.6, 81.0, 1.0, 2.0, 0.7542881960868781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1054176.902118115, 1054176.902118115, 233000.8552773635], 
processed observation next is [1.0, 0.34782608695652173, 0.5545023696682465, 0.81, 1.0, 1.0, 0.7039616820323833, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29282691725503196, 0.29282691725503196, 0.3477624705632291], 
reward next is 0.6522, 
noisyNet noise sample is [array([-1.6489154], dtype=float32), 1.4728885]. 
=============================================
[2019-03-27 14:59:33,945] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9856328e-10 9.9202031e-01 1.5756796e-18 1.0255080e-14 7.9797087e-03], sum to 1.0000
[2019-03-27 14:59:33,955] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4118
[2019-03-27 14:59:33,967] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1874117.748292412 W.
[2019-03-27 14:59:33,977] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.6, 67.5, 1.0, 2.0, 0.4468229917203047, 1.0, 2.0, 0.4468229917203047, 1.0, 2.0, 0.7717789894502392, 6.911200000000001, 6.9112, 170.5573041426782, 1874117.748292412, 1874117.748292412, 379653.5834916938], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6085800.0000, 
sim time next is 6086400.0000, 
raw observation next is [30.73333333333333, 66.66666666666666, 1.0, 2.0, 0.762361562796447, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.9905021850268, 6.9112, 168.912484478195, 1962398.282465328, 1906138.740256138, 399333.2288029151], 
processed observation next is [1.0, 0.43478260869565216, 0.6556082148499209, 0.6666666666666665, 1.0, 1.0, 0.7136886298752373, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00793021850268003, 0.0, 0.8294376272579267, 0.5451106340181466, 0.5294829834044827, 0.5960197444819628], 
reward next is 0.0075, 
noisyNet noise sample is [array([-0.792836], dtype=float32), -1.3847911]. 
=============================================
[2019-03-27 14:59:40,823] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4461412e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6695127e-29], sum to 1.0000
[2019-03-27 14:59:40,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7369
[2019-03-27 14:59:40,830] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.86666666666667, 62.66666666666666, 1.0, 2.0, 0.5167895115338861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722141.0243194941, 722141.0243194941, 186268.4968835686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6270000.0000, 
sim time next is 6270600.0000, 
raw observation next is [30.88333333333333, 62.33333333333334, 1.0, 2.0, 0.5146297733259568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719122.070858105, 719122.0708581056, 185920.072765458], 
processed observation next is [0.0, 0.5652173913043478, 0.6627172195892573, 0.6233333333333334, 1.0, 1.0, 0.4152165943686227, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19975613079391805, 0.1997561307939182, 0.277492645918594], 
reward next is 0.7225, 
noisyNet noise sample is [array([1.3425153], dtype=float32), 0.7244363]. 
=============================================
[2019-03-27 14:59:44,344] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 4.10135e-37], sum to 1.0000
[2019-03-27 14:59:44,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1723
[2019-03-27 14:59:44,361] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 88.16666666666667, 1.0, 2.0, 0.5272988100172635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736831.3916006412, 736831.3916006412, 187983.5238761928], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6322200.0000, 
sim time next is 6322800.0000, 
raw observation next is [26.73333333333334, 88.33333333333334, 1.0, 2.0, 0.5260192534080648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735042.7593023086, 735042.759302308, 187772.991973549], 
processed observation next is [0.0, 0.17391304347826086, 0.4660347551342816, 0.8833333333333334, 1.0, 1.0, 0.4289388595277889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20417854425064128, 0.2041785442506411, 0.2802581969754463], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.05755216], dtype=float32), 1.673605]. 
=============================================
[2019-03-27 14:59:44,618] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4093071e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.4536024e-30], sum to 1.0000
[2019-03-27 14:59:44,629] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2893
[2019-03-27 14:59:44,639] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 90.5, 1.0, 2.0, 0.5208180888733931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727772.3258259722, 727772.325825973, 186922.3721718123], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6226200.0000, 
sim time next is 6226800.0000, 
raw observation next is [26.43333333333333, 90.66666666666667, 1.0, 2.0, 0.521970848881068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729383.7039258432, 729383.7039258427, 187110.2387879196], 
processed observation next is [0.0, 0.043478260869565216, 0.4518167456556081, 0.9066666666666667, 1.0, 1.0, 0.42406126371213015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20260658442384535, 0.2026065844238452, 0.2792690131162979], 
reward next is 0.7207, 
noisyNet noise sample is [array([1.2839109], dtype=float32), 1.5475174]. 
=============================================
[2019-03-27 14:59:45,269] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9145759e-12 9.9895573e-01 5.9797460e-19 1.6294329e-15 1.0443040e-03], sum to 1.0000
[2019-03-27 14:59:45,279] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3392
[2019-03-27 14:59:45,290] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3295645.875156956 W.
[2019-03-27 14:59:45,297] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.9, 55.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 9.506081524122191, 6.9112, 168.8981999234713, 3295645.875156956, 1454908.437713527, 307865.5538458219], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6528000.0000, 
sim time next is 6528600.0000, 
raw observation next is [31.95, 55.5, 1.0, 2.0, 0.883511746785305, 1.0, 1.0, 0.883511746785305, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2471131.505614188, 2471131.505614188, 462564.0076228277], 
processed observation next is [1.0, 0.5652173913043478, 0.7132701421800948, 0.555, 1.0, 1.0, 0.859652706970247, 1.0, 0.5, 0.859652706970247, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6864254182261633, 0.6864254182261633, 0.6903940412281011], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5008681], dtype=float32), -0.42764044]. 
=============================================
[2019-03-27 14:59:48,007] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.26495968e-25 1.00000000e+00 1.47277485e-33 3.54212206e-35
 3.74261508e-18], sum to 1.0000
[2019-03-27 14:59:48,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0553
[2019-03-27 14:59:48,024] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 86.66666666666666, 1.0, 2.0, 0.5201588571803704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726850.8241783377, 726850.8241783377, 186814.997434339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6565800.0000, 
sim time next is 6566400.0000, 
raw observation next is [26.9, 87.0, 1.0, 2.0, 0.5200690135913529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726725.237111787, 726725.237111787, 186800.3863176981], 
processed observation next is [1.0, 0.0, 0.4739336492890995, 0.87, 1.0, 1.0, 0.4217698958931963, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20186812141994082, 0.20186812141994082, 0.278806546742833], 
reward next is 0.7212, 
noisyNet noise sample is [array([-1.5120262], dtype=float32), -0.8518595]. 
=============================================
[2019-03-27 15:00:01,593] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4138916e-28 1.0000000e+00 3.1597925e-35 1.4486339e-37 1.8183697e-19], sum to 1.0000
[2019-03-27 15:00:01,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9379
[2019-03-27 15:00:01,613] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 87.33333333333334, 1.0, 2.0, 0.5325411868914179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 744159.4963270944, 744159.4963270938, 188851.6051236656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6477600.0000, 
sim time next is 6478200.0000, 
raw observation next is [27.05, 87.5, 1.0, 2.0, 0.5321549901357916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743619.6459154987, 743619.6459154981, 188787.3500403001], 
processed observation next is [1.0, 1.0, 0.4810426540284361, 0.875, 1.0, 1.0, 0.43633131341661635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2065610127543052, 0.20656101275430505, 0.2817721642392539], 
reward next is 0.7182, 
noisyNet noise sample is [array([0.01046932], dtype=float32), -0.5787893]. 
=============================================
[2019-03-27 15:00:08,018] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:00:08,030] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5729
[2019-03-27 15:00:08,038] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 67.0, 1.0, 2.0, 0.4079333296678194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 603191.0876093928, 603191.0876093934, 174673.6715649245], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6724800.0000, 
sim time next is 6725400.0000, 
raw observation next is [27.08333333333334, 67.33333333333334, 1.0, 2.0, 0.4045694827339936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 599843.3394211539, 599843.3394211545, 174411.3904310869], 
processed observation next is [1.0, 0.8695652173913043, 0.4826224328594, 0.6733333333333335, 1.0, 1.0, 0.2826138346192694, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16662314983920942, 0.16662314983920956, 0.2603155081060999], 
reward next is 0.7397, 
noisyNet noise sample is [array([0.43658346], dtype=float32), -1.1687634]. 
=============================================
[2019-03-27 15:00:11,178] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 15:00:11,181] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:00:11,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:00:11,182] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:00:11,183] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:00:11,185] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:00:11,187] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:00:11,187] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:00:11,186] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:00:11,188] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:00:11,192] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:00:11,228] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run76
[2019-03-27 15:00:11,228] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run76
[2019-03-27 15:00:11,269] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run76
[2019-03-27 15:00:11,293] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run76
[2019-03-27 15:00:11,294] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run76
[2019-03-27 15:00:22,579] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:00:22,581] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.13333333333333, 58.33333333333334, 1.0, 2.0, 0.3009258596036454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485919.5579017429, 485919.5579017422, 166074.2232063537]
[2019-03-27 15:00:22,582] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:00:22,587] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8434554960736745
[2019-03-27 15:00:27,228] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:00:27,230] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 92.66666666666667, 1.0, 2.0, 0.3413312394322154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 528197.6875266971, 528197.6875266965, 168928.8784802679]
[2019-03-27 15:00:27,230] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:00:27,234] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8844546264279681
[2019-03-27 15:00:32,159] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:00:32,160] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.56808124, 88.2143662, 1.0, 2.0, 0.3706114736117022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560895.5810548547, 560895.5810548547, 171278.0252048764]
[2019-03-27 15:00:32,161] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:00:32,169] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8870929146977777
[2019-03-27 15:00:53,602] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:00:53,602] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.5, 94.0, 1.0, 2.0, 0.4829443464766867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678814.1831495363, 678814.1831495357, 181479.5684186425]
[2019-03-27 15:00:53,603] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:00:53,604] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8503520578096018
[2019-03-27 15:01:00,578] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:01:00,580] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 1.0, 2.0, 0.3055869598939788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486628.50575581, 486628.5057558094, 166099.293527577]
[2019-03-27 15:01:00,582] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:01:00,586] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33390508499525184
[2019-03-27 15:01:03,183] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:01:03,183] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.52466341, 100.0, 1.0, 2.0, 0.4878769253308928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685330.4824653873, 685330.4824653873, 182181.3902899519]
[2019-03-27 15:01:03,184] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:01:03,186] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.622387e-36], sampled 0.9660830811064939
[2019-03-27 15:01:13,213] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:01:13,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.5210283407119982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728066.2248126034, 728066.2248126034, 186956.1836537216]
[2019-03-27 15:01:13,215] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:01:13,218] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.9575676e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.7983557e-31], sampled 0.39130104764470364
[2019-03-27 15:01:25,326] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:01:25,326] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.85, 51.83333333333333, 1.0, 2.0, 0.5195172380397252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725953.9428611128, 725953.9428611135, 186709.5921711856]
[2019-03-27 15:01:25,326] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:01:25,328] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.838601e-35], sampled 0.022732174825612628
[2019-03-27 15:01:37,041] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:01:37,045] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.6, 87.33333333333334, 1.0, 2.0, 0.9813925246211247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9128296438552, 1371778.047146945, 1371778.047146944, 293311.3582477776]
[2019-03-27 15:01:37,046] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:01:37,051] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.6367084e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0736715e-28], sampled 0.4027569040437593
[2019-03-27 15:01:49,506] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:01:49,508] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.02909043166667, 84.14485781, 1.0, 2.0, 0.5976690393033292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835203.2795303658, 835203.2795303658, 200311.4978427085]
[2019-03-27 15:01:49,511] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:01:49,514] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4300793e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.3745987e-31], sampled 0.09466337084451215
[2019-03-27 15:02:05,109] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:02:05,111] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.1, 81.0, 1.0, 2.0, 0.7630746691780447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1066462.853497463, 1066462.853497463, 235051.3920771941]
[2019-03-27 15:02:05,113] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:02:05,116] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8982419e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2465400e-29], sampled 0.02347467123252034
[2019-03-27 15:02:14,494] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0601356], dtype=float32), 0.08471362]
[2019-03-27 15:02:14,498] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.95137052, 91.68462012500001, 1.0, 2.0, 0.4973038444642738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694903.6252494564, 694903.6252494564, 183173.2962021713]
[2019-03-27 15:02:14,499] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:02:14,503] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8544043e-36], sampled 0.1783021208406368
[2019-03-27 15:02:19,487] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.5423 2927690336.0773 1357.0000
[2019-03-27 15:02:19,746] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.4419 3163732911.2962 1831.0000
[2019-03-27 15:02:19,842] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.8227 2779378712.4956 933.0000
[2019-03-27 15:02:20,057] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.0163 3007411548.1453 1761.0000
[2019-03-27 15:02:20,261] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.3644 2842298802.9008 1148.0000
[2019-03-27 15:02:21,280] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1875000, evaluation results [1875000.0, 7882.441948068439, 3163732911.296218, 1831.0, 8249.542291310483, 2927690336.0772805, 1357.0, 8661.822736060973, 2779378712.495647, 933.0, 8002.0162897562095, 3007411548.1453137, 1761.0, 8496.364443070861, 2842298802.9008203, 1148.0]
[2019-03-27 15:02:22,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7031315e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3349918e-30], sum to 1.0000
[2019-03-27 15:02:22,497] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2393
[2019-03-27 15:02:22,503] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 68.0, 1.0, 2.0, 0.7824940040863644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1220329.309675551, 1220329.309675551, 255869.7009395369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6771600.0000, 
sim time next is 6772200.0000, 
raw observation next is [25.43333333333333, 66.66666666666667, 1.0, 2.0, 0.8505409820210375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1326035.591225285, 1326035.591225284, 274888.0411811259], 
processed observation next is [1.0, 0.391304347826087, 0.40442338072669815, 0.6666666666666667, 1.0, 1.0, 0.8199288940012499, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3683432197848014, 0.3683432197848011, 0.4102806584792924], 
reward next is 0.5897, 
noisyNet noise sample is [array([1.119042], dtype=float32), 0.78932023]. 
=============================================
[2019-03-27 15:02:23,249] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:02:23,259] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6145
[2019-03-27 15:02:23,265] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.6, 39.0, 1.0, 2.0, 0.2794126779231882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 449936.831319606, 449936.831319606, 163595.2500997748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6880800.0000, 
sim time next is 6881400.0000, 
raw observation next is [29.55, 40.0, 1.0, 2.0, 0.2864170410564127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459727.3100937979, 459727.3100937979, 164248.2140410685], 
processed observation next is [0.0, 0.6521739130434783, 0.5995260663507109, 0.4, 1.0, 1.0, 0.14026149524868997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12770203058161053, 0.12770203058161053, 0.24514658812099777], 
reward next is 0.7549, 
noisyNet noise sample is [array([2.7294962], dtype=float32), 1.7854565]. 
=============================================
[2019-03-27 15:02:23,459] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.3701219e-15 9.9999964e-01 7.4229292e-22 5.2238810e-21 3.2570296e-07], sum to 1.0000
[2019-03-27 15:02:23,468] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4258
[2019-03-27 15:02:23,480] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2268929.752544003 W.
[2019-03-27 15:02:23,485] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.93333333333333, 69.83333333333333, 1.0, 2.0, 0.9813725155261321, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984530810707755, 6.9112, 168.9124611206956, 2268929.752544003, 2216906.504150988, 458407.5678540447], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6605400.0000, 
sim time next is 6606000.0000, 
raw observation next is [30.1, 69.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.985524491993781, 6.9112, 168.9123198736635, 2351507.381455599, 2298779.227517975, 476628.2129262654], 
processed observation next is [1.0, 0.4782608695652174, 0.6255924170616115, 0.69, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007432449199378066, 0.0, 0.8294368189743208, 0.6531964948487775, 0.6385497854216597, 0.7113853924272617], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38826585], dtype=float32), -1.4994869]. 
=============================================
[2019-03-27 15:02:23,503] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[64.096436]
 [62.10658 ]
 [61.005016]
 [60.86986 ]
 [60.500458]], R is [[64.56269073]
 [63.91706467]
 [63.27789307]
 [62.6451149 ]
 [62.4029274 ]].
[2019-03-27 15:02:24,523] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3930551e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6499314e-32], sum to 1.0000
[2019-03-27 15:02:24,533] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2551
[2019-03-27 15:02:24,540] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 76.0, 1.0, 2.0, 0.3432735805155303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 533249.7136302579, 533249.7136302573, 169393.0204603326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6739200.0000, 
sim time next is 6739800.0000, 
raw observation next is [23.96666666666667, 76.5, 1.0, 2.0, 0.3415297842794948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531305.4473124315, 531305.4473124315, 169256.6529758127], 
processed observation next is [1.0, 0.0, 0.33491311216429714, 0.765, 1.0, 1.0, 0.20666239069818654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14758484647567544, 0.14758484647567544, 0.2526218701131533], 
reward next is 0.7474, 
noisyNet noise sample is [array([-0.5671233], dtype=float32), -0.40253705]. 
=============================================
[2019-03-27 15:02:38,735] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:02:38,747] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8227
[2019-03-27 15:02:38,752] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.35, 65.33333333333333, 1.0, 2.0, 0.4393459612669904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 631599.922484929, 631599.922484929, 176916.4926968014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6945000.0000, 
sim time next is 6945600.0000, 
raw observation next is [28.5, 64.66666666666667, 1.0, 2.0, 0.4408929778499958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633161.8353945035, 633161.8353945035, 177054.3946692837], 
processed observation next is [0.0, 0.391304347826087, 0.5497630331753555, 0.6466666666666667, 1.0, 1.0, 0.3263770817469829, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17587828760958432, 0.17587828760958432, 0.26426029055116973], 
reward next is 0.7357, 
noisyNet noise sample is [array([1.10779], dtype=float32), -0.33250785]. 
=============================================
[2019-03-27 15:02:41,430] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:02:41,438] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8166
[2019-03-27 15:02:41,443] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 45.0, 1.0, 2.0, 0.31181910228717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491540.4331209147, 491540.4331209154, 166362.4966810736], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6884400.0000, 
sim time next is 6885000.0000, 
raw observation next is [29.3, 46.0, 1.0, 2.0, 0.3207212474971916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503496.4204848484, 503496.4204848491, 167206.9356454084], 
processed observation next is [0.0, 0.6956521739130435, 0.5876777251184835, 0.46, 1.0, 1.0, 0.18159186445444772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13986011680134677, 0.13986011680134697, 0.24956259051553492], 
reward next is 0.7504, 
noisyNet noise sample is [array([0.10703926], dtype=float32), 0.648285]. 
=============================================
[2019-03-27 15:02:41,465] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[76.117065]
 [76.10886 ]
 [76.08757 ]
 [75.960396]
 [75.97347 ]], R is [[76.12722015]
 [76.11765289]
 [76.1089325 ]
 [76.10042572]
 [76.0927124 ]].
[2019-03-27 15:02:42,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 8.131158e-37], sum to 1.0000
[2019-03-27 15:02:42,279] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5710
[2019-03-27 15:02:42,283] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 89.66666666666666, 1.0, 2.0, 0.5690731935054382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795227.5014856862, 795227.5014856867, 195121.7565226524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7191600.0000, 
sim time next is 7192200.0000, 
raw observation next is [26.46666666666667, 89.33333333333333, 1.0, 2.0, 0.5704420546150114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797141.0768989144, 797141.0768989144, 195364.6506978352], 
processed observation next is [1.0, 0.21739130434782608, 0.45339652448657203, 0.8933333333333333, 1.0, 1.0, 0.48246030676507395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22142807691636512, 0.22142807691636512, 0.2915890308922913], 
reward next is 0.7084, 
noisyNet noise sample is [array([-0.7007969], dtype=float32), -1.8161093]. 
=============================================
[2019-03-27 15:02:45,181] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 2.766635e-37], sum to 1.0000
[2019-03-27 15:02:45,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5033
[2019-03-27 15:02:45,200] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 77.33333333333334, 1.0, 2.0, 0.447679540859505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 641649.3312446683, 641649.3312446683, 177875.1561538553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6997200.0000, 
sim time next is 6997800.0000, 
raw observation next is [26.35, 77.66666666666666, 1.0, 2.0, 0.4481809201821601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 642322.6302807647, 642322.6302807654, 177942.220495202], 
processed observation next is [0.0, 1.0, 0.4478672985781992, 0.7766666666666666, 1.0, 1.0, 0.3351577351592291, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17842295285576795, 0.17842295285576815, 0.2655854037241821], 
reward next is 0.7344, 
noisyNet noise sample is [array([-0.3013516], dtype=float32), -0.04955856]. 
=============================================
[2019-03-27 15:02:46,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0420300e-31 1.0000000e+00 1.9021169e-37 0.0000000e+00 2.5207929e-27], sum to 1.0000
[2019-03-27 15:02:46,738] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2171
[2019-03-27 15:02:46,744] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 64.33333333333334, 1.0, 2.0, 0.7743874539833375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1179373.118487598, 1179373.118487598, 250502.7218039786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7299600.0000, 
sim time next is 7300200.0000, 
raw observation next is [26.81666666666667, 63.66666666666666, 1.0, 2.0, 0.7866281003577029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1199196.73302138, 1199196.733021379, 253857.3074249674], 
processed observation next is [1.0, 0.4782608695652174, 0.46998420221169057, 0.6366666666666666, 1.0, 1.0, 0.7429254221177144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33311020361704996, 0.33311020361704974, 0.37889150361935436], 
reward next is 0.6211, 
noisyNet noise sample is [array([0.23374219], dtype=float32), 0.55181473]. 
=============================================
[2019-03-27 15:03:03,168] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:03:03,180] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4832
[2019-03-27 15:03:03,188] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 83.33333333333333, 1.0, 2.0, 0.4757443886555056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664768.2625797977, 664768.2625797983, 179882.6405718185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7155600.0000, 
sim time next is 7156200.0000, 
raw observation next is [26.1, 83.16666666666667, 1.0, 2.0, 0.4749271329788318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664114.457493421, 664114.457493421, 179823.7690709625], 
processed observation next is [1.0, 0.8260869565217391, 0.4360189573459717, 0.8316666666666667, 1.0, 1.0, 0.3673820879263034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18447623819261696, 0.18447623819261696, 0.2683936851805411], 
reward next is 0.7316, 
noisyNet noise sample is [array([0.12529828], dtype=float32), 1.0945904]. 
=============================================
[2019-03-27 15:03:10,588] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.380687e-38], sum to 1.0000
[2019-03-27 15:03:10,601] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6999
[2019-03-27 15:03:10,610] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 69.0, 1.0, 2.0, 0.3908386473102726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585495.8023767654, 585495.8023767654, 173276.329776291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7324800.0000, 
sim time next is 7325400.0000, 
raw observation next is [26.4, 69.5, 1.0, 2.0, 0.3891532766210155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 583257.8294127703, 583257.8294127703, 173082.2079879798], 
processed observation next is [1.0, 0.782608695652174, 0.45023696682464454, 0.695, 1.0, 1.0, 0.2640400923144765, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16201606372576954, 0.16201606372576954, 0.2583316537134027], 
reward next is 0.7417, 
noisyNet noise sample is [array([1.4704988], dtype=float32), 1.0546628]. 
=============================================
[2019-03-27 15:03:11,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:03:11,273] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7055
[2019-03-27 15:03:11,277] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.35, 74.33333333333334, 1.0, 2.0, 0.3809585040310223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575809.4812119086, 575809.4812119086, 172561.7600756117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7333800.0000, 
sim time next is 7334400.0000, 
raw observation next is [25.3, 74.66666666666667, 1.0, 2.0, 0.3851772154447255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 582164.1210056443, 582164.1210056443, 173128.0558478968], 
processed observation next is [1.0, 0.9130434782608695, 0.39810426540284366, 0.7466666666666667, 1.0, 1.0, 0.25924965716231985, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1617122558349012, 0.1617122558349012, 0.2584000833550698], 
reward next is 0.7416, 
noisyNet noise sample is [array([0.7677882], dtype=float32), 1.4146748]. 
=============================================
[2019-03-27 15:03:11,608] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:03:11,618] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0697
[2019-03-27 15:03:11,627] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.73333333333333, 95.0, 1.0, 2.0, 0.5076060487552021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732893.2444389308, 732893.2444389308, 187789.0648705919], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7613400.0000, 
sim time next is 7614000.0000, 
raw observation next is [23.7, 95.0, 1.0, 2.0, 0.4901608647690107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708669.5828691515, 708669.5828691515, 185082.6332041952], 
processed observation next is [1.0, 0.13043478260869565, 0.3222748815165877, 0.95, 1.0, 1.0, 0.38573598164941053, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19685266190809764, 0.19685266190809764, 0.2762427361256645], 
reward next is 0.7238, 
noisyNet noise sample is [array([-0.66168296], dtype=float32), -0.806573]. 
=============================================
[2019-03-27 15:03:11,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.70082]
 [74.73668]
 [75.06612]
 [75.05527]
 [74.56056]], R is [[74.73603821]
 [74.70839691]
 [74.67722321]
 [74.64492035]
 [74.59163666]].
[2019-03-27 15:03:16,542] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.1211375e-25 1.0000000e+00 8.3579266e-31 2.8191637e-33 6.3764370e-19], sum to 1.0000
[2019-03-27 15:03:16,555] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9095
[2019-03-27 15:03:16,566] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2027738.169441751 W.
[2019-03-27 15:03:16,574] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.73333333333333, 73.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.719739035374308, 6.9112, 168.9085130632426, 2027738.169441751, 1454147.839323367, 311346.7398159166], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7638600.0000, 
sim time next is 7639200.0000, 
raw observation next is [28.0, 72.0, 1.0, 2.0, 0.4032956355950993, 1.0, 1.0, 0.4032956355950993, 1.0, 1.0, 0.6789855563180751, 6.9112, 6.9112, 170.5573041426782, 1691406.081112815, 1691406.081112815, 351621.9524227174], 
processed observation next is [1.0, 0.43478260869565216, 0.5260663507109005, 0.72, 1.0, 1.0, 0.2810790790302401, 1.0, 0.5, 0.2810790790302401, 1.0, 0.5, 0.6085189711196037, 0.0, 0.0, 0.8375144448122397, 0.4698350225313375, 0.4698350225313375, 0.5248088842130111], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.14851081], dtype=float32), -0.028686244]. 
=============================================
[2019-03-27 15:03:20,953] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:03:20,962] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0513
[2019-03-27 15:03:20,966] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 90.0, 1.0, 2.0, 0.383760969435442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 575045.0046162286, 575045.004616228, 172340.1760897095], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7540200.0000, 
sim time next is 7540800.0000, 
raw observation next is [23.43333333333333, 90.0, 1.0, 2.0, 0.3853932460323327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576839.8275490114, 576839.8275490114, 172480.00652421], 
processed observation next is [0.0, 0.2608695652173913, 0.30963665086887826, 0.9, 1.0, 1.0, 0.2595099349787141, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16023328543028095, 0.16023328543028095, 0.2574328455585224], 
reward next is 0.7426, 
noisyNet noise sample is [array([0.00125779], dtype=float32), 0.87019336]. 
=============================================
[2019-03-27 15:03:27,638] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 15:03:27,640] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:03:27,641] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:03:27,643] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:03:27,643] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:03:27,645] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:03:27,644] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:03:27,648] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:03:27,646] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:03:27,649] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:03:27,653] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:03:27,679] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run77
[2019-03-27 15:03:27,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run77
[2019-03-27 15:03:27,724] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run77
[2019-03-27 15:03:27,741] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run77
[2019-03-27 15:03:27,762] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run77
[2019-03-27 15:04:38,712] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06178383], dtype=float32), 0.08385213]
[2019-03-27 15:04:38,714] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.83333333333334, 71.66666666666667, 1.0, 2.0, 0.8318563575599729, 1.0, 2.0, 0.7365182182942489, 1.0, 2.0, 1.03, 7.005108131211306, 6.9112, 170.5573041426782, 3090764.177124445, 3023493.96279924, 566212.6663446366]
[2019-03-27 15:04:38,714] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:04:38,718] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.1070677e-09 9.7597539e-03 2.6372629e-15 3.1708700e-09 9.9024022e-01], sampled 0.7240640196361825
[2019-03-27 15:04:39,990] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06178383], dtype=float32), 0.08385213]
[2019-03-27 15:04:39,992] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.16666666666667, 83.16666666666667, 1.0, 2.0, 0.5815724154656606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 812700.6978889741, 812700.6978889747, 197363.1852121624]
[2019-03-27 15:04:39,993] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:04:39,997] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23120490030236096
[2019-03-27 15:04:51,606] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06178383], dtype=float32), 0.08385213]
[2019-03-27 15:04:51,608] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.0, 82.0, 1.0, 2.0, 0.6028915093014207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842504.23328884, 842504.23328884, 201286.199577556]
[2019-03-27 15:04:51,608] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:04:51,610] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.40812e-38 1.00000e+00 0.00000e+00 0.00000e+00 5.67697e-34], sampled 0.004326137852104339
[2019-03-27 15:05:27,676] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06178383], dtype=float32), 0.08385213]
[2019-03-27 15:05:27,677] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.066327315, 71.534062765, 1.0, 2.0, 0.4318710595912179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 628288.5272968095, 628288.5272968088, 176791.1345398859]
[2019-03-27 15:05:27,681] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:05:27,685] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3307358e-38], sampled 0.22577964100684944
[2019-03-27 15:05:36,196] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8248.4209 2927930797.7601 1362.0000
[2019-03-27 15:05:36,386] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.7448 2779441479.1017 935.0000
[2019-03-27 15:05:36,483] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.4138 3007513746.7599 1763.0000
[2019-03-27 15:05:36,487] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.0737 3164236443.5710 1820.0000
[2019-03-27 15:05:36,701] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.8035 2842252071.6449 1148.0000
[2019-03-27 15:05:37,718] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1900000, evaluation results [1900000.0, 7881.073731544775, 3164236443.570966, 1820.0, 8248.420923188387, 2927930797.760129, 1362.0, 8659.74477554324, 2779441479.101747, 935.0, 7999.413824417624, 3007513746.759889, 1763.0, 8494.803485645733, 2842252071.644905, 1148.0]
[2019-03-27 15:05:43,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6926567e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1368795e-32], sum to 1.0000
[2019-03-27 15:05:43,264] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2855
[2019-03-27 15:05:43,271] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 84.33333333333334, 1.0, 2.0, 1.020149849039546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1431354.713377931, 1431354.71337793, 305976.8251228392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7634400.0000, 
sim time next is 7635000.0000, 
raw observation next is [26.13333333333333, 82.66666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.069194164158114, 6.9112, 168.9119911385719, 1569688.929202491, 1457603.072006361, 311954.6087144644], 
processed observation next is [1.0, 0.34782608695652173, 0.43759873617693507, 0.8266666666666665, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.015799416415811417, 0.0, 0.8294352047345253, 0.4360247025562475, 0.4048897422239892, 0.4656038936036782], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6966702], dtype=float32), -4.0893154]. 
=============================================
[2019-03-27 15:05:43,295] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[65.58796]
 [66.65585]
 [67.88783]
 [69.33277]
 [69.17877]], R is [[63.81542206]
 [63.72058868]
 [63.66653442]
 [63.66853333]
 [63.74329376]].
[2019-03-27 15:05:44,141] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8434526e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5672742e-31], sum to 1.0000
[2019-03-27 15:05:44,151] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3478
[2019-03-27 15:05:44,156] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.28333333333333, 89.16666666666667, 1.0, 2.0, 0.7334498719807727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1025039.654350252, 1025039.654350252, 228229.3575811335], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7879800.0000, 
sim time next is 7880400.0000, 
raw observation next is [26.3, 89.0, 1.0, 2.0, 0.62097983022792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867791.8885017479, 867791.8885017479, 204711.8173570828], 
processed observation next is [1.0, 0.21739130434782608, 0.4454976303317536, 0.89, 1.0, 1.0, 0.5433491930456867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24105330236159664, 0.24105330236159664, 0.30554002590609375], 
reward next is 0.6945, 
noisyNet noise sample is [array([-0.3749815], dtype=float32), -1.2319171]. 
=============================================
[2019-03-27 15:05:44,334] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:05:44,334] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:05:44,421] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run10
[2019-03-27 15:05:44,622] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8681755e-14 1.0000000e+00 9.1809043e-21 7.8155887e-18 1.8024508e-08], sum to 1.0000
[2019-03-27 15:05:44,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4169
[2019-03-27 15:05:44,636] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1852614.013705402 W.
[2019-03-27 15:05:44,643] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 75.0, 1.0, 2.0, 0.4417005476040678, 1.0, 1.0, 0.4417005476040678, 1.0, 1.0, 0.7592486176644723, 6.911199999999999, 6.9112, 170.5573041426782, 1852614.013705402, 1852614.013705403, 375939.0342113749], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7812000.0000, 
sim time next is 7812600.0000, 
raw observation next is [29.1, 74.5, 1.0, 2.0, 0.457366142177987, 1.0, 2.0, 0.457366142177987, 1.0, 2.0, 0.7873253298234383, 6.911200000000001, 6.9112, 170.5573041426782, 1918378.663991994, 1918378.663991993, 385775.7256631735], 
processed observation next is [1.0, 0.43478260869565216, 0.5781990521327015, 0.745, 1.0, 1.0, 0.34622426768432163, 1.0, 1.0, 0.34622426768432163, 1.0, 1.0, 0.7406406461261441, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5328829622199983, 0.5328829622199981, 0.5757846651689157], 
reward next is 0.4242, 
noisyNet noise sample is [array([1.2018708], dtype=float32), -1.0144929]. 
=============================================
[2019-03-27 15:05:46,738] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0339871e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7389805e-30], sum to 1.0000
[2019-03-27 15:05:46,745] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5123
[2019-03-27 15:05:46,752] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.58333333333334, 59.66666666666666, 1.0, 2.0, 0.986055042668314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1378299.485932685, 1378299.485932685, 294702.6286454824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7650600.0000, 
sim time next is 7651200.0000, 
raw observation next is [30.66666666666667, 59.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.584305587778193, 6.9112, 168.9037514645026, 2641418.123513496, 1454524.119951271, 310281.303578973], 
processed observation next is [1.0, 0.5652173913043478, 0.6524486571879939, 0.5933333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.16731055877781928, 0.0, 0.829394744162706, 0.7337272565315267, 0.40403447776424195, 0.4631064232521985], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2451284], dtype=float32), -0.902701]. 
=============================================
[2019-03-27 15:05:47,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:05:47,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:05:47,159] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run10
[2019-03-27 15:05:48,934] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.290057e-34 1.000000e+00 0.000000e+00 0.000000e+00 2.901744e-29], sum to 1.0000
[2019-03-27 15:05:48,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7728
[2019-03-27 15:05:48,950] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 91.0, 1.0, 2.0, 0.6892488017322556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 963238.0245145586, 963238.0245145586, 218540.254209167], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7873200.0000, 
sim time next is 7873800.0000, 
raw observation next is [26.11666666666667, 90.83333333333334, 1.0, 2.0, 0.8381606666026209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1171459.915470557, 1171459.915470558, 253491.8299410449], 
processed observation next is [1.0, 0.13043478260869565, 0.43680884676145365, 0.9083333333333334, 1.0, 1.0, 0.8050128513284589, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3254055320751547, 0.325405532075155, 0.37834601483738045], 
reward next is 0.6217, 
noisyNet noise sample is [array([0.46876827], dtype=float32), 0.26203218]. 
=============================================
[2019-03-27 15:05:49,557] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:05:49,557] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:05:49,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run10
[2019-03-27 15:05:49,893] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9416234e-10 9.9813879e-01 1.4897769e-16 1.9238998e-12 1.8612294e-03], sum to 1.0000
[2019-03-27 15:05:49,902] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3473
[2019-03-27 15:05:49,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1943992.42521697 W.
[2019-03-27 15:05:49,920] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.21666666666667, 78.50000000000001, 1.0, 2.0, 0.4634672570826473, 1.0, 1.0, 0.4634672570826473, 1.0, 2.0, 0.7945019189170909, 6.911199999999999, 6.9112, 170.5573041426782, 1943992.42521697, 1943992.42521697, 389070.1163829853], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7895400.0000, 
sim time next is 7896000.0000, 
raw observation next is [28.33333333333334, 78.0, 1.0, 2.0, 0.7620354005427482, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.983594822115434, 6.9112, 168.912525316949, 1961941.828147405, 1910582.58080917, 399488.042603517], 
processed observation next is [1.0, 0.391304347826087, 0.5418641390205374, 0.78, 1.0, 1.0, 0.7132956633045159, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007239482211543358, 0.0, 0.8294378277949048, 0.544983841152057, 0.5307173835581027, 0.5962508098559955], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.77951205], dtype=float32), -0.7511424]. 
=============================================
[2019-03-27 15:05:49,931] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[49.861652]
 [50.791416]
 [51.26522 ]
 [52.09355 ]
 [53.76134 ]], R is [[49.59611511]
 [49.51945496]
 [49.09869766]
 [48.66212082]
 [48.60463333]].
[2019-03-27 15:05:51,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:05:51,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:05:52,006] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run10
[2019-03-27 15:05:52,928] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:05:52,928] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:05:53,010] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run10
[2019-03-27 15:05:54,171] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:05:54,174] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:05:54,259] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run10
[2019-03-27 15:05:54,624] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:05:54,624] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:05:54,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run10
[2019-03-27 15:05:55,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:05:55,059] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:05:55,117] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-03-27 15:05:57,978] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:05:57,979] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:05:58,056] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run10
[2019-03-27 15:06:00,984] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:06:00,985] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:01,071] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run10
[2019-03-27 15:06:02,460] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:06:02,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:02,548] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run10
[2019-03-27 15:06:03,300] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:06:03,301] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:03,366] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run10
[2019-03-27 15:06:03,386] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:06:03,389] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:03,447] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run10
[2019-03-27 15:06:03,961] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:06:03,962] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:04,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run10
[2019-03-27 15:06:05,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:06:05,338] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:05,436] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run10
[2019-03-27 15:06:05,734] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:06:05,735] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:05,790] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run10
[2019-03-27 15:06:31,351] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:06:31,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5650
[2019-03-27 15:06:31,365] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.25, 90.5, 1.0, 2.0, 0.2269942232922713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 377042.6333263163, 377042.633326317, 158453.9648819507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 693000.0000, 
sim time next is 693600.0000, 
raw observation next is [18.2, 90.66666666666667, 1.0, 2.0, 0.2260716568205668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 375605.6039571176, 375605.603957117, 158354.8183598971], 
processed observation next is [1.0, 0.0, 0.06161137440758297, 0.9066666666666667, 1.0, 1.0, 0.06755621303682745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10433488998808822, 0.10433488998808806, 0.23635047516402552], 
reward next is 0.7636, 
noisyNet noise sample is [array([-0.33589843], dtype=float32), -0.6586763]. 
=============================================
[2019-03-27 15:06:33,016] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:06:33,023] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4307
[2019-03-27 15:06:33,031] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 75.0, 1.0, 2.0, 0.2451580971734862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 405215.5084417796, 405215.5084417796, 160358.747180089], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 633000.0000, 
sim time next is 633600.0000, 
raw observation next is [21.0, 74.0, 1.0, 2.0, 0.2443662082116722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403511.8456770882, 403511.8456770882, 160304.7886285869], 
processed observation next is [1.0, 0.34782608695652173, 0.19431279620853087, 0.74, 1.0, 1.0, 0.08959784121888217, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11208662379919117, 0.11208662379919117, 0.23926087855012967], 
reward next is 0.7607, 
noisyNet noise sample is [array([0.71400833], dtype=float32), -0.08951338]. 
=============================================
[2019-03-27 15:06:37,946] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:06:37,958] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7584
[2019-03-27 15:06:37,966] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 70.5, 1.0, 2.0, 0.2499026127298319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 411276.6103205053, 411276.6103205059, 160891.6280815469], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [21.6, 71.0, 1.0, 2.0, 0.2496570161819618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 411226.8252311549, 411226.8252311542, 160859.4074592386], 
processed observation next is [1.0, 0.8260869565217391, 0.22274881516587688, 0.71, 1.0, 1.0, 0.095972308652966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1142296736753208, 0.11422967367532061, 0.24008866784960986], 
reward next is 0.7599, 
noisyNet noise sample is [array([0.27745765], dtype=float32), 0.094797865]. 
=============================================
[2019-03-27 15:06:37,978] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.00225 ]
 [81.037155]
 [80.83245 ]
 [80.89318 ]
 [80.95164 ]], R is [[81.00078583]
 [80.95064545]
 [80.90085602]
 [80.85123444]
 [80.80181122]].
[2019-03-27 15:06:41,079] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 15:06:41,081] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:06:41,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:41,082] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:06:41,082] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:41,082] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:06:41,083] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:41,084] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:06:41,085] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:06:41,088] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:41,088] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:06:41,121] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run78
[2019-03-27 15:06:41,145] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run78
[2019-03-27 15:06:41,146] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run78
[2019-03-27 15:06:41,187] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run78
[2019-03-27 15:06:41,206] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run78
[2019-03-27 15:06:42,743] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06267574], dtype=float32), 0.090208024]
[2019-03-27 15:06:42,745] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.91855497, 68.32736499, 1.0, 2.0, 0.7426239326014102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1067437.320441642, 1067437.320441642, 234330.9601813256]
[2019-03-27 15:06:42,749] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:06:42,755] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1618159e-34], sampled 0.12211198724857464
[2019-03-27 15:07:25,529] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06267574], dtype=float32), 0.090208024]
[2019-03-27 15:07:25,531] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.66666666666667, 84.66666666666667, 1.0, 2.0, 0.4763880511565401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672417.3032064809, 672417.3032064809, 180847.8978213254]
[2019-03-27 15:07:25,532] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:07:25,535] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.90272359162989
[2019-03-27 15:07:37,893] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06267574], dtype=float32), 0.090208024]
[2019-03-27 15:07:37,893] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.1, 73.0, 1.0, 2.0, 0.8092852620896226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1131080.550562035, 1131080.550562036, 246199.5374570639]
[2019-03-27 15:07:37,893] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:07:37,895] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.9521192e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7725095e-28], sampled 0.325772353801525
[2019-03-27 15:08:10,312] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06267574], dtype=float32), 0.090208024]
[2019-03-27 15:08:10,313] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.15, 60.5, 1.0, 2.0, 0.5568798034928384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104276, 778182.1176022607, 778182.1176022607, 192989.1954007358]
[2019-03-27 15:08:10,315] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:08:10,317] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.848622e-35], sampled 0.5662957617540204
[2019-03-27 15:08:27,353] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06267574], dtype=float32), 0.090208024]
[2019-03-27 15:08:27,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.73333333333333, 89.66666666666666, 1.0, 2.0, 0.53079302813172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741715.8109090619, 741715.8109090626, 188561.3232133546]
[2019-03-27 15:08:27,357] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:08:27,361] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.531767e-38 1.000000e+00 0.000000e+00 0.000000e+00 3.893276e-34], sampled 0.5900234700462671
[2019-03-27 15:08:38,722] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06267574], dtype=float32), 0.090208024]
[2019-03-27 15:08:38,722] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.5, 81.0, 1.0, 2.0, 0.6190397319102937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 865079.5826478878, 865079.5826478872, 204346.8896482849]
[2019-03-27 15:08:38,724] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:08:38,726] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0439737e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3116953e-32], sampled 0.11232852200276833
[2019-03-27 15:08:51,175] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7872.9925 3163862687.1382 1831.0000
[2019-03-27 15:08:51,401] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.6866 3007570501.4621 1765.0000
[2019-03-27 15:08:51,498] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.5600 2842433455.2117 1146.0000
[2019-03-27 15:08:51,584] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.4046 2779536071.8896 937.0000
[2019-03-27 15:08:51,674] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.7000 2927579865.4062 1354.0000
[2019-03-27 15:08:52,692] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1925000, evaluation results [1925000.0, 7872.992473288962, 3163862687.1382113, 1831.0, 8251.700046094189, 2927579865.40624, 1354.0, 8659.404555797591, 2779536071.889597, 937.0, 8001.686625563573, 3007570501.4620595, 1765.0, 8495.560006126883, 2842433455.2116876, 1146.0]
[2019-03-27 15:09:08,556] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.9538286e-38], sum to 1.0000
[2019-03-27 15:09:08,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2380
[2019-03-27 15:09:08,574] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 60.5, 1.0, 2.0, 0.2436747180262885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402194.8189099152, 402194.8189099146, 160246.7239985138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 756600.0000, 
sim time next is 757200.0000, 
raw observation next is [22.9, 62.0, 1.0, 2.0, 0.2448529244925271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 403930.1500833399, 403930.1500833393, 160369.7330999935], 
processed observation next is [1.0, 0.782608695652174, 0.2843601895734597, 0.62, 1.0, 1.0, 0.09018424637653867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11220281946759442, 0.11220281946759425, 0.23935781059700523], 
reward next is 0.7606, 
noisyNet noise sample is [array([-0.67163545], dtype=float32), 0.77456087]. 
=============================================
[2019-03-27 15:09:10,783] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.8900877e-35], sum to 1.0000
[2019-03-27 15:09:10,794] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9805
[2019-03-27 15:09:10,800] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 96.0, 1.0, 2.0, 0.5819565319624869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900300.5488694628, 900300.5488694628, 207873.489656366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1002000.0000, 
sim time next is 1002600.0000, 
raw observation next is [21.6, 96.0, 1.0, 2.0, 0.5813509950007977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 899373.4979127507, 899373.4979127507, 207752.8146813911], 
processed observation next is [1.0, 0.6086956521739131, 0.22274881516587688, 0.96, 1.0, 1.0, 0.495603608434696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24982597164243076, 0.24982597164243076, 0.31007882788267327], 
reward next is 0.6899, 
noisyNet noise sample is [array([2.4309742], dtype=float32), 0.22514805]. 
=============================================
[2019-03-27 15:09:17,300] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.7085171e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2343467e-28], sum to 1.0000
[2019-03-27 15:09:17,309] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9858
[2019-03-27 15:09:17,313] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.71666666666667, 67.0, 1.0, 2.0, 0.7495945480135784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1159024.965604882, 1159024.965604882, 246130.2975684384], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1095000.0000, 
sim time next is 1095600.0000, 
raw observation next is [25.73333333333333, 67.0, 1.0, 2.0, 0.7365807830015667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1138469.343710732, 1138469.343710732, 242778.3475346128], 
processed observation next is [1.0, 0.6956521739130435, 0.41864139020537117, 0.67, 1.0, 1.0, 0.682627449399478, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3162414843640922, 0.3162414843640922, 0.36235574258897435], 
reward next is 0.6376, 
noisyNet noise sample is [array([0.19715096], dtype=float32), -0.02518594]. 
=============================================
[2019-03-27 15:09:17,629] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:09:17,639] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6154
[2019-03-27 15:09:17,645] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 85.5, 1.0, 2.0, 0.2791042447665226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 450229.5764741094, 450229.5764741094, 163615.5172352122], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 883800.0000, 
sim time next is 884400.0000, 
raw observation next is [21.2, 85.0, 1.0, 2.0, 0.2794616597946839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 450509.3060950939, 450509.3060950933, 163634.2051445661], 
processed observation next is [0.0, 0.21739130434782608, 0.20379146919431282, 0.85, 1.0, 1.0, 0.13188151782492036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12514147391530386, 0.1251414739153037, 0.24423015693218822], 
reward next is 0.7558, 
noisyNet noise sample is [array([-0.11890566], dtype=float32), 0.68513024]. 
=============================================
[2019-03-27 15:09:28,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.179676e-36], sum to 1.0000
[2019-03-27 15:09:28,483] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6478
[2019-03-27 15:09:28,494] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.654795584693358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008446.270262785, 1008446.270262785, 222905.818017141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 987000.0000, 
sim time next is 987600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.584112130816609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 899525.1845637473, 899525.1845637478, 207878.2152914389], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4989302780922999, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24986810682326313, 0.2498681068232633, 0.31026599297229684], 
reward next is 0.6897, 
noisyNet noise sample is [array([0.8477716], dtype=float32), -0.48134884]. 
=============================================
[2019-03-27 15:09:31,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.852173e-36], sum to 1.0000
[2019-03-27 15:09:31,916] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0134
[2019-03-27 15:09:31,921] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 72.0, 1.0, 2.0, 0.446095451224536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695558.2386561892, 695558.2386561886, 184143.6675486088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1083600.0000, 
sim time next is 1084200.0000, 
raw observation next is [24.76666666666667, 71.5, 1.0, 2.0, 0.4523465566367458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703886.8113209782, 703886.8113209788, 184994.2462644377], 
processed observation next is [1.0, 0.5652173913043478, 0.3728278041074251, 0.715, 1.0, 1.0, 0.3401765742611395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19552411425582727, 0.19552411425582744, 0.27611081532005627], 
reward next is 0.7239, 
noisyNet noise sample is [array([-0.10345597], dtype=float32), 1.8730752]. 
=============================================
[2019-03-27 15:09:34,297] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.338698e-35 1.000000e+00 0.000000e+00 0.000000e+00 3.581985e-30], sum to 1.0000
[2019-03-27 15:09:34,308] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8651
[2019-03-27 15:09:34,314] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 78.66666666666666, 1.0, 2.0, 0.4607359118258154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104309, 643790.2279077637, 643790.227907763, 177674.0386986545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1273200.0000, 
sim time next is 1273800.0000, 
raw observation next is [27.05, 79.33333333333334, 1.0, 2.0, 0.4675282410209751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653284.1258421162, 653284.1258421162, 178665.6062579248], 
processed observation next is [1.0, 0.7391304347826086, 0.4810426540284361, 0.7933333333333334, 1.0, 1.0, 0.35846776026623506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18146781273392115, 0.18146781273392115, 0.26666508396705196], 
reward next is 0.7333, 
noisyNet noise sample is [array([-0.26492018], dtype=float32), -0.6133732]. 
=============================================
[2019-03-27 15:09:36,938] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.19131495e-35], sum to 1.0000
[2019-03-27 15:09:36,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7712
[2019-03-27 15:09:36,963] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.08333333333334, 95.0, 1.0, 2.0, 0.4494937511500388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665620.2517701269, 665620.2517701269, 180780.0247312058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1325400.0000, 
sim time next is 1326000.0000, 
raw observation next is [23.06666666666667, 95.0, 1.0, 2.0, 0.650022440921547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 963176.7206946, 963176.7206946007, 217447.8527135873], 
processed observation next is [1.0, 0.34782608695652173, 0.29225908372827825, 0.95, 1.0, 1.0, 0.5783402902669241, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26754908908183334, 0.26754908908183356, 0.3245490339008765], 
reward next is 0.6755, 
noisyNet noise sample is [array([-0.87251437], dtype=float32), 0.11680195]. 
=============================================
[2019-03-27 15:09:36,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.05686 ]
 [72.96495 ]
 [72.683304]
 [72.5259  ]
 [72.53282 ]], R is [[72.54949951]
 [72.55418396]
 [72.56103516]
 [72.56230164]
 [72.56421661]].
[2019-03-27 15:09:40,131] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.2726175e-34], sum to 1.0000
[2019-03-27 15:09:40,141] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3429
[2019-03-27 15:09:40,148] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 94.33333333333334, 1.0, 2.0, 0.3779315394398082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 571128.6884031432, 571128.6884031432, 172144.6344543866], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1459200.0000, 
sim time next is 1459800.0000, 
raw observation next is [22.5, 94.5, 1.0, 2.0, 0.3758193335885419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 568786.0975120612, 568786.0975120618, 171964.8258838967], 
processed observation next is [0.0, 0.9130434782608695, 0.2654028436018958, 0.945, 1.0, 1.0, 0.24797510070908665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1579961381977948, 0.15799613819779496, 0.25666391922969656], 
reward next is 0.7433, 
noisyNet noise sample is [array([-0.55343795], dtype=float32), 0.31537125]. 
=============================================
[2019-03-27 15:09:40,769] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.894709e-37 1.000000e+00 0.000000e+00 0.000000e+00 7.862027e-33], sum to 1.0000
[2019-03-27 15:09:40,781] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2146
[2019-03-27 15:09:40,788] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 96.0, 1.0, 2.0, 0.3566330248468683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548512.7026637949, 548512.7026637949, 170497.4604648544], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1468800.0000, 
sim time next is 1469400.0000, 
raw observation next is [21.76666666666667, 96.0, 1.0, 2.0, 0.3552193625251076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546872.3453331747, 546872.3453331747, 170375.8955436849], 
processed observation next is [0.0, 0.0, 0.23064770932069528, 0.96, 1.0, 1.0, 0.22315585846398506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15190898481477075, 0.15190898481477075, 0.2542923814084849], 
reward next is 0.7457, 
noisyNet noise sample is [array([1.4211346], dtype=float32), 0.34619722]. 
=============================================
[2019-03-27 15:09:41,698] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:09:41,712] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3433
[2019-03-27 15:09:41,725] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.85, 92.5, 1.0, 2.0, 0.3702202436027319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575654.3116356224, 575654.311635623, 172952.9782742802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1222200.0000, 
sim time next is 1222800.0000, 
raw observation next is [21.83333333333334, 92.66666666666667, 1.0, 2.0, 0.354959699653924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551870.001663338, 551870.001663338, 170931.3663231286], 
processed observation next is [1.0, 0.13043478260869565, 0.23380726698262277, 0.9266666666666667, 1.0, 1.0, 0.2228430116312337, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15329722268426058, 0.15329722268426058, 0.2551214422733263], 
reward next is 0.7449, 
noisyNet noise sample is [array([-0.21408096], dtype=float32), 0.52580744]. 
=============================================
[2019-03-27 15:09:45,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:09:45,552] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9832
[2019-03-27 15:09:45,558] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 72.33333333333334, 1.0, 2.0, 0.4279153722120074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 618066.4080205425, 618066.4080205419, 175668.1792200477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1432200.0000, 
sim time next is 1432800.0000, 
raw observation next is [27.2, 71.0, 1.0, 2.0, 0.4270633068830873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617325.7241782949, 617325.7241782949, 175610.5870608765], 
processed observation next is [0.0, 0.6086956521739131, 0.4881516587677725, 0.71, 1.0, 1.0, 0.30971482756998475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17147936782730414, 0.17147936782730414, 0.26210535382220373], 
reward next is 0.7379, 
noisyNet noise sample is [array([0.29143813], dtype=float32), -0.32489502]. 
=============================================
[2019-03-27 15:09:59,217] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 15:09:59,218] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:09:59,218] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:09:59,221] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:09:59,222] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:09:59,223] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:09:59,223] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:09:59,224] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:09:59,225] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:09:59,226] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:09:59,228] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:09:59,261] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run79
[2019-03-27 15:09:59,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run79
[2019-03-27 15:09:59,307] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run79
[2019-03-27 15:09:59,335] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run79
[2019-03-27 15:09:59,355] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run79
[2019-03-27 15:10:05,013] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0627135], dtype=float32), 0.091530025]
[2019-03-27 15:10:05,014] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.73333333333333, 78.66666666666667, 1.0, 2.0, 0.3030353406860182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481738.2384693777, 481738.2384693783, 165733.1128130777]
[2019-03-27 15:10:05,015] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:10:05,017] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8684457002554217
[2019-03-27 15:10:20,961] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0627135], dtype=float32), 0.091530025]
[2019-03-27 15:10:20,962] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.23333333333333, 43.33333333333334, 1.0, 2.0, 0.2214732417225825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 368663.2467142401, 368663.2467142408, 157801.2752354397]
[2019-03-27 15:10:20,964] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:10:20,966] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18711123328228063
[2019-03-27 15:10:58,737] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0627135], dtype=float32), 0.091530025]
[2019-03-27 15:10:58,738] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.896633695, 64.24355161, 1.0, 2.0, 0.5278534800834851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737606.7398623616, 737606.739862361, 188073.0636031724]
[2019-03-27 15:10:58,739] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:10:58,743] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.4156430e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8737046e-32], sampled 0.49867635049258696
[2019-03-27 15:11:06,702] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0627135], dtype=float32), 0.091530025]
[2019-03-27 15:11:06,702] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.66666666666666, 79.83333333333334, 1.0, 2.0, 0.9625432155790615, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991354844248, 6.9112, 168.9123159588522, 2242575.561801734, 2175327.567227333, 451981.593533274]
[2019-03-27 15:11:06,703] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:11:06,706] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2324211e-15 1.0000000e+00 1.9113482e-21 2.1175137e-18 4.8309744e-08], sampled 0.5497822529356073
[2019-03-27 15:11:06,707] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2242575.561801734 W.
[2019-03-27 15:12:01,347] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.0627135], dtype=float32), 0.091530025]
[2019-03-27 15:12:01,349] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.26666666666667, 92.33333333333333, 1.0, 2.0, 0.3164641826768507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499628.4358157842, 499628.4358157848, 166981.1764986522]
[2019-03-27 15:12:01,349] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:12:01,354] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0629619355378861
[2019-03-27 15:12:08,861] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0018 2842326359.3784 1151.0000
[2019-03-27 15:12:08,914] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.0296 2927897984.0781 1360.0000
[2019-03-27 15:12:09,338] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.2513 3163158606.4185 1825.0000
[2019-03-27 15:12:09,365] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8009.6632 3007360546.9964 1744.0000
[2019-03-27 15:12:09,385] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.2697 2779519731.7979 930.0000
[2019-03-27 15:12:10,402] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1950000, evaluation results [1950000.0, 7889.2513249153135, 3163158606.418461, 1825.0, 8251.029567894515, 2927897984.078088, 1360.0, 8663.269677208707, 2779519731.7979455, 930.0, 8009.663208699871, 3007360546.996386, 1744.0, 8496.001804541933, 2842326359.378411, 1151.0]
[2019-03-27 15:12:13,953] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.810480e-34 1.000000e+00 0.000000e+00 0.000000e+00 1.561981e-27], sum to 1.0000
[2019-03-27 15:12:13,962] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4759
[2019-03-27 15:12:13,968] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 91.0, 1.0, 2.0, 0.8618038511760767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1204523.687623024, 1204523.687623024, 259643.2187026259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1677600.0000, 
sim time next is 1678200.0000, 
raw observation next is [25.31666666666667, 90.50000000000001, 1.0, 2.0, 0.9560773333030395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1336370.52086463, 1336370.52086463, 285825.2544364419], 
processed observation next is [1.0, 0.43478260869565216, 0.39889415481832563, 0.9050000000000001, 1.0, 1.0, 0.9470811244614933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3712140335735083, 0.3712140335735083, 0.42660485736782366], 
reward next is 0.5734, 
noisyNet noise sample is [array([0.32948816], dtype=float32), 0.47240847]. 
=============================================
[2019-03-27 15:12:28,475] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.033498e-38 1.000000e+00 0.000000e+00 0.000000e+00 9.834435e-35], sum to 1.0000
[2019-03-27 15:12:28,483] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8107
[2019-03-27 15:12:28,493] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4677171844808998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 657284.2766999168, 657284.2766999175, 179175.2532192061], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2073600.0000, 
sim time next is 2074200.0000, 
raw observation next is [24.48333333333333, 94.16666666666667, 1.0, 2.0, 0.4673698672905682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656852.309046034, 656852.309046034, 179131.1315331012], 
processed observation next is [0.0, 0.0, 0.3593996840442337, 0.9416666666666668, 1.0, 1.0, 0.3582769485428532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18245897473500944, 0.18245897473500944, 0.2673598978105988], 
reward next is 0.7326, 
noisyNet noise sample is [array([0.1311436], dtype=float32), 0.500409]. 
=============================================
[2019-03-27 15:12:35,886] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:12:35,899] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3152
[2019-03-27 15:12:35,911] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 87.0, 1.0, 2.0, 0.3202477053859197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504834.306482217, 504834.306482217, 167356.9322906791], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1792800.0000, 
sim time next is 1793400.0000, 
raw observation next is [21.95, 87.33333333333333, 1.0, 2.0, 0.3240488823107133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 510889.4066064937, 510889.4066064943, 167819.2239146206], 
processed observation next is [1.0, 0.782608695652174, 0.2393364928909953, 0.8733333333333333, 1.0, 1.0, 0.1856010630249558, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14191372405735936, 0.14191372405735952, 0.25047645360391135], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.76400584], dtype=float32), -0.75634795]. 
=============================================
[2019-03-27 15:12:37,834] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1108890e-21 1.0000000e+00 3.2891136e-27 1.6150970e-26 7.0817294e-14], sum to 1.0000
[2019-03-27 15:12:37,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2099
[2019-03-27 15:12:37,851] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 85.66666666666667, 1.0, 2.0, 0.8730856200375396, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510398, 1220301.021401105, 1220301.021401105, 262638.147935437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1858800.0000, 
sim time next is 1859400.0000, 
raw observation next is [26.2, 85.5, 1.0, 2.0, 0.8294444870855994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1159271.048460962, 1159271.048460961, 251263.558830178], 
processed observation next is [1.0, 0.5217391304347826, 0.44075829383886256, 0.855, 1.0, 1.0, 0.7945114302236137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32201973568360054, 0.3220197356836003, 0.3750202370599672], 
reward next is 0.6250, 
noisyNet noise sample is [array([0.67092955], dtype=float32), 0.42991647]. 
=============================================
[2019-03-27 15:12:50,698] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.450353e-36 1.000000e+00 0.000000e+00 0.000000e+00 1.722567e-30], sum to 1.0000
[2019-03-27 15:12:50,710] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9145
[2019-03-27 15:12:50,716] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.03333333333333, 80.33333333333333, 1.0, 2.0, 0.5604960811354082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783237.3559466929, 783237.3559466923, 193617.3095072526], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2140800.0000, 
sim time next is 2141400.0000, 
raw observation next is [28.86666666666667, 81.16666666666667, 1.0, 2.0, 0.5592848233189696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781544.1212854272, 781544.1212854272, 193406.0303552382], 
processed observation next is [0.0, 0.782608695652174, 0.567140600315956, 0.8116666666666668, 1.0, 1.0, 0.4690178594204452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21709558924595201, 0.21709558924595201, 0.2886657169481167], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.0148955], dtype=float32), 0.32427904]. 
=============================================
[2019-03-27 15:12:54,682] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9509109e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3369457e-27], sum to 1.0000
[2019-03-27 15:12:54,690] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7079
[2019-03-27 15:12:54,694] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.23333333333333, 81.50000000000001, 1.0, 2.0, 0.7557380808748202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1056204.237521417, 1056204.237521418, 233334.1507147295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2351400.0000, 
sim time next is 2352000.0000, 
raw observation next is [27.36666666666667, 81.0, 1.0, 2.0, 0.740376489022167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1034724.728228783, 1034724.728228783, 229799.892047152], 
processed observation next is [1.0, 0.21739130434782608, 0.49605055292259104, 0.81, 1.0, 1.0, 0.6872005891833337, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2874235356191064, 0.2874235356191064, 0.342984913503212], 
reward next is 0.6570, 
noisyNet noise sample is [array([0.4396075], dtype=float32), 0.14087476]. 
=============================================
[2019-03-27 15:12:54,710] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[64.88814 ]
 [65.09753 ]
 [64.90852 ]
 [64.792046]
 [65.152176]], R is [[64.78690338]
 [64.79077911]
 [64.83663177]
 [64.86865234]
 [64.90118408]].
[2019-03-27 15:12:58,234] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7503207e-16 9.9999964e-01 1.7685083e-23 3.4682626e-19 3.5528589e-07], sum to 1.0000
[2019-03-27 15:12:58,245] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1202
[2019-03-27 15:12:58,256] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2278426.470115617 W.
[2019-03-27 15:12:58,261] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.33333333333334, 64.0, 1.0, 2.0, 0.8146760366595585, 1.0, 2.0, 0.8146760366595585, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2278426.470115617, 2278426.470115618, 427082.6490184662], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2306400.0000, 
sim time next is 2307000.0000, 
raw observation next is [32.36666666666667, 64.0, 1.0, 2.0, 0.5490781678234629, 1.0, 2.0, 0.5490781678234629, 1.0, 1.0, 0.9535671945845061, 6.911200000000001, 6.9112, 170.5573041426782, 2303455.653030286, 2303455.653030285, 450743.919802667], 
processed observation next is [1.0, 0.6956521739130435, 0.7330173775671407, 0.64, 1.0, 1.0, 0.456720684124654, 1.0, 1.0, 0.456720684124654, 1.0, 0.5, 0.9433746275420807, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6398487925084128, 0.6398487925084125, 0.6727521191084582], 
reward next is 0.3272, 
noisyNet noise sample is [array([-0.16777496], dtype=float32), -0.54925823]. 
=============================================
[2019-03-27 15:12:58,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[67.92814 ]
 [66.962494]
 [67.021736]
 [66.297646]
 [65.60822 ]], R is [[67.44826508]
 [67.13634491]
 [66.84624481]
 [66.51039886]
 [65.84529877]].
[2019-03-27 15:13:17,066] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 15:13:17,068] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:13:17,069] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:13:17,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:13:17,073] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:13:17,071] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:13:17,076] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:13:17,075] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:13:17,076] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:13:17,079] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:13:17,077] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:13:17,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run80
[2019-03-27 15:13:17,136] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run80
[2019-03-27 15:13:17,158] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run80
[2019-03-27 15:13:17,176] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run80
[2019-03-27 15:13:17,199] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run80
[2019-03-27 15:13:42,033] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06242298], dtype=float32), 0.09040296]
[2019-03-27 15:13:42,035] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.3, 84.0, 1.0, 2.0, 0.2895328725365481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 467031.9574577213, 467031.957457722, 164753.6000866236]
[2019-03-27 15:13:42,038] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:13:42,039] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2504510727914019
[2019-03-27 15:15:03,747] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06242298], dtype=float32), 0.09040296]
[2019-03-27 15:15:03,749] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.4, 58.0, 1.0, 2.0, 0.9377742281491186, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.971152159556889, 6.9112, 168.9125512354329, 2207908.812328085, 2165376.788992208, 445918.3849091693]
[2019-03-27 15:15:03,751] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:15:03,754] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8639070e-19 1.0000000e+00 2.4986405e-26 6.9633265e-25 2.3097783e-13], sampled 0.30881244282452025
[2019-03-27 15:15:03,755] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2207908.812328085 W.
[2019-03-27 15:15:25,922] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.5819 2928024337.3167 1361.0000
[2019-03-27 15:15:26,245] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8024.7004 3006388174.1788 1701.0000
[2019-03-27 15:15:26,284] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.2702 2779486538.5962 929.0000
[2019-03-27 15:15:26,375] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.9853 3163036562.6865 1845.0000
[2019-03-27 15:15:26,640] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8502.7307 2841957724.7355 1136.0000
[2019-03-27 15:15:27,659] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1975000, evaluation results [1975000.0, 7890.9852501310215, 3163036562.686485, 1845.0, 8249.581949094167, 2928024337.3167186, 1361.0, 8663.270151812227, 2779486538.5961733, 929.0, 8024.700410319452, 3006388174.1788235, 1701.0, 8502.730731521278, 2841957724.7354794, 1136.0]
[2019-03-27 15:15:31,659] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:15:31,672] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3962
[2019-03-27 15:15:31,681] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4573981071325827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648801.6628119738, 648801.6628119744, 178438.1370797681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2617200.0000, 
sim time next is 2617800.0000, 
raw observation next is [25.16666666666667, 88.16666666666667, 1.0, 2.0, 0.461440988732043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652397.9849566373, 652397.9849566367, 178758.6166126817], 
processed observation next is [0.0, 0.30434782608695654, 0.39178515007898923, 0.8816666666666667, 1.0, 1.0, 0.3511337213639072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1812216624879548, 0.18122166248795463, 0.2668039053920623], 
reward next is 0.7332, 
noisyNet noise sample is [array([-0.215837], dtype=float32), -1.5728831]. 
=============================================
[2019-03-27 15:15:41,903] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.4762644e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3853715e-28], sum to 1.0000
[2019-03-27 15:15:41,907] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1225
[2019-03-27 15:15:41,916] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.96666666666667, 77.0, 1.0, 2.0, 0.5022919807211012, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701876.0610103646, 701876.061010364, 183958.6540507872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2568000.0000, 
sim time next is 2568600.0000, 
raw observation next is [28.9, 77.5, 1.0, 2.0, 0.4997078685267168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698263.9734801159, 698263.9734801159, 183553.2057705262], 
processed observation next is [1.0, 0.7391304347826086, 0.5687203791469194, 0.775, 1.0, 1.0, 0.3972383958153214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19396221485558773, 0.19396221485558773, 0.2739600086127257], 
reward next is 0.7260, 
noisyNet noise sample is [array([-0.63329], dtype=float32), 1.5197419]. 
=============================================
[2019-03-27 15:15:42,527] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:15:42,537] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3186
[2019-03-27 15:15:42,542] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3214245079321091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 507786.9718983622, 507786.9718983629, 167602.9986496499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2919600.0000, 
sim time next is 2920200.0000, 
raw observation next is [20.83333333333333, 95.0, 1.0, 2.0, 0.3199885840274344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 506358.0131187652, 506358.0131187658, 167510.158817047], 
processed observation next is [1.0, 0.8260869565217391, 0.1864139020537123, 0.95, 1.0, 1.0, 0.1807091373824511, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14065500364410144, 0.1406550036441016, 0.25001516241350297], 
reward next is 0.7500, 
noisyNet noise sample is [array([0.5253403], dtype=float32), 0.74135756]. 
=============================================
[2019-03-27 15:15:44,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1634394e-35], sum to 1.0000
[2019-03-27 15:15:44,108] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9418
[2019-03-27 15:15:44,114] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666667, 83.0, 1.0, 2.0, 0.5730577034332178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 890176.2850489374, 890176.2850489368, 206468.2355443447], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2805000.0000, 
sim time next is 2805600.0000, 
raw observation next is [23.33333333333334, 83.0, 1.0, 2.0, 0.5879318061340045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 909290.8951521824, 909290.8951521824, 209054.5582509945], 
processed observation next is [1.0, 0.4782608695652174, 0.3048973143759877, 0.83, 1.0, 1.0, 0.5035322965469934, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2525808042089396, 0.2525808042089396, 0.31202172873282763], 
reward next is 0.6880, 
noisyNet noise sample is [array([-2.1133301], dtype=float32), 0.8747176]. 
=============================================
[2019-03-27 15:15:46,501] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:15:46,508] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6110
[2019-03-27 15:15:46,515] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3955351687338005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590196.8710161274, 590196.8710161281, 173635.6708689709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2675400.0000, 
sim time next is 2676000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3950281865701634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 589440.5759568586, 589440.5759568592, 173566.3096669186], 
processed observation next is [0.0, 1.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.271118297072486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1637334933213496, 0.16373349332134976, 0.2590541935327143], 
reward next is 0.7409, 
noisyNet noise sample is [array([0.3967993], dtype=float32), -0.31314778]. 
=============================================
[2019-03-27 15:15:46,534] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.10289]
 [82.04459]
 [81.88035]
 [81.84306]
 [81.78682]], R is [[82.08321381]
 [82.00322723]
 [81.9239502 ]
 [81.84545135]
 [81.76776123]].
[2019-03-27 15:15:47,775] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:15:47,784] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1278
[2019-03-27 15:15:47,790] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.00000000000001, 1.0, 2.0, 0.4415283653261661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634208.0951602483, 634208.0951602483, 177162.4779117545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2686200.0000, 
sim time next is 2686800.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.442320514599603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635328.231742431, 635328.231742431, 177274.088662323], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 1.0, 1.0, 0.3280970055416903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17648006437289748, 0.17648006437289748, 0.2645881920333179], 
reward next is 0.7354, 
noisyNet noise sample is [array([1.3534464], dtype=float32), -1.141746]. 
=============================================
[2019-03-27 15:15:52,050] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:15:52,068] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9848
[2019-03-27 15:15:52,078] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.5290479725213855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810072.181546793, 810072.1815467936, 196816.9398456459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3054000.0000, 
sim time next is 3054600.0000, 
raw observation next is [22.0, 97.0, 1.0, 2.0, 0.5855272744419193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893171.9750684722, 893171.9750684722, 207240.9124076013], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.97, 1.0, 1.0, 0.500635270411951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24810332640790894, 0.24810332640790894, 0.30931479463821093], 
reward next is 0.6907, 
noisyNet noise sample is [array([-1.4655573], dtype=float32), 0.039289933]. 
=============================================
[2019-03-27 15:16:04,336] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:16:04,349] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6170
[2019-03-27 15:16:04,359] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3084721257190241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 491020.2512022325, 491020.2512022319, 166416.1234467001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2923200.0000, 
sim time next is 2923800.0000, 
raw observation next is [20.08333333333334, 99.5, 1.0, 2.0, 0.3084631333891708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490992.1554524192, 490992.1554524192, 166413.9369656206], 
processed observation next is [1.0, 0.8695652173913043, 0.15086887835703036, 0.995, 1.0, 1.0, 0.1668230522761094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13638670984789422, 0.13638670984789422, 0.24837901039644866], 
reward next is 0.7516, 
noisyNet noise sample is [array([-1.1751786], dtype=float32), -0.60065025]. 
=============================================
[2019-03-27 15:16:06,710] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5771071e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2482507e-33], sum to 1.0000
[2019-03-27 15:16:06,722] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6853
[2019-03-27 15:16:06,728] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 89.83333333333334, 1.0, 2.0, 0.705100973192129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1075383.100681327, 1075383.100681328, 233407.1101784439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2905800.0000, 
sim time next is 2906400.0000, 
raw observation next is [22.66666666666667, 90.66666666666667, 1.0, 2.0, 0.6911292910529765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1055892.195042201, 1055892.195042202, 230315.1323058094], 
processed observation next is [1.0, 0.6521739130434783, 0.27330173775671435, 0.9066666666666667, 1.0, 1.0, 0.6278666157264777, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2933033875117225, 0.2933033875117228, 0.3437539288146409], 
reward next is 0.6562, 
noisyNet noise sample is [array([0.36492074], dtype=float32), 1.1883156]. 
=============================================
[2019-03-27 15:16:09,015] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9348525e-36], sum to 1.0000
[2019-03-27 15:16:09,025] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7899
[2019-03-27 15:16:09,030] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333333, 92.33333333333334, 1.0, 2.0, 0.4914239763024455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686684.7820207494, 686684.7820207487, 182263.2816266926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3181200.0000, 
sim time next is 3181800.0000, 
raw observation next is [25.16666666666667, 93.16666666666667, 1.0, 2.0, 0.4920327516193084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687535.7214298253, 687535.7214298247, 182357.0076422961], 
processed observation next is [1.0, 0.8260869565217391, 0.39178515007898923, 0.9316666666666668, 1.0, 1.0, 0.3879912670112149, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19098214484161813, 0.19098214484161796, 0.27217463827208377], 
reward next is 0.7278, 
noisyNet noise sample is [array([0.380541], dtype=float32), 1.387511]. 
=============================================
[2019-03-27 15:16:34,231] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 15:16:34,232] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:16:34,233] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:16:34,234] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:16:34,234] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:16:34,235] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:16:34,235] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:16:34,238] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:16:34,239] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:16:34,240] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:16:34,239] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:16:35,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run81
[2019-03-27 15:16:35,853] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run81
[2019-03-27 15:16:35,874] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run81
[2019-03-27 15:16:35,892] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run81
[2019-03-27 15:16:35,931] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run81
[2019-03-27 15:16:42,990] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06320414], dtype=float32), 0.09162351]
[2019-03-27 15:16:42,991] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.7, 75.0, 1.0, 2.0, 0.4802929978796964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 772164.0973032619, 772164.0973032626, 191668.8623320326]
[2019-03-27 15:16:42,992] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:16:42,994] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.092776255034283
[2019-03-27 15:17:00,268] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06320414], dtype=float32), 0.09162351]
[2019-03-27 15:17:00,269] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.6, 96.0, 1.0, 2.0, 0.3458799522248043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535052.604252729, 535052.604252729, 169477.3413485671]
[2019-03-27 15:17:00,270] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:17:00,272] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9652404091921979
[2019-03-27 15:17:02,546] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06320414], dtype=float32), 0.09162351]
[2019-03-27 15:17:02,549] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.41129192, 85.47433529, 1.0, 2.0, 0.3325096707545757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521010.3895934674, 521010.3895934674, 168533.5830291353]
[2019-03-27 15:17:02,549] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:17:02,555] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0634259701381863
[2019-03-27 15:17:19,110] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06320414], dtype=float32), 0.09162351]
[2019-03-27 15:17:19,114] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.83333333333334, 1.0, 2.0, 0.4830160154093411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 674932.2969368307, 674932.2969368313, 180977.9562291006]
[2019-03-27 15:17:19,115] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:17:19,118] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2184850948427034
[2019-03-27 15:18:15,479] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06320414], dtype=float32), 0.09162351]
[2019-03-27 15:18:15,480] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.65, 56.66666666666667, 1.0, 2.0, 0.3493251485510231, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6040829112872127, 6.911199999999999, 6.9112, 168.9128830969035, 976382.8291950462, 976382.8291950468, 238817.4186955281]
[2019-03-27 15:18:15,480] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:18:15,482] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.6044328e-30 1.0000000e+00 2.2958094e-37 0.0000000e+00 2.5847420e-25], sampled 0.4230702901864889
[2019-03-27 15:18:20,315] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06320414], dtype=float32), 0.09162351]
[2019-03-27 15:18:20,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.63333333333333, 86.66666666666667, 1.0, 2.0, 0.8158653122606526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1140281.957457591, 1140281.957457591, 247840.6586379914]
[2019-03-27 15:18:20,317] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:18:20,320] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1282414e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6250384e-33], sampled 0.6125857541320612
[2019-03-27 15:18:21,645] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06320414], dtype=float32), 0.09162351]
[2019-03-27 15:18:21,646] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.8, 85.0, 1.0, 2.0, 0.5557951665828937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 776665.8952107, 776665.8952107007, 192798.7634729398]
[2019-03-27 15:18:21,647] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:18:21,650] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2859442e-36], sampled 0.8542314343099365
[2019-03-27 15:18:23,236] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06320414], dtype=float32), 0.09162351]
[2019-03-27 15:18:23,238] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.41148275, 80.27872855166667, 1.0, 2.0, 0.4969419064547196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694397.7087090629, 694397.7087090629, 183116.5505080148]
[2019-03-27 15:18:23,239] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:18:23,241] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.412282e-38], sampled 0.06663974256143324
[2019-03-27 15:18:24,070] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06320414], dtype=float32), 0.09162351]
[2019-03-27 15:18:24,072] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.1, 86.5, 1.0, 2.0, 0.5055602929471503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706444.5450412078, 706444.5450412084, 184470.9201185991]
[2019-03-27 15:18:24,074] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:18:24,078] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8554117e-38], sampled 0.6123073750333161
[2019-03-27 15:18:43,720] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8246.8195 2927975900.8869 1366.0000
[2019-03-27 15:18:44,820] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.7297 3163742530.2975 1837.0000
[2019-03-27 15:18:44,901] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8003.8147 3007470446.0262 1756.0000
[2019-03-27 15:18:45,029] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.2207 2779483746.5850 937.0000
[2019-03-27 15:18:45,063] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.3370 2842539947.0055 1154.0000
[2019-03-27 15:18:46,083] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 2000000, evaluation results [2000000.0, 7878.729679509579, 3163742530.2974696, 1837.0, 8246.819483862888, 2927975900.8869376, 1366.0, 8660.220712924973, 2779483746.5850425, 937.0, 8003.814694942403, 3007470446.026185, 1756.0, 8493.337048736214, 2842539947.0054674, 1154.0]
[2019-03-27 15:18:50,125] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.7460294e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2141561e-31], sum to 1.0000
[2019-03-27 15:18:50,135] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6502
[2019-03-27 15:18:50,142] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 89.33333333333334, 1.0, 2.0, 0.5373291143985411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750852.3889982343, 750852.3889982337, 189651.3903222882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3370200.0000, 
sim time next is 3370800.0000, 
raw observation next is [26.9, 89.66666666666667, 1.0, 2.0, 0.5371120742056108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750548.9944536419, 750548.9944536426, 189614.9975068734], 
processed observation next is [1.0, 0.0, 0.4739336492890995, 0.8966666666666667, 1.0, 1.0, 0.44230370386218165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20848583179267832, 0.20848583179267852, 0.2830074589654827], 
reward next is 0.7170, 
noisyNet noise sample is [array([1.0809599], dtype=float32), -0.29677963]. 
=============================================
[2019-03-27 15:18:50,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3538900e-14 9.9999988e-01 3.9145822e-21 3.1588793e-18 9.4048346e-08], sum to 1.0000
[2019-03-27 15:18:50,548] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9527
[2019-03-27 15:18:50,558] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2918248.509008014 W.
[2019-03-27 15:18:50,562] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.804875304189241, 6.9112, 168.9081035435659, 2918248.509008014, 2284262.705587852, 474049.283497955], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3508800.0000, 
sim time next is 3509400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.9372368636372326, 1.0, 1.0, 0.9372368636372326, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2621555.204485043, 2621555.204485043, 492217.2434753077], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.63, 1.0, 1.0, 0.9243817634183525, 1.0, 0.5, 0.9243817634183525, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.728209779023623, 0.728209779023623, 0.7346526022019518], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.61078435], dtype=float32), -1.5421175]. 
=============================================
[2019-03-27 15:18:52,381] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.4935705e-07 9.2953604e-01 1.7695317e-13 9.6640020e-09 7.0463330e-02], sum to 1.0000
[2019-03-27 15:18:52,391] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6212
[2019-03-27 15:18:52,402] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2169104.594500079 W.
[2019-03-27 15:18:52,410] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.66666666666666, 71.66666666666667, 1.0, 2.0, 0.5170819719636994, 1.0, 2.0, 0.5170819719636994, 1.0, 2.0, 0.8980003836797595, 6.911199999999999, 6.9112, 170.5573041426782, 2169104.594500079, 2169104.59450008, 427159.6106594765], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3404400.0000, 
sim time next is 3405000.0000, 
raw observation next is [30.83333333333334, 70.83333333333333, 1.0, 2.0, 0.9035471825012726, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005583814919717, 6.9112, 168.9123948050309, 2160001.588981396, 2093042.684903914, 435038.8073150304], 
processed observation next is [1.0, 0.391304347826087, 0.6603475513428123, 0.7083333333333333, 1.0, 1.0, 0.8837917861461116, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009438381491971714, 0.0, 0.8294371869216384, 0.6000004413837211, 0.5814007458066428, 0.6493116527090006], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.04073856], dtype=float32), -0.47326678]. 
=============================================
[2019-03-27 15:18:52,424] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[42.758324]
 [41.81645 ]
 [41.260674]
 [42.56731 ]
 [42.01248 ]], R is [[42.57786179]
 [42.514534  ]
 [42.0893898 ]
 [41.66849518]
 [41.62975693]].
[2019-03-27 15:18:55,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3926871e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3499053e-37], sum to 1.0000
[2019-03-27 15:18:55,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8484
[2019-03-27 15:18:55,462] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.8066290140806415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1127366.130540363, 1127366.130540363, 245540.9838231572], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3639600.0000, 
sim time next is 3640200.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.7734705947839404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1080999.454055136, 1080999.454055136, 237499.5673800751], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.7270730057637836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30027762612642667, 0.30027762612642667, 0.35447696623891806], 
reward next is 0.6455, 
noisyNet noise sample is [array([1.8540566], dtype=float32), -0.013066784]. 
=============================================
[2019-03-27 15:19:01,554] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:19:01,564] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3350
[2019-03-27 15:19:01,574] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 77.66666666666667, 1.0, 2.0, 0.5932357976949668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 829005.6974817791, 829005.6974817785, 199492.5158258075], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3917400.0000, 
sim time next is 3918000.0000, 
raw observation next is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.596136839469091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833061.2910813255, 833061.2910813255, 200028.4995750689], 
processed observation next is [0.0, 0.34782608695652173, 0.6524486571879939, 0.7633333333333334, 1.0, 1.0, 0.5134178788784229, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23140591418925707, 0.23140591418925707, 0.2985499993657745], 
reward next is 0.7015, 
noisyNet noise sample is [array([-0.8019459], dtype=float32), 0.23703237]. 
=============================================
[2019-03-27 15:19:01,600] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.91578 ]
 [74.89046 ]
 [74.718315]
 [74.70466 ]
 [74.70162 ]], R is [[74.88594818]
 [74.83934021]
 [74.79441833]
 [74.75016022]
 [74.70653534]].
[2019-03-27 15:19:02,303] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:19:02,312] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9801
[2019-03-27 15:19:02,318] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 63.33333333333333, 1.0, 2.0, 0.5572060011874463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778638.1123257191, 778638.1123257191, 193045.2620676647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [32.16666666666666, 65.16666666666667, 1.0, 2.0, 0.5640217650058872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788165.9753220618, 788165.9753220612, 194235.5952034292], 
processed observation next is [0.0, 0.782608695652174, 0.7235387045813582, 0.6516666666666667, 1.0, 1.0, 0.4747250180793821, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21893499314501716, 0.218934993145017, 0.28990387343795404], 
reward next is 0.7101, 
noisyNet noise sample is [array([-1.2485238], dtype=float32), 0.24796225]. 
=============================================
[2019-03-27 15:19:03,769] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7670591e-38], sum to 1.0000
[2019-03-27 15:19:03,778] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1454
[2019-03-27 15:19:03,785] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 62.00000000000001, 1.0, 2.0, 0.6138143539392955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 857774.4083206037, 857774.4083206031, 203348.1752246291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3950400.0000, 
sim time next is 3951000.0000, 
raw observation next is [34.0, 61.5, 1.0, 2.0, 0.6090855167490602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851163.4504296836, 851163.4504296836, 202451.2057352744], 
processed observation next is [0.0, 0.7391304347826086, 0.8104265402843602, 0.615, 1.0, 1.0, 0.5290186948783857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2364342917860232, 0.2364342917860232, 0.30216597870936474], 
reward next is 0.6978, 
noisyNet noise sample is [array([0.9442862], dtype=float32), 0.40754855]. 
=============================================
[2019-03-27 15:19:03,811] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.55192]
 [76.48656]
 [76.45385]
 [76.29352]
 [76.24057]], R is [[76.5506897 ]
 [76.48168182]
 [76.40979767]
 [76.34215546]
 [76.27619934]].
[2019-03-27 15:19:05,686] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:19:05,697] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2875
[2019-03-27 15:19:05,701] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5175745588374312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723238.3915178634, 723238.391517864, 186395.5096457264], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3807000.0000, 
sim time next is 3807600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5172292827914047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722755.7521721631, 722755.7521721631, 186339.6530535103], 
processed observation next is [0.0, 0.043478260869565216, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41834853348362006, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20076548671448974, 0.20076548671448974, 0.27811888515449296], 
reward next is 0.7219, 
noisyNet noise sample is [array([-1.5808922], dtype=float32), -0.12333403]. 
=============================================
[2019-03-27 15:19:12,306] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5752903e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8807825e-33], sum to 1.0000
[2019-03-27 15:19:12,315] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9006
[2019-03-27 15:19:12,327] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 78.16666666666667, 1.0, 2.0, 0.4854504547419786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678335.0957684433, 678335.0957684427, 181347.2992891828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3715800.0000, 
sim time next is 3716400.0000, 
raw observation next is [27.0, 77.33333333333334, 1.0, 2.0, 0.4822345228592408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673839.9483724871, 673839.9483724865, 180858.7175798029], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.7733333333333334, 1.0, 1.0, 0.37618617211956723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18717776343680198, 0.18717776343680181, 0.26993838444746704], 
reward next is 0.7301, 
noisyNet noise sample is [array([0.5941968], dtype=float32), -0.6641938]. 
=============================================
[2019-03-27 15:19:15,842] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1942426e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.4036083e-36], sum to 1.0000
[2019-03-27 15:19:15,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0441
[2019-03-27 15:19:15,858] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 70.66666666666667, 1.0, 2.0, 0.766064290039546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1070643.213574274, 1070643.213574274, 235748.9150830416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3743400.0000, 
sim time next is 3744000.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.8019746525445421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1120857.637960667, 1120857.637960668, 244393.6951459325], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7, 1.0, 1.0, 0.7614152440295688, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31134934387796304, 0.3113493438779633, 0.36476670917303355], 
reward next is 0.6352, 
noisyNet noise sample is [array([-0.35618407], dtype=float32), 1.164719]. 
=============================================
[2019-03-27 15:19:15,868] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[60.93336 ]
 [60.919754]
 [60.90119 ]
 [61.061863]
 [61.209267]], R is [[60.9630394 ]
 [61.00154495]
 [61.03960419]
 [61.06964111]
 [61.10537338]].
[2019-03-27 15:19:17,682] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0614684e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4555015e-32], sum to 1.0000
[2019-03-27 15:19:17,691] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7343
[2019-03-27 15:19:17,703] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.9442914179935956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1319886.351569978, 1319886.351569978, 282409.2252557108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3744600.0000, 
sim time next is 3745200.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.051227352278316, 6.9112, 168.9009588348533, 2972819.990297092, 1454718.769020088, 309141.0851418076], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.2140027352278316, 0.0, 0.8293810310726565, 0.8257833306380812, 0.40408854695002444, 0.4614046046892651], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.60808283], dtype=float32), 0.32579234]. 
=============================================
[2019-03-27 15:19:31,868] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0957374e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8580720e-35], sum to 1.0000
[2019-03-27 15:19:31,880] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4201
[2019-03-27 15:19:31,885] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5825627364649959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814085.1222146007, 814085.1222146007, 197542.2599234823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4141800.0000, 
sim time next is 4142400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5832246196768941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815010.4062550204, 815010.4062550204, 197662.1721715487], 
processed observation next is [1.0, 0.9565217391304348, 0.5734597156398105, 0.84, 1.0, 1.0, 0.497860987562523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22639177951528344, 0.22639177951528344, 0.29501816742022197], 
reward next is 0.7050, 
noisyNet noise sample is [array([-1.8658811], dtype=float32), -0.5363396]. 
=============================================
[2019-03-27 15:19:42,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5113167e-07 7.1778073e-04 1.2319939e-14 2.7086060e-06 9.9927932e-01], sum to 1.0000
[2019-03-27 15:19:42,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7440
[2019-03-27 15:19:42,212] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 51.66666666666667, 1.0, 2.0, 0.9808689396119655, 1.0, 2.0, 0.8110245093202455, 1.0, 2.0, 1.03, 7.005119888869075, 6.9112, 170.5573041426782, 3403852.456780057, 3336573.819966322, 624737.9869610744], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4285200.0000, 
sim time next is 4285800.0000, 
raw observation next is [38.0, 51.5, 1.0, 2.0, 0.9881256916019573, 1.0, 2.0, 0.8146528853152413, 1.0, 2.0, 1.03, 7.0051204616048, 6.9112, 170.5573041426782, 3419101.522946735, 3351822.475859103, 627800.0404850319], 
processed observation next is [1.0, 0.6086956521739131, 1.0, 0.515, 1.0, 1.0, 0.9856936043397075, 1.0, 1.0, 0.776690223271375, 1.0, 1.0, 1.0365853658536586, 0.009392046160480038, 0.0, 0.8375144448122397, 0.9497504230407597, 0.9310617988497508, 0.9370149857985551], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.45462662], dtype=float32), -0.81824917]. 
=============================================
[2019-03-27 15:19:43,512] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.6537905e-22 1.0000000e+00 3.3575667e-29 7.7313855e-30 1.4920068e-18], sum to 1.0000
[2019-03-27 15:19:43,521] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3571
[2019-03-27 15:19:43,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2242838.174362237 W.
[2019-03-27 15:19:43,537] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 71.0, 1.0, 2.0, 0.9627308472156908, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005991321538847, 6.9112, 168.9123931407056, 2242838.174362237, 2175590.172687831, 452087.4207292175], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4107600.0000, 
sim time next is 4108200.0000, 
raw observation next is [34.0, 70.33333333333334, 1.0, 2.0, 0.584414957705121, 1.0, 1.0, 0.584414957705121, 1.0, 2.0, 1.014935512553959, 6.9112, 6.9112, 170.5573041426782, 2451843.59685103, 2451843.59685103, 478430.5538138907], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.7033333333333335, 1.0, 1.0, 0.49929512976520596, 1.0, 0.5, 0.49929512976520596, 1.0, 1.0, 1.01821403969995, 0.0, 0.0, 0.8375144448122397, 0.6810676657919528, 0.6810676657919528, 0.7140754534535683], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.07736549], dtype=float32), -1.1133108]. 
=============================================
[2019-03-27 15:19:52,509] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 15:19:52,512] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:19:52,513] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:19:52,514] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:19:52,516] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:19:52,517] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:19:52,518] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:19:52,516] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:19:52,519] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:19:52,524] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:19:52,521] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:19:52,550] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run82
[2019-03-27 15:19:52,575] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run82
[2019-03-27 15:19:52,576] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run82
[2019-03-27 15:19:52,618] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run82
[2019-03-27 15:19:52,619] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run82
[2019-03-27 15:19:58,922] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:19:58,924] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.42481046, 83.46623780833335, 1.0, 2.0, 0.3173009566068277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499421.3417027149, 499421.3417027156, 166931.0575795554]
[2019-03-27 15:19:58,925] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:19:58,928] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6246788723081375
[2019-03-27 15:20:21,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:20:21,261] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.34802925, 92.49459926333333, 1.0, 2.0, 0.7659967398392901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1100675.619721115, 1100675.619721114, 239873.7145719393]
[2019-03-27 15:20:21,262] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:20:21,264] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7256125928471293
[2019-03-27 15:20:23,476] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:20:23,476] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.1645836, 79.38098416, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.998392766013713, 6.9112, 168.9122322160612, 1515654.573912271, 1453797.290002301, 311354.890166484]
[2019-03-27 15:20:23,478] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:20:23,482] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09985791751438344
[2019-03-27 15:20:37,804] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:20:37,805] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3488612530156051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537408.1714995031, 537408.1714995025, 169603.7349907542]
[2019-03-27 15:20:37,807] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:20:37,809] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4644639950176067
[2019-03-27 15:20:42,969] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:20:42,971] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 79.0, 1.0, 2.0, 0.727935036171031, 1.0, 2.0, 0.6845575575997781, 1.0, 2.0, 1.03, 7.000837549874296, 6.9112, 178.6582176852504, 2872313.231839863, 2805052.395267362, 532399.2034176152]
[2019-03-27 15:20:42,974] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:20:42,975] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1543818e-10 9.9997914e-01 1.1155233e-15 9.3074409e-11 2.0855161e-05], sampled 0.3996100844473103
[2019-03-27 15:20:42,977] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2872313.231839863 W.
[2019-03-27 15:20:45,314] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:20:45,315] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.76666666666667, 69.66666666666667, 1.0, 2.0, 0.564388770310292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 788679.0202103799, 788679.0202103805, 194298.721359473]
[2019-03-27 15:20:45,316] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:20:45,318] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.49829634688915503
[2019-03-27 15:20:56,081] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:20:56,082] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.14438556, 79.21896086, 1.0, 2.0, 0.6684895060447523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 934213.7624332526, 934213.7624332533, 214200.5467221189]
[2019-03-27 15:20:56,082] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:20:56,085] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3441494439226366
[2019-03-27 15:21:07,310] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:21:07,312] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.8, 44.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.262230136702647, 6.9112, 168.9108822290216, 1702955.288485556, 1453925.486151999, 311354.6882441633]
[2019-03-27 15:21:07,314] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:21:07,320] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1867879e-28 1.0000000e+00 8.0171453e-35 7.3296542e-37 6.9341650e-26], sampled 0.5197111668459395
[2019-03-27 15:21:07,320] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1702955.288485556 W.
[2019-03-27 15:21:17,023] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:21:17,025] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.15431087, 69.08554874, 1.0, 2.0, 0.8322629863710496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1163212.474801313, 1163212.474801313, 251984.1863751029]
[2019-03-27 15:21:17,026] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:21:17,029] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5039872220753675
[2019-03-27 15:21:24,120] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06817438], dtype=float32), 0.09019805]
[2019-03-27 15:21:24,121] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.93333333333333, 92.0, 1.0, 2.0, 0.63564957585903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 888300.7831047346, 888300.7831047346, 207575.0359223495]
[2019-03-27 15:21:24,122] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:21:24,125] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7363465420571417
[2019-03-27 15:22:00,434] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0627 2927403560.4135 1343.0000
[2019-03-27 15:22:00,629] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.2509 2842588591.8580 1143.0000
[2019-03-27 15:22:00,798] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.6532 2779505839.6768 936.0000
[2019-03-27 15:22:00,983] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.7524 3164203310.9712 1810.0000
[2019-03-27 15:22:01,081] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8940 3007766440.1283 1773.0000
[2019-03-27 15:22:02,098] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2025000, evaluation results [2025000.0, 7879.752385124175, 3164203310.9712315, 1810.0, 8254.062673661605, 2927403560.4135303, 1343.0, 8657.653201971758, 2779505839.6767807, 936.0, 7997.893977801062, 3007766440.128304, 1773.0, 8495.250948527148, 2842588591.8579917, 1143.0]
[2019-03-27 15:22:13,800] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:22:13,813] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8072
[2019-03-27 15:22:13,818] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 72.66666666666666, 1.0, 2.0, 0.529589629599696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740033.6286030613, 740033.6286030613, 188361.9485519146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4560000.0000, 
sim time next is 4560600.0000, 
raw observation next is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.5285019838757005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738513.2547740127, 738513.2547740127, 188182.1144252415], 
processed observation next is [0.0, 0.782608695652174, 0.581358609794629, 0.7333333333333334, 1.0, 1.0, 0.4319301010550608, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20514257077055909, 0.20514257077055909, 0.28086882750036046], 
reward next is 0.7191, 
noisyNet noise sample is [array([-1.1126686], dtype=float32), -0.49506566]. 
=============================================
[2019-03-27 15:22:18,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.6260671e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0374709e-36], sum to 1.0000
[2019-03-27 15:22:18,152] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6519
[2019-03-27 15:22:18,159] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 94.0, 1.0, 2.0, 0.4832657363366631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675281.3501446685, 675281.3501446685, 181015.265353408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4660200.0000, 
sim time next is 4660800.0000, 
raw observation next is [24.83333333333333, 94.0, 1.0, 2.0, 0.485225621398558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678020.8288486532, 678020.8288486532, 181313.2733322842], 
processed observation next is [1.0, 0.9565217391304348, 0.3759873617693521, 0.94, 1.0, 1.0, 0.37978990529946755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1883391191246259, 0.1883391191246259, 0.2706168258690809], 
reward next is 0.7294, 
noisyNet noise sample is [array([-0.41781995], dtype=float32), 0.068763584]. 
=============================================
[2019-03-27 15:22:18,186] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:22:18,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9135
[2019-03-27 15:22:18,206] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 84.0, 1.0, 2.0, 0.5419766475752494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757349.0692028189, 757349.0692028189, 190433.6967397708], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4486800.0000, 
sim time next is 4487400.0000, 
raw observation next is [27.5, 84.0, 1.0, 2.0, 0.5371457553079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750596.0763388888, 750596.0763388888, 189620.0435537461], 
processed observation next is [0.0, 0.9565217391304348, 0.5023696682464456, 0.84, 1.0, 1.0, 0.4423442835034939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2084989100941358, 0.2084989100941358, 0.2830149903787255], 
reward next is 0.7170, 
noisyNet noise sample is [array([1.2354335], dtype=float32), 0.49459162]. 
=============================================
[2019-03-27 15:22:24,571] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0504488e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6367979e-36], sum to 1.0000
[2019-03-27 15:22:24,587] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1227
[2019-03-27 15:22:24,595] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 72.0, 1.0, 2.0, 0.5308248171535388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741760.2475375492, 741760.2475375487, 188566.5813750938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4559400.0000, 
sim time next is 4560000.0000, 
raw observation next is [29.33333333333334, 72.66666666666666, 1.0, 2.0, 0.529589629599696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740033.6286030613, 740033.6286030613, 188361.9485519146], 
processed observation next is [0.0, 0.782608695652174, 0.5892575039494474, 0.7266666666666666, 1.0, 1.0, 0.43324051758999516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2055648968341837, 0.2055648968341837, 0.2811372366446486], 
reward next is 0.7189, 
noisyNet noise sample is [array([-1.0429705], dtype=float32), -0.007582935]. 
=============================================
[2019-03-27 15:22:24,608] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[77.19441]
 [77.16716]
 [77.15536]
 [77.11449]
 [76.95882]], R is [[77.16266632]
 [77.10959625]
 [77.05665588]
 [77.00396729]
 [76.95204926]].
[2019-03-27 15:22:25,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:22:25,706] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9213
[2019-03-27 15:22:25,712] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5179501717900411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104302, 723763.4371379042, 723763.4371379042, 186460.0544104053], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4642200.0000, 
sim time next is 4642800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5296884744035278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740171.7996700645, 740171.799670065, 188381.3108428722], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.43335960771509374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20560327768612902, 0.20560327768612918, 0.2811661355863764], 
reward next is 0.7188, 
noisyNet noise sample is [array([0.2010256], dtype=float32), 0.24607962]. 
=============================================
[2019-03-27 15:22:35,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3957942e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3971060e-32], sum to 1.0000
[2019-03-27 15:22:35,946] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0859
[2019-03-27 15:22:35,952] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5156445368819844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720540.5398331, 720540.5398331007, 186083.7356781453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4741200.0000, 
sim time next is 4741800.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.5152865481673516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720040.1315345133, 720040.131534514, 186026.0277832874], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.41600788935825495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2000111476484759, 0.2000111476484761, 0.27765078773624985], 
reward next is 0.7223, 
noisyNet noise sample is [array([-1.3037974], dtype=float32), 0.69627935]. 
=============================================
[2019-03-27 15:22:38,376] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6248473e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7919890e-34], sum to 1.0000
[2019-03-27 15:22:38,386] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4162
[2019-03-27 15:22:38,391] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5940333328374745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830120.6328028388, 830120.6328028388, 199632.858922759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4942200.0000, 
sim time next is 4942800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5994737717057516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 837726.269262195, 837726.2692621943, 200640.8238202819], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.5174382791635561, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23270174146172082, 0.23270174146172062, 0.29946391614967444], 
reward next is 0.7005, 
noisyNet noise sample is [array([-1.8292636], dtype=float32), -0.28408453]. 
=============================================
[2019-03-27 15:23:02,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2797138e-28 1.0000000e+00 2.2532970e-35 0.0000000e+00 6.9628145e-27], sum to 1.0000
[2019-03-27 15:23:02,783] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8250
[2019-03-27 15:23:02,791] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 87.83333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.989205733017505, 6.9112, 168.9122639831455, 1509132.551469104, 1453792.826604967, 311354.258822194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5284200.0000, 
sim time next is 5284800.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.9837427349584081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128929847002, 1375065.268637225, 1375065.268637225, 294015.5863604259], 
processed observation next is [1.0, 0.17391304347826086, 0.5545023696682465, 0.88, 1.0, 1.0, 0.9804129336848291, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294396332118822, 0.3819625746214514, 0.3819625746214514, 0.43882923337377006], 
reward next is 0.5612, 
noisyNet noise sample is [array([2.4404926], dtype=float32), 0.26641315]. 
=============================================
[2019-03-27 15:23:08,433] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 15:23:08,436] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:23:08,436] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:23:08,437] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:23:08,437] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:23:08,438] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:23:08,440] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:23:08,440] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:23:08,441] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:23:08,442] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:23:08,440] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:23:08,472] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run83
[2019-03-27 15:23:08,496] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run83
[2019-03-27 15:23:08,517] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run83
[2019-03-27 15:23:08,538] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run83
[2019-03-27 15:23:08,539] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run83
[2019-03-27 15:23:35,372] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06922126], dtype=float32), 0.09083246]
[2019-03-27 15:23:35,374] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.1, 96.0, 1.0, 2.0, 0.4087115546040583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 602338.6898745953, 602338.6898745946, 174535.2351880883]
[2019-03-27 15:23:35,375] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:23:35,380] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6972150075696129
[2019-03-27 15:23:37,315] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06922126], dtype=float32), 0.09083246]
[2019-03-27 15:23:37,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.4, 94.0, 1.0, 2.0, 0.4600946989538006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649294.4116061226, 649294.4116061226, 178406.6916801751]
[2019-03-27 15:23:37,317] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:23:37,319] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8747427050470006
[2019-03-27 15:23:51,719] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06922126], dtype=float32), 0.09083246]
[2019-03-27 15:23:51,720] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.6, 81.0, 1.0, 2.0, 0.5517090803431476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770953.9428384441, 770953.9428384441, 192094.2460881112]
[2019-03-27 15:23:51,722] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:23:51,725] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5137028260413613
[2019-03-27 15:24:15,118] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06922126], dtype=float32), 0.09083246]
[2019-03-27 15:24:15,120] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.66666666666667, 74.33333333333334, 1.0, 2.0, 0.803981098227416, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981776881411, 6.9112, 168.9123160024913, 2020645.134684862, 1953403.935003806, 408814.6356015941]
[2019-03-27 15:24:15,121] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:24:15,122] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.8678887e-23 1.0000000e+00 9.8371235e-30 2.2288515e-31 2.5965482e-20], sampled 0.1837854137706827
[2019-03-27 15:24:15,123] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2020645.134684862 W.
[2019-03-27 15:24:17,169] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06922126], dtype=float32), 0.09083246]
[2019-03-27 15:24:17,169] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.0, 56.33333333333333, 1.0, 2.0, 0.8996846911822716, 1.0, 2.0, 0.7704323851053986, 1.0, 2.0, 1.03, 7.005113482393471, 6.9112, 170.5573041426782, 3233267.619656062, 3165993.572061401, 591822.9568192473]
[2019-03-27 15:24:17,170] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:24:17,173] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.7536704e-08 3.1653080e-08 4.6413553e-14 1.1590297e-04 9.9988401e-01], sampled 0.25066266138943605
[2019-03-27 15:24:26,124] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06922126], dtype=float32), 0.09083246]
[2019-03-27 15:24:26,127] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.04212191, 83.09061070000001, 1.0, 2.0, 0.5678504141972562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793518.1414833749, 793518.1414833749, 194910.080337397]
[2019-03-27 15:24:26,128] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:24:26,131] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7862445327224118
[2019-03-27 15:24:43,709] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06922126], dtype=float32), 0.09083246]
[2019-03-27 15:24:43,712] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.191602985, 96.15053431999999, 1.0, 2.0, 0.7142294446996623, 1.0, 2.0, 0.7142294446996623, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 1997226.81787235, 1997226.817872349, 380767.4818630956]
[2019-03-27 15:24:43,714] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:24:43,717] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.5029257e-20 1.0000000e+00 1.0904105e-25 2.9392045e-25 2.8127214e-16], sampled 0.7000981632454369
[2019-03-27 15:24:43,721] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1997226.81787235 W.
[2019-03-27 15:24:49,382] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06922126], dtype=float32), 0.09083246]
[2019-03-27 15:24:49,383] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.33296994, 85.20396728166668, 1.0, 2.0, 0.5308889449061581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741849.8892168357, 741849.8892168363, 188577.1098563323]
[2019-03-27 15:24:49,384] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:24:49,389] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6684121361503255
[2019-03-27 15:24:56,559] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06922126], dtype=float32), 0.09083246]
[2019-03-27 15:24:56,560] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.06666666666667, 85.0, 1.0, 2.0, 0.5151451390202629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719842.4652826025, 719842.465282603, 186003.2007643265]
[2019-03-27 15:24:56,561] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:24:56,564] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18042202341716984
[2019-03-27 15:25:16,859] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.2747 2779274427.0067 926.0000
[2019-03-27 15:25:17,307] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8022.3041 3006854641.9605 1708.0000
[2019-03-27 15:25:17,347] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.0139 3163069066.6000 1850.0000
[2019-03-27 15:25:17,426] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8502.7719 2841817422.4104 1140.0000
[2019-03-27 15:25:17,533] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8247.4301 2928197534.3134 1366.0000
[2019-03-27 15:25:18,551] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2050000, evaluation results [2050000.0, 7889.013885723023, 3163069066.6000166, 1850.0, 8247.430077393909, 2928197534.313371, 1366.0, 8664.274669050594, 2779274427.006671, 926.0, 8022.304109881037, 3006854641.9605355, 1708.0, 8502.771885808184, 2841817422.410404, 1140.0]
[2019-03-27 15:25:25,006] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:25:25,018] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0701
[2019-03-27 15:25:25,024] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.4, 60.66666666666667, 1.0, 2.0, 0.5477234903574468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765382.5040770748, 765382.5040770742, 191411.6194791924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5665200.0000, 
sim time next is 5665800.0000, 
raw observation next is [32.45, 60.33333333333333, 1.0, 2.0, 0.5475165141936236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765093.1738036061, 765093.1738036054, 191376.2337570608], 
processed observation next is [0.0, 0.5652173913043478, 0.7369668246445499, 0.6033333333333333, 1.0, 1.0, 0.4548391737272573, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2125258816121128, 0.2125258816121126, 0.2856361697866579], 
reward next is 0.7144, 
noisyNet noise sample is [array([-0.6303591], dtype=float32), 0.5655816]. 
=============================================
[2019-03-27 15:25:30,096] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.688282e-36 1.000000e+00 0.000000e+00 0.000000e+00 4.390788e-35], sum to 1.0000
[2019-03-27 15:25:30,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2230
[2019-03-27 15:25:30,115] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 94.5, 1.0, 2.0, 0.5438222850261066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759929.0545554649, 759929.0545554649, 190746.7301540707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5536200.0000, 
sim time next is 5536800.0000, 
raw observation next is [26.3, 95.0, 1.0, 2.0, 0.543770311091435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759856.4009861683, 759856.4009861677, 190737.9164334008], 
processed observation next is [1.0, 0.08695652173913043, 0.4454976303317536, 0.95, 1.0, 1.0, 0.4503256760137771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21107122249615787, 0.2110712224961577, 0.28468345736328476], 
reward next is 0.7153, 
noisyNet noise sample is [array([0.54572743], dtype=float32), 1.6216367]. 
=============================================
[2019-03-27 15:25:33,901] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8871140e-24 1.0000000e+00 1.1088770e-30 2.6817451e-33 3.2196303e-21], sum to 1.0000
[2019-03-27 15:25:33,914] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6792
[2019-03-27 15:25:33,920] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.86666666666667, 83.66666666666667, 1.0, 2.0, 1.00421400093909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1403698.707146698, 1403698.707146698, 300218.0830243564], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5379600.0000, 
sim time next is 5380200.0000, 
raw observation next is [30.05, 82.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.02237276053178, 6.9112, 168.9122278416244, 1532678.382387036, 1453808.940028786, 311355.970576055], 
processed observation next is [1.0, 0.2608695652173913, 0.6232227488151659, 0.825, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.011117276053177961, 0.0, 0.8294363670548762, 0.42574399510751, 0.4038358166746628, 0.46471040384485823], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5054285], dtype=float32), 0.4123299]. 
=============================================
[2019-03-27 15:25:35,328] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:25:35,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5663
[2019-03-27 15:25:35,344] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 88.0, 1.0, 2.0, 0.5652142775962699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 789833.0161352528, 789833.0161352522, 194444.7492981492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5601600.0000, 
sim time next is 5602200.0000, 
raw observation next is [28.01666666666667, 88.16666666666667, 1.0, 2.0, 0.5691164941061998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795288.0327493323, 795288.032749333, 195133.373759114], 
processed observation next is [1.0, 0.8695652173913043, 0.5268562401263824, 0.8816666666666667, 1.0, 1.0, 0.4808632459110841, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22091334243037006, 0.22091334243037025, 0.29124384143151344], 
reward next is 0.7088, 
noisyNet noise sample is [array([1.2226281], dtype=float32), 1.1219019]. 
=============================================
[2019-03-27 15:25:49,367] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0561611e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3844967e-30], sum to 1.0000
[2019-03-27 15:25:49,377] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0976
[2019-03-27 15:25:49,386] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.81666666666667, 65.66666666666667, 1.0, 2.0, 0.5311470644255317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742210.7044822134, 742210.7044822134, 188620.4143638385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5680200.0000, 
sim time next is 5680800.0000, 
raw observation next is [30.6, 67.0, 1.0, 2.0, 0.5343108440677539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746633.2392995686, 746633.239299568, 189146.6838414601], 
processed observation next is [0.0, 0.782608695652174, 0.6492890995260664, 0.67, 1.0, 1.0, 0.4389287277924745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20739812202765795, 0.20739812202765778, 0.2823084833454628], 
reward next is 0.7177, 
noisyNet noise sample is [array([0.9742089], dtype=float32), -1.0458479]. 
=============================================
[2019-03-27 15:25:50,064] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5498758e-10 1.1953628e-07 3.1667609e-18 2.8357640e-06 9.9999702e-01], sum to 1.0000
[2019-03-27 15:25:50,076] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2202
[2019-03-27 15:25:50,081] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.2, 63.66666666666667, 1.0, 2.0, 0.7191126483356772, 1.0, 2.0, 0.6801463636821012, 1.0, 2.0, 1.03, 7.005099239218907, 6.9112, 170.5573041426782, 2853932.072505943, 2786668.227876597, 527433.047636045], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5847600.0000, 
sim time next is 5848200.0000, 
raw observation next is [32.15, 64.0, 1.0, 2.0, 0.716423132467904, 1.0, 2.0, 0.6788016057482147, 1.0, 2.0, 1.03, 7.005099027140421, 6.9112, 170.5573041426782, 2848282.960145113, 2781019.267436214, 526565.8178000259], 
processed observation next is [1.0, 0.6956521739130435, 0.7227488151658767, 0.64, 1.0, 1.0, 0.658341123455306, 1.0, 1.0, 0.6130139828291743, 1.0, 1.0, 1.0365853658536586, 0.009389902714042098, 0.0, 0.8375144448122397, 0.7911897111514203, 0.772505352065615, 0.7859191310448148], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.10275869], dtype=float32), 0.7198518]. 
=============================================
[2019-03-27 15:25:51,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0847245e-29 1.0000000e+00 8.7750962e-36 1.8439626e-35 2.7160305e-24], sum to 1.0000
[2019-03-27 15:25:51,157] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1100
[2019-03-27 15:25:51,170] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.06666666666666, 86.66666666666667, 1.0, 2.0, 0.5626046578740379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786184.9720289028, 786184.9720289034, 193986.0249515066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5863200.0000, 
sim time next is 5863800.0000, 
raw observation next is [27.98333333333333, 86.83333333333333, 1.0, 2.0, 0.5614436689102426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 784562.0043596969, 784562.0043596963, 193782.7089093108], 
processed observation next is [1.0, 0.8695652173913043, 0.5252764612954185, 0.8683333333333333, 1.0, 1.0, 0.4716188782051115, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2179338900999158, 0.21793389009991565, 0.28922792374524003], 
reward next is 0.7108, 
noisyNet noise sample is [array([0.54760534], dtype=float32), 0.15883851]. 
=============================================
[2019-03-27 15:25:52,505] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2766183e-27 1.0000000e+00 5.7051057e-34 2.4302129e-35 3.2720866e-24], sum to 1.0000
[2019-03-27 15:25:52,517] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4222
[2019-03-27 15:25:52,520] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 93.66666666666667, 1.0, 2.0, 0.525196342282297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733892.4519447049, 733892.4519447049, 187638.0130278393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5881800.0000, 
sim time next is 5882400.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.5250008546552274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733619.1895011489, 733619.1895011495, 187605.9281109089], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.94, 1.0, 1.0, 0.4277118730785872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20378310819476358, 0.20378310819476375, 0.2800088479267297], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.7381918], dtype=float32), 0.4039705]. 
=============================================
[2019-03-27 15:25:53,112] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9641124e-21 1.0000000e+00 4.9216354e-27 2.9225985e-27 2.0974496e-16], sum to 1.0000
[2019-03-27 15:25:53,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3260
[2019-03-27 15:25:53,126] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.68333333333333, 95.16666666666667, 1.0, 2.0, 0.6947619233359846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 970946.2365854465, 970946.2365854458, 219717.5590062633], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5890200.0000, 
sim time next is 5890800.0000, 
raw observation next is [25.66666666666667, 95.33333333333334, 1.0, 2.0, 0.6529595923711611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 912501.3919981535, 912501.3919981542, 211018.2105547009], 
processed observation next is [1.0, 0.17391304347826086, 0.4154818325434442, 0.9533333333333335, 1.0, 1.0, 0.5818790269532061, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25347260888837597, 0.2534726088883762, 0.31495255306671777], 
reward next is 0.6850, 
noisyNet noise sample is [array([-0.76607746], dtype=float32), 1.0008125]. 
=============================================
[2019-03-27 15:25:59,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6660228e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6441832e-34], sum to 1.0000
[2019-03-27 15:25:59,025] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9395
[2019-03-27 15:25:59,029] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.55, 70.0, 1.0, 2.0, 0.5205017066922573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727330.073473646, 727330.073473646, 186870.5722752665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5734200.0000, 
sim time next is 5734800.0000, 
raw observation next is [29.7, 69.0, 1.0, 2.0, 0.519301015276218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725651.6980632185, 725651.6980632185, 186675.3680298512], 
processed observation next is [0.0, 0.391304347826087, 0.6066350710900474, 0.69, 1.0, 1.0, 0.42084459671833485, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2015699161286718, 0.2015699161286718, 0.27861995228336], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.23202279], dtype=float32), 0.5245302]. 
=============================================
[2019-03-27 15:26:00,331] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4831494e-17 1.0000000e+00 4.2848968e-24 5.2803189e-22 1.4298067e-13], sum to 1.0000
[2019-03-27 15:26:00,339] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1303
[2019-03-27 15:26:00,347] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.18333333333333, 87.16666666666667, 1.0, 2.0, 0.9108189792333158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1273072.091990388, 1273072.091990388, 272930.8928384309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5811000.0000, 
sim time next is 5811600.0000, 
raw observation next is [27.36666666666667, 86.33333333333334, 1.0, 2.0, 0.8800687012622707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1230066.84180666, 1230066.841806659, 264513.9030617427], 
processed observation next is [1.0, 0.2608695652173913, 0.49605055292259104, 0.8633333333333334, 1.0, 1.0, 0.8555044593521333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34168523383518334, 0.34168523383518307, 0.394796870241407], 
reward next is 0.6052, 
noisyNet noise sample is [array([0.41297275], dtype=float32), -0.57740104]. 
=============================================
[2019-03-27 15:26:02,326] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.1790086e-18 1.0000000e+00 5.4422165e-24 5.1728604e-21 2.8021355e-12], sum to 1.0000
[2019-03-27 15:26:02,336] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5785
[2019-03-27 15:26:02,344] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 95.0, 1.0, 2.0, 0.9543050645449079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1333891.751063515, 1333891.751063515, 285311.5487068324], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5886600.0000, 
sim time next is 5887200.0000, 
raw observation next is [25.83333333333334, 95.0, 1.0, 2.0, 0.9649197290024352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1348737.95474605, 1348737.95474605, 288418.6792180605], 
processed observation next is [1.0, 0.13043478260869565, 0.42338072669826254, 0.95, 1.0, 1.0, 0.957734613255946, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3746494318739028, 0.3746494318739028, 0.4304756406239709], 
reward next is 0.5695, 
noisyNet noise sample is [array([0.00447616], dtype=float32), 0.24680863]. 
=============================================
[2019-03-27 15:26:04,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7079960e-11 6.1477406e-11 4.4772420e-18 7.0247756e-06 9.9999297e-01], sum to 1.0000
[2019-03-27 15:26:04,124] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3695
[2019-03-27 15:26:04,130] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.85, 61.33333333333334, 1.0, 2.0, 0.7747988968972302, 1.0, 2.0, 0.7079894879628778, 1.0, 2.0, 1.03, 7.005103630720798, 6.9112, 170.5573041426782, 2970902.31211337, 2903635.321672584, 545994.0792050846], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5838600.0000, 
sim time next is 5839200.0000, 
raw observation next is [32.9, 61.0, 1.0, 2.0, 0.7350344520841936, 1.0, 2.0, 0.6881072655563594, 1.0, 2.0, 1.03, 7.005100494752551, 6.9112, 170.5573041426782, 2887375.067523498, 2820110.323504331, 532622.3644832625], 
processed observation next is [1.0, 0.6086956521739131, 0.7582938388625592, 0.61, 1.0, 1.0, 0.680764400101438, 1.0, 1.0, 0.6242256211522402, 1.0, 1.0, 1.0365853658536586, 0.009390049475255059, 0.0, 0.8375144448122397, 0.8020486298676384, 0.7833639787512031, 0.7949587529600933], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9674464], dtype=float32), -0.58953947]. 
=============================================
[2019-03-27 15:26:08,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1186841e-28 1.0000000e+00 3.2744781e-35 6.5772094e-36 3.4748783e-23], sum to 1.0000
[2019-03-27 15:26:08,215] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3275
[2019-03-27 15:26:08,221] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 82.83333333333334, 1.0, 2.0, 0.5585933127316969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780577.4499351569, 780577.4499351569, 193285.6528329755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5946600.0000, 
sim time next is 5947200.0000, 
raw observation next is [28.5, 83.0, 1.0, 2.0, 0.5555106039364517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776268.1032273677, 776268.1032273677, 192750.558847794], 
processed observation next is [1.0, 0.8695652173913043, 0.5497630331753555, 0.83, 1.0, 1.0, 0.4644706071523514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2156300286742688, 0.2156300286742688, 0.2876874012653642], 
reward next is 0.7123, 
noisyNet noise sample is [array([-1.909022], dtype=float32), 0.43540663]. 
=============================================
[2019-03-27 15:26:14,564] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9263375e-32 1.0000000e+00 9.9795768e-38 0.0000000e+00 2.8058471e-27], sum to 1.0000
[2019-03-27 15:26:14,577] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7930
[2019-03-27 15:26:14,591] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 65.5, 1.0, 2.0, 0.5559283677469988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776852.0980397784, 776852.0980397784, 192821.5390709247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6361800.0000, 
sim time next is 6362400.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.5368629694898716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750200.7778971322, 750200.7778971328, 189573.3209990672], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.65, 1.0, 1.0, 0.4420035776986404, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20838910497142563, 0.20838910497142576, 0.28294525522248837], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.61881083], dtype=float32), 2.380257]. 
=============================================
[2019-03-27 15:26:16,590] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9584151e-24 1.0000000e+00 6.2878261e-30 1.3235492e-27 3.0818592e-17], sum to 1.0000
[2019-03-27 15:26:16,600] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7840
[2019-03-27 15:26:16,604] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333334, 86.33333333333334, 1.0, 2.0, 0.5393763918031707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753714.2263053434, 753714.2263053434, 189995.6976512825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6037800.0000, 
sim time next is 6038400.0000, 
raw observation next is [27.46666666666667, 86.66666666666667, 1.0, 2.0, 0.540030127717527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754628.068830808, 754628.068830808, 190105.7250444031], 
processed observation next is [1.0, 0.9130434782608695, 0.500789889415482, 0.8666666666666667, 1.0, 1.0, 0.4458194309849723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20961890800855779, 0.20961890800855779, 0.2837398881259748], 
reward next is 0.7163, 
noisyNet noise sample is [array([-0.31334245], dtype=float32), 0.01734783]. 
=============================================
[2019-03-27 15:26:25,131] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 15:26:25,135] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:26:25,138] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:26:25,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:26:25,140] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:26:25,141] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:26:25,143] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:26:25,141] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:26:25,143] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:26:25,147] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:26:25,149] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:26:25,177] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run84
[2019-03-27 15:26:25,178] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run84
[2019-03-27 15:26:25,178] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run84
[2019-03-27 15:26:25,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run84
[2019-03-27 15:26:25,266] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run84
[2019-03-27 15:26:38,162] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06993415], dtype=float32), 0.09424525]
[2019-03-27 15:26:38,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.24069163333333, 56.03552087833333, 1.0, 2.0, 0.4035417884975329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654163.197324791, 654163.1973247904, 179736.65790234]
[2019-03-27 15:26:38,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:26:38,164] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8075937543413372
[2019-03-27 15:27:21,964] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06993415], dtype=float32), 0.09424525]
[2019-03-27 15:27:21,965] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.45, 80.0, 1.0, 2.0, 0.5887987974335565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 822802.8971210233, 822802.897121024, 198671.6088026537]
[2019-03-27 15:27:21,965] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:27:21,967] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5137190e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0775842e-36], sampled 0.3042259209641388
[2019-03-27 15:27:22,900] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06993415], dtype=float32), 0.09424525]
[2019-03-27 15:27:22,902] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.93419232, 60.53915045, 1.0, 2.0, 0.3443641300720082, 1.0, 2.0, 0.3443641300720082, 1.0, 2.0, 0.5980466109770297, 6.9112, 6.9112, 184.5923449428631, 1444009.192291556, 1444009.192291556, 328315.0406892909]
[2019-03-27 15:27:22,903] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:27:22,907] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8633421e-10 7.5480944e-01 1.7415463e-15 1.8377602e-07 2.4519037e-01], sampled 0.050858393809469105
[2019-03-27 15:27:56,276] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06993415], dtype=float32), 0.09424525]
[2019-03-27 15:27:56,279] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.93333333333334, 90.0, 1.0, 2.0, 0.5440535270929651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760252.3043683963, 760252.3043683963, 190785.8175681132]
[2019-03-27 15:27:56,282] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:27:56,284] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6810844e-30 1.0000000e+00 3.9423214e-36 5.6009214e-37 1.4680467e-25], sampled 0.6673790088673465
[2019-03-27 15:27:58,691] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06993415], dtype=float32), 0.09424525]
[2019-03-27 15:27:58,692] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.45, 55.5, 1.0, 2.0, 0.5402883518491643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754989.0346719248, 754989.0346719254, 190150.0236700869]
[2019-03-27 15:27:58,694] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:27:58,697] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8487741e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5210399e-31], sampled 0.9357340201191997
[2019-03-27 15:28:01,151] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06993415], dtype=float32), 0.09424525]
[2019-03-27 15:28:01,153] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.86174411, 92.54341320500001, 1.0, 2.0, 1.025890942296793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9126178596043, 1434019.384442601, 1434019.384442602, 306912.1512505966]
[2019-03-27 15:28:01,154] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:28:01,157] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1726321e-25 1.0000000e+00 4.0624036e-31 2.2771336e-31 4.8486573e-21], sampled 0.07327374703604173
[2019-03-27 15:28:17,436] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06993415], dtype=float32), 0.09424525]
[2019-03-27 15:28:17,438] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.35, 72.0, 1.0, 2.0, 0.5427403888978976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 758416.6895103599, 758416.6895103592, 190562.2281419557]
[2019-03-27 15:28:17,440] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:28:17,442] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1124664e-30 1.0000000e+00 3.7072279e-36 3.4634459e-37 8.8034387e-26], sampled 0.4863304722041589
[2019-03-27 15:28:33,636] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7960.9750 3160310866.9787 1669.0000
[2019-03-27 15:28:33,929] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8557.1105 2839644399.0326 999.0000
[2019-03-27 15:28:34,044] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8707.9949 2778901490.5162 824.0000
[2019-03-27 15:28:34,077] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8104.0946 3003027864.9261 1501.0000
[2019-03-27 15:28:34,301] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8322.8500 2926563152.2006 1200.0000
[2019-03-27 15:28:35,321] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2075000, evaluation results [2075000.0, 7960.974957929137, 3160310866.9786677, 1669.0, 8322.850048668606, 2926563152.20062, 1200.0, 8707.994865422492, 2778901490.516212, 824.0, 8104.0946138604895, 3003027864.926125, 1501.0, 8557.11054397203, 2839644399.03262, 999.0]
[2019-03-27 15:28:36,316] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1699893e-25 1.0000000e+00 7.0058824e-31 5.7372430e-30 1.1183058e-18], sum to 1.0000
[2019-03-27 15:28:36,329] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2676
[2019-03-27 15:28:36,337] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 88.5, 1.0, 2.0, 0.5218063898653075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729153.8158078616, 729153.8158078616, 187083.4496020554], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6219000.0000, 
sim time next is 6219600.0000, 
raw observation next is [26.73333333333333, 88.66666666666666, 1.0, 2.0, 0.521727724133706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729043.8533712581, 729043.8533712574, 187070.6405458976], 
processed observation next is [1.0, 1.0, 0.4660347551342811, 0.8866666666666666, 1.0, 1.0, 0.4237683423297663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20251218149201614, 0.20251218149201594, 0.2792099112625337], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.3458396], dtype=float32), -0.26740292]. 
=============================================
[2019-03-27 15:28:37,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3207890e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3044642e-36], sum to 1.0000
[2019-03-27 15:28:37,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9744
[2019-03-27 15:28:37,576] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.96666666666667, 70.5, 1.0, 2.0, 0.5333704921153345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745318.7528925957, 745318.7528925957, 188990.0026327585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6342600.0000, 
sim time next is 6343200.0000, 
raw observation next is [30.1, 70.0, 1.0, 2.0, 0.5352173682689337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747900.4405693329, 747900.4405693322, 189298.1262683131], 
processed observation next is [0.0, 0.43478260869565216, 0.6255924170616115, 0.7, 1.0, 1.0, 0.4400209256252213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20775012238037024, 0.20775012238037005, 0.28253451681837777], 
reward next is 0.7175, 
noisyNet noise sample is [array([0.25634807], dtype=float32), 0.25866765]. 
=============================================
[2019-03-27 15:28:55,265] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.1700468e-10 1.3402738e-03 7.5920377e-17 7.0295201e-08 9.9865961e-01], sum to 1.0000
[2019-03-27 15:28:55,276] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2878
[2019-03-27 15:28:55,284] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.95, 68.5, 1.0, 2.0, 0.5422783804126883, 1.0, 2.0, 0.5422783804126883, 1.0, 2.0, 0.9323268709432335, 6.911200000000001, 6.9112, 170.5573041426782, 2274903.680803148, 2274903.680803147, 443742.3953999954], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6442200.0000, 
sim time next is 6442800.0000, 
raw observation next is [29.96666666666667, 68.33333333333333, 1.0, 2.0, 0.5449523415967239, 1.0, 2.0, 0.5449523415967239, 1.0, 2.0, 0.9369972200597444, 6.911200000000001, 6.9112, 170.5573041426782, 2286131.434213534, 2286131.434213533, 445747.3380704537], 
processed observation next is [1.0, 0.5652173913043478, 0.6192733017377569, 0.6833333333333332, 1.0, 1.0, 0.45174980915267937, 1.0, 1.0, 0.45174980915267937, 1.0, 1.0, 0.9231673415362736, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6350365095037594, 0.6350365095037592, 0.665294534433513], 
reward next is 0.3347, 
noisyNet noise sample is [array([0.8829152], dtype=float32), 0.43894225]. 
=============================================
[2019-03-27 15:28:59,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8203742e-29 1.0000000e+00 5.3270597e-35 9.2083582e-38 1.2388157e-25], sum to 1.0000
[2019-03-27 15:28:59,131] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6727
[2019-03-27 15:28:59,137] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 86.5, 1.0, 2.0, 0.5123973425287498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716001.5143104362, 716001.5143104362, 185561.2818871424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6643800.0000, 
sim time next is 6644400.0000, 
raw observation next is [26.66666666666667, 86.66666666666667, 1.0, 2.0, 0.5128216749064335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 716594.6577153507, 716594.65771535, 185629.3045243928], 
processed observation next is [1.0, 0.9130434782608695, 0.4628751974723541, 0.8666666666666667, 1.0, 1.0, 0.4130381625378716, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19905407158759741, 0.19905407158759722, 0.27705866346924296], 
reward next is 0.7229, 
noisyNet noise sample is [array([-1.8995813], dtype=float32), -0.5282191]. 
=============================================
[2019-03-27 15:29:10,805] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0192778e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3326614e-30], sum to 1.0000
[2019-03-27 15:29:10,816] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1711
[2019-03-27 15:29:10,824] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 61.0, 1.0, 2.0, 0.4787080597107752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668910.7701746545, 668910.7701746545, 180327.0397785197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6970800.0000, 
sim time next is 6971400.0000, 
raw observation next is [30.0, 60.5, 1.0, 2.0, 0.4765513060145896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665896.1396225395, 665896.1396225395, 180003.3291291465], 
processed observation next is [0.0, 0.6956521739130435, 0.6208530805687204, 0.605, 1.0, 1.0, 0.3693389229091441, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18497114989514987, 0.18497114989514987, 0.2686616852673828], 
reward next is 0.7313, 
noisyNet noise sample is [array([-1.1071317], dtype=float32), 0.89492285]. 
=============================================
[2019-03-27 15:29:11,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9635566e-11 9.9805224e-01 1.3321108e-17 9.2209027e-12 1.9477579e-03], sum to 1.0000
[2019-03-27 15:29:11,446] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4530
[2019-03-27 15:29:11,457] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1820239.047823352 W.
[2019-03-27 15:29:11,465] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.2, 72.0, 1.0, 2.0, 0.4339882536074183, 1.0, 1.0, 0.4339882536074183, 1.0, 2.0, 0.7425540944668603, 6.9112, 6.9112, 170.5573041426782, 1820239.047823352, 1820239.047823352, 370775.0481537223], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6692400.0000, 
sim time next is 6693000.0000, 
raw observation next is [29.25, 71.33333333333333, 1.0, 2.0, 0.7144480804580142, 1.0, 2.0, 0.7144480804580142, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1997863.599961916, 1997863.599961916, 380365.4986187465], 
processed observation next is [1.0, 0.4782608695652174, 0.5853080568720379, 0.7133333333333333, 1.0, 1.0, 0.655961542720499, 1.0, 1.0, 0.655961542720499, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5549621111005322, 0.5549621111005322, 0.5677096994309649], 
reward next is 0.4323, 
noisyNet noise sample is [array([0.89789194], dtype=float32), 0.21255611]. 
=============================================
[2019-03-27 15:29:11,482] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[63.774162]
 [63.6119  ]
 [63.13845 ]
 [63.330574]
 [63.15635 ]], R is [[63.09334183]
 [62.90901566]
 [62.37703705]
 [61.84552383]
 [61.68984604]].
[2019-03-27 15:29:14,600] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:29:14,614] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9386
[2019-03-27 15:29:14,618] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.81666666666666, 45.16666666666667, 1.0, 2.0, 0.3151149670062715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499939.8225847081, 499939.8225847087, 167050.0548113685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6868200.0000, 
sim time next is 6868800.0000, 
raw observation next is [28.9, 44.0, 1.0, 2.0, 0.3082228783828225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490716.1005350184, 490716.1005350178, 166395.1013074031], 
processed observation next is [0.0, 0.5217391304347826, 0.5687203791469194, 0.44, 1.0, 1.0, 0.1665335884130391, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13631002792639402, 0.13631002792639382, 0.24835089747373595], 
reward next is 0.7516, 
noisyNet noise sample is [array([-0.10368965], dtype=float32), 0.45411906]. 
=============================================
[2019-03-27 15:29:15,952] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6428675e-10 7.3559213e-01 1.8930089e-16 4.3543139e-10 2.6440787e-01], sum to 1.0000
[2019-03-27 15:29:15,966] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5290
[2019-03-27 15:29:15,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2072608.845508866 W.
[2019-03-27 15:29:15,985] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.06666666666667, 62.0, 1.0, 2.0, 0.8411091716074298, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.964836054300722, 6.9112, 168.9126369803162, 2072608.845508866, 2034557.654558418, 419572.6495946812], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6705600.0000, 
sim time next is 6706200.0000, 
raw observation next is [30.03333333333333, 62.5, 1.0, 2.0, 0.7362980460974213, 1.0, 1.0, 0.7362980460974213, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2059022.980463395, 2059022.980463396, 390039.5363888513], 
processed observation next is [1.0, 0.6086956521739131, 0.622432859399684, 0.625, 1.0, 1.0, 0.6822868025270137, 1.0, 0.5, 0.6822868025270137, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5719508279064985, 0.5719508279064989, 0.5821485617744049], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.36521283], dtype=float32), 0.4341147]. 
=============================================
[2019-03-27 15:29:17,927] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 5.505614e-38], sum to 1.0000
[2019-03-27 15:29:17,938] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7860
[2019-03-27 15:29:17,946] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 54.0, 1.0, 2.0, 0.3236519531712713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508717.8792475551, 508717.8792475551, 167619.3360133275], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6807600.0000, 
sim time next is 6808200.0000, 
raw observation next is [27.25, 54.5, 1.0, 2.0, 0.322858277554824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508053.5868684333, 508053.5868684333, 167581.9541114929], 
processed observation next is [1.0, 0.8260869565217391, 0.490521327014218, 0.545, 1.0, 1.0, 0.18416659946364336, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14112599635234258, 0.14112599635234258, 0.2501223195693924], 
reward next is 0.7499, 
noisyNet noise sample is [array([-2.113308], dtype=float32), 1.5660903]. 
=============================================
[2019-03-27 15:29:28,987] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:29:28,995] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8649
[2019-03-27 15:29:29,006] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.45, 69.33333333333334, 1.0, 2.0, 0.4330510175006954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627035.6647681793, 627035.6647681793, 176587.9157469352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6941400.0000, 
sim time next is 6942000.0000, 
raw observation next is [27.6, 68.66666666666667, 1.0, 2.0, 0.4346868807780826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628596.8167288657, 628596.8167288657, 176719.6224060748], 
processed observation next is [0.0, 0.34782608695652173, 0.5071090047393366, 0.6866666666666668, 1.0, 1.0, 0.31889985635913565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17461022686912936, 0.17461022686912936, 0.26376063045682807], 
reward next is 0.7362, 
noisyNet noise sample is [array([0.7019915], dtype=float32), 1.732697]. 
=============================================
[2019-03-27 15:29:29,045] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.95337 ]
 [76.92316 ]
 [76.76576 ]
 [76.76718 ]
 [76.754425]], R is [[76.95731354]
 [76.92417908]
 [76.89154053]
 [76.85926056]
 [76.8272934 ]].
[2019-03-27 15:29:35,801] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6244103e-28 1.0000000e+00 1.4408487e-34 8.3663859e-37 3.7785818e-24], sum to 1.0000
[2019-03-27 15:29:35,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5976
[2019-03-27 15:29:35,815] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.26666666666667, 71.83333333333334, 1.0, 2.0, 0.3184777028823806, 1.0, 2.0, 0.3184777028823806, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 890122.8089006072, 890122.8089006072, 253535.3712482479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7060200.0000, 
sim time next is 7060800.0000, 
raw observation next is [28.13333333333333, 72.66666666666667, 1.0, 2.0, 0.4525505631892381, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632349.3662274355, 632349.3662274355, 176497.0610894489], 
processed observation next is [1.0, 0.7391304347826086, 0.532385466034755, 0.7266666666666667, 1.0, 1.0, 0.3404223652882387, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1756526017298432, 0.1756526017298432, 0.26342844938723714], 
reward next is 0.7366, 
noisyNet noise sample is [array([0.04147227], dtype=float32), 0.8541475]. 
=============================================
[2019-03-27 15:29:41,777] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 15:29:41,779] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:29:41,780] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:29:41,782] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:29:41,783] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:29:41,787] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:29:41,788] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:29:41,788] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:29:41,789] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:29:41,789] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:29:41,793] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:29:41,819] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run85
[2019-03-27 15:29:41,840] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run85
[2019-03-27 15:29:41,841] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run85
[2019-03-27 15:29:41,889] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run85
[2019-03-27 15:29:41,908] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run85
[2019-03-27 15:29:56,786] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06858635], dtype=float32), 0.093377426]
[2019-03-27 15:29:56,788] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 86.0, 1.0, 2.0, 0.278570810106858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 449665.4145380659, 449665.4145380653, 163577.5260373691]
[2019-03-27 15:29:56,789] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:29:56,791] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6474927965827117
[2019-03-27 15:30:03,403] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06858635], dtype=float32), 0.093377426]
[2019-03-27 15:30:03,404] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.3, 94.0, 1.0, 2.0, 0.4597026772879609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651966.8511016708, 651966.8511016708, 178762.9624516959]
[2019-03-27 15:30:03,405] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:30:03,411] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.3366643e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5572581e-35], sampled 0.3909451644364198
[2019-03-27 15:30:03,995] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06858635], dtype=float32), 0.093377426]
[2019-03-27 15:30:03,996] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.14486968166667, 96.70916489333334, 1.0, 2.0, 0.3998962083960962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 609263.0632793584, 609263.0632793579, 175700.7148147326]
[2019-03-27 15:30:03,997] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:30:04,004] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8171530885815886
[2019-03-27 15:30:49,197] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06858635], dtype=float32), 0.093377426]
[2019-03-27 15:30:49,197] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.48333333333333, 56.0, 1.0, 2.0, 0.5105931785703195, 1.0, 2.0, 0.5105931785703195, 1.0, 2.0, 0.8867314954322247, 6.9112, 6.9112, 169.0403247858759, 2141876.790531199, 2141876.790531199, 422250.6190203172]
[2019-03-27 15:30:49,197] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:30:49,201] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0585342e-09 4.8375019e-01 6.9579492e-15 3.2170810e-08 5.1624972e-01], sampled 0.57092778620624
[2019-03-27 15:30:54,373] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06858635], dtype=float32), 0.093377426]
[2019-03-27 15:30:54,374] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.22698878, 83.78735262000001, 1.0, 2.0, 0.5044389076189558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704877.0576512363, 704877.0576512363, 184293.0243221777]
[2019-03-27 15:30:54,374] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:30:54,375] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.5782698e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2068838e-30], sampled 0.942714651918192
[2019-03-27 15:30:57,928] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06858635], dtype=float32), 0.093377426]
[2019-03-27 15:30:57,931] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.39528610666667, 66.14794227333334, 1.0, 2.0, 0.5404796451500179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755256.4394540767, 755256.4394540767, 190182.7058410859]
[2019-03-27 15:30:57,934] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:30:57,938] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3083666e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5422418e-36], sampled 0.3399116934606631
[2019-03-27 15:31:47,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06858635], dtype=float32), 0.093377426]
[2019-03-27 15:31:47,147] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.24633457, 96.63891792000001, 1.0, 2.0, 0.4804766582400499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 677336.4054983762, 677336.4054983769, 181360.1056130325]
[2019-03-27 15:31:47,147] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:31:47,149] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1587119e-31 1.0000000e+00 4.0042949e-37 0.0000000e+00 3.4139176e-28], sampled 0.3272898566912661
[2019-03-27 15:31:50,185] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8672.0789 2779266002.0739 909.0000
[2019-03-27 15:31:50,318] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7902.0033 3163210834.3128 1812.0000
[2019-03-27 15:31:50,709] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1221 2927942114.5496 1346.0000
[2019-03-27 15:31:50,737] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8510.6634 2841436213.5068 1114.0000
[2019-03-27 15:31:50,787] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8033.5259 3005846634.0860 1665.0000
[2019-03-27 15:31:51,801] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2100000, evaluation results [2100000.0, 7902.003293637075, 3163210834.312837, 1812.0, 8255.122116319062, 2927942114.549569, 1346.0, 8672.078866237112, 2779266002.073918, 909.0, 8033.525917007911, 3005846634.086022, 1665.0, 8510.66340230211, 2841436213.5067863, 1114.0]
[2019-03-27 15:31:56,874] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.5902624e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0470134e-36], sum to 1.0000
[2019-03-27 15:31:56,887] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4227
[2019-03-27 15:31:56,893] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 81.0, 1.0, 2.0, 0.537309990994946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765116.9800715566, 765116.9800715559, 191474.8870578634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7110000.0000, 
sim time next is 7110600.0000, 
raw observation next is [26.16666666666667, 80.16666666666667, 1.0, 2.0, 0.50004453226819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711263.0992687366, 711263.0992687372, 185205.4327044014], 
processed observation next is [1.0, 0.30434782608695654, 0.4391785150078992, 0.8016666666666667, 1.0, 1.0, 0.3976440147809518, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1975730831302046, 0.1975730831302048, 0.27642601896179314], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.6601071], dtype=float32), -1.2283229]. 
=============================================
[2019-03-27 15:32:14,729] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:32:14,741] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2674
[2019-03-27 15:32:14,748] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 64.0, 1.0, 2.0, 0.4366183348891791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626130.4426361421, 626130.4426361421, 176330.9720670191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7576800.0000, 
sim time next is 7577400.0000, 
raw observation next is [28.58333333333333, 64.5, 1.0, 2.0, 0.4371093883038558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 626595.5557816135, 626595.5557816129, 176370.2868652242], 
processed observation next is [0.0, 0.6956521739130435, 0.5537124802527644, 0.645, 1.0, 1.0, 0.32181854012512745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17405432105044819, 0.17405432105044802, 0.2632392341272003], 
reward next is 0.7368, 
noisyNet noise sample is [array([-0.10876676], dtype=float32), 0.8804806]. 
=============================================
[2019-03-27 15:32:18,967] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:32:18,976] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2114
[2019-03-27 15:32:18,982] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 92.66666666666666, 1.0, 2.0, 0.3170576002367826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500480.1425461788, 500480.1425461788, 167043.2197491211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7432800.0000, 
sim time next is 7433400.0000, 
raw observation next is [21.26666666666667, 92.33333333333333, 1.0, 2.0, 0.3164641826768507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499628.4358157842, 499628.4358157848, 166981.1764986522], 
processed observation next is [0.0, 0.0, 0.2069510268562403, 0.9233333333333333, 1.0, 1.0, 0.17646287069500086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13878567661549562, 0.13878567661549576, 0.24922563656515254], 
reward next is 0.7508, 
noisyNet noise sample is [array([-0.73973364], dtype=float32), 0.048072204]. 
=============================================
[2019-03-27 15:32:26,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3819499e-30 1.0000000e+00 3.3703854e-37 0.0000000e+00 4.2749657e-26], sum to 1.0000
[2019-03-27 15:32:26,030] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5421
[2019-03-27 15:32:26,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2073278.23449272 W.
[2019-03-27 15:32:26,049] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.8, 61.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.783888908472125, 6.9112, 168.9085003681954, 2073278.23449272, 1454179.020257187, 311350.7647990198], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7736400.0000, 
sim time next is 7737000.0000, 
raw observation next is [31.86666666666667, 60.66666666666667, 1.0, 2.0, 0.6288523286015436, 1.0, 1.0, 0.6288523286015436, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1758309.515143739, 1758309.515143739, 345152.7634043568], 
processed observation next is [1.0, 0.5652173913043478, 0.7093206951026858, 0.6066666666666667, 1.0, 1.0, 0.5528341308452333, 1.0, 0.5, 0.5528341308452333, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48841930976214976, 0.48841930976214976, 0.5151533782154579], 
reward next is 0.4848, 
noisyNet noise sample is [array([0.76451945], dtype=float32), -1.7540686]. 
=============================================
[2019-03-27 15:32:26,084] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[62.08955 ]
 [63.802765]
 [61.69847 ]
 [61.522076]
 [60.977474]], R is [[62.27174377]
 [61.64902878]
 [61.03253937]
 [60.42221451]
 [59.84251404]].
[2019-03-27 15:32:26,108] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8056586e-33 1.0000000e+00 1.8177209e-38 0.0000000e+00 2.4265784e-30], sum to 1.0000
[2019-03-27 15:32:26,121] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5610
[2019-03-27 15:32:26,129] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.23333333333333, 94.16666666666667, 1.0, 2.0, 0.4579213494839708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650251.0205941919, 650251.0205941924, 178605.0607037616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7607400.0000, 
sim time next is 7608000.0000, 
raw observation next is [24.16666666666667, 94.33333333333334, 1.0, 2.0, 0.4568600848926832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650005.2741146933, 650005.2741146926, 178610.5267381745], 
processed observation next is [1.0, 0.043478260869565216, 0.34439178515007923, 0.9433333333333335, 1.0, 1.0, 0.3456145601116665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1805570205874148, 0.1805570205874146, 0.26658287572861866], 
reward next is 0.7334, 
noisyNet noise sample is [array([-1.1053246], dtype=float32), 0.024176218]. 
=============================================
[2019-03-27 15:32:26,140] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.802795]
 [65.49914 ]
 [64.94468 ]
 [64.84509 ]
 [64.85156 ]], R is [[66.05698395]
 [66.12984467]
 [66.20199585]
 [66.27342224]
 [66.34407806]].
[2019-03-27 15:32:26,402] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:32:26,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5109
[2019-03-27 15:32:26,420] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 78.33333333333334, 1.0, 2.0, 0.520807005037641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727756.832369921, 727756.832369921, 186920.7514987606], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7932000.0000, 
sim time next is 7932600.0000, 
raw observation next is [28.25, 79.0, 1.0, 2.0, 0.5223759645920587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729949.9928753187, 729949.9928753187, 187176.5749648716], 
processed observation next is [1.0, 0.8260869565217391, 0.537914691943128, 0.79, 1.0, 1.0, 0.4245493549301912, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20276388690981076, 0.20276388690981076, 0.27936802233562924], 
reward next is 0.7206, 
noisyNet noise sample is [array([1.3479407], dtype=float32), -0.20109935]. 
=============================================
[2019-03-27 15:32:28,528] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:28,531] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:28,616] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run11
[2019-03-27 15:32:30,359] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2114588: loss 2.5425
[2019-03-27 15:32:30,362] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2114589: learning rate 0.0000
[2019-03-27 15:32:31,611] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:31,611] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:31,696] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run11
[2019-03-27 15:32:33,420] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2115810: loss 1.6297
[2019-03-27 15:32:33,423] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2115811: learning rate 0.0000
[2019-03-27 15:32:34,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3540029e-30 1.0000000e+00 4.8090163e-36 1.9519905e-38 3.4872357e-26], sum to 1.0000
[2019-03-27 15:32:34,736] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2672
[2019-03-27 15:32:34,742] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.85, 88.0, 1.0, 2.0, 0.522364377997228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729933.796608091, 729933.7966080903, 187174.5724984668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7853400.0000, 
sim time next is 7854000.0000, 
raw observation next is [26.8, 88.0, 1.0, 2.0, 0.5205528639608469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727401.5832481319, 727401.5832481312, 186879.2071351952], 
processed observation next is [1.0, 0.9130434782608695, 0.4691943127962086, 0.88, 1.0, 1.0, 0.42235284814559865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20205599534670332, 0.20205599534670313, 0.27892418975402267], 
reward next is 0.7211, 
noisyNet noise sample is [array([0.18355128], dtype=float32), -0.18180153]. 
=============================================
[2019-03-27 15:32:34,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.88014]
 [76.76558]
 [76.91686]
 [76.73267]
 [76.85989]], R is [[76.4797287 ]
 [76.43556976]
 [76.39134979]
 [76.34700012]
 [76.30243683]].
[2019-03-27 15:32:35,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:35,199] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:35,271] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run11
[2019-03-27 15:32:36,959] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2117223: loss 1.8889
[2019-03-27 15:32:36,962] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2117224: learning rate 0.0000
[2019-03-27 15:32:37,165] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:37,166] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:37,260] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run11
[2019-03-27 15:32:37,568] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2936959e-09 2.9108419e-06 7.7779446e-16 6.9623422e-07 9.9999642e-01], sum to 1.0000
[2019-03-27 15:32:37,572] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6515
[2019-03-27 15:32:37,576] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.16666666666667, 70.16666666666667, 1.0, 2.0, 0.4841742414184848, 1.0, 2.0, 0.4841742414184848, 1.0, 2.0, 0.8378733825245368, 6.9112, 6.9112, 170.5573041426782, 2030929.259067507, 2030929.259067507, 403856.7105199996], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7912200.0000, 
sim time next is 7912800.0000, 
raw observation next is [30.2, 70.0, 1.0, 2.0, 0.4908563244681499, 1.0, 2.0, 0.4908563244681499, 1.0, 2.0, 0.8493289239549059, 6.9112, 6.9112, 170.5573041426782, 2058985.02581021, 2058985.02581021, 408328.1373667527], 
processed observation next is [1.0, 0.6086956521739131, 0.6303317535545023, 0.7, 1.0, 1.0, 0.3865738849013855, 1.0, 1.0, 0.3865738849013855, 1.0, 1.0, 0.8162547853108607, 0.0, 0.0, 0.8375144448122397, 0.5719402849472806, 0.5719402849472806, 0.609444981144407], 
reward next is 0.3906, 
noisyNet noise sample is [array([1.2622516], dtype=float32), 0.6330404]. 
=============================================
[2019-03-27 15:32:38,950] A3C_AGENT_WORKER-Thread-22 INFO:Local step 132500, global step 2118047: loss 4.1672
[2019-03-27 15:32:38,953] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 132500, global step 2118047: learning rate 0.0000
[2019-03-27 15:32:39,032] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:39,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:39,115] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run11
[2019-03-27 15:32:39,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8379287e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5238031e-34], sum to 1.0000
[2019-03-27 15:32:39,751] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2043
[2019-03-27 15:32:39,756] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 81.0, 1.0, 2.0, 0.5236008400448076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731662.1806736994, 731662.1806736994, 187376.9230011358], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7934400.0000, 
sim time next is 7935000.0000, 
raw observation next is [27.9, 81.66666666666667, 1.0, 2.0, 0.5235415690042754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731579.3287968005, 731579.3287968005, 187367.2348529376], 
processed observation next is [1.0, 0.8695652173913043, 0.5213270142180094, 0.8166666666666668, 1.0, 1.0, 0.4259536975955125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20321648022133346, 0.20321648022133346, 0.2796525893327427], 
reward next is 0.7203, 
noisyNet noise sample is [array([2.0910785], dtype=float32), -0.20159292]. 
=============================================
[2019-03-27 15:32:39,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.37256 ]
 [76.678856]
 [76.4311  ]
 [76.346855]
 [76.35639 ]], R is [[76.25249481]
 [76.21029663]
 [76.16851807]
 [76.12717438]
 [76.08654022]].
[2019-03-27 15:32:40,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:40,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:40,727] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2118801: loss 1.4131
[2019-03-27 15:32:40,732] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2118801: learning rate 0.0000
[2019-03-27 15:32:40,761] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run11
[2019-03-27 15:32:40,922] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:40,922] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:40,999] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-03-27 15:32:41,538] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:41,538] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:41,594] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run11
[2019-03-27 15:32:42,278] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2119524: loss 3.6192
[2019-03-27 15:32:42,280] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2119524: learning rate 0.0000
[2019-03-27 15:32:42,524] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2119645: loss 0.3252
[2019-03-27 15:32:42,526] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2119645: learning rate 0.0000
[2019-03-27 15:32:43,008] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:43,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:43,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run11
[2019-03-27 15:32:43,285] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2119938: loss 0.4943
[2019-03-27 15:32:43,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2119938: learning rate 0.0000
[2019-03-27 15:32:44,831] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2120595: loss 0.2621
[2019-03-27 15:32:44,835] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2120595: learning rate 0.0000
[2019-03-27 15:32:46,956] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:46,956] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:47,030] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run11
[2019-03-27 15:32:47,766] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2121740: loss 0.0023
[2019-03-27 15:32:47,769] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2121742: learning rate 0.0000
[2019-03-27 15:32:48,275] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:48,276] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:48,372] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run11
[2019-03-27 15:32:48,499] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:48,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:48,571] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run11
[2019-03-27 15:32:48,691] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2122121: loss 0.7850
[2019-03-27 15:32:48,693] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2122121: learning rate 0.0000
[2019-03-27 15:32:49,132] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:49,133] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:49,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run11
[2019-03-27 15:32:49,835] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2122704: loss 0.2775
[2019-03-27 15:32:49,837] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2122704: learning rate 0.0000
[2019-03-27 15:32:50,094] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2122833: loss 0.6863
[2019-03-27 15:32:50,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2122833: learning rate 0.0000
[2019-03-27 15:32:50,130] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2122852: loss 0.0087
[2019-03-27 15:32:50,131] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2122853: learning rate 0.0000
[2019-03-27 15:32:50,538] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:50,538] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:50,617] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run11
[2019-03-27 15:32:50,898] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2123157: loss 0.2056
[2019-03-27 15:32:50,900] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2123157: learning rate 0.0000
[2019-03-27 15:32:51,837] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:51,838] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:51,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run11
[2019-03-27 15:32:52,239] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2123757: loss 0.1985
[2019-03-27 15:32:52,244] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2123759: learning rate 0.0000
[2019-03-27 15:32:52,362] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 15:32:52,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:52,444] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run11
[2019-03-27 15:32:53,434] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2124328: loss 0.0024
[2019-03-27 15:32:53,438] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2124329: learning rate 0.0000
[2019-03-27 15:32:53,526] A3C_AGENT_WORKER-Thread-21 INFO:Local step 132500, global step 2124369: loss 0.2046
[2019-03-27 15:32:53,529] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 132500, global step 2124369: learning rate 0.0000
[2019-03-27 15:32:54,214] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2124630: loss 0.4382
[2019-03-27 15:32:54,218] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2124631: learning rate 0.0000
[2019-03-27 15:32:54,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:32:54,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7550
[2019-03-27 15:32:54,393] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 86.0, 1.0, 2.0, 0.3212297688783738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 505721.5080215401, 505721.5080215401, 167409.7090361676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 223200.0000, 
sim time next is 223800.0000, 
raw observation next is [22.16666666666667, 86.0, 1.0, 2.0, 0.3201727374327979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504177.9168911105, 504177.9168911105, 167295.3436498964], 
processed observation next is [0.0, 0.6086956521739131, 0.24960505529225935, 0.86, 1.0, 1.0, 0.1809310089551782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1400494213586418, 0.1400494213586418, 0.2496945427610394], 
reward next is 0.7503, 
noisyNet noise sample is [array([2.4757483], dtype=float32), 0.54875404]. 
=============================================
[2019-03-27 15:32:54,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8007411e-27 1.0000000e+00 8.7608272e-33 6.4417113e-35 2.3813701e-22], sum to 1.0000
[2019-03-27 15:32:54,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3556
[2019-03-27 15:32:54,605] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.88333333333333, 94.16666666666667, 1.0, 2.0, 0.7717516638050138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1155141.158100615, 1155141.158100615, 247338.3886303107], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 123000.0000, 
sim time next is 123600.0000, 
raw observation next is [22.86666666666667, 94.33333333333334, 1.0, 2.0, 0.7560734810938807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1131465.596934165, 1131465.596934165, 243349.7419247602], 
processed observation next is [1.0, 0.43478260869565216, 0.28278041074249627, 0.9433333333333335, 1.0, 1.0, 0.7061126278239527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3142959991483792, 0.3142959991483792, 0.3632085700369555], 
reward next is 0.6368, 
noisyNet noise sample is [array([0.7796848], dtype=float32), 0.50418097]. 
=============================================
[2019-03-27 15:32:55,217] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:32:55,220] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 15:32:55,223] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:32:55,224] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:55,224] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:32:55,225] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:32:55,225] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:32:55,226] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:55,228] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:55,228] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:55,227] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:32:55,230] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4056
[2019-03-27 15:32:55,232] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:32:55,237] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.53333333333333, 76.33333333333333, 1.0, 2.0, 0.3164592760256142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498106.7996662029, 498106.7996662023, 166832.9639899812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 304800.0000, 
sim time next is 305400.0000, 
raw observation next is [23.56666666666667, 76.16666666666667, 1.0, 2.0, 0.3167429366759382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498449.7570176365, 498449.7570176359, 166856.1608256341], 
processed observation next is [0.0, 0.5217391304347826, 0.31595576619273325, 0.7616666666666667, 1.0, 1.0, 0.17679871888667256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13845826583823237, 0.1384582658382322, 0.24903904600840912], 
reward next is 0.7510, 
noisyNet noise sample is [array([2.3687673], dtype=float32), -0.35174528]. 
=============================================
[2019-03-27 15:32:55,254] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run86
[2019-03-27 15:32:55,277] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run86
[2019-03-27 15:32:55,302] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run86
[2019-03-27 15:32:55,303] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run86
[2019-03-27 15:32:55,341] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run86
[2019-03-27 15:33:17,680] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06915744], dtype=float32), 0.099350214]
[2019-03-27 15:33:17,681] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.66699906333334, 90.75792811333332, 1.0, 2.0, 0.442895023775045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 652546.7390254283, 652546.7390254289, 179392.0077975456]
[2019-03-27 15:33:17,682] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:33:17,687] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.066992e-37], sampled 0.7046082228568115
[2019-03-27 15:33:22,764] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06915744], dtype=float32), 0.099350214]
[2019-03-27 15:33:22,768] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.66730335166667, 98.65754129833334, 1.0, 2.0, 0.4612807522115803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652364.573554521, 652364.573554521, 178759.5282786439]
[2019-03-27 15:33:22,769] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:33:22,772] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5712173556003831
[2019-03-27 15:34:09,151] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06915744], dtype=float32), 0.099350214]
[2019-03-27 15:34:09,154] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 80.66666666666667, 1.0, 2.0, 0.5285315391338506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738554.5687871398, 738554.5687871391, 188187.0811544497]
[2019-03-27 15:34:09,156] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:34:09,159] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0724061e-29 1.0000000e+00 5.7958264e-36 2.7225996e-37 9.5469741e-25], sampled 0.75432585245729
[2019-03-27 15:34:31,869] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06915744], dtype=float32), 0.099350214]
[2019-03-27 15:34:31,871] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.8, 87.0, 1.0, 2.0, 0.7406611294908749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1035122.725842849, 1035122.72584285, 229868.1972868438]
[2019-03-27 15:34:31,872] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:34:31,878] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.642321e-31 1.000000e+00 2.349518e-37 0.000000e+00 3.826925e-27], sampled 0.485028043803988
[2019-03-27 15:34:44,075] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06915744], dtype=float32), 0.099350214]
[2019-03-27 15:34:44,075] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.39129326, 64.66861232, 1.0, 2.0, 0.8606031628920898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104142, 1273047.998394797, 1273047.998394797, 269259.1742173856]
[2019-03-27 15:34:44,077] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:34:44,080] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1157867e-17 1.0000000e+00 3.1861256e-23 8.0870590e-19 2.4160010e-09], sampled 0.7210115707769842
[2019-03-27 15:35:03,350] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8041.9688 3156489347.6592 1481.0000
[2019-03-27 15:35:03,486] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8393.6934 2926739209.9518 1058.0000
[2019-03-27 15:35:03,663] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8633.7170 2837365316.9797 834.0000
[2019-03-27 15:35:03,843] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8753.8513 2778998855.4450 742.0000
[2019-03-27 15:35:03,941] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8258.2820 2997413894.8568 1132.0000
[2019-03-27 15:35:04,959] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2125000, evaluation results [2125000.0, 8041.968827889563, 3156489347.6591654, 1481.0, 8393.693429223531, 2926739209.951787, 1058.0, 8753.851254323896, 2778998855.4450364, 742.0, 8258.281967407556, 2997413894.8568025, 1132.0, 8633.716992621117, 2837365316.9797215, 834.0]
[2019-03-27 15:35:05,293] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.7562036e-23 1.0000000e+00 1.3874261e-29 2.1372645e-27 1.6163935e-16], sum to 1.0000
[2019-03-27 15:35:05,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1137
[2019-03-27 15:35:05,314] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 96.0, 1.0, 2.0, 0.7678708023734497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1143481.316695742, 1143481.316695743, 245610.7743564423], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 136800.0000, 
sim time next is 137400.0000, 
raw observation next is [22.78333333333333, 96.0, 1.0, 2.0, 0.8757011245039606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1304635.5303332, 1304635.5303332, 274777.8038964959], 
processed observation next is [1.0, 0.6086956521739131, 0.27883096366508686, 0.96, 1.0, 1.0, 0.8502423186794706, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.36239875842588887, 0.36239875842588887, 0.4101161252186506], 
reward next is 0.5899, 
noisyNet noise sample is [array([0.586258], dtype=float32), 2.2189655]. 
=============================================
[2019-03-27 15:35:05,444] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133000, global step 2125191: loss 0.0031
[2019-03-27 15:35:05,446] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133000, global step 2125191: learning rate 0.0000
[2019-03-27 15:35:07,130] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 1.444744e-36], sum to 1.0000
[2019-03-27 15:35:07,140] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2931
[2019-03-27 15:35:07,148] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.35, 89.0, 1.0, 2.0, 0.3444837827850009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534850.1042379659, 534850.1042379659, 169515.0482132377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 91800.0000, 
sim time next is 92400.0000, 
raw observation next is [22.36666666666667, 89.0, 1.0, 2.0, 0.3454056423985683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536040.1334539771, 536040.1334539765, 169605.132465738], 
processed observation next is [1.0, 0.043478260869565216, 0.2590837282780413, 0.89, 1.0, 1.0, 0.2113320992753835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1489000370705492, 0.14890003707054902, 0.2531419887548328], 
reward next is 0.7469, 
noisyNet noise sample is [array([0.27578115], dtype=float32), 1.1725566]. 
=============================================
[2019-03-27 15:35:07,899] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2126109: loss 0.0031
[2019-03-27 15:35:07,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2126109: learning rate 0.0000
[2019-03-27 15:35:09,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:35:09,837] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5115
[2019-03-27 15:35:09,842] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 82.0, 1.0, 2.0, 0.2994262534653076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475955.9677280707, 475955.9677280707, 165318.6499212074], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 291600.0000, 
sim time next is 292200.0000, 
raw observation next is [22.38333333333334, 81.50000000000001, 1.0, 2.0, 0.3000432669004225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 476756.8328659078, 476756.8328659085, 165372.3594807952], 
processed observation next is [0.0, 0.391304347826087, 0.2598736176935233, 0.8150000000000002, 1.0, 1.0, 0.1566786348197861, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13243245357386327, 0.13243245357386346, 0.2468244171355152], 
reward next is 0.7532, 
noisyNet noise sample is [array([0.03652834], dtype=float32), 0.21619475]. 
=============================================
[2019-03-27 15:35:10,234] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2126982: loss 0.0048
[2019-03-27 15:35:10,237] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2126983: learning rate 0.0000
[2019-03-27 15:35:10,376] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2127030: loss 0.0042
[2019-03-27 15:35:10,378] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2127030: learning rate 0.0000
[2019-03-27 15:35:11,242] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2127361: loss 0.0055
[2019-03-27 15:35:11,252] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2127361: learning rate 0.0000
[2019-03-27 15:35:12,444] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4126601e-37], sum to 1.0000
[2019-03-27 15:35:12,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6090
[2019-03-27 15:35:12,469] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333333, 87.83333333333334, 1.0, 2.0, 0.301990562396778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480988.7206813574, 480988.7206813568, 165693.9882292077], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 237000.0000, 
sim time next is 237600.0000, 
raw observation next is [21.4, 88.0, 1.0, 2.0, 0.3012355422874743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479912.221147465, 479912.2211474656, 165618.7734097744], 
processed observation next is [0.0, 0.782608695652174, 0.21327014218009477, 0.88, 1.0, 1.0, 0.15811511118972804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13330895031874027, 0.13330895031874043, 0.24719219911906629], 
reward next is 0.7528, 
noisyNet noise sample is [array([2.6149905], dtype=float32), 2.3803992]. 
=============================================
[2019-03-27 15:35:13,265] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2128127: loss 0.0031
[2019-03-27 15:35:13,268] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2128129: learning rate 0.0000
[2019-03-27 15:35:16,278] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2129258: loss 0.1572
[2019-03-27 15:35:16,281] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2129258: learning rate 0.0000
[2019-03-27 15:35:16,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:35:16,378] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9422
[2019-03-27 15:35:16,387] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 92.0, 1.0, 2.0, 0.2854282215026877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458581.6140000548, 458581.6140000548, 164172.945245895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 264000.0000, 
sim time next is 264600.0000, 
raw observation next is [20.5, 92.0, 1.0, 2.0, 0.2853967690869965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458531.0327374417, 458531.0327374417, 164169.5043470557], 
processed observation next is [0.0, 0.043478260869565216, 0.1706161137440759, 0.92, 1.0, 1.0, 0.139032251912044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12736973131595603, 0.12736973131595603, 0.2450291109657548], 
reward next is 0.7550, 
noisyNet noise sample is [array([0.44663915], dtype=float32), 0.48853183]. 
=============================================
[2019-03-27 15:35:17,700] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2129787: loss 0.0025
[2019-03-27 15:35:17,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2129788: learning rate 0.0000
[2019-03-27 15:35:19,682] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2130536: loss 0.0031
[2019-03-27 15:35:19,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2130537: learning rate 0.0000
[2019-03-27 15:35:19,929] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2130625: loss 0.0030
[2019-03-27 15:35:19,932] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2130626: learning rate 0.0000
[2019-03-27 15:35:20,034] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2130662: loss 0.0101
[2019-03-27 15:35:20,040] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2130662: learning rate 0.0000
[2019-03-27 15:35:21,088] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2131061: loss 0.0022
[2019-03-27 15:35:21,092] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2131061: learning rate 0.0000
[2019-03-27 15:35:22,850] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2131727: loss 0.0040
[2019-03-27 15:35:22,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2131728: learning rate 0.0000
[2019-03-27 15:35:24,387] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2132309: loss 0.0293
[2019-03-27 15:35:24,389] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2132309: learning rate 0.0000
[2019-03-27 15:35:24,465] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133000, global step 2132338: loss 0.0022
[2019-03-27 15:35:24,467] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133000, global step 2132338: learning rate 0.0000
[2019-03-27 15:35:24,728] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.1393811e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0942251e-29], sum to 1.0000
[2019-03-27 15:35:24,737] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4026
[2019-03-27 15:35:24,749] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333333, 66.5, 1.0, 2.0, 0.2553916522124396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417912.0570407765, 417912.0570407765, 161435.166928166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 582600.0000, 
sim time next is 583200.0000, 
raw observation next is [22.7, 67.0, 1.0, 2.0, 0.255544752789682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 418460.2671677591, 418460.2671677597, 161455.1958298487], 
processed observation next is [1.0, 0.782608695652174, 0.27488151658767773, 0.67, 1.0, 1.0, 0.1030659672164843, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11623896310215531, 0.11623896310215548, 0.24097790422365478], 
reward next is 0.7590, 
noisyNet noise sample is [array([-1.850463], dtype=float32), -0.34654278]. 
=============================================
[2019-03-27 15:35:25,295] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2132650: loss 0.0071
[2019-03-27 15:35:25,297] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2132651: learning rate 0.0000
[2019-03-27 15:35:26,744] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133500, global step 2133194: loss 0.0157
[2019-03-27 15:35:26,749] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133500, global step 2133194: learning rate 0.0000
[2019-03-27 15:35:29,022] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2134051: loss 0.0169
[2019-03-27 15:35:29,025] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2134051: learning rate 0.0000
[2019-03-27 15:35:29,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:35:29,838] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1126
[2019-03-27 15:35:29,844] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.13333333333333, 91.0, 1.0, 2.0, 0.2286987242212161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 382335.3404763004, 382335.3404762998, 157779.9089168127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 623400.0000, 
sim time next is 624000.0000, 
raw observation next is [17.36666666666667, 90.0, 1.0, 2.0, 0.2157828236034253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 360580.0990561401, 360580.0990561408, 156757.1322968522], 
processed observation next is [1.0, 0.21739130434782608, 0.02211690363349157, 0.9, 1.0, 1.0, 0.05516002843786179, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10016113862670559, 0.10016113862670578, 0.23396586909977943], 
reward next is 0.7660, 
noisyNet noise sample is [array([-0.69970953], dtype=float32), -0.3613083]. 
=============================================
[2019-03-27 15:35:29,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.622696]
 [77.67792 ]
 [77.56914 ]
 [77.590935]
 [77.64632 ]], R is [[77.61703491]
 [77.6053772 ]
 [77.59764099]
 [77.58998108]
 [77.58229828]].
[2019-03-27 15:35:31,538] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2134996: loss 0.0805
[2019-03-27 15:35:31,542] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2134996: learning rate 0.0000
[2019-03-27 15:35:31,588] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2135014: loss 0.0488
[2019-03-27 15:35:31,590] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2135015: learning rate 0.0000
[2019-03-27 15:35:32,546] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2135372: loss 0.0260
[2019-03-27 15:35:32,549] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2135372: learning rate 0.0000
[2019-03-27 15:35:34,740] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2136195: loss 0.1498
[2019-03-27 15:35:34,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2136196: learning rate 0.0000
[2019-03-27 15:35:34,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:35:34,956] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9682
[2019-03-27 15:35:34,964] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 72.0, 1.0, 2.0, 0.4270096543093645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702212.8281481456, 702212.8281481449, 183554.951037988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 552000.0000, 
sim time next is 552600.0000, 
raw observation next is [21.8, 70.5, 1.0, 2.0, 0.4084047101630429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671773.8923314483, 671773.8923314483, 180715.5563780493], 
processed observation next is [1.0, 0.391304347826087, 0.23222748815165886, 0.705, 1.0, 1.0, 0.28723459055788303, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18660385898095785, 0.18660385898095785, 0.2697247110120139], 
reward next is 0.7303, 
noisyNet noise sample is [array([0.46731615], dtype=float32), 0.17706005]. 
=============================================
[2019-03-27 15:35:37,726] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2137323: loss 0.0036
[2019-03-27 15:35:37,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2137324: learning rate 0.0000
[2019-03-27 15:35:39,083] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2137826: loss 0.0137
[2019-03-27 15:35:39,087] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2137826: learning rate 0.0000
[2019-03-27 15:35:40,650] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:35:40,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5512
[2019-03-27 15:35:40,671] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.1, 91.0, 1.0, 2.0, 0.2244193785912199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 373045.611749599, 373045.6117495984, 158173.3704268954], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 694800.0000, 
sim time next is 695400.0000, 
raw observation next is [18.03333333333333, 91.33333333333334, 1.0, 2.0, 0.22362272208739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 371807.4158450303, 371807.4158450303, 158085.7699185944], 
processed observation next is [1.0, 0.043478260869565216, 0.05371248025276459, 0.9133333333333334, 1.0, 1.0, 0.06460568926191566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10327983773473064, 0.10327983773473064, 0.23594891032626028], 
reward next is 0.7641, 
noisyNet noise sample is [array([-0.73977315], dtype=float32), -1.1068642]. 
=============================================
[2019-03-27 15:35:40,933] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2138525: loss 0.1084
[2019-03-27 15:35:40,936] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2138525: learning rate 0.0000
[2019-03-27 15:35:41,094] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2138585: loss 0.0223
[2019-03-27 15:35:41,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2138585: learning rate 0.0000
[2019-03-27 15:35:41,218] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2138633: loss 0.0026
[2019-03-27 15:35:41,220] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2138633: learning rate 0.0000
[2019-03-27 15:35:42,426] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2139093: loss 0.0027
[2019-03-27 15:35:42,429] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2139093: learning rate 0.0000
[2019-03-27 15:35:43,989] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2139685: loss 0.0082
[2019-03-27 15:35:43,991] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2139686: learning rate 0.0000
[2019-03-27 15:35:44,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.254623e-33 1.000000e+00 0.000000e+00 0.000000e+00 4.788951e-29], sum to 1.0000
[2019-03-27 15:35:44,856] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7996
[2019-03-27 15:35:44,862] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 58.33333333333334, 1.0, 2.0, 0.5886806297422197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 964628.6644423761, 964628.6644423761, 213178.2764068074], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 645600.0000, 
sim time next is 646200.0000, 
raw observation next is [24.1, 58.0, 1.0, 2.0, 0.6083669729066368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 997175.8916316932, 997175.8916316939, 217401.3184324259], 
processed observation next is [1.0, 0.4782608695652174, 0.3412322274881518, 0.58, 1.0, 1.0, 0.5281529794055865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2769933032310259, 0.2769933032310261, 0.3244795797498894], 
reward next is 0.6755, 
noisyNet noise sample is [array([0.49134082], dtype=float32), 0.35932687]. 
=============================================
[2019-03-27 15:35:45,533] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2140266: loss 0.0043
[2019-03-27 15:35:45,534] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2140267: learning rate 0.0000
[2019-03-27 15:35:45,584] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133500, global step 2140286: loss 0.0097
[2019-03-27 15:35:45,587] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133500, global step 2140288: learning rate 0.0000
[2019-03-27 15:35:46,736] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2140723: loss 0.0108
[2019-03-27 15:35:46,738] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2140724: learning rate 0.0000
[2019-03-27 15:35:46,880] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:35:46,888] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4174
[2019-03-27 15:35:46,896] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2941999846469605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469623.3579305353, 469623.3579305353, 164904.2378586496], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 896400.0000, 
sim time next is 897000.0000, 
raw observation next is [22.5, 79.00000000000001, 1.0, 2.0, 0.2940206808622922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 469337.0086301719, 469337.0086301725, 164884.1994933909], 
processed observation next is [0.0, 0.391304347826087, 0.2654028436018958, 0.7900000000000001, 1.0, 1.0, 0.14942250706300267, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13037139128615885, 0.13037139128615904, 0.24609582013938938], 
reward next is 0.7539, 
noisyNet noise sample is [array([-1.316233], dtype=float32), 1.4381104]. 
=============================================
[2019-03-27 15:35:46,912] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[78.86746 ]
 [78.738525]
 [78.74263 ]
 [78.73453 ]
 [78.724014]], R is [[78.83769989]
 [78.80319977]
 [78.76908112]
 [78.73537445]
 [78.70207977]].
[2019-03-27 15:35:48,002] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134000, global step 2141200: loss 0.0029
[2019-03-27 15:35:48,007] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134000, global step 2141200: learning rate 0.0000
[2019-03-27 15:35:50,326] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2142061: loss 0.0020
[2019-03-27 15:35:50,329] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2142061: learning rate 0.0000
[2019-03-27 15:35:52,725] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2142965: loss 0.0044
[2019-03-27 15:35:52,729] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2142967: learning rate 0.0000
[2019-03-27 15:35:52,935] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2143044: loss 0.0023
[2019-03-27 15:35:52,937] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2143044: learning rate 0.0000
[2019-03-27 15:35:53,181] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:35:53,193] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8952
[2019-03-27 15:35:53,201] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.28333333333333, 74.83333333333334, 1.0, 2.0, 0.2638325361547078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 433725.1113663855, 433725.1113663848, 162303.5312655763], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 719400.0000, 
sim time next is 720000.0000, 
raw observation next is [21.5, 74.0, 1.0, 2.0, 0.2544481820642139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 417782.1902334738, 417782.1902334731, 161352.5543790924], 
processed observation next is [1.0, 0.34782608695652173, 0.21800947867298584, 0.74, 1.0, 1.0, 0.10174479766772755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11605060839818716, 0.11605060839818697, 0.24082470802849615], 
reward next is 0.7592, 
noisyNet noise sample is [array([0.28255978], dtype=float32), 0.8891636]. 
=============================================
[2019-03-27 15:35:53,219] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[77.58247 ]
 [77.61865 ]
 [77.65771 ]
 [77.683105]
 [77.683426]], R is [[77.68135071]
 [77.66230011]
 [77.64488983]
 [77.62937927]
 [77.61458588]].
[2019-03-27 15:35:53,883] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2143402: loss 0.0024
[2019-03-27 15:35:53,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2143402: learning rate 0.0000
[2019-03-27 15:35:56,062] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2144219: loss 0.0020
[2019-03-27 15:35:56,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2144219: learning rate 0.0000
[2019-03-27 15:35:56,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:35:56,742] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8060
[2019-03-27 15:35:56,751] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.0, 1.0, 2.0, 0.3241191120042129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504031.8680298316, 504031.8680298316, 167111.298592803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 968400.0000, 
sim time next is 969000.0000, 
raw observation next is [21.9, 92.16666666666667, 1.0, 2.0, 0.3322008264389752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516494.7689646671, 516494.7689646671, 168073.3060268565], 
processed observation next is [1.0, 0.21739130434782608, 0.23696682464454974, 0.9216666666666667, 1.0, 1.0, 0.1954226824565966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14347076915685197, 0.14347076915685197, 0.25085568063709923], 
reward next is 0.7491, 
noisyNet noise sample is [array([0.5415522], dtype=float32), 0.13153508]. 
=============================================
[2019-03-27 15:35:56,766] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.29167]
 [79.19041]
 [79.18044]
 [79.26958]
 [79.27225]], R is [[79.27308655]
 [79.23093414]
 [79.18921661]
 [79.14784241]
 [79.10663605]].
[2019-03-27 15:35:58,896] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2145285: loss 22.4996
[2019-03-27 15:35:58,898] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2145286: learning rate 0.0000
[2019-03-27 15:36:00,273] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2145801: loss 0.0015
[2019-03-27 15:36:00,277] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2145801: learning rate 0.0000
[2019-03-27 15:36:00,338] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:36:00,352] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0928
[2019-03-27 15:36:00,358] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.03333333333333, 61.83333333333334, 1.0, 2.0, 0.2895031986759039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463378.7550511046, 463378.7550511046, 164486.5213511981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 820200.0000, 
sim time next is 820800.0000, 
raw observation next is [25.0, 62.0, 1.0, 2.0, 0.2895444917947173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463480.6529531262, 463480.6529531256, 164493.9486541766], 
processed observation next is [0.0, 0.5217391304347826, 0.38388625592417064, 0.62, 1.0, 1.0, 0.1440295081864064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12874462582031282, 0.12874462582031265, 0.2455133562002636], 
reward next is 0.7545, 
noisyNet noise sample is [array([1.0233119], dtype=float32), 0.10095641]. 
=============================================
[2019-03-27 15:36:02,005] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6470546e-31 1.0000000e+00 1.1868543e-38 0.0000000e+00 8.9460601e-27], sum to 1.0000
[2019-03-27 15:36:02,014] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3345
[2019-03-27 15:36:02,021] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 72.5, 1.0, 2.0, 0.8097466957770957, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1131725.807345365, 1131725.807345365, 246314.3487172135], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1254600.0000, 
sim time next is 1255200.0000, 
raw observation next is [28.33333333333333, 72.66666666666667, 1.0, 2.0, 0.9024523296608995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1261370.891667297, 1261370.891667298, 270610.2987629104], 
processed observation next is [1.0, 0.5217391304347826, 0.541864139020537, 0.7266666666666667, 1.0, 1.0, 0.8824726863384331, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35038080324091586, 0.35038080324091614, 0.40389596830285135], 
reward next is 0.5961, 
noisyNet noise sample is [array([1.2210565], dtype=float32), -0.7579981]. 
=============================================
[2019-03-27 15:36:02,026] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 9.710108e-36], sum to 1.0000
[2019-03-27 15:36:02,038] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3091
[2019-03-27 15:36:02,046] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.76666666666667, 86.16666666666666, 1.0, 2.0, 0.3353269215834158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520066.7343381367, 520066.7343381373, 168316.2579304135], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 935400.0000, 
sim time next is 936000.0000, 
raw observation next is [22.7, 87.0, 1.0, 2.0, 0.3364201343177969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 521150.1697820369, 521150.1697820375, 168383.5065111133], 
processed observation next is [0.0, 0.8695652173913043, 0.27488151658767773, 0.87, 1.0, 1.0, 0.20050618592505648, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1447639360505658, 0.14476393605056598, 0.25131866643449746], 
reward next is 0.7487, 
noisyNet noise sample is [array([0.7255999], dtype=float32), -1.01061]. 
=============================================
[2019-03-27 15:36:02,060] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.71858 ]
 [79.70464 ]
 [79.68129 ]
 [79.663445]
 [79.6404  ]], R is [[79.80500793]
 [79.7557373 ]
 [79.70700836]
 [79.65880585]
 [79.61123657]].
[2019-03-27 15:36:02,130] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2146494: loss 0.0052
[2019-03-27 15:36:02,133] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2146495: learning rate 0.0000
[2019-03-27 15:36:02,151] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1102574e-35], sum to 1.0000
[2019-03-27 15:36:02,161] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5178
[2019-03-27 15:36:02,168] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.05, 78.5, 1.0, 2.0, 0.3171040195629773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500848.2221666477, 500848.2221666477, 167076.9413814815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1110600.0000, 
sim time next is 1111200.0000, 
raw observation next is [22.93333333333333, 79.0, 1.0, 2.0, 0.3163582521154338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 500184.4699186072, 500184.4699186066, 167037.5268476], 
processed observation next is [1.0, 0.8695652173913043, 0.28593996840442326, 0.79, 1.0, 1.0, 0.17633524351257082, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13894013053294646, 0.1389401305329463, 0.2493097415635821], 
reward next is 0.7507, 
noisyNet noise sample is [array([-0.2507803], dtype=float32), -0.45898896]. 
=============================================
[2019-03-27 15:36:02,376] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2146587: loss 0.0014
[2019-03-27 15:36:02,379] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2146588: learning rate 0.0000
[2019-03-27 15:36:02,549] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2146653: loss 17.0294
[2019-03-27 15:36:02,554] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2146653: learning rate 0.0000
[2019-03-27 15:36:03,556] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2147033: loss 0.0023
[2019-03-27 15:36:03,558] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2147033: learning rate 0.0000
[2019-03-27 15:36:05,326] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2147699: loss 0.0083
[2019-03-27 15:36:05,331] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2147699: learning rate 0.0000
[2019-03-27 15:36:06,833] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134000, global step 2148265: loss 0.0041
[2019-03-27 15:36:06,837] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134000, global step 2148265: learning rate 0.0000
[2019-03-27 15:36:06,854] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2148269: loss 15.8494
[2019-03-27 15:36:06,858] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2148271: learning rate 0.0000
[2019-03-27 15:36:07,939] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2148681: loss 0.0224
[2019-03-27 15:36:07,942] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2148682: learning rate 0.0000
[2019-03-27 15:36:09,322] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134500, global step 2149194: loss 15.3353
[2019-03-27 15:36:09,324] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134500, global step 2149194: learning rate 0.0000
[2019-03-27 15:36:11,458] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 15:36:11,460] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:36:11,462] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:36:11,462] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:36:11,463] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:36:11,465] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:36:11,464] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:36:11,468] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:36:11,470] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:36:11,472] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:36:11,474] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:36:11,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run87
[2019-03-27 15:36:11,521] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run87
[2019-03-27 15:36:11,546] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run87
[2019-03-27 15:36:11,568] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run87
[2019-03-27 15:36:11,588] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run87
[2019-03-27 15:36:27,820] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06683473], dtype=float32), 0.09929114]
[2019-03-27 15:36:27,823] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.73333333333333, 94.00000000000001, 1.0, 2.0, 0.3062010337276371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486895.9573944967, 486895.957394496, 166107.9671324808]
[2019-03-27 15:36:27,825] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:36:27,827] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8940918191303777
[2019-03-27 15:36:35,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06683473], dtype=float32), 0.09929114]
[2019-03-27 15:36:35,596] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.75, 88.0, 1.0, 2.0, 0.3937778143103614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587622.5538527947, 587622.5538527947, 173401.3058275043]
[2019-03-27 15:36:35,598] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:36:35,601] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8487016e-38], sampled 0.5323118878823699
[2019-03-27 15:37:11,388] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06683473], dtype=float32), 0.09929114]
[2019-03-27 15:37:11,390] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 63.00000000000001, 1.0, 2.0, 0.5633089863129142, 1.0, 2.0, 0.5633089863129142, 1.0, 2.0, 0.9782814200970934, 6.9112, 6.9112, 170.5573041426782, 2363212.269806828, 2363212.269806828, 461685.5234875095]
[2019-03-27 15:37:11,390] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:37:11,394] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8074938e-09 7.3483889e-04 1.4345174e-15 9.9468593e-07 9.9926418e-01], sampled 0.038946122762390845
[2019-03-27 15:37:16,557] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06683473], dtype=float32), 0.09929114]
[2019-03-27 15:37:16,558] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 0.5938503871600896, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.912699735907165, 6.9112, 168.9128484728823, 1660378.652781473, 1659314.689192521, 363438.8286106332]
[2019-03-27 15:37:16,559] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:37:16,563] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.6528288e-24 1.0000000e+00 1.0755772e-29 4.1249336e-29 2.9836322e-19], sampled 0.21875742152571998
[2019-03-27 15:37:16,563] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1660378.652781473 W.
[2019-03-27 15:38:05,309] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06683473], dtype=float32), 0.09929114]
[2019-03-27 15:38:05,311] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.28052773166667, 66.68132142833333, 1.0, 2.0, 0.5402543046642927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754941.4408530998, 754941.4408530991, 190144.6997095792]
[2019-03-27 15:38:05,312] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:38:05,316] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5442084e-36], sampled 0.8126869187577622
[2019-03-27 15:38:20,118] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7888.0313 3163477972.8879 1814.0000
[2019-03-27 15:38:20,261] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8006.2767 3007278629.2546 1748.0000
[2019-03-27 15:38:20,476] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.4155 2779348555.4351 934.0000
[2019-03-27 15:38:20,492] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.8867 2927385837.5512 1348.0000
[2019-03-27 15:38:20,591] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.9008 2842277352.5536 1148.0000
[2019-03-27 15:38:21,607] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2150000, evaluation results [2150000.0, 7888.031251936043, 3163477972.88788, 1814.0, 8251.886719647651, 2927385837.5511885, 1348.0, 8659.415484067282, 2779348555.4350953, 934.0, 8006.27672949967, 3007278629.2546077, 1748.0, 8495.90081376094, 2842277352.5535984, 1148.0]
[2019-03-27 15:38:21,707] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2150042: loss 15.4663
[2019-03-27 15:38:21,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2150045: learning rate 0.0000
[2019-03-27 15:38:23,389] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:38:23,399] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5019
[2019-03-27 15:38:23,403] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 89.0, 1.0, 2.0, 0.5346183266987886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847247.159568022, 847247.159568022, 200622.6061289288], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1069200.0000, 
sim time next is 1069800.0000, 
raw observation next is [21.58333333333334, 88.16666666666667, 1.0, 2.0, 0.4926710899670272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781138.8377246531, 781138.8377246531, 193012.5482907254], 
processed observation next is [1.0, 0.391304347826087, 0.22195892575039528, 0.8816666666666667, 1.0, 1.0, 0.38876034935786413, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2169830104790703, 0.2169830104790703, 0.2880784302846648], 
reward next is 0.7119, 
noisyNet noise sample is [array([-0.90597016], dtype=float32), -1.2437387]. 
=============================================
[2019-03-27 15:38:24,123] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2150949: loss 15.6205
[2019-03-27 15:38:24,125] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2150950: learning rate 0.0000
[2019-03-27 15:38:24,354] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2151036: loss 15.6463
[2019-03-27 15:38:24,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2151037: learning rate 0.0000
[2019-03-27 15:38:25,310] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2151395: loss 15.9551
[2019-03-27 15:38:25,311] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2151395: learning rate 0.0000
[2019-03-27 15:38:27,326] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2152159: loss 15.4285
[2019-03-27 15:38:27,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2152159: learning rate 0.0000
[2019-03-27 15:38:30,263] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2153253: loss 0.0726
[2019-03-27 15:38:30,264] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2153253: learning rate 0.0000
[2019-03-27 15:38:31,553] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2153736: loss 15.3100
[2019-03-27 15:38:31,554] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2153736: learning rate 0.0000
[2019-03-27 15:38:33,429] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2154437: loss 15.5015
[2019-03-27 15:38:33,431] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2154437: learning rate 0.0000
[2019-03-27 15:38:33,815] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2154584: loss 14.9239
[2019-03-27 15:38:33,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2154585: learning rate 0.0000
[2019-03-27 15:38:34,071] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2154683: loss 0.0751
[2019-03-27 15:38:34,072] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2154683: learning rate 0.0000
[2019-03-27 15:38:34,950] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2155013: loss 15.1745
[2019-03-27 15:38:34,953] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2155013: learning rate 0.0000
[2019-03-27 15:38:36,567] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2155619: loss 15.4113
[2019-03-27 15:38:36,571] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2155621: learning rate 0.0000
[2019-03-27 15:38:38,304] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134500, global step 2156276: loss 14.2450
[2019-03-27 15:38:38,306] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134500, global step 2156277: learning rate 0.0000
[2019-03-27 15:38:38,456] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2156332: loss 0.0764
[2019-03-27 15:38:38,461] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2156333: learning rate 0.0000
[2019-03-27 15:38:39,168] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2156599: loss 14.7884
[2019-03-27 15:38:39,172] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2156600: learning rate 0.0000
[2019-03-27 15:38:40,013] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3269164e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1856933e-32], sum to 1.0000
[2019-03-27 15:38:40,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1537
[2019-03-27 15:38:40,035] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333333, 85.0, 1.0, 2.0, 0.7987309430605135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1205591.685212153, 1205591.685212153, 255609.3047495016], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1597200.0000, 
sim time next is 1597800.0000, 
raw observation next is [23.86666666666667, 85.0, 1.0, 2.0, 0.7951079317555241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1198860.199280805, 1198860.199280804, 254490.2984209997], 
processed observation next is [1.0, 0.4782608695652174, 0.33017377567140627, 0.85, 1.0, 1.0, 0.7531420864524386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3330167220224458, 0.3330167220224456, 0.37983626629999956], 
reward next is 0.6202, 
noisyNet noise sample is [array([-0.19640248], dtype=float32), 0.46693954]. 
=============================================
[2019-03-27 15:38:40,550] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0931965e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8897040e-31], sum to 1.0000
[2019-03-27 15:38:40,564] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9792
[2019-03-27 15:38:40,572] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 77.5, 1.0, 2.0, 0.9357744376790966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1351334.99541321, 1351334.995413211, 286461.4781470044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1243800.0000, 
sim time next is 1244400.0000, 
raw observation next is [26.4, 76.66666666666667, 1.0, 2.0, 0.9627624593586369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1386511.577371322, 1386511.577371322, 293996.219844892], 
processed observation next is [1.0, 0.391304347826087, 0.45023696682464454, 0.7666666666666667, 1.0, 1.0, 0.955135493203177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.38514210482536726, 0.38514210482536726, 0.43880032812670444], 
reward next is 0.5612, 
noisyNet noise sample is [array([1.2303513], dtype=float32), 0.04852509]. 
=============================================
[2019-03-27 15:38:40,958] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135000, global step 2157266: loss 0.0871
[2019-03-27 15:38:40,962] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135000, global step 2157267: learning rate 0.0000
[2019-03-27 15:38:42,889] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2157999: loss 0.0342
[2019-03-27 15:38:42,890] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2157999: learning rate 0.0000
[2019-03-27 15:38:45,293] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2158899: loss 0.0730
[2019-03-27 15:38:45,296] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2158900: learning rate 0.0000
[2019-03-27 15:38:45,697] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2159051: loss 0.0400
[2019-03-27 15:38:45,704] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2159051: learning rate 0.0000
[2019-03-27 15:38:46,545] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2159374: loss 0.0521
[2019-03-27 15:38:46,549] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2159374: learning rate 0.0000
[2019-03-27 15:38:46,945] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7673535e-18 1.0000000e+00 2.5129718e-24 1.8228423e-19 9.0515997e-12], sum to 1.0000
[2019-03-27 15:38:46,957] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4121
[2019-03-27 15:38:46,968] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2010695.502628085 W.
[2019-03-27 15:38:46,975] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.13333333333333, 79.5, 1.0, 2.0, 0.7968718709554454, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.981048088804309, 6.9112, 168.9113387685273, 2010695.502628085, 1961143.339028578, 407959.0484304859], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1695000.0000, 
sim time next is 1695600.0000, 
raw observation next is [28.2, 79.0, 1.0, 2.0, 0.5850538764402062, 1.0, 1.0, 0.5850538764402062, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1635752.902200183, 1635752.902200184, 328794.6361107092], 
processed observation next is [1.0, 0.6521739130434783, 0.5355450236966824, 0.79, 1.0, 1.0, 0.5000649113737424, 1.0, 0.5, 0.5000649113737424, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.45437580616671747, 0.4543758061667178, 0.49073826285180483], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.85441387], dtype=float32), -0.13737667]. 
=============================================
[2019-03-27 15:38:48,739] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2160197: loss 0.0545
[2019-03-27 15:38:48,741] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2160197: learning rate 0.0000
[2019-03-27 15:38:49,473] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:38:49,485] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5383
[2019-03-27 15:38:49,493] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.35, 53.0, 1.0, 2.0, 0.373145159209325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561859.0580244237, 561859.0580244237, 171267.7089141779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1521000.0000, 
sim time next is 1521600.0000, 
raw observation next is [29.2, 53.33333333333334, 1.0, 2.0, 0.3717713452669654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561023.7390579762, 561023.7390579769, 171235.8826332963], 
processed observation next is [0.0, 0.6086956521739131, 0.5829383886255924, 0.5333333333333334, 1.0, 1.0, 0.24309800634574147, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15583992751610448, 0.15583992751610468, 0.2555759442288005], 
reward next is 0.7444, 
noisyNet noise sample is [array([1.1307039], dtype=float32), 0.22513624]. 
=============================================
[2019-03-27 15:38:51,780] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2161337: loss 0.0190
[2019-03-27 15:38:51,784] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2161338: learning rate 0.0000
[2019-03-27 15:38:53,097] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2161835: loss 0.0341
[2019-03-27 15:38:53,098] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2161835: learning rate 0.0000
[2019-03-27 15:38:54,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:38:54,400] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1889
[2019-03-27 15:38:54,407] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.86666666666667, 88.83333333333334, 1.0, 2.0, 0.3972205839056678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589641.9198454384, 589641.9198454384, 173491.0654401467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1425000.0000, 
sim time next is 1425600.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4029470154690746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594825.9957638703, 594825.9957638709, 173869.0295096684], 
processed observation next is [0.0, 0.5217391304347826, 0.3364928909952607, 0.89, 1.0, 1.0, 0.2806590547820176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16522944326774175, 0.16522944326774192, 0.2595060141935349], 
reward next is 0.7405, 
noisyNet noise sample is [array([0.81410617], dtype=float32), -0.21326274]. 
=============================================
[2019-03-27 15:38:54,672] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2162425: loss 0.0694
[2019-03-27 15:38:54,674] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2162426: learning rate 0.0000
[2019-03-27 15:38:55,270] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2162657: loss 0.0513
[2019-03-27 15:38:55,275] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2162658: learning rate 0.0000
[2019-03-27 15:38:55,374] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2162699: loss 0.0079
[2019-03-27 15:38:55,377] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2162699: learning rate 0.0000
[2019-03-27 15:38:56,397] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2163078: loss 0.0780
[2019-03-27 15:38:56,402] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2163079: learning rate 0.0000
[2019-03-27 15:38:58,033] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2163692: loss 0.0144
[2019-03-27 15:38:58,036] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2163693: learning rate 0.0000
[2019-03-27 15:38:59,499] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135000, global step 2164247: loss 0.0285
[2019-03-27 15:38:59,503] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135000, global step 2164248: learning rate 0.0000
[2019-03-27 15:38:59,691] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2164316: loss 0.0085
[2019-03-27 15:38:59,695] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2164319: learning rate 0.0000
[2019-03-27 15:39:00,416] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2164592: loss 0.0335
[2019-03-27 15:39:00,420] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2164592: learning rate 0.0000
[2019-03-27 15:39:02,497] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135500, global step 2165372: loss 0.0171
[2019-03-27 15:39:02,499] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135500, global step 2165372: learning rate 0.0000
[2019-03-27 15:39:04,065] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2165960: loss 0.0055
[2019-03-27 15:39:04,069] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2165960: learning rate 0.0000
[2019-03-27 15:39:06,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5899141e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1290105e-37], sum to 1.0000
[2019-03-27 15:39:06,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6701
[2019-03-27 15:39:06,574] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.01666666666667, 91.66666666666667, 1.0, 2.0, 0.8180472116404325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1189346.0457431, 1189346.0457431, 254830.9724059374], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1847400.0000, 
sim time next is 1848000.0000, 
raw observation next is [24.13333333333333, 91.33333333333334, 1.0, 2.0, 0.8218425929990477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1191510.566723731, 1191510.566723732, 255367.1571649751], 
processed observation next is [1.0, 0.391304347826087, 0.3428120063191152, 0.9133333333333334, 1.0, 1.0, 0.7853525216855997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33097515742325867, 0.3309751574232589, 0.3811450106939927], 
reward next is 0.6189, 
noisyNet noise sample is [array([-0.5003685], dtype=float32), -0.6012265]. 
=============================================
[2019-03-27 15:39:06,608] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.51005 ]
 [70.27064 ]
 [70.16464 ]
 [70.369156]
 [70.402725]], R is [[70.39881897]
 [70.31448364]
 [70.19827271]
 [70.08368683]
 [69.97913361]].
[2019-03-27 15:39:06,722] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2166974: loss 0.0205
[2019-03-27 15:39:06,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2166978: learning rate 0.0000
[2019-03-27 15:39:06,780] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2166997: loss 0.0123
[2019-03-27 15:39:06,782] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2166997: learning rate 0.0000
[2019-03-27 15:39:07,835] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2167394: loss 0.0175
[2019-03-27 15:39:07,842] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2167395: learning rate 0.0000
[2019-03-27 15:39:09,917] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2168172: loss 0.0321
[2019-03-27 15:39:09,919] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2168172: learning rate 0.0000
[2019-03-27 15:39:13,129] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136000, global step 2169376: loss 0.0239
[2019-03-27 15:39:13,132] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136000, global step 2169376: learning rate 0.0000
[2019-03-27 15:39:14,115] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2169743: loss 0.0186
[2019-03-27 15:39:14,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2169743: learning rate 0.0000
[2019-03-27 15:39:15,754] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2170368: loss 0.0104
[2019-03-27 15:39:15,758] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2170369: learning rate 0.0000
[2019-03-27 15:39:16,361] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2170595: loss 0.0061
[2019-03-27 15:39:16,364] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2170595: learning rate 0.0000
[2019-03-27 15:39:16,714] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136000, global step 2170728: loss 0.0316
[2019-03-27 15:39:16,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136000, global step 2170728: learning rate 0.0000
[2019-03-27 15:39:17,465] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2171012: loss 0.0265
[2019-03-27 15:39:17,468] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2171012: learning rate 0.0000
[2019-03-27 15:39:19,018] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2171596: loss 0.0149
[2019-03-27 15:39:19,021] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2171596: learning rate 0.0000
[2019-03-27 15:39:20,690] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135500, global step 2172219: loss 0.0096
[2019-03-27 15:39:20,696] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135500, global step 2172219: learning rate 0.0000
[2019-03-27 15:39:20,837] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136000, global step 2172278: loss 0.0263
[2019-03-27 15:39:20,845] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136000, global step 2172279: learning rate 0.0000
[2019-03-27 15:39:21,359] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2172475: loss 0.0276
[2019-03-27 15:39:21,363] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2172475: learning rate 0.0000
[2019-03-27 15:39:23,793] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136000, global step 2173388: loss 0.0155
[2019-03-27 15:39:23,797] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136000, global step 2173389: learning rate 0.0000
[2019-03-27 15:39:25,369] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136000, global step 2173985: loss 0.0195
[2019-03-27 15:39:25,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136000, global step 2173986: learning rate 0.0000
[2019-03-27 15:39:28,096] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136000, global step 2174999: loss 0.0102
[2019-03-27 15:39:28,099] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 15:39:28,099] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136000, global step 2175000: learning rate 0.0000
[2019-03-27 15:39:28,101] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:39:28,102] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:39:28,104] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:39:28,105] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:39:28,105] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:39:28,107] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:39:28,108] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:39:28,103] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:39:28,109] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:39:28,115] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:39:28,138] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run88
[2019-03-27 15:39:28,162] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run88
[2019-03-27 15:39:28,188] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run88
[2019-03-27 15:39:28,213] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run88
[2019-03-27 15:39:28,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run88
[2019-03-27 15:39:37,117] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06655202], dtype=float32), 0.09930341]
[2019-03-27 15:39:37,118] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.76775179333333, 80.11694573666666, 1.0, 2.0, 0.2042114873708143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 341597.7569401355, 341597.7569401355, 143645.7568947916]
[2019-03-27 15:39:37,118] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:39:37,119] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6675638741913791
[2019-03-27 15:40:03,923] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06655202], dtype=float32), 0.09930341]
[2019-03-27 15:40:03,925] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.51666666666667, 91.33333333333334, 1.0, 2.0, 0.5313139170014843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742443.9412922456, 742443.9412922449, 188647.7379852303]
[2019-03-27 15:40:03,926] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:40:03,928] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2736016e-36], sampled 0.9447775690151793
[2019-03-27 15:40:36,215] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06655202], dtype=float32), 0.09930341]
[2019-03-27 15:40:36,216] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.25, 78.5, 1.0, 2.0, 0.5665172074837314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 791654.4148218458, 791654.4148218451, 194673.38113685]
[2019-03-27 15:40:36,217] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:40:36,221] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9241735702296908
[2019-03-27 15:40:39,422] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06655202], dtype=float32), 0.09930341]
[2019-03-27 15:40:39,423] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.00093156333333, 59.57509354333334, 1.0, 2.0, 0.5369207160458916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750281.5001921565, 750281.5001921572, 189582.7386417212]
[2019-03-27 15:40:39,425] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:40:39,428] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5421704e-38], sampled 0.29728049321605754
[2019-03-27 15:41:04,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06655202], dtype=float32), 0.09930341]
[2019-03-27 15:41:04,543] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.08333333333334, 94.16666666666667, 1.0, 2.0, 0.5084645254834032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710504.1308242374, 710504.1308242374, 184931.8795463392]
[2019-03-27 15:41:04,544] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:41:04,546] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7647297e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.6042719e-32], sampled 0.8880737925873079
[2019-03-27 15:41:28,432] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06655202], dtype=float32), 0.09930341]
[2019-03-27 15:41:28,434] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.70665407, 92.85674494, 1.0, 2.0, 0.2791233600042766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 453999.4890399021, 453999.4890399027, 163823.5088669572]
[2019-03-27 15:41:28,436] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:41:28,438] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2864459934230317
[2019-03-27 15:41:36,617] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5710 2927421443.5046 1345.0000
[2019-03-27 15:41:36,718] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.8211 2779258879.9240 934.0000
[2019-03-27 15:41:37,038] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.0752 2842466405.4694 1148.0000
[2019-03-27 15:41:37,178] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.8687 3163988944.4174 1812.0000
[2019-03-27 15:41:37,261] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0742 3007634930.1783 1767.0000
[2019-03-27 15:41:38,280] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2175000, evaluation results [2175000.0, 7881.8687368819965, 3163988944.4173646, 1812.0, 8253.571030888, 2927421443.504579, 1345.0, 8660.821108101049, 2779258879.9239545, 934.0, 7998.074184278494, 3007634930.17833, 1767.0, 8494.075187376231, 2842466405.4694185, 1148.0]
[2019-03-27 15:41:38,440] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136000, global step 2175068: loss 0.0191
[2019-03-27 15:41:38,446] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136000, global step 2175070: learning rate 0.0000
[2019-03-27 15:41:39,357] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136000, global step 2175414: loss 0.0173
[2019-03-27 15:41:39,358] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136000, global step 2175414: learning rate 0.0000
[2019-03-27 15:41:41,276] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136000, global step 2176137: loss 0.0095
[2019-03-27 15:41:41,277] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136000, global step 2176137: learning rate 0.0000
[2019-03-27 15:41:45,261] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136500, global step 2177620: loss 0.8023
[2019-03-27 15:41:45,262] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136500, global step 2177620: learning rate 0.0000
[2019-03-27 15:41:45,445] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136000, global step 2177691: loss 0.0166
[2019-03-27 15:41:45,451] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136000, global step 2177692: learning rate 0.0000
[2019-03-27 15:41:47,017] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136000, global step 2178287: loss 0.0269
[2019-03-27 15:41:47,021] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136000, global step 2178288: learning rate 0.0000
[2019-03-27 15:41:47,630] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7943873e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4274282e-35], sum to 1.0000
[2019-03-27 15:41:47,643] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3095
[2019-03-27 15:41:47,649] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 83.66666666666666, 1.0, 2.0, 0.539062313220602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753275.1832972945, 753275.1832972952, 189942.7619957579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2238600.0000, 
sim time next is 2239200.0000, 
raw observation next is [27.8, 84.0, 1.0, 2.0, 0.5383594888326082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752292.7223970952, 752292.7223970952, 189824.4975520402], 
processed observation next is [1.0, 0.9565217391304348, 0.5165876777251186, 0.84, 1.0, 1.0, 0.4438066130513352, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20897020066585978, 0.20897020066585978, 0.28332014560006], 
reward next is 0.7167, 
noisyNet noise sample is [array([-0.37966636], dtype=float32), 0.77015334]. 
=============================================
[2019-03-27 15:41:47,775] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136000, global step 2178567: loss 0.0358
[2019-03-27 15:41:47,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136000, global step 2178567: learning rate 0.0000
[2019-03-27 15:41:48,681] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136000, global step 2178912: loss 0.0087
[2019-03-27 15:41:48,682] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136000, global step 2178912: learning rate 0.0000
[2019-03-27 15:41:48,697] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136500, global step 2178916: loss 0.9241
[2019-03-27 15:41:48,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136500, global step 2178916: learning rate 0.0000
[2019-03-27 15:41:50,207] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136000, global step 2179478: loss 0.0116
[2019-03-27 15:41:50,212] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136000, global step 2179479: learning rate 0.0000
[2019-03-27 15:41:51,969] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136000, global step 2180139: loss 0.0104
[2019-03-27 15:41:51,974] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136000, global step 2180141: learning rate 0.0000
[2019-03-27 15:41:52,503] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136000, global step 2180340: loss 0.0032
[2019-03-27 15:41:52,506] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136000, global step 2180341: learning rate 0.0000
[2019-03-27 15:41:53,002] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136500, global step 2180527: loss 0.6305
[2019-03-27 15:41:53,004] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136500, global step 2180527: learning rate 0.0000
[2019-03-27 15:41:55,569] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136500, global step 2181484: loss 0.4013
[2019-03-27 15:41:55,571] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136500, global step 2181484: learning rate 0.0000
[2019-03-27 15:41:57,356] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136500, global step 2182170: loss 0.3015
[2019-03-27 15:41:57,358] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136500, global step 2182170: learning rate 0.0000
[2019-03-27 15:41:59,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.2075290e-09 5.3655796e-05 1.9868749e-16 1.6749832e-06 9.9994469e-01], sum to 1.0000
[2019-03-27 15:41:59,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9178
[2019-03-27 15:41:59,510] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.06666666666667, 65.33333333333334, 1.0, 2.0, 0.5412958305224186, 1.0, 2.0, 0.5412958305224186, 1.0, 2.0, 0.9400518483508654, 6.911200000000001, 6.9112, 170.5573041426782, 2270778.056174424, 2270778.056174424, 444877.8901533462], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2211600.0000, 
sim time next is 2212200.0000, 
raw observation next is [32.05, 65.5, 1.0, 2.0, 0.5472662807054507, 1.0, 2.0, 0.5472662807054507, 1.0, 2.0, 0.9504205458607451, 6.9112, 6.9112, 170.5573041426782, 2295847.564565038, 2295847.564565038, 449370.7372180774], 
processed observation next is [1.0, 0.6086956521739131, 0.7180094786729857, 0.655, 1.0, 1.0, 0.45453768759692853, 1.0, 1.0, 0.45453768759692853, 1.0, 1.0, 0.9395372510496891, 0.0, 0.0, 0.8375144448122397, 0.6377354346013994, 0.6377354346013994, 0.6707025928628021], 
reward next is 0.3293, 
noisyNet noise sample is [array([-1.4217551], dtype=float32), -0.3930355]. 
=============================================
[2019-03-27 15:41:59,828] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136500, global step 2183089: loss 0.2296
[2019-03-27 15:41:59,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136500, global step 2183091: learning rate 0.0000
[2019-03-27 15:41:59,927] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136500, global step 2183125: loss 0.3220
[2019-03-27 15:41:59,929] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136500, global step 2183125: learning rate 0.0000
[2019-03-27 15:42:00,727] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136500, global step 2183432: loss 0.3055
[2019-03-27 15:42:00,733] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136500, global step 2183433: learning rate 0.0000
[2019-03-27 15:42:01,238] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6311549e-30 1.0000000e+00 7.4995803e-37 0.0000000e+00 1.0179175e-27], sum to 1.0000
[2019-03-27 15:42:01,251] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8622
[2019-03-27 15:42:01,258] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.03333333333333, 81.66666666666667, 1.0, 2.0, 0.8144438761476739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1138294.243665228, 1138294.243665228, 247487.0289644283], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2432400.0000, 
sim time next is 2433000.0000, 
raw observation next is [27.96666666666667, 81.83333333333334, 1.0, 2.0, 0.7973981488146628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1114458.05784803, 1114458.057848029, 243272.9607770369], 
processed observation next is [1.0, 0.13043478260869565, 0.524486571879937, 0.8183333333333335, 1.0, 1.0, 0.7559013841140515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3095716827355639, 0.3095716827355636, 0.3630939713090103], 
reward next is 0.6369, 
noisyNet noise sample is [array([0.17687836], dtype=float32), 2.5680308]. 
=============================================
[2019-03-27 15:42:01,277] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[58.547073]
 [58.56291 ]
 [58.7929  ]
 [58.74806 ]
 [58.343887]], R is [[58.35140228]
 [58.39850235]
 [58.43725967]
 [58.4705162 ]
 [58.48089218]].
[2019-03-27 15:42:02,591] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136500, global step 2184126: loss 0.1323
[2019-03-27 15:42:02,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136500, global step 2184126: learning rate 0.0000
[2019-03-27 15:42:06,469] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137000, global step 2185590: loss 0.0170
[2019-03-27 15:42:06,470] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137000, global step 2185590: learning rate 0.0000
[2019-03-27 15:42:06,560] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136500, global step 2185624: loss 0.0274
[2019-03-27 15:42:06,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136500, global step 2185624: learning rate 0.0000
[2019-03-27 15:42:08,140] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136500, global step 2186221: loss 0.0615
[2019-03-27 15:42:08,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136500, global step 2186223: learning rate 0.0000
[2019-03-27 15:42:09,017] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136500, global step 2186549: loss 0.0592
[2019-03-27 15:42:09,019] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136500, global step 2186550: learning rate 0.0000
[2019-03-27 15:42:09,852] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136500, global step 2186860: loss 0.2432
[2019-03-27 15:42:09,852] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136500, global step 2186860: learning rate 0.0000
[2019-03-27 15:42:09,899] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137000, global step 2186880: loss 0.0128
[2019-03-27 15:42:09,902] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137000, global step 2186880: learning rate 0.0000
[2019-03-27 15:42:11,314] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9734419e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.1637895e-29], sum to 1.0000
[2019-03-27 15:42:11,324] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0284
[2019-03-27 15:42:11,330] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.96666666666667, 77.0, 1.0, 2.0, 0.5022919807211318, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701876.0610104074, 701876.0610104068, 183958.6539877447], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2568000.0000, 
sim time next is 2568600.0000, 
raw observation next is [28.9, 77.5, 1.0, 2.0, 0.4997078685267168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698263.9734801159, 698263.9734801159, 183553.2057726097], 
processed observation next is [1.0, 0.7391304347826086, 0.5687203791469194, 0.775, 1.0, 1.0, 0.3972383958153214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19396221485558773, 0.19396221485558773, 0.2739600086158354], 
reward next is 0.7260, 
noisyNet noise sample is [array([-0.08395709], dtype=float32), 0.99712783]. 
=============================================
[2019-03-27 15:42:11,371] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136500, global step 2187430: loss 0.4833
[2019-03-27 15:42:11,375] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136500, global step 2187431: learning rate 0.0000
[2019-03-27 15:42:13,143] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136500, global step 2188095: loss 0.2967
[2019-03-27 15:42:13,149] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136500, global step 2188096: learning rate 0.0000
[2019-03-27 15:42:13,701] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136500, global step 2188301: loss 0.0227
[2019-03-27 15:42:13,702] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136500, global step 2188301: learning rate 0.0000
[2019-03-27 15:42:14,169] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137000, global step 2188477: loss 0.0056
[2019-03-27 15:42:14,172] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137000, global step 2188478: learning rate 0.0000
[2019-03-27 15:42:16,538] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137000, global step 2189378: loss 0.0224
[2019-03-27 15:42:16,540] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137000, global step 2189379: learning rate 0.0000
[2019-03-27 15:42:16,933] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:42:16,947] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1553
[2019-03-27 15:42:16,951] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 91.0, 1.0, 2.0, 0.4390332765670147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632769.1165571843, 632769.1165571848, 177076.3524584505], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2615400.0000, 
sim time next is 2616000.0000, 
raw observation next is [24.56666666666666, 90.33333333333333, 1.0, 2.0, 0.444090748929569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636703.5922903854, 636703.5922903861, 177381.4598196742], 
processed observation next is [0.0, 0.2608695652173913, 0.3633491311216427, 0.9033333333333333, 1.0, 1.0, 0.3302298179874325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17686210896955148, 0.17686210896955168, 0.26474844749205106], 
reward next is 0.7353, 
noisyNet noise sample is [array([0.54975295], dtype=float32), 0.9727909]. 
=============================================
[2019-03-27 15:42:16,970] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.65935]
 [77.68388]
 [77.70136]
 [77.70691]
 [77.57785]], R is [[77.59011841]
 [77.54992676]
 [77.51062775]
 [77.47231293]
 [77.43480682]].
[2019-03-27 15:42:17,925] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.740543e-37 1.000000e+00 0.000000e+00 0.000000e+00 6.891624e-35], sum to 1.0000
[2019-03-27 15:42:17,937] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6509
[2019-03-27 15:42:17,945] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4059744611742992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598168.7936836937, 598168.793683693, 174143.7409613761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2828400.0000, 
sim time next is 2829000.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4040913762003742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595394.5450848016, 595394.545084801, 173887.1380199762], 
processed observation next is [1.0, 0.7391304347826086, 0.3364928909952607, 0.89, 1.0, 1.0, 0.28203780265105327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16538737363466713, 0.16538737363466696, 0.25953304182086], 
reward next is 0.7405, 
noisyNet noise sample is [array([0.3263743], dtype=float32), -0.21046948]. 
=============================================
[2019-03-27 15:42:17,958] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.36964]
 [77.15438]
 [76.84442]
 [76.07769]
 [74.31577]], R is [[77.48143005]
 [77.44670105]
 [77.41272736]
 [77.3789444 ]
 [77.32806396]].
[2019-03-27 15:42:18,644] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137000, global step 2190168: loss 0.0160
[2019-03-27 15:42:18,648] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137000, global step 2190168: learning rate 0.0000
[2019-03-27 15:42:21,228] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137000, global step 2191137: loss 0.0213
[2019-03-27 15:42:21,231] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137000, global step 2191138: learning rate 0.0000
[2019-03-27 15:42:21,460] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137000, global step 2191227: loss 0.0067
[2019-03-27 15:42:21,464] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137000, global step 2191227: learning rate 0.0000
[2019-03-27 15:42:21,698] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:42:21,711] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4854
[2019-03-27 15:42:21,718] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.47593610972697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 665036.2425720012, 665036.2425720006, 179911.5940921991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2702400.0000, 
sim time next is 2703000.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4759084983839051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664997.6485445339, 664997.6485445339, 179907.4664712944], 
processed observation next is [0.0, 0.2608695652173913, 0.3364928909952607, 1.0, 1.0, 1.0, 0.36856445588422304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1847215690401483, 0.1847215690401483, 0.26851860667357375], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.18637587], dtype=float32), -1.5782037]. 
=============================================
[2019-03-27 15:42:21,732] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.82652 ]
 [75.850105]
 [75.87718 ]
 [75.90473 ]
 [75.92714 ]], R is [[75.76689148]
 [75.74069977]
 [75.71476746]
 [75.68910217]
 [75.66371155]].
[2019-03-27 15:42:22,406] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137000, global step 2191575: loss 0.0053
[2019-03-27 15:42:22,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137000, global step 2191575: learning rate 0.0000
[2019-03-27 15:42:23,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3846868e-07 6.1398178e-02 1.3730672e-13 9.7435520e-07 9.3860072e-01], sum to 1.0000
[2019-03-27 15:42:23,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9659
[2019-03-27 15:42:23,197] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.56666666666667, 85.66666666666667, 1.0, 2.0, 0.4894997763691125, 1.0, 2.0, 0.4894997763691125, 1.0, 1.0, 0.8453223995052765, 6.911200000000001, 6.9112, 170.5573041426782, 2053289.287826199, 2053289.287826198, 407118.3599293994], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2540400.0000, 
sim time next is 2541000.0000, 
raw observation next is [27.68333333333333, 84.83333333333333, 1.0, 2.0, 0.4953097902799819, 1.0, 2.0, 0.4953097902799819, 1.0, 2.0, 0.855934652579235, 6.9112, 6.9112, 170.5573041426782, 2077684.003915817, 2077684.003915817, 411157.6828857971], 
processed observation next is [1.0, 0.391304347826087, 0.5110584518167456, 0.8483333333333333, 1.0, 1.0, 0.39193950636142394, 1.0, 1.0, 0.39193950636142394, 1.0, 1.0, 0.8243105519258962, 0.0, 0.0, 0.8375144448122397, 0.5771344455321714, 0.5771344455321714, 0.6136681834116375], 
reward next is 0.3863, 
noisyNet noise sample is [array([0.5976558], dtype=float32), -1.2607166]. 
=============================================
[2019-03-27 15:42:23,213] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[55.224304]
 [55.26666 ]
 [54.74575 ]
 [54.28016 ]
 [54.957035]], R is [[55.56625366]
 [55.01059341]
 [54.89840698]
 [54.77323151]
 [54.2254982 ]].
[2019-03-27 15:42:23,798] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137000, global step 2192089: loss 0.0138
[2019-03-27 15:42:23,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137000, global step 2192090: learning rate 0.0000
[2019-03-27 15:42:27,233] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137500, global step 2193384: loss 0.0319
[2019-03-27 15:42:27,235] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137500, global step 2193386: learning rate 0.0000
[2019-03-27 15:42:28,007] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137000, global step 2193675: loss 0.0082
[2019-03-27 15:42:28,014] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137000, global step 2193675: learning rate 0.0000
[2019-03-27 15:42:29,723] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137000, global step 2194320: loss 0.0030
[2019-03-27 15:42:29,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137000, global step 2194320: learning rate 0.0000
[2019-03-27 15:42:30,561] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137000, global step 2194638: loss 0.0041
[2019-03-27 15:42:30,565] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137000, global step 2194639: learning rate 0.0000
[2019-03-27 15:42:30,695] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:42:30,705] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2433
[2019-03-27 15:42:30,713] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3041065107829917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484272.064722271, 484272.064722271, 165928.8890831842], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3011400.0000, 
sim time next is 3012000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3044939415586104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484889.0046768906, 484889.0046768906, 165973.4332609786], 
processed observation next is [1.0, 0.8695652173913043, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16204089344410894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13469139018802517, 0.13469139018802517, 0.24772154218056508], 
reward next is 0.7523, 
noisyNet noise sample is [array([-0.8305105], dtype=float32), -0.0089506535]. 
=============================================
[2019-03-27 15:42:30,724] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.20093 ]
 [81.20751 ]
 [81.16859 ]
 [81.128746]
 [80.95803 ]], R is [[81.13272095]
 [81.0737381 ]
 [81.0153656 ]
 [80.95749664]
 [80.90007019]].
[2019-03-27 15:42:30,724] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137500, global step 2194694: loss 0.0197
[2019-03-27 15:42:30,731] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137500, global step 2194695: learning rate 0.0000
[2019-03-27 15:42:30,934] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:42:30,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6399
[2019-03-27 15:42:30,954] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 95.0, 1.0, 2.0, 0.3585729966423505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549726.1945690847, 549726.1945690847, 170547.4506901155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2753400.0000, 
sim time next is 2754000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3533609519993398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 543689.7888835866, 543689.788883586, 170102.0400789446], 
processed observation next is [0.0, 0.9130434782608695, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2209168096377588, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15102494135655184, 0.15102494135655165, 0.2538836419088726], 
reward next is 0.7461, 
noisyNet noise sample is [array([0.09906907], dtype=float32), -0.8937836]. 
=============================================
[2019-03-27 15:42:30,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.75647 ]
 [79.76375 ]
 [79.76601 ]
 [79.762024]
 [79.74155 ]], R is [[79.82498169]
 [79.77218628]
 [79.71923828]
 [79.66615295]
 [79.61305237]].
[2019-03-27 15:42:31,076] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:42:31,092] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6192
[2019-03-27 15:42:31,097] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.43333333333334, 91.33333333333334, 1.0, 2.0, 0.5765178304463094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 885489.105802712, 885489.105802712, 206113.2114519725], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2888400.0000, 
sim time next is 2889000.0000, 
raw observation next is [22.5, 91.0, 1.0, 2.0, 0.5827558170493199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 894439.6703646185, 894439.6703646185, 207286.7940517343], 
processed observation next is [1.0, 0.43478260869565216, 0.2654028436018958, 0.91, 1.0, 1.0, 0.4972961651196625, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24845546399017182, 0.24845546399017182, 0.30938327470408106], 
reward next is 0.6906, 
noisyNet noise sample is [array([1.0555658], dtype=float32), 0.09294363]. 
=============================================
[2019-03-27 15:42:31,111] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.16501 ]
 [76.06129 ]
 [76.039986]
 [76.00878 ]
 [75.96649 ]], R is [[76.08152771]
 [76.01308441]
 [75.93767548]
 [75.86656952]
 [75.81222534]].
[2019-03-27 15:42:31,529] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137000, global step 2194997: loss 0.0203
[2019-03-27 15:42:31,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137000, global step 2194997: learning rate 0.0000
[2019-03-27 15:42:32,993] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137000, global step 2195540: loss 0.0065
[2019-03-27 15:42:33,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137000, global step 2195544: learning rate 0.0000
[2019-03-27 15:42:35,058] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137000, global step 2196315: loss 0.0125
[2019-03-27 15:42:35,061] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137000, global step 2196315: learning rate 0.0000
[2019-03-27 15:42:35,067] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137500, global step 2196318: loss 0.0165
[2019-03-27 15:42:35,069] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137500, global step 2196319: learning rate 0.0000
[2019-03-27 15:42:35,753] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137000, global step 2196576: loss 0.0058
[2019-03-27 15:42:35,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137000, global step 2196576: learning rate 0.0000
[2019-03-27 15:42:37,717] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137500, global step 2197318: loss 0.0292
[2019-03-27 15:42:37,718] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137500, global step 2197318: learning rate 0.0000
[2019-03-27 15:42:39,775] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137500, global step 2198083: loss 0.0185
[2019-03-27 15:42:39,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137500, global step 2198083: learning rate 0.0000
[2019-03-27 15:42:42,411] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137500, global step 2199073: loss 0.0141
[2019-03-27 15:42:42,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137500, global step 2199073: learning rate 0.0000
[2019-03-27 15:42:42,908] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137500, global step 2199263: loss 0.0096
[2019-03-27 15:42:42,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137500, global step 2199263: learning rate 0.0000
[2019-03-27 15:42:42,973] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:42:42,984] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0722
[2019-03-27 15:42:42,992] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3044939415586104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484889.0046768906, 484889.0046768906, 165973.4332609786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3046781921487323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485182.2398720158, 485182.2398720158, 165994.6212026615], 
processed observation next is [1.0, 0.8695652173913043, 0.1469194312796209, 1.0, 1.0, 1.0, 0.1622628821069064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13477284440889328, 0.13477284440889328, 0.24775316597412167], 
reward next is 0.7522, 
noisyNet noise sample is [array([-0.7536276], dtype=float32), 1.0478859]. 
=============================================
[2019-03-27 15:42:43,596] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137500, global step 2199516: loss 0.0089
[2019-03-27 15:42:43,599] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137500, global step 2199517: learning rate 0.0000
[2019-03-27 15:42:44,891] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 15:42:44,895] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:42:44,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:42:44,896] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:42:44,897] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:42:44,897] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:42:44,898] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:42:44,900] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:42:44,902] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:42:44,902] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:42:44,899] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:42:44,934] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run89
[2019-03-27 15:42:44,956] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run89
[2019-03-27 15:42:44,994] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run89
[2019-03-27 15:42:44,995] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run89
[2019-03-27 15:42:45,036] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run89
[2019-03-27 15:43:01,613] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06628586], dtype=float32), 0.09975221]
[2019-03-27 15:43:01,614] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.05, 92.0, 1.0, 2.0, 0.3069399753827529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 487461.1029079882, 487461.1029079889, 166138.9938259644]
[2019-03-27 15:43:01,614] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:43:01,616] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06010712509872107
[2019-03-27 15:44:17,138] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06628586], dtype=float32), 0.09975221]
[2019-03-27 15:44:17,142] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.16666666666666, 86.33333333333333, 1.0, 2.0, 0.6428329486795274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 898343.5717615067, 898343.5717615074, 208997.3445512358]
[2019-03-27 15:44:17,144] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:44:17,148] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19771522268567843
[2019-03-27 15:44:18,574] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06628586], dtype=float32), 0.09975221]
[2019-03-27 15:44:18,575] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.5, 70.0, 1.0, 2.0, 0.8799148297887841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1229851.651994917, 1229851.651994916, 264478.7335297376]
[2019-03-27 15:44:18,577] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:44:18,582] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3232089e-30 1.0000000e+00 1.7246263e-36 0.0000000e+00 5.1613502e-28], sampled 0.3962951844278051
[2019-03-27 15:44:31,159] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06628586], dtype=float32), 0.09975221]
[2019-03-27 15:44:31,160] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.7, 85.0, 1.0, 2.0, 0.7153688190441067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 999758.416846588, 999758.416846588, 224198.0104722572]
[2019-03-27 15:44:31,161] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:44:31,167] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9561327183545083
[2019-03-27 15:44:52,944] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.2956 2927714677.7411 1359.0000
[2019-03-27 15:44:53,573] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06628586], dtype=float32), 0.09975221]
[2019-03-27 15:44:53,574] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.95, 70.5, 1.0, 2.0, 0.3346100301591377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520079.4529109856, 520079.4529109863, 168350.3490281597]
[2019-03-27 15:44:53,574] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:44:53,576] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0735751671291911
[2019-03-27 15:44:53,642] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8510.1476 2841646201.1519 1125.0000
[2019-03-27 15:44:53,699] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7892.2855 3163188197.6356 1875.0000
[2019-03-27 15:44:53,749] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.9386 2779460488.7125 931.0000
[2019-03-27 15:44:53,870] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8041.3443 3005368253.1388 1656.0000
[2019-03-27 15:44:54,889] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2200000, evaluation results [2200000.0, 7892.2854636156535, 3163188197.6356444, 1875.0, 8252.29560140311, 2927714677.7410593, 1359.0, 8661.938640828937, 2779460488.7124853, 931.0, 8041.344334312575, 3005368253.1387963, 1656.0, 8510.147642895032, 2841646201.1518936, 1125.0]
[2019-03-27 15:44:55,005] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137500, global step 2200050: loss 0.0103
[2019-03-27 15:44:55,008] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137500, global step 2200051: learning rate 0.0000
[2019-03-27 15:44:58,373] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138000, global step 2201323: loss 0.0093
[2019-03-27 15:44:58,374] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138000, global step 2201323: learning rate 0.0000
[2019-03-27 15:44:59,318] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137500, global step 2201675: loss 0.0090
[2019-03-27 15:44:59,325] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137500, global step 2201675: learning rate 0.0000
[2019-03-27 15:45:01,138] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137500, global step 2202369: loss 0.0121
[2019-03-27 15:45:01,143] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137500, global step 2202369: learning rate 0.0000
[2019-03-27 15:45:01,880] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137500, global step 2202647: loss 0.0261
[2019-03-27 15:45:01,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137500, global step 2202648: learning rate 0.0000
[2019-03-27 15:45:02,124] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138000, global step 2202739: loss 0.0088
[2019-03-27 15:45:02,129] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138000, global step 2202741: learning rate 0.0000
[2019-03-27 15:45:03,008] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137500, global step 2203072: loss 0.0082
[2019-03-27 15:45:03,010] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137500, global step 2203072: learning rate 0.0000
[2019-03-27 15:45:04,227] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137500, global step 2203528: loss 0.0106
[2019-03-27 15:45:04,231] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137500, global step 2203530: learning rate 0.0000
[2019-03-27 15:45:06,290] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137500, global step 2204303: loss 0.0074
[2019-03-27 15:45:06,293] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137500, global step 2204303: learning rate 0.0000
[2019-03-27 15:45:06,400] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138000, global step 2204343: loss 0.0039
[2019-03-27 15:45:06,403] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138000, global step 2204343: learning rate 0.0000
[2019-03-27 15:45:06,832] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137500, global step 2204506: loss 0.0078
[2019-03-27 15:45:06,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137500, global step 2204506: learning rate 0.0000
[2019-03-27 15:45:08,943] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138000, global step 2205300: loss 0.0065
[2019-03-27 15:45:08,945] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138000, global step 2205300: learning rate 0.0000
[2019-03-27 15:45:10,856] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138000, global step 2206022: loss 0.0029
[2019-03-27 15:45:10,859] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138000, global step 2206022: learning rate 0.0000
[2019-03-27 15:45:11,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4421937e-37], sum to 1.0000
[2019-03-27 15:45:11,018] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9758
[2019-03-27 15:45:11,024] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.540977909640847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807130.6990653153, 807130.6990653153, 196616.662729004], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3070200.0000, 
sim time next is 3070800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.4581022426458296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683473.5253452257, 683473.5253452263, 182696.7856311444], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.94, 1.0, 1.0, 0.347111135717867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18985375704034047, 0.18985375704034063, 0.272681769598723], 
reward next is 0.7273, 
noisyNet noise sample is [array([-0.02898945], dtype=float32), -1.0571707]. 
=============================================
[2019-03-27 15:45:13,255] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.023121e-36], sum to 1.0000
[2019-03-27 15:45:13,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6455
[2019-03-27 15:45:13,278] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 93.16666666666667, 1.0, 2.0, 0.4920327516193084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687535.7214298253, 687535.7214298247, 182357.0076422961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3181800.0000, 
sim time next is 3182400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4913679868666576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686606.5206404879, 686606.5206404879, 182254.3936055178], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.94, 1.0, 1.0, 0.387190345622479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19072403351124664, 0.19072403351124664, 0.27202148299331014], 
reward next is 0.7280, 
noisyNet noise sample is [array([1.1520427], dtype=float32), 1.3160756]. 
=============================================
[2019-03-27 15:45:13,839] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138000, global step 2207144: loss 0.0046
[2019-03-27 15:45:13,843] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138000, global step 2207144: learning rate 0.0000
[2019-03-27 15:45:14,104] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138000, global step 2207240: loss 0.0030
[2019-03-27 15:45:14,107] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138000, global step 2207241: learning rate 0.0000
[2019-03-27 15:45:14,704] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138000, global step 2207461: loss 0.0036
[2019-03-27 15:45:14,706] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138000, global step 2207462: learning rate 0.0000
[2019-03-27 15:45:16,249] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138000, global step 2208041: loss 0.0070
[2019-03-27 15:45:16,251] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138000, global step 2208041: learning rate 0.0000
[2019-03-27 15:45:20,032] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138500, global step 2209470: loss 1.1941
[2019-03-27 15:45:20,034] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138500, global step 2209470: learning rate 0.0000
[2019-03-27 15:45:20,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.382512e-32 1.000000e+00 6.751386e-38 0.000000e+00 6.384214e-30], sum to 1.0000
[2019-03-27 15:45:20,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2390
[2019-03-27 15:45:20,336] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.85, 90.0, 1.0, 2.0, 0.5371561813274274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750610.650584857, 750610.6505848564, 189622.3825395429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3371400.0000, 
sim time next is 3372000.0000, 
raw observation next is [26.8, 90.33333333333334, 1.0, 2.0, 0.5372417824557576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750730.3100555604, 750730.310055561, 189636.7238444064], 
processed observation next is [1.0, 0.0, 0.4691943127962086, 0.9033333333333334, 1.0, 1.0, 0.44245997886235855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20853619723765568, 0.20853619723765585, 0.28303988633493493], 
reward next is 0.7170, 
noisyNet noise sample is [array([1.4686196], dtype=float32), 0.07658159]. 
=============================================
[2019-03-27 15:45:20,353] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[65.45353 ]
 [65.637215]
 [66.32051 ]
 [69.16089 ]
 [74.98364 ]], R is [[65.54199219]
 [65.60355377]
 [65.66451263]
 [65.72480774]
 [65.7843399 ]].
[2019-03-27 15:45:20,454] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138000, global step 2209626: loss 0.0037
[2019-03-27 15:45:20,455] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138000, global step 2209627: learning rate 0.0000
[2019-03-27 15:45:22,429] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138000, global step 2210375: loss 0.0046
[2019-03-27 15:45:22,431] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138000, global step 2210375: learning rate 0.0000
[2019-03-27 15:45:22,909] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138000, global step 2210557: loss 0.0037
[2019-03-27 15:45:22,912] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138000, global step 2210557: learning rate 0.0000
[2019-03-27 15:45:23,808] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138500, global step 2210900: loss 0.0086
[2019-03-27 15:45:23,812] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138500, global step 2210900: learning rate 0.0000
[2019-03-27 15:45:24,271] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138000, global step 2211073: loss 0.0041
[2019-03-27 15:45:24,273] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138000, global step 2211074: learning rate 0.0000
[2019-03-27 15:45:25,195] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138000, global step 2211417: loss 0.0040
[2019-03-27 15:45:25,200] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138000, global step 2211417: learning rate 0.0000
[2019-03-27 15:45:26,199] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.3271823e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4027444e-36], sum to 1.0000
[2019-03-27 15:45:26,207] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6311
[2019-03-27 15:45:26,212] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.91666666666666, 79.33333333333334, 1.0, 2.0, 0.7438852858019134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1039630.902383026, 1039630.902383027, 230599.472486983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3553800.0000, 
sim time next is 3554400.0000, 
raw observation next is [26.83333333333334, 79.66666666666667, 1.0, 2.0, 0.6998548810922793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 978067.0431556643, 978067.0431556643, 220810.3659287074], 
processed observation next is [1.0, 0.13043478260869565, 0.4707740916271725, 0.7966666666666667, 1.0, 1.0, 0.6383793748099751, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2716852897654623, 0.2716852897654623, 0.32956771034135435], 
reward next is 0.6704, 
noisyNet noise sample is [array([0.10031594], dtype=float32), 1.8038747]. 
=============================================
[2019-03-27 15:45:27,266] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138000, global step 2212193: loss 0.0100
[2019-03-27 15:45:27,269] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138000, global step 2212193: learning rate 0.0000
[2019-03-27 15:45:27,831] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138000, global step 2212401: loss 0.0107
[2019-03-27 15:45:27,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138000, global step 2212401: learning rate 0.0000
[2019-03-27 15:45:27,976] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138500, global step 2212455: loss 0.0285
[2019-03-27 15:45:27,979] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138500, global step 2212456: learning rate 0.0000
[2019-03-27 15:45:30,384] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138500, global step 2213357: loss 0.0703
[2019-03-27 15:45:30,386] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138500, global step 2213357: learning rate 0.0000
[2019-03-27 15:45:32,361] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138500, global step 2214101: loss 0.0314
[2019-03-27 15:45:32,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138500, global step 2214102: learning rate 0.0000
[2019-03-27 15:45:35,507] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138500, global step 2215275: loss 0.0468
[2019-03-27 15:45:35,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138500, global step 2215275: learning rate 0.0000
[2019-03-27 15:45:35,575] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138500, global step 2215304: loss 0.0405
[2019-03-27 15:45:35,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138500, global step 2215304: learning rate 0.0000
[2019-03-27 15:45:35,728] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:45:35,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8801
[2019-03-27 15:45:35,744] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333333, 72.66666666666667, 1.0, 2.0, 0.5276683472450144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737347.9509567774, 737347.9509567774, 188044.7479910673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3613800.0000, 
sim time next is 3614400.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.525114077381621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733777.4579177726, 733777.457917772, 187624.4689086815], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.74, 1.0, 1.0, 0.42784828600195296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2038270716438257, 0.20382707164382555, 0.28003652075922614], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.55919874], dtype=float32), 1.5861449]. 
=============================================
[2019-03-27 15:45:36,126] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138500, global step 2215512: loss 0.2386
[2019-03-27 15:45:36,128] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138500, global step 2215513: learning rate 0.0000
[2019-03-27 15:45:36,935] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3609059e-09 2.5199288e-06 1.2277600e-16 1.3499074e-05 9.9998403e-01], sum to 1.0000
[2019-03-27 15:45:36,953] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3820
[2019-03-27 15:45:36,961] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 63.00000000000001, 1.0, 2.0, 0.753840040011612, 1.0, 2.0, 0.6975100595200685, 1.0, 2.0, 1.03, 7.005101977777136, 6.9112, 170.5573041426782, 2926876.468037909, 2859610.661667907, 538873.31380431], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3601200.0000, 
sim time next is 3601800.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.7520734029281471, 1.0, 2.0, 0.6966267409783361, 1.0, 2.0, 1.03, 7.005101838454649, 6.9112, 170.5573041426782, 2923165.568966598, 2855899.86239896, 538280.5330518797], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.63, 1.0, 1.0, 0.7012932565399362, 1.0, 1.0, 0.6344900493714892, 1.0, 1.0, 1.0365853658536586, 0.009390183845464861, 0.0, 0.8375144448122397, 0.811990435824055, 0.7933055173330444, 0.8034037806744474], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5754503], dtype=float32), -0.10541915]. 
=============================================
[2019-03-27 15:45:37,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8669289e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7295817e-34], sum to 1.0000
[2019-03-27 15:45:37,512] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8533
[2019-03-27 15:45:37,517] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.7734705947839404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1080999.454055136, 1080999.454055136, 237499.5673800751], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3640200.0000, 
sim time next is 3640800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.759871013071652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1061983.228120634, 1061983.228120635, 234295.6190430396], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7106879675562071, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29499534114462056, 0.29499534114462084, 0.34969495379558146], 
reward next is 0.6503, 
noisyNet noise sample is [array([-0.62993133], dtype=float32), -0.5416883]. 
=============================================
[2019-03-27 15:45:37,694] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138500, global step 2216095: loss 0.2062
[2019-03-27 15:45:37,698] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138500, global step 2216096: learning rate 0.0000
[2019-03-27 15:45:38,781] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1207581e-08 2.6925593e-06 1.4763934e-15 5.4358002e-06 9.9999189e-01], sum to 1.0000
[2019-03-27 15:45:38,790] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4792
[2019-03-27 15:45:38,795] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.5407347238907184, 1.0, 2.0, 0.5407347238907184, 1.0, 2.0, 0.9390773916924008, 6.911199999999999, 6.9112, 170.5573041426782, 2268422.033992746, 2268422.033992747, 444458.630583101], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3502800.0000, 
sim time next is 3503400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.5507223659253128, 1.0, 2.0, 0.5507223659253128, 1.0, 2.0, 0.956422622942798, 6.9112, 6.9112, 170.5573041426782, 2310359.657766395, 2310359.657766395, 451994.3452345281], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.63, 1.0, 1.0, 0.45870164569314786, 1.0, 1.0, 0.45870164569314786, 1.0, 1.0, 0.9468568572473145, 0.0, 0.0, 0.8375144448122397, 0.6417665716017763, 0.6417665716017763, 0.6746184257231763], 
reward next is 0.3254, 
noisyNet noise sample is [array([-0.7115913], dtype=float32), 0.60886246]. 
=============================================
[2019-03-27 15:45:41,097] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139000, global step 2217373: loss 0.0729
[2019-03-27 15:45:41,099] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139000, global step 2217373: learning rate 0.0000
[2019-03-27 15:45:41,694] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2757918e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2366038e-36], sum to 1.0000
[2019-03-27 15:45:41,705] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3565
[2019-03-27 15:45:41,711] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.41666666666667, 81.5, 1.0, 2.0, 0.7433220553183536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1038843.363983553, 1038843.363983553, 230470.2475516235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3557400.0000, 
sim time next is 3558000.0000, 
raw observation next is [26.33333333333334, 82.0, 1.0, 2.0, 0.6731278364712743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 940698.6977311402, 940698.6977311409, 215147.9812941711], 
processed observation next is [1.0, 0.17391304347826086, 0.44707740916271754, 0.82, 1.0, 1.0, 0.6061781162304509, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2613051938142056, 0.2613051938142058, 0.32111638999130016], 
reward next is 0.6789, 
noisyNet noise sample is [array([-0.7428132], dtype=float32), -0.093520515]. 
=============================================
[2019-03-27 15:45:41,728] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[57.406525]
 [57.85065 ]
 [57.806656]
 [57.66864 ]
 [57.74905 ]], R is [[57.40048599]
 [57.48249435]
 [57.5898056 ]
 [57.68832397]
 [57.78331757]].
[2019-03-27 15:45:41,845] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138500, global step 2217652: loss 0.2252
[2019-03-27 15:45:41,847] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138500, global step 2217652: learning rate 0.0000
[2019-03-27 15:45:43,817] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138500, global step 2218395: loss 0.0338
[2019-03-27 15:45:43,823] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138500, global step 2218398: learning rate 0.0000
[2019-03-27 15:45:44,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4319992e-38], sum to 1.0000
[2019-03-27 15:45:44,237] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3151
[2019-03-27 15:45:44,245] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5240767536253047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732327.4355252552, 732327.4355252557, 187454.3107003784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3702000.0000, 
sim time next is 3702600.0000, 
raw observation next is [28.5, 76.5, 1.0, 2.0, 0.5230039235708138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730827.7827572296, 730827.7827572296, 187278.7301356065], 
processed observation next is [1.0, 0.8695652173913043, 0.5497630331753555, 0.765, 1.0, 1.0, 0.4253059320130287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2030077174325638, 0.2030077174325638, 0.2795204927397112], 
reward next is 0.7205, 
noisyNet noise sample is [array([-1.3350927], dtype=float32), -0.8430728]. 
=============================================
[2019-03-27 15:45:44,367] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138500, global step 2218599: loss 0.1519
[2019-03-27 15:45:44,370] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138500, global step 2218600: learning rate 0.0000
[2019-03-27 15:45:45,033] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139000, global step 2218854: loss 0.0589
[2019-03-27 15:45:45,034] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139000, global step 2218854: learning rate 0.0000
[2019-03-27 15:45:45,642] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138500, global step 2219079: loss 0.1127
[2019-03-27 15:45:45,646] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138500, global step 2219080: learning rate 0.0000
[2019-03-27 15:45:46,489] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138500, global step 2219396: loss 0.1494
[2019-03-27 15:45:46,492] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138500, global step 2219397: learning rate 0.0000
[2019-03-27 15:45:48,498] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138500, global step 2220154: loss 0.1381
[2019-03-27 15:45:48,502] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138500, global step 2220155: learning rate 0.0000
[2019-03-27 15:45:49,041] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139000, global step 2220357: loss 0.0127
[2019-03-27 15:45:49,042] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139000, global step 2220357: learning rate 0.0000
[2019-03-27 15:45:49,125] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138500, global step 2220392: loss 0.3485
[2019-03-27 15:45:49,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138500, global step 2220392: learning rate 0.0000
[2019-03-27 15:45:49,559] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1549719e-09 4.7315070e-01 3.4773757e-16 6.2512541e-08 5.2684921e-01], sum to 1.0000
[2019-03-27 15:45:49,567] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3288
[2019-03-27 15:45:49,581] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2715138.740257666 W.
[2019-03-27 15:45:49,590] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.6530302730418015, 1.0, 2.0, 0.6471051760351634, 1.0, 2.0, 1.03, 7.005094028929012, 6.9112, 170.5573041426782, 2715138.740257666, 2647878.627970913, 506903.0041660251], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3775800.0000, 
sim time next is 3776400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.9283943611324246, 1.0, 2.0, 0.9283943611324246, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2596796.043344305, 2596796.043344305, 487222.7786428212], 
processed observation next is [1.0, 0.7391304347826086, 0.7630331753554502, 0.63, 1.0, 1.0, 0.9137281459426803, 1.0, 1.0, 0.9137281459426803, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.721332234262307, 0.721332234262307, 0.7271981770788376], 
reward next is 0.2728, 
noisyNet noise sample is [array([-1.6940081], dtype=float32), -0.8119459]. 
=============================================
[2019-03-27 15:45:51,265] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139000, global step 2221194: loss 0.0197
[2019-03-27 15:45:51,271] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139000, global step 2221196: learning rate 0.0000
[2019-03-27 15:45:53,335] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139000, global step 2221970: loss 0.0177
[2019-03-27 15:45:53,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139000, global step 2221971: learning rate 0.0000
[2019-03-27 15:45:53,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:45:53,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2793
[2019-03-27 15:45:53,742] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 63.33333333333333, 1.0, 2.0, 0.5572060011874463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778638.1123257191, 778638.1123257191, 193045.2620676647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [32.16666666666666, 65.16666666666667, 1.0, 2.0, 0.5640217650058872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788165.9753220618, 788165.9753220612, 194235.5952034292], 
processed observation next is [0.0, 0.782608695652174, 0.7235387045813582, 0.6516666666666667, 1.0, 1.0, 0.4747250180793821, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21893499314501716, 0.218934993145017, 0.28990387343795404], 
reward next is 0.7101, 
noisyNet noise sample is [array([-0.9902089], dtype=float32), 0.16306755]. 
=============================================
[2019-03-27 15:45:56,549] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139000, global step 2223169: loss 0.0523
[2019-03-27 15:45:56,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139000, global step 2223169: learning rate 0.0000
[2019-03-27 15:45:56,688] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139000, global step 2223223: loss 0.0308
[2019-03-27 15:45:56,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139000, global step 2223224: learning rate 0.0000
[2019-03-27 15:45:57,257] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139000, global step 2223436: loss 0.0074
[2019-03-27 15:45:57,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139000, global step 2223437: learning rate 0.0000
[2019-03-27 15:45:59,060] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139000, global step 2224108: loss 0.0090
[2019-03-27 15:45:59,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139000, global step 2224108: learning rate 0.0000
[2019-03-27 15:46:01,428] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 15:46:01,430] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:46:01,431] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:46:01,432] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:46:01,434] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:46:01,433] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:46:01,436] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:46:01,435] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:46:01,440] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:46:01,441] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:46:01,439] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:46:01,473] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run90
[2019-03-27 15:46:01,495] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run90
[2019-03-27 15:46:01,532] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run90
[2019-03-27 15:46:01,551] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run90
[2019-03-27 15:46:01,575] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run90
[2019-03-27 15:46:06,357] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06959177], dtype=float32), 0.09935078]
[2019-03-27 15:46:06,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.86666666666667, 90.16666666666667, 1.0, 2.0, 0.2918847654377892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467566.0319162112, 467566.0319162112, 164780.12513733]
[2019-03-27 15:46:06,359] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:46:06,361] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8090710880250225
[2019-03-27 15:46:53,950] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06959177], dtype=float32), 0.09935078]
[2019-03-27 15:46:53,950] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.90000000000001, 54.66666666666667, 1.0, 2.0, 0.5590688749624217, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564911552, 781242.2442663937, 781242.2442663937, 193370.6809612343]
[2019-03-27 15:46:53,950] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:46:53,953] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7069708e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9293223e-31], sampled 0.3026893487948801
[2019-03-27 15:47:02,711] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06959177], dtype=float32), 0.09935078]
[2019-03-27 15:47:02,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.16666666666667, 51.33333333333333, 1.0, 2.0, 0.947839780810322, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1324849.181733544, 1324849.181733544, 283438.9465818944]
[2019-03-27 15:47:02,716] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:47:02,720] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9584749e-29 1.0000000e+00 1.7225034e-35 1.2148116e-35 9.2851249e-28], sampled 0.7337514020292996
[2019-03-27 15:47:08,888] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06959177], dtype=float32), 0.09935078]
[2019-03-27 15:47:08,891] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.6, 75.0, 1.0, 2.0, 0.9362016790540533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1308571.924403525, 1308571.924403525, 280093.670866798]
[2019-03-27 15:47:08,893] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:47:08,895] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0069999e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.1333731e-34], sampled 0.21841759527847537
[2019-03-27 15:47:40,416] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06959177], dtype=float32), 0.09935078]
[2019-03-27 15:47:40,420] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.8, 78.66666666666667, 1.0, 2.0, 0.523758587546786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104127, 731882.687659302, 731882.6876593014, 187403.8350053522]
[2019-03-27 15:47:40,421] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:47:40,424] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5234973e-31 1.0000000e+00 2.5652170e-38 0.0000000e+00 2.5987150e-30], sampled 0.3470925607959713
[2019-03-27 15:47:59,779] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06959177], dtype=float32), 0.09935078]
[2019-03-27 15:47:59,780] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.967712815, 70.43457709166667, 1.0, 2.0, 0.2854117196230016, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473815.0116240539, 473815.0116240539, 164372.7204193017]
[2019-03-27 15:47:59,781] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:47:59,785] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4518424453172001
[2019-03-27 15:48:00,324] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06959177], dtype=float32), 0.09935078]
[2019-03-27 15:48:00,324] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.12352057, 71.91286769333334, 1.0, 2.0, 0.4164487110079942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 621112.0011412012, 621112.0011412019, 176499.3743961915]
[2019-03-27 15:48:00,324] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:48:00,327] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4166506036076958
[2019-03-27 15:48:09,693] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8503.9536 2841790927.4747 1136.0000
[2019-03-27 15:48:10,131] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.6801 2927867161.0952 1364.0000
[2019-03-27 15:48:10,159] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.9597 2779286102.5529 928.0000
[2019-03-27 15:48:10,306] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.6264 3163699337.2285 1875.0000
[2019-03-27 15:48:10,442] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8028.7832 3006252726.8290 1695.0000
[2019-03-27 15:48:11,460] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2225000, evaluation results [2225000.0, 7886.626362918879, 3163699337.228491, 1875.0, 8249.680059315804, 2927867161.0952425, 1364.0, 8663.959696250897, 2779286102.5528846, 928.0, 8028.783194361737, 3006252726.8290257, 1695.0, 8503.953571778015, 2841790927.474701, 1136.0]
[2019-03-27 15:48:12,550] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139500, global step 2225414: loss 28.3555
[2019-03-27 15:48:12,552] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139500, global step 2225414: learning rate 0.0000
[2019-03-27 15:48:13,310] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139000, global step 2225705: loss 0.0075
[2019-03-27 15:48:13,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139000, global step 2225705: learning rate 0.0000
[2019-03-27 15:48:15,120] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139000, global step 2226381: loss 0.0073
[2019-03-27 15:48:15,123] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139000, global step 2226382: learning rate 0.0000
[2019-03-27 15:48:15,758] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139000, global step 2226622: loss 0.0077
[2019-03-27 15:48:15,760] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139000, global step 2226622: learning rate 0.0000
[2019-03-27 15:48:16,367] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139500, global step 2226853: loss 35.7991
[2019-03-27 15:48:16,369] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139500, global step 2226853: learning rate 0.0000
[2019-03-27 15:48:16,889] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139000, global step 2227046: loss 0.0086
[2019-03-27 15:48:16,898] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139000, global step 2227047: learning rate 0.0000
[2019-03-27 15:48:17,755] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139000, global step 2227378: loss 0.0080
[2019-03-27 15:48:17,757] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139000, global step 2227379: learning rate 0.0000
[2019-03-27 15:48:19,810] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139000, global step 2228150: loss 0.0114
[2019-03-27 15:48:19,813] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139000, global step 2228150: learning rate 0.0000
[2019-03-27 15:48:20,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0417264e-13 1.0000000e+00 3.1094162e-20 7.8599406e-15 3.1386410e-10], sum to 1.0000
[2019-03-27 15:48:20,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3713
[2019-03-27 15:48:20,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2509472.623968591 W.
[2019-03-27 15:48:20,312] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.66666666666667, 71.0, 1.0, 2.0, 0.897206220921914, 1.0, 2.0, 0.897206220921914, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2509472.623968591, 2509472.623968591, 469981.9025269275], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4102800.0000, 
sim time next is 4103400.0000, 
raw observation next is [32.83333333333333, 71.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.001402194436011, 6.9112, 168.9068943317833, 3057790.623174532, 2284391.422379829, 473554.6018102238], 
processed observation next is [1.0, 0.4782608695652174, 0.7551342812006318, 0.71, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.1090202194436011, 0.0, 0.8294101770798651, 0.8493862842151477, 0.6345531728832858, 0.7067979131495877], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4654956], dtype=float32), 0.47668698]. 
=============================================
[2019-03-27 15:48:20,421] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139000, global step 2228380: loss 0.0112
[2019-03-27 15:48:20,423] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139000, global step 2228380: learning rate 0.0000
[2019-03-27 15:48:20,592] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139500, global step 2228445: loss 46.1007
[2019-03-27 15:48:20,596] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139500, global step 2228445: learning rate 0.0000
[2019-03-27 15:48:22,803] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139500, global step 2229279: loss 65.3662
[2019-03-27 15:48:22,810] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139500, global step 2229279: learning rate 0.0000
[2019-03-27 15:48:24,721] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139500, global step 2229998: loss 49.8623
[2019-03-27 15:48:24,724] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139500, global step 2229998: learning rate 0.0000
[2019-03-27 15:48:27,817] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139500, global step 2231161: loss 51.7647
[2019-03-27 15:48:27,821] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139500, global step 2231162: learning rate 0.0000
[2019-03-27 15:48:28,140] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139500, global step 2231280: loss 61.1901
[2019-03-27 15:48:28,145] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139500, global step 2231280: learning rate 0.0000
[2019-03-27 15:48:28,619] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139500, global step 2231464: loss 42.1276
[2019-03-27 15:48:28,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139500, global step 2231464: learning rate 0.0000
[2019-03-27 15:48:29,100] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:48:29,105] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7968
[2019-03-27 15:48:29,109] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 90.66666666666667, 1.0, 2.0, 0.8263011242058245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1154875.345869785, 1154875.345869785, 250469.8526333685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4083600.0000, 
sim time next is 4084200.0000, 
raw observation next is [27.0, 91.5, 1.0, 2.0, 0.8671886007029677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1212054.125597888, 1212054.125597888, 261072.4786098207], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.915, 1.0, 1.0, 0.8399862659071899, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33668170155496885, 0.33668170155496885, 0.3896604158355533], 
reward next is 0.6103, 
noisyNet noise sample is [array([-0.16336624], dtype=float32), -0.3898966]. 
=============================================
[2019-03-27 15:48:30,468] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139500, global step 2232151: loss 71.7893
[2019-03-27 15:48:30,471] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139500, global step 2232151: learning rate 0.0000
[2019-03-27 15:48:33,608] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140000, global step 2233338: loss 0.0068
[2019-03-27 15:48:33,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140000, global step 2233338: learning rate 0.0000
[2019-03-27 15:48:33,622] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7681515e-06 1.1536941e-03 2.1227649e-14 2.2357608e-04 9.9862099e-01], sum to 1.0000
[2019-03-27 15:48:33,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4337
[2019-03-27 15:48:33,635] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 50.0, 1.0, 2.0, 0.8889057652913162, 1.0, 2.0, 0.7650429221599205, 1.0, 2.0, 1.03, 7.005112631929873, 6.9112, 170.5573041426782, 3210620.627215636, 3143347.188842644, 587638.5156566598], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4288200.0000, 
sim time next is 4288800.0000, 
raw observation next is [38.0, 49.00000000000001, 1.0, 2.0, 0.8856853021867638, 1.0, 2.0, 0.7634326906076445, 1.0, 2.0, 1.03, 7.005112377839421, 6.9112, 170.5573041426782, 3203854.373748922, 3136581.117391258, 586396.3613970126], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49000000000000005, 1.0, 1.0, 0.8622714484177877, 1.0, 1.0, 0.7149791453104151, 1.0, 1.0, 1.0365853658536586, 0.009391237783942064, 0.0, 0.8375144448122397, 0.8899595482635894, 0.8712725326086828, 0.8752184498462875], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.36839962], dtype=float32), 1.7269009]. 
=============================================
[2019-03-27 15:48:34,871] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139500, global step 2233810: loss 43.9709
[2019-03-27 15:48:34,875] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139500, global step 2233810: learning rate 0.0000
[2019-03-27 15:48:36,690] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139500, global step 2234499: loss 43.4782
[2019-03-27 15:48:36,692] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139500, global step 2234499: learning rate 0.0000
[2019-03-27 15:48:37,039] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139500, global step 2234627: loss 52.9778
[2019-03-27 15:48:37,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139500, global step 2234627: learning rate 0.0000
[2019-03-27 15:48:37,303] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140000, global step 2234726: loss 0.0226
[2019-03-27 15:48:37,307] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140000, global step 2234726: learning rate 0.0000
[2019-03-27 15:48:38,194] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139500, global step 2235061: loss 34.0641
[2019-03-27 15:48:38,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139500, global step 2235061: learning rate 0.0000
[2019-03-27 15:48:39,166] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139500, global step 2235424: loss 45.5041
[2019-03-27 15:48:39,177] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139500, global step 2235424: learning rate 0.0000
[2019-03-27 15:48:41,363] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139500, global step 2236246: loss 58.6031
[2019-03-27 15:48:41,366] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139500, global step 2236246: learning rate 0.0000
[2019-03-27 15:48:41,529] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140000, global step 2236306: loss 0.0142
[2019-03-27 15:48:41,531] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140000, global step 2236306: learning rate 0.0000
[2019-03-27 15:48:41,830] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139500, global step 2236421: loss 60.7953
[2019-03-27 15:48:41,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139500, global step 2236422: learning rate 0.0000
[2019-03-27 15:48:42,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7188664e-12 1.0000000e+00 6.8194750e-19 4.5787311e-14 9.2017999e-10], sum to 1.0000
[2019-03-27 15:48:42,796] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8247
[2019-03-27 15:48:42,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2032294.983860952 W.
[2019-03-27 15:48:42,814] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 84.0, 1.0, 2.0, 0.726749283216792, 1.0, 1.0, 0.726749283216792, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2032294.983860952, 2032294.983860952, 385806.5767889969], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4262400.0000, 
sim time next is 4263000.0000, 
raw observation next is [31.33333333333334, 82.5, 1.0, 2.0, 0.6193452822779623, 1.0, 2.0, 0.6193452822779623, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1731705.753931049, 1731705.753931049, 341529.8207757097], 
processed observation next is [1.0, 0.34782608695652173, 0.6840442338072673, 0.825, 1.0, 1.0, 0.5413798581662196, 1.0, 1.0, 0.5413798581662196, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.48102937609195806, 0.48102937609195806, 0.5097460011577757], 
reward next is 0.4903, 
noisyNet noise sample is [array([-1.2888677], dtype=float32), -0.1695887]. 
=============================================
[2019-03-27 15:48:42,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[36.723557]
 [37.249264]
 [36.555553]
 [37.016586]
 [37.73475 ]], R is [[38.26782608]
 [38.30931473]
 [37.92622375]
 [37.54696274]
 [37.17149353]].
[2019-03-27 15:48:43,894] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140000, global step 2237195: loss 0.0077
[2019-03-27 15:48:43,897] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140000, global step 2237195: learning rate 0.0000
[2019-03-27 15:48:44,304] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4349304e-06 3.1157631e-07 2.7013465e-12 4.8026752e-01 5.1972377e-01], sum to 1.0000
[2019-03-27 15:48:44,317] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5973
[2019-03-27 15:48:44,323] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.66666666666667, 76.5, 1.0, 2.0, 0.7141195356345817, 1.0, 2.0, 0.6776498073315534, 1.0, 2.0, 1.03, 7.005098845494632, 6.9112, 170.5573041426782, 2843444.460073826, 2776180.897485194, 525827.0120644732], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4611000.0000, 
sim time next is 4611600.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.7060254933074764, 1.0, 2.0, 0.6736027861680008, 1.0, 2.0, 1.03, 7.005098207265324, 6.9112, 170.5573041426782, 2826443.79334654, 2759180.687947523, 523240.2623243606], 
processed observation next is [1.0, 0.391304347826087, 0.7156398104265403, 0.75, 1.0, 1.0, 0.6458138473584053, 1.0, 1.0, 0.6067503447807239, 1.0, 1.0, 1.0365853658536586, 0.009389820726532428, 0.0, 0.8375144448122397, 0.7851232759295944, 0.766439079985423, 0.7809556154094934], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.3518596], dtype=float32), 0.61981815]. 
=============================================
[2019-03-27 15:48:45,861] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140000, global step 2237928: loss 0.0173
[2019-03-27 15:48:45,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140000, global step 2237928: learning rate 0.0000
[2019-03-27 15:48:47,020] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:48:47,029] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1236
[2019-03-27 15:48:47,042] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5527835446855871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772455.9368315182, 772455.9368315176, 192278.7896705648], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4482600.0000, 
sim time next is 4483200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5501475598523213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 191825.7660821197], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4580091082558088, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2135475268641311, 0.21354752686413092, 0.2863071135554025], 
reward next is 0.7137, 
noisyNet noise sample is [array([-1.0239843], dtype=float32), 1.9996462]. 
=============================================
[2019-03-27 15:48:48,982] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140000, global step 2239094: loss 0.0136
[2019-03-27 15:48:48,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140000, global step 2239094: learning rate 0.0000
[2019-03-27 15:48:49,490] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140000, global step 2239284: loss 0.0148
[2019-03-27 15:48:49,492] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140000, global step 2239285: learning rate 0.0000
[2019-03-27 15:48:49,751] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140000, global step 2239381: loss 0.0138
[2019-03-27 15:48:49,752] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140000, global step 2239381: learning rate 0.0000
[2019-03-27 15:48:49,939] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5043422e-30 1.0000000e+00 7.0186169e-38 0.0000000e+00 1.3254867e-31], sum to 1.0000
[2019-03-27 15:48:49,950] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8234
[2019-03-27 15:48:49,959] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 94.00000000000001, 1.0, 2.0, 0.9450630098866242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1320965.517572562, 1320965.517572562, 282636.4787975435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4597800.0000, 
sim time next is 4598400.0000, 
raw observation next is [27.33333333333334, 94.0, 1.0, 2.0, 0.9336194138699042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1304960.355947803, 1304960.355947803, 279355.7588205826], 
processed observation next is [1.0, 0.21739130434782608, 0.4944707740916275, 0.94, 1.0, 1.0, 0.9200233902047039, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3624889877632786, 0.3624889877632786, 0.41694889376206357], 
reward next is 0.5831, 
noisyNet noise sample is [array([-1.0349965], dtype=float32), 1.7105538]. 
=============================================
[2019-03-27 15:48:51,736] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140000, global step 2240135: loss 0.0399
[2019-03-27 15:48:51,737] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140000, global step 2240135: learning rate 0.0000
[2019-03-27 15:48:52,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2622202e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.5845463e-37], sum to 1.0000
[2019-03-27 15:48:52,087] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5529
[2019-03-27 15:48:52,094] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6171607559031557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862452.7339116574, 862452.733911658, 203986.8338748405], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4404600.0000, 
sim time next is 4405200.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6174361852499398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862837.7896362707, 862837.7896362707, 204039.5391622155], 
processed observation next is [1.0, 1.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5390797412649877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23967716378785298, 0.23967716378785298, 0.30453662561524697], 
reward next is 0.6955, 
noisyNet noise sample is [array([0.10211331], dtype=float32), 1.1352938]. 
=============================================
[2019-03-27 15:48:53,537] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.3921725e-08 3.1128823e-04 2.7773317e-15 1.5384071e-03 9.9815023e-01], sum to 1.0000
[2019-03-27 15:48:53,545] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5213
[2019-03-27 15:48:53,552] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 61.0, 1.0, 2.0, 0.81926920504928, 1.0, 2.0, 0.7302246420389025, 1.0, 2.0, 1.03, 7.005107138307058, 6.9112, 170.5573041426782, 3064321.079542066, 2997051.576474564, 561646.7022065471], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4380000.0000, 
sim time next is 4380600.0000, 
raw observation next is [33.5, 62.0, 1.0, 2.0, 0.7866771088593171, 1.0, 2.0, 0.713928593943921, 1.0, 2.0, 1.03, 7.005104567560958, 6.9112, 170.5573041426782, 2995854.18570442, 2928586.52416692, 550103.3286476838], 
processed observation next is [1.0, 0.6956521739130435, 0.7867298578199052, 0.62, 1.0, 1.0, 0.7429844685052013, 1.0, 1.0, 0.6553356553541217, 1.0, 1.0, 1.0365853658536586, 0.009390456756095756, 0.0, 0.8375144448122397, 0.8321817182512278, 0.8134962567130334, 0.8210497442502744], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5095751], dtype=float32), -0.31414303]. 
=============================================
[2019-03-27 15:48:55,591] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140500, global step 2241579: loss 1.3440
[2019-03-27 15:48:55,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140500, global step 2241579: learning rate 0.0000
[2019-03-27 15:48:56,094] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140000, global step 2241768: loss 0.0250
[2019-03-27 15:48:56,096] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140000, global step 2241769: learning rate 0.0000
[2019-03-27 15:48:58,276] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140000, global step 2242585: loss 0.0141
[2019-03-27 15:48:58,278] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140000, global step 2242585: learning rate 0.0000
[2019-03-27 15:48:58,446] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140000, global step 2242646: loss 0.0109
[2019-03-27 15:48:58,447] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140000, global step 2242646: learning rate 0.0000
[2019-03-27 15:48:58,824] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140500, global step 2242787: loss 1.4848
[2019-03-27 15:48:58,826] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140500, global step 2242787: learning rate 0.0000
[2019-03-27 15:48:59,540] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140000, global step 2243060: loss 0.0111
[2019-03-27 15:48:59,545] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140000, global step 2243063: learning rate 0.0000
[2019-03-27 15:49:00,441] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140000, global step 2243397: loss 0.0107
[2019-03-27 15:49:00,445] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140000, global step 2243399: learning rate 0.0000
[2019-03-27 15:49:00,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7434363e-31 1.0000000e+00 1.0610892e-37 0.0000000e+00 5.4067093e-31], sum to 1.0000
[2019-03-27 15:49:00,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9123
[2019-03-27 15:49:00,908] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7650871540744784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1069276.89129477, 1069276.89129477, 235521.5928027905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4680000.0000, 
sim time next is 4680600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.835497732681488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1167736.004468091, 1167736.004468092, 252810.7880729979], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.801804497206612, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3243711123522475, 0.3243711123522478, 0.3773295344373103], 
reward next is 0.6227, 
noisyNet noise sample is [array([0.05151541], dtype=float32), -1.3195342]. 
=============================================
[2019-03-27 15:49:00,915] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:49:00,924] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7711
[2019-03-27 15:49:00,933] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 79.0, 1.0, 2.0, 0.5354309014307969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748198.9319862404, 748198.9319862397, 189334.0538590824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4531200.0000, 
sim time next is 4531800.0000, 
raw observation next is [28.83333333333334, 79.0, 1.0, 2.0, 0.5412194199662537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756290.5553582262, 756290.5553582262, 190306.7846473849], 
processed observation next is [0.0, 0.43478260869565216, 0.5655608214849924, 0.79, 1.0, 1.0, 0.4472523132123538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2100807098217295, 0.2100807098217295, 0.2840399770856491], 
reward next is 0.7160, 
noisyNet noise sample is [array([-0.9979228], dtype=float32), -1.6573638]. 
=============================================
[2019-03-27 15:49:02,712] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.9252815e-09 2.8051423e-05 2.0862756e-16 1.3707028e-04 9.9983490e-01], sum to 1.0000
[2019-03-27 15:49:02,724] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3164
[2019-03-27 15:49:02,728] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.6583490679131738, 1.0, 1.0, 0.6497645734708495, 1.0, 2.0, 1.03, 7.005094448248592, 6.9112, 170.5573041426782, 2726309.276335893, 2659048.863673469, 508494.5303746471], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4809600.0000, 
sim time next is 4810200.0000, 
raw observation next is [31.83333333333334, 63.5, 1.0, 2.0, 0.5971225730580989, 1.0, 2.0, 0.5971225730580989, 1.0, 2.0, 1.03, 6.919074010755846, 6.9112, 170.5573041426782, 2505210.333098258, 2499569.858899566, 486936.5148930911], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.635, 1.0, 1.0, 0.5146055097085529, 1.0, 1.0, 0.5146055097085529, 1.0, 1.0, 1.0365853658536586, 0.000787401075584615, 0.0, 0.8375144448122397, 0.6958917591939605, 0.6943249608054349, 0.7267709177508822], 
reward next is 0.2339, 
noisyNet noise sample is [array([0.2374715], dtype=float32), -0.55028677]. 
=============================================
[2019-03-27 15:49:02,770] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140000, global step 2244278: loss 0.0125
[2019-03-27 15:49:02,771] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140000, global step 2244278: learning rate 0.0000
[2019-03-27 15:49:02,845] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140500, global step 2244303: loss 1.6262
[2019-03-27 15:49:02,850] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140500, global step 2244304: learning rate 0.0000
[2019-03-27 15:49:03,195] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140000, global step 2244443: loss 0.0097
[2019-03-27 15:49:03,199] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140000, global step 2244443: learning rate 0.0000
[2019-03-27 15:49:05,275] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140500, global step 2245232: loss 0.4054
[2019-03-27 15:49:05,277] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140500, global step 2245233: learning rate 0.0000
[2019-03-27 15:49:07,221] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140500, global step 2245967: loss 0.3113
[2019-03-27 15:49:07,224] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140500, global step 2245968: learning rate 0.0000
[2019-03-27 15:49:07,624] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.8409985e-25 1.0000000e+00 9.6686987e-31 1.6430534e-28 1.6411855e-22], sum to 1.0000
[2019-03-27 15:49:07,635] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4087
[2019-03-27 15:49:07,647] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 86.5, 1.0, 2.0, 0.5606127076914825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783400.390023372, 783400.3900233727, 193637.4480344224], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4581000.0000, 
sim time next is 4581600.0000, 
raw observation next is [28.0, 87.33333333333333, 1.0, 2.0, 0.5657433917194599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790572.6777913194, 790572.6777913194, 194537.339257731], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.8733333333333333, 1.0, 1.0, 0.4767992671318793, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21960352160869984, 0.21960352160869984, 0.290354237698106], 
reward next is 0.7096, 
noisyNet noise sample is [array([-0.7594236], dtype=float32), 1.0314488]. 
=============================================
[2019-03-27 15:49:10,150] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140500, global step 2247067: loss 0.2672
[2019-03-27 15:49:10,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140500, global step 2247067: learning rate 0.0000
[2019-03-27 15:49:10,705] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140500, global step 2247271: loss 0.1211
[2019-03-27 15:49:10,706] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140500, global step 2247271: learning rate 0.0000
[2019-03-27 15:49:10,805] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140500, global step 2247308: loss 0.0890
[2019-03-27 15:49:10,807] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140500, global step 2247308: learning rate 0.0000
[2019-03-27 15:49:13,159] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140500, global step 2248195: loss 0.6232
[2019-03-27 15:49:13,161] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140500, global step 2248196: learning rate 0.0000
[2019-03-27 15:49:13,670] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.06204594e-35 1.00000000e+00 0.00000000e+00 0.00000000e+00
 3.00542920e-33], sum to 1.0000
[2019-03-27 15:49:13,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5490
[2019-03-27 15:49:13,686] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666667, 94.0, 1.0, 2.0, 0.4810413142265207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672172.116678605, 672172.1166786043, 180678.4378513148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4659600.0000, 
sim time next is 4660200.0000, 
raw observation next is [24.75, 94.0, 1.0, 2.0, 0.4832657363366631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675281.3501446685, 675281.3501446685, 181015.265353408], 
processed observation next is [1.0, 0.9565217391304348, 0.3720379146919432, 0.94, 1.0, 1.0, 0.37742859799597966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18757815281796347, 0.18757815281796347, 0.2701720378409075], 
reward next is 0.7298, 
noisyNet noise sample is [array([0.14949487], dtype=float32), 0.61178756]. 
=============================================
[2019-03-27 15:49:14,882] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.8176436e-08 2.7626204e-01 2.5247233e-14 3.1205378e-05 7.2370660e-01], sum to 1.0000
[2019-03-27 15:49:14,893] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0596
[2019-03-27 15:49:14,904] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1842653.620641312 W.
[2019-03-27 15:49:14,914] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.6767923828174476, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.982147791824763, 6.9112, 168.9125342156441, 1842653.620641312, 1792320.941121988, 380557.3955418333], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4870800.0000, 
sim time next is 4871400.0000, 
raw observation next is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.5119127782473011, 1.0, 1.0, 0.5119127782473011, 1.0, 2.0, 0.879040479744748, 6.9112, 6.9112, 170.5573041426782, 2147398.633767966, 2147398.633767966, 421622.1671676077], 
processed observation next is [1.0, 0.391304347826087, 0.581358609794629, 0.7333333333333334, 1.0, 1.0, 0.41194310632204945, 1.0, 0.5, 0.41194310632204945, 1.0, 1.0, 0.8524883899326194, 0.0, 0.0, 0.8375144448122397, 0.5964996204911017, 0.5964996204911017, 0.6292868166680712], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.77617306], dtype=float32), 1.4076539]. 
=============================================
[2019-03-27 15:49:16,798] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141000, global step 2249560: loss 0.0143
[2019-03-27 15:49:16,802] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141000, global step 2249560: learning rate 0.0000
[2019-03-27 15:49:17,260] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140500, global step 2249733: loss 0.0632
[2019-03-27 15:49:17,261] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140500, global step 2249733: learning rate 0.0000
[2019-03-27 15:49:17,984] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 15:49:17,987] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:49:17,988] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:49:17,989] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:49:17,989] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:49:17,990] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:49:17,991] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:49:17,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:49:17,991] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:49:17,993] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:49:17,995] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:49:18,022] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run91
[2019-03-27 15:49:18,045] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run91
[2019-03-27 15:49:18,046] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run91
[2019-03-27 15:49:18,068] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run91
[2019-03-27 15:49:18,121] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run91
[2019-03-27 15:49:37,573] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:49:37,574] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.0, 78.0, 1.0, 2.0, 0.2863880714988819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462651.4318866978, 462651.4318866985, 164450.1638312574]
[2019-03-27 15:49:37,574] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:49:37,575] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6335132564271646
[2019-03-27 15:49:41,213] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:49:41,214] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.15, 70.5, 1.0, 2.0, 0.2470215394383943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 409154.3985814012, 409154.3985814018, 160473.9424362277]
[2019-03-27 15:49:41,216] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:49:41,217] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.008426228757509202
[2019-03-27 15:49:50,122] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:49:50,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.36666666666667, 94.83333333333334, 1.0, 2.0, 0.4649283893598679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654700.6073903277, 654700.607390327, 178935.8505071668]
[2019-03-27 15:49:50,126] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:49:50,128] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9866584944609624
[2019-03-27 15:49:52,278] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:49:52,279] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.05, 76.83333333333334, 1.0, 2.0, 0.5691461926551131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795329.5492814106, 795329.5492814106, 195139.2908913219]
[2019-03-27 15:49:52,280] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:49:52,283] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.890483e-38], sampled 0.5873534823624679
[2019-03-27 15:50:24,054] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:50:24,055] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 75.0, 1.0, 2.0, 0.6240140405236253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872033.8040864693, 872033.8040864693, 205306.2115416442]
[2019-03-27 15:50:24,056] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:50:24,061] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.820167e-37], sampled 0.4777002743189319
[2019-03-27 15:50:27,212] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:50:27,213] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.685800505, 73.35595933166667, 1.0, 2.0, 0.8967355726665198, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005987570721143, 6.9112, 168.9123159637482, 2150467.220017923, 2083221.910018562, 433192.0324448166]
[2019-03-27 15:50:27,214] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:50:27,216] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.40572709e-09 5.11268675e-01 9.12015727e-16 1.41756345e-05
 4.88717109e-01], sampled 0.8461553731853877
[2019-03-27 15:51:03,440] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:51:03,442] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.60166482, 48.33553576, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.119512790386717, 6.9112, 168.9119476957302, 1610001.442353645, 1462218.061906204, 312693.245409475]
[2019-03-27 15:51:03,444] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:51:03,447] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0775017e-24 1.0000000e+00 2.6609711e-30 1.8511176e-26 1.1161785e-19], sampled 0.5884756113534297
[2019-03-27 15:51:04,625] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:51:04,625] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.4, 76.0, 1.0, 2.0, 0.7360636077617738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1028694.274218877, 1028694.274218878, 228823.2601007484]
[2019-03-27 15:51:04,626] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:51:04,630] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6147052e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3030426e-36], sampled 0.09627213120153855
[2019-03-27 15:51:12,988] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:51:12,991] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.98333333333333, 74.16666666666667, 1.0, 2.0, 0.6417394156181044, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005971645981187, 6.9112, 168.912316070084, 1793604.749502814, 1726370.736976897, 372529.5534045186]
[2019-03-27 15:51:12,993] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:51:12,997] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9834803e-16 1.0000000e+00 3.9280583e-22 8.7634478e-15 2.1380957e-09], sampled 0.7948156040863869
[2019-03-27 15:51:12,999] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1793604.749502814 W.
[2019-03-27 15:51:25,432] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7999.2987 3158915566.8698 1606.0000
[2019-03-27 15:51:25,576] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06822546], dtype=float32), 0.102053374]
[2019-03-27 15:51:25,577] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.5, 63.0, 1.0, 2.0, 0.4318951001144428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 666326.2092118219, 666326.2092118212, 181218.6865423119]
[2019-03-27 15:51:25,578] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:51:25,580] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6073639402258254
[2019-03-27 15:51:25,962] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8578.0283 2838544860.4110 948.0000
[2019-03-27 15:51:26,254] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8700.7882 2778567876.7077 843.0000
[2019-03-27 15:51:26,540] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8174.3366 2999253368.6782 1320.0000
[2019-03-27 15:51:26,540] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8319.7887 2926318461.8396 1210.0000
[2019-03-27 15:51:27,561] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2250000, evaluation results [2250000.0, 7999.298704054137, 3158915566.8698487, 1606.0, 8319.788721032372, 2926318461.8395915, 1210.0, 8700.788195122237, 2778567876.707654, 843.0, 8174.336622669021, 2999253368.6781626, 1320.0, 8578.028266717261, 2838544860.410986, 948.0]
[2019-03-27 15:51:29,079] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140500, global step 2250576: loss 0.1034
[2019-03-27 15:51:29,085] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140500, global step 2250579: learning rate 0.0000
[2019-03-27 15:51:29,135] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140500, global step 2250601: loss 0.0436
[2019-03-27 15:51:29,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140500, global step 2250601: learning rate 0.0000
[2019-03-27 15:51:29,760] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141000, global step 2250834: loss 0.0123
[2019-03-27 15:51:29,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141000, global step 2250836: learning rate 0.0000
[2019-03-27 15:51:30,178] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140500, global step 2250991: loss 0.1242
[2019-03-27 15:51:30,179] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140500, global step 2250992: learning rate 0.0000
[2019-03-27 15:51:31,162] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140500, global step 2251362: loss 1.6436
[2019-03-27 15:51:31,165] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140500, global step 2251363: learning rate 0.0000
[2019-03-27 15:51:32,989] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.0938617e-31 1.0000000e+00 9.5347195e-38 2.1409094e-37 4.9546551e-28], sum to 1.0000
[2019-03-27 15:51:32,998] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1786
[2019-03-27 15:51:33,005] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 82.33333333333334, 1.0, 2.0, 0.6701306356375218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 936508.2514518601, 936508.2514518608, 214529.4205903352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4866000.0000, 
sim time next is 4866600.0000, 
raw observation next is [27.83333333333334, 80.66666666666667, 1.0, 2.0, 0.6673898951587246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 932676.3819837064, 932676.381983707, 213962.9556299086], 
processed observation next is [1.0, 0.30434782608695654, 0.5181674565560824, 0.8066666666666668, 1.0, 1.0, 0.5992649339261742, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25907677277325175, 0.2590767727732519, 0.3193476949700128], 
reward next is 0.6807, 
noisyNet noise sample is [array([-0.01250854], dtype=float32), -0.90640265]. 
=============================================
[2019-03-27 15:51:33,578] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141000, global step 2252273: loss 0.0074
[2019-03-27 15:51:33,581] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141000, global step 2252274: learning rate 0.0000
[2019-03-27 15:51:33,719] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140500, global step 2252324: loss 0.0997
[2019-03-27 15:51:33,724] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140500, global step 2252324: learning rate 0.0000
[2019-03-27 15:51:34,090] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140500, global step 2252459: loss 0.0277
[2019-03-27 15:51:34,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140500, global step 2252459: learning rate 0.0000
[2019-03-27 15:51:35,707] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7156503e-25 1.0000000e+00 3.6474377e-31 1.3339839e-30 1.1249226e-22], sum to 1.0000
[2019-03-27 15:51:35,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1912
[2019-03-27 15:51:35,721] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.9514258391823721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1329864.757426841, 1329864.757426841, 284472.7959539494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5196000.0000, 
sim time next is 5196600.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.9401650299887463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1314115.109905021, 1314115.109905021, 281222.5073731586], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.9279096746852364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.36503197497361695, 0.36503197497361695, 0.41973508563157996], 
reward next is 0.5803, 
noisyNet noise sample is [array([-0.17320184], dtype=float32), 1.4596832]. 
=============================================
[2019-03-27 15:51:36,080] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141000, global step 2253219: loss 0.0074
[2019-03-27 15:51:36,085] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141000, global step 2253219: learning rate 0.0000
[2019-03-27 15:51:36,298] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.820330e-38 1.000000e+00 0.000000e+00 0.000000e+00 2.767374e-37], sum to 1.0000
[2019-03-27 15:51:36,312] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3936
[2019-03-27 15:51:36,320] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5225655432050414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730214.9944851904, 730214.9944851898, 187206.8633915978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5083800.0000, 
sim time next is 5084400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5224280315099159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730022.7743752343, 730022.7743752343, 187184.4101223437], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4246120861565252, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20278410399312063, 0.20278410399312063, 0.279379716600513], 
reward next is 0.7206, 
noisyNet noise sample is [array([0.7901561], dtype=float32), 0.17219995]. 
=============================================
[2019-03-27 15:51:38,212] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141000, global step 2254022: loss 0.0147
[2019-03-27 15:51:38,214] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141000, global step 2254022: learning rate 0.0000
[2019-03-27 15:51:40,680] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141000, global step 2254944: loss 0.0209
[2019-03-27 15:51:40,682] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141000, global step 2254945: learning rate 0.0000
[2019-03-27 15:51:40,781] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:51:40,789] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8960
[2019-03-27 15:51:40,794] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 72.0, 1.0, 2.0, 0.4948729272264979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691505.6965093168, 691505.6965093162, 182796.6702290063], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4908600.0000, 
sim time next is 4909200.0000, 
raw observation next is [28.33333333333333, 72.66666666666666, 1.0, 2.0, 0.493160040738451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689111.4355003275, 689111.4355003275, 182531.3177667337], 
processed observation next is [1.0, 0.8260869565217391, 0.541864139020537, 0.7266666666666666, 1.0, 1.0, 0.3893494466728325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19141984319453542, 0.19141984319453542, 0.272434802636916], 
reward next is 0.7276, 
noisyNet noise sample is [array([0.5676214], dtype=float32), 1.931586]. 
=============================================
[2019-03-27 15:51:41,537] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141000, global step 2255265: loss 0.0354
[2019-03-27 15:51:41,540] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141000, global step 2255265: learning rate 0.0000
[2019-03-27 15:51:41,689] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141000, global step 2255323: loss 0.0161
[2019-03-27 15:51:41,691] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141000, global step 2255323: learning rate 0.0000
[2019-03-27 15:51:43,727] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141000, global step 2256100: loss 0.0218
[2019-03-27 15:51:43,732] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141000, global step 2256100: learning rate 0.0000
[2019-03-27 15:51:47,434] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141500, global step 2257491: loss -0.2307
[2019-03-27 15:51:47,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141500, global step 2257491: learning rate 0.0000
[2019-03-27 15:51:48,055] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141000, global step 2257725: loss 0.0539
[2019-03-27 15:51:48,059] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141000, global step 2257725: learning rate 0.0000
[2019-03-27 15:51:50,454] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141000, global step 2258627: loss 0.1272
[2019-03-27 15:51:50,456] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141000, global step 2258627: learning rate 0.0000
[2019-03-27 15:51:50,548] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141000, global step 2258658: loss 0.1232
[2019-03-27 15:51:50,550] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141000, global step 2258658: learning rate 0.0000
[2019-03-27 15:51:51,019] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141500, global step 2258840: loss -8.4605
[2019-03-27 15:51:51,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141500, global step 2258840: learning rate 0.0000
[2019-03-27 15:51:51,488] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141000, global step 2259019: loss 0.1767
[2019-03-27 15:51:51,490] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141000, global step 2259020: learning rate 0.0000
[2019-03-27 15:51:52,442] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141000, global step 2259373: loss 0.2358
[2019-03-27 15:51:52,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141000, global step 2259373: learning rate 0.0000
[2019-03-27 15:51:54,786] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141500, global step 2260258: loss -2.9210
[2019-03-27 15:51:54,788] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141500, global step 2260259: learning rate 0.0000
[2019-03-27 15:51:55,076] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141000, global step 2260369: loss 0.1452
[2019-03-27 15:51:55,079] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141000, global step 2260369: learning rate 0.0000
[2019-03-27 15:51:55,548] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141000, global step 2260545: loss 0.1698
[2019-03-27 15:51:55,551] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141000, global step 2260545: learning rate 0.0000
[2019-03-27 15:51:56,676] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:51:56,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0700
[2019-03-27 15:51:56,700] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.9, 58.66666666666667, 1.0, 2.0, 0.5801915235340569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810770.275935575, 810770.275935575, 197116.7241329293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5334000.0000, 
sim time next is 5334600.0000, 
raw observation next is [34.65, 59.83333333333334, 1.0, 2.0, 0.5850136715667648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817511.4272832045, 817511.4272832045, 197989.959642475], 
processed observation next is [1.0, 0.7391304347826086, 0.8412322274881516, 0.5983333333333334, 1.0, 1.0, 0.5000164717671864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2270865075786679, 0.2270865075786679, 0.29550740245145524], 
reward next is 0.7045, 
noisyNet noise sample is [array([1.1707865], dtype=float32), -1.1412051]. 
=============================================
[2019-03-27 15:51:57,470] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141500, global step 2261269: loss -0.2186
[2019-03-27 15:51:57,476] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141500, global step 2261270: learning rate 0.0000
[2019-03-27 15:51:58,050] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.3666439e-07 3.7808756e-09 2.1968760e-13 8.8534685e-04 9.9911386e-01], sum to 1.0000
[2019-03-27 15:51:58,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5675
[2019-03-27 15:51:58,067] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.9, 68.0, 1.0, 2.0, 0.6662764546678558, 1.0, 2.0, 0.6537282668481905, 1.0, 2.0, 1.03, 7.005095073236496, 6.9112, 170.5573041426782, 2742958.563979199, 2675697.703612515, 510889.4009787605], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5389200.0000, 
sim time next is 5389800.0000, 
raw observation next is [33.1, 67.33333333333334, 1.0, 2.0, 0.5681861965660014, 1.0, 2.0, 0.5681861965660014, 1.0, 2.0, 0.9867515213886285, 6.9112, 6.9112, 170.5573041426782, 2383692.816363524, 2383692.816363524, 465501.7447478886], 
processed observation next is [1.0, 0.391304347826087, 0.7677725118483413, 0.6733333333333335, 1.0, 1.0, 0.47974240550120656, 1.0, 1.0, 0.47974240550120656, 1.0, 1.0, 0.98384331876662, 0.0, 0.0, 0.8375144448122397, 0.6621368934343123, 0.6621368934343123, 0.6947787235043114], 
reward next is 0.3052, 
noisyNet noise sample is [array([0.84414256], dtype=float32), -1.0320182]. 
=============================================
[2019-03-27 15:51:59,681] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141500, global step 2262090: loss -3.6131
[2019-03-27 15:51:59,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141500, global step 2262090: learning rate 0.0000
[2019-03-27 15:52:01,447] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:52:01,458] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5680
[2019-03-27 15:52:01,463] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.08333333333333, 77.5, 1.0, 2.0, 0.5459009997419011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762834.8621751659, 762834.8621751665, 191100.422018578], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5251800.0000, 
sim time next is 5252400.0000, 
raw observation next is [28.9, 78.0, 1.0, 2.0, 0.545697812507305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762550.829095166, 762550.8290951665, 191065.5801851521], 
processed observation next is [1.0, 0.8260869565217391, 0.5687203791469194, 0.78, 1.0, 1.0, 0.452647966876271, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2118196747486572, 0.21181967474865737, 0.28517250773903297], 
reward next is 0.7148, 
noisyNet noise sample is [array([-0.3622038], dtype=float32), 0.41852307]. 
=============================================
[2019-03-27 15:52:02,035] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141500, global step 2262974: loss 0.1905
[2019-03-27 15:52:02,037] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141500, global step 2262974: learning rate 0.0000
[2019-03-27 15:52:02,757] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141500, global step 2263243: loss 0.1245
[2019-03-27 15:52:02,760] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141500, global step 2263243: learning rate 0.0000
[2019-03-27 15:52:02,786] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141500, global step 2263252: loss 0.1088
[2019-03-27 15:52:02,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141500, global step 2263252: learning rate 0.0000
[2019-03-27 15:52:05,042] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141500, global step 2264108: loss 0.1799
[2019-03-27 15:52:05,046] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141500, global step 2264108: learning rate 0.0000
[2019-03-27 15:52:07,402] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.9966214e-08 1.0217195e-09 3.2598297e-14 1.2245800e-04 9.9987745e-01], sum to 1.0000
[2019-03-27 15:52:07,410] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2535
[2019-03-27 15:52:07,422] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.3, 72.0, 1.0, 2.0, 0.585369049614288, 1.0, 2.0, 0.585369049614288, 1.0, 2.0, 1.016592454677166, 6.911200000000001, 6.9112, 170.5573041426782, 2455850.309692469, 2455850.309692469, 479200.3617437404], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5475600.0000, 
sim time next is 5476200.0000, 
raw observation next is [32.53333333333333, 71.33333333333333, 1.0, 2.0, 0.6872316095171852, 1.0, 2.0, 0.6642058442728552, 1.0, 2.0, 1.03, 7.005096725401872, 6.9112, 170.5573041426782, 2786970.150914134, 2719708.107034186, 517327.5404385418], 
processed observation next is [1.0, 0.391304347826087, 0.7409162717219588, 0.7133333333333333, 1.0, 1.0, 0.6231706138761267, 1.0, 1.0, 0.5954287280395846, 1.0, 1.0, 1.0365853658536586, 0.009389672540187188, 0.0, 0.8375144448122397, 0.7741583752539261, 0.7554744741761628, 0.7721306573709579], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5337902], dtype=float32), -0.3354515]. 
=============================================
[2019-03-27 15:52:08,892] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142000, global step 2265559: loss 2.2537
[2019-03-27 15:52:08,897] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142000, global step 2265559: learning rate 0.0000
[2019-03-27 15:52:09,246] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141500, global step 2265687: loss 0.1377
[2019-03-27 15:52:09,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141500, global step 2265687: learning rate 0.0000
[2019-03-27 15:52:10,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:52:10,408] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6412
[2019-03-27 15:52:10,414] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 82.33333333333334, 1.0, 2.0, 0.6309069682024004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 881670.3834702455, 881670.3834702455, 206646.1115757138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5428200.0000, 
sim time next is 5428800.0000, 
raw observation next is [30.8, 83.0, 1.0, 2.0, 0.6339479583494733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 885921.8330604339, 885921.8330604339, 207241.9374351025], 
processed observation next is [1.0, 0.8695652173913043, 0.6587677725118484, 0.83, 1.0, 1.0, 0.558973443794546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24608939807234273, 0.24608939807234273, 0.30931632453000374], 
reward next is 0.6907, 
noisyNet noise sample is [array([1.1066622], dtype=float32), -1.123631]. 
=============================================
[2019-03-27 15:52:11,882] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141500, global step 2266670: loss 0.4679
[2019-03-27 15:52:11,884] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141500, global step 2266670: learning rate 0.0000
[2019-03-27 15:52:11,906] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141500, global step 2266680: loss 0.4351
[2019-03-27 15:52:11,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141500, global step 2266680: learning rate 0.0000
[2019-03-27 15:52:12,082] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:52:12,097] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0333
[2019-03-27 15:52:12,104] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.81666666666667, 53.83333333333334, 1.0, 2.0, 0.5579740119941156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779711.722389029, 779711.722389029, 193177.2254882831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5757000.0000, 
sim time next is 5757600.0000, 
raw observation next is [33.63333333333333, 54.66666666666667, 1.0, 2.0, 0.5401031139084347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754730.0946113296, 754730.0946113289, 190118.7845046877], 
processed observation next is [0.0, 0.6521739130434783, 0.7930489731437599, 0.5466666666666667, 1.0, 1.0, 0.4459073661547406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20964724850314712, 0.20964724850314692, 0.28375937985774285], 
reward next is 0.7162, 
noisyNet noise sample is [array([1.1110979], dtype=float32), 1.1651305]. 
=============================================
[2019-03-27 15:52:12,354] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142000, global step 2266845: loss 2.1646
[2019-03-27 15:52:12,357] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142000, global step 2266846: learning rate 0.0000
[2019-03-27 15:52:12,812] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141500, global step 2267014: loss 0.4849
[2019-03-27 15:52:12,814] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141500, global step 2267014: learning rate 0.0000
[2019-03-27 15:52:13,979] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141500, global step 2267458: loss 0.3021
[2019-03-27 15:52:13,984] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141500, global step 2267459: learning rate 0.0000
[2019-03-27 15:52:14,150] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5620156e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.8755832e-37], sum to 1.0000
[2019-03-27 15:52:14,161] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0196
[2019-03-27 15:52:14,166] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.53333333333333, 82.33333333333334, 1.0, 2.0, 0.6259115741287404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874686.6207055601, 874686.6207055601, 205673.065510432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5431200.0000, 
sim time next is 5431800.0000, 
raw observation next is [30.46666666666667, 82.16666666666667, 1.0, 2.0, 0.62326405176578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870985.2960817601, 870985.2960817601, 205160.4808125163], 
processed observation next is [1.0, 0.8695652173913043, 0.6429699842022119, 0.8216666666666668, 1.0, 1.0, 0.5461012671876868, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24194036002271113, 0.24194036002271113, 0.30620967285450196], 
reward next is 0.6938, 
noisyNet noise sample is [array([-0.26930776], dtype=float32), 2.031035]. 
=============================================
[2019-03-27 15:52:15,826] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142000, global step 2268156: loss 1.9032
[2019-03-27 15:52:15,833] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142000, global step 2268157: learning rate 0.0000
[2019-03-27 15:52:16,201] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141500, global step 2268298: loss 0.5839
[2019-03-27 15:52:16,205] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141500, global step 2268298: learning rate 0.0000
[2019-03-27 15:52:16,966] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141500, global step 2268586: loss 0.4573
[2019-03-27 15:52:16,969] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141500, global step 2268587: learning rate 0.0000
[2019-03-27 15:52:18,551] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142000, global step 2269181: loss 1.4503
[2019-03-27 15:52:18,555] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142000, global step 2269182: learning rate 0.0000
[2019-03-27 15:52:21,026] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142000, global step 2270110: loss 1.2359
[2019-03-27 15:52:21,029] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142000, global step 2270111: learning rate 0.0000
[2019-03-27 15:52:22,764] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8185112e-31 1.0000000e+00 3.5910975e-37 1.8207051e-38 1.2241529e-27], sum to 1.0000
[2019-03-27 15:52:22,771] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1843
[2019-03-27 15:52:22,776] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 87.0, 1.0, 2.0, 0.5148567721783132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719439.3769543099, 719439.3769543099, 185956.1199489629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5703600.0000, 
sim time next is 5704200.0000, 
raw observation next is [26.5, 87.0, 1.0, 2.0, 0.5174428119056992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723054.2308655551, 723054.2308655551, 186373.5141258332], 
processed observation next is [0.0, 0.0, 0.4549763033175356, 0.87, 1.0, 1.0, 0.418605797476746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2008483974626542, 0.2008483974626542, 0.2781694240684078], 
reward next is 0.7218, 
noisyNet noise sample is [array([-0.4115454], dtype=float32), -1.1168339]. 
=============================================
[2019-03-27 15:52:23,285] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142000, global step 2270959: loss 1.1644
[2019-03-27 15:52:23,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142000, global step 2270960: learning rate 0.0000
[2019-03-27 15:52:24,128] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142000, global step 2271276: loss 0.8613
[2019-03-27 15:52:24,138] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142000, global step 2271277: learning rate 0.0000
[2019-03-27 15:52:24,273] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142000, global step 2271327: loss 1.2406
[2019-03-27 15:52:24,277] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142000, global step 2271329: learning rate 0.0000
[2019-03-27 15:52:25,966] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3917325e-26 1.0000000e+00 2.3275166e-32 2.7582490e-28 5.7316968e-19], sum to 1.0000
[2019-03-27 15:52:25,980] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1917
[2019-03-27 15:52:25,986] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 90.0, 1.0, 2.0, 0.5464752681517986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763637.6257394119, 763637.6257394119, 191197.798214859], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5609400.0000, 
sim time next is 5610000.0000, 
raw observation next is [26.93333333333334, 90.0, 1.0, 2.0, 0.5440535270929651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760252.3043683963, 760252.3043683963, 190785.8175681132], 
processed observation next is [1.0, 0.9565217391304348, 0.4755134281200636, 0.9, 1.0, 1.0, 0.45066690011200605, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21118119565788787, 0.21118119565788787, 0.28475495159419884], 
reward next is 0.7152, 
noisyNet noise sample is [array([-2.0337772], dtype=float32), -0.37221]. 
=============================================
[2019-03-27 15:52:26,002] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.21382]
 [75.25538]
 [75.321  ]
 [75.40705]
 [75.61461]], R is [[75.20640564]
 [75.16897583]
 [75.13121796]
 [75.09308624]
 [75.05477905]].
[2019-03-27 15:52:26,219] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142000, global step 2272055: loss 0.8363
[2019-03-27 15:52:26,221] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142000, global step 2272055: learning rate 0.0000
[2019-03-27 15:52:27,556] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:52:27,567] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5224
[2019-03-27 15:52:27,574] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 91.0, 1.0, 2.0, 0.4993119367968658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697710.5388176047, 697710.5388176052, 183488.0833109441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5632200.0000, 
sim time next is 5632800.0000, 
raw observation next is [25.6, 91.0, 1.0, 2.0, 0.4983819537638847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696410.6066368707, 696410.6066368707, 183342.7465845901], 
processed observation next is [0.0, 0.17391304347826086, 0.4123222748815167, 0.91, 1.0, 1.0, 0.3956409081492587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19344739073246406, 0.19344739073246406, 0.27364589042476134], 
reward next is 0.7264, 
noisyNet noise sample is [array([-1.0924782], dtype=float32), 0.32327414]. 
=============================================
[2019-03-27 15:52:30,221] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142500, global step 2273556: loss 12.1592
[2019-03-27 15:52:30,223] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142500, global step 2273556: learning rate 0.0000
[2019-03-27 15:52:30,602] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142000, global step 2273700: loss 1.3454
[2019-03-27 15:52:30,605] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142000, global step 2273701: learning rate 0.0000
[2019-03-27 15:52:33,109] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142000, global step 2274643: loss 0.6336
[2019-03-27 15:52:33,116] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142000, global step 2274644: learning rate 0.0000
[2019-03-27 15:52:33,169] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142000, global step 2274665: loss 0.8321
[2019-03-27 15:52:33,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142000, global step 2274665: learning rate 0.0000
[2019-03-27 15:52:33,596] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142500, global step 2274829: loss 33.8249
[2019-03-27 15:52:33,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142500, global step 2274829: learning rate 0.0000
[2019-03-27 15:52:34,011] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142000, global step 2274985: loss 0.8902
[2019-03-27 15:52:34,016] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142000, global step 2274986: learning rate 0.0000
[2019-03-27 15:52:34,052] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 15:52:34,055] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:52:34,056] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:52:34,059] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:52:34,060] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:52:34,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:52:34,064] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:52:34,064] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:52:34,062] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:52:34,063] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:52:34,074] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:52:34,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run92
[2019-03-27 15:52:34,119] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run92
[2019-03-27 15:52:34,119] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run92
[2019-03-27 15:52:34,120] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run92
[2019-03-27 15:52:34,160] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run92
[2019-03-27 15:52:52,316] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07110079], dtype=float32), 0.10207379]
[2019-03-27 15:52:52,318] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.81161426, 75.42022512, 1.0, 2.0, 0.4091462647179557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606037.8770608385, 606037.8770608385, 174969.3444917518]
[2019-03-27 15:52:52,318] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:52:52,322] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14359551521870006
[2019-03-27 15:52:55,438] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07110079], dtype=float32), 0.10207379]
[2019-03-27 15:52:55,440] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.15097921333334, 92.39471348333333, 1.0, 2.0, 0.3923078316390741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 590623.6554660735, 590623.6554660728, 173828.4384835661]
[2019-03-27 15:52:55,441] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:52:55,444] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3620407e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0142831e-32], sampled 0.8937509361151151
[2019-03-27 15:53:08,008] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07110079], dtype=float32), 0.10207379]
[2019-03-27 15:53:08,008] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.15, 90.66666666666667, 1.0, 2.0, 0.4761736845432868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665368.315175805, 665368.3151758057, 179947.0001062051]
[2019-03-27 15:53:08,009] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:53:08,012] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8048512e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.8828887e-32], sampled 0.6470422068051562
[2019-03-27 15:53:17,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07110079], dtype=float32), 0.10207379]
[2019-03-27 15:53:17,691] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.344776675, 88.45302284499999, 1.0, 2.0, 0.4519981789825545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642255.1738357664, 642255.1738357664, 177795.1296865353]
[2019-03-27 15:53:17,691] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:53:17,694] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.4913087e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.0364924e-34], sampled 0.6962637431199512
[2019-03-27 15:53:56,986] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07110079], dtype=float32), 0.10207379]
[2019-03-27 15:53:56,990] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.5072402645211846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708792.8360748034, 708792.8360748028, 184737.793928313]
[2019-03-27 15:53:56,992] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:53:56,994] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9161892e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2357010e-32], sampled 0.9311256759366094
[2019-03-27 15:54:06,058] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07110079], dtype=float32), 0.10207379]
[2019-03-27 15:54:06,058] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.06666666666667, 59.83333333333333, 1.0, 2.0, 0.5413215084498287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756433.2628154951, 756433.2628154951, 190323.2964620353]
[2019-03-27 15:54:06,060] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:54:06,062] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.2240644e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6066436e-31], sampled 0.1842136806532687
[2019-03-27 15:54:07,096] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07110079], dtype=float32), 0.10207379]
[2019-03-27 15:54:07,096] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.87274613333333, 60.75506053333334, 1.0, 2.0, 0.5313664491039539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742517.3739120477, 742517.373912047, 188656.9885035759]
[2019-03-27 15:54:07,099] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:54:07,102] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.331003e-38], sampled 0.5193021763397324
[2019-03-27 15:54:32,687] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07110079], dtype=float32), 0.10207379]
[2019-03-27 15:54:32,695] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.05, 79.66666666666667, 1.0, 2.0, 0.83946407399747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1173282.636559313, 1173282.636559313, 253832.7109925358]
[2019-03-27 15:54:32,696] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 15:54:32,699] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.7674666e-26 1.0000000e+00 4.1308006e-32 1.9346139e-31 1.0249733e-21], sampled 0.9473671270717628
[2019-03-27 15:54:41,073] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07110079], dtype=float32), 0.10207379]
[2019-03-27 15:54:41,074] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.45761443166667, 87.503859925, 1.0, 2.0, 0.3885250197029515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590013.8323506267, 590013.8323506273, 173911.0367561038]
[2019-03-27 15:54:41,075] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:54:41,081] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.0710872e-28 1.0000000e+00 4.2991190e-34 4.2573907e-33 2.9925397e-23], sampled 0.576700194251196
[2019-03-27 15:54:42,452] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8665.5888 2835847529.9500 735.0000
[2019-03-27 15:54:42,593] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8765.6768 2778312197.6657 702.0000
[2019-03-27 15:54:42,770] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8091.1562 3154336140.5572 1349.0000
[2019-03-27 15:54:43,085] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8407.8596 2925775009.6575 1016.0000
[2019-03-27 15:54:43,200] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8314.4648 2994894387.2409 996.0000
[2019-03-27 15:54:44,219] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2275000, evaluation results [2275000.0, 8091.156198885066, 3154336140.55724, 1349.0, 8407.85955689698, 2925775009.6574864, 1016.0, 8765.676838300114, 2778312197.66569, 702.0, 8314.464803155894, 2994894387.2409334, 996.0, 8665.588846359402, 2835847529.949951, 735.0]
[2019-03-27 15:54:45,362] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142000, global step 2275435: loss 0.6085
[2019-03-27 15:54:45,366] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142000, global step 2275436: learning rate 0.0000
[2019-03-27 15:54:46,020] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1559469e-10 9.2687321e-07 5.8463213e-19 1.5983096e-07 9.9999893e-01], sum to 1.0000
[2019-03-27 15:54:46,027] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7681
[2019-03-27 15:54:46,032] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.5614451103431051, 1.0, 2.0, 0.5614451103431051, 1.0, 2.0, 0.9709145156837155, 6.9112, 6.9112, 170.5573041426782, 2355385.50732503, 2355385.50732503, 459389.025642855], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6091200.0000, 
sim time next is 6091800.0000, 
raw observation next is [31.01666666666667, 65.0, 1.0, 2.0, 0.4655801178652571, 1.0, 2.0, 0.4655801178652571, 1.0, 2.0, 0.8043614751369506, 6.9112, 6.9112, 170.5573041426782, 1952862.801105285, 1952862.801105285, 391461.0688253737], 
processed observation next is [1.0, 0.5217391304347826, 0.6690363349131123, 0.65, 1.0, 1.0, 0.3561206239340447, 1.0, 1.0, 0.3561206239340447, 1.0, 1.0, 0.7614164330938421, 0.0, 0.0, 0.8375144448122397, 0.5424618891959125, 0.5424618891959125, 0.5842702519781697], 
reward next is 0.4157, 
noisyNet noise sample is [array([1.3560431], dtype=float32), -0.9277963]. 
=============================================
[2019-03-27 15:54:47,478] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142500, global step 2276230: loss 27.5816
[2019-03-27 15:54:47,486] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142500, global step 2276231: learning rate 0.0000
[2019-03-27 15:54:47,799] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142000, global step 2276352: loss 1.2050
[2019-03-27 15:54:47,800] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142000, global step 2276352: learning rate 0.0000
[2019-03-27 15:54:47,835] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.1220371e-21 1.0000000e+00 2.6602747e-27 1.1495473e-26 3.4252353e-16], sum to 1.0000
[2019-03-27 15:54:47,847] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8464
[2019-03-27 15:54:47,852] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 92.0, 1.0, 2.0, 0.6178315982302748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 863390.5851196287, 863390.585119628, 204108.6099141939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5981400.0000, 
sim time next is 5982000.0000, 
raw observation next is [26.5, 91.33333333333334, 1.0, 2.0, 0.6156870231956513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 860392.4285413763, 860392.4285413756, 203698.7310376298], 
processed observation next is [1.0, 0.21739130434782608, 0.4549763033175356, 0.9133333333333334, 1.0, 1.0, 0.5369723171031943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23899789681704897, 0.23899789681704878, 0.30402795677258176], 
reward next is 0.6960, 
noisyNet noise sample is [array([1.2361972], dtype=float32), -0.5044896]. 
=============================================
[2019-03-27 15:54:47,868] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[51.072723]
 [51.056564]
 [51.20294 ]
 [51.14379 ]
 [51.317703]], R is [[51.1169014 ]
 [51.30109406]
 [51.48244095]
 [51.65008163]
 [51.81639862]].
[2019-03-27 15:54:48,160] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142000, global step 2276485: loss 0.6911
[2019-03-27 15:54:48,162] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142000, global step 2276485: learning rate 0.0000
[2019-03-27 15:54:50,050] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142500, global step 2277196: loss 25.5356
[2019-03-27 15:54:50,054] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142500, global step 2277196: learning rate 0.0000
[2019-03-27 15:54:50,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7141716e-19 1.0000000e+00 4.4726805e-26 1.1732537e-18 8.6863829e-11], sum to 1.0000
[2019-03-27 15:54:50,996] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4358
[2019-03-27 15:54:51,004] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 91.33333333333334, 1.0, 2.0, 0.5486649305677026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766698.5365819472, 766698.5365819478, 191572.0490867477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5960400.0000, 
sim time next is 5961000.0000, 
raw observation next is [26.95, 91.16666666666667, 1.0, 2.0, 0.5459869711336645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762955.0406100419, 762955.0406100419, 191114.8963855663], 
processed observation next is [1.0, 1.0, 0.476303317535545, 0.9116666666666667, 1.0, 1.0, 0.4529963507634511, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21193195572501164, 0.21193195572501164, 0.2852461140083079], 
reward next is 0.7148, 
noisyNet noise sample is [array([-0.302784], dtype=float32), 0.5349354]. 
=============================================
[2019-03-27 15:54:51,025] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.686035]
 [78.50656 ]
 [78.31813 ]
 [78.10579 ]
 [77.87253 ]], R is [[78.75500488]
 [78.68152618]
 [78.60808563]
 [78.53472137]
 [78.46148682]].
[2019-03-27 15:54:52,461] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142500, global step 2278109: loss -35.3022
[2019-03-27 15:54:52,463] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142500, global step 2278109: learning rate 0.0000
[2019-03-27 15:54:53,362] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4619493e-18 1.0000000e+00 6.0050986e-25 5.9424183e-22 1.5797384e-13], sum to 1.0000
[2019-03-27 15:54:53,377] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1173
[2019-03-27 15:54:53,389] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 93.66666666666667, 1.0, 2.0, 0.6710105856193039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 937738.5253616713, 937738.525361672, 214711.4824841856], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5978400.0000, 
sim time next is 5979000.0000, 
raw observation next is [25.93333333333333, 93.83333333333334, 1.0, 2.0, 0.6643650645838307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928447.3380341934, 928447.3380341934, 213340.6206680305], 
processed observation next is [1.0, 0.17391304347826086, 0.42812006319115314, 0.9383333333333335, 1.0, 1.0, 0.5956205597395551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2579020383428315, 0.2579020383428315, 0.31841883681795596], 
reward next is 0.6816, 
noisyNet noise sample is [array([-0.09042627], dtype=float32), 0.56718767]. 
=============================================
[2019-03-27 15:54:53,408] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[51.029957]
 [51.308968]
 [51.502953]
 [51.57827 ]
 [51.722755]], R is [[51.29256439]
 [51.45917511]
 [51.6221199 ]
 [51.77825165]
 [51.91521835]].
[2019-03-27 15:54:53,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.1237256e-19 1.0000000e+00 1.7775432e-25 8.1611376e-24 1.4299621e-13], sum to 1.0000
[2019-03-27 15:54:53,806] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3250
[2019-03-27 15:54:53,815] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.13333333333334, 89.0, 1.0, 2.0, 0.6905942257001743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 965119.1339047726, 965119.1339047733, 218828.645237434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5984400.0000, 
sim time next is 5985000.0000, 
raw observation next is [27.3, 88.5, 1.0, 2.0, 0.6996532994182234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 977785.1973600307, 977785.1973600313, 220771.1993436064], 
processed observation next is [1.0, 0.2608695652173913, 0.4928909952606636, 0.885, 1.0, 1.0, 0.6381365053231607, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2716069992666752, 0.27160699926667536, 0.3295092527516513], 
reward next is 0.6705, 
noisyNet noise sample is [array([-1.2556583], dtype=float32), -0.98779964]. 
=============================================
[2019-03-27 15:54:53,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[52.530525]
 [52.26008 ]
 [52.4851  ]
 [52.630264]
 [52.745262]], R is [[52.53847504]
 [52.68648148]
 [52.78963089]
 [52.93123245]
 [53.09643555]].
[2019-03-27 15:54:54,682] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142500, global step 2278935: loss 43.8623
[2019-03-27 15:54:54,686] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142500, global step 2278937: learning rate 0.0000
[2019-03-27 15:54:55,557] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5537520e-18 1.0000000e+00 1.8720612e-24 2.0002687e-18 6.3851074e-10], sum to 1.0000
[2019-03-27 15:54:55,574] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7952
[2019-03-27 15:54:55,580] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 88.33333333333334, 1.0, 2.0, 0.5317020573634238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742986.5080006435, 742986.5080006435, 188712.1889039932], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5872200.0000, 
sim time next is 5872800.0000, 
raw observation next is [26.9, 88.66666666666667, 1.0, 2.0, 0.5308549561676368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741802.3776865877, 741802.3776865872, 188571.6623203371], 
processed observation next is [1.0, 1.0, 0.4739336492890995, 0.8866666666666667, 1.0, 1.0, 0.43476500743088764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20605621602405216, 0.206056216024052, 0.2814502422691599], 
reward next is 0.7185, 
noisyNet noise sample is [array([-1.5856092], dtype=float32), -2.1459162]. 
=============================================
[2019-03-27 15:54:55,715] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142500, global step 2279323: loss -59.6406
[2019-03-27 15:54:55,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142500, global step 2279326: learning rate 0.0000
[2019-03-27 15:54:55,824] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142500, global step 2279360: loss 49.4115
[2019-03-27 15:54:55,826] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142500, global step 2279360: learning rate 0.0000
[2019-03-27 15:54:57,249] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7624730e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5051317e-26], sum to 1.0000
[2019-03-27 15:54:57,261] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1464
[2019-03-27 15:54:57,268] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.56666666666667, 80.0, 1.0, 2.0, 0.5403742517199186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755109.1122414505, 755109.1122414505, 190163.9371617231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6031200.0000, 
sim time next is 6031800.0000, 
raw observation next is [28.4, 81.0, 1.0, 2.0, 0.5396856101988879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754146.4755506408, 754146.4755506401, 190047.8956497751], 
processed observation next is [1.0, 0.8260869565217391, 0.5450236966824644, 0.81, 1.0, 1.0, 0.44540434963721437, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20948513209740025, 0.20948513209740005, 0.28365357559667925], 
reward next is 0.7163, 
noisyNet noise sample is [array([0.76125276], dtype=float32), 0.026036227]. 
=============================================
[2019-03-27 15:54:57,526] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0814332e-26 1.0000000e+00 2.3288742e-33 3.3035819e-30 6.9461029e-19], sum to 1.0000
[2019-03-27 15:54:57,539] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3152
[2019-03-27 15:54:57,546] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.36666666666667, 84.0, 1.0, 2.0, 0.5538129679943189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773894.9699436781, 773894.9699436788, 192457.2471815583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5948400.0000, 
sim time next is 5949000.0000, 
raw observation next is [28.3, 84.5, 1.0, 2.0, 0.5560742024513614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777055.9614196413, 777055.9614196407, 192848.2197328485], 
processed observation next is [1.0, 0.8695652173913043, 0.5402843601895735, 0.845, 1.0, 1.0, 0.4651496415076643, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2158488781721226, 0.21584887817212242, 0.2878331637803709], 
reward next is 0.7122, 
noisyNet noise sample is [array([-0.24949762], dtype=float32), 1.0648987]. 
=============================================
[2019-03-27 15:54:57,563] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.355774]
 [80.19853 ]
 [80.17337 ]
 [80.16757 ]
 [80.08758 ]], R is [[80.22019958]
 [80.13075256]
 [80.04206848]
 [79.9539566 ]
 [79.86593628]].
[2019-03-27 15:54:57,721] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142500, global step 2280082: loss 30.8309
[2019-03-27 15:54:57,725] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142500, global step 2280082: learning rate 0.0000
[2019-03-27 15:55:01,000] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0469561e-16 1.0000000e+00 2.5099935e-23 1.2975191e-19 2.9487840e-10], sum to 1.0000
[2019-03-27 15:55:01,011] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4398
[2019-03-27 15:55:01,015] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 91.0, 1.0, 2.0, 0.680065376765337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 950398.2746481972, 950398.2746481972, 216601.6165847068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5896800.0000, 
sim time next is 5897400.0000, 
raw observation next is [27.0, 90.16666666666667, 1.0, 2.0, 0.6862671686628288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959069.2553212187, 959069.2553212187, 217909.5682786445], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.9016666666666667, 1.0, 1.0, 0.6220086369431672, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2664081264781163, 0.2664081264781163, 0.32523816160991714], 
reward next is 0.6748, 
noisyNet noise sample is [array([0.6864382], dtype=float32), 0.31504753]. 
=============================================
[2019-03-27 15:55:01,414] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5146256e-28 1.0000000e+00 3.5176536e-36 3.8381541e-34 1.0694772e-21], sum to 1.0000
[2019-03-27 15:55:01,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8175
[2019-03-27 15:55:01,430] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.2, 79.33333333333334, 1.0, 2.0, 0.5585684835667567, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780542.740949457, 780542.740949457, 193284.0644730243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5937600.0000, 
sim time next is 5938200.0000, 
raw observation next is [30.1, 79.5, 1.0, 2.0, 0.5539095570806589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 774029.9921404026, 774029.9921404033, 192476.6007790316], 
processed observation next is [1.0, 0.7391304347826086, 0.6255924170616115, 0.795, 1.0, 1.0, 0.46254163503693846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2150083311501118, 0.215008331150112, 0.2872785086254203], 
reward next is 0.7127, 
noisyNet noise sample is [array([-0.30959117], dtype=float32), 0.18873635]. 
=============================================
[2019-03-27 15:55:01,687] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143000, global step 2281576: loss 0.1878
[2019-03-27 15:55:01,688] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143000, global step 2281576: learning rate 0.0000
[2019-03-27 15:55:02,016] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142500, global step 2281695: loss 16.8257
[2019-03-27 15:55:02,020] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142500, global step 2281695: learning rate 0.0000
[2019-03-27 15:55:04,499] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142500, global step 2282629: loss 24.0466
[2019-03-27 15:55:04,503] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142500, global step 2282630: learning rate 0.0000
[2019-03-27 15:55:04,534] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142500, global step 2282643: loss -71.4527
[2019-03-27 15:55:04,536] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142500, global step 2282643: learning rate 0.0000
[2019-03-27 15:55:04,906] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143000, global step 2282783: loss 0.0935
[2019-03-27 15:55:04,907] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143000, global step 2282783: learning rate 0.0000
[2019-03-27 15:55:05,506] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142500, global step 2283008: loss -95.2054
[2019-03-27 15:55:05,510] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142500, global step 2283009: learning rate 0.0000
[2019-03-27 15:55:06,743] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142500, global step 2283478: loss 14.0657
[2019-03-27 15:55:06,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142500, global step 2283478: learning rate 0.0000
[2019-03-27 15:55:08,513] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143000, global step 2284144: loss 0.2799
[2019-03-27 15:55:08,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143000, global step 2284144: learning rate 0.0000
[2019-03-27 15:55:09,289] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142500, global step 2284440: loss -46.5505
[2019-03-27 15:55:09,292] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142500, global step 2284440: learning rate 0.0000
[2019-03-27 15:55:09,470] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142500, global step 2284505: loss -128.6758
[2019-03-27 15:55:09,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142500, global step 2284505: learning rate 0.0000
[2019-03-27 15:55:10,882] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:55:10,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9602
[2019-03-27 15:55:10,902] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333334, 89.66666666666667, 1.0, 2.0, 0.5264028862061872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735579.0213199375, 735579.0213199375, 187836.5630195024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6240000.0000, 
sim time next is 6240600.0000, 
raw observation next is [26.8, 89.5, 1.0, 2.0, 0.5272986350211235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736831.1469815219, 736831.1469815212, 187984.0942567497], 
processed observation next is [0.0, 0.21739130434782608, 0.4691943127962086, 0.895, 1.0, 1.0, 0.4304802831579801, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2046753186059783, 0.2046753186059781, 0.2805732750100742], 
reward next is 0.7194, 
noisyNet noise sample is [array([1.5916018], dtype=float32), 1.2331833]. 
=============================================
[2019-03-27 15:55:11,168] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143000, global step 2285137: loss 0.4591
[2019-03-27 15:55:11,174] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143000, global step 2285140: learning rate 0.0000
[2019-03-27 15:55:13,604] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143000, global step 2286054: loss 0.5494
[2019-03-27 15:55:13,605] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143000, global step 2286055: learning rate 0.0000
[2019-03-27 15:55:15,859] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143000, global step 2286905: loss 0.5145
[2019-03-27 15:55:15,863] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143000, global step 2286906: learning rate 0.0000
[2019-03-27 15:55:16,808] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143000, global step 2287264: loss 0.4813
[2019-03-27 15:55:16,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143000, global step 2287266: learning rate 0.0000
[2019-03-27 15:55:16,954] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143000, global step 2287319: loss 0.5543
[2019-03-27 15:55:16,959] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143000, global step 2287320: learning rate 0.0000
[2019-03-27 15:55:17,686] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:55:17,697] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1680
[2019-03-27 15:55:17,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333334, 88.33333333333334, 1.0, 2.0, 0.5260192534080648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735042.7593023086, 735042.759302308, 187772.991973549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6322800.0000, 
sim time next is 6323400.0000, 
raw observation next is [26.7, 88.5, 1.0, 2.0, 0.5249457330007851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733542.1376911312, 733542.1376911306, 187596.7300470663], 
processed observation next is [0.0, 0.17391304347826086, 0.46445497630331756, 0.885, 1.0, 1.0, 0.427645461446729, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20376170491420312, 0.20376170491420295, 0.27999511947323324], 
reward next is 0.7200, 
noisyNet noise sample is [array([-1.3872486], dtype=float32), 0.18859929]. 
=============================================
[2019-03-27 15:55:18,947] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143000, global step 2288068: loss 0.7323
[2019-03-27 15:55:18,951] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143000, global step 2288069: learning rate 0.0000
[2019-03-27 15:55:19,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6509115e-30 1.0000000e+00 1.1026476e-36 0.0000000e+00 5.7040843e-26], sum to 1.0000
[2019-03-27 15:55:19,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9391
[2019-03-27 15:55:19,617] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 86.0, 1.0, 2.0, 0.5203364587886706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727099.0830599112, 727099.0830599112, 186843.8861402845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6564600.0000, 
sim time next is 6565200.0000, 
raw observation next is [27.0, 86.33333333333334, 1.0, 2.0, 0.5202701149958331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727006.3449525684, 727006.3449525678, 186833.0928271207], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.8633333333333334, 1.0, 1.0, 0.42201218674196755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.201946206931269, 0.20194620693126883, 0.27885536242853837], 
reward next is 0.7211, 
noisyNet noise sample is [array([-1.0249386], dtype=float32), 2.1371222]. 
=============================================
[2019-03-27 15:55:19,848] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:55:19,856] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4315
[2019-03-27 15:55:19,861] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.61666666666667, 68.66666666666667, 1.0, 2.0, 0.5432324178786426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759104.4885729792, 759104.4885729786, 190647.2397184563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6261000.0000, 
sim time next is 6261600.0000, 
raw observation next is [30.63333333333333, 68.33333333333334, 1.0, 2.0, 0.5423444730103086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757863.2456525544, 757863.2456525537, 190496.8040392407], 
processed observation next is [0.0, 0.4782608695652174, 0.6508688783570299, 0.6833333333333335, 1.0, 1.0, 0.44860779880760077, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2105175682368207, 0.2105175682368205, 0.2843235881182697], 
reward next is 0.7157, 
noisyNet noise sample is [array([2.0004451], dtype=float32), -1.5168749]. 
=============================================
[2019-03-27 15:55:21,872] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:55:21,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6803
[2019-03-27 15:55:21,889] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.85, 73.16666666666667, 1.0, 2.0, 0.541994185220643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757373.5847509764, 757373.5847509764, 190437.789080957], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6257400.0000, 
sim time next is 6258000.0000, 
raw observation next is [30.0, 72.33333333333334, 1.0, 2.0, 0.5419008612283716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757243.1288852462, 757243.1288852456, 190422.0233671023], 
processed observation next is [0.0, 0.43478260869565216, 0.6208530805687204, 0.7233333333333334, 1.0, 1.0, 0.4480733267811705, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21034531357923505, 0.21034531357923492, 0.28421197517477953], 
reward next is 0.7158, 
noisyNet noise sample is [array([-1.688192], dtype=float32), 0.20996049]. 
=============================================
[2019-03-27 15:55:21,922] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.90867]
 [74.89156]
 [74.72943]
 [74.74433]
 [74.75173]], R is [[74.88044739]
 [74.84741211]
 [74.81471252]
 [74.78236389]
 [74.75035858]].
[2019-03-27 15:55:22,969] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143500, global step 2289581: loss 79.4048
[2019-03-27 15:55:22,974] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143500, global step 2289581: learning rate 0.0000
[2019-03-27 15:55:23,283] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143000, global step 2289695: loss 0.5587
[2019-03-27 15:55:23,288] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143000, global step 2289697: learning rate 0.0000
[2019-03-27 15:55:25,629] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143000, global step 2290581: loss 0.7600
[2019-03-27 15:55:25,634] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143000, global step 2290581: learning rate 0.0000
[2019-03-27 15:55:25,701] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143000, global step 2290606: loss 0.6697
[2019-03-27 15:55:25,704] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143000, global step 2290606: learning rate 0.0000
[2019-03-27 15:55:26,357] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143500, global step 2290852: loss 42.3883
[2019-03-27 15:55:26,359] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143500, global step 2290852: learning rate 0.0000
[2019-03-27 15:55:26,708] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143000, global step 2290987: loss 0.8482
[2019-03-27 15:55:26,713] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143000, global step 2290987: learning rate 0.0000
[2019-03-27 15:55:26,936] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.517363e-36 1.000000e+00 0.000000e+00 0.000000e+00 2.131891e-34], sum to 1.0000
[2019-03-27 15:55:26,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9454
[2019-03-27 15:55:26,953] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 64.5, 1.0, 2.0, 0.5319731121381953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743365.4053665273, 743365.4053665273, 188757.3890295669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6363000.0000, 
sim time next is 6363600.0000, 
raw observation next is [31.0, 64.0, 1.0, 2.0, 0.5296479157171096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740115.104356034, 740115.104356034, 188371.7588722059], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.64, 1.0, 1.0, 0.4333107418278428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2055875289877872, 0.2055875289877872, 0.2811518789137401], 
reward next is 0.7188, 
noisyNet noise sample is [array([0.6416493], dtype=float32), 0.7669269]. 
=============================================
[2019-03-27 15:55:27,848] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143000, global step 2291412: loss 0.7300
[2019-03-27 15:55:27,850] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143000, global step 2291412: learning rate 0.0000
[2019-03-27 15:55:30,262] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143500, global step 2292318: loss 127.6312
[2019-03-27 15:55:30,265] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143500, global step 2292318: learning rate 0.0000
[2019-03-27 15:55:30,670] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143000, global step 2292476: loss 1.0505
[2019-03-27 15:55:30,671] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143000, global step 2292476: learning rate 0.0000
[2019-03-27 15:55:30,826] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143000, global step 2292534: loss 1.0041
[2019-03-27 15:55:30,829] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143000, global step 2292534: learning rate 0.0000
[2019-03-27 15:55:32,730] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143500, global step 2293235: loss 93.8372
[2019-03-27 15:55:32,733] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143500, global step 2293235: learning rate 0.0000
[2019-03-27 15:55:32,936] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1986717e-29 1.0000000e+00 1.3703958e-34 4.9606436e-38 2.1101763e-25], sum to 1.0000
[2019-03-27 15:55:32,946] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7074
[2019-03-27 15:55:32,952] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 83.0, 1.0, 2.0, 0.510108049918798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712801.4841913255, 712801.4841913255, 185194.6590759626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6400200.0000, 
sim time next is 6400800.0000, 
raw observation next is [27.0, 83.0, 1.0, 2.0, 0.5092870476654828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711653.8691810739, 711653.8691810733, 185063.6780795819], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.83, 1.0, 1.0, 0.4087795755005817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19768163032807606, 0.1976816303280759, 0.2762144448948984], 
reward next is 0.7238, 
noisyNet noise sample is [array([-0.4543386], dtype=float32), 0.25041026]. 
=============================================
[2019-03-27 15:55:35,226] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143500, global step 2294178: loss 46.9920
[2019-03-27 15:55:35,228] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143500, global step 2294178: learning rate 0.0000
[2019-03-27 15:55:37,075] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143500, global step 2294877: loss 38.3068
[2019-03-27 15:55:37,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143500, global step 2294878: learning rate 0.0000
[2019-03-27 15:55:37,564] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5831922e-28 1.0000000e+00 9.0517531e-35 2.1587311e-36 8.8189048e-25], sum to 1.0000
[2019-03-27 15:55:37,576] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3100
[2019-03-27 15:55:37,581] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.95, 91.0, 1.0, 2.0, 0.723737547369012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1011459.641126569, 1011459.641126569, 226050.5412515593], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6679800.0000, 
sim time next is 6680400.0000, 
raw observation next is [26.03333333333333, 90.66666666666667, 1.0, 2.0, 0.6725707463073679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 939919.8176081327, 939919.8176081327, 215034.3510619343], 
processed observation next is [1.0, 0.30434782608695654, 0.4328593996840442, 0.9066666666666667, 1.0, 1.0, 0.605506923261889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2610888382244813, 0.2610888382244813, 0.32094679262975273], 
reward next is 0.6791, 
noisyNet noise sample is [array([0.5783114], dtype=float32), 0.22686544]. 
=============================================
[2019-03-27 15:55:38,014] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143500, global step 2295228: loss 12.1358
[2019-03-27 15:55:38,016] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143500, global step 2295228: learning rate 0.0000
[2019-03-27 15:55:38,193] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143500, global step 2295299: loss 27.2886
[2019-03-27 15:55:38,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143500, global step 2295299: learning rate 0.0000
[2019-03-27 15:55:40,206] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143500, global step 2296058: loss 19.2128
[2019-03-27 15:55:40,209] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143500, global step 2296060: learning rate 0.0000
[2019-03-27 15:55:44,153] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144000, global step 2297539: loss 34.1128
[2019-03-27 15:55:44,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144000, global step 2297540: learning rate 0.0000
[2019-03-27 15:55:44,710] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143500, global step 2297747: loss 18.7040
[2019-03-27 15:55:44,713] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143500, global step 2297748: learning rate 0.0000
[2019-03-27 15:55:46,766] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143500, global step 2298523: loss 21.0767
[2019-03-27 15:55:46,769] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143500, global step 2298524: learning rate 0.0000
[2019-03-27 15:55:46,810] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143500, global step 2298539: loss -37.5850
[2019-03-27 15:55:46,812] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143500, global step 2298539: learning rate 0.0000
[2019-03-27 15:55:47,605] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144000, global step 2298836: loss 37.6308
[2019-03-27 15:55:47,607] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144000, global step 2298837: learning rate 0.0000
[2019-03-27 15:55:47,945] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143500, global step 2298966: loss -66.0987
[2019-03-27 15:55:47,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143500, global step 2298966: learning rate 0.0000
[2019-03-27 15:55:48,963] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143500, global step 2299346: loss 7.0612
[2019-03-27 15:55:48,968] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143500, global step 2299350: learning rate 0.0000
[2019-03-27 15:55:50,710] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 15:55:50,711] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:55:50,712] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:55:50,714] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:55:50,715] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:55:50,716] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:55:50,716] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:55:50,717] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:55:50,719] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:55:50,719] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:55:50,720] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:55:50,739] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run93
[2019-03-27 15:55:50,764] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run93
[2019-03-27 15:55:50,765] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run93
[2019-03-27 15:55:50,765] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run93
[2019-03-27 15:55:50,787] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run93
[2019-03-27 15:56:02,643] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:56:02,646] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.64327208666667, 91.92569777666668, 1.0, 2.0, 0.2559998313540604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 421949.0138133526, 421949.013813352, 161482.593554464]
[2019-03-27 15:56:02,649] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:56:02,655] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7549653994558795
[2019-03-27 15:56:22,439] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:56:22,442] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.486140125, 89.60221984500001, 1.0, 2.0, 0.36047069319512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 555942.1998991336, 555942.199899133, 171163.3430569507]
[2019-03-27 15:56:22,447] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:56:22,449] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5314712006962139
[2019-03-27 15:56:29,485] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:56:29,486] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.19646169, 90.925884, 1.0, 2.0, 0.6037985481171737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 843772.2683693325, 843772.2683693332, 201454.816731421]
[2019-03-27 15:56:29,487] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:56:29,491] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.43231561e-30 1.00000000e+00 1.02810656e-35 6.98304467e-38
 5.84835899e-26], sampled 0.025674658964433994
[2019-03-27 15:56:29,859] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:56:29,861] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.25, 69.5, 1.0, 2.0, 0.7810506359349566, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988159736141146, 6.9112, 168.9124368909914, 1988553.384697465, 1933955.667086293, 403873.413557491]
[2019-03-27 15:56:29,861] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:56:29,864] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7443702e-12 9.9963510e-01 9.7992927e-18 1.2081931e-12 3.6487327e-04], sampled 0.4621166413527348
[2019-03-27 15:56:29,865] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1988553.384697465 W.
[2019-03-27 15:57:06,438] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:57:06,439] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.71666666666667, 69.16666666666667, 1.0, 2.0, 0.8575848073644571, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.998027201507725, 6.9112, 168.9124399019743, 2095668.636018531, 2034070.619301105, 422865.0675679167]
[2019-03-27 15:57:06,440] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:57:06,442] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2032057e-16 1.0000000e+00 1.0115371e-22 3.4400265e-19 1.2484110e-09], sampled 0.8444269637585002
[2019-03-27 15:57:06,446] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2095668.636018531 W.
[2019-03-27 15:57:14,559] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:57:14,560] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.9, 84.0, 1.0, 2.0, 0.6146994956877259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859011.8491990155, 859011.8491990155, 203516.5844316198]
[2019-03-27 15:57:14,562] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:57:14,566] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.3097017e-23 1.0000000e+00 1.7335510e-28 5.9594516e-27 2.2410946e-16], sampled 0.1179377800201904
[2019-03-27 15:57:19,367] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:57:19,368] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.71122309, 83.56440072, 1.0, 2.0, 0.6325497038381928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 883967.0043011481, 883967.0043011488, 206967.7723143359]
[2019-03-27 15:57:19,368] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:57:19,372] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1948722e-23 1.0000000e+00 2.3605415e-29 4.2458214e-28 2.6552393e-17], sampled 0.3440026427074455
[2019-03-27 15:57:33,098] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:57:33,099] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.03333333333333, 81.00000000000001, 1.0, 2.0, 0.5759554872904371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 804848.5184491287, 804848.5184491294, 196351.1937501297]
[2019-03-27 15:57:33,100] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 15:57:33,101] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.361742e-38 1.000000e+00 0.000000e+00 0.000000e+00 3.428573e-36], sampled 0.08287424467663462
[2019-03-27 15:57:39,585] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:57:39,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.75, 90.5, 1.0, 2.0, 0.4968094246478205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 694212.5258494577, 694212.5258494583, 183097.758938083]
[2019-03-27 15:57:39,589] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 15:57:39,592] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0474970e-29 1.0000000e+00 4.4766031e-35 7.4134518e-37 4.1041452e-25], sampled 0.03679009987093307
[2019-03-27 15:57:56,065] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:57:56,066] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.7486146, 92.99049697500001, 1.0, 2.0, 0.7149555443975916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1050027.604521161, 1050027.604521161, 230837.1781052851]
[2019-03-27 15:57:56,067] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 15:57:56,069] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.7646424e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1263286e-33], sampled 0.04163029240256033
[2019-03-27 15:57:57,209] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07217655], dtype=float32), 0.10252706]
[2019-03-27 15:57:57,210] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.99068531333334, 91.57923312, 1.0, 2.0, 0.5171270524018818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722612.8508781121, 722612.8508781121, 186322.7408027426]
[2019-03-27 15:57:57,211] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 15:57:57,215] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6876073066879397
[2019-03-27 15:57:58,972] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8350.3546 2926110235.7187 1147.0000
[2019-03-27 15:57:59,793] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8735.8113 2778296457.0632 774.0000
[2019-03-27 15:57:59,849] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8222.7494 2998071185.3215 1217.0000
[2019-03-27 15:57:59,933] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8031.5624 3157171362.2956 1516.0000
[2019-03-27 15:58:00,030] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8618.2703 2837136429.8093 854.0000
[2019-03-27 15:58:01,049] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2300000, evaluation results [2300000.0, 8031.562413088977, 3157171362.295578, 1516.0, 8350.354557112549, 2926110235.71874, 1147.0, 8735.811336357801, 2778296457.0632153, 774.0, 8222.74944117843, 2998071185.321452, 1217.0, 8618.270265082803, 2837136429.8092723, 854.0]
[2019-03-27 15:58:01,806] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144000, global step 2300288: loss 40.8132
[2019-03-27 15:58:01,811] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144000, global step 2300289: learning rate 0.0000
[2019-03-27 15:58:02,426] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143500, global step 2300519: loss 4.9379
[2019-03-27 15:58:02,430] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143500, global step 2300522: learning rate 0.0000
[2019-03-27 15:58:02,586] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143500, global step 2300577: loss 15.2264
[2019-03-27 15:58:02,590] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143500, global step 2300579: learning rate 0.0000
[2019-03-27 15:58:04,176] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144000, global step 2301177: loss 39.1488
[2019-03-27 15:58:04,178] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144000, global step 2301177: learning rate 0.0000
[2019-03-27 15:58:06,713] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144000, global step 2302129: loss 41.4769
[2019-03-27 15:58:06,716] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144000, global step 2302130: learning rate 0.0000
[2019-03-27 15:58:08,392] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144000, global step 2302756: loss 34.3893
[2019-03-27 15:58:08,394] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144000, global step 2302756: learning rate 0.0000
[2019-03-27 15:58:08,487] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:58:08,496] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0476
[2019-03-27 15:58:08,501] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 75.83333333333334, 1.0, 2.0, 0.4289399245452645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622872.2189962886, 622872.218996288, 176230.1622233329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6937800.0000, 
sim time next is 6938400.0000, 
raw observation next is [26.5, 74.66666666666667, 1.0, 2.0, 0.430541564149567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 624812.9897712702, 624812.9897712702, 176408.8813876651], 
processed observation next is [0.0, 0.30434782608695654, 0.4549763033175356, 0.7466666666666667, 1.0, 1.0, 0.3139054989753819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17355916382535283, 0.17355916382535283, 0.2632968378920375], 
reward next is 0.7367, 
noisyNet noise sample is [array([-0.5283039], dtype=float32), -0.33424988]. 
=============================================
[2019-03-27 15:58:09,522] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144000, global step 2303179: loss 33.7759
[2019-03-27 15:58:09,524] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144000, global step 2303180: learning rate 0.0000
[2019-03-27 15:58:09,577] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144000, global step 2303201: loss 33.9925
[2019-03-27 15:58:09,580] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144000, global step 2303201: learning rate 0.0000
[2019-03-27 15:58:11,687] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144000, global step 2303996: loss 37.8910
[2019-03-27 15:58:11,690] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144000, global step 2303996: learning rate 0.0000
[2019-03-27 15:58:12,909] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:58:12,920] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7085
[2019-03-27 15:58:12,925] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.63333333333333, 52.0, 1.0, 2.0, 0.4622469387438832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649180.906625034, 649180.906625034, 178316.582657033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6960000.0000, 
sim time next is 6960600.0000, 
raw observation next is [31.45, 52.0, 1.0, 2.0, 0.4579014458159271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647568.229588663, 647568.2295886637, 178262.1412250422], 
processed observation next is [0.0, 0.5652173913043478, 0.6895734597156398, 0.52, 1.0, 1.0, 0.3468692118264183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1798800637746286, 0.17988006377462878, 0.26606289735080924], 
reward next is 0.7339, 
noisyNet noise sample is [array([0.56006044], dtype=float32), 0.19103085]. 
=============================================
[2019-03-27 15:58:13,550] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:58:13,558] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8008
[2019-03-27 15:58:13,563] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 86.0, 1.0, 2.0, 0.484231669189441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676631.5062332678, 676631.5062332678, 181162.0447605168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7074000.0000, 
sim time next is 7074600.0000, 
raw observation next is [25.85, 86.16666666666667, 1.0, 2.0, 0.4853674686727924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678219.0997227237, 678219.0997227244, 181334.7618072692], 
processed observation next is [1.0, 0.9130434782608695, 0.4241706161137442, 0.8616666666666667, 1.0, 1.0, 0.3799608056298704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18839419436742325, 0.18839419436742344, 0.2706488982198048], 
reward next is 0.7294, 
noisyNet noise sample is [array([-2.4993982], dtype=float32), 0.112506755]. 
=============================================
[2019-03-27 15:58:16,187] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144000, global step 2305682: loss 34.5675
[2019-03-27 15:58:16,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144000, global step 2305682: learning rate 0.0000
[2019-03-27 15:58:16,479] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144500, global step 2305788: loss -136.9024
[2019-03-27 15:58:16,481] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144500, global step 2305789: learning rate 0.0000
[2019-03-27 15:58:17,512] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1448403e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7296656e-37], sum to 1.0000
[2019-03-27 15:58:17,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0921
[2019-03-27 15:58:17,528] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 71.0, 1.0, 2.0, 0.705546750514573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1011798.27218437, 1011798.27218437, 225494.4398628975], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7026000.0000, 
sim time next is 7026600.0000, 
raw observation next is [27.55, 70.0, 1.0, 2.0, 0.6918252489627995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 992878.5556813459, 992878.5556813465, 222552.263091632], 
processed observation next is [1.0, 0.30434782608695654, 0.504739336492891, 0.7, 1.0, 1.0, 0.6287051192322886, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27579959880037386, 0.275799598800374, 0.33216755685318206], 
reward next is 0.6678, 
noisyNet noise sample is [array([0.8686176], dtype=float32), 1.4964015]. 
=============================================
[2019-03-27 15:58:18,340] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144000, global step 2306490: loss 28.9660
[2019-03-27 15:58:18,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144000, global step 2306491: learning rate 0.0000
[2019-03-27 15:58:18,513] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144000, global step 2306552: loss 29.4298
[2019-03-27 15:58:18,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144000, global step 2306554: learning rate 0.0000
[2019-03-27 15:58:19,503] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144500, global step 2306924: loss 65.6779
[2019-03-27 15:58:19,508] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144500, global step 2306925: learning rate 0.0000
[2019-03-27 15:58:19,525] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144000, global step 2306930: loss 29.1226
[2019-03-27 15:58:19,528] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144000, global step 2306930: learning rate 0.0000
[2019-03-27 15:58:20,512] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144000, global step 2307301: loss 27.1925
[2019-03-27 15:58:20,516] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144000, global step 2307301: learning rate 0.0000
[2019-03-27 15:58:22,822] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:58:22,830] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6038
[2019-03-27 15:58:22,847] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 65.66666666666667, 1.0, 2.0, 0.3727971163437828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565719.840333914, 565719.840333914, 171743.2627513431], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6898800.0000, 
sim time next is 6899400.0000, 
raw observation next is [26.6, 66.5, 1.0, 2.0, 0.3763526246168568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 569809.6062249024, 569809.606224903, 172061.2672329463], 
processed observation next is [0.0, 0.8695652173913043, 0.4597156398104266, 0.665, 1.0, 1.0, 0.2486176200203094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.158280446173584, 0.15828044617358417, 0.25680786154171087], 
reward next is 0.7432, 
noisyNet noise sample is [array([-0.4866894], dtype=float32), 0.2484963]. 
=============================================
[2019-03-27 15:58:23,131] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:58:23,141] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5158
[2019-03-27 15:58:23,145] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.08333333333333, 52.0, 1.0, 2.0, 0.4351438101654626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626252.4519089366, 626252.4519089366, 176405.8876938332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6963000.0000, 
sim time next is 6963600.0000, 
raw observation next is [31.26666666666667, 52.0, 1.0, 2.0, 0.4399316923856428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 629187.1688081487, 629187.1688081481, 176587.303306195], 
processed observation next is [0.0, 0.6086956521739131, 0.6808846761453398, 0.52, 1.0, 1.0, 0.3252189064887262, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17477421355781908, 0.1747742135578189, 0.26356313926297764], 
reward next is 0.7364, 
noisyNet noise sample is [array([-0.44306204], dtype=float32), -0.61801934]. 
=============================================
[2019-03-27 15:58:23,615] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144500, global step 2308468: loss -140.3298
[2019-03-27 15:58:23,620] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144500, global step 2308470: learning rate 0.0000
[2019-03-27 15:58:23,740] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144000, global step 2308513: loss 23.8545
[2019-03-27 15:58:23,745] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144000, global step 2308517: learning rate 0.0000
[2019-03-27 15:58:23,850] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144000, global step 2308557: loss 23.6160
[2019-03-27 15:58:23,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144000, global step 2308557: learning rate 0.0000
[2019-03-27 15:58:25,892] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144500, global step 2309322: loss -117.2559
[2019-03-27 15:58:25,894] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144500, global step 2309322: learning rate 0.0000
[2019-03-27 15:58:28,247] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144500, global step 2310214: loss 4.6675
[2019-03-27 15:58:28,251] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144500, global step 2310214: learning rate 0.0000
[2019-03-27 15:58:29,961] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144500, global step 2310856: loss -61.5736
[2019-03-27 15:58:29,963] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144500, global step 2310856: learning rate 0.0000
[2019-03-27 15:58:31,075] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144500, global step 2311272: loss 51.0032
[2019-03-27 15:58:31,078] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144500, global step 2311273: learning rate 0.0000
[2019-03-27 15:58:31,264] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144500, global step 2311341: loss -17.6167
[2019-03-27 15:58:31,269] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144500, global step 2311342: learning rate 0.0000
[2019-03-27 15:58:33,369] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144500, global step 2312126: loss -78.3289
[2019-03-27 15:58:33,371] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144500, global step 2312126: learning rate 0.0000
[2019-03-27 15:58:33,638] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:58:33,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5586
[2019-03-27 15:58:33,662] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 92.0, 1.0, 2.0, 0.3657914902121746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 559219.1450910879, 559219.1450910879, 171306.429318115], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7246800.0000, 
sim time next is 7247400.0000, 
raw observation next is [22.48333333333333, 91.83333333333334, 1.0, 2.0, 0.3647708525045902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558178.6940419843, 558178.6940419843, 171232.6250948838], 
processed observation next is [1.0, 0.9130434782608695, 0.26461295418641384, 0.9183333333333334, 1.0, 1.0, 0.2346636777163737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1550496372338845, 0.1550496372338845, 0.25557108223116987], 
reward next is 0.7444, 
noisyNet noise sample is [array([0.643775], dtype=float32), 1.047145]. 
=============================================
[2019-03-27 15:58:37,100] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145000, global step 2313531: loss 0.0100
[2019-03-27 15:58:37,106] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145000, global step 2313534: learning rate 0.0000
[2019-03-27 15:58:37,772] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144500, global step 2313784: loss -58.3153
[2019-03-27 15:58:37,777] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144500, global step 2313784: learning rate 0.0000
[2019-03-27 15:58:38,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:58:38,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9133
[2019-03-27 15:58:38,838] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 85.66666666666667, 1.0, 2.0, 0.6509787782288559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 909732.0479033501, 909732.0479033495, 210620.214580618], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7197000.0000, 
sim time next is 7197600.0000, 
raw observation next is [27.66666666666667, 85.33333333333334, 1.0, 2.0, 0.6224901344569533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869903.3362911043, 869903.3362911043, 205004.9677661726], 
processed observation next is [1.0, 0.30434782608695654, 0.5102685624012641, 0.8533333333333334, 1.0, 1.0, 0.5451688366951245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24163981563641787, 0.24163981563641787, 0.3059775638301084], 
reward next is 0.6940, 
noisyNet noise sample is [array([0.1223421], dtype=float32), 0.65740883]. 
=============================================
[2019-03-27 15:58:39,941] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144500, global step 2314590: loss -59.5516
[2019-03-27 15:58:39,944] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144500, global step 2314590: learning rate 0.0000
[2019-03-27 15:58:40,001] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144500, global step 2314617: loss 32.1834
[2019-03-27 15:58:40,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144500, global step 2314620: learning rate 0.0000
[2019-03-27 15:58:40,216] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145000, global step 2314699: loss 0.0057
[2019-03-27 15:58:40,219] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145000, global step 2314699: learning rate 0.0000
[2019-03-27 15:58:41,068] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144500, global step 2315019: loss -88.2061
[2019-03-27 15:58:41,070] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144500, global step 2315019: learning rate 0.0000
[2019-03-27 15:58:41,961] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144500, global step 2315355: loss -121.5302
[2019-03-27 15:58:41,965] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144500, global step 2315355: learning rate 0.0000
[2019-03-27 15:58:44,406] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145000, global step 2316264: loss 0.0107
[2019-03-27 15:58:44,408] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145000, global step 2316265: learning rate 0.0000
[2019-03-27 15:58:45,287] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144500, global step 2316592: loss -175.5851
[2019-03-27 15:58:45,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144500, global step 2316593: learning rate 0.0000
[2019-03-27 15:58:45,447] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144500, global step 2316651: loss -80.2178
[2019-03-27 15:58:45,454] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144500, global step 2316651: learning rate 0.0000
[2019-03-27 15:58:46,914] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145000, global step 2317204: loss 0.0043
[2019-03-27 15:58:46,916] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145000, global step 2317204: learning rate 0.0000
[2019-03-27 15:58:49,317] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145000, global step 2318100: loss 0.0073
[2019-03-27 15:58:49,320] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145000, global step 2318100: learning rate 0.0000
[2019-03-27 15:58:50,940] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145000, global step 2318723: loss 0.0042
[2019-03-27 15:58:50,942] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145000, global step 2318724: learning rate 0.0000
[2019-03-27 15:58:52,248] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145000, global step 2319210: loss 0.0054
[2019-03-27 15:58:52,251] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145000, global step 2319211: learning rate 0.0000
[2019-03-27 15:58:52,452] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145000, global step 2319286: loss 0.0065
[2019-03-27 15:58:52,456] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145000, global step 2319286: learning rate 0.0000
[2019-03-27 15:58:54,494] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145000, global step 2320039: loss 0.0079
[2019-03-27 15:58:54,496] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145000, global step 2320040: learning rate 0.0000
[2019-03-27 15:58:58,921] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145500, global step 2321711: loss -56.3532
[2019-03-27 15:58:58,925] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145500, global step 2321711: learning rate 0.0000
[2019-03-27 15:58:58,968] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145000, global step 2321731: loss 0.0039
[2019-03-27 15:58:58,970] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145000, global step 2321732: learning rate 0.0000
[2019-03-27 15:58:59,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:58:59,562] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2697
[2019-03-27 15:58:59,571] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 76.0, 1.0, 2.0, 0.4164171264541464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606483.6185302931, 606483.6185302931, 174708.7663398163], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7488000.0000, 
sim time next is 7488600.0000, 
raw observation next is [26.26666666666667, 75.83333333333334, 1.0, 2.0, 0.4176749148650095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607589.897698016, 607589.897698016, 174791.688336351], 
processed observation next is [0.0, 0.6956521739130435, 0.44391785150079005, 0.7583333333333334, 1.0, 1.0, 0.29840351188555364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16877497158278223, 0.16877497158278223, 0.2608831169199269], 
reward next is 0.7391, 
noisyNet noise sample is [array([0.90749264], dtype=float32), 0.5868389]. 
=============================================
[2019-03-27 15:59:01,041] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145000, global step 2322512: loss 0.0066
[2019-03-27 15:59:01,044] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145000, global step 2322512: learning rate 0.0000
[2019-03-27 15:59:01,175] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145000, global step 2322562: loss 0.0036
[2019-03-27 15:59:01,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145000, global step 2322563: learning rate 0.0000
[2019-03-27 15:59:02,074] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145500, global step 2322902: loss -82.3341
[2019-03-27 15:59:02,076] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145500, global step 2322904: learning rate 0.0000
[2019-03-27 15:59:02,128] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145000, global step 2322920: loss 0.0037
[2019-03-27 15:59:02,130] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145000, global step 2322920: learning rate 0.0000
[2019-03-27 15:59:03,211] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145000, global step 2323321: loss 0.0076
[2019-03-27 15:59:03,214] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145000, global step 2323322: learning rate 0.0000
[2019-03-27 15:59:04,082] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 15:59:04,090] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2003
[2019-03-27 15:59:04,095] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 93.0, 1.0, 2.0, 0.4091832003651533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 602415.1954450014, 602415.1954450014, 174523.8407494047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7524000.0000, 
sim time next is 7524600.0000, 
raw observation next is [23.46666666666667, 92.83333333333333, 1.0, 2.0, 0.407581719654442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600886.2128957993, 600886.2128957993, 174406.3971178725], 
processed observation next is [0.0, 0.08695652173913043, 0.31121642969984215, 0.9283333333333332, 1.0, 1.0, 0.2862430357282434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1669128369154998, 0.1669128369154998, 0.26030805539980967], 
reward next is 0.7397, 
noisyNet noise sample is [array([1.7253788], dtype=float32), -1.1936401]. 
=============================================
[2019-03-27 15:59:06,135] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145500, global step 2324429: loss -52.2374
[2019-03-27 15:59:06,137] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145500, global step 2324429: learning rate 0.0000
[2019-03-27 15:59:06,279] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145000, global step 2324483: loss 0.0038
[2019-03-27 15:59:06,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145000, global step 2324483: learning rate 0.0000
[2019-03-27 15:59:06,521] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145000, global step 2324574: loss 0.0046
[2019-03-27 15:59:06,527] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145000, global step 2324576: learning rate 0.0000
[2019-03-27 15:59:07,183] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6539884e-14 1.0000000e+00 7.8386608e-20 1.2529598e-17 2.4489426e-08], sum to 1.0000
[2019-03-27 15:59:07,192] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0535
[2019-03-27 15:59:07,201] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2093264.909687986 W.
[2019-03-27 15:59:07,211] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.2, 80.33333333333334, 1.0, 2.0, 0.8558674228828411, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.988157954662475, 6.9112, 168.9124975863291, 2093264.909687986, 2038668.436297055, 422768.3216694564], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7720800.0000, 
sim time next is 7721400.0000, 
raw observation next is [28.4, 79.16666666666666, 1.0, 2.0, 0.4914770432458181, 1.0, 1.0, 0.4914770432458181, 1.0, 2.0, 0.8466132917824489, 6.911199999999999, 6.9112, 170.5573041426782, 2061591.247603085, 2061591.247603086, 408073.9662323376], 
processed observation next is [1.0, 0.34782608695652173, 0.5450236966824644, 0.7916666666666665, 1.0, 1.0, 0.38732173885038323, 1.0, 0.5, 0.38732173885038323, 1.0, 1.0, 0.812943038759084, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5726642354453013, 0.5726642354453017, 0.6090656212422949], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.66809493], dtype=float32), -2.29393]. 
=============================================
[2019-03-27 15:59:07,661] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 15:59:07,664] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 15:59:07,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:59:07,668] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 15:59:07,669] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 15:59:07,670] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 15:59:07,670] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:59:07,670] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:59:07,671] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:59:07,672] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 15:59:07,674] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 15:59:07,704] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run94
[2019-03-27 15:59:07,726] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run94
[2019-03-27 15:59:07,750] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run94
[2019-03-27 15:59:07,778] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run94
[2019-03-27 15:59:07,799] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run94
[2019-03-27 16:00:16,222] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07235287], dtype=float32), 0.101422414]
[2019-03-27 16:00:16,226] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.77888077333333, 62.61500948666666, 1.0, 2.0, 0.6671637966152699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 932360.2709107123, 932360.2709107117, 213925.5847169719]
[2019-03-27 16:00:16,228] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:00:16,230] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7225635e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6310959e-32], sampled 0.07608847867151858
[2019-03-27 16:00:35,003] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07235287], dtype=float32), 0.101422414]
[2019-03-27 16:00:35,004] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.75, 59.5, 1.0, 2.0, 0.7693176348524395, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005979612016122, 6.9112, 168.9123160194808, 1972133.136466453, 1904893.472602755, 400395.6936109425]
[2019-03-27 16:00:35,007] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:00:35,009] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.6454754e-20 1.0000000e+00 7.3469112e-26 6.8786314e-25 1.7931355e-14], sampled 0.6565905273735347
[2019-03-27 16:00:35,010] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1972133.136466453 W.
[2019-03-27 16:00:43,421] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07235287], dtype=float32), 0.101422414]
[2019-03-27 16:00:43,422] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.90536603166667, 78.31691871999999, 1.0, 2.0, 0.7868906027034556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1099764.916701303, 1099764.916701302, 240714.4683764301]
[2019-03-27 16:00:43,424] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:00:43,427] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.8879915e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6994006e-36], sampled 0.628867036114802
[2019-03-27 16:00:54,893] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07235287], dtype=float32), 0.101422414]
[2019-03-27 16:00:54,895] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.8, 86.5, 1.0, 2.0, 0.663437006082933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 927149.8141302812, 927149.8141302812, 213149.8031518069]
[2019-03-27 16:00:54,896] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:00:54,901] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1230743399166685
[2019-03-27 16:01:03,285] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07235287], dtype=float32), 0.101422414]
[2019-03-27 16:01:03,288] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.66666666666667, 72.66666666666667, 1.0, 2.0, 0.5220017480920546, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8693903983245891, 6.911199999999999, 6.9112, 168.9129312629076, 1459355.026378051, 1459355.026378052, 313003.838070776]
[2019-03-27 16:01:03,289] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:01:03,292] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0985822e-27 1.0000000e+00 1.6239071e-33 1.3772657e-35 6.0484383e-24], sampled 0.7810905433289932
[2019-03-27 16:01:08,200] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07235287], dtype=float32), 0.101422414]
[2019-03-27 16:01:08,200] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.918503195, 87.32477524000001, 1.0, 2.0, 0.2321029355672538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 385179.9839061525, 385179.9839061525, 158972.3275873422]
[2019-03-27 16:01:08,201] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 16:01:08,205] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.20939388780378565
[2019-03-27 16:01:15,788] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8020.5689 3006778287.8302 1707.0000
[2019-03-27 16:01:16,308] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6753 2779536092.7461 933.0000
[2019-03-27 16:01:16,345] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8501.7738 2841756525.7183 1139.0000
[2019-03-27 16:01:16,359] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7890.1086 3163738202.6932 1838.0000
[2019-03-27 16:01:16,693] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.8973 2927709641.5252 1361.0000
[2019-03-27 16:01:17,712] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2325000, evaluation results [2325000.0, 7890.108637879103, 3163738202.6932154, 1838.0, 8249.897318892235, 2927709641.525192, 1361.0, 8660.675308756297, 2779536092.746124, 933.0, 8020.5689393418625, 3006778287.830168, 1707.0, 8501.77379411456, 2841756525.7183347, 1139.0]
[2019-03-27 16:01:18,712] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145500, global step 2325382: loss 28.9830
[2019-03-27 16:01:18,714] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145500, global step 2325382: learning rate 0.0000
[2019-03-27 16:01:20,056] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:20,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:20,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run12
[2019-03-27 16:01:20,599] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145500, global step 2326111: loss -117.2954
[2019-03-27 16:01:20,602] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145500, global step 2326112: learning rate 0.0000
[2019-03-27 16:01:21,995] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145500, global step 2326706: loss -50.4132
[2019-03-27 16:01:21,999] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145500, global step 2326706: learning rate 0.0000
[2019-03-27 16:01:22,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:22,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:22,766] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run12
[2019-03-27 16:01:23,118] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145500, global step 2327134: loss 12.3511
[2019-03-27 16:01:23,120] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145500, global step 2327134: learning rate 0.0000
[2019-03-27 16:01:23,323] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145500, global step 2327227: loss -66.0833
[2019-03-27 16:01:23,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145500, global step 2327227: learning rate 0.0000
[2019-03-27 16:01:24,981] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145500, global step 2327895: loss 1.1649
[2019-03-27 16:01:24,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145500, global step 2327895: learning rate 0.0000
[2019-03-27 16:01:26,454] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:26,455] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:26,536] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run12
[2019-03-27 16:01:28,600] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:28,600] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:28,679] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run12
[2019-03-27 16:01:28,972] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145500, global step 2329476: loss 4.8832
[2019-03-27 16:01:28,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145500, global step 2329476: learning rate 0.0000
[2019-03-27 16:01:30,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:30,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:30,556] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run12
[2019-03-27 16:01:30,708] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145500, global step 2330204: loss 1.2755
[2019-03-27 16:01:30,709] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145500, global step 2330204: learning rate 0.0000
[2019-03-27 16:01:30,813] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145500, global step 2330251: loss 1.0401
[2019-03-27 16:01:30,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145500, global step 2330251: learning rate 0.0000
[2019-03-27 16:01:31,389] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145500, global step 2330508: loss -60.4701
[2019-03-27 16:01:31,392] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145500, global step 2330508: learning rate 0.0000
[2019-03-27 16:01:31,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:31,634] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:31,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-03-27 16:01:32,090] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145500, global step 2330815: loss 0.8266
[2019-03-27 16:01:32,095] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145500, global step 2330816: learning rate 0.0000
[2019-03-27 16:01:32,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:32,673] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:32,754] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run12
[2019-03-27 16:01:32,831] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:32,832] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:32,927] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run12
[2019-03-27 16:01:34,061] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:34,062] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:34,156] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run12
[2019-03-27 16:01:34,227] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145500, global step 2331806: loss 0.3872
[2019-03-27 16:01:34,228] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145500, global step 2331807: learning rate 0.0000
[2019-03-27 16:01:34,567] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145500, global step 2331958: loss -0.1215
[2019-03-27 16:01:34,570] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145500, global step 2331960: learning rate 0.0000
[2019-03-27 16:01:37,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:37,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:37,825] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run12
[2019-03-27 16:01:39,576] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:39,579] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:39,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:39,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:39,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run12
[2019-03-27 16:01:39,793] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run12
[2019-03-27 16:01:40,129] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:40,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:40,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run12
[2019-03-27 16:01:40,993] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:40,995] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:41,047] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run12
[2019-03-27 16:01:42,820] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.66943089e-25 1.00000000e+00 3.36284436e-31 1.45731959e-29
 1.32340444e-17], sum to 1.0000
[2019-03-27 16:01:42,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5170
[2019-03-27 16:01:42,835] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 96.0, 1.0, 2.0, 0.8019058968446152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1194224.090016205, 1194224.090016205, 254402.1387896959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 135000.0000, 
sim time next is 135600.0000, 
raw observation next is [22.8, 96.0, 1.0, 2.0, 0.8174763486541843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217378.522407393, 1217378.522407393, 258538.744758705], 
processed observation next is [1.0, 0.5652173913043478, 0.2796208530805688, 0.96, 1.0, 1.0, 0.7800919863303425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3381607006687203, 0.3381607006687203, 0.3858787235204552], 
reward next is 0.6141, 
noisyNet noise sample is [array([0.22726448], dtype=float32), 0.11058604]. 
=============================================
[2019-03-27 16:01:42,932] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:42,932] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:43,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run12
[2019-03-27 16:01:43,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 16:01:43,486] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:01:43,548] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run12
[2019-03-27 16:01:43,766] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.4884039e-21 1.0000000e+00 5.5252542e-27 8.5038369e-24 2.3437246e-12], sum to 1.0000
[2019-03-27 16:01:43,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9818
[2019-03-27 16:01:43,779] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 63.66666666666667, 1.0, 2.0, 0.6924300270297054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044727.012058795, 1044727.012058795, 229092.3450298621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 45600.0000, 
sim time next is 46200.0000, 
raw observation next is [27.3, 63.33333333333333, 1.0, 2.0, 0.6532585512819375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 984577.2517540853, 984577.2517540853, 220135.292964243], 
processed observation next is [1.0, 0.5217391304347826, 0.4928909952606636, 0.6333333333333333, 1.0, 1.0, 0.5822392184119729, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2734936810428015, 0.2734936810428015, 0.3285601387526015], 
reward next is 0.6714, 
noisyNet noise sample is [array([1.8389627], dtype=float32), 0.7427751]. 
=============================================
[2019-03-27 16:01:43,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:01:43,905] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4432
[2019-03-27 16:01:43,910] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.73333333333333, 87.0, 1.0, 2.0, 0.3089933277960151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 489982.389484485, 489982.3894844856, 166309.8875247809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 231600.0000, 
sim time next is 232200.0000, 
raw observation next is [21.7, 87.0, 1.0, 2.0, 0.3078086338347638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488466.0768189292, 488466.0768189292, 166205.6340837586], 
processed observation next is [0.0, 0.6956521739130435, 0.2274881516587678, 0.87, 1.0, 1.0, 0.16603449859610098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13568502133859145, 0.13568502133859145, 0.24806811057277403], 
reward next is 0.7519, 
noisyNet noise sample is [array([-0.8487901], dtype=float32), -0.114210434]. 
=============================================
[2019-03-27 16:01:54,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:01:54,278] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7272
[2019-03-27 16:01:54,286] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 93.0, 1.0, 2.0, 0.2955235204301194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472817.2235251571, 472817.2235251571, 165141.3872876969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 207000.0000, 
sim time next is 207600.0000, 
raw observation next is [20.63333333333333, 93.0, 1.0, 2.0, 0.2963497847265903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473810.7724524142, 473810.7724524142, 165207.7891244677], 
processed observation next is [0.0, 0.391304347826087, 0.17693522906793036, 0.93, 1.0, 1.0, 0.15222865629709673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13161410345900396, 0.13161410345900396, 0.2465787897380115], 
reward next is 0.7534, 
noisyNet noise sample is [array([-0.5776064], dtype=float32), 1.6318644]. 
=============================================
[2019-03-27 16:01:55,245] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.7947339e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5829421e-31], sum to 1.0000
[2019-03-27 16:01:55,264] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8221
[2019-03-27 16:01:55,272] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 54.0, 1.0, 2.0, 0.623737770370467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1020892.736967789, 1020892.736967788, 220761.780446653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 561600.0000, 
sim time next is 562200.0000, 
raw observation next is [24.9, 54.33333333333333, 1.0, 2.0, 0.5921154285601717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 968977.7556978606, 968977.75569786, 213881.6616905544], 
processed observation next is [1.0, 0.5217391304347826, 0.3791469194312796, 0.5433333333333333, 1.0, 1.0, 0.5085728054941828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26916048769385015, 0.26916048769385004, 0.31922636073217076], 
reward next is 0.6808, 
noisyNet noise sample is [array([0.49369788], dtype=float32), -0.20678498]. 
=============================================
[2019-03-27 16:01:56,241] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:01:56,253] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9749
[2019-03-27 16:01:56,262] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 87.33333333333334, 1.0, 2.0, 0.3049696997277485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485351.9326370611, 485351.9326370611, 166002.5276416708], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 235200.0000, 
sim time next is 235800.0000, 
raw observation next is [21.5, 87.5, 1.0, 2.0, 0.3039927782642947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483923.8271242945, 483923.8271242945, 165901.2731270735], 
processed observation next is [0.0, 0.7391304347826086, 0.21800947867298584, 0.875, 1.0, 1.0, 0.1614370822461382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13442328531230402, 0.13442328531230402, 0.24761384048816942], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.22231567], dtype=float32), -0.5603014]. 
=============================================
[2019-03-27 16:01:56,758] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:01:56,771] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7854
[2019-03-27 16:01:56,776] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.93333333333333, 86.0, 1.0, 2.0, 0.2746407143913493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 443667.5745969001, 443667.5745969001, 163180.4369165069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 337800.0000, 
sim time next is 338400.0000, 
raw observation next is [20.9, 86.0, 1.0, 2.0, 0.2723060848230452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440156.2608427943, 440156.2608427943, 162950.0363039614], 
processed observation next is [0.0, 0.9565217391304348, 0.1895734597156398, 0.86, 1.0, 1.0, 0.1232603431602954, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12226562801188731, 0.12226562801188731, 0.2432090094088976], 
reward next is 0.7568, 
noisyNet noise sample is [array([-1.814756], dtype=float32), -0.23207471]. 
=============================================
[2019-03-27 16:01:58,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 6.128663e-38], sum to 1.0000
[2019-03-27 16:01:58,043] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6284
[2019-03-27 16:01:58,057] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.31666666666667, 56.5, 1.0, 2.0, 0.4380545207939711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718684.7162382543, 718684.716238255, 185290.6793592843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 565800.0000, 
sim time next is 566400.0000, 
raw observation next is [24.23333333333333, 57.0, 1.0, 2.0, 0.5954843412428402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 976858.2892104761, 976858.2892104761, 214635.2878927916], 
processed observation next is [1.0, 0.5652173913043478, 0.3475513428120062, 0.57, 1.0, 1.0, 0.5126317364371569, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2713495247806878, 0.2713495247806878, 0.32035117595939044], 
reward next is 0.6796, 
noisyNet noise sample is [array([-0.03746321], dtype=float32), -1.4335322]. 
=============================================
[2019-03-27 16:01:58,252] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:01:58,268] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2432
[2019-03-27 16:01:58,274] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.26666666666667, 86.0, 1.0, 2.0, 0.2169482358025774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 361651.4476100616, 361651.4476100621, 157256.6847639962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 603600.0000, 
sim time next is 604200.0000, 
raw observation next is [18.18333333333334, 86.5, 1.0, 2.0, 0.2164753084279003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 360915.5059647441, 360915.5059647434, 157198.1531305439], 
processed observation next is [1.0, 1.0, 0.060821484992101514, 0.865, 1.0, 1.0, 0.05599434750349433, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10025430721242891, 0.10025430721242871, 0.23462410915006554], 
reward next is 0.7654, 
noisyNet noise sample is [array([-1.6879128], dtype=float32), 0.28752846]. 
=============================================
[2019-03-27 16:02:09,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:02:09,232] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3498
[2019-03-27 16:02:09,243] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.41666666666667, 69.0, 1.0, 2.0, 0.5024554194723655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 822644.3336216464, 822644.3336216458, 196248.3106412334], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 723000.0000, 
sim time next is 723600.0000, 
raw observation next is [22.6, 68.0, 1.0, 2.0, 0.5138985822273554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 841031.7897803773, 841031.7897803773, 198340.4809023116], 
processed observation next is [1.0, 0.391304347826087, 0.27014218009478685, 0.68, 1.0, 1.0, 0.41433564123777755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23361994160566035, 0.23361994160566035, 0.2960305685109128], 
reward next is 0.7040, 
noisyNet noise sample is [array([-1.824065], dtype=float32), 1.4369894]. 
=============================================
[2019-03-27 16:02:14,646] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:02:14,659] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8329
[2019-03-27 16:02:14,668] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 70.0, 1.0, 2.0, 0.3030077255568939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481149.0376832679, 481149.0376832685, 165681.0202890487], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 835200.0000, 
sim time next is 835800.0000, 
raw observation next is [24.06666666666667, 70.5, 1.0, 2.0, 0.3044113315652733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 482975.088490818, 482975.0884908187, 165805.0815643549], 
processed observation next is [0.0, 0.6956521739130435, 0.3396524486571882, 0.705, 1.0, 1.0, 0.16194136333165454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.134159746803005, 0.1341597468030052, 0.2474702709915745], 
reward next is 0.7525, 
noisyNet noise sample is [array([0.672484], dtype=float32), 0.103723645]. 
=============================================
[2019-03-27 16:02:15,436] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 3.533104e-37], sum to 1.0000
[2019-03-27 16:02:15,449] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9116
[2019-03-27 16:02:15,453] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333333, 49.16666666666667, 1.0, 2.0, 0.6247414232348923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027999.983668458, 1027999.983668458, 221030.3127641119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 745800.0000, 
sim time next is 746400.0000, 
raw observation next is [25.36666666666667, 49.33333333333334, 1.0, 2.0, 0.5581621855736757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 918740.3667638666, 918740.3667638672, 206917.3151846794], 
processed observation next is [1.0, 0.6521739130434783, 0.40126382306477115, 0.4933333333333334, 1.0, 1.0, 0.46766528382370565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2552056574344074, 0.25520565743440754, 0.30883181370847673], 
reward next is 0.6912, 
noisyNet noise sample is [array([1.4378276], dtype=float32), 0.545766]. 
=============================================
[2019-03-27 16:02:21,144] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 16:02:21,144] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 16:02:21,145] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:02:21,149] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 16:02:21,150] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:02:21,151] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 16:02:21,152] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:02:21,152] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 16:02:21,152] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:02:21,153] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 16:02:21,154] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:02:21,183] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run95
[2019-03-27 16:02:21,207] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run95
[2019-03-27 16:02:21,230] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run95
[2019-03-27 16:02:21,231] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run95
[2019-03-27 16:02:21,231] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run95
[2019-03-27 16:02:24,202] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07102222], dtype=float32), 0.1069395]
[2019-03-27 16:02:24,203] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.45, 40.0, 1.0, 2.0, 0.4308048168822926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715197.9878604923, 715197.987860493, 183868.8906255657]
[2019-03-27 16:02:24,205] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:02:24,210] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7881809966519503
[2019-03-27 16:02:34,119] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07102222], dtype=float32), 0.1069395]
[2019-03-27 16:02:34,122] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.4, 53.5, 1.0, 2.0, 0.3199653316179308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521473.4997377692, 521473.4997377686, 168572.948524397]
[2019-03-27 16:02:34,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:02:34,127] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6430537016891038
[2019-03-27 16:03:21,883] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07102222], dtype=float32), 0.1069395]
[2019-03-27 16:03:21,884] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.55611824, 70.10824799, 1.0, 2.0, 0.8322953269589729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1187204.959844361, 1187204.959844361, 255405.2948825446]
[2019-03-27 16:03:21,886] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 16:03:21,888] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3978723e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.5675395e-32], sampled 0.023587180760606286
[2019-03-27 16:03:27,660] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07102222], dtype=float32), 0.1069395]
[2019-03-27 16:03:27,661] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.18183730333334, 88.79892127, 1.0, 2.0, 0.6996425771151199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 977770.2057411695, 977770.2057411695, 220779.4785887797]
[2019-03-27 16:03:27,663] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:03:27,669] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.069320e-33 1.000000e+00 0.000000e+00 0.000000e+00 7.584541e-29], sampled 0.3865800611662219
[2019-03-27 16:03:35,526] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07102222], dtype=float32), 0.1069395]
[2019-03-27 16:03:35,528] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.251696215, 77.42585137500001, 1.0, 2.0, 0.5478407743471322, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765546.4544345596, 765546.4544345596, 191431.5577882491]
[2019-03-27 16:03:35,529] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:03:35,532] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6285556107396564
[2019-03-27 16:03:44,363] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07102222], dtype=float32), 0.1069395]
[2019-03-27 16:03:44,366] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.7, 68.0, 1.0, 2.0, 0.5915443086629262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 826641.0395784925, 826641.0395784919, 199180.3711695637]
[2019-03-27 16:03:44,366] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:03:44,369] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4598112e-38], sampled 0.2037964764355067
[2019-03-27 16:04:07,147] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07102222], dtype=float32), 0.1069395]
[2019-03-27 16:04:07,148] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.99746905833334, 72.72562915, 1.0, 2.0, 0.877856651830602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104267, 1226973.286875718, 1226973.286875717, 263915.6429484148]
[2019-03-27 16:04:07,149] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 16:04:07,151] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.8406351e-31 1.0000000e+00 4.5152554e-37 0.0000000e+00 6.1730669e-27], sampled 0.8270291936374808
[2019-03-27 16:04:30,001] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8025.0988 3006310884.8974 1689.0000
[2019-03-27 16:04:30,179] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.7195 2927905342.2138 1362.0000
[2019-03-27 16:04:30,346] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7907.4511 3163249772.3378 1821.0000
[2019-03-27 16:04:30,366] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.6468 2779495876.6026 929.0000
[2019-03-27 16:04:30,591] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8505.4194 2841604662.9519 1129.0000
[2019-03-27 16:04:31,612] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 2350000, evaluation results [2350000.0, 7907.451079840801, 3163249772.3378253, 1821.0, 8252.71952573406, 2927905342.2137904, 1362.0, 8663.646773078643, 2779495876.6026187, 929.0, 8025.0988106110435, 3006310884.8973794, 1689.0, 8505.419435351498, 2841604662.9518876, 1129.0]
[2019-03-27 16:04:32,213] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:04:32,223] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4978
[2019-03-27 16:04:32,229] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 69.5, 1.0, 2.0, 0.302115832327624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480020.9036320557, 480020.9036320557, 165605.2726722674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 834600.0000, 
sim time next is 835200.0000, 
raw observation next is [24.1, 70.0, 1.0, 2.0, 0.3030077255568939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481149.0376832679, 481149.0376832685, 165681.0202890487], 
processed observation next is [0.0, 0.6956521739130435, 0.3412322274881518, 0.7, 1.0, 1.0, 0.16025027175529388, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13365251046757443, 0.1336525104675746, 0.24728510490902791], 
reward next is 0.7527, 
noisyNet noise sample is [array([0.9328194], dtype=float32), 0.3148379]. 
=============================================
[2019-03-27 16:04:51,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 2.608847e-37], sum to 1.0000
[2019-03-27 16:04:51,412] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2634
[2019-03-27 16:04:51,419] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 87.66666666666667, 1.0, 2.0, 0.3536174830784391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546127.3581518098, 546127.3581518098, 170362.154002153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1208400.0000, 
sim time next is 1209000.0000, 
raw observation next is [22.65, 87.83333333333334, 1.0, 2.0, 0.3526292549418842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 545029.5503396417, 545029.5503396412, 170282.855050377], 
processed observation next is [1.0, 1.0, 0.2725118483412322, 0.8783333333333334, 1.0, 1.0, 0.22003524691793275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15139709731656714, 0.151397097316567, 0.25415351500056266], 
reward next is 0.7458, 
noisyNet noise sample is [array([-0.17194925], dtype=float32), -1.4508605]. 
=============================================
[2019-03-27 16:04:51,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.86184 ]
 [81.80264 ]
 [81.7271  ]
 [81.637726]
 [81.5294  ]], R is [[81.8392868 ]
 [81.76662445]
 [81.6951828 ]
 [81.62397766]
 [81.55327606]].
[2019-03-27 16:04:51,789] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:04:51,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5222
[2019-03-27 16:04:51,804] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.5, 1.0, 2.0, 0.3266065538592764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507045.1089431385, 507045.1089431385, 167315.7245968776], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 966600.0000, 
sim time next is 967200.0000, 
raw observation next is [21.9, 92.33333333333333, 1.0, 2.0, 0.3249702401700564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504792.0294788929, 504792.0294788922, 167151.6441352735], 
processed observation next is [1.0, 0.17391304347826086, 0.23696682464454974, 0.9233333333333333, 1.0, 1.0, 0.18671113273500767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14022000818858135, 0.14022000818858119, 0.24948006587354254], 
reward next is 0.7505, 
noisyNet noise sample is [array([-0.57090217], dtype=float32), -0.44313294]. 
=============================================
[2019-03-27 16:04:51,890] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:04:51,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7073
[2019-03-27 16:04:51,909] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 96.33333333333334, 1.0, 2.0, 0.3815997228179991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572996.761448401, 572996.761448401, 172195.7943425744], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1042800.0000, 
sim time next is 1043400.0000, 
raw observation next is [22.56666666666667, 96.16666666666666, 1.0, 2.0, 0.3827724543523893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 574503.0928234248, 574503.0928234248, 172321.7144258202], 
processed observation next is [1.0, 0.043478260869565216, 0.26856240126382325, 0.9616666666666666, 1.0, 1.0, 0.2563523546414329, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15958419245095135, 0.15958419245095135, 0.257196588695254], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.28875783], dtype=float32), -0.88450813]. 
=============================================
[2019-03-27 16:05:03,323] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:05:03,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2341
[2019-03-27 16:05:03,342] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.96666666666667, 95.0, 1.0, 2.0, 0.321592243169331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 507438.3278033506, 507438.32780335, 167564.5567187917], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1372800.0000, 
sim time next is 1373400.0000, 
raw observation next is [20.95, 95.0, 1.0, 2.0, 0.320668518181103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 506182.9164492217, 506182.9164492223, 167473.3356634478], 
processed observation next is [1.0, 0.9130434782608695, 0.19194312796208532, 0.95, 1.0, 1.0, 0.18152833515795538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14060636568033938, 0.14060636568033952, 0.2499602024827579], 
reward next is 0.7500, 
noisyNet noise sample is [array([1.9817084], dtype=float32), 1.4014195]. 
=============================================
[2019-03-27 16:05:26,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:05:26,201] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7197
[2019-03-27 16:05:26,208] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.56666666666667, 98.33333333333334, 1.0, 2.0, 0.4781503930941028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683549.6838074777, 683549.6838074777, 182215.226587118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1665600.0000, 
sim time next is 1666200.0000, 
raw observation next is [23.58333333333333, 98.16666666666667, 1.0, 2.0, 0.4773386055273855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682501.2686907761, 682501.2686907767, 182104.4983785113], 
processed observation next is [1.0, 0.2608695652173913, 0.31674565560821466, 0.9816666666666667, 1.0, 1.0, 0.3702874765390186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1895836857474378, 0.18958368574743797, 0.2717977587738975], 
reward next is 0.7282, 
noisyNet noise sample is [array([-1.6697332], dtype=float32), 1.1515846]. 
=============================================
[2019-03-27 16:05:33,235] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:05:33,248] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9364
[2019-03-27 16:05:33,263] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666666, 95.0, 1.0, 2.0, 0.4116493389435469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607956.8027249908, 607956.8027249908, 175098.0413635816], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1626000.0000, 
sim time next is 1626600.0000, 
raw observation next is [23.18333333333333, 95.0, 1.0, 2.0, 0.4111974519779803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606916.7836883408, 606916.7836883408, 174989.8254714457], 
processed observation next is [1.0, 0.8260869565217391, 0.29778830963665076, 0.95, 1.0, 1.0, 0.2905993397325064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16858799546898356, 0.16858799546898356, 0.2611788439872324], 
reward next is 0.7388, 
noisyNet noise sample is [array([-0.06939295], dtype=float32), 1.7000843]. 
=============================================
[2019-03-27 16:05:38,086] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 16:05:38,088] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 16:05:38,090] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 16:05:38,091] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:05:38,092] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 16:05:38,094] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 16:05:38,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:05:38,097] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:05:38,094] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 16:05:38,098] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:05:38,100] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:05:38,123] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run96
[2019-03-27 16:05:38,145] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run96
[2019-03-27 16:05:38,168] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run96
[2019-03-27 16:05:38,188] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run96
[2019-03-27 16:05:38,189] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run96
[2019-03-27 16:05:42,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:05:42,980] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.98950006666666, 90.50504444, 1.0, 2.0, 0.3887389311294139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 574750.5856229638, 574750.5856229638, 172058.5263836774]
[2019-03-27 16:05:42,981] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:05:42,983] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22900629003530948
[2019-03-27 16:06:01,189] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:06:01,190] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.36666666666667, 55.66666666666666, 1.0, 2.0, 0.2652398406573226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 435563.3056249613, 435563.3056249607, 162451.6363125475]
[2019-03-27 16:06:01,193] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:06:01,198] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9443983524945444
[2019-03-27 16:06:31,366] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:06:31,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.90000000000001, 63.0, 1.0, 2.0, 0.5906023275893998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 825324.1766915764, 825324.1766915771, 199007.9544383061]
[2019-03-27 16:06:31,368] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:06:31,371] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7871818023292806
[2019-03-27 16:06:41,984] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:06:41,985] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.06666666666667, 68.33333333333333, 1.0, 2.0, 0.5320635227363666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743491.7870016469, 743491.7870016469, 188771.9128973125]
[2019-03-27 16:06:41,985] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 16:06:41,988] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5690814216567335
[2019-03-27 16:06:44,503] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:06:44,503] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.1648447, 95.098507485, 1.0, 2.0, 0.8789871170146529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1228554.243251834, 1228554.243251834, 264220.4437611481]
[2019-03-27 16:06:44,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:06:44,507] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9691630463693858
[2019-03-27 16:06:56,644] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:06:56,646] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.08353572, 73.75304424, 1.0, 2.0, 0.4703390811318822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669605.3133845159, 669605.3133845152, 180669.4067968349]
[2019-03-27 16:06:56,646] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:06:56,648] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43386176991862957
[2019-03-27 16:06:58,240] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:06:58,241] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.03831229666667, 84.13209518666667, 1.0, 2.0, 0.5282554170622625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738168.5897853184, 738168.5897853178, 188140.418532859]
[2019-03-27 16:06:58,242] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:06:58,245] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.5788915e-38], sampled 0.1476609202847765
[2019-03-27 16:07:00,955] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:07:00,956] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.74158613, 72.28393889, 1.0, 2.0, 0.5142182461847009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718546.8255894241, 718546.8255894235, 185853.2115041066]
[2019-03-27 16:07:00,957] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:07:00,961] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04197835337061462
[2019-03-27 16:07:07,773] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:07:07,774] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.77692492, 80.68837785, 1.0, 2.0, 0.5554964624363085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776248.334735776, 776248.334735776, 192747.8888271998]
[2019-03-27 16:07:07,775] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:07:07,780] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7023617e-38], sampled 0.4134095939578316
[2019-03-27 16:07:13,263] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:07:13,264] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.8167537, 87.72318618, 1.0, 2.0, 0.8202532138568079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1146417.953164276, 1146417.953164275, 248942.0335334252]
[2019-03-27 16:07:13,265] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 16:07:13,272] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7708504358455593
[2019-03-27 16:07:14,015] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.0698006], dtype=float32), 0.10720077]
[2019-03-27 16:07:14,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.4, 95.0, 1.0, 2.0, 0.5963936877344737, 0.0, 2.0, 0.0, 1.0, 1.0, 1.021951906358145, 6.9112, 6.9112, 168.9124958772547, 1667495.194448194, 1667495.194448194, 362369.7373631869]
[2019-03-27 16:07:14,017] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 16:07:14,022] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.8193548e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8848721e-33], sampled 0.4676191496558846
[2019-03-27 16:07:14,024] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1667495.194448194 W.
[2019-03-27 16:07:46,604] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.8551 2779368836.8079 935.0000
[2019-03-27 16:07:46,922] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7880.9415 3163690371.7887 1799.0000
[2019-03-27 16:07:46,941] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.9817 2927335508.4460 1340.0000
[2019-03-27 16:07:47,136] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2261 3007711750.8227 1768.0000
[2019-03-27 16:07:47,203] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.0571 2842451216.6243 1148.0000
[2019-03-27 16:07:48,220] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2375000, evaluation results [2375000.0, 7880.941515057095, 3163690371.7886896, 1799.0, 8253.981728143634, 2927335508.446033, 1340.0, 8658.855084644587, 2779368836.8078885, 935.0, 7998.226076340774, 3007711750.822709, 1768.0, 8494.057121969383, 2842451216.624326, 1148.0]
[2019-03-27 16:07:49,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:07:49,503] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1795
[2019-03-27 16:07:49,509] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 58.33333333333333, 1.0, 2.0, 0.3491524393875517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537979.8451074867, 537979.845107486, 169654.2106731027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1528800.0000, 
sim time next is 1529400.0000, 
raw observation next is [27.25, 58.66666666666667, 1.0, 2.0, 0.346292124444137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534723.2371210987, 534723.2371210987, 169422.5073354292], 
processed observation next is [0.0, 0.6956521739130435, 0.490521327014218, 0.5866666666666667, 1.0, 1.0, 0.21240014993269515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1485342325336385, 0.1485342325336385, 0.2528694139334764], 
reward next is 0.7471, 
noisyNet noise sample is [array([-1.0698738], dtype=float32), 0.6670057]. 
=============================================
[2019-03-27 16:08:09,641] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4836590e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3972005e-34], sum to 1.0000
[2019-03-27 16:08:09,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6411
[2019-03-27 16:08:09,659] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 84.0, 1.0, 2.0, 0.940212314966993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1314181.243361939, 1314181.243361939, 281236.2605846092], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1863000.0000, 
sim time next is 1863600.0000, 
raw observation next is [26.9, 83.66666666666666, 1.0, 2.0, 1.0333137698404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1444402.279950556, 1444402.279950556, 309238.4309818231], 
processed observation next is [1.0, 0.5652173913043478, 0.4739336492890995, 0.8366666666666666, 1.0, 1.0, 1.0401370720968675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.40122285554182113, 0.40122285554182113, 0.4615498969877957], 
reward next is 0.5385, 
noisyNet noise sample is [array([0.30566862], dtype=float32), 1.9229372]. 
=============================================
[2019-03-27 16:08:23,809] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:08:23,823] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5937
[2019-03-27 16:08:23,832] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.88333333333333, 90.5, 1.0, 2.0, 0.4203797812308506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617110.0769192945, 617110.0769192945, 175860.9918136906], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1966200.0000, 
sim time next is 1966800.0000, 
raw observation next is [23.76666666666667, 91.0, 1.0, 2.0, 0.4171357845113204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 613512.9860393727, 613512.9860393734, 175550.7873338096], 
processed observation next is [1.0, 0.782608695652174, 0.32543443917851517, 0.91, 1.0, 1.0, 0.29775395724255466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17042027389982575, 0.17042027389982595, 0.26201610049822327], 
reward next is 0.7380, 
noisyNet noise sample is [array([-0.9990059], dtype=float32), 0.15064783]. 
=============================================
[2019-03-27 16:08:25,104] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3402012e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.8558512e-37], sum to 1.0000
[2019-03-27 16:08:25,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6211
[2019-03-27 16:08:25,123] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.63333333333333, 97.66666666666667, 1.0, 2.0, 0.4488983664727622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642619.5072123181, 642619.5072123181, 177953.8439553231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1984800.0000, 
sim time next is 1985400.0000, 
raw observation next is [23.7, 97.5, 1.0, 2.0, 0.4504408295010301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643562.3911438324, 643562.3911438324, 178017.7064078728], 
processed observation next is [1.0, 1.0, 0.3222748815165877, 0.975, 1.0, 1.0, 0.33788051747112063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17876733087328678, 0.17876733087328678, 0.26569806926548184], 
reward next is 0.7343, 
noisyNet noise sample is [array([0.08832955], dtype=float32), -0.43440372]. 
=============================================
[2019-03-27 16:08:26,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:08:26,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4611
[2019-03-27 16:08:26,305] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.03333333333334, 91.33333333333334, 1.0, 2.0, 0.4750164030414903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 663750.7142235433, 663750.7142235427, 179774.0483028352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2064000.0000, 
sim time next is 2064600.0000, 
raw observation next is [25.0, 91.5, 1.0, 2.0, 0.4756561449921946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664644.9191639184, 664644.9191639189, 179869.5470382831], 
processed observation next is [0.0, 0.9130434782608695, 0.38388625592417064, 0.915, 1.0, 1.0, 0.3682604156532466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.184623588656644, 0.18462358865664416, 0.26846201050490015], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.13998051], dtype=float32), 0.34573472]. 
=============================================
[2019-03-27 16:08:33,195] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4872524e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.2094064e-37], sum to 1.0000
[2019-03-27 16:08:33,206] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1901
[2019-03-27 16:08:33,211] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 83.0, 1.0, 2.0, 0.5092346835890735, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711580.6735652142, 711580.6735652142, 185058.4487246028], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2481600.0000, 
sim time next is 2482200.0000, 
raw observation next is [28.15, 83.5, 1.0, 2.0, 0.5076300319646234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709337.6599594321, 709337.6599594315, 184803.1042819293], 
processed observation next is [1.0, 0.7391304347826086, 0.533175355450237, 0.835, 1.0, 1.0, 0.4067831710417149, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19703823887762004, 0.19703823887761987, 0.27582552877899896], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.33025226], dtype=float32), 0.32820112]. 
=============================================
[2019-03-27 16:08:38,934] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3749153e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.2776996e-30], sum to 1.0000
[2019-03-27 16:08:38,942] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2368
[2019-03-27 16:08:38,947] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 84.0, 1.0, 2.0, 0.5383594888326082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752292.7223970952, 752292.7223970952, 189824.4975520402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2239200.0000, 
sim time next is 2239800.0000, 
raw observation next is [27.71666666666667, 84.16666666666667, 1.0, 2.0, 0.5362253578679649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749309.478857817, 749309.4788578164, 189466.5266259519], 
processed observation next is [1.0, 0.9565217391304348, 0.5126382306477094, 0.8416666666666667, 1.0, 1.0, 0.4412353709252589, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20814152190494917, 0.208141521904949, 0.2827858606357491], 
reward next is 0.7172, 
noisyNet noise sample is [array([-0.5147652], dtype=float32), 0.35577324]. 
=============================================
[2019-03-27 16:08:39,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3826140e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.7160657e-33], sum to 1.0000
[2019-03-27 16:08:39,832] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6228
[2019-03-27 16:08:39,841] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666666, 86.16666666666667, 1.0, 2.0, 0.7564965720769556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1057264.817172797, 1057264.817172797, 233509.6306334129], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2257800.0000, 
sim time next is 2258400.0000, 
raw observation next is [26.13333333333333, 86.33333333333334, 1.0, 2.0, 0.7262191595118556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014929.476273502, 1014929.476273502, 226603.468039346], 
processed observation next is [1.0, 0.13043478260869565, 0.43759873617693507, 0.8633333333333334, 1.0, 1.0, 0.6701435656769344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2819248545204172, 0.2819248545204172, 0.33821413140200896], 
reward next is 0.6618, 
noisyNet noise sample is [array([2.1294827], dtype=float32), 1.7905918]. 
=============================================
[2019-03-27 16:08:52,971] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8919024e-26 1.0000000e+00 4.3808878e-33 4.9455761e-34 2.9597780e-21], sum to 1.0000
[2019-03-27 16:08:52,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5884
[2019-03-27 16:08:52,988] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.48333333333333, 93.16666666666666, 1.0, 2.0, 0.7783651110863742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1087843.514641576, 1087843.514641576, 238669.8148637448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2533800.0000, 
sim time next is 2534400.0000, 
raw observation next is [26.5, 93.0, 1.0, 2.0, 0.7620022220373106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1064963.264275522, 1064963.264275522, 234797.8168785044], 
processed observation next is [1.0, 0.34782608695652173, 0.4549763033175356, 0.93, 1.0, 1.0, 0.713255689201579, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2958231289654228, 0.2958231289654228, 0.3504445028037379], 
reward next is 0.6496, 
noisyNet noise sample is [array([0.2665549], dtype=float32), 2.8058743]. 
=============================================
[2019-03-27 16:08:54,755] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 16:08:54,757] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 16:08:54,758] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 16:08:54,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:08:54,759] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 16:08:54,759] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:08:54,759] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:08:54,760] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 16:08:54,760] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 16:08:54,761] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:08:54,761] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:08:54,796] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run97
[2019-03-27 16:08:54,816] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run97
[2019-03-27 16:08:54,843] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run97
[2019-03-27 16:08:54,844] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run97
[2019-03-27 16:08:54,890] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run97
[2019-03-27 16:09:00,211] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07136156], dtype=float32), 0.10846228]
[2019-03-27 16:09:00,213] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.53875258, 81.18394745, 1.0, 2.0, 0.2591433270046233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 424907.8813396281, 424907.8813396275, 161823.95364591]
[2019-03-27 16:09:00,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:09:00,219] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.578388607966882
[2019-03-27 16:09:01,412] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07136156], dtype=float32), 0.10846228]
[2019-03-27 16:09:01,412] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.84026917333333, 84.38262542, 1.0, 2.0, 0.308560400940558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490449.606919374, 490449.606919374, 166363.6050112053]
[2019-03-27 16:09:01,416] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 16:09:01,419] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6445666613428457
[2019-03-27 16:09:22,891] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07136156], dtype=float32), 0.10846228]
[2019-03-27 16:09:22,893] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.93333333333333, 75.0, 1.0, 2.0, 0.7559068726203672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1076681.348343648, 1076681.348343648, 236154.0423038241]
[2019-03-27 16:09:22,895] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:09:22,899] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6046138e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6011493e-31], sampled 0.3399862558043958
[2019-03-27 16:09:23,789] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07136156], dtype=float32), 0.10846228]
[2019-03-27 16:09:23,790] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.95186024, 91.67843853666666, 1.0, 2.0, 0.409658647229667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600550.172618207, 600550.172618207, 174272.1078350175]
[2019-03-27 16:09:23,790] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 16:09:23,792] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.28192096500007047
[2019-03-27 16:10:02,464] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07136156], dtype=float32), 0.10846228]
[2019-03-27 16:10:02,466] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5785132676295686, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808424.1578612714, 808424.1578612714, 196811.2420558016]
[2019-03-27 16:10:02,468] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:10:02,471] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.0235587e-28 1.0000000e+00 2.8759828e-34 4.1832991e-35 2.2870758e-23], sampled 0.7537513700013387
[2019-03-27 16:11:03,424] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8610.2252 2837001517.4073 862.0000
[2019-03-27 16:11:03,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8334.5817 2926521270.6423 1170.0000
[2019-03-27 16:11:03,591] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8051.5728 3156511708.3635 1477.0000
[2019-03-27 16:11:03,925] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8238.7758 2996569188.1588 1162.0000
[2019-03-27 16:11:04,005] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8718.1933 2778374772.8609 811.0000
[2019-03-27 16:11:05,022] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2400000, evaluation results [2400000.0, 8051.572815275384, 3156511708.3635225, 1477.0, 8334.581652114937, 2926521270.642308, 1170.0, 8718.193323966148, 2778374772.860857, 811.0, 8238.775821976578, 2996569188.158819, 1162.0, 8610.225172672268, 2837001517.4072585, 862.0]
[2019-03-27 16:11:14,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:11:14,867] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9821
[2019-03-27 16:11:14,871] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4758230458241832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664878.2063941116, 664878.206394111, 179894.6935287791], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2700600.0000, 
sim time next is 2701200.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4759033026671583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664990.3861802926, 664990.3861802932, 179906.6897810711], 
processed observation next is [0.0, 0.2608695652173913, 0.3364928909952607, 1.0, 1.0, 1.0, 0.36855819598452805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18471955171674795, 0.18471955171674811, 0.26851744743443445], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.76417524], dtype=float32), -1.9486964]. 
=============================================
[2019-03-27 16:11:25,632] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:11:25,642] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1079
[2019-03-27 16:11:25,649] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.00000000000001, 1.0, 2.0, 0.4415283653261661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634208.0951602483, 634208.0951602483, 177162.4779117545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2686200.0000, 
sim time next is 2686800.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.442320514599603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635328.231742431, 635328.231742431, 177274.088662323], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 1.0, 1.0, 0.3280970055416903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17648006437289748, 0.17648006437289748, 0.2645881920333179], 
reward next is 0.7354, 
noisyNet noise sample is [array([-0.9822385], dtype=float32), 0.40674406]. 
=============================================
[2019-03-27 16:11:30,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:11:30,513] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8978
[2019-03-27 16:11:30,521] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 99.0, 1.0, 2.0, 0.3832877370347389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576040.0824591557, 576040.0824591557, 172482.5858854541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3120600.0000, 
sim time next is 3121200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3798861857858509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 571866.868337525, 571866.868337525, 172141.3025700911], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2528749226335553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15885190787153475, 0.15885190787153475, 0.2569273172687927], 
reward next is 0.7431, 
noisyNet noise sample is [array([-0.09917022], dtype=float32), -0.6939719]. 
=============================================
[2019-03-27 16:11:31,170] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.7312619e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3650197e-32], sum to 1.0000
[2019-03-27 16:11:31,176] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9535
[2019-03-27 16:11:31,183] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.7874252853470698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1174616.704686965, 1174616.704686964, 250870.7695471696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3065400.0000, 
sim time next is 3066000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.7852241844331339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1171331.216517576, 1171331.216517575, 250300.4449124924], 
processed observation next is [1.0, 0.4782608695652174, 0.28909952606635075, 0.94, 1.0, 1.0, 0.741233957148354, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32536978236599334, 0.32536978236599307, 0.37358275360073495], 
reward next is 0.6264, 
noisyNet noise sample is [array([-1.8603563], dtype=float32), -0.3922266]. 
=============================================
[2019-03-27 16:11:31,198] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[75.32577 ]
 [75.37797 ]
 [75.21623 ]
 [75.53649 ]
 [75.299614]], R is [[75.17876434]
 [75.05254364]
 [74.93125916]
 [74.79730225]
 [74.70552826]].
[2019-03-27 16:11:33,408] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:11:33,419] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7347
[2019-03-27 16:11:33,424] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 93.16666666666667, 1.0, 2.0, 0.354584679752377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544869.9315685949, 544869.9315685949, 170179.3055353848], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2850600.0000, 
sim time next is 2851200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.352172829773883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542171.901748241, 542171.9017482405, 169985.6101424381], 
processed observation next is [1.0, 0.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21948533707696743, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15060330604117805, 0.15060330604117791, 0.25370986588423594], 
reward next is 0.7463, 
noisyNet noise sample is [array([1.9102422], dtype=float32), 1.1138697]. 
=============================================
[2019-03-27 16:11:40,370] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:11:40,384] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1063
[2019-03-27 16:11:40,389] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.5764305647285458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868067.4489496684, 868067.4489496684, 204174.1835280488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.6057840286348392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912274.0170086146, 912274.0170086146, 210012.8193050214], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 1.0, 1.0, 1.0, 0.5250409983552279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2534094491690596, 0.2534094491690596, 0.31345196911197226], 
reward next is 0.6865, 
noisyNet noise sample is [array([-0.17440312], dtype=float32), 0.6024454]. 
=============================================
[2019-03-27 16:11:44,555] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:11:44,565] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2833
[2019-03-27 16:11:44,571] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4551870254107773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644494.1438965485, 644494.1438965485, 177965.0173941553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3214800.0000, 
sim time next is 3215400.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4557218816054515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645251.6411246454, 645251.6411246454, 178042.8757508932], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.34424323084994163, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17923656697906815, 0.17923656697906815, 0.26573563544909434], 
reward next is 0.7343, 
noisyNet noise sample is [array([-1.2253392], dtype=float32), -1.5562048]. 
=============================================
[2019-03-27 16:11:50,067] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.314408e-37 1.000000e+00 0.000000e+00 0.000000e+00 6.243779e-36], sum to 1.0000
[2019-03-27 16:11:50,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1995
[2019-03-27 16:11:50,082] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 77.66666666666667, 1.0, 2.0, 0.5705677971913229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 797316.8567040372, 797316.8567040366, 195390.7187670969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3264000.0000, 
sim time next is 3264600.0000, 
raw observation next is [29.33333333333333, 78.33333333333334, 1.0, 2.0, 0.5638418887388965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787914.5223730415, 787914.5223730408, 194202.8663751448], 
processed observation next is [0.0, 0.782608695652174, 0.5892575039494469, 0.7833333333333334, 1.0, 1.0, 0.47450829968541747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21886514510362265, 0.21886514510362245, 0.28985502444051464], 
reward next is 0.7101, 
noisyNet noise sample is [array([-0.06545743], dtype=float32), -0.3661016]. 
=============================================
[2019-03-27 16:11:55,894] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:11:55,906] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0718
[2019-03-27 16:11:55,915] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 79.0, 1.0, 2.0, 0.7490208551332743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1046811.75168703, 1046811.75168703, 231779.4291621183], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3478800.0000, 
sim time next is 3479400.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.7951577808035653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1111325.240513955, 1111325.240513955, 242723.3068325832], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.79, 1.0, 1.0, 0.7532021455464641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30870145569832086, 0.30870145569832086, 0.3622735922874376], 
reward next is 0.6377, 
noisyNet noise sample is [array([1.2458589], dtype=float32), 0.3032999]. 
=============================================
[2019-03-27 16:12:11,040] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9422372e-10 1.2912075e-05 1.4442104e-17 9.7719444e-10 9.9998713e-01], sum to 1.0000
[2019-03-27 16:12:11,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7009
[2019-03-27 16:12:11,055] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.33333333333334, 64.33333333333333, 1.0, 2.0, 0.5890562959382645, 1.0, 2.0, 0.5890562959382645, 1.0, 2.0, 1.02299598215092, 6.911200000000001, 6.9112, 170.5573041426782, 2471335.03635211, 2471335.03635211, 482191.3498173538], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3516000.0000, 
sim time next is 3516600.0000, 
raw observation next is [32.16666666666666, 65.66666666666667, 1.0, 2.0, 0.5861419786761273, 1.0, 2.0, 0.5861419786761273, 1.0, 2.0, 1.017934776846034, 6.9112, 6.9112, 170.5573041426782, 2459096.23784185, 2459096.23784185, 479824.7224714476], 
processed observation next is [1.0, 0.6956521739130435, 0.7235387045813582, 0.6566666666666667, 1.0, 1.0, 0.5013758779230448, 1.0, 1.0, 0.5013758779230448, 1.0, 1.0, 1.0218716790805293, 0.0, 0.0, 0.8375144448122397, 0.6830822882894029, 0.6830822882894029, 0.7161563021961904], 
reward next is 0.2838, 
noisyNet noise sample is [array([1.2913059], dtype=float32), -1.1117243]. 
=============================================
[2019-03-27 16:12:11,583] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 16:12:11,586] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 16:12:11,587] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 16:12:11,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:12:11,589] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 16:12:11,590] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 16:12:11,589] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:12:11,590] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 16:12:11,592] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:12:11,592] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:12:11,596] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:12:11,630] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run98
[2019-03-27 16:12:11,652] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run98
[2019-03-27 16:12:11,675] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run98
[2019-03-27 16:12:11,703] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run98
[2019-03-27 16:12:11,734] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run98
[2019-03-27 16:12:19,793] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07269179], dtype=float32), 0.1072064]
[2019-03-27 16:12:19,794] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.83333333333333, 90.16666666666666, 1.0, 2.0, 0.3253650468691521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510154.310519564, 510154.310519564, 167699.3337190155]
[2019-03-27 16:12:19,794] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:12:19,797] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6326407975925237
[2019-03-27 16:12:36,518] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07269179], dtype=float32), 0.1072064]
[2019-03-27 16:12:36,518] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.11197119666667, 84.20535322166667, 1.0, 2.0, 0.4515211591638297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 649811.3599252015, 649811.3599252021, 178769.9360900029]
[2019-03-27 16:12:36,519] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 16:12:36,521] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4821463819274727
[2019-03-27 16:13:24,041] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07269179], dtype=float32), 0.1072064]
[2019-03-27 16:13:24,042] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.8, 72.66666666666667, 1.0, 2.0, 0.5515164372901733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770684.6472874513, 770684.6472874513, 192060.6628597149]
[2019-03-27 16:13:24,045] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:13:24,047] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.39117226077914424
[2019-03-27 16:13:47,910] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07269179], dtype=float32), 0.1072064]
[2019-03-27 16:13:47,912] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.2, 78.0, 1.0, 2.0, 0.9514507015617052, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005990760869125, 6.9112, 168.9123159639662, 2227050.326922357, 2159802.753730718, 448717.6946133251]
[2019-03-27 16:13:47,914] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:13:47,920] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.0627436e-18 1.0000000e+00 4.5550588e-24 4.5458261e-23 1.7810497e-13], sampled 0.27606223795240603
[2019-03-27 16:13:47,922] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2227050.326922357 W.
[2019-03-27 16:14:01,868] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07269179], dtype=float32), 0.1072064]
[2019-03-27 16:14:01,868] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.50703916, 76.12672724000001, 1.0, 2.0, 0.3112561729952384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490433.9809394385, 490433.9809394385, 166275.3330413124]
[2019-03-27 16:14:01,868] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:14:01,870] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24418864440820898
[2019-03-27 16:14:19,615] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.6845 2842130161.8094 1152.0000
[2019-03-27 16:14:20,547] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7880.0599 3164037773.3536 1882.0000
[2019-03-27 16:14:20,618] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.6599 2779088780.2030 926.0000
[2019-03-27 16:14:20,658] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8011.6712 3007067500.3241 1731.0000
[2019-03-27 16:14:20,734] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.1471 2927422555.0666 1344.0000
[2019-03-27 16:14:21,756] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 2425000, evaluation results [2425000.0, 7880.059895802928, 3164037773.3535657, 1882.0, 8253.147085104945, 2927422555.0665936, 1344.0, 8662.659906591674, 2779088780.2030396, 926.0, 8011.671183341091, 3007067500.3240843, 1731.0, 8498.684534625574, 2842130161.809373, 1152.0]
[2019-03-27 16:14:30,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8436912e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2598414e-34], sum to 1.0000
[2019-03-27 16:14:30,192] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4834
[2019-03-27 16:14:30,195] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6887043417611763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 962476.78678245, 962476.78678245, 218422.5750313775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3643200.0000, 
sim time next is 3643800.0000, 
raw observation next is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6748815702130709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 943150.6355299543, 943150.6355299538, 215513.4039092174], 
processed observation next is [1.0, 0.17391304347826086, 0.470774091627172, 0.7983333333333335, 1.0, 1.0, 0.608291048449483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26198628764720955, 0.2619862876472094, 0.3216617968794289], 
reward next is 0.6783, 
noisyNet noise sample is [array([-1.5456661], dtype=float32), -1.4284964]. 
=============================================
[2019-03-27 16:14:30,451] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.5185677e-38], sum to 1.0000
[2019-03-27 16:14:30,455] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6451
[2019-03-27 16:14:30,463] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.550857715978353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769763.821872252, 769763.821872252, 191948.1791811568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3529200.0000, 
sim time next is 3529800.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5506210968943039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769433.0526470332, 769433.0526470332, 191907.5643870359], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.79, 1.0, 1.0, 0.45857963481241437, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2137314035130648, 0.2137314035130648, 0.2864292005776655], 
reward next is 0.7136, 
noisyNet noise sample is [array([1.1578407], dtype=float32), -1.816173]. 
=============================================
[2019-03-27 16:14:43,707] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:14:43,718] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1384
[2019-03-27 16:14:43,726] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 55.66666666666667, 1.0, 2.0, 0.5987374763543966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836696.9383127566, 836696.9383127566, 200511.287819026], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3856800.0000, 
sim time next is 3857400.0000, 
raw observation next is [35.0, 55.5, 1.0, 2.0, 0.5924232812887424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827869.8199108832, 827869.8199108832, 199343.5388423999], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.555, 1.0, 1.0, 0.5089437123960752, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2299638388641342, 0.2299638388641342, 0.2975276699140297], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.56301886], dtype=float32), -1.0392861]. 
=============================================
[2019-03-27 16:14:43,848] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.9962998e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7486794e-36], sum to 1.0000
[2019-03-27 16:14:43,859] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8802
[2019-03-27 16:14:43,871] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4884502095470469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 682528.0935166902, 682528.0935166902, 181805.8168284014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3715200.0000, 
sim time next is 3715800.0000, 
raw observation next is [27.0, 78.16666666666667, 1.0, 2.0, 0.4854504547419786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678335.0957684433, 678335.0957684427, 181347.2992891828], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.7816666666666667, 1.0, 1.0, 0.3800607888457574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18842641549123426, 0.1884264154912341, 0.2706676108793773], 
reward next is 0.7293, 
noisyNet noise sample is [array([-0.59196126], dtype=float32), -0.95158756]. 
=============================================
[2019-03-27 16:14:44,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:14:44,837] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7986
[2019-03-27 16:14:44,842] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 59.33333333333333, 1.0, 2.0, 0.6260128455368718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874828.201770172, 874828.201770172, 205692.8945068055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3852600.0000, 
sim time next is 3853200.0000, 
raw observation next is [35.0, 58.66666666666667, 1.0, 2.0, 0.6194142991590283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865603.2364676001, 865603.2364676001, 204419.2682650468], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5866666666666667, 1.0, 1.0, 0.5414630110349739, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24044534346322224, 0.24044534346322224, 0.3051033854702191], 
reward next is 0.6949, 
noisyNet noise sample is [array([1.651245], dtype=float32), -1.2783388]. 
=============================================
[2019-03-27 16:14:47,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:14:47,313] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7636
[2019-03-27 16:14:47,320] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 78.16666666666667, 1.0, 2.0, 0.5485338291444588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766515.270746748, 766515.2707467487, 191549.6587717301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3697800.0000, 
sim time next is 3698400.0000, 
raw observation next is [29.0, 77.33333333333334, 1.0, 2.0, 0.5458690742630228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762790.2339132758, 762790.2339132765, 191094.7013062788], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7733333333333334, 1.0, 1.0, 0.4528543063409913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21188617608702104, 0.21188617608702123, 0.2852159720989236], 
reward next is 0.7148, 
noisyNet noise sample is [array([-0.40467277], dtype=float32), 0.38131517]. 
=============================================
[2019-03-27 16:14:53,559] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:14:53,573] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1532
[2019-03-27 16:14:53,579] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 66.16666666666666, 1.0, 2.0, 0.5418233254587831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757134.7430933043, 757134.7430933043, 190408.9722613196], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3783000.0000, 
sim time next is 3783600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5365075531008595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749703.9514058627, 749703.9514058627, 189514.2663557047], 
processed observation next is [1.0, 0.8260869565217391, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44157536518175844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20825109761273963, 0.20825109761273963, 0.28285711396373836], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.00686199], dtype=float32), 0.38945308]. 
=============================================
[2019-03-27 16:15:14,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1307749e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6159146e-36], sum to 1.0000
[2019-03-27 16:15:14,206] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6316
[2019-03-27 16:15:14,212] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.598794308316437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836776.3885920225, 836776.3885920225, 200521.6796239286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4236000.0000, 
sim time next is 4236600.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5983498642354573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 836155.0622102371, 836155.0622102377, 200439.0969074108], 
processed observation next is [1.0, 0.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5160841737776594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2322652950583992, 0.23226529505839938, 0.29916283120509074], 
reward next is 0.7008, 
noisyNet noise sample is [array([0.6327454], dtype=float32), -0.38909552]. 
=============================================
[2019-03-27 16:15:20,049] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.2835196e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6850192e-37], sum to 1.0000
[2019-03-27 16:15:20,064] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3498
[2019-03-27 16:15:20,073] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6162785628958777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861219.4114177788, 861219.4114177788, 203818.1714228774], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4407600.0000, 
sim time next is 4408200.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6170304800647188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 862270.6057377337, 862270.605737733, 203961.9125217134], 
processed observation next is [0.0, 0.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5385909398370107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23951961270492603, 0.23951961270492583, 0.3044207649577812], 
reward next is 0.6956, 
noisyNet noise sample is [array([2.456547], dtype=float32), -2.1462398]. 
=============================================
[2019-03-27 16:15:24,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:15:24,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6496
[2019-03-27 16:15:24,456] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 71.0, 1.0, 2.0, 0.6423674400303991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 897692.7597030667, 897692.7597030667, 208905.309976897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4449600.0000, 
sim time next is 4450200.0000, 
raw observation next is [33.0, 70.33333333333334, 1.0, 2.0, 0.6710449508241483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 937786.5720021, 937786.5720021005, 214727.2930612], 
processed observation next is [0.0, 0.5217391304347826, 0.7630331753554502, 0.7033333333333335, 1.0, 1.0, 0.603668615450781, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26049627000058334, 0.26049627000058345, 0.32048849710626864], 
reward next is 0.6795, 
noisyNet noise sample is [array([2.1084511], dtype=float32), 0.8772861]. 
=============================================
[2019-03-27 16:15:28,311] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 16:15:28,315] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 16:15:28,316] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 16:15:28,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:15:28,319] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:15:28,317] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 16:15:28,320] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 16:15:28,321] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 16:15:28,326] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:15:28,327] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:15:28,327] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:15:28,358] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run99
[2019-03-27 16:15:28,380] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run99
[2019-03-27 16:15:28,404] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run99
[2019-03-27 16:15:28,425] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run99
[2019-03-27 16:15:28,426] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run99
[2019-03-27 16:15:40,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07385825], dtype=float32), 0.10721163]
[2019-03-27 16:15:40,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 56.5, 1.0, 2.0, 0.5711455446654399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928565.8693541674, 928565.8693541674, 209314.956994964]
[2019-03-27 16:15:40,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:15:40,832] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.004383552805474422
[2019-03-27 16:16:31,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07385825], dtype=float32), 0.10721163]
[2019-03-27 16:16:31,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.88758946, 67.76513077000001, 1.0, 2.0, 0.6677168107071385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 933133.4466813625, 933133.4466813625, 214042.6010357476]
[2019-03-27 16:16:31,037] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 16:16:31,039] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5519064017048613
[2019-03-27 16:17:05,986] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07385825], dtype=float32), 0.10721163]
[2019-03-27 16:17:05,987] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.8, 94.66666666666666, 1.0, 2.0, 0.7736997397295282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1081319.869184322, 1081319.869184322, 237559.1751878699]
[2019-03-27 16:17:05,988] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 16:17:05,990] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32947442971333774
[2019-03-27 16:17:08,223] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07385825], dtype=float32), 0.10721163]
[2019-03-27 16:17:08,226] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.13333333333333, 77.66666666666667, 1.0, 2.0, 0.771046925068774, 1.0, 2.0, 0.771046925068774, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2156294.395227167, 2156294.395227166, 406021.9418862485]
[2019-03-27 16:17:08,227] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:17:08,229] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7977070e-24 1.0000000e+00 1.5924821e-30 1.4253852e-31 6.3568280e-22], sampled 0.1438466128878242
[2019-03-27 16:17:08,231] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2156294.395227167 W.
[2019-03-27 16:17:23,611] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07385825], dtype=float32), 0.10721163]
[2019-03-27 16:17:23,614] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 81.0, 1.0, 2.0, 0.6244935311834086, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.97252416512244, 6.9112, 168.9125711222648, 1746125.974331915, 1702620.600240332, 369192.0642224727]
[2019-03-27 16:17:23,615] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:17:23,617] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1556171e-22 1.0000000e+00 3.0292615e-29 1.7474540e-28 5.1717607e-19], sampled 0.4478450038387972
[2019-03-27 16:17:23,618] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1746125.974331915 W.
[2019-03-27 16:17:37,042] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8492.9167 2842363576.1196 1154.0000
[2019-03-27 16:17:37,334] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9128 2779283755.0072 935.0000
[2019-03-27 16:17:37,380] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.4557 2927360683.5277 1342.0000
[2019-03-27 16:17:37,515] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.3879 3007586005.2045 1766.0000
[2019-03-27 16:17:37,651] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7880.1564 3164167717.7463 1850.0000
[2019-03-27 16:17:38,667] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2450000, evaluation results [2450000.0, 7880.15638698276, 3164167717.7462597, 1850.0, 8253.455668669689, 2927360683.527691, 1342.0, 8659.912796178214, 2779283755.007173, 935.0, 8001.387918332129, 3007586005.204479, 1766.0, 8492.916697518733, 2842363576.119565, 1154.0]
[2019-03-27 16:17:45,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:17:45,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5415
[2019-03-27 16:17:45,054] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5817448407078488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812941.7405118359, 812941.7405118353, 197394.2623503959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5820091056976335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813311.1710184453, 813311.1710184453, 197442.0592235731], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49639651288871506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2259197697273459, 0.2259197697273459, 0.29468964063219866], 
reward next is 0.7053, 
noisyNet noise sample is [array([1.642482], dtype=float32), -0.52899253]. 
=============================================
[2019-03-27 16:17:45,070] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.341064]
 [75.33055 ]
 [75.307396]
 [75.24895 ]
 [75.17038 ]], R is [[75.49837494]
 [75.44877625]
 [75.39979553]
 [75.35149384]
 [75.30383301]].
[2019-03-27 16:17:45,647] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.6073804e-10 3.5704227e-07 2.9871860e-18 1.8044719e-09 9.9999964e-01], sum to 1.0000
[2019-03-27 16:17:45,654] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0313
[2019-03-27 16:17:45,658] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.5, 61.5, 1.0, 2.0, 0.7787916211989351, 1.0, 2.0, 0.70998585011373, 1.0, 2.0, 1.03, 7.005103945624667, 6.9112, 170.5573041426782, 2979289.541753986, 2912022.32573475, 547370.853101408], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4627800.0000, 
sim time next is 4628400.0000, 
raw observation next is [34.66666666666666, 61.0, 1.0, 2.0, 0.8060363581693503, 1.0, 2.0, 0.7236082185989379, 1.0, 2.0, 1.03, 7.005106094514222, 6.9112, 170.5573041426782, 3036522.110501505, 2969253.35514527, 556912.9452809162], 
processed observation next is [1.0, 0.5652173913043478, 0.842022116903633, 0.61, 1.0, 1.0, 0.7663088652642774, 1.0, 1.0, 0.66699785373366, 1.0, 1.0, 1.0365853658536586, 0.009390609451422182, 0.0, 0.8375144448122397, 0.8434783640281959, 0.8247925986514639, 0.8312133511655465], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.40399903], dtype=float32), -1.2791327]. 
=============================================
[2019-03-27 16:17:46,843] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:17:46,851] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7051
[2019-03-27 16:17:46,856] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 81.0, 1.0, 2.0, 0.8259522448463741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1154387.471438376, 1154387.471438376, 250382.1220618755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4778400.0000, 
sim time next is 4779000.0000, 
raw observation next is [29.0, 79.5, 1.0, 2.0, 0.8609422907561163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1203318.821810219, 1203318.821810219, 259421.2778980691], 
processed observation next is [1.0, 0.30434782608695654, 0.5734597156398105, 0.795, 1.0, 1.0, 0.8324605912724292, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3342552282806164, 0.3342552282806164, 0.38719593716129713], 
reward next is 0.6128, 
noisyNet noise sample is [array([0.08936866], dtype=float32), 0.9595474]. 
=============================================
[2019-03-27 16:17:46,869] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[55.42942 ]
 [55.831123]
 [56.370438]
 [55.886417]
 [55.831406]], R is [[55.19239426]
 [55.26676941]
 [55.36094284]
 [55.50312424]
 [55.6453476 ]].
[2019-03-27 16:17:51,870] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:17:51,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4117
[2019-03-27 16:17:51,888] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5126314567395316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716328.7655173802, 716328.7655173802, 185598.6667908782], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4519200.0000, 
sim time next is 4519800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5130121937667768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716860.9702976157, 716860.9702976157, 185659.7436488764], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41326770333346596, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19912804730489325, 0.19912804730489325, 0.277104094998323], 
reward next is 0.7229, 
noisyNet noise sample is [array([0.76786083], dtype=float32), 0.5138915]. 
=============================================
[2019-03-27 16:17:54,635] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 7.433866e-38], sum to 1.0000
[2019-03-27 16:17:54,645] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5707
[2019-03-27 16:17:54,651] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5233320700164087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731286.4812083687, 731286.4812083694, 187332.1264624194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4565400.0000, 
sim time next is 4566000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5224396684751482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730039.0410538552, 730039.0410538552, 187186.3100326881], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4246261065965641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20278862251495977, 0.20278862251495977, 0.2793825522875942], 
reward next is 0.7206, 
noisyNet noise sample is [array([-1.0973921], dtype=float32), -0.6243627]. 
=============================================
[2019-03-27 16:17:54,679] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.686295]
 [77.65796 ]
 [77.450775]
 [77.44435 ]
 [77.43853 ]], R is [[77.61700439]
 [77.56123352]
 [77.50579071]
 [77.45023346]
 [77.39466858]].
[2019-03-27 16:17:58,575] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7266636e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.3750994e-34], sum to 1.0000
[2019-03-27 16:17:58,587] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0123
[2019-03-27 16:17:58,591] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.33333333333334, 55.0, 1.0, 2.0, 0.5363547090061656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749490.2950517103, 749490.295051711, 189488.7147888515], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4551600.0000, 
sim time next is 4552200.0000, 
raw observation next is [33.0, 56.0, 1.0, 2.0, 0.5342712176942296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746577.8468902777, 746577.8468902771, 189140.3617933015], 
processed observation next is [0.0, 0.6956521739130435, 0.7630331753554502, 0.56, 1.0, 1.0, 0.43888098517377055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20738273524729936, 0.2073827352472992, 0.2822990474526888], 
reward next is 0.7177, 
noisyNet noise sample is [array([-0.81713784], dtype=float32), 1.0806302]. 
=============================================
[2019-03-27 16:17:59,075] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9007355e-13 8.1425908e-12 3.0535551e-22 1.6918132e-11 1.0000000e+00], sum to 1.0000
[2019-03-27 16:17:59,079] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9256
[2019-03-27 16:17:59,085] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.16666666666667, 62.5, 1.0, 2.0, 0.6054194104262095, 1.0, 2.0, 0.6054194104262095, 1.0, 2.0, 1.03, 6.935272011130491, 6.9112, 170.5573041426782, 2540054.853383373, 2522811.092623794, 489953.192919658], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4626600.0000, 
sim time next is 4627200.0000, 
raw observation next is [34.33333333333334, 62.00000000000001, 1.0, 2.0, 0.7304955680076806, 1.0, 2.0, 0.685837823518103, 1.0, 2.0, 1.03, 7.005100136826354, 6.9112, 170.5573041426782, 2877841.265156115, 2810576.777534043, 531135.0377341561], 
processed observation next is [1.0, 0.5652173913043478, 0.8262243285939973, 0.6200000000000001, 1.0, 1.0, 0.6752958650694947, 1.0, 1.0, 0.6214913536362686, 1.0, 1.0, 1.0365853658536586, 0.009390013682635433, 0.0, 0.8375144448122397, 0.7994003514322542, 0.7807157715372341, 0.7927388622897853], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6717848], dtype=float32), 0.22300656]. 
=============================================
[2019-03-27 16:18:01,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9729502e-28 1.0000000e+00 1.2728979e-35 9.7680969e-37 1.9496082e-24], sum to 1.0000
[2019-03-27 16:18:01,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7776
[2019-03-27 16:18:01,309] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.41666666666667, 94.0, 1.0, 2.0, 0.4683299941844786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661572.8714201549, 661572.8714201543, 179706.5787568477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4657800.0000, 
sim time next is 4658400.0000, 
raw observation next is [24.5, 94.0, 1.0, 2.0, 0.4724233350370068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664883.6020439009, 664883.6020439015, 180001.7606253187], 
processed observation next is [1.0, 0.9565217391304348, 0.3601895734597157, 0.94, 1.0, 1.0, 0.36436546390000824, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18468988945663914, 0.1846898894566393, 0.2686593442168936], 
reward next is 0.7313, 
noisyNet noise sample is [array([-1.0486988], dtype=float32), -0.044238128]. 
=============================================
[2019-03-27 16:18:08,811] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.6517721e-10 3.3087117e-04 4.3206985e-17 1.6321139e-09 9.9966919e-01], sum to 1.0000
[2019-03-27 16:18:08,823] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3384
[2019-03-27 16:18:08,827] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333334, 71.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.410357623260555, 6.9112, 168.8985381075596, 3227705.609209482, 1454868.519119384, 308148.565598272], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4954800.0000, 
sim time next is 4955400.0000, 
raw observation next is [29.5, 70.0, 1.0, 2.0, 0.5593861427828583, 1.0, 1.0, 0.5593861427828583, 1.0, 1.0, 0.9587939658121474, 6.911199999999999, 6.9112, 170.5573041426782, 2346739.575889118, 2346739.575889118, 456032.8516546953], 
processed observation next is [1.0, 0.34782608695652173, 0.5971563981042655, 0.7, 1.0, 1.0, 0.4691399310636846, 1.0, 0.5, 0.4691399310636846, 1.0, 0.5, 0.9497487387953016, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6518721044136438, 0.6518721044136438, 0.6806460472458139], 
reward next is 0.3194, 
noisyNet noise sample is [array([0.6852703], dtype=float32), -0.7869292]. 
=============================================
[2019-03-27 16:18:14,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.6601100e-16 1.0000000e+00 2.0293852e-23 5.7161530e-18 4.7740816e-08], sum to 1.0000
[2019-03-27 16:18:14,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3173
[2019-03-27 16:18:14,530] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.4864589666265494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679744.7718240013, 679744.7718240013, 181501.3452180053], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4839000.0000, 
sim time next is 4839600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4853966873318575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678259.9409003592, 678259.9409003598, 181339.3974380624], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3799960088335632, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18840553913898866, 0.18840553913898883, 0.27065581707173497], 
reward next is 0.7293, 
noisyNet noise sample is [array([0.00208122], dtype=float32), 0.21989131]. 
=============================================
[2019-03-27 16:18:16,397] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.0220695e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4363089e-30], sum to 1.0000
[2019-03-27 16:18:16,406] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1241
[2019-03-27 16:18:16,415] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 61.66666666666667, 1.0, 2.0, 0.5224315252581876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730027.658095248, 730027.6580952486, 187185.5399929009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5062800.0000, 
sim time next is 5063400.0000, 
raw observation next is [31.5, 61.0, 1.0, 2.0, 0.520904719009403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727893.4211147603, 727893.4211147596, 186936.6864803091], 
processed observation next is [0.0, 0.6086956521739131, 0.6919431279620853, 0.61, 1.0, 1.0, 0.422776769890847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20219261697632232, 0.20219261697632213, 0.27900997982135683], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.9463131], dtype=float32), -1.4599974]. 
=============================================
[2019-03-27 16:18:19,789] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.4305617e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.8524737e-34], sum to 1.0000
[2019-03-27 16:18:19,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7678
[2019-03-27 16:18:19,809] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4769108836161082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666526.0922993605, 666526.0922993605, 180073.6285332501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5016600.0000, 
sim time next is 5017200.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4764020168090374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665815.2861503358, 665815.2861503358, 179997.5090513399], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.36915905639643054, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1849486905973155, 0.1849486905973155, 0.2686529985840894], 
reward next is 0.7313, 
noisyNet noise sample is [array([0.29522765], dtype=float32), -1.487645]. 
=============================================
[2019-03-27 16:18:20,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2677336e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.9810330e-31], sum to 1.0000
[2019-03-27 16:18:20,598] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0963
[2019-03-27 16:18:20,606] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5224838780806448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730100.8392509491, 730100.8392509497, 187193.9312395524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5061600.0000, 
sim time next is 5062200.0000, 
raw observation next is [31.16666666666667, 62.33333333333333, 1.0, 2.0, 0.5346691542317358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747134.109469819, 747134.109469819, 189205.5921577475], 
processed observation next is [0.0, 0.6086956521739131, 0.6761453396524489, 0.6233333333333333, 1.0, 1.0, 0.4393604267852238, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20753725263050526, 0.20753725263050526, 0.2823964062055933], 
reward next is 0.7176, 
noisyNet noise sample is [array([0.47316042], dtype=float32), -0.34905255]. 
=============================================
[2019-03-27 16:18:20,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8399279e-18 1.0000000e+00 1.6761207e-25 1.1517757e-21 4.9832822e-11], sum to 1.0000
[2019-03-27 16:18:20,760] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9703
[2019-03-27 16:18:20,763] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5089364113203795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711163.7423623501, 711163.7423623508, 185008.1472031107], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4926600.0000, 
sim time next is 4927200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5086982487952844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710830.8339312291, 710830.8339312284, 184970.2242233995], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4080701792714269, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1974530094253414, 0.19745300942534122, 0.27607496152746197], 
reward next is 0.7239, 
noisyNet noise sample is [array([-0.26725188], dtype=float32), -1.781056]. 
=============================================
[2019-03-27 16:18:21,597] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1902366e-23 1.0000000e+00 1.9773818e-31 4.4320109e-30 1.3763952e-18], sum to 1.0000
[2019-03-27 16:18:21,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2180
[2019-03-27 16:18:21,617] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.6938076321160952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 969611.9831980718, 969611.9831980718, 219512.3938610719], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4948800.0000, 
sim time next is 4949400.0000, 
raw observation next is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.6910034200028303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965691.251060816, 965691.251060816, 218913.5188449852], 
processed observation next is [1.0, 0.2608695652173913, 0.470774091627172, 0.8483333333333333, 1.0, 1.0, 0.6277149638588317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26824756973911557, 0.26824756973911557, 0.32673659529102267], 
reward next is 0.6733, 
noisyNet noise sample is [array([-0.983188], dtype=float32), -0.56729114]. 
=============================================
[2019-03-27 16:18:24,723] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2401035e-38], sum to 1.0000
[2019-03-27 16:18:24,736] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2825
[2019-03-27 16:18:24,741] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4814624873965489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672885.4902672878, 672885.4902672871, 180758.0926966805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5113800.0000, 
sim time next is 5114400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4805817283252084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671654.9019920448, 671654.9019920448, 180625.1653267668], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3741948534038655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1865708061089013, 0.1865708061089013, 0.2695897989951743], 
reward next is 0.7304, 
noisyNet noise sample is [array([-1.4028964], dtype=float32), -0.5289638]. 
=============================================
[2019-03-27 16:18:34,043] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.3263402e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5018611e-36], sum to 1.0000
[2019-03-27 16:18:34,052] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8184
[2019-03-27 16:18:34,057] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6245572213472688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872793.1887043887, 872793.1887043887, 205411.2944045779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5341200.0000, 
sim time next is 5341800.0000, 
raw observation next is [31.75, 76.5, 1.0, 2.0, 0.6264332755560198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 875415.9782338598, 875415.9782338605, 205774.8840607514], 
processed observation next is [1.0, 0.8260869565217391, 0.7037914691943128, 0.765, 1.0, 1.0, 0.5499196091036382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24317110506496104, 0.24317110506496123, 0.3071266926279872], 
reward next is 0.6929, 
noisyNet noise sample is [array([1.0825962], dtype=float32), 1.4609523]. 
=============================================
[2019-03-27 16:18:44,573] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3198444e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4220784e-30], sum to 1.0000
[2019-03-27 16:18:44,585] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6734
[2019-03-27 16:18:44,593] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 83.83333333333333, 1.0, 2.0, 0.5586093919426904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780599.9272565171, 780599.9272565171, 193288.5829771732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5269800.0000, 
sim time next is 5270400.0000, 
raw observation next is [28.5, 84.0, 1.0, 2.0, 0.5593961494088633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781699.7455093674, 781699.7455093674, 193425.619918706], 
processed observation next is [1.0, 0.0, 0.5497630331753555, 0.84, 1.0, 1.0, 0.4691519872395943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2171388181970465, 0.2171388181970465, 0.2886949551025463], 
reward next is 0.7113, 
noisyNet noise sample is [array([-1.1912338], dtype=float32), 0.3421581]. 
=============================================
[2019-03-27 16:18:45,195] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 16:18:45,198] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 16:18:45,199] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 16:18:45,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:18:45,200] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 16:18:45,202] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 16:18:45,202] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:18:45,201] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 16:18:45,204] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:18:45,203] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:18:45,207] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:18:45,240] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run100
[2019-03-27 16:18:45,262] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run100
[2019-03-27 16:18:45,285] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run100
[2019-03-27 16:18:45,306] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run100
[2019-03-27 16:18:45,325] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run100
[2019-03-27 16:19:18,673] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07426385], dtype=float32), 0.10811757]
[2019-03-27 16:19:18,674] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.63333333333333, 94.5, 1.0, 2.0, 0.3847759175415083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580554.8553163463, 580554.8553163469, 172954.8529640491]
[2019-03-27 16:19:18,677] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:19:18,681] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8487902219032954
[2019-03-27 16:19:33,407] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07426385], dtype=float32), 0.10811757]
[2019-03-27 16:19:33,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 1.0, 2.0, 0.3022952447846516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481388.7555171007, 481388.7555171007, 165721.4112967301]
[2019-03-27 16:19:33,411] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:19:33,415] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07060050391906447
[2019-03-27 16:19:59,903] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07426385], dtype=float32), 0.10811757]
[2019-03-27 16:19:59,904] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.21666666666667, 72.5, 1.0, 2.0, 0.8558520497399986, 1.0, 1.0, 0.8558520497399986, 0.0, 1.0, 0.0, 6.9112, 6.9112, 172.86236624, 2393664.256979185, 2393664.256979185, 448455.9977910187]
[2019-03-27 16:19:59,904] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 16:19:59,906] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.3357592e-08 5.2616149e-01 2.9867486e-14 6.9807271e-08 4.7383836e-01], sampled 0.5165973166391913
[2019-03-27 16:19:59,906] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2393664.256979185 W.
[2019-03-27 16:20:04,798] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07426385], dtype=float32), 0.10811757]
[2019-03-27 16:20:04,798] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.33333333333333, 72.66666666666666, 1.0, 2.0, 0.493160040738451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689111.4355003275, 689111.4355003275, 182531.3177667337]
[2019-03-27 16:20:04,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:20:04,804] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2643448049489341
[2019-03-27 16:20:53,802] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7900.0371 3163475645.0718 1832.0000
[2019-03-27 16:20:53,964] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.8971 2927936710.4635 1363.0000
[2019-03-27 16:20:54,149] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.1461 2779434189.8788 933.0000
[2019-03-27 16:20:54,274] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8022.5851 3006609926.9004 1709.0000
[2019-03-27 16:20:54,292] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.2508 2841992943.8779 1141.0000
[2019-03-27 16:20:55,310] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2475000, evaluation results [2475000.0, 7900.037140061201, 3163475645.0718303, 1832.0, 8249.89707635404, 2927936710.4634833, 1363.0, 8663.146089575595, 2779434189.8788095, 933.0, 8022.585140398482, 3006609926.9004335, 1709.0, 8500.25084448864, 2841992943.8778996, 1141.0]
[2019-03-27 16:21:00,985] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4794380e-08 3.2520035e-01 1.7935736e-17 3.2880700e-09 6.7479962e-01], sum to 1.0000
[2019-03-27 16:21:01,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4632
[2019-03-27 16:21:01,004] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.09999999999999, 54.0, 1.0, 2.0, 0.6712075273997786, 1.0, 2.0, 0.656193803214152, 1.0, 2.0, 1.03, 7.005095462006149, 6.9112, 170.5573041426782, 2753315.033042797, 2686053.894184597, 512389.7998845175], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5488800.0000, 
sim time next is 5489400.0000, 
raw observation next is [36.2, 53.0, 1.0, 2.0, 0.6586492345452287, 1.0, 2.0, 0.6499146567868769, 1.0, 2.0, 1.03, 7.005094471913149, 6.9112, 170.5573041426782, 2726939.68944835, 2659679.25983404, 508586.5850176261], 
processed observation next is [1.0, 0.5217391304347826, 0.9146919431279622, 0.53, 1.0, 1.0, 0.5887340175243719, 1.0, 1.0, 0.5782104298637071, 1.0, 1.0, 1.0365853658536586, 0.009389447191314914, 0.0, 0.8375144448122397, 0.7574832470689862, 0.7387997943983444, 0.7590844552501882], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7454108], dtype=float32), 0.34541044]. 
=============================================
[2019-03-27 16:21:05,828] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0904395e-28 1.0000000e+00 4.1740037e-36 5.3668922e-36 3.3963109e-24], sum to 1.0000
[2019-03-27 16:21:05,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3722
[2019-03-27 16:21:05,845] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 88.0, 1.0, 2.0, 0.5491351243688971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767355.8177284313, 767355.817728432, 191652.4824874598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5527200.0000, 
sim time next is 5527800.0000, 
raw observation next is [27.4, 88.5, 1.0, 2.0, 0.5492782176054444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767555.847002081, 767555.847002081, 191676.9936162308], 
processed observation next is [1.0, 1.0, 0.4976303317535545, 0.885, 1.0, 1.0, 0.4569617079583667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21320995750057806, 0.21320995750057806, 0.2860850650988519], 
reward next is 0.7139, 
noisyNet noise sample is [array([-0.6150594], dtype=float32), 0.9510377]. 
=============================================
[2019-03-27 16:21:09,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2613750e-10 7.7032524e-07 2.5882677e-21 2.3295751e-10 9.9999928e-01], sum to 1.0000
[2019-03-27 16:21:09,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2796
[2019-03-27 16:21:09,874] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.9, 71.0, 1.0, 2.0, 0.740194037774532, 1.0, 2.0, 0.6906870584015286, 1.0, 2.0, 1.03, 7.005100901632516, 6.9112, 170.5573041426782, 2898212.725096152, 2830947.689612315, 534324.1357831956], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5418000.0000, 
sim time next is 5418600.0000, 
raw observation next is [30.9, 71.66666666666667, 1.0, 2.0, 0.2866932548271915, 1.0, 2.0, 0.2866932548271915, 1.0, 2.0, 0.4978913727266647, 6.9112, 6.9112, 170.5573041426782, 1202106.045943506, 1202106.045943506, 299040.601895802], 
processed observation next is [1.0, 0.7391304347826086, 0.6635071090047393, 0.7166666666666667, 1.0, 1.0, 0.1405942829243271, 1.0, 1.0, 0.1405942829243271, 1.0, 1.0, 0.38767240576422524, 0.0, 0.0, 0.8375144448122397, 0.3339183460954184, 0.3339183460954184, 0.4463292565608985], 
reward next is 0.5537, 
noisyNet noise sample is [array([1.4836091], dtype=float32), -0.49935865]. 
=============================================
[2019-03-27 16:21:11,462] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:21:11,472] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5926
[2019-03-27 16:21:11,478] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 92.0, 1.0, 2.0, 0.508023626854121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709887.8341999968, 709887.8341999968, 184862.5178650199], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5626800.0000, 
sim time next is 5627400.0000, 
raw observation next is [25.68333333333333, 91.83333333333334, 1.0, 2.0, 0.506999960384305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708456.9348052735, 708456.9348052735, 184699.8230652409], 
processed observation next is [0.0, 0.13043478260869565, 0.41627172195892564, 0.9183333333333334, 1.0, 1.0, 0.40602404865578906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19679359300146487, 0.19679359300146487, 0.27567137770931477], 
reward next is 0.7243, 
noisyNet noise sample is [array([-1.3396808], dtype=float32), 0.55937546]. 
=============================================
[2019-03-27 16:21:25,654] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:21:25,673] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0705
[2019-03-27 16:21:25,678] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 91.0, 1.0, 2.0, 0.4978311659426183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695640.7150467213, 695640.7150467207, 183256.7908125539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5634000.0000, 
sim time next is 5634600.0000, 
raw observation next is [25.76666666666667, 90.16666666666667, 1.0, 2.0, 0.4987063692652957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696864.0751059032, 696864.0751059025, 183393.5386969224], 
processed observation next is [0.0, 0.21739130434782608, 0.42022116903633505, 0.9016666666666667, 1.0, 1.0, 0.3960317701991515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19357335419608424, 0.19357335419608404, 0.27372169954764536], 
reward next is 0.7263, 
noisyNet noise sample is [array([0.1199263], dtype=float32), -0.3988731]. 
=============================================
[2019-03-27 16:21:30,084] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0368940e-24 1.0000000e+00 9.5593675e-32 8.4309370e-31 4.1623987e-20], sum to 1.0000
[2019-03-27 16:21:30,095] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9915
[2019-03-27 16:21:30,100] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 86.0, 1.0, 2.0, 0.7260829362993917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1014739.006210879, 1014739.00621088, 226577.8577497597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5988000.0000, 
sim time next is 5988600.0000, 
raw observation next is [28.25, 85.5, 1.0, 2.0, 0.7465990207154669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1043425.397041194, 1043425.397041194, 231227.2041670242], 
processed observation next is [1.0, 0.30434782608695654, 0.537914691943128, 0.855, 1.0, 1.0, 0.6946976153198396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28984038806699836, 0.28984038806699836, 0.34511523010003614], 
reward next is 0.6549, 
noisyNet noise sample is [array([0.7334272], dtype=float32), -0.7812768]. 
=============================================
[2019-03-27 16:21:30,710] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3261779e-26 1.0000000e+00 8.8854553e-34 2.8536670e-34 1.6593146e-23], sum to 1.0000
[2019-03-27 16:21:30,719] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2384
[2019-03-27 16:21:30,728] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 92.66666666666666, 1.0, 2.0, 0.6906817531133346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 965241.5107897021, 965241.5107897026, 218846.6931405332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5895600.0000, 
sim time next is 5896200.0000, 
raw observation next is [26.6, 91.83333333333334, 1.0, 2.0, 0.6992387478547796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 977205.583323247, 977205.5833232463, 220681.1643941478], 
processed observation next is [1.0, 0.21739130434782608, 0.4597156398104266, 0.9183333333333334, 1.0, 1.0, 0.6376370456081681, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27144599536756864, 0.2714459953675684, 0.32937487223007134], 
reward next is 0.6706, 
noisyNet noise sample is [array([-0.5336177], dtype=float32), -1.4027072]. 
=============================================
[2019-03-27 16:21:34,087] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0098984e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2537686e-36], sum to 1.0000
[2019-03-27 16:21:34,098] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5879
[2019-03-27 16:21:34,104] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.15, 86.5, 1.0, 2.0, 0.5655401374016147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790288.5435167112, 790288.5435167112, 194501.6358680622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5862600.0000, 
sim time next is 5863200.0000, 
raw observation next is [28.06666666666666, 86.66666666666667, 1.0, 2.0, 0.5626046578740379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786184.9720289028, 786184.9720289034, 193986.0249515066], 
processed observation next is [1.0, 0.8695652173913043, 0.5292259083728275, 0.8666666666666667, 1.0, 1.0, 0.47301766008920226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.218384714452473, 0.21838471445247318, 0.28953138052463673], 
reward next is 0.7105, 
noisyNet noise sample is [array([0.98194987], dtype=float32), -0.6272551]. 
=============================================
[2019-03-27 16:21:46,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.3537740e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2917038e-31], sum to 1.0000
[2019-03-27 16:21:46,852] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5135
[2019-03-27 16:21:46,865] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 90.66666666666666, 1.0, 2.0, 0.6207729797828606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 867502.7060887204, 867502.7060887197, 204673.5221518826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5982600.0000, 
sim time next is 5983200.0000, 
raw observation next is [26.8, 90.0, 1.0, 2.0, 0.7027348165219205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 982093.6955925846, 982093.6955925851, 221436.6562364791], 
processed observation next is [1.0, 0.2608695652173913, 0.4691943127962086, 0.9, 1.0, 1.0, 0.6418491765324343, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2728038043312735, 0.27280380433127366, 0.3305024719947449], 
reward next is 0.6695, 
noisyNet noise sample is [array([0.3264113], dtype=float32), 1.596817]. 
=============================================
[2019-03-27 16:21:48,840] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:21:48,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7826
[2019-03-27 16:21:48,857] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 87.33333333333334, 1.0, 2.0, 0.5312311069662969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742328.1843169428, 742328.1843169421, 188633.784903191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6319200.0000, 
sim time next is 6319800.0000, 
raw observation next is [26.95, 87.5, 1.0, 2.0, 0.5303874794034517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741148.9103894912, 741148.9103894919, 188493.8891668532], 
processed observation next is [0.0, 0.13043478260869565, 0.476303317535545, 0.875, 1.0, 1.0, 0.43420178241379714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20587469733041425, 0.20587469733041444, 0.28133416293560176], 
reward next is 0.7187, 
noisyNet noise sample is [array([-1.9241587], dtype=float32), -0.08668569]. 
=============================================
[2019-03-27 16:21:51,725] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:21:51,734] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8868
[2019-03-27 16:21:51,739] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.55, 74.83333333333334, 1.0, 2.0, 0.5418757336280602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757208.0034848315, 757208.0034848315, 190417.7333256967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6256200.0000, 
sim time next is 6256800.0000, 
raw observation next is [29.7, 74.0, 1.0, 2.0, 0.5419591470242147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757324.6055087395, 757324.6055087395, 190431.8515311663], 
processed observation next is [0.0, 0.43478260869565216, 0.6066350710900474, 0.74, 1.0, 1.0, 0.4481435506315839, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21036794597464986, 0.21036794597464986, 0.2842266440763676], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.60809886], dtype=float32), 0.12616509]. 
=============================================
[2019-03-27 16:21:59,599] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 16:21:59,611] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8151
[2019-03-27 16:21:59,619] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 63.0, 1.0, 2.0, 0.5382711482541161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752169.2333426882, 752169.2333426889, 189809.7982757424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6352200.0000, 
sim time next is 6352800.0000, 
raw observation next is [31.33333333333333, 63.0, 1.0, 2.0, 0.5339955561636565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746192.5087479838, 746192.5087479833, 189093.9941111217], 
processed observation next is [0.0, 0.5217391304347826, 0.6840442338072668, 0.63, 1.0, 1.0, 0.43854886284777883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20727569687443995, 0.2072756968744398, 0.28222984195689804], 
reward next is 0.7178, 
noisyNet noise sample is [array([0.68077654], dtype=float32), -0.18082064]. 
=============================================
[2019-03-27 16:22:01,955] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 16:22:01,957] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 16:22:01,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:22:01,958] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 16:22:01,960] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:22:01,960] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 16:22:01,961] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 16:22:01,961] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:22:01,963] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 16:22:01,962] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:22:01,965] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 16:22:02,836] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run101
[2019-03-27 16:22:02,852] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run101
[2019-03-27 16:22:02,852] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run101
[2019-03-27 16:22:02,883] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run101
[2019-03-27 16:22:02,903] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/10/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run101
[2019-03-27 16:22:09,718] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:22:09,721] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.6, 75.0, 1.0, 2.0, 0.2560203251143444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418924.0461048051, 418924.0461048051, 161497.9383172683]
[2019-03-27 16:22:09,722] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 16:22:09,725] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9173219494245223
[2019-03-27 16:22:19,791] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:22:19,791] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.2, 79.0, 1.0, 2.0, 0.3219883821243794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 517031.1649710028, 517031.1649710028, 168380.4179316611]
[2019-03-27 16:22:19,792] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 16:22:19,797] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3641253124936956
[2019-03-27 16:22:27,537] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:22:27,539] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.558227815, 82.1328668, 1.0, 2.0, 0.4429489956013832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639626.2901976953, 639626.2901976953, 177792.8436540304]
[2019-03-27 16:22:27,539] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:22:27,542] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4879265626464686
[2019-03-27 16:22:43,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:22:43,699] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.22623566, 94.5039015, 1.0, 2.0, 0.6209359106118263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867730.4877060207, 867730.4877060207, 204710.6081883075]
[2019-03-27 16:22:43,700] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:22:43,701] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0320111e-30 1.0000000e+00 8.1258518e-38 2.5920314e-38 7.8138281e-26], sampled 0.7091395775267041
[2019-03-27 16:22:50,648] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:22:50,649] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 1.0, 2.0, 0.3027236976142853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482070.9057581156, 482070.9057581156, 165770.3967613646]
[2019-03-27 16:22:50,650] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:22:50,655] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9872320689781376
[2019-03-27 16:22:51,000] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:22:51,001] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.3, 59.33333333333334, 1.0, 2.0, 0.5040161673193051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704286.146214223, 704286.1462142223, 184228.0508752891]
[2019-03-27 16:22:51,004] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 16:22:51,006] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.0382327e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9419423e-32], sampled 0.2635696934012838
[2019-03-27 16:22:54,342] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:22:54,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.4983102221022667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696310.3400400049, 696310.3400400049, 183332.0542456737]
[2019-03-27 16:22:54,344] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:22:54,349] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7942837e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3078155e-36], sampled 0.5159493229721005
[2019-03-27 16:22:55,323] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:22:55,326] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.80689263333333, 86.87482642, 1.0, 2.0, 0.4804931977667076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671405.9765994432, 671405.9765994432, 180596.1129624229]
[2019-03-27 16:22:55,327] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:22:55,330] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8268279327899516
[2019-03-27 16:23:21,668] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:23:21,669] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.43843350666667, 79.73895581000001, 1.0, 2.0, 0.5892338495688506, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9842478864054658, 6.911199999999999, 6.9112, 168.9129564289078, 1656509.830131546, 1656509.830131546, 352725.5205970918]
[2019-03-27 16:23:21,672] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 16:23:21,675] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.9502532e-23 1.0000000e+00 3.9314471e-29 3.0411888e-28 4.8402374e-18], sampled 0.5698744342557401
[2019-03-27 16:23:21,675] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1656509.830131546 W.
[2019-03-27 16:23:53,847] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:23:53,850] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.5, 79.0, 1.0, 2.0, 0.5538822400133826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773991.8055218145, 773991.8055218138, 192467.5010352749]
[2019-03-27 16:23:53,850] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 16:23:53,854] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1927507e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0196025e-36], sampled 0.5666752004264651
[2019-03-27 16:23:58,304] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:23:58,304] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.3, 82.0, 1.0, 2.0, 0.7033260853027912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 982920.3944460244, 982920.3944460244, 221565.6566799715]
[2019-03-27 16:23:58,304] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 16:23:58,306] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8055791e-30 1.0000000e+00 2.7088835e-37 1.1945486e-38 1.3274078e-26], sampled 0.4969620174508105
[2019-03-27 16:24:06,677] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:24:06,679] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.76666666666667, 69.0, 1.0, 2.0, 0.588131535570932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821870.0871197655, 821870.0871197655, 198555.7400276006]
[2019-03-27 16:24:06,682] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 16:24:06,686] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5620145e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8331375e-31], sampled 0.7376959825046473
[2019-03-27 16:24:07,702] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07387228], dtype=float32), 0.110629894]
[2019-03-27 16:24:07,707] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.1, 59.16666666666667, 1.0, 2.0, 0.8632549129565608, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986056654432467, 6.9112, 168.9124494546263, 2103604.799686959, 2050499.072082659, 424799.906878471]
[2019-03-27 16:24:07,709] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 16:24:07,711] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4716998e-13 9.9978608e-01 1.7435113e-21 1.2850179e-14 2.1393527e-04], sampled 0.35414199208811126
[2019-03-27 16:24:07,713] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2103604.799686959 W.
[2019-03-27 16:24:11,078] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7959.1036 3160504291.6422 1685.0000
[2019-03-27 16:24:11,207] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8554.1693 2839716717.7365 1010.0000
[2019-03-27 16:24:11,313] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8704.1253 2778607054.5349 842.0000
[2019-03-27 16:24:11,714] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8297.1987 2927185437.5411 1263.0000
[2019-03-27 16:24:11,777] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8098.1792 3002910132.4356 1503.0000
[2019-03-27 16:24:12,791] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2500000, evaluation results [2500000.0, 7959.103592726073, 3160504291.642234, 1685.0, 8297.198663278876, 2927185437.541059, 1263.0, 8704.125286607508, 2778607054.5348577, 842.0, 8098.179197435376, 3002910132.435557, 1503.0, 8554.169340562392, 2839716717.7365437, 1010.0]
