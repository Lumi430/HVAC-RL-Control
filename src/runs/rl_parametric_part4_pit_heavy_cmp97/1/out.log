Using TensorFlow backend.
[2019-05-04 21:49:28,582] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='iw_af4_3', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.001, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Heavy-Pit-Train-Repeat-Cmp97-v1', eval_act_func='8', eval_env_res_max_keep=200, eval_epi_num=1, eval_freq=50000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_add_time_to_state=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_r_term_zero=True, is_warm_start=False, job_mode='Train', learning_rate=0.0001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=10000000, metric_func='part4_cmp97', model_dir='None', model_param=[512, 4], model_type='nn', num_threads=16, output='./Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_heuri_cmp97', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=10.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=13, test_env=['Part4-Heavy-Pit-Test-Repeat-Real97-v1'], test_mode='Multiple', train_act_func='8', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=5.0, weight_initer='glorot_uniform', window_len=3)
[2019-05-04 21:49:28,582] A3C_AGENT_MAIN INFO:Start compiling...
2019-05-04 21:49:28.633276: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-05-04 21:49:47,964] A3C_AGENT_MAIN INFO:Start the learning...
[2019-05-04 21:49:47,964] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Heavy-Pit-Train-Repeat-Cmp97-v1', 'Part4-Heavy-Pit-Test-Repeat-Real97-v1'] ...
[2019-05-04 21:49:47,985] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation worker starts!
[2019-05-04 21:49:47,999] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation worker starts!
[2019-05-04 21:49:47,999] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:47,999] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-05-04 21:49:48,074] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:48,075] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res2/Eplus-env-sub_run1
[2019-05-04 21:49:49,000] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:49,002] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-05-04 21:49:49,085] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:49,086] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res3/Eplus-env-sub_run1
[2019-05-04 21:49:50,003] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:50,004] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-05-04 21:49:50,233] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:50,235] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res4/Eplus-env-sub_run1
[2019-05-04 21:49:51,005] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:51,006] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-05-04 21:49:51,093] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:51,094] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res5/Eplus-env-sub_run1
[2019-05-04 21:49:52,007] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:52,008] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-05-04 21:49:52,096] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:52,097] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res6/Eplus-env-sub_run1
[2019-05-04 21:49:53,009] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:53,010] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-05-04 21:49:53,114] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:53,115] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res7/Eplus-env-sub_run1
[2019-05-04 21:49:54,011] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:54,011] A3C_AGENT_WORKER-Thread-8 INFO:Local worker starts!
[2019-05-04 21:49:54,140] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:54,141] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res8/Eplus-env-sub_run1
[2019-05-04 21:49:54,788] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-05-04 21:49:54,789] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation job starts!
[2019-05-04 21:49:54,789] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation job starts!
[2019-05-04 21:49:54,789] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:54,789] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:54,792] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res1/Eplus-env-sub_run1
[2019-05-04 21:49:54,792] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Test-Repeat-Real97-v1-res1/Eplus-env-sub_run1
[2019-05-04 21:49:55,012] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:55,013] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-05-04 21:49:55,140] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:55,142] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res9/Eplus-env-sub_run1
[2019-05-04 21:49:56,014] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:56,015] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-05-04 21:49:56,188] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:56,190] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res10/Eplus-env-sub_run1
[2019-05-04 21:49:57,016] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:57,017] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-05-04 21:49:57,146] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:57,148] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res11/Eplus-env-sub_run1
[2019-05-04 21:49:58,017] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:58,018] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-05-04 21:49:58,148] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:58,150] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res12/Eplus-env-sub_run1
[2019-05-04 21:49:59,019] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:49:59,019] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-05-04 21:49:59,140] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:49:59,141] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res13/Eplus-env-sub_run1
[2019-05-04 21:50:00,020] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:50:00,022] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-05-04 21:50:00,162] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:50:00,163] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res14/Eplus-env-sub_run1
[2019-05-04 21:50:01,023] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:50:01,024] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-05-04 21:50:01,165] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:50:01,167] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res15/Eplus-env-sub_run1
[2019-05-04 21:50:02,025] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:50:02,026] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-05-04 21:50:02,435] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:50:02,437] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res16/Eplus-env-sub_run1
[2019-05-04 21:50:03,026] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-05-04 21:50:03,027] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-05-04 21:50:03,159] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:50:03,161] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res17/Eplus-env-sub_run1
[2019-05-04 21:52:23,139] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-05-04 21:52:23,140] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 DEBUG:Observation this: [9.705555556, 60.5, 0.0, 154.0, 360.1659808, 176.35925, 3.244444444000001, 5.335290603367788, 17.5, 23.61746709710178, 22.2, 1.0, 0.0]
[2019-05-04 21:52:23,140] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 DEBUG:Observation forecast: []
[2019-05-04 21:52:23,141] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Softmax [0.08357292 0.090333   0.08958042 0.09921812 0.08709928 0.08470093
 0.0869986  0.09835128 0.10834383 0.07483447 0.09696718], sampled 0.46572882681132
[2019-05-04 21:52:36,900] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation: average rewards by now are 2648.1874 256676.3727 68584.6320
[2019-05-04 21:52:36,910] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 21:52:37,037] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 21:52:55,767] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation: average rewards by now are 2647.6109 264286.6316 919.3311
[2019-05-04 21:52:55,778] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 21:52:55,892] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 21:52:56,780] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 2647.610902094356, 264286.63160514744, 919.3310644733092, 2648.1873527594585, 256676.3726757499, 68584.63195242849]
[2019-05-04 21:53:01,323] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.09024093 0.0884853  0.08780693 0.09318355 0.08719342 0.08529475
 0.09127694 0.09198291 0.10493642 0.08176545 0.09783349], sum to 1.0000
[2019-05-04 21:53:01,324] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0724
[2019-05-04 21:53:01,445] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [6.9, 84.0, 7.45, 270.0, 50.0, 0.0, 12.05, 0.5505201385521862, 20.0, 25.77308309299681, 22.7, 1.0, 27.409417223527964], 
current ob forecast is [], 
actual action is [1.9000000000000004, 17.5], 
sim time this is 55800.0000, 
sim time next is 56700.0000, 
raw observation next is [6.749999999999999, 83.0, 7.575, 270.0, 42.0, 0.0, 1.9, 0.4890524061464482, 17.5, 25.58879950089498, 22.7, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5064102564102564, 0.83, 0.6886363636363636, 0.75, 0.1111111111111111, 0.0, 0.5316666666666666, 0.6630174687154827, -0.05555555555555555, 1.0841142144135685, 0.6714285714285714, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8197266], dtype=float32), 0.93267244]. 
=============================================
[2019-05-04 21:53:08,140] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.09041483 0.11016928 0.12825118 0.12092227 0.08350921 0.15319206
 0.06330918 0.07027017 0.07536127 0.04659941 0.05800119], sum to 1.0000
[2019-05-04 21:53:08,145] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8516
[2019-05-04 21:53:08,266] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.4, 71.0, 4.225, 245.0, 0.0, 0.0, -3.4, -0.5666983932922444, 40.0, 21.45625126852543, 21.5, 0.0, 27.582303981749234], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 55.0], 
sim time this is 168300.0000, 
sim time next is 169200.0000, 
raw observation next is [-8.4, 71.0, 4.1, 240.0, 0.0, 0.0, -3.4, -0.5593779616644349, 55.0, 21.38479492853644, 21.5, 0.0, 49.751863506814914], 
processed observation next is [1.0, 1.0, 0.11794871794871795, 0.71, 0.3727272727272727, 0.6666666666666666, 0.0, 0.0, 0.44333333333333336, 0.3135406794451884, 0.7777777777777778, 0.48354213264806284, 0.5, 0.0, 0.585316041256646], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6842673], dtype=float32), -1.2169368]. 
=============================================
[2019-05-04 21:53:17,960] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.15288253 0.08071486 0.11987871 0.08796657 0.02728325 0.0582984
 0.02120416 0.04431938 0.2728423  0.07455096 0.06005883], sum to 1.0000
[2019-05-04 21:53:17,963] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1930
[2019-05-04 21:53:18,099] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.875, 67.75, 6.675000000000001, 257.5, 0.0, 0.0, -5.6, -0.9029931460477066, 55.0, 20.19285142594414, 21.5, 0.0, 48.44862123495236], 
current ob forecast is [], 
actual action is [-5.875, 20.0], 
sim time this is 278100.0000, 
sim time next is 279000.0000, 
raw observation next is [-11.15, 68.5, 6.15, 255.0, 0.0, 0.0, -5.875, -0.9254679305179812, 20.0, 20.15310062052331, 21.5, 0.0, 34.36553989208217], 
processed observation next is [1.0, 0.21739130434782608, 0.04743589743589743, 0.685, 0.5590909090909091, 0.7083333333333334, 0.0, 0.0, 0.40208333333333335, 0.19151068982733963, 0.0, 0.3075858029319016, 0.5, 0.0, 0.40430046931861374], 
reward next is 0.5457, 
noisyNet noise sample is [array([0.6271889], dtype=float32), 0.84522694]. 
=============================================
[2019-05-04 21:53:18,128] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[13.033484]
 [13.152904]
 [13.439305]
 [13.691199]
 [13.755395]], R is [[13.08008862]
 [12.94928741]
 [12.95890713]
 [12.82931805]
 [12.70102501]].
[2019-05-04 21:53:26,615] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.22272596 0.20518544 0.07472953 0.06895938 0.0059334  0.01271051
 0.00258129 0.00463015 0.2021904  0.12599759 0.07435635], sum to 1.0000
[2019-05-04 21:53:26,615] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5563
[2019-05-04 21:53:26,882] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.7, 57.0, 5.1, 270.0, 0.0, 0.0, -6.425000000000001, -0.5749091272141696, 65.0, 21.99192398650725, 22.7, 1.0, 71.82506438266323], 
current ob forecast is [], 
actual action is [-6.699999999999999, 30.0], 
sim time this is 324000.0000, 
sim time next is 324900.0000, 
raw observation next is [-11.85, 58.5, 5.1, 272.5, 0.0, 0.0, -6.699999999999999, -0.5964366325901013, 30.0, 22.00958589833276, 22.7, 1.0, 45.87854921065483], 
processed observation next is [1.0, 0.782608695652174, 0.029487179487179497, 0.585, 0.4636363636363636, 0.7569444444444444, 0.0, 0.0, 0.38833333333333336, 0.3011877891366329, 0.2222222222222222, 0.5727979854761084, 0.6714285714285714, 1.0, 0.5397476377724097], 
reward next is 0.3586, 
noisyNet noise sample is [array([-0.5297824], dtype=float32), 0.6230555]. 
=============================================
[2019-05-04 21:53:32,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.80990847e-04 1.75533223e-03 7.74502580e-04 3.09099967e-04
 1.16256124e-04 9.23793050e-05 1.62504948e-05 1.61629068e-05
 2.40992159e-02 1.53816184e-02 9.57158208e-01], sum to 1.0000
[2019-05-04 21:53:32,622] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9874
[2019-05-04 21:53:32,895] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.8, 47.25, 6.450000000000001, 227.5, 53.25, 873.25, -6.1, -0.9377016207876444, 65.0, 20.68329435368481, 22.7, 1.0, 71.08698685887059], 
current ob forecast is [], 
actual action is [-5.800000000000001, 65.0], 
sim time this is 395100.0000, 
sim time next is 396000.0000, 
raw observation next is [-10.5, 46.0, 6.4, 230.0, 51.5, 859.5, -5.800000000000001, -0.8928337594577735, 65.0, 20.8921460969533, 22.7, 1.0, 72.07479984310663], 
processed observation next is [1.0, 0.6086956521739131, 0.0641025641025641, 0.46, 0.5818181818181819, 0.6388888888888888, 0.13624338624338625, 0.8595, 0.4033333333333333, 0.20238874684740882, 1.0, 0.41316372813618585, 0.6714285714285714, 1.0, 0.8479388216836075], 
reward next is 0.4487, 
noisyNet noise sample is [array([-0.9476754], dtype=float32), -0.6367704]. 
=============================================
[2019-05-04 21:53:32,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[48.256706]
 [47.898735]
 [48.05534 ]
 [47.74251 ]
 [47.33755 ]], R is [[48.26691437]
 [48.07926941]
 [47.64325333]
 [47.61009216]
 [47.41519547]].
[2019-05-04 21:53:35,748] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7309: loss -0.6751
[2019-05-04 21:53:35,812] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7309: learning rate 0.0001
[2019-05-04 21:53:35,855] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00848097 0.02079485 0.00564488 0.00772675 0.00300207 0.00253092
 0.00106295 0.00130935 0.01316039 0.02596924 0.91031766], sum to 1.0000
[2019-05-04 21:53:35,855] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4361
[2019-05-04 21:53:36,026] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.6, 52.0, 5.1, 170.0, 0.0, 0.0, -5.75, -1.029096699238006, 60.0, 19.99883933040422, 21.5, 0.0, 50.43477054702932], 
current ob forecast is [], 
actual action is [-5.6, 65.0], 
sim time this is 450000.0000, 
sim time next is 450900.0000, 
raw observation next is [-10.325, 50.0, 4.975, 167.5, 0.0, 0.0, -5.6, -1.032064718536081, 65.0, 19.9238717160477, 21.5, 0.0, 58.72685024683117], 
processed observation next is [1.0, 0.21739130434782608, 0.06858974358974361, 0.5, 0.4522727272727272, 0.4652777777777778, 0.0, 0.0, 0.4066666666666666, 0.1559784271546397, 1.0, 0.2748388165782428, 0.5, 0.0, 0.690904120550955], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0000956], dtype=float32), 0.44570094]. 
=============================================
[2019-05-04 21:53:37,073] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 7485: loss 1.7067
[2019-05-04 21:53:37,087] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 7485: learning rate 0.0001
[2019-05-04 21:53:37,801] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7608: loss 9.0498
[2019-05-04 21:53:37,801] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7608: learning rate 0.0001
[2019-05-04 21:53:37,904] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7629: loss -2.2920
[2019-05-04 21:53:37,905] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7629: learning rate 0.0001
[2019-05-04 21:53:38,059] A3C_AGENT_WORKER-Thread-8 INFO:Local step 500, global step 7659: loss 0.7910
[2019-05-04 21:53:38,084] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 500, global step 7664: learning rate 0.0001
[2019-05-04 21:53:38,569] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7780: loss 13.5698
[2019-05-04 21:53:38,571] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7780: learning rate 0.0001
[2019-05-04 21:53:38,926] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7886: loss 0.2565
[2019-05-04 21:53:38,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7886: learning rate 0.0001
[2019-05-04 21:53:39,034] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7926: loss 13.2987
[2019-05-04 21:53:39,036] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7927: learning rate 0.0001
[2019-05-04 21:53:39,451] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8080: loss -5.4784
[2019-05-04 21:53:39,453] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8080: learning rate 0.0001
[2019-05-04 21:53:39,480] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8091: loss 0.3383
[2019-05-04 21:53:39,482] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8092: learning rate 0.0001
[2019-05-04 21:53:39,567] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8124: loss 11.1521
[2019-05-04 21:53:39,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8124: learning rate 0.0001
[2019-05-04 21:53:39,680] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 8171: loss 6.6668
[2019-05-04 21:53:39,682] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 8171: learning rate 0.0001
[2019-05-04 21:53:39,807] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8219: loss 3.2022
[2019-05-04 21:53:39,809] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8219: learning rate 0.0001
[2019-05-04 21:53:39,876] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8242: loss 1.0301
[2019-05-04 21:53:39,894] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8248: learning rate 0.0001
[2019-05-04 21:53:40,022] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8272: loss 4.8282
[2019-05-04 21:53:40,023] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8272: learning rate 0.0001
[2019-05-04 21:53:40,097] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8290: loss 7.4549
[2019-05-04 21:53:40,098] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8290: learning rate 0.0001
[2019-05-04 21:53:47,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3733979e-01 7.5888491e-01 5.1909126e-05 2.3006204e-04 2.0925384e-06
 1.7228057e-06 4.1286435e-06 4.6511805e-06 5.2237185e-05 1.5538966e-04
 3.2731872e-03], sum to 1.0000
[2019-05-04 21:53:47,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2836
[2019-05-04 21:53:47,887] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 84.0, 6.1, 170.0, 0.0, 0.0, 5.65, -0.5670291648637101, 20.0, 22.19744049767692, 22.7, 1.0, 29.007446588906785], 
current ob forecast is [], 
actual action is [5.5, 20.0], 
sim time this is 496800.0000, 
sim time next is 497700.0000, 
raw observation next is [0.65, 87.0, 5.975, 162.5, 0.0, 0.0, 5.5, -0.5989419804885188, 20.0, 22.06525900283167, 22.7, 1.0, 21.845134320275843], 
processed observation next is [1.0, 0.782608695652174, 0.35000000000000003, 0.87, 0.5431818181818181, 0.4513888888888889, 0.0, 0.0, 0.5916666666666667, 0.30035267317049374, 0.0, 0.58075128611881, 0.6714285714285714, 1.0, 0.2570015802385393], 
reward next is 0.6001, 
noisyNet noise sample is [array([0.05257129], dtype=float32), 0.010853071]. 
=============================================
[2019-05-04 21:53:49,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7755450e-02 9.3908691e-01 5.5760721e-04 3.8564019e-04 1.4545151e-04
 1.5833095e-04 2.6985398e-04 1.6340925e-04 1.6661253e-03 3.2730089e-03
 6.5381601e-03], sum to 1.0000
[2019-05-04 21:53:49,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0746
[2019-05-04 21:53:50,058] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.85, 87.0, 5.1, 282.5, 0.0, 0.0, 3.3, -1.218497748121284, 20.0, 18.8055115003667, 22.7, 1.0, 8.446392799029683], 
current ob forecast is [], 
actual action is [3.15, 20.0], 
sim time this is 580500.0000, 
sim time next is 581400.0000, 
raw observation next is [-2.0, 87.0, 5.1, 285.0, 0.0, 0.0, 3.15, -1.243568127887921, 20.0, 18.64274376969959, 22.7, 1.0, 8.652356159933843], 
processed observation next is [0.0, 0.7391304347826086, 0.28205128205128205, 0.87, 0.4636363636363636, 0.7916666666666666, 0.0, 0.0, 0.5525, 0.08547729070402636, 0.0, 0.09182053852851269, 0.6714285714285714, 1.0, 0.10179242541098639], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.27918655], dtype=float32), -1.1156824]. 
=============================================
[2019-05-04 21:53:58,577] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5721740e-02 7.1262860e-01 3.5036333e-05 4.3114909e-05 1.3397665e-06
 3.9194137e-07 1.0171462e-04 3.0428482e-05 8.0003100e-04 7.4408203e-03
 2.3319665e-01], sum to 1.0000
[2019-05-04 21:53:58,580] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0573
[2019-05-04 21:53:58,729] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 69.0, 3.6, 240.0, 0.0, 0.0, 1.6, -0.6885231130347494, 65.0, 21.29495194719658, 21.5, 0.0, 57.787267174138016], 
current ob forecast is [], 
actual action is [1.6, 20.0], 
sim time this is 684000.0000, 
sim time next is 684900.0000, 
raw observation next is [-3.525, 69.5, 3.45, 237.5, 0.0, 0.0, 1.6, -0.6999832198417635, 20.0, 21.35045423960422, 21.5, 0.0, 40.31361963406073], 
processed observation next is [0.0, 0.9565217391304348, 0.24294871794871795, 0.695, 0.31363636363636366, 0.6597222222222222, 0.0, 0.0, 0.5266666666666667, 0.2666722600527455, 0.0, 0.47863631994345973, 0.5, 0.0, 0.47427787804777327], 
reward next is 0.4757, 
noisyNet noise sample is [array([0.3967796], dtype=float32), 0.36692283]. 
=============================================
[2019-05-04 21:53:58,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3121333e-02 1.5729235e-01 1.4213966e-04 9.1022484e-05 3.1364998e-06
 9.7364911e-08 2.0406607e-05 2.8010534e-05 6.5767847e-04 3.4605749e-03
 8.2518321e-01], sum to 1.0000
[2019-05-04 21:53:58,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4922
[2019-05-04 21:53:58,995] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 71.0, 2.875, 232.5, 0.0, 0.0, 1.1, -0.76766101600988, 65.0, 20.78641295868702, 21.5, 0.0, 61.319354988389385], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 688500.0000, 
sim time next is 689400.0000, 
raw observation next is [-3.9, 71.0, 2.75, 235.0, 0.0, 0.0, 1.1, -0.7281137094986815, 65.0, 21.03663472506237, 21.5, 0.0, 59.137466350386546], 
processed observation next is [0.0, 1.0, 0.23333333333333334, 0.71, 0.25, 0.6527777777777778, 0.0, 0.0, 0.5183333333333333, 0.25729543016710615, 1.0, 0.4338049607231958, 0.5, 0.0, 0.6957348982398417], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.9016157], dtype=float32), 0.8771199]. 
=============================================
[2019-05-04 21:54:00,767] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.5885182e-04 9.5408326e-01 3.9840870e-06 2.1316375e-06 1.4649459e-09
 1.8382428e-09 2.2774836e-07 7.5651059e-08 2.5523739e-05 1.0310944e-04
 4.5022916e-02], sum to 1.0000
[2019-05-04 21:54:00,767] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7962
[2019-05-04 21:54:00,898] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 76.0, 5.225, 250.0, 0.0, 0.0, 2.7, -1.079200451016108, 20.0, 19.46101640901362, 21.5, 0.0, 10.125281175935847], 
current ob forecast is [], 
actual action is [2.7, 20.0], 
sim time this is 715500.0000, 
sim time next is 716400.0000, 
raw observation next is [-2.3, 76.0, 5.1, 250.0, 0.0, 0.0, 2.7, -1.105306445351584, 20.0, 19.33857714376207, 21.5, 0.0, 8.787244055473737], 
processed observation next is [1.0, 0.30434782608695654, 0.27435897435897433, 0.76, 0.4636363636363636, 0.6944444444444444, 0.0, 0.0, 0.545, 0.13156451821613868, 0.0, 0.19122530625172424, 0.5, 0.0, 0.10337934182910279], 
reward next is 0.8466, 
noisyNet noise sample is [array([0.14063182], dtype=float32), -0.6863641]. 
=============================================
[2019-05-04 21:54:02,346] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9709697e-02 7.2040629e-01 4.0283398e-04 2.7024551e-04 9.8221799e-06
 4.1473400e-06 9.0524700e-05 6.2375395e-05 2.6107938e-03 1.5164648e-02
 2.4126872e-01], sum to 1.0000
[2019-05-04 21:54:02,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1563
[2019-05-04 21:54:02,391] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 71.0, 3.0, 230.0, 0.0, 0.0, 1.225, -0.73836984243471, 20.0, 21.0381670058018, 21.5, 0.0, 38.83401721439693], 
current ob forecast is [], 
actual action is [1.1, 20.0], 
sim time this is 687600.0000, 
sim time next is 688500.0000, 
raw observation next is [-3.9, 71.0, 2.875, 232.5, 0.0, 0.0, 1.1, -0.763903718307359, 20.0, 20.99074824377863, 21.5, 0.0, 28.890059680120753], 
processed observation next is [0.0, 1.0, 0.23333333333333334, 0.71, 0.26136363636363635, 0.6458333333333334, 0.0, 0.0, 0.5183333333333333, 0.2453654272308803, 0.0, 0.42724974911123276, 0.5, 0.0, 0.33988305506024413], 
reward next is 0.6101, 
noisyNet noise sample is [array([-0.38302886], dtype=float32), -1.6646402]. 
=============================================
[2019-05-04 21:54:02,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[36.04039 ]
 [35.624153]
 [36.582825]
 [36.88199 ]
 [37.18361 ]], R is [[36.52927399]
 [36.65711212]
 [36.2905426 ]
 [36.68968201]
 [37.04520416]].
[2019-05-04 21:54:06,299] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.4251449e-05 9.9946755e-01 2.2394256e-06 2.7369204e-08 1.8143757e-10
 2.5161109e-10 4.4630642e-08 2.1902943e-08 2.5967538e-06 1.5286784e-04
 2.9050690e-04], sum to 1.0000
[2019-05-04 21:54:06,300] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5524
[2019-05-04 21:54:06,322] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.575, 66.25, 2.4, 97.5, 0.0, 0.0, -1.45, -0.7235129923933639, 20.0, 21.17911370378651, 21.5, 0.0, 7.148877936987236], 
current ob forecast is [], 
actual action is [-1.5750000000000002, 20.0], 
sim time this is 773100.0000, 
sim time next is 774000.0000, 
raw observation next is [-6.7, 67.0, 2.0, 10.0, 0.0, 0.0, -1.575, -0.7440102733085835, 20.0, 21.09291045397563, 21.5, 0.0, 7.386304935833039], 
processed observation next is [1.0, 1.0, 0.16153846153846155, 0.67, 0.18181818181818182, 0.027777777777777776, 0.0, 0.0, 0.47375, 0.2519965755638055, 0.0, 0.4418443505679472, 0.5, 0.0, 0.08689770512744752], 
reward next is 0.8631, 
noisyNet noise sample is [array([-0.1915282], dtype=float32), -0.62808883]. 
=============================================
[2019-05-04 21:54:06,363] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[54.514088]
 [57.322784]
 [59.109932]
 [60.19552 ]
 [60.32747 ]], R is [[52.19900894]
 [52.54291534]
 [52.88680649]
 [53.23149872]
 [53.57910919]].
[2019-05-04 21:54:18,850] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15165: loss -0.3513
[2019-05-04 21:54:18,853] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15165: learning rate 0.0001
[2019-05-04 21:54:20,674] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1000, global step 15391: loss 1.3897
[2019-05-04 21:54:20,674] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1000, global step 15391: learning rate 0.0001
[2019-05-04 21:54:20,866] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 15419: loss 1.2811
[2019-05-04 21:54:20,868] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 15419: learning rate 0.0001
[2019-05-04 21:54:21,161] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15454: loss 3.7605
[2019-05-04 21:54:21,162] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15454: learning rate 0.0001
[2019-05-04 21:54:21,503] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15497: loss 2.9018
[2019-05-04 21:54:21,503] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15497: learning rate 0.0001
[2019-05-04 21:54:22,420] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15746: loss 0.9805
[2019-05-04 21:54:22,421] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15746: learning rate 0.0001
[2019-05-04 21:54:22,564] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15807: loss 0.1926
[2019-05-04 21:54:22,567] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15811: learning rate 0.0001
[2019-05-04 21:54:22,641] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15845: loss 1.0191
[2019-05-04 21:54:22,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15845: learning rate 0.0001
[2019-05-04 21:54:22,733] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.3609834e-06 9.9998379e-01 2.5234559e-08 1.1895223e-09 6.5599436e-12
 7.3975456e-13 8.5416030e-10 3.6606718e-11 1.6036147e-08 3.4602436e-07
 1.3460428e-05], sum to 1.0000
[2019-05-04 21:54:22,736] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0796
[2019-05-04 21:54:22,844] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.85, 93.0, 3.825, 147.5, 0.0, 0.0, 10.0, -0.7850220277688232, 20.0, 20.87472276186965, 22.7, 1.0, 3.897393799634734], 
current ob forecast is [], 
actual action is [9.85, 20.0], 
sim time this is 926100.0000, 
sim time next is 927000.0000, 
raw observation next is [4.7, 94.0, 3.55, 145.0, 0.0, 0.0, 9.85, -0.8139121270315134, 20.0, 20.6117869786095, 22.7, 1.0, 4.166329769174866], 
processed observation next is [1.0, 0.7391304347826086, 0.45384615384615384, 0.94, 0.3227272727272727, 0.4027777777777778, 0.0, 0.0, 0.6641666666666667, 0.2286959576561622, 0.0, 0.37311242551564305, 0.6714285714285714, 1.0, 0.04901564434323372], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5458431], dtype=float32), 0.60029155]. 
=============================================
[2019-05-04 21:54:22,868] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[64.25295 ]
 [64.39991 ]
 [64.23518 ]
 [64.110825]
 [63.948013]], R is [[63.23435211]
 [62.60200882]
 [61.9759903 ]
 [61.35623169]
 [60.75494766]].
[2019-05-04 21:54:23,080] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16025: loss 0.7778
[2019-05-04 21:54:23,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16025: learning rate 0.0001
[2019-05-04 21:54:23,378] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16172: loss 0.5135
[2019-05-04 21:54:23,379] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16172: learning rate 0.0001
[2019-05-04 21:54:23,616] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16277: loss 0.1110
[2019-05-04 21:54:23,618] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16277: learning rate 0.0001
[2019-05-04 21:54:23,960] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16412: loss -0.2227
[2019-05-04 21:54:23,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16413: learning rate 0.0001
[2019-05-04 21:54:24,386] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16552: loss 0.3420
[2019-05-04 21:54:24,389] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16554: learning rate 0.0001
[2019-05-04 21:54:24,462] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16575: loss -0.1382
[2019-05-04 21:54:24,464] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16576: learning rate 0.0001
[2019-05-04 21:54:24,542] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16607: loss 16.2564
[2019-05-04 21:54:24,544] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16607: learning rate 0.0001
[2019-05-04 21:54:24,980] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16719: loss 99.4624
[2019-05-04 21:54:24,993] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16719: learning rate 0.0001
[2019-05-04 21:54:30,540] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.12259756e-02 1.27470523e-01 2.26708385e-03 1.78925961e-03
 3.19331273e-04 1.00552745e-04 1.64438190e-03 1.22721540e-03
 1.26770167e-02 1.33730154e-02 8.07905555e-01], sum to 1.0000
[2019-05-04 21:54:30,542] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5014
[2019-05-04 21:54:30,717] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.2, 86.0, 5.6, 210.0, 124.0, 0.0, 17.05, 0.08572249618178929, 65.0, 24.0635860315389, 22.7, 1.0, 47.75595454093059], 
current ob forecast is [], 
actual action is [17.2, 65.0], 
sim time this is 993600.0000, 
sim time next is 994500.0000, 
raw observation next is [12.325, 86.0, 5.999999999999999, 205.0, 126.0, 0.0, 17.2, 0.08801062992751396, 65.0, 24.05764979062465, 22.7, 1.0, 47.586481511755125], 
processed observation next is [1.0, 0.5217391304347826, 0.6493589743589744, 0.86, 0.5454545454545454, 0.5694444444444444, 0.3333333333333333, 0.0, 0.7866666666666667, 0.5293368766425046, 1.0, 0.8653785415178074, 0.6714285714285714, 1.0, 0.559840958961825], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.70411205], dtype=float32), 0.5314791]. 
=============================================
[2019-05-04 21:54:30,818] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[18.568573]
 [18.537445]
 [18.51639 ]
 [18.516035]
 [18.514887]], R is [[18.41847229]
 [18.23428726]
 [18.05194473]
 [17.87142563]
 [17.69271088]].
[2019-05-04 21:54:32,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.20101597e-07 9.99997497e-01 1.41602025e-11 8.59557092e-10
 3.22313587e-14 4.02864302e-16 4.18337538e-13 5.91397862e-14
 6.80112890e-08 2.83382402e-08 2.13620206e-06], sum to 1.0000
[2019-05-04 21:54:32,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5709
[2019-05-04 21:54:32,254] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 77.0, 5.1, 200.0, 0.0, 0.0, 19.4, 0.205579124303626, 20.0, 24.48634459376797, 22.7, 1.0, 8.541276007193572], 
current ob forecast is [], 
actual action is [19.4, 20.0], 
sim time this is 1022400.0000, 
sim time next is 1023300.0000, 
raw observation next is [14.4, 77.0, 4.85, 202.5, 0.0, 0.0, 19.4, 0.1931800104734425, 20.0, 24.41924648645659, 21.5, 0.0, 7.274880189750211], 
processed observation next is [1.0, 0.8695652173913043, 0.7025641025641025, 0.77, 0.44090909090909086, 0.5625, 0.0, 0.0, 0.8233333333333334, 0.5643933368244808, 0.0, 0.9170352123509414, 0.5, 0.0, 0.08558682576176718], 
reward next is 0.8644, 
noisyNet noise sample is [array([0.42233407], dtype=float32), 0.6215606]. 
=============================================
[2019-05-04 21:54:36,021] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0243467e-08 9.9999988e-01 3.8432639e-13 7.7544161e-13 3.1615210e-17
 1.8538013e-20 8.0946937e-16 2.4123833e-15 1.5825936e-10 4.6306962e-11
 7.5970725e-08], sum to 1.0000
[2019-05-04 21:54:36,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9601
[2019-05-04 21:54:36,051] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 58.5, 4.35, 165.0, 0.0, 0.0, 19.7, -0.1332180409692388, 20.0, 22.65386045134434, 22.7, 1.0, 0.012936596887809348], 
current ob forecast is [], 
actual action is [19.4, 20.0], 
sim time this is 1107000.0000, 
sim time next is 1107900.0000, 
raw observation next is [14.1, 59.25, 4.475, 162.5, 0.0, 0.0, 19.4, -0.1415051553089925, 20.0, 22.61922432669326, 22.7, 1.0, 0.12929645990625643], 
processed observation next is [1.0, 0.8260869565217391, 0.694871794871795, 0.5925, 0.4068181818181818, 0.4513888888888889, 0.0, 0.0, 0.8233333333333334, 0.45283161489700247, 0.0, 0.6598891895276086, 0.6714285714285714, 1.0, 0.0015211348224265463], 
reward next is 0.9485, 
noisyNet noise sample is [array([-0.11201401], dtype=float32), 0.84781116]. 
=============================================
[2019-05-04 21:54:41,722] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5920627e-08 9.9999869e-01 3.5444974e-12 2.0256526e-10 1.9026047e-14
 1.0139838e-17 3.6651728e-13 1.8354750e-13 2.3533404e-07 2.3373057e-09
 1.1303567e-06], sum to 1.0000
[2019-05-04 21:54:41,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8513
[2019-05-04 21:54:41,857] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.1, 81.5, 7.15, 135.0, 0.0, 0.0, 21.1, -0.5526692622422961, 20.0, 20.69121484585109, 21.5, 0.0, 1.3610459939125328], 
current ob forecast is [], 
actual action is [21.1, 20.0], 
sim time this is 1215000.0000, 
sim time next is 1215900.0000, 
raw observation next is [16.1, 82.25, 7.425000000000001, 137.5, 0.0, 0.0, 21.1, -0.5532035539780572, 20.0, 20.68556798745934, 21.5, 0.0, 1.3816984222841706], 
processed observation next is [0.0, 0.043478260869565216, 0.7461538461538462, 0.8225, 0.675, 0.3819444444444444, 0.0, 0.0, 0.8516666666666667, 0.3155988153406476, 0.0, 0.3836525696370486, 0.5, 0.0, 0.01625527555628436], 
reward next is 0.9337, 
noisyNet noise sample is [array([-0.2764969], dtype=float32), -0.0057657543]. 
=============================================
[2019-05-04 21:54:44,075] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23325: loss 2.6273
[2019-05-04 21:54:44,077] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23325: learning rate 0.0001
[2019-05-04 21:54:44,455] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23463: loss 0.0218
[2019-05-04 21:54:44,456] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23463: learning rate 0.0001
[2019-05-04 21:54:45,105] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 23628: loss 10.2363
[2019-05-04 21:54:45,106] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 23628: learning rate 0.0001
[2019-05-04 21:54:45,241] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23673: loss 1.4021
[2019-05-04 21:54:45,246] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23673: learning rate 0.0001
[2019-05-04 21:54:45,254] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1500, global step 23676: loss 2.9564
[2019-05-04 21:54:45,256] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1500, global step 23677: learning rate 0.0001
[2019-05-04 21:54:45,672] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23769: loss 4.8576
[2019-05-04 21:54:45,673] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23769: learning rate 0.0001
[2019-05-04 21:54:46,420] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23885: loss 7.4698
[2019-05-04 21:54:46,422] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23885: learning rate 0.0001
[2019-05-04 21:54:46,988] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23945: loss 3.5864
[2019-05-04 21:54:46,988] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23945: learning rate 0.0001
[2019-05-04 21:54:47,706] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24016: loss 0.3664
[2019-05-04 21:54:47,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24016: learning rate 0.0001
[2019-05-04 21:54:49,113] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24221: loss 0.0527
[2019-05-04 21:54:49,113] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24221: learning rate 0.0001
[2019-05-04 21:54:49,535] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24287: loss 0.0707
[2019-05-04 21:54:49,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24288: learning rate 0.0001
[2019-05-04 21:54:49,683] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24309: loss -0.0098
[2019-05-04 21:54:49,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24309: learning rate 0.0001
[2019-05-04 21:54:50,090] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24382: loss -1.6160
[2019-05-04 21:54:50,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24382: learning rate 0.0001
[2019-05-04 21:54:50,492] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24467: loss 28.8901
[2019-05-04 21:54:50,496] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24467: learning rate 0.0001
[2019-05-04 21:54:50,666] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24505: loss 3.7584
[2019-05-04 21:54:50,667] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24505: learning rate 0.0001
[2019-05-04 21:54:51,056] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24627: loss 0.3930
[2019-05-04 21:54:51,059] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24628: learning rate 0.0001
[2019-05-04 21:54:53,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3028298e-02 8.9590549e-01 3.4673320e-04 1.2590016e-04 1.5628733e-05
 5.9742947e-06 3.1742118e-05 6.2098756e-05 6.3150370e-04 1.2319940e-02
 2.7526677e-02], sum to 1.0000
[2019-05-04 21:54:53,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4212
[2019-05-04 21:54:53,059] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 3.0, 340.0, 93.0, 0.0, 5.375, -0.8285830320014513, 20.0, 20.2268155827976, 22.7, 1.0, 6.607549162092891], 
current ob forecast is [], 
actual action is [5.5, 20.0], 
sim time this is 1429200.0000, 
sim time next is 1430100.0000, 
raw observation next is [0.65, 92.0, 3.0, 265.0, 91.5, 0.0, 5.5, -0.807272543104121, 20.0, 20.31908518791384, 22.7, 1.0, 6.591319972413742], 
processed observation next is [1.0, 0.5652173913043478, 0.35000000000000003, 0.92, 0.2727272727272727, 0.7361111111111112, 0.24206349206349206, 0.0, 0.5916666666666667, 0.23090915229862632, 0.0, 0.3312978839876912, 0.6714285714285714, 1.0, 0.07754494085192637], 
reward next is 0.2131, 
noisyNet noise sample is [array([1.7154309], dtype=float32), -1.4598078]. 
=============================================
[2019-05-04 21:54:53,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.8167098e-02 6.9892299e-01 2.1949401e-03 2.6481361e-03 7.7091355e-04
 2.6840123e-04 5.8074272e-04 1.7488712e-03 4.3796040e-03 4.9223877e-02
 1.7109449e-01], sum to 1.0000
[2019-05-04 21:54:53,896] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3442
[2019-05-04 21:54:53,978] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 88.0, 1.5, 240.0, 0.0, 0.0, 6.1, -0.8826278931395928, 20.0, 19.85658409480165, 22.7, 1.0, 25.188086431175588], 
current ob forecast is [], 
actual action is [6.1, 20.0], 
sim time this is 1447200.0000, 
sim time next is 1448100.0000, 
raw observation next is [1.1, 89.0, 1.875, 205.0, 0.0, 0.0, 6.1, -0.8949771722166545, 20.0, 19.83321818146609, 22.7, 1.0, 20.654541527483246], 
processed observation next is [1.0, 0.782608695652174, 0.36153846153846153, 0.89, 0.17045454545454544, 0.5694444444444444, 0.0, 0.0, 0.6016666666666667, 0.20167427592778184, 0.0, 0.26188831163801296, 0.6714285714285714, 1.0, 0.24299460620568525], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.35556513], dtype=float32), 1.4156748]. 
=============================================
[2019-05-04 21:55:03,709] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.56687617e-05 9.99525547e-01 7.11947726e-14 4.33264796e-12
 1.53555519e-16 8.52862826e-20 1.28825326e-14 7.69467970e-16
 1.60149307e-11 1.95463304e-04 2.53364298e-04], sum to 1.0000
[2019-05-04 21:55:03,713] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0409
[2019-05-04 21:55:03,735] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.575, 74.5, 7.325, 120.0, 0.0, 0.0, 12.7, -0.4113614953300276, 20.0, 21.51073691599026, 21.5, 0.0, 3.444867277932817], 
current ob forecast is [], 
actual action is [12.575, 20.0], 
sim time this is 1628100.0000, 
sim time next is 1629000.0000, 
raw observation next is [7.45, 75.0, 7.45, 120.0, 0.0, 0.0, 12.575, -0.4180610582147157, 20.0, 21.47686176340635, 21.5, 0.0, 3.6465833246416537], 
processed observation next is [1.0, 0.8695652173913043, 0.5243589743589744, 0.75, 0.6772727272727272, 0.3333333333333333, 0.0, 0.0, 0.7095833333333333, 0.3606463139284281, 0.0, 0.49669453762947874, 0.5, 0.0, 0.04290098028990181], 
reward next is 0.9071, 
noisyNet noise sample is [array([-0.43996078], dtype=float32), 0.8354613]. 
=============================================
[2019-05-04 21:55:03,771] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[89.49194 ]
 [89.70821 ]
 [89.435844]
 [89.109795]
 [88.74648 ]], R is [[88.23800659]
 [88.26509857]
 [88.29439545]
 [88.32563019]
 [88.35834503]].
[2019-05-04 21:55:09,405] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.1262869e-05 9.9240839e-01 6.2342811e-09 6.4571442e-09 2.6673761e-12
 4.5187514e-14 2.9238081e-11 2.4764012e-11 7.0435959e-08 3.2117055e-03
 4.3385220e-03], sum to 1.0000
[2019-05-04 21:55:09,407] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2963
[2019-05-04 21:55:09,514] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.2, 82.0, 8.2, 110.0, 0.0, 0.0, 12.05, -0.4798918702288283, 20.0, 21.34628927473904, 21.5, 0.0, 3.606077159187383], 
current ob forecast is [], 
actual action is [12.2, 20.0], 
sim time this is 1638000.0000, 
sim time next is 1638900.0000, 
raw observation next is [7.2, 83.0, 7.174999999999999, 107.5, 0.0, 0.0, 12.2, -0.4909707304471617, 20.0, 21.30602050420482, 21.5, 0.0, 3.2941188772046908], 
processed observation next is [1.0, 1.0, 0.5179487179487179, 0.83, 0.6522727272727272, 0.2986111111111111, 0.0, 0.0, 0.7033333333333334, 0.3363430898509461, 0.0, 0.47228864345783145, 0.5, 0.0, 0.03875433973181989], 
reward next is 0.9112, 
noisyNet noise sample is [array([0.56213075], dtype=float32), -0.22131081]. 
=============================================
[2019-05-04 21:55:09,929] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3782708e-06 9.9949265e-01 7.4313061e-10 6.2101663e-10 1.1326854e-12
 7.0193372e-16 2.6370711e-12 2.4380270e-12 8.0794408e-08 2.0690935e-04
 2.9695718e-04], sum to 1.0000
[2019-05-04 21:55:09,932] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7163
[2019-05-04 21:55:09,959] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 8.2, 240.0, 0.0, 0.0, 5.5, -0.7397342557561721, 20.0, 20.21117658181693, 21.5, 0.0, 8.366669904430973], 
current ob forecast is [], 
actual action is [5.5, 20.0], 
sim time this is 1720800.0000, 
sim time next is 1721700.0000, 
raw observation next is [0.375, 92.75, 8.325, 240.0, 0.0, 0.0, 5.5, -0.7933037117528411, 20.0, 20.11458675229439, 21.5, 0.0, 8.509468525986065], 
processed observation next is [1.0, 0.9565217391304348, 0.34294871794871795, 0.9275, 0.7568181818181817, 0.6666666666666666, 0.0, 0.0, 0.5916666666666667, 0.23556542941571965, 0.0, 0.30208382175634135, 0.5, 0.0, 0.10011139442336547], 
reward next is 0.8499, 
noisyNet noise sample is [array([-1.4232441], dtype=float32), -0.022350252]. 
=============================================
[2019-05-04 21:55:10,620] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.5906552e-05 9.9394083e-01 7.3123259e-09 5.9709588e-09 1.1251870e-12
 1.6175851e-13 6.7600777e-12 1.7842811e-10 1.5470876e-07 1.3018308e-03
 4.7012358e-03], sum to 1.0000
[2019-05-04 21:55:10,623] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8272
[2019-05-04 21:55:10,645] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.9500000000000001, 89.0, 7.7, 240.0, 0.0, 0.0, 6.1, -0.8441306885494755, 20.0, 19.68063340182842, 21.5, 0.0, 8.453753218093409], 
current ob forecast is [], 
actual action is [5.95, 20.0], 
sim time this is 1714500.0000, 
sim time next is 1715400.0000, 
raw observation next is [0.8, 90.0, 7.7, 240.0, 0.0, 0.0, 5.95, -0.8539064501957508, 20.0, 19.63947842983941, 21.5, 0.0, 8.503224875798216], 
processed observation next is [1.0, 0.8695652173913043, 0.35384615384615387, 0.9, 0.7000000000000001, 0.6666666666666666, 0.0, 0.0, 0.5991666666666667, 0.2153645166014164, 0.0, 0.23421120426277295, 0.5, 0.0, 0.10003793971527312], 
reward next is 0.8500, 
noisyNet noise sample is [array([1.222906], dtype=float32), -0.46616018]. 
=============================================
[2019-05-04 21:55:17,458] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 31570: loss 0.4799
[2019-05-04 21:55:17,458] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 31570: learning rate 0.0001
[2019-05-04 21:55:17,771] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 31615: loss -2.9742
[2019-05-04 21:55:17,772] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 31615: learning rate 0.0001
[2019-05-04 21:55:18,248] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2000, global step 31702: loss 3.3043
[2019-05-04 21:55:18,249] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2000, global step 31702: learning rate 0.0001
[2019-05-04 21:55:18,819] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31829: loss 4.1519
[2019-05-04 21:55:18,842] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31834: learning rate 0.0001
[2019-05-04 21:55:18,978] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31877: loss 3.1786
[2019-05-04 21:55:18,985] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31878: learning rate 0.0001
[2019-05-04 21:55:18,989] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31882: loss 2.1265
[2019-05-04 21:55:18,990] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31882: learning rate 0.0001
[2019-05-04 21:55:19,105] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31920: loss 1.6105
[2019-05-04 21:55:19,108] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31921: learning rate 0.0001
[2019-05-04 21:55:19,126] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31932: loss 2.2151
[2019-05-04 21:55:19,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31932: learning rate 0.0001
[2019-05-04 21:55:19,167] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31947: loss 3.6013
[2019-05-04 21:55:19,169] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31948: learning rate 0.0001
[2019-05-04 21:55:19,199] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31960: loss 2.5851
[2019-05-04 21:55:19,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31961: learning rate 0.0001
[2019-05-04 21:55:19,443] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32074: loss 0.7636
[2019-05-04 21:55:19,443] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32074: loss 0.8482
[2019-05-04 21:55:19,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32075: learning rate 0.0001
[2019-05-04 21:55:19,446] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32077: learning rate 0.0001
[2019-05-04 21:55:20,244] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 32441: loss 8.0362
[2019-05-04 21:55:20,245] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 32442: learning rate 0.0001
[2019-05-04 21:55:20,567] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32546: loss 42.7419
[2019-05-04 21:55:20,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32546: learning rate 0.0001
[2019-05-04 21:55:21,680] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32715: loss 12.1730
[2019-05-04 21:55:21,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32715: learning rate 0.0001
[2019-05-04 21:55:22,508] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32825: loss 0.3709
[2019-05-04 21:55:22,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32825: learning rate 0.0001
[2019-05-04 21:55:24,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3016941e-04 9.5887184e-02 1.9723755e-08 1.8130423e-08 4.2032583e-10
 3.7772726e-11 1.1480527e-07 3.1379396e-10 7.2390003e-06 4.4015594e-02
 8.5995972e-01], sum to 1.0000
[2019-05-04 21:55:24,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0549
[2019-05-04 21:55:25,127] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 71.0, 7.7, 240.0, 178.0, 62.0, 0.5, -0.6448053665027634, 65.0, 21.74816760255584, 22.7, 1.0, 69.92732243465248], 
current ob forecast is [], 
actual action is [0.5, 20.0], 
sim time this is 1864800.0000, 
sim time next is 1865700.0000, 
raw observation next is [-4.5, 74.0, 7.300000000000001, 240.0, 182.0, 73.0, 0.5, -0.6540394436118538, 20.0, 21.78970322893532, 22.7, 1.0, 46.07055074503878], 
processed observation next is [0.0, 0.6086956521739131, 0.21794871794871795, 0.74, 0.6636363636363637, 0.6666666666666666, 0.48148148148148145, 0.073, 0.5083333333333333, 0.28198685212938207, 0.0, 0.5413861755621886, 0.6714285714285714, 1.0, 0.5420064793533974], 
reward next is 0.2708, 
noisyNet noise sample is [array([-1.6068461], dtype=float32), -0.081658006]. 
=============================================
[2019-05-04 21:55:28,251] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2476282e-03 9.6341264e-01 2.0150696e-06 2.5438856e-06 5.1758651e-08
 1.3097573e-08 7.8280191e-06 2.0265276e-07 5.4362721e-05 2.7996708e-02
 5.2760104e-03], sum to 1.0000
[2019-05-04 21:55:28,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4356
[2019-05-04 21:55:28,286] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.9, 79.0, 6.6, 250.0, 0.0, 0.0, -0.7499999999999991, -0.6446382176261938, 20.0, 21.60988863737585, 21.5, 0.0, 29.60370264492713], 
current ob forecast is [], 
actual action is [-0.9000000000000004, 20.0], 
sim time this is 1891800.0000, 
sim time next is 1892700.0000, 
raw observation next is [-6.050000000000001, 77.0, 6.6, 255.0, 0.0, 0.0, -0.9000000000000004, -0.6786301247753447, 20.0, 21.45132391770251, 21.5, 0.0, 24.587389587438334], 
processed observation next is [0.0, 0.9130434782608695, 0.17820512820512818, 0.77, 0.6, 0.7083333333333334, 0.0, 0.0, 0.48500000000000004, 0.27378995840821846, 0.0, 0.4930462739575014, 0.5, 0.0, 0.28926340691103924], 
reward next is 0.6607, 
noisyNet noise sample is [array([-0.5053542], dtype=float32), -0.47437358]. 
=============================================
[2019-05-04 21:55:33,910] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.37380993e-10 1.06511805e-04 1.09409004e-17 4.68535163e-16
 1.27717901e-19 1.66385587e-20 8.85295702e-16 1.16685799e-18
 2.98074261e-12 3.05667090e-05 9.99862909e-01], sum to 1.0000
[2019-05-04 21:55:33,915] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5359
[2019-05-04 21:55:34,049] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 79.0, 4.1, 210.0, 128.0, 392.5, -2.699999999999999, -0.7119279279617626, 65.0, 21.4123976341988, 22.7, 1.0, 67.69867866683843], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 1936800.0000, 
sim time next is 1937700.0000, 
raw observation next is [-6.875, 78.0, 4.35, 212.5, 149.5, 314.25, -2.3, -0.6793394896204044, 65.0, 21.56676347128917, 22.7, 1.0, 67.21085310111432], 
processed observation next is [1.0, 0.43478260869565216, 0.15705128205128205, 0.78, 0.39545454545454545, 0.5902777777777778, 0.3955026455026455, 0.31425, 0.46166666666666667, 0.27355350345986523, 1.0, 0.5095376387555959, 0.6714285714285714, 1.0, 0.790715918836639], 
reward next is 0.3849, 
noisyNet noise sample is [array([-0.9344857], dtype=float32), 1.2456814]. 
=============================================
[2019-05-04 21:55:37,799] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1655730e-02 8.8788307e-01 2.3193154e-05 3.9851300e-05 1.3146083e-06
 9.0388681e-07 2.2529863e-05 3.3185720e-06 1.4391814e-03 4.0660268e-03
 9.4864897e-02], sum to 1.0000
[2019-05-04 21:55:37,800] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6752
[2019-05-04 21:55:37,877] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.45, 77.0, 3.85, 255.0, 0.0, 0.0, 0.8250000000000002, -0.3080121497527052, 20.0, 22.90191562030421, 22.7, 1.0, 24.00325222186478], 
current ob forecast is [], 
actual action is [0.5499999999999998, 20.0], 
sim time this is 1963800.0000, 
sim time next is 1964700.0000, 
raw observation next is [-4.725, 78.0, 3.725, 252.5, 0.0, 0.0, 0.5499999999999998, -0.3197801662554351, 20.0, 22.66862756226762, 22.7, 1.0, 19.20496341523921], 
processed observation next is [1.0, 0.7391304347826086, 0.21217948717948718, 0.78, 0.3386363636363636, 0.7013888888888888, 0.0, 0.0, 0.5091666666666667, 0.39340661124818826, 0.0, 0.66694679460966, 0.6714285714285714, 1.0, 0.22594074606163775], 
reward next is 0.7241, 
noisyNet noise sample is [array([1.6499829], dtype=float32), 1.4965966]. 
=============================================
[2019-05-04 21:55:38,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1182935e-06 9.9937856e-01 8.3865250e-13 4.8598712e-13 1.8193780e-17
 4.1323776e-17 1.0960442e-11 1.5575521e-15 8.7846797e-09 7.9653088e-05
 5.4064026e-04], sum to 1.0000
[2019-05-04 21:55:38,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8454
[2019-05-04 21:55:38,856] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.050000000000001, 86.0, 2.775, 260.0, 0.0, 0.0, -1.2, -0.8255425409751341, 20.0, 20.56897129370089, 21.5, 0.0, 8.174646430573803], 
current ob forecast is [], 
actual action is [-1.0500000000000007, 20.0], 
sim time this is 1991700.0000, 
sim time next is 1992600.0000, 
raw observation next is [-5.9, 85.0, 3.05, 260.0, 0.0, 0.0, -1.050000000000001, -0.8444945185990553, 20.0, 20.48789465673998, 21.5, 0.0, 8.340551406047071], 
processed observation next is [1.0, 0.043478260869565216, 0.18205128205128204, 0.85, 0.2772727272727273, 0.7222222222222222, 0.0, 0.0, 0.4825, 0.2185018271336482, 0.0, 0.35541352239142554, 0.5, 0.0, 0.09812413418878907], 
reward next is 0.8519, 
noisyNet noise sample is [array([0.3316633], dtype=float32), 0.32912138]. 
=============================================
[2019-05-04 21:55:48,332] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.25326627e-03 3.23747993e-01 1.35223945e-05 1.18045809e-05
 8.08215987e-07 4.82125245e-07 5.32426166e-05 1.87922296e-06
 8.37427913e-04 2.78360695e-02 6.38243437e-01], sum to 1.0000
[2019-05-04 21:55:48,333] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6245
[2019-05-04 21:55:48,355] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.749999999999999, 90.0, 4.975, 240.0, 0.0, 0.0, -0.5999999999999996, -1.179667152072335, 20.0, 18.88860970305746, 21.5, 0.0, 42.48668386046477], 
current ob forecast is [], 
actual action is [-0.7499999999999991, 65.0], 
sim time this is 2088900.0000, 
sim time next is 2089800.0000, 
raw observation next is [-5.9, 89.0, 4.85, 240.0, 0.0, 0.0, -0.7499999999999991, -1.160350332201939, 65.0, 18.89459578140625, 21.5, 0.0, 62.51607446108299], 
processed observation next is [1.0, 0.17391304347826086, 0.18205128205128204, 0.89, 0.44090909090909086, 0.6666666666666666, 0.0, 0.0, 0.4875, 0.11321655593268698, 1.0, 0.12779939734374984, 0.5, 0.0, 0.7354832289539175], 
reward next is 0.1727, 
noisyNet noise sample is [array([1.3889898], dtype=float32), 0.73124045]. 
=============================================
[2019-05-04 21:55:51,707] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6999403e-04 8.7095708e-02 5.8243760e-08 2.3184477e-08 2.2628215e-10
 1.8687739e-10 3.6421746e-08 1.4787906e-09 2.3036626e-05 4.1952836e-03
 9.0831578e-01], sum to 1.0000
[2019-05-04 21:55:51,708] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4927
[2019-05-04 21:55:52,004] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 75.0, 5.1, 250.0, 250.5, 80.5, -2.425, -0.6274367376821287, 20.0, 21.74065345238125, 22.7, 1.0, 41.11739878898057], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 2113200.0000, 
sim time next is 2114100.0000, 
raw observation next is [-7.149999999999999, 72.25, 4.85, 252.5, 271.75, 90.75, -2.3, -0.5850676682499837, 65.0, 21.80965549766829, 22.7, 1.0, 66.51853956215092], 
processed observation next is [1.0, 0.4782608695652174, 0.15000000000000005, 0.7225, 0.44090909090909086, 0.7013888888888888, 0.718915343915344, 0.09075, 0.46166666666666667, 0.3049774439166721, 1.0, 0.5442364996668988, 0.6714285714285714, 1.0, 0.7825710536723638], 
reward next is 0.4246, 
noisyNet noise sample is [array([-1.72261], dtype=float32), -0.18156935]. 
=============================================
[2019-05-04 21:55:58,317] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0070836e-06 9.8798728e-01 3.2845704e-10 2.9500527e-11 4.3717796e-14
 5.9815231e-14 1.7339340e-09 7.8372199e-13 1.3907263e-07 2.0109817e-04
 1.1807429e-02], sum to 1.0000
[2019-05-04 21:55:58,326] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6403
[2019-05-04 21:55:58,361] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.7, 78.0, 3.0, 257.5, 0.0, 0.0, -1.7, -1.146604127762209, 20.0, 19.1358986483617, 21.5, 0.0, 8.73075391854402], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 20.0], 
sim time this is 2173500.0000, 
sim time next is 2174400.0000, 
raw observation next is [-6.7, 78.0, 3.0, 260.0, 0.0, 0.0, -1.7, -1.16517442564988, 20.0, 19.05682775554401, 21.5, 0.0, 8.931402437068423], 
processed observation next is [1.0, 0.17391304347826086, 0.16153846153846155, 0.78, 0.2727272727272727, 0.7222222222222222, 0.0, 0.0, 0.4716666666666667, 0.11160852478337335, 0.0, 0.15097539364914425, 0.5, 0.0, 0.10507532278904026], 
reward next is 0.8449, 
noisyNet noise sample is [array([2.4871726], dtype=float32), 1.3030202]. 
=============================================
[2019-05-04 21:56:02,572] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 39305: loss -0.9237
[2019-05-04 21:56:02,574] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 39305: learning rate 0.0001
[2019-05-04 21:56:04,220] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 39562: loss 0.2904
[2019-05-04 21:56:04,222] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 39562: learning rate 0.0001
[2019-05-04 21:56:04,287] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2500, global step 39573: loss 0.3552
[2019-05-04 21:56:04,289] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2500, global step 39575: learning rate 0.0001
[2019-05-04 21:56:05,877] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39796: loss 0.3552
[2019-05-04 21:56:05,877] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39796: learning rate 0.0001
[2019-05-04 21:56:06,368] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39866: loss 1.1709
[2019-05-04 21:56:06,368] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39866: learning rate 0.0001
[2019-05-04 21:56:06,638] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39911: loss -0.8466
[2019-05-04 21:56:06,639] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39911: learning rate 0.0001
[2019-05-04 21:56:07,233] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40050: loss 2.5591
[2019-05-04 21:56:07,234] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40050: loss 1.6640
[2019-05-04 21:56:07,235] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40050: learning rate 0.0001
[2019-05-04 21:56:07,239] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40050: learning rate 0.0001
[2019-05-04 21:56:07,357] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40085: loss 1.3002
[2019-05-04 21:56:07,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40085: learning rate 0.0001
[2019-05-04 21:56:07,661] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40174: loss -0.6419
[2019-05-04 21:56:07,661] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40174: learning rate 0.0001
[2019-05-04 21:56:07,713] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40190: loss 0.1161
[2019-05-04 21:56:07,716] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40193: learning rate 0.0001
[2019-05-04 21:56:07,911] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40255: loss 0.1291
[2019-05-04 21:56:07,911] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40255: learning rate 0.0001
[2019-05-04 21:56:07,956] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 40265: loss 1.1974
[2019-05-04 21:56:07,956] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40265: loss 0.1672
[2019-05-04 21:56:07,958] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 40265: learning rate 0.0001
[2019-05-04 21:56:07,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40267: learning rate 0.0001
[2019-05-04 21:56:08,134] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40324: loss 0.1612
[2019-05-04 21:56:08,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40324: learning rate 0.0001
[2019-05-04 21:56:08,348] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 40379: loss 0.7775
[2019-05-04 21:56:08,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 40382: learning rate 0.0001
[2019-05-04 21:56:18,128] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1288533e-07 9.8669434e-01 7.0196057e-13 2.9341977e-14 4.9044104e-17
 3.5912930e-17 6.8054295e-12 4.7047552e-17 1.3607226e-08 2.8593777e-06
 1.3302429e-02], sum to 1.0000
[2019-05-04 21:56:18,131] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1075
[2019-05-04 21:56:18,152] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.425, 62.75, 4.225, 82.5, 0.0, 0.0, 2.7, -0.9166838392966858, 20.0, 20.09692695269322, 21.5, 0.0, 6.056435971158216], 
current ob forecast is [], 
actual action is [2.575, 20.0], 
sim time this is 2344500.0000, 
sim time next is 2345400.0000, 
raw observation next is [-2.55, 63.5, 4.35, 85.0, 0.0, 0.0, 2.575, -0.9381881569623078, 20.0, 20.00504863564819, 21.5, 0.0, 5.402160055057334], 
processed observation next is [0.0, 0.13043478260869565, 0.26794871794871794, 0.635, 0.39545454545454545, 0.2361111111111111, 0.0, 0.0, 0.5429166666666667, 0.1872706143458974, 0.0, 0.2864355193783129, 0.5, 0.0, 0.06355482417714511], 
reward next is 0.8864, 
noisyNet noise sample is [array([0.6301043], dtype=float32), -0.39667648]. 
=============================================
[2019-05-04 21:56:19,450] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9505460e-07 7.7490765e-01 3.2071681e-13 8.2876758e-16 8.9315757e-20
 7.3385667e-20 1.2612913e-13 7.6273259e-19 2.4267091e-09 1.6550415e-06
 2.2509050e-01], sum to 1.0000
[2019-05-04 21:56:19,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2699
[2019-05-04 21:56:19,461] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 62.0, 3.975, 80.0, 0.0, 0.0, 2.7, -0.760321285110415, 20.0, 20.75357012964462, 21.5, 0.0, 4.282614813473017], 
current ob forecast is [], 
actual action is [2.7, 20.0], 
sim time this is 2342700.0000, 
sim time next is 2343600.0000, 
raw observation next is [-2.3, 62.0, 4.1, 80.0, 0.0, 0.0, 2.7, -0.7806970962414003, 20.0, 20.67002104210226, 21.5, 0.0, 4.107311259814024], 
processed observation next is [0.0, 0.13043478260869565, 0.27435897435897433, 0.62, 0.3727272727272727, 0.2222222222222222, 0.0, 0.0, 0.545, 0.2397676345861999, 0.0, 0.38143157744317996, 0.5, 0.0, 0.04832130893898851], 
reward next is 0.9017, 
noisyNet noise sample is [array([-0.45520234], dtype=float32), -1.0027946]. 
=============================================
[2019-05-04 21:56:23,227] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.1700721e-06 9.5559996e-01 5.5246665e-11 1.0097250e-11 4.3997945e-15
 5.1571626e-15 6.5214417e-11 2.5617913e-14 6.7565355e-07 1.8649790e-04
 4.4206765e-02], sum to 1.0000
[2019-05-04 21:56:23,227] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6997
[2019-05-04 21:56:23,274] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.75, 50.5, 4.6, 75.0, 0.0, 0.0, -1.475000000000001, -1.069530525660383, 20.0, 19.69279582822745, 21.5, 0.0, 10.329549265341347], 
current ob forecast is [], 
actual action is [-1.75, 20.0], 
sim time this is 2424600.0000, 
sim time next is 2425500.0000, 
raw observation next is [-7.024999999999999, 51.75, 4.35, 77.5, 0.0, 0.0, -1.75, -1.089989814898186, 20.0, 19.60383028007877, 21.5, 0.0, 10.483229190929594], 
processed observation next is [0.0, 0.043478260869565216, 0.15320512820512824, 0.5175, 0.39545454545454545, 0.2152777777777778, 0.0, 0.0, 0.4708333333333333, 0.1366700617006047, 0.0, 0.22911861143982445, 0.5, 0.0, 0.12333210812858346], 
reward next is 0.8267, 
noisyNet noise sample is [array([-0.6711906], dtype=float32), -0.48107353]. 
=============================================
[2019-05-04 21:56:23,317] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[53.09002 ]
 [53.363438]
 [53.591366]
 [53.734577]
 [53.84421 ]], R is [[53.07901382]
 [53.37670135]
 [53.6734314 ]
 [53.96941757]
 [54.26476669]].
[2019-05-04 21:56:26,127] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2693426e-07 9.4882649e-01 1.9644035e-12 1.4336530e-13 2.8500459e-16
 1.7128339e-16 1.4910570e-12 1.6038681e-16 2.2461096e-07 1.8113400e-05
 5.1154237e-02], sum to 1.0000
[2019-05-04 21:56:26,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7191
[2019-05-04 21:56:26,141] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 42.0, 5.6, 70.0, 0.0, 0.0, 1.6, -0.7822008982960459, 20.0, 20.88212103738753, 21.5, 0.0, 7.219780131724178], 
current ob forecast is [], 
actual action is [1.6, 20.0], 
sim time this is 2408400.0000, 
sim time next is 2409300.0000, 
raw observation next is [-3.675, 42.5, 5.475, 70.0, 0.0, 0.0, 1.6, -0.8055709865698718, 20.0, 20.79031934233085, 21.5, 0.0, 6.452138747998748], 
processed observation next is [0.0, 0.9130434782608695, 0.23910256410256409, 0.425, 0.4977272727272727, 0.19444444444444445, 0.0, 0.0, 0.5266666666666667, 0.23147633781004273, 0.0, 0.39861704890440713, 0.5, 0.0, 0.07590751468233821], 
reward next is 0.8741, 
noisyNet noise sample is [array([1.729369], dtype=float32), 0.22287303]. 
=============================================
[2019-05-04 21:56:28,312] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8129464e-06 1.1243661e-01 1.3565344e-12 2.2276001e-14 2.2539538e-17
 1.2301417e-17 3.9662934e-12 1.1286736e-17 1.2605018e-08 4.4731502e-07
 8.8756108e-01], sum to 1.0000
[2019-05-04 21:56:28,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0974
[2019-05-04 21:56:28,438] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.04999999999999993, 30.25, 4.475, 62.5, 89.0, 841.0, 4.4, -0.762200737593043, 65.0, 21.24143001183747, 22.7, 1.0, 65.66379675553578], 
current ob forecast is [], 
actual action is [4.95, 20.0], 
sim time this is 2463300.0000, 
sim time next is 2464200.0000, 
raw observation next is [0.5, 29.5, 4.35, 65.0, 90.0, 845.0, 4.95, -0.7616120055604677, 20.0, 21.29242421798291, 22.7, 1.0, 42.83750376196442], 
processed observation next is [0.0, 0.5217391304347826, 0.34615384615384615, 0.295, 0.39545454545454545, 0.18055555555555555, 0.23809523809523808, 0.845, 0.5825, 0.2461293314798441, 0.0, 0.47034631685470124, 0.6714285714285714, 1.0, 0.5039706324936991], 
reward next is 0.0059, 
noisyNet noise sample is [array([1.4805582], dtype=float32), -2.076099]. 
=============================================
[2019-05-04 21:56:32,261] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3920321e-05 9.8286957e-01 7.6937301e-10 4.9689305e-11 1.2764598e-12
 2.2251420e-13 6.6283157e-09 2.4758533e-12 8.8914243e-07 2.3970530e-05
 1.7051620e-02], sum to 1.0000
[2019-05-04 21:56:32,279] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8385
[2019-05-04 21:56:32,302] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.7, 41.0, 2.0, 60.0, 0.0, 0.0, 3.3, -0.5349824852347079, 20.0, 22.07106329096639, 21.5, 0.0, 18.32456159810586], 
current ob forecast is [], 
actual action is [3.3, 20.0], 
sim time this is 2514600.0000, 
sim time next is 2515500.0000, 
raw observation next is [-1.7, 42.5, 2.0, 60.0, 0.0, 0.0, 3.3, -0.5640186591511736, 20.0, 21.92860451787423, 21.5, 0.0, 15.078316349367222], 
processed observation next is [1.0, 0.08695652173913043, 0.28974358974358977, 0.425, 0.18181818181818182, 0.16666666666666666, 0.0, 0.0, 0.5549999999999999, 0.31199378028294217, 0.0, 0.5612292168391758, 0.5, 0.0, 0.17739195705137908], 
reward next is 0.7726, 
noisyNet noise sample is [array([0.39651325], dtype=float32), 0.09625963]. 
=============================================
[2019-05-04 21:56:32,369] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[44.33928 ]
 [44.308784]
 [43.748066]
 [44.504112]
 [44.87958 ]], R is [[44.69208527]
 [44.97957993]
 [45.21746445]
 [45.39575577]
 [45.46250534]].
[2019-05-04 21:56:36,882] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7125343e-07 4.9373798e-02 1.0965926e-11 1.9838346e-13 7.7435916e-16
 7.5723247e-16 5.9585579e-12 6.0672690e-15 7.1196875e-09 3.5418441e-05
 9.5059055e-01], sum to 1.0000
[2019-05-04 21:56:36,882] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6192
[2019-05-04 21:56:37,064] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 49.0, 3.0, 360.0, 134.5, 39.0, 3.4, -0.6125777234900371, 20.0, 22.21677563892528, 22.7, 1.0, 45.82609879708475], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 2541600.0000, 
sim time next is 2542500.0000, 
raw observation next is [-1.05, 48.5, 3.15, 272.5, 133.75, 42.0, 3.8, -0.578562411046964, 65.0, 22.22716919475415, 22.7, 1.0, 68.86171324393315], 
processed observation next is [1.0, 0.43478260869565216, 0.3064102564102564, 0.485, 0.2863636363636364, 0.7569444444444444, 0.35383597883597884, 0.042, 0.5633333333333332, 0.3071458629843453, 1.0, 0.6038813135363073, 0.6714285714285714, 1.0, 0.8101378028698017], 
reward next is 0.3721, 
noisyNet noise sample is [array([-1.2156583], dtype=float32), 0.45729557]. 
=============================================
[2019-05-04 21:56:37,079] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[55.24721]
 [53.98212]
 [55.40713]
 [56.30596]
 [56.37671]], R is [[55.87422562]
 [55.75187302]
 [55.70814514]
 [55.95849609]
 [56.31518555]].
[2019-05-04 21:56:37,403] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9981537e-07 9.8528808e-01 2.0140212e-11 1.4746442e-12 6.5172028e-16
 1.3243285e-15 5.9322984e-11 1.2308792e-14 1.7972573e-09 4.8209131e-06
 1.4706813e-02], sum to 1.0000
[2019-05-04 21:56:37,405] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8045
[2019-05-04 21:56:37,437] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 71.0, 3.6, 265.0, 0.0, 0.0, 0.0, -0.6998740210791147, 20.0, 21.06400993578015, 21.5, 0.0, 40.05014882634661], 
current ob forecast is [], 
actual action is [0.0, 20.0], 
sim time this is 2597400.0000, 
sim time next is 2598300.0000, 
raw observation next is [-5.0, 72.5, 3.6, 262.5, 0.0, 0.0, 0.0, -0.7408371858087227, 20.0, 20.87097875804614, 21.5, 0.0, 31.15262468717691], 
processed observation next is [1.0, 0.043478260869565216, 0.20512820512820512, 0.725, 0.32727272727272727, 0.7291666666666666, 0.0, 0.0, 0.5, 0.2530542713970924, 0.0, 0.41013982257801984, 0.5, 0.0, 0.36650146690796365], 
reward next is 0.5835, 
noisyNet noise sample is [array([-1.0595899], dtype=float32), -1.3796664]. 
=============================================
[2019-05-04 21:56:40,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.26258977e-11 9.99998808e-01 8.04055131e-16 2.47810635e-18
 2.25252929e-20 2.20935554e-22 1.25450439e-15 3.05817106e-20
 1.19623495e-11 6.94544111e-09 1.14537295e-06], sum to 1.0000
[2019-05-04 21:56:40,705] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1742
[2019-05-04 21:56:40,722] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 56.0, 3.6, 290.0, 0.0, 0.0, 2.2, -0.6800931394551393, 20.0, 20.97213316180728, 21.5, 0.0, 7.48216904428088], 
current ob forecast is [], 
actual action is [2.2, 20.0], 
sim time this is 2584800.0000, 
sim time next is 2585700.0000, 
raw observation next is [-3.075, 56.75, 3.725, 287.5, 0.0, 0.0, 2.2, -0.7394962409270448, 20.0, 20.85162784700542, 21.5, 0.0, 7.710503854392274], 
processed observation next is [1.0, 0.9565217391304348, 0.2544871794871795, 0.5675, 0.3386363636363636, 0.7986111111111112, 0.0, 0.0, 0.5366666666666667, 0.2535012530243184, 0.0, 0.4073754067150602, 0.5, 0.0, 0.09071181005167382], 
reward next is 0.8593, 
noisyNet noise sample is [array([1.2268896], dtype=float32), -1.967673]. 
=============================================
[2019-05-04 21:56:47,754] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 47173: loss 1.9654
[2019-05-04 21:56:47,777] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 47179: learning rate 0.0001
[2019-05-04 21:56:47,880] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 47202: loss 1.7491
[2019-05-04 21:56:47,884] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 47202: learning rate 0.0001
[2019-05-04 21:56:48,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0182904e-07 8.1151885e-01 5.9538355e-12 9.6495416e-14 6.7814735e-17
 1.3361520e-16 4.4161680e-12 6.7567355e-17 6.0107475e-09 2.9884699e-05
 1.8845117e-01], sum to 1.0000
[2019-05-04 21:56:48,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7087
[2019-05-04 21:56:48,449] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.04999999999999999, 52.0, 5.85, 235.0, 7.0, 82.0, 5.225, -0.5700578827657513, 20.0, 21.58093765122715, 22.7, 1.0, 5.9043301248732485], 
current ob forecast is [], 
actual action is [4.95, 20.0], 
sim time this is 2655000.0000, 
sim time next is 2655900.0000, 
raw observation next is [-0.325, 53.0, 5.475, 232.5, 0.0, 0.0, 4.95, -0.5590901933961049, 20.0, 21.5015951335138, 22.7, 1.0, 5.280350741726088], 
processed observation next is [1.0, 0.7391304347826086, 0.325, 0.53, 0.4977272727272727, 0.6458333333333334, 0.0, 0.0, 0.5825, 0.31363660220129835, 0.0, 0.500227876216257, 0.6714285714285714, 1.0, 0.06212177343207163], 
reward next is 0.8661, 
noisyNet noise sample is [array([0.7455963], dtype=float32), -0.11325985]. 
=============================================
[2019-05-04 21:56:49,963] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3000, global step 47623: loss -0.1918
[2019-05-04 21:56:49,964] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3000, global step 47623: learning rate 0.0001
[2019-05-04 21:56:50,426] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47734: loss -0.6349
[2019-05-04 21:56:50,430] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47734: learning rate 0.0001
[2019-05-04 21:56:50,496] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47754: loss 1.9124
[2019-05-04 21:56:50,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47754: learning rate 0.0001
[2019-05-04 21:56:50,591] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47782: loss 1.9511
[2019-05-04 21:56:50,595] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47783: learning rate 0.0001
[2019-05-04 21:56:50,722] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0604528e-05 9.9661344e-01 1.7118055e-08 4.4591197e-09 4.6200613e-11
 1.4843581e-11 2.8815032e-08 9.6298421e-11 2.4090480e-07 3.1329892e-04
 3.0623057e-03], sum to 1.0000
[2019-05-04 21:56:50,722] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1183
[2019-05-04 21:56:50,831] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.5, 64.0, 2.1, 65.0, 112.0, 781.0, -3.75, -0.5506992069036709, 20.0, 22.03613347794701, 22.7, 1.0, 23.909966917279295], 
current ob forecast is [], 
actual action is [-3.5, 20.0], 
sim time this is 2719800.0000, 
sim time next is 2720700.0000, 
raw observation next is [-8.25, 64.0, 2.1, 67.5, 112.25, 785.5, -3.5, -0.5913987377455575, 20.0, 21.85522781888598, 22.7, 1.0, 18.253458928081017], 
processed observation next is [1.0, 0.4782608695652174, 0.12179487179487179, 0.64, 0.19090909090909092, 0.1875, 0.296957671957672, 0.7855, 0.44166666666666665, 0.3028670874181475, 0.0, 0.5507468312694256, 0.6714285714285714, 1.0, 0.21474657562448254], 
reward next is 0.6474, 
noisyNet noise sample is [array([0.14175744], dtype=float32), 0.3457102]. 
=============================================
[2019-05-04 21:56:50,892] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 47882: loss 5.4149
[2019-05-04 21:56:50,894] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 47882: learning rate 0.0001
[2019-05-04 21:56:50,898] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47884: loss 0.0396
[2019-05-04 21:56:50,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47884: learning rate 0.0001
[2019-05-04 21:56:51,145] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47972: loss 1.7113
[2019-05-04 21:56:51,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47972: learning rate 0.0001
[2019-05-04 21:56:51,384] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48038: loss 1.8392
[2019-05-04 21:56:51,385] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48038: learning rate 0.0001
[2019-05-04 21:56:51,533] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48072: loss 2.3082
[2019-05-04 21:56:51,533] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48072: learning rate 0.0001
[2019-05-04 21:56:51,569] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48083: loss 0.9113
[2019-05-04 21:56:51,569] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48083: learning rate 0.0001
[2019-05-04 21:56:52,035] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48173: loss 0.7649
[2019-05-04 21:56:52,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48173: learning rate 0.0001
[2019-05-04 21:56:52,136] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 48185: loss -0.8197
[2019-05-04 21:56:52,137] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 48185: learning rate 0.0001
[2019-05-04 21:56:52,224] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48197: loss 0.0149
[2019-05-04 21:56:52,225] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48197: learning rate 0.0001
[2019-05-04 21:56:52,731] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 48261: loss -1.6817
[2019-05-04 21:56:52,733] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 48261: learning rate 0.0001
[2019-05-04 21:56:59,133] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6955682e-08 5.0689989e-01 6.8724010e-10 3.4692096e-11 8.4621817e-14
 2.9916394e-14 3.6384842e-10 1.8466812e-14 2.1193825e-08 3.7503114e-06
 4.9309638e-01], sum to 1.0000
[2019-05-04 21:56:59,137] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0335
[2019-05-04 21:56:59,161] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.25, 55.25, 1.5, 262.5, 0.0, 0.0, 1.0, -0.5618357341634971, 20.0, 21.54359110438627, 22.7, 1.0, 15.76098884888494], 
current ob forecast is [], 
actual action is [0.75, 20.0], 
sim time this is 2744100.0000, 
sim time next is 2745000.0000, 
raw observation next is [-4.5, 56.5, 1.5, 185.0, 0.0, 0.0, 0.75, -0.5977654904732911, 20.0, 21.30520271861233, 22.7, 1.0, 16.962558398256352], 
processed observation next is [1.0, 0.782608695652174, 0.21794871794871795, 0.565, 0.13636363636363635, 0.5138888888888888, 0.0, 0.0, 0.5125, 0.300744836508903, 0.0, 0.47217181694461835, 0.6714285714285714, 1.0, 0.19955951056772178], 
reward next is 0.6335, 
noisyNet noise sample is [array([0.7434937], dtype=float32), 0.51667655]. 
=============================================
[2019-05-04 21:56:59,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[49.289642]
 [49.440887]
 [49.349373]
 [50.131615]
 [50.971863]], R is [[49.02389145]
 [49.27542496]
 [49.55858231]
 [49.8627243 ]
 [50.1258316 ]].
[2019-05-04 21:56:59,557] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.1799366e-10 9.9983335e-01 2.8900154e-14 9.7341501e-14 8.1061301e-18
 3.5371590e-19 2.4792814e-13 1.1340920e-16 9.7434699e-11 5.6689796e-08
 1.6658605e-04], sum to 1.0000
[2019-05-04 21:56:59,557] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9984
[2019-05-04 21:56:59,585] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 59.0, 2.35, 52.5, 0.0, 0.0, -1.0, -0.8413846716669412, 20.0, 20.44059486695634, 21.5, 0.0, 12.298986297887408], 
current ob forecast is [], 
actual action is [-1.0, 20.0], 
sim time this is 2758500.0000, 
sim time next is 2759400.0000, 
raw observation next is [-6.0, 59.0, 2.6, 55.0, 0.0, 0.0, -1.0, -0.8661409226441794, 20.0, 20.29981023128524, 21.5, 0.0, 12.095950511680039], 
processed observation next is [1.0, 0.9565217391304348, 0.1794871794871795, 0.59, 0.23636363636363636, 0.1527777777777778, 0.0, 0.0, 0.48333333333333334, 0.21128635911860685, 0.0, 0.32854431875503415, 0.5, 0.0, 0.14230530013741222], 
reward next is 0.8077, 
noisyNet noise sample is [array([-0.01854062], dtype=float32), 1.3472234]. 
=============================================
[2019-05-04 21:57:03,401] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4382065e-08 9.9880588e-01 2.8686166e-12 2.2859789e-12 6.0340741e-17
 3.3188978e-17 2.1218131e-11 4.5852710e-15 2.1889559e-09 3.5726418e-07
 1.1937354e-03], sum to 1.0000
[2019-05-04 21:57:03,401] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5209
[2019-05-04 21:57:03,557] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.5, 51.25, 1.575, 45.0, 161.75, 571.75, 3.0, -0.5977754916851951, 20.0, 21.99965724611415, 22.7, 1.0, 44.258275395785205], 
current ob forecast is [], 
actual action is [3.5, 20.0], 
sim time this is 2803500.0000, 
sim time next is 2804400.0000, 
raw observation next is [-1.0, 50.0, 2.1, 60.0, 149.5, 635.5, 3.5, -0.6184215781712035, 20.0, 21.86223372274421, 22.7, 1.0, 30.756813186901436], 
processed observation next is [1.0, 0.4782608695652174, 0.3076923076923077, 0.5, 0.19090909090909092, 0.16666666666666666, 0.3955026455026455, 0.6355, 0.5583333333333333, 0.29385947394293216, 0.0, 0.5517476746777444, 0.6714285714285714, 1.0, 0.36184486102236985], 
reward next is 0.4814, 
noisyNet noise sample is [array([-0.21173765], dtype=float32), -0.8730194]. 
=============================================
[2019-05-04 21:57:03,994] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-05-04 21:57:03,994] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation job starts!
[2019-05-04 21:57:03,994] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:57:03,996] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res1/Eplus-env-sub_run2
[2019-05-04 21:57:04,019] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation job starts!
[2019-05-04 21:57:04,029] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 21:57:04,032] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Test-Repeat-Real97-v1-res1/Eplus-env-sub_run2
[2019-05-04 21:57:43,831] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 DEBUG:NoisyNet noise sample: [array([4.788948e-09], dtype=float32), 0.04885532]
[2019-05-04 21:57:43,831] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 DEBUG:Observation this: [-6.866666667, 62.7, 0.759968, 278.0, 0.0, 0.0, -1.705555556, 97.85748662075976, 20.0, 11.06407350227254, 21.5, 0.0, 15.123760543372148]
[2019-05-04 21:57:43,831] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 DEBUG:Observation forecast: []
[2019-05-04 21:57:43,832] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Softmax [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], sampled 0.15693395748421546
[2019-05-04 21:58:37,467] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation: average rewards by now are 4595.7612 73651.5758 272758.3191
[2019-05-04 21:58:37,478] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 21:58:37,478] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 21:58:37,592] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 21:58:37,592] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:00:49,921] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation: average rewards by now are 6266.4546 164228.0135 -1600.2915
[2019-05-04 22:00:49,931] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:00:49,931] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:00:50,043] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:00:50,043] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:00:50,934] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 50000, evaluation results [50000.0, 6266.454642784191, 164228.01353417436, -1600.291509062841, 4595.761158783212, 73651.57584369082, 272758.3191094142]
[2019-05-04 22:00:52,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4105461e-09 9.8835981e-01 1.3959679e-12 3.3725383e-13 4.4757337e-17
 9.9507379e-18 3.3090822e-12 2.1113269e-16 1.4907432e-09 1.2555396e-06
 1.1638973e-02], sum to 1.0000
[2019-05-04 22:00:52,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3887
[2019-05-04 22:00:52,334] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.75, 25.5, 4.85, 132.5, 112.25, 0.0, 11.5, -0.658368076975611, 20.0, 21.5361287933345, 22.7, 1.0, 14.536955703451222], 
current ob forecast is [], 
actual action is [11.75, 20.0], 
sim time this is 2817900.0000, 
sim time next is 2818800.0000, 
raw observation next is [7.0, 24.0, 5.1, 140.0, 106.5, 0.0, 11.75, -0.7056588648881492, 20.0, 21.54839278956775, 22.7, 1.0, 12.043409353747107], 
processed observation next is [1.0, 0.6521739130434783, 0.5128205128205128, 0.24, 0.4636363636363636, 0.3888888888888889, 0.28174603174603174, 0.0, 0.6958333333333333, 0.2647803783706169, 0.0, 0.5069132556525356, 0.6714285714285714, 1.0, 0.141687168867613], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2144494], dtype=float32), 0.60988545]. 
=============================================
[2019-05-04 22:00:52,887] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.1367829e-11 9.4193947e-01 1.3494958e-14 7.7572963e-15 4.2525523e-19
 1.7342926e-20 1.1895256e-13 1.3342104e-19 3.7701434e-10 3.1721268e-07
 5.8060128e-02], sum to 1.0000
[2019-05-04 22:00:52,887] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5759
[2019-05-04 22:00:53,042] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.0, 33.5, 4.6, 125.0, 0.0, 0.0, 9.5, -0.6018055970916388, 65.0, 21.03208453960006, 22.7, 1.0, 102.26430754273348], 
current ob forecast is [], 
actual action is [9.0, 20.0], 
sim time this is 2831400.0000, 
sim time next is 2832300.0000, 
raw observation next is [3.5, 35.25, 4.35, 122.5, 0.0, 0.0, 9.0, -0.5609314404262977, 20.0, 21.67693818123848, 22.7, 1.0, 45.60587411301444], 
processed observation next is [1.0, 0.782608695652174, 0.4230769230769231, 0.3525, 0.39545454545454545, 0.3402777777777778, 0.0, 0.0, 0.65, 0.3130228531912341, 0.0, 0.5252768830340686, 0.6714285714285714, 1.0, 0.5365396954472287], 
reward next is 0.5922, 
noisyNet noise sample is [array([-0.7848731], dtype=float32), 1.3237785]. 
=============================================
[2019-05-04 22:00:59,893] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6161655e-08 9.9770987e-01 2.6924493e-12 2.2339121e-12 3.1571718e-16
 4.6228193e-17 2.2112599e-10 1.2980761e-15 3.2359520e-09 4.7670160e-06
 2.2853196e-03], sum to 1.0000
[2019-05-04 22:00:59,893] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1507
[2019-05-04 22:00:59,914] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.75, 84.25, 5.0, 275.0, 0.0, 0.0, 2.5, -0.7622107225243662, 65.0, 20.22502801467654, 21.5, 0.0, 68.58525561614972], 
current ob forecast is [], 
actual action is [2.25, 20.0], 
sim time this is 2947500.0000, 
sim time next is 2948400.0000, 
raw observation next is [-3.0, 84.0, 4.6, 280.0, 0.0, 0.0, 2.25, -0.7421758417439311, 20.0, 20.52863203209318, 21.5, 0.0, 40.2621274296038], 
processed observation next is [0.0, 0.13043478260869565, 0.2564102564102564, 0.84, 0.41818181818181815, 0.7777777777777778, 0.0, 0.0, 0.5375, 0.25260805275202297, 0.0, 0.36123314744188306, 0.5, 0.0, 0.47367208740710354], 
reward next is 0.4763, 
noisyNet noise sample is [array([0.14363924], dtype=float32), 0.9579967]. 
=============================================
[2019-05-04 22:01:18,744] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.0674588e-10 5.9239608e-01 6.7127629e-13 9.8025461e-13 1.1409796e-17
 1.6479003e-17 2.0876660e-11 3.2898810e-16 9.5868112e-11 3.4696473e-07
 4.0760359e-01], sum to 1.0000
[2019-05-04 22:01:18,744] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5232
[2019-05-04 22:01:18,864] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3433878e-10 9.9780935e-01 3.9556611e-14 2.0459901e-15 1.4914945e-19
 8.2352434e-20 6.1897492e-13 2.9694192e-18 7.0437872e-13 2.0660716e-08
 2.1906360e-03], sum to 1.0000
[2019-05-04 22:01:18,864] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4178
[2019-05-04 22:01:18,868] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.0, 100.0, 8.2, 160.0, 42.0, 237.0, 11.0, -0.5359832678211053, 65.0, 21.75923661995482, 22.7, 1.0, 73.28900586934587], 
current ob forecast is [], 
actual action is [11.0, 20.0], 
sim time this is 3139200.0000, 
sim time next is 3140100.0000, 
raw observation next is [6.25, 100.0, 7.424999999999999, 160.0, 62.5, 314.5, 11.0, -0.4679056479340183, 20.0, 22.20487893458968, 22.7, 1.0, 44.37539196745315], 
processed observation next is [1.0, 0.34782608695652173, 0.4935897435897436, 1.0, 0.6749999999999999, 0.4444444444444444, 0.16534391534391535, 0.3145, 0.6833333333333333, 0.3440314506886606, 0.0, 0.6006969906556685, 0.6714285714285714, 1.0, 0.5220634349112135], 
reward next is 0.4279, 
noisyNet noise sample is [array([-0.44528353], dtype=float32), 0.8199561]. 
=============================================
[2019-05-04 22:01:18,975] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 56.0, 5.4, 120.0, 57.0, 486.0, 5.75, -0.08036165512502488, 20.0, 24.21745168566502, 22.7, 1.0, 39.63363206885791], 
current ob forecast is [], 
actual action is [5.5, 20.0], 
sim time this is 3083400.0000, 
sim time next is 3084300.0000, 
raw observation next is [0.25, 64.0, 6.550000000000001, 125.0, 44.5, 386.5, 5.5, -0.1159773270846949, 20.0, 24.06586209799835, 22.7, 1.0, 28.081335908823323], 
processed observation next is [0.0, 0.6956521739130435, 0.33974358974358976, 0.64, 0.5954545454545456, 0.3472222222222222, 0.11772486772486772, 0.3865, 0.5916666666666667, 0.4613408909717684, 0.0, 0.8665517282854788, 0.6714285714285714, 1.0, 0.3303686577508626], 
reward next is 0.6196, 
noisyNet noise sample is [array([0.3069235], dtype=float32), -1.4489897]. 
=============================================
[2019-05-04 22:01:19,794] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 55117: loss 0.2153
[2019-05-04 22:01:19,795] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 55117: learning rate 0.0001
[2019-05-04 22:01:19,867] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55151: loss 1.0302
[2019-05-04 22:01:19,870] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55153: learning rate 0.0001
[2019-05-04 22:01:21,314] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3500, global step 55806: loss -0.0033
[2019-05-04 22:01:21,315] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3500, global step 55807: learning rate 0.0001
[2019-05-04 22:01:21,501] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55875: loss 2.2625
[2019-05-04 22:01:21,502] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55875: learning rate 0.0001
[2019-05-04 22:01:21,912] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56006: loss 0.2436
[2019-05-04 22:01:21,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56006: learning rate 0.0001
[2019-05-04 22:01:22,029] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56040: loss 0.0898
[2019-05-04 22:01:22,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56040: learning rate 0.0001
[2019-05-04 22:01:22,099] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56054: loss 1.2098
[2019-05-04 22:01:22,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56054: learning rate 0.0001
[2019-05-04 22:01:22,474] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56134: loss 0.1983
[2019-05-04 22:01:22,474] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56134: learning rate 0.0001
[2019-05-04 22:01:22,582] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56150: loss 1.6369
[2019-05-04 22:01:22,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56150: learning rate 0.0001
[2019-05-04 22:01:23,110] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 56229: loss 1.8692
[2019-05-04 22:01:23,111] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 56229: learning rate 0.0001
[2019-05-04 22:01:23,314] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56251: loss 1.9426
[2019-05-04 22:01:23,315] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56251: learning rate 0.0001
[2019-05-04 22:01:23,860] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56309: loss 4.1124
[2019-05-04 22:01:23,861] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56309: learning rate 0.0001
[2019-05-04 22:01:23,873] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56311: loss 1.6282
[2019-05-04 22:01:23,874] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56311: learning rate 0.0001
[2019-05-04 22:01:24,270] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 56363: loss 2.1174
[2019-05-04 22:01:24,270] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 56363: learning rate 0.0001
[2019-05-04 22:01:24,322] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56373: loss 0.4845
[2019-05-04 22:01:24,323] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56373: learning rate 0.0001
[2019-05-04 22:01:24,473] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 56399: loss 0.1257
[2019-05-04 22:01:24,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 56400: learning rate 0.0001
[2019-05-04 22:01:30,609] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.1130230e-07 8.7322152e-01 7.9428242e-10 7.6705378e-11 1.3089733e-13
 9.6292518e-15 3.4571750e-09 3.2673300e-13 6.3639995e-08 3.6980797e-05
 1.2674105e-01], sum to 1.0000
[2019-05-04 22:01:30,609] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3844
[2019-05-04 22:01:30,713] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 65.0, 3.85, 320.0, 0.0, 0.0, 1.0, -0.2230174753156495, 20.0, 22.69881653819201, 22.0, 1.0, 21.59155277193643], 
current ob forecast is [], 
actual action is [1.0, 20.0], 
sim time this is 3262500.0000, 
sim time next is 3263400.0000, 
raw observation next is [-4.0, 65.0, 3.6, 320.0, 0.0, 0.0, 1.0, -0.2612298073605454, 20.0, 22.44029985310761, 22.0, 1.0, 17.72207297350379], 
processed observation next is [1.0, 0.782608695652174, 0.23076923076923078, 0.65, 0.32727272727272727, 0.8888888888888888, 0.0, 0.0, 0.5166666666666667, 0.4129233975464848, 0.0, 0.6343285504439444, 0.5714285714285714, 1.0, 0.2084949761588681], 
reward next is 0.7415, 
noisyNet noise sample is [array([0.04641639], dtype=float32), -0.20069735]. 
=============================================
[2019-05-04 22:01:33,695] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9462545e-09 9.9940836e-01 8.3299296e-12 3.2491669e-13 4.3912803e-16
 4.3956163e-18 1.5579404e-11 3.8972174e-16 4.9974364e-10 1.4091958e-07
 5.9150188e-04], sum to 1.0000
[2019-05-04 22:01:33,699] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8076
[2019-05-04 22:01:33,719] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 92.0, 3.975, 322.5, 0.0, 0.0, -1.0, -0.5826618800763511, 20.0, 21.20986024483012, 20.8, 0.0, 8.305018117719952], 
current ob forecast is [], 
actual action is [-1.0, 20.0], 
sim time this is 3276900.0000, 
sim time next is 3277800.0000, 
raw observation next is [-6.0, 92.0, 4.35, 325.0, 0.0, 0.0, -1.0, -0.6088380335127389, 20.0, 21.06408551129836, 20.8, 0.0, 8.526938002710923], 
processed observation next is [1.0, 0.9565217391304348, 0.1794871794871795, 0.92, 0.39545454545454545, 0.9027777777777778, 0.0, 0.0, 0.48333333333333334, 0.297053988829087, 0.0, 0.43772650161405124, 0.4000000000000001, 0.0, 0.10031691767895203], 
reward next is 0.8497, 
noisyNet noise sample is [array([-0.20093684], dtype=float32), 0.99494624]. 
=============================================
[2019-05-04 22:01:37,118] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.1277384e-08 6.0867476e-01 4.7455689e-11 1.9785340e-11 3.8333340e-15
 8.4241916e-15 1.8142972e-10 2.7183512e-14 2.2481768e-08 1.0141922e-06
 3.9132422e-01], sum to 1.0000
[2019-05-04 22:01:37,118] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8622
[2019-05-04 22:01:37,313] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.25, 54.0, 3.35, 250.0, 117.0, 808.25, -0.5, -0.3502343056785933, 65.0, 22.49900814368979, 22.0, 1.0, 69.57146895957514], 
current ob forecast is [], 
actual action is [-0.25, 65.0], 
sim time this is 3329100.0000, 
sim time next is 3330000.0000, 
raw observation next is [-5.0, 54.0, 3.6, 240.0, 116.0, 805.5, -0.25, -0.2786637119266251, 65.0, 22.93691117890774, 22.0, 1.0, 67.6373073447462], 
processed observation next is [1.0, 0.5652173913043478, 0.20512820512820512, 0.54, 0.32727272727272727, 0.6666666666666666, 0.30687830687830686, 0.8055, 0.49583333333333335, 0.4071120960244583, 1.0, 0.7052730255582483, 0.5714285714285714, 1.0, 0.7957330275852494], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.52390814], dtype=float32), 0.36400685]. 
=============================================
[2019-05-04 22:01:37,352] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[51.279568]
 [51.192394]
 [50.941406]
 [51.20317 ]
 [50.958195]], R is [[51.30688095]
 [50.7938118 ]
 [50.89108276]
 [50.84793472]
 [50.33945465]].
[2019-05-04 22:01:40,253] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6396207e-10 9.9991620e-01 7.8411724e-13 8.1638262e-14 8.1415464e-17
 9.2418144e-18 1.1839105e-12 1.8254479e-16 9.9497288e-10 1.0827993e-08
 8.3749372e-05], sum to 1.0000
[2019-05-04 22:01:40,254] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7189
[2019-05-04 22:01:40,275] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 49.0, 7.2, 222.5, 114.0, 808.75, 8.0, -0.6092007620662213, 20.0, 21.07181537952292, 22.0, 1.0, 13.045428445152737], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 3417300.0000, 
sim time next is 3418200.0000, 
raw observation next is [3.0, 49.0, 6.7, 225.0, 113.0, 806.0, 8.0, -0.5664349359369738, 20.0, 21.45067780625963, 22.0, 1.0, 12.376601707792593], 
processed observation next is [1.0, 0.5652173913043478, 0.41025641025641024, 0.49, 0.6090909090909091, 0.625, 0.29894179894179895, 0.806, 0.6333333333333333, 0.3111883546876754, 0.0, 0.49295397232280436, 0.5714285714285714, 1.0, 0.145607078915207], 
reward next is 0.8694, 
noisyNet noise sample is [array([0.26796198], dtype=float32), -0.04427261]. 
=============================================
[2019-05-04 22:01:42,406] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9259914e-09 9.9882799e-01 1.9575294e-14 4.5724118e-15 3.2407747e-18
 8.0697854e-20 6.0132747e-13 1.1756846e-18 9.7806832e-11 4.1691770e-09
 1.1719592e-03], sum to 1.0000
[2019-05-04 22:01:42,407] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3204
[2019-05-04 22:01:42,489] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 72.0, 6.7, 245.0, 0.0, 0.0, 5.0, -1.174001410212372, 20.0, 18.67110046539679, 20.8, 0.0, 8.482972877426677], 
current ob forecast is [], 
actual action is [5.0, 20.0], 
sim time this is 3476700.0000, 
sim time next is 3477600.0000, 
raw observation next is [0.0, 72.0, 6.2, 240.0, 0.0, 0.0, 5.0, -1.185161083566175, 20.0, 18.62498013276688, 20.8, 0.0, 8.613867889774898], 
processed observation next is [1.0, 0.2608695652173913, 0.3333333333333333, 0.72, 0.5636363636363636, 0.6666666666666666, 0.0, 0.0, 0.5833333333333334, 0.10494630547794166, 0.0, 0.08928287610955417, 0.4000000000000001, 0.0, 0.10133962223264587], 
reward next is 0.3996, 
noisyNet noise sample is [array([-0.29263422], dtype=float32), 0.40451816]. 
=============================================
[2019-05-04 22:01:52,519] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4000, global step 62711: loss 3.4502
[2019-05-04 22:01:52,520] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4000, global step 62711: learning rate 0.0001
[2019-05-04 22:01:52,530] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 62711: loss 10.2832
[2019-05-04 22:01:52,531] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 62711: learning rate 0.0001
[2019-05-04 22:01:53,051] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6362128e-08 9.9968004e-01 1.3790863e-11 2.3008996e-11 4.9546523e-15
 1.7848437e-15 9.0095972e-11 1.7865204e-14 2.6235709e-09 3.8670208e-07
 3.1963349e-04], sum to 1.0000
[2019-05-04 22:01:53,055] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3969
[2019-05-04 22:01:53,072] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 72.0, 6.2, 285.0, 0.0, 0.0, 4.0, -0.7844336819320579, 20.0, 20.2974320352321, 20.8, 0.0, 8.325252392369402], 
current ob forecast is [], 
actual action is [4.0, 20.0], 
sim time this is 3537000.0000, 
sim time next is 3537900.0000, 
raw observation next is [-1.0, 69.0, 5.95, 287.5, 0.0, 0.0, 4.0, -0.8012418624139412, 20.0, 20.22122686810499, 20.8, 0.0, 8.448284244368198], 
processed observation next is [1.0, 0.9565217391304348, 0.3076923076923077, 0.69, 0.5409090909090909, 0.7986111111111112, 0.0, 0.0, 0.5666666666666667, 0.2329193791953529, 0.0, 0.31731812401499837, 0.4000000000000001, 0.0, 0.09939157934550821], 
reward next is 0.8506, 
noisyNet noise sample is [array([0.33238673], dtype=float32), -0.42011565]. 
=============================================
[2019-05-04 22:01:53,280] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4076512e-09 9.9995029e-01 2.5189185e-12 4.9753843e-13 7.5909253e-17
 1.6931645e-16 2.1976650e-11 4.3762817e-16 3.4570780e-10 2.3093158e-09
 4.9687944e-05], sum to 1.0000
[2019-05-04 22:01:53,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9288
[2019-05-04 22:01:53,296] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 72.0, 5.7, 270.0, 0.0, 0.0, 5.0, -0.6190765219326394, 20.0, 20.92281419638559, 20.8, 0.0, 7.3089767178625635], 
current ob forecast is [], 
actual action is [5.0, 20.0], 
sim time this is 3531600.0000, 
sim time next is 3532500.0000, 
raw observation next is [-0.25, 73.5, 5.95, 272.5, 0.0, 0.0, 5.0, -0.6307417599837106, 20.0, 20.87547690917413, 20.8, 0.0, 7.426253351207237], 
processed observation next is [1.0, 0.9130434782608695, 0.3269230769230769, 0.735, 0.5409090909090909, 0.7569444444444444, 0.0, 0.0, 0.5833333333333334, 0.28975274667209644, 0.0, 0.41078241559630413, 0.4000000000000001, 0.0, 0.08736768648479101], 
reward next is 0.8626, 
noisyNet noise sample is [array([0.01959618], dtype=float32), -0.05948224]. 
=============================================
[2019-05-04 22:01:53,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[51.01939 ]
 [51.142002]
 [51.128548]
 [50.99236 ]
 [50.78339 ]], R is [[51.31671524]
 [51.66756058]
 [52.01641846]
 [52.36353302]
 [52.70914078]].
[2019-05-04 22:01:54,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.84225814e-07 3.17176729e-01 3.40917655e-10 4.87649532e-10
 5.59576419e-13 8.42230813e-14 1.57807500e-09 2.43264529e-13
 1.47164965e-08 1.54239774e-06 6.82821393e-01], sum to 1.0000
[2019-05-04 22:01:54,484] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5665
[2019-05-04 22:01:54,517] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 70.0, 2.6, 280.0, 0.0, 0.0, -0.75, -1.074968937216267, 20.0, 19.25340384720477, 21.5, 0.0, 20.921578903264415], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3564000.0000, 
sim time next is 3564900.0000, 
raw observation next is [-6.0, 70.0, 2.975, 277.5, 0.0, 0.0, -1.0, -1.025882127414708, 65.0, 19.22873476612283, 21.5, 0.0, 67.26081497049303], 
processed observation next is [0.0, 0.2608695652173913, 0.1794871794871795, 0.7, 0.27045454545454545, 0.7708333333333334, 0.0, 0.0, 0.48333333333333334, 0.15803929086176396, 1.0, 0.17553353801754717, 0.5, 0.0, 0.7913037055352121], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.34494174], dtype=float32), -2.221574]. 
=============================================
[2019-05-04 22:01:57,540] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7756664e-08 6.9466629e-03 8.6245566e-11 1.2615510e-11 3.6176951e-15
 7.6970546e-16 4.2026001e-11 1.9597844e-15 6.7959167e-09 3.8327204e-07
 9.9305290e-01], sum to 1.0000
[2019-05-04 22:01:57,540] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4922
[2019-05-04 22:01:57,722] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 54.0, 3.6, 270.0, 112.5, 787.0, 0.75, -0.4997706987808672, 65.0, 21.93652389269931, 22.7, 1.0, 64.89663884685207], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3582000.0000, 
sim time next is 3582900.0000, 
raw observation next is [-3.75, 54.25, 3.35, 265.0, 113.25, 801.5, 1.0, -0.4682829079570405, 65.0, 22.07615610761255, 22.7, 1.0, 64.21482701777663], 
processed observation next is [0.0, 0.4782608695652174, 0.23717948717948717, 0.5425, 0.30454545454545456, 0.7361111111111112, 0.2996031746031746, 0.8015, 0.5166666666666667, 0.34390569734765314, 1.0, 0.5823080153732215, 0.6714285714285714, 1.0, 0.7554685531503132], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.07335805], dtype=float32), 1.666635]. 
=============================================
[2019-05-04 22:01:59,143] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64063: loss 8.3930
[2019-05-04 22:01:59,178] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64066: learning rate 0.0001
[2019-05-04 22:01:59,362] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64088: loss 0.1600
[2019-05-04 22:01:59,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64089: learning rate 0.0001
[2019-05-04 22:01:59,541] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4000, global step 64114: loss 6.0382
[2019-05-04 22:01:59,541] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4000, global step 64114: learning rate 0.0001
[2019-05-04 22:01:59,625] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64127: loss 2.7069
[2019-05-04 22:01:59,626] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64127: loss 8.7390
[2019-05-04 22:01:59,629] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64128: learning rate 0.0001
[2019-05-04 22:01:59,629] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64128: learning rate 0.0001
[2019-05-04 22:01:59,869] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64188: loss 1.0071
[2019-05-04 22:01:59,876] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64189: learning rate 0.0001
[2019-05-04 22:01:59,915] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64201: loss 1.5137
[2019-05-04 22:01:59,916] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64201: learning rate 0.0001
[2019-05-04 22:01:59,942] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 64211: loss 0.6075
[2019-05-04 22:01:59,944] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 64211: learning rate 0.0001
[2019-05-04 22:01:59,973] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64223: loss 0.3193
[2019-05-04 22:01:59,975] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64223: learning rate 0.0001
[2019-05-04 22:02:00,009] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64238: loss 0.5597
[2019-05-04 22:02:00,010] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64239: learning rate 0.0001
[2019-05-04 22:02:00,104] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 64276: loss 0.0813
[2019-05-04 22:02:00,105] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 64276: learning rate 0.0001
[2019-05-04 22:02:00,111] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64277: loss 0.0807
[2019-05-04 22:02:00,111] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64277: learning rate 0.0001
[2019-05-04 22:02:00,187] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64311: loss 0.1174
[2019-05-04 22:02:00,190] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64314: learning rate 0.0001
[2019-05-04 22:02:00,221] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64327: loss 1.1269
[2019-05-04 22:02:00,223] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64327: learning rate 0.0001
[2019-05-04 22:02:05,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.0534932e-09 2.5128347e-01 1.7349669e-12 1.5045554e-13 8.2070558e-17
 2.4538397e-17 3.4444364e-12 1.2861511e-17 1.5528661e-09 2.0903641e-08
 7.4871653e-01], sum to 1.0000
[2019-05-04 22:02:05,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7917
[2019-05-04 22:02:05,976] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.0, 50.0, 5.7, 250.0, 75.5, 613.5, 10.25, -0.08892164526382484, 65.0, 23.53841553545903, 22.7, 1.0, 74.55449511232315], 
current ob forecast is [], 
actual action is [10.0, 65.0], 
sim time this is 3686400.0000, 
sim time next is 3687300.0000, 
raw observation next is [4.75, 52.25, 5.550000000000001, 250.0, 69.75, 567.75, 10.0, 0.01146278599572392, 65.0, 24.27858014287239, 22.7, 1.0, 67.94732614801653], 
processed observation next is [0.0, 0.6956521739130435, 0.4551282051282051, 0.5225, 0.5045454545454546, 0.6944444444444444, 0.18452380952380953, 0.56775, 0.6666666666666666, 0.5038209286652413, 1.0, 0.8969400204103414, 0.6714285714285714, 1.0, 0.7993803076237239], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7864941], dtype=float32), -0.39501894]. 
=============================================
[2019-05-04 22:02:09,243] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4828121e-10 9.9993372e-01 1.4367025e-12 7.5725510e-13 2.8757716e-18
 1.9252290e-19 1.6535335e-12 1.3593625e-17 6.8083164e-11 2.1138412e-08
 6.6199951e-05], sum to 1.0000
[2019-05-04 22:02:09,244] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8419
[2019-05-04 22:02:09,259] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 5.1, 260.0, 0.0, 0.0, 2.0, -0.5629874239775852, 20.0, 21.69906091753135, 21.5, 0.0, 7.775594227026615], 
current ob forecast is [], 
actual action is [2.0, 20.0], 
sim time this is 3726000.0000, 
sim time next is 3726900.0000, 
raw observation next is [-3.0, 65.0, 5.1, 262.5, 0.0, 0.0, 2.0, -0.5798255566305665, 20.0, 21.62570398797556, 21.5, 0.0, 7.940469429279279], 
processed observation next is [1.0, 0.13043478260869565, 0.2564102564102564, 0.65, 0.4636363636363636, 0.7291666666666666, 0.0, 0.0, 0.5333333333333333, 0.30672481445647787, 0.0, 0.517957712567937, 0.5, 0.0, 0.09341728740328563], 
reward next is 0.8566, 
noisyNet noise sample is [array([1.3839858], dtype=float32), -0.2347013]. 
=============================================
[2019-05-04 22:02:12,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.06570145e-14 1.92073658e-02 2.50870454e-20 4.15528849e-21
 9.89559643e-25 1.46327499e-28 6.27899403e-18 1.74055737e-25
 2.88465710e-15 4.48934076e-13 9.80792701e-01], sum to 1.0000
[2019-05-04 22:02:12,072] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7645
[2019-05-04 22:02:12,261] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 68.25, 5.825, 225.0, 12.75, 110.25, 4.0, -0.257104035845945, 20.0, 22.70912099788457, 22.7, 1.0, 21.317815881200257], 
current ob forecast is [], 
actual action is [3.5, 65.0], 
sim time this is 3779100.0000, 
sim time next is 3780000.0000, 
raw observation next is [-2.0, 71.0, 5.7, 220.0, 0.0, 0.0, 3.5, -0.1840933700295497, 65.0, 22.76354527065797, 22.7, 1.0, 82.67797328126214], 
processed observation next is [1.0, 0.782608695652174, 0.28205128205128205, 0.71, 0.5181818181818182, 0.6111111111111112, 0.0, 0.0, 0.5583333333333333, 0.4386355433234834, 1.0, 0.680506467236853, 0.6714285714285714, 1.0, 0.972682038603084], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.49029288], dtype=float32), 0.2235982]. 
=============================================
[2019-05-04 22:02:12,308] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[93.06755 ]
 [93.49193 ]
 [94.12613 ]
 [94.287964]
 [92.92645 ]], R is [[92.87531281]
 [92.64576721]
 [92.34035492]
 [91.90731812]
 [90.9882431 ]].
[2019-05-04 22:02:15,083] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6024724e-11 9.9961179e-01 2.0212220e-14 4.0831521e-14 2.7782660e-18
 1.5358552e-19 3.7061591e-12 3.4020295e-18 4.8949490e-12 4.5142537e-10
 3.8828448e-04], sum to 1.0000
[2019-05-04 22:02:15,084] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4938
[2019-05-04 22:02:15,208] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 69.5, 6.2, 237.5, 0.0, 0.0, 3.0, -0.006872895994746673, 65.0, 23.37565573366391, 22.7, 1.0, 83.75530733100506], 
current ob forecast is [], 
actual action is [3.0, 20.0], 
sim time this is 3784500.0000, 
sim time next is 3785400.0000, 
raw observation next is [-2.0, 68.0, 6.2, 235.0, 0.0, 0.0, 3.0, 0.02825068252824959, 20.0, 23.93234781220509, 22.7, 1.0, 39.15388046472986], 
processed observation next is [1.0, 0.8260869565217391, 0.28205128205128205, 0.68, 0.5636363636363636, 0.6527777777777778, 0.0, 0.0, 0.55, 0.5094168941760832, 0.0, 0.8474782588864416, 0.6714285714285714, 1.0, 0.4606338878203513], 
reward next is 0.4894, 
noisyNet noise sample is [array([-1.3344028], dtype=float32), -0.44159847]. 
=============================================
[2019-05-04 22:02:28,283] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0119873e-08 9.8189455e-01 1.0032222e-11 3.4696560e-12 6.9971062e-15
 1.7804500e-17 7.7567258e-11 4.7532990e-15 2.0483717e-09 3.6950507e-08
 1.8105451e-02], sum to 1.0000
[2019-05-04 22:02:28,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7411
[2019-05-04 22:02:28,305] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.25, 42.0, 3.475, 360.0, 0.0, 0.0, -1.0, -0.5304132925752771, 20.0, 21.50911888463372, 22.7, 1.0, 5.540995921268998], 
current ob forecast is [], 
actual action is [-1.25, 20.0], 
sim time this is 3957300.0000, 
sim time next is 3958200.0000, 
raw observation next is [-6.5, 43.0, 3.35, 360.0, 0.0, 0.0, -1.25, -0.5512735702848663, 20.0, 21.42389049215419, 22.7, 1.0, 5.170622257100869], 
processed observation next is [1.0, 0.8260869565217391, 0.16666666666666666, 0.43, 0.30454545454545456, 1.0, 0.0, 0.0, 0.4791666666666667, 0.3162421432383779, 0.0, 0.4891272131648842, 0.6714285714285714, 1.0, 0.06083085008353964], 
reward next is 0.8519, 
noisyNet noise sample is [array([0.07601918], dtype=float32), -1.1983168]. 
=============================================
[2019-05-04 22:02:28,371] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9236953e-10 9.9999833e-01 8.7604294e-14 2.8506392e-14 1.1880914e-19
 3.2576513e-21 8.3759544e-14 4.2610882e-18 1.4498496e-12 3.1316394e-10
 1.6828708e-06], sum to 1.0000
[2019-05-04 22:02:28,374] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8116
[2019-05-04 22:02:28,498] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 45.0, 3.85, 352.5, 0.0, 0.0, -2.0, -0.6179161928639577, 20.0, 21.15384904046871, 21.5, 0.0, 6.163438214706908], 
current ob forecast is [], 
actual action is [-2.0, 20.0], 
sim time this is 3962700.0000, 
sim time next is 3963600.0000, 
raw observation next is [-7.0, 45.0, 4.1, 350.0, 0.0, 0.0, -2.0, -0.6361800321656936, 20.0, 21.07970548743226, 21.5, 0.0, 6.641923276668209], 
processed observation next is [1.0, 0.9130434782608695, 0.15384615384615385, 0.45, 0.3727272727272727, 0.9722222222222222, 0.0, 0.0, 0.4666666666666667, 0.2879399892781021, 0.0, 0.43995792677603723, 0.5, 0.0, 0.07814027384315539], 
reward next is 0.8719, 
noisyNet noise sample is [array([0.32840526], dtype=float32), -1.3496894]. 
=============================================
[2019-05-04 22:02:29,285] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4500, global step 71018: loss 0.1051
[2019-05-04 22:02:29,291] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4500, global step 71018: learning rate 0.0001
[2019-05-04 22:02:29,946] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 71237: loss 0.0171
[2019-05-04 22:02:29,946] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 71237: learning rate 0.0001
[2019-05-04 22:02:33,333] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4987342e-10 9.9965012e-01 4.8841827e-14 8.7819631e-15 2.4994911e-19
 1.3405376e-19 6.9606669e-13 1.5539012e-18 9.4428432e-12 7.0543837e-10
 3.4989580e-04], sum to 1.0000
[2019-05-04 22:02:33,333] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2462
[2019-05-04 22:02:33,390] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 37.0, 2.1, 100.0, 118.5, 828.5, -1.5, -0.7029007777806243, 20.0, 21.3848771139845, 22.7, 1.0, 18.122375427280428], 
current ob forecast is [], 
actual action is [-1.0, 20.0], 
sim time this is 4017600.0000, 
sim time next is 4018500.0000, 
raw observation next is [-5.5, 35.0, 1.95, 87.5, 118.25, 834.75, -1.0, -0.7327081731412685, 20.0, 21.18445032071163, 22.7, 1.0, 14.476794220947381], 
processed observation next is [1.0, 0.5217391304347826, 0.19230769230769232, 0.35, 0.17727272727272728, 0.24305555555555555, 0.31283068783068785, 0.83475, 0.48333333333333334, 0.25576394228624383, 0.0, 0.4549214743873757, 0.6714285714285714, 1.0, 0.1703152261287927], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.36431375], dtype=float32), 0.36001077]. 
=============================================
[2019-05-04 22:02:33,424] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71720: loss 1.5970
[2019-05-04 22:02:33,426] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[61.283447]
 [60.788223]
 [61.69918 ]
 [61.425972]
 [61.80633 ]], R is [[60.87413025]
 [60.32598114]
 [59.833992  ]
 [59.23565292]
 [58.98318481]].
[2019-05-04 22:02:33,430] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71722: learning rate 0.0001
[2019-05-04 22:02:34,260] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71906: loss 0.5687
[2019-05-04 22:02:34,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71906: learning rate 0.0001
[2019-05-04 22:02:34,889] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72005: loss 0.3173
[2019-05-04 22:02:34,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72005: learning rate 0.0001
[2019-05-04 22:02:34,899] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4500, global step 72007: loss 0.2091
[2019-05-04 22:02:34,899] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4500, global step 72007: learning rate 0.0001
[2019-05-04 22:02:35,355] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72090: loss 0.2183
[2019-05-04 22:02:35,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72090: learning rate 0.0001
[2019-05-04 22:02:35,620] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 72156: loss 0.1402
[2019-05-04 22:02:35,620] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 72156: learning rate 0.0001
[2019-05-04 22:02:35,815] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72213: loss 2.8381
[2019-05-04 22:02:35,815] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72213: learning rate 0.0001
[2019-05-04 22:02:36,142] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 72331: loss 4.5768
[2019-05-04 22:02:36,142] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 72331: learning rate 0.0001
[2019-05-04 22:02:36,181] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72349: loss 7.1522
[2019-05-04 22:02:36,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72349: learning rate 0.0001
[2019-05-04 22:02:36,249] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 72367: loss 6.6711
[2019-05-04 22:02:36,251] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 72370: learning rate 0.0001
[2019-05-04 22:02:36,352] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 72419: loss 6.0482
[2019-05-04 22:02:36,353] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 72419: learning rate 0.0001
[2019-05-04 22:02:36,373] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0671090e-11 3.0564724e-03 9.4930777e-15 9.3467163e-16 2.3945964e-19
 7.1507381e-21 6.2273659e-14 2.2860897e-20 2.1575196e-11 9.4539532e-12
 9.9694353e-01], sum to 1.0000
[2019-05-04 22:02:36,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8468
[2019-05-04 22:02:36,412] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72439: loss 4.9993
[2019-05-04 22:02:36,413] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72439: learning rate 0.0001
[2019-05-04 22:02:36,555] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 36.0, 1.55, 65.0, 92.0, 469.0, 1.25, -0.864536573621717, 20.0, 20.67327808349152, 22.7, 1.0, 25.3706999650182], 
current ob forecast is [], 
actual action is [1.5, 65.0], 
sim time this is 4091400.0000, 
sim time next is 4092300.0000, 
raw observation next is [-3.25, 37.0, 2.325, 97.5, 95.0, 521.5, 1.5, -0.7833524501824917, 65.0, 20.78335714865474, 22.7, 1.0, 78.77335859173222], 
processed observation next is [1.0, 0.34782608695652173, 0.25, 0.37, 0.2113636363636364, 0.2708333333333333, 0.25132275132275134, 0.5215, 0.525, 0.23888251660583612, 1.0, 0.3976224498078201, 0.6714285714285714, 1.0, 0.9267453951968496], 
reward next is 0.8118, 
noisyNet noise sample is [array([-2.499169], dtype=float32), 1.3725893]. 
=============================================
[2019-05-04 22:02:36,693] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72548: loss 1.8936
[2019-05-04 22:02:36,694] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72548: learning rate 0.0001
[2019-05-04 22:02:36,759] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 72567: loss 2.8764
[2019-05-04 22:02:36,761] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 72567: learning rate 0.0001
[2019-05-04 22:02:38,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6398981e-09 8.8427097e-01 2.2112618e-12 2.5187870e-13 3.1636260e-17
 4.8089952e-18 1.9093463e-11 4.2098893e-16 2.8277873e-09 1.3215657e-09
 1.1572902e-01], sum to 1.0000
[2019-05-04 22:02:38,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6296
[2019-05-04 22:02:38,657] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.75, 35.0, 0.775, 32.5, 69.0, 351.75, 1.0, -0.9005262578755433, 20.0, 20.64158577549063, 22.7, 1.0, 46.414352753567336], 
current ob forecast is [], 
actual action is [1.25, 20.0], 
sim time this is 4090500.0000, 
sim time next is 4091400.0000, 
raw observation next is [-3.5, 36.0, 1.55, 65.0, 92.0, 469.0, 1.25, -0.9073771538964465, 20.0, 20.6501206875106, 22.7, 1.0, 33.711391554609925], 
processed observation next is [1.0, 0.34782608695652173, 0.24358974358974358, 0.36, 0.1409090909090909, 0.18055555555555555, 0.24338624338624337, 0.469, 0.5208333333333334, 0.19754094870118452, 0.0, 0.37858866964437154, 0.6714285714285714, 1.0, 0.39660460652482266], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33120456], dtype=float32), -0.4636896]. 
=============================================
[2019-05-04 22:02:44,196] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6392606e-11 9.9974793e-01 2.6171285e-15 1.5521242e-15 2.0316572e-20
 1.1526191e-21 3.3543267e-14 2.5614568e-19 2.8162477e-12 1.7273151e-10
 2.5210186e-04], sum to 1.0000
[2019-05-04 22:02:44,199] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8316
[2019-05-04 22:02:44,213] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 48.0, 6.7, 220.0, 0.0, 0.0, 7.0, -0.6693225754347246, 20.0, 21.37080887837619, 21.5, 0.0, 5.737421924406463], 
current ob forecast is [], 
actual action is [7.0, 20.0], 
sim time this is 4237200.0000, 
sim time next is 4238100.0000, 
raw observation next is [2.25, 47.25, 6.45, 222.5, 0.0, 0.0, 7.0, -0.6823536550986392, 20.0, 21.31293787577628, 21.5, 0.0, 5.992275565078065], 
processed observation next is [0.0, 0.043478260869565216, 0.391025641025641, 0.4725, 0.5863636363636364, 0.6180555555555556, 0.0, 0.0, 0.6166666666666667, 0.27254878163378693, 0.0, 0.47327683939661164, 0.5, 0.0, 0.07049735958915371], 
reward next is 0.8795, 
noisyNet noise sample is [array([-1.290838], dtype=float32), -0.67451257]. 
=============================================
[2019-05-04 22:02:44,542] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.9398561e-10 9.9984217e-01 2.6592006e-12 1.9548396e-14 1.9559358e-18
 5.9748742e-20 4.0767951e-13 2.0376412e-18 1.7981533e-11 8.6230689e-10
 1.5780267e-04], sum to 1.0000
[2019-05-04 22:02:44,546] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7812
[2019-05-04 22:02:44,557] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 45.0, 6.075, 242.5, 0.0, 0.0, 8.0, -0.8008617137395877, 20.0, 20.80622870787458, 21.5, 0.0, 6.952662797638676], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 4247100.0000, 
sim time next is 4248000.0000, 
raw observation next is [3.0, 45.0, 6.2, 240.0, 0.0, 0.0, 8.0, -0.812507062099582, 20.0, 20.75466198700568, 21.5, 0.0, 7.011106440183397], 
processed observation next is [0.0, 0.17391304347826086, 0.41025641025641024, 0.45, 0.5636363636363636, 0.6666666666666666, 0.0, 0.0, 0.6333333333333333, 0.22916431263347267, 0.0, 0.39352314100081137, 0.5, 0.0, 0.0824836051786282], 
reward next is 0.8675, 
noisyNet noise sample is [array([-1.2049929], dtype=float32), 1.1902939]. 
=============================================
[2019-05-04 22:02:44,570] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[62.420475]
 [62.502167]
 [62.631397]
 [62.77859 ]
 [62.94013 ]], R is [[62.52555466]
 [62.76850128]
 [63.00971603]
 [63.2492218 ]
 [63.48704529]].
[2019-05-04 22:02:50,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4385140e-14 9.9998677e-01 2.9439228e-18 2.1077654e-19 1.9047755e-25
 5.0543200e-26 2.3706947e-17 2.4408961e-25 1.1713305e-15 4.9766714e-15
 1.3208670e-05], sum to 1.0000
[2019-05-04 22:02:50,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7711
[2019-05-04 22:02:50,701] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.475, 41.75, 3.425, 252.5, 0.0, 0.0, 6.55, -0.4406444670681953, 20.0, 22.29148164423675, 22.7, 1.0, 13.532786100959363], 
current ob forecast is [], 
actual action is [6.475, 20.0], 
sim time this is 4214700.0000, 
sim time next is 4215600.0000, 
raw observation next is [1.4, 42.0, 3.4, 250.0, 0.0, 0.0, 6.475, -0.4654615216402488, 20.0, 22.16850515992022, 22.7, 1.0, 10.830658865932842], 
processed observation next is [0.0, 0.8260869565217391, 0.36923076923076925, 0.42, 0.3090909090909091, 0.6944444444444444, 0.0, 0.0, 0.6079166666666667, 0.3448461594532504, 0.0, 0.5955007371314599, 0.6714285714285714, 1.0, 0.12741951606979812], 
reward next is 0.8226, 
noisyNet noise sample is [array([2.1121418], dtype=float32), -1.1121364]. 
=============================================
[2019-05-04 22:02:52,117] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9459485e-13 6.0841459e-01 7.0699018e-19 3.8947586e-20 1.0028169e-27
 5.2432489e-28 4.1330501e-19 1.4359012e-24 1.2488138e-16 1.4827931e-13
 3.9158541e-01], sum to 1.0000
[2019-05-04 22:02:52,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0770
[2019-05-04 22:02:52,249] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 53.0, 5.7, 240.0, 146.0, 92.0, 8.0, -0.7478388759493022, 20.0, 21.32373303470489, 22.7, 1.0, 46.29036683268901], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 4266000.0000, 
sim time next is 4266900.0000, 
raw observation next is [3.25, 53.25, 5.825, 240.0, 164.0, 111.5, 8.0, -0.7556314956852427, 20.0, 21.27569990370041, 22.7, 1.0, 33.49798930558715], 
processed observation next is [0.0, 0.391304347826087, 0.4166666666666667, 0.5325, 0.5295454545454545, 0.6666666666666666, 0.43386243386243384, 0.1115, 0.6333333333333333, 0.24812283477158578, 0.0, 0.46795712910005854, 0.6714285714285714, 1.0, 0.39409399183043703], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5716393], dtype=float32), 1.4969125]. 
=============================================
[2019-05-04 22:02:56,922] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2330255e-10 9.9993086e-01 1.0571896e-15 1.4699636e-16 1.3594129e-21
 7.8255214e-25 1.7411767e-15 5.7380293e-22 1.6525285e-13 2.0826791e-12
 6.9100868e-05], sum to 1.0000
[2019-05-04 22:02:56,922] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9668
[2019-05-04 22:02:56,949] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 74.0, 3.775, 195.0, 0.0, 0.0, 8.1, -0.8339337320991208, 20.0, 20.4468422311449, 21.5, 0.0, 23.860534503332246], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 4344300.0000, 
sim time next is 4345200.0000, 
raw observation next is [2.9, 75.0, 3.5, 190.0, 0.0, 0.0, 8.0, -0.8575903405012612, 20.0, 20.32587895552757, 21.5, 0.0, 19.1876927135272], 
processed observation next is [1.0, 0.30434782608695654, 0.4076923076923077, 0.75, 0.3181818181818182, 0.5277777777777778, 0.0, 0.0, 0.6333333333333333, 0.21413655316624627, 0.0, 0.33226842221822445, 0.5, 0.0, 0.2257375613356141], 
reward next is 0.7243, 
noisyNet noise sample is [array([2.2947145], dtype=float32), -2.6764972]. 
=============================================
[2019-05-04 22:02:59,645] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.6757954e-20 1.0000000e+00 3.6622012e-24 5.6447743e-26 1.8408239e-32
 5.3297254e-34 2.2365772e-23 1.8197464e-31 9.9935301e-21 1.9183708e-18
 3.8110495e-10], sum to 1.0000
[2019-05-04 22:02:59,652] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0481
[2019-05-04 22:02:59,678] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.4, 61.0, 4.8, 250.0, 0.0, 0.0, 14.6, -0.2579098631284855, 20.0, 22.36970645802922, 21.5, 0.0, 2.33474039480102], 
current ob forecast is [], 
actual action is [14.4, 20.0], 
sim time this is 4399200.0000, 
sim time next is 4400100.0000, 
raw observation next is [9.175, 61.25, 4.899999999999999, 250.0, 0.0, 0.0, 14.4, -0.3033861029172252, 20.0, 22.30095678732936, 21.5, 0.0, 2.5206808184882674], 
processed observation next is [1.0, 0.9565217391304348, 0.5685897435897436, 0.6125, 0.4454545454545453, 0.6944444444444444, 0.0, 0.0, 0.74, 0.3988712990275916, 0.0, 0.6144223981899083, 0.5, 0.0, 0.029655068452803145], 
reward next is 0.9203, 
noisyNet noise sample is [array([-0.18957745], dtype=float32), 0.1532279]. 
=============================================
[2019-05-04 22:03:00,862] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5000, global step 78831: loss 0.0086
[2019-05-04 22:03:00,863] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5000, global step 78831: learning rate 0.0001
[2019-05-04 22:03:02,986] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 79147: loss 0.0586
[2019-05-04 22:03:02,990] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 79147: learning rate 0.0001
[2019-05-04 22:03:03,688] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8208612e-12 9.9998593e-01 6.3940654e-16 4.1021323e-17 9.0950940e-21
 1.0098308e-21 2.4653775e-15 7.3655441e-21 3.2381229e-13 5.3079562e-13
 1.4123884e-05], sum to 1.0000
[2019-05-04 22:03:03,689] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7527
[2019-05-04 22:03:03,718] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.9, 71.0, 4.449999999999999, 300.0, 0.0, 0.0, 4.15, -0.9148001109196016, 20.0, 19.79266022660662, 21.5, 0.0, 8.890626011719133], 
current ob forecast is [], 
actual action is [4.1, 20.0], 
sim time this is 4512600.0000, 
sim time next is 4513500.0000, 
raw observation next is [-0.95, 71.0, 4.274999999999999, 300.0, 0.0, 0.0, 4.1, -0.927253345695767, 20.0, 19.74265331404747, 21.5, 0.0, 8.882038573975452], 
processed observation next is [1.0, 0.21739130434782608, 0.308974358974359, 0.71, 0.3886363636363635, 0.8333333333333334, 0.0, 0.0, 0.5683333333333334, 0.19091555143474434, 0.0, 0.2489504734353528, 0.5, 0.0, 0.10449457145853473], 
reward next is 0.8455, 
noisyNet noise sample is [array([0.2227868], dtype=float32), -1.2131804]. 
=============================================
[2019-05-04 22:03:03,742] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[68.07879 ]
 [68.15886 ]
 [68.28482 ]
 [68.388374]
 [68.485756]], R is [[68.19300079]
 [68.35647583]
 [68.51824188]
 [68.67834473]
 [68.83685303]].
[2019-05-04 22:03:05,338] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80003: loss 0.0236
[2019-05-04 22:03:05,350] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80010: learning rate 0.0001
[2019-05-04 22:03:05,446] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80019: loss 0.0634
[2019-05-04 22:03:05,449] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80019: learning rate 0.0001
[2019-05-04 22:03:05,459] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80022: loss 0.0421
[2019-05-04 22:03:05,548] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80022: learning rate 0.0001
[2019-05-04 22:03:05,567] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80028: loss 0.0007
[2019-05-04 22:03:05,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80030: learning rate 0.0001
[2019-05-04 22:03:05,646] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80034: loss 0.0225
[2019-05-04 22:03:05,647] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80034: learning rate 0.0001
[2019-05-04 22:03:05,866] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5000, global step 80052: loss 0.0119
[2019-05-04 22:03:05,869] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5000, global step 80054: learning rate 0.0001
[2019-05-04 22:03:05,971] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 80065: loss 0.0098
[2019-05-04 22:03:05,974] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 80065: learning rate 0.0001
[2019-05-04 22:03:06,083] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80083: loss 0.0579
[2019-05-04 22:03:06,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80083: learning rate 0.0001
[2019-05-04 22:03:06,093] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 80083: loss 0.0715
[2019-05-04 22:03:06,178] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 80083: learning rate 0.0001
[2019-05-04 22:03:06,504] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80200: loss 0.0015
[2019-05-04 22:03:06,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80200: learning rate 0.0001
[2019-05-04 22:03:06,649] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80227: loss 0.0897
[2019-05-04 22:03:06,651] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80227: learning rate 0.0001
[2019-05-04 22:03:06,777] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80241: loss 0.0345
[2019-05-04 22:03:06,780] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80242: learning rate 0.0001
[2019-05-04 22:03:06,904] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80261: loss 0.0016
[2019-05-04 22:03:06,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80261: learning rate 0.0001
[2019-05-04 22:03:07,090] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 80305: loss 0.0243
[2019-05-04 22:03:07,095] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 80307: learning rate 0.0001
[2019-05-04 22:03:13,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1542803e-09 9.9998999e-01 9.8523891e-14 3.8061005e-14 2.3737236e-18
 1.1993014e-19 1.7337453e-13 3.4787005e-17 1.6312797e-10 8.2043421e-11
 1.0025925e-05], sum to 1.0000
[2019-05-04 22:03:13,870] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9593
[2019-05-04 22:03:13,885] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.2, 65.0, 0.0, 0.0, 0.0, 0.0, 4.95, -0.7975321693344974, 20.0, 20.40477602153983, 21.5, 0.0, 5.5211824335389705], 
current ob forecast is [], 
actual action is [4.8, 20.0], 
sim time this is 4586400.0000, 
sim time next is 4587300.0000, 
raw observation next is [-0.425, 65.5, 0.0, 0.0, 0.0, 0.0, 4.8, -0.8117039264351797, 20.0, 20.34635770118542, 21.5, 0.0, 5.594965731719564], 
processed observation next is [1.0, 0.08695652173913043, 0.3224358974358974, 0.655, 0.0, 0.0, 0.0, 0.0, 0.58, 0.22943202452160674, 0.0, 0.33519395731220264, 0.5, 0.0, 0.06582312625552429], 
reward next is 0.8842, 
noisyNet noise sample is [array([0.28594908], dtype=float32), -0.6627995]. 
=============================================
[2019-05-04 22:03:15,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7937119e-08 9.1384131e-01 1.4413433e-10 4.0020743e-12 5.0575998e-16
 8.2958943e-17 1.9857285e-09 7.3648022e-16 6.9034707e-09 5.9027144e-08
 8.6158663e-02], sum to 1.0000
[2019-05-04 22:03:15,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3072
[2019-05-04 22:03:15,725] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 71.0, 1.3, 65.0, 123.0, 171.0, 3.0, -0.7882455963450975, 20.0, 20.94481426007687, 22.7, 1.0, 43.32909470896461], 
current ob forecast is [], 
actual action is [3.0, 20.0], 
sim time this is 4609800.0000, 
sim time next is 4610700.0000, 
raw observation next is [-2.0, 71.0, 1.95, 97.5, 133.25, 255.5, 3.0, -0.8014520006227904, 20.0, 20.86061436496796, 22.7, 1.0, 31.011773824117892], 
processed observation next is [1.0, 0.34782608695652173, 0.28205128205128205, 0.71, 0.17727272727272728, 0.2708333333333333, 0.3525132275132275, 0.2555, 0.55, 0.23284933312573652, 0.0, 0.4086591949954231, 0.6714285714285714, 1.0, 0.3648443979307987], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1586508], dtype=float32), -0.51650935]. 
=============================================
[2019-05-04 22:03:16,924] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5281467e-17 9.9992859e-01 1.1957217e-23 2.8182119e-24 1.3045308e-30
 3.4720983e-31 8.2192975e-22 1.3677730e-30 4.9855489e-19 4.2902858e-15
 7.1401446e-05], sum to 1.0000
[2019-05-04 22:03:16,925] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0208
[2019-05-04 22:03:16,940] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.5, 49.0, 2.1, 65.0, 120.0, 859.0, 8.25, -0.5579208574790814, 20.0, 21.54089299138757, 22.7, 1.0, 2.9396413084082718], 
current ob forecast is [], 
actual action is [8.5, 20.0], 
sim time this is 4624200.0000, 
sim time next is 4625100.0000, 
raw observation next is [3.75, 49.0, 2.1, 57.5, 124.75, 847.5, 8.5, -0.54824301468419, 20.0, 21.57270376695349, 22.7, 1.0, 1.8043496018087246], 
processed observation next is [1.0, 0.5217391304347826, 0.42948717948717946, 0.49, 0.19090909090909092, 0.1597222222222222, 0.330026455026455, 0.8475, 0.6416666666666667, 0.31725232843860335, 0.0, 0.5103862524219274, 0.6714285714285714, 1.0, 0.021227642374220288], 
reward next is 0.9111, 
noisyNet noise sample is [array([1.8434752], dtype=float32), -0.8859154]. 
=============================================
[2019-05-04 22:03:18,511] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4010576e-14 9.3356502e-01 2.9828795e-20 2.4103524e-21 1.0622742e-25
 1.6939554e-28 4.1048463e-19 1.6439775e-25 3.1816203e-15 1.4240062e-14
 6.6435039e-02], sum to 1.0000
[2019-05-04 22:03:18,512] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6544
[2019-05-04 22:03:18,540] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 50.0, 0.0, 0.0, 199.0, 364.0, 9.925, -0.3018213828590811, 20.0, 22.76496188027171, 22.7, 1.0, 1.967790117265871], 
current ob forecast is [], 
actual action is [10.0, 20.0], 
sim time this is 4633200.0000, 
sim time next is 4634100.0000, 
raw observation next is [5.25, 48.25, 0.0, 0.0, 197.0, 246.5, 10.0, -0.3156321932631991, 20.0, 22.7547862007049, 22.7, 1.0, 1.8453711569585132], 
processed observation next is [1.0, 0.6521739130434783, 0.46794871794871795, 0.4825, 0.0, 0.0, 0.5211640211640212, 0.2465, 0.6666666666666666, 0.39478926891226696, 0.0, 0.6792551715292713, 0.6714285714285714, 1.0, 0.02171024890539427], 
reward next is 0.9283, 
noisyNet noise sample is [array([-0.12989767], dtype=float32), 1.0578709]. 
=============================================
[2019-05-04 22:03:24,559] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.31109824e-08 9.70645964e-01 3.87611367e-11 3.67665273e-12
 4.52998249e-16 1.32880626e-17 1.94587506e-11 2.14032753e-15
 1.17044934e-08 1.32313955e-08 2.93539632e-02], sum to 1.0000
[2019-05-04 22:03:24,559] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3708
[2019-05-04 22:03:24,694] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.25, 50.75, 3.225, 360.0, 145.25, 757.25, 2.5, -0.5124198669647314, 65.0, 21.95097812920191, 22.7, 1.0, 69.03706036106722], 
current ob forecast is [], 
actual action is [2.75, 20.0], 
sim time this is 4790700.0000, 
sim time next is 4791600.0000, 
raw observation next is [-2.0, 46.0, 3.1, 360.0, 137.5, 784.5, 2.75, -0.5041572252812411, 20.0, 22.08585152761213, 22.7, 1.0, 43.08458651979305], 
processed observation next is [0.0, 0.4782608695652174, 0.28205128205128205, 0.46, 0.2818181818181818, 1.0, 0.3637566137566138, 0.7845, 0.5458333333333333, 0.33194759157291964, 0.0, 0.5836930753731616, 0.6714285714285714, 1.0, 0.5068774884681535], 
reward next is 0.4547, 
noisyNet noise sample is [array([1.321503], dtype=float32), 0.0066103255]. 
=============================================
[2019-05-04 22:03:30,985] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8385353e-14 1.0000000e+00 2.6319528e-18 2.5290204e-18 6.8268756e-24
 1.5716353e-25 9.5358301e-19 1.1045452e-23 1.5000544e-13 3.1900013e-14
 1.7527727e-08], sum to 1.0000
[2019-05-04 22:03:30,988] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8902
[2019-05-04 22:03:31,037] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 46.0, 3.1, 360.0, 137.5, 784.5, 2.75, -0.8016066134751311, 20.0, 20.50624395381755, 22.7, 1.0, 11.2162195272924], 
current ob forecast is [], 
actual action is [3.0, 20.0], 
sim time this is 4791600.0000, 
sim time next is 4792500.0000, 
raw observation next is [-1.25, 45.25, 3.1, 360.0, 129.75, 811.75, 3.0, -0.8073639761960515, 20.0, 20.44694126221524, 22.7, 1.0, 8.729737737699217], 
processed observation next is [0.0, 0.4782608695652174, 0.30128205128205127, 0.4525, 0.2818181818181818, 1.0, 0.34325396825396826, 0.81175, 0.55, 0.23087867460131617, 0.0, 0.34956303745932005, 0.6714285714285714, 1.0, 0.10270279691410844], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.9666736], dtype=float32), -1.1757774]. 
=============================================
[2019-05-04 22:03:31,070] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.634605]
 [78.85239 ]
 [78.89304 ]
 [78.251045]
 [77.377754]], R is [[77.88574982]
 [77.10689545]
 [76.33583069]
 [75.57247162]
 [74.81674957]].
[2019-05-04 22:03:33,611] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6094426e-09 8.5649389e-01 2.3488377e-12 2.3816255e-12 1.0180622e-16
 5.2097969e-18 2.3108494e-11 2.4766920e-16 6.9035988e-10 1.3711958e-10
 1.4350612e-01], sum to 1.0000
[2019-05-04 22:03:33,611] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3414
[2019-05-04 22:03:33,646] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.75, 35.5, 3.975, 112.5, 45.25, 276.0, 8.0, -0.5237170543845668, 20.0, 21.82526098312062, 22.7, 1.0, 7.1164208515378276], 
current ob forecast is [], 
actual action is [7.75, 20.0], 
sim time this is 4814100.0000, 
sim time next is 4815000.0000, 
raw observation next is [2.5, 37.0, 3.35, 195.0, 33.0, 185.0, 7.75, -0.5519238792613346, 20.0, 21.64376908354668, 22.7, 1.0, 6.332404165403233], 
processed observation next is [0.0, 0.7391304347826086, 0.3974358974358974, 0.37, 0.30454545454545456, 0.5416666666666666, 0.0873015873015873, 0.185, 0.6291666666666667, 0.3160253735795551, 0.0, 0.5205384405066685, 0.6714285714285714, 1.0, 0.07449887253415569], 
reward next is 0.8404, 
noisyNet noise sample is [array([1.7755514], dtype=float32), -0.48420358]. 
=============================================
[2019-05-04 22:03:33,668] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[57.766323]
 [58.30381 ]
 [58.765617]
 [59.260834]
 [59.54594 ]], R is [[57.35876465]
 [57.65325165]
 [57.92546463]
 [58.17123413]
 [58.38361359]].
[2019-05-04 22:03:35,837] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5500, global step 86628: loss 0.2125
[2019-05-04 22:03:35,839] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5500, global step 86628: learning rate 0.0001
[2019-05-04 22:03:36,134] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 86712: loss 0.0081
[2019-05-04 22:03:36,152] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 86712: learning rate 0.0001
[2019-05-04 22:03:42,817] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3501584e-16 1.0000000e+00 1.4664680e-18 7.2902562e-21 1.2831998e-27
 1.4412379e-28 3.9773550e-19 4.1807838e-25 1.9807715e-16 1.4599549e-17
 1.7713075e-08], sum to 1.0000
[2019-05-04 22:03:42,830] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0222
[2019-05-04 22:03:42,844] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87731: loss 0.8503
[2019-05-04 22:03:42,844] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87731: learning rate 0.0001
[2019-05-04 22:03:42,848] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 36.0, 2.6, 67.5, 0.0, 0.0, 6.0, -0.5446481844852641, 20.0, 21.90849914851588, 21.0, 0.0, 5.7963124109630435], 
current ob forecast is [], 
actual action is [6.0, 20.0], 
sim time this is 4916700.0000, 
sim time next is 4917600.0000, 
raw observation next is [1.0, 36.0, 2.6, 70.0, 0.0, 0.0, 6.0, -0.5630241135266503, 20.0, 21.83410734325543, 21.0, 0.0, 5.142027507398557], 
processed observation next is [0.0, 0.9565217391304348, 0.358974358974359, 0.36, 0.23636363636363636, 0.19444444444444445, 0.0, 0.0, 0.6, 0.3123252954911166, 0.0, 0.5477296204650612, 0.42857142857142855, 0.0, 0.06049444126351243], 
reward next is 0.8895, 
noisyNet noise sample is [array([-0.19479024], dtype=float32), -0.9326095]. 
=============================================
[2019-05-04 22:03:43,144] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4453742e-11 9.9995852e-01 1.1225736e-15 4.5899669e-15 1.1706238e-21
 8.9304709e-22 9.1795705e-15 1.1655898e-19 1.0266259e-12 2.5792113e-11
 4.1462597e-05], sum to 1.0000
[2019-05-04 22:03:43,147] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1991
[2019-05-04 22:03:43,170] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 39.0, 2.975, 60.0, 0.0, 0.0, 6.0, -0.4955179670941738, 20.0, 22.09120647323637, 21.0, 0.0, 5.0066404649410226], 
current ob forecast is [], 
actual action is [6.0, 20.0], 
sim time this is 4911300.0000, 
sim time next is 4912200.0000, 
raw observation next is [1.0, 38.0, 2.85, 60.0, 0.0, 0.0, 6.0, -0.5122615854121276, 20.0, 22.02550102196094, 21.0, 0.0, 4.629798195450374], 
processed observation next is [0.0, 0.8695652173913043, 0.358974358974359, 0.38, 0.2590909090909091, 0.16666666666666666, 0.0, 0.0, 0.6, 0.32924613819595744, 0.0, 0.5750715745658488, 0.42857142857142855, 0.0, 0.05446821406412205], 
reward next is 0.8955, 
noisyNet noise sample is [array([0.01560193], dtype=float32), -0.11273484]. 
=============================================
[2019-05-04 22:03:43,250] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87923: loss 0.4379
[2019-05-04 22:03:43,252] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87925: learning rate 0.0001
[2019-05-04 22:03:43,410] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87996: loss 0.0655
[2019-05-04 22:03:43,411] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87996: learning rate 0.0001
[2019-05-04 22:03:43,492] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 88040: loss 0.1517
[2019-05-04 22:03:43,494] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 88040: learning rate 0.0001
[2019-05-04 22:03:43,494] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88041: loss -0.8940
[2019-05-04 22:03:43,496] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88042: learning rate 0.0001
[2019-05-04 22:03:43,513] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 88047: loss 0.0115
[2019-05-04 22:03:43,514] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 88047: learning rate 0.0001
[2019-05-04 22:03:43,529] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88051: loss -0.2069
[2019-05-04 22:03:43,533] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88054: learning rate 0.0001
[2019-05-04 22:03:43,533] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 88054: loss -1.5106
[2019-05-04 22:03:43,536] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 88055: learning rate 0.0001
[2019-05-04 22:03:43,867] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 88158: loss 0.1729
[2019-05-04 22:03:43,868] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 88158: learning rate 0.0001
[2019-05-04 22:03:43,984] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88189: loss 0.2375
[2019-05-04 22:03:43,987] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88191: learning rate 0.0001
[2019-05-04 22:03:44,077] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88211: loss 0.1359
[2019-05-04 22:03:44,078] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88212: learning rate 0.0001
[2019-05-04 22:03:44,164] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5500, global step 88222: loss 0.0177
[2019-05-04 22:03:44,167] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5500, global step 88222: learning rate 0.0001
[2019-05-04 22:03:44,561] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88276: loss 0.0147
[2019-05-04 22:03:44,583] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88276: learning rate 0.0001
[2019-05-04 22:03:44,661] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88287: loss -4.3129
[2019-05-04 22:03:44,661] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88287: learning rate 0.0001
[2019-05-04 22:03:55,326] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.00795349e-12 9.99997377e-01 4.84850386e-18 8.02438400e-18
 2.30275637e-24 1.17237526e-23 2.02844969e-17 6.09846909e-23
 1.00961436e-15 4.24013513e-14 2.57954684e-06], sum to 1.0000
[2019-05-04 22:03:55,343] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1384
[2019-05-04 22:03:55,369] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.0, 72.0, 5.1, 210.0, 206.5, 245.0, 17.5, -0.4538643557052129, 20.0, 21.73284762320236, 22.2, 1.0, 6.021052630409194], 
current ob forecast is [], 
actual action is [18.0, 20.0], 
sim time this is 5148000.0000, 
sim time next is 5148900.0000, 
raw observation next is [13.25, 69.75, 5.5, 215.0, 148.75, 122.5, 18.0, -0.4201670242999049, 20.0, 22.03692917612014, 22.2, 1.0, 5.313612059723717], 
processed observation next is [1.0, 0.6086956521739131, 0.6730769230769231, 0.6975, 0.5, 0.5972222222222222, 0.39351851851851855, 0.1225, 0.8, 0.359944325233365, 0.0, 0.5767041680171628, 0.5999999999999999, 1.0, 0.06251308305557314], 
reward next is 0.8875, 
noisyNet noise sample is [array([-0.60394216], dtype=float32), -0.37647635]. 
=============================================
[2019-05-04 22:03:56,676] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4997772e-13 1.0000000e+00 1.8401227e-16 5.8351962e-16 6.0084566e-21
 1.9354358e-21 2.5081386e-14 7.2000192e-21 3.9785658e-13 2.9874681e-13
 1.0494870e-08], sum to 1.0000
[2019-05-04 22:03:56,680] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3684
[2019-05-04 22:03:56,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.75, 54.0, 5.550000000000001, 295.0, 0.0, 0.0, 12.0, -0.6714653324549368, 20.0, 20.488250639564, 19.4, 0.0, 5.032824035075618], 
current ob forecast is [], 
actual action is [11.75, 20.0], 
sim time this is 5181300.0000, 
sim time next is 5182200.0000, 
raw observation next is [6.5, 56.0, 5.4, 290.0, 0.0, 0.0, 11.75, -0.682725411885313, 20.0, 20.44666394488158, 19.4, 0.0, 5.103613831571451], 
processed observation next is [1.0, 1.0, 0.5, 0.56, 0.49090909090909096, 0.8055555555555556, 0.0, 0.0, 0.6958333333333333, 0.2724248627048957, 0.0, 0.34952342069736836, 0.1999999999999998, 0.0, 0.060042515665546484], 
reward next is 0.8900, 
noisyNet noise sample is [array([2.1695333], dtype=float32), 0.4211635]. 
=============================================
[2019-05-04 22:03:57,036] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.0338867e-14 1.0000000e+00 3.7369816e-17 4.6018092e-18 4.3709947e-23
 2.1401897e-23 1.0593452e-16 1.7981766e-22 5.9880445e-14 2.2293440e-13
 8.0558443e-10], sum to 1.0000
[2019-05-04 22:03:57,038] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4615
[2019-05-04 22:03:57,064] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.75, 54.0, 5.550000000000001, 295.0, 0.0, 0.0, 12.0, -0.6702959600125352, 20.0, 20.49067727208051, 19.4, 0.0, 5.030043203324423], 
current ob forecast is [], 
actual action is [11.75, 20.0], 
sim time this is 5181300.0000, 
sim time next is 5182200.0000, 
raw observation next is [6.5, 56.0, 5.4, 290.0, 0.0, 0.0, 11.75, -0.6815700706136698, 20.0, 20.44906479569396, 19.4, 0.0, 5.100877248327518], 
processed observation next is [1.0, 1.0, 0.5, 0.56, 0.49090909090909096, 0.8055555555555556, 0.0, 0.0, 0.6958333333333333, 0.2728099764621101, 0.0, 0.3498663993848515, 0.1999999999999998, 0.0, 0.06001032056855904], 
reward next is 0.8900, 
noisyNet noise sample is [array([-0.4746366], dtype=float32), -0.37368762]. 
=============================================
[2019-05-04 22:03:59,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.36622116e-12 9.95604753e-01 5.71365479e-17 3.60760067e-17
 1.62585731e-23 3.48392057e-23 1.13238914e-15 1.19677575e-23
 2.73588010e-12 4.37739983e-11 4.39525442e-03], sum to 1.0000
[2019-05-04 22:03:59,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3683
[2019-05-04 22:03:59,175] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 80.0, 5.1, 300.0, 96.0, 0.0, 6.75, -0.5674433967371123, 20.0, 21.58051060007627, 22.2, 1.0, 32.27043628568435], 
current ob forecast is [], 
actual action is [7.0, 20.0], 
sim time this is 5216400.0000, 
sim time next is 5217300.0000, 
raw observation next is [1.75, 81.5, 4.975, 295.0, 95.0, 0.0, 7.0, -0.5649453597017015, 20.0, 21.57145391983178, 22.2, 1.0, 25.181476592445808], 
processed observation next is [1.0, 0.391304347826087, 0.3782051282051282, 0.815, 0.4522727272727272, 0.8194444444444444, 0.25132275132275134, 0.0, 0.6166666666666667, 0.31168488009943285, 0.0, 0.5102077028331112, 0.5999999999999999, 1.0, 0.2962526657934801], 
reward next is 0.6688, 
noisyNet noise sample is [array([0.70587623], dtype=float32), -0.6299293]. 
=============================================
[2019-05-04 22:04:07,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7928539e-14 1.0000000e+00 1.6357776e-16 9.1106930e-17 1.9888518e-22
 1.1546741e-22 6.1504190e-16 5.9636966e-22 5.8269497e-14 7.7318760e-14
 5.3643898e-08], sum to 1.0000
[2019-05-04 22:04:07,905] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8040
[2019-05-04 22:04:07,921] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.5, 68.5, 4.6, 325.0, 0.0, 0.0, 3.75, -0.573015536860141, 20.0, 21.25197249956906, 19.4, 0.0, 7.455924326219663], 
current ob forecast is [], 
actual action is [3.5, 20.0], 
sim time this is 5279400.0000, 
sim time next is 5280300.0000, 
raw observation next is [-1.75, 69.75, 4.35, 322.5, 0.0, 0.0, 3.5, -0.5909515618982549, 20.0, 21.17806408826462, 19.4, 0.0, 7.593498128359793], 
processed observation next is [1.0, 0.08695652173913043, 0.28846153846153844, 0.6975, 0.39545454545454545, 0.8958333333333334, 0.0, 0.0, 0.5583333333333333, 0.303016146033915, 0.0, 0.4540091554663742, 0.1999999999999998, 0.0, 0.08933527209835052], 
reward next is 0.8607, 
noisyNet noise sample is [array([-0.09845297], dtype=float32), 0.042188182]. 
=============================================
[2019-05-04 22:04:08,018] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0499452e-13 9.9999285e-01 6.7983457e-16 2.8299103e-16 8.6893869e-22
 1.7280507e-22 2.5732359e-15 4.4563786e-21 1.2436765e-14 2.9468934e-12
 7.1799800e-06], sum to 1.0000
[2019-05-04 22:04:08,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5216
[2019-05-04 22:04:08,037] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 76.25, 4.1, 330.0, 0.0, 0.0, 3.0, -0.6186753651823657, 20.0, 21.10403466693084, 19.4, 0.0, 8.153134525275084], 
current ob forecast is [], 
actual action is [3.0, 20.0], 
sim time this is 5285700.0000, 
sim time next is 5286600.0000, 
raw observation next is [-2.0, 74.5, 4.1, 330.0, 0.0, 0.0, 3.0, -0.634402656233601, 20.0, 21.03923952940324, 19.4, 0.0, 8.21765717082087], 
processed observation next is [1.0, 0.17391304347826086, 0.28205128205128205, 0.745, 0.3727272727272727, 0.9166666666666666, 0.0, 0.0, 0.55, 0.288532447922133, 0.0, 0.43417707562903424, 0.1999999999999998, 0.0, 0.09667831965671611], 
reward next is 0.8533, 
noisyNet noise sample is [array([-0.79437494], dtype=float32), -0.037045684]. 
=============================================
[2019-05-04 22:04:08,180] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6000, global step 94335: loss 2.4014
[2019-05-04 22:04:08,180] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6000, global step 94335: learning rate 0.0001
[2019-05-04 22:04:08,316] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 94369: loss 2.3462
[2019-05-04 22:04:08,321] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 94369: learning rate 0.0001
[2019-05-04 22:04:11,083] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0043893e-13 9.9998331e-01 2.8712898e-17 2.2244545e-18 1.1417764e-24
 7.8213315e-26 2.4820238e-17 2.1992202e-24 5.3936437e-15 9.5814426e-13
 1.6728394e-05], sum to 1.0000
[2019-05-04 22:04:11,085] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2555
[2019-05-04 22:04:11,136] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.0, 34.75, 3.6, 307.5, 126.25, 764.25, 12.0, -0.02689385312810159, 20.0, 23.83237143830565, 22.2, 1.0, 0.9117664240019149], 
current ob forecast is [], 
actual action is [12.0, 20.0], 
sim time this is 5323500.0000, 
sim time next is 5324400.0000, 
raw observation next is [7.0, 34.0, 3.6, 310.0, 122.5, 744.5, 12.0, -0.05009658090228705, 20.0, 23.56614201475274, 22.2, 1.0, 0.8044693240577901], 
processed observation next is [1.0, 0.6521739130434783, 0.5128205128205128, 0.34, 0.32727272727272727, 0.8611111111111112, 0.32407407407407407, 0.7445, 0.7, 0.48330113969923766, 0.0, 0.795163144964677, 0.5999999999999999, 1.0, 0.009464344988915178], 
reward next is 0.9405, 
noisyNet noise sample is [array([0.22387986], dtype=float32), -1.1969687]. 
=============================================
[2019-05-04 22:04:12,977] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6693086e-14 9.9999988e-01 6.7338094e-18 1.0264733e-18 5.7839782e-24
 1.4738598e-25 1.4733422e-18 2.4743432e-23 3.9695716e-15 2.6746177e-15
 1.7008516e-07], sum to 1.0000
[2019-05-04 22:04:12,979] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8575
[2019-05-04 22:04:12,996] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 56.25, 0.0, 0.0, 0.0, 0.0, 4.0, -0.7837973264901663, 20.0, 20.47590380745274, 19.4, 0.0, 6.751303940913837], 
current ob forecast is [], 
actual action is [4.0, 20.0], 
sim time this is 5375700.0000, 
sim time next is 5376600.0000, 
raw observation next is [-1.0, 57.5, 0.0, 0.0, 0.0, 0.0, 4.0, -0.7989090878734141, 20.0, 20.4170721259995, 19.4, 0.0, 6.761869216421762], 
processed observation next is [0.0, 0.21739130434782608, 0.3076923076923077, 0.575, 0.0, 0.0, 0.0, 0.0, 0.5666666666666667, 0.23369697070886197, 0.0, 0.34529601799992854, 0.1999999999999998, 0.0, 0.07955140254613838], 
reward next is 0.8704, 
noisyNet noise sample is [array([1.5794511], dtype=float32), 0.99652547]. 
=============================================
[2019-05-04 22:04:13,685] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 96106: loss 1.1091
[2019-05-04 22:04:13,688] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 96107: learning rate 0.0001
[2019-05-04 22:04:14,000] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96200: loss 5.3499
[2019-05-04 22:04:14,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96201: learning rate 0.0001
[2019-05-04 22:04:14,021] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96209: loss 8.8196
[2019-05-04 22:04:14,022] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96209: learning rate 0.0001
[2019-05-04 22:04:14,028] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96210: loss 4.6132
[2019-05-04 22:04:14,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96210: learning rate 0.0001
[2019-05-04 22:04:14,096] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96231: loss 0.2158
[2019-05-04 22:04:14,097] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 96231: loss -1.6753
[2019-05-04 22:04:14,099] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 96233: learning rate 0.0001
[2019-05-04 22:04:14,101] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96233: learning rate 0.0001
[2019-05-04 22:04:14,299] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96289: loss 0.7634
[2019-05-04 22:04:14,300] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96289: learning rate 0.0001
[2019-05-04 22:04:14,415] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 96329: loss 1.7745
[2019-05-04 22:04:14,420] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 96331: learning rate 0.0001
[2019-05-04 22:04:14,449] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96340: loss -2.6481
[2019-05-04 22:04:14,454] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96341: learning rate 0.0001
[2019-05-04 22:04:14,492] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96354: loss 2.4305
[2019-05-04 22:04:14,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96354: learning rate 0.0001
[2019-05-04 22:04:14,970] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 96534: loss 0.4285
[2019-05-04 22:04:14,976] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 96535: learning rate 0.0001
[2019-05-04 22:04:14,977] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6000, global step 96535: loss 0.5297
[2019-05-04 22:04:14,981] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6000, global step 96535: learning rate 0.0001
[2019-05-04 22:04:15,084] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 96578: loss 0.2709
[2019-05-04 22:04:15,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 96578: learning rate 0.0001
[2019-05-04 22:04:15,160] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96605: loss 0.3107
[2019-05-04 22:04:15,161] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96607: learning rate 0.0001
[2019-05-04 22:04:15,622] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.8226792e-18 1.0000000e+00 2.7690527e-22 2.7534095e-23 1.1317413e-28
 1.5290285e-31 5.9719668e-20 1.7543586e-30 1.6444726e-18 5.5054630e-17
 1.1741367e-09], sum to 1.0000
[2019-05-04 22:04:15,622] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9989
[2019-05-04 22:04:15,626] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6412863e-16 9.9999988e-01 2.1307352e-20 1.5918949e-20 8.7228280e-29
 2.3296605e-29 1.1442268e-19 4.2312045e-28 5.8890627e-17 7.6867986e-16
 9.3786987e-08], sum to 1.0000
[2019-05-04 22:04:15,626] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4956
[2019-05-04 22:04:15,655] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.75, 25.75, 3.85, 292.5, 129.5, 441.25, 17.0, -0.2865281175139153, 20.0, 22.74175962071637, 20.56, 1.0, 0.445655395249911], 
current ob forecast is [], 
actual action is [16.75, 20.0], 
sim time this is 5415300.0000, 
sim time next is 5416200.0000, 
raw observation next is [11.5, 27.5, 4.1, 285.0, 125.0, 371.0, 16.75, -0.2822907010608567, 20.0, 22.76910542524832, 20.56, 1.0, 0.2839842220767337], 
processed observation next is [0.0, 0.6956521739130435, 0.6282051282051282, 0.275, 0.3727272727272727, 0.7916666666666666, 0.3306878306878307, 0.371, 0.7791666666666667, 0.4059030996463811, 0.0, 0.6813007750354743, 0.36571428571428555, 1.0, 0.0033409908479615728], 
reward next is 0.9467, 
noisyNet noise sample is [array([-0.45802826], dtype=float32), 0.67743486]. 
=============================================
[2019-05-04 22:04:15,656] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.75, 25.75, 3.85, 292.5, 129.5, 441.25, 17.0, -0.3197090137671111, 20.0, 22.58764994358987, 20.56, 1.0, 0.0], 
current ob forecast is [], 
actual action is [16.75, 20.0], 
sim time this is 5415300.0000, 
sim time next is 5416200.0000, 
raw observation next is [11.5, 27.5, 4.1, 285.0, 125.0, 371.0, 16.75, -0.3154287388401764, 20.0, 22.61431236098205, 20.56, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6282051282051282, 0.275, 0.3727272727272727, 0.7916666666666666, 0.3306878306878307, 0.371, 0.7791666666666667, 0.39485708705327455, 0.0, 0.659187480140293, 0.36571428571428555, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-0.11254267], dtype=float32), -2.5883303]. 
=============================================
[2019-05-04 22:04:19,354] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2658937e-13 9.9999845e-01 2.2981511e-17 6.1492097e-17 7.7686478e-24
 5.8525961e-24 2.2968905e-16 2.4362666e-23 4.6773989e-15 1.2178416e-13
 1.5139492e-06], sum to 1.0000
[2019-05-04 22:04:19,356] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6124
[2019-05-04 22:04:19,373] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.5, 36.5, 6.2, 330.0, 89.0, 577.0, 18.75, -0.2023311573729177, 20.0, 22.99369739641879, 20.56, 1.0, 0.0], 
current ob forecast is [], 
actual action is [18.5, 20.0], 
sim time this is 5502600.0000, 
sim time next is 5503500.0000, 
raw observation next is [13.25, 37.25, 6.45, 335.0, 78.0, 514.75, 18.5, -0.1963093985960377, 20.0, 23.02888428509208, 20.56, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6730769230769231, 0.3725, 0.5863636363636364, 0.9305555555555556, 0.20634920634920634, 0.51475, 0.8083333333333333, 0.43456353380132073, 0.0, 0.7184120407274399, 0.36571428571428555, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([1.0222856], dtype=float32), 0.19441138]. 
=============================================
[2019-05-04 22:04:19,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[71.686966]
 [71.910614]
 [72.09069 ]
 [72.30149 ]
 [72.41356 ]], R is [[71.54064941]
 [71.77523804]
 [72.00748444]
 [72.23740387]
 [72.46082306]].
[2019-05-04 22:04:23,842] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-05-04 22:04:23,844] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation job starts!
[2019-05-04 22:04:23,844] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:04:23,844] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation job starts!
[2019-05-04 22:04:23,845] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:04:23,854] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Test-Repeat-Real97-v1-res1/Eplus-env-sub_run3
[2019-05-04 22:04:23,871] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res1/Eplus-env-sub_run3
[2019-05-04 22:05:59,216] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation: average rewards by now are 4595.7612 73651.5758 272758.3191
[2019-05-04 22:05:59,227] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:05:59,227] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:05:59,227] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:05:59,336] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:05:59,336] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:05:59,336] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:07:31,629] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:NoisyNet noise sample: [array([4.788948e-09], dtype=float32), 0.09948877]
[2019-05-04 22:07:31,629] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:Observation this: [2.75, 80.75, 3.1, 295.0, 163.0, 261.5, 7.5, -0.980395474867701, 20.0, 19.25106910575979, 22.2, 1.0, 6.343148265527371]
[2019-05-04 22:07:31,629] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:Observation forecast: []
[2019-05-04 22:07:31,630] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Softmax [2.8398967e-12 2.9130206e-01 2.6522355e-15 3.4671581e-16 1.6121353e-20
 2.2124462e-21 2.4306737e-15 3.2963105e-20 5.1797659e-12 8.3020188e-12
 7.0869792e-01], sampled 0.07316615350444422
[2019-05-04 22:07:33,668] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation: average rewards by now are 3543.5555 133705.6126 -3696.4564
[2019-05-04 22:07:33,678] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:07:33,678] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:07:33,678] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:07:33,795] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:07:33,795] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:07:33,795] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:07:34,681] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 100000, evaluation results [100000.0, 3543.55548027374, 133705.61260275578, -3696.4564447771113, 4595.761158783212, 73651.57584369082, 272758.3191094142]
[2019-05-04 22:07:38,634] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0078091e-23 1.0000000e+00 1.3233291e-24 1.7925784e-25 8.6782046e-33
 2.7595274e-32 5.2736678e-25 4.7945920e-32 7.0123889e-21 2.4333576e-20
 4.5717495e-18], sum to 1.0000
[2019-05-04 22:07:38,635] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6609
[2019-05-04 22:07:38,653] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.75, 59.0, 4.475, 250.0, 0.0, 0.0, 23.0, 0.2191191314707129, 20.0, 24.02003436579616, 19.4, 0.0, 0.0], 
current ob forecast is [], 
actual action is [22.75, 20.0], 
sim time this is 5699700.0000, 
sim time next is 5700600.0000, 
raw observation next is [17.5, 59.0, 4.35, 250.0, 0.0, 0.0, 22.75, 0.211867605134969, 20.0, 23.98861942141357, 19.4, 0.0, 0.0], 
processed observation next is [1.0, 1.0, 0.782051282051282, 0.59, 0.39545454545454545, 0.6944444444444444, 0.0, 0.0, 0.8791666666666667, 0.5706225350449897, 0.0, 0.8555170602019386, 0.1999999999999998, 0.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([0.4237134], dtype=float32), 0.65950346]. 
=============================================
[2019-05-04 22:07:39,615] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6500, global step 102068: loss 0.1325
[2019-05-04 22:07:39,618] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6500, global step 102068: learning rate 0.0001
[2019-05-04 22:07:39,982] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 102217: loss 0.2037
[2019-05-04 22:07:39,984] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 102217: learning rate 0.0001
[2019-05-04 22:07:41,451] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3055434e-18 1.0000000e+00 2.0697448e-21 2.0204558e-23 1.3155143e-29
 5.5334110e-29 1.3191780e-21 1.5841656e-27 2.8083872e-18 3.2899947e-18
 3.3602428e-15], sum to 1.0000
[2019-05-04 22:07:41,454] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5024
[2019-05-04 22:07:41,469] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 35.5, 5.7, 230.0, 121.5, 735.25, 30.0, 0.9198797762314571, 20.0, 26.71121720651689, 22.2, 1.0, 0.0], 
current ob forecast is [], 
actual action is [30.0, 20.0], 
sim time this is 5757300.0000, 
sim time next is 5758200.0000, 
raw observation next is [25.5, 35.0, 5.7, 230.0, 118.0, 717.0, 30.0, 0.9664590194899215, 20.0, 27.17492597717421, 22.2, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.9871794871794872, 0.35, 0.5181818181818182, 0.6388888888888888, 0.31216931216931215, 0.717, 1.0, 0.8221530064966406, 0.0, 1.3107037110248874, 0.5999999999999999, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-1.6178111], dtype=float32), -1.7842879]. 
=============================================
[2019-05-04 22:07:41,686] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6374344e-19 1.0000000e+00 5.5773563e-24 7.4818773e-26 4.0227233e-32
 1.0927176e-32 3.5886993e-23 1.0117612e-29 1.5453983e-19 4.4550552e-19
 3.9736436e-17], sum to 1.0000
[2019-05-04 22:07:41,688] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1334
[2019-05-04 22:07:41,703] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 40.25, 3.475, 222.5, 0.0, 0.0, 27.5, 1.025135975563262, 20.0, 27.12698781265242, 22.2, 1.0, 0.0], 
current ob forecast is [], 
actual action is [27.25, 20.0], 
sim time this is 5769900.0000, 
sim time next is 5770800.0000, 
raw observation next is [22.0, 41.0, 3.6, 220.0, 0.0, 0.0, 27.25, 1.024699598655438, 20.0, 27.11663308707423, 22.2, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8974358974358975, 0.41, 0.32727272727272727, 0.6111111111111112, 0.0, 0.0, 0.9541666666666667, 0.841566532885146, 0.0, 1.3023761552963187, 0.5999999999999999, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([0.04653124], dtype=float32), 1.2861179]. 
=============================================
[2019-05-04 22:07:42,377] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9585572e-20 1.0000000e+00 1.5368038e-26 4.0500818e-26 3.6984902e-33
 1.6523572e-34 1.6787642e-24 2.5496714e-34 6.0845984e-20 2.2842096e-21
 1.0777411e-15], sum to 1.0000
[2019-05-04 22:07:42,380] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9083
[2019-05-04 22:07:42,398] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.0, 59.0, 5.7, 200.0, 61.0, 61.0, 22.0, 0.5783748448840084, 20.0, 25.44869769384271, 22.2, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0, 20.0], 
sim time this is 5817600.0000, 
sim time next is 5818500.0000, 
raw observation next is [17.0, 59.0, 5.425000000000001, 197.5, 56.0, 30.5, 22.0, 0.6332934633924759, 20.0, 25.72924682087002, 22.2, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7692307692307693, 0.59, 0.4931818181818182, 0.5486111111111112, 0.14814814814814814, 0.0305, 0.8666666666666667, 0.7110978211308253, 0.0, 1.1041781172671459, 0.5999999999999999, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([0.81446755], dtype=float32), -0.09130623]. 
=============================================
[2019-05-04 22:07:42,426] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[ 97.4986  ]
 [ 97.649536]
 [ 99.48369 ]
 [101.02983 ]
 [ 99.15064 ]], R is [[97.68080139]
 [97.6539917 ]
 [97.62744904]
 [97.6011734 ]
 [97.57515717]].
[2019-05-04 22:07:43,879] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 103915: loss 5.0500
[2019-05-04 22:07:43,882] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 103916: learning rate 0.0001
[2019-05-04 22:07:44,063] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103999: loss 7.7812
[2019-05-04 22:07:44,065] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103999: learning rate 0.0001
[2019-05-04 22:07:44,274] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 104077: loss 8.9304
[2019-05-04 22:07:44,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 104077: learning rate 0.0001
[2019-05-04 22:07:44,290] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104084: loss 8.4715
[2019-05-04 22:07:44,291] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104084: learning rate 0.0001
[2019-05-04 22:07:44,451] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104145: loss 6.8379
[2019-05-04 22:07:44,453] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104146: learning rate 0.0001
[2019-05-04 22:07:44,517] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104172: loss 5.7132
[2019-05-04 22:07:44,518] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104172: learning rate 0.0001
[2019-05-04 22:07:44,821] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104300: loss 2.3647
[2019-05-04 22:07:44,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104300: learning rate 0.0001
[2019-05-04 22:07:44,961] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104353: loss 2.7127
[2019-05-04 22:07:44,963] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104353: learning rate 0.0001
[2019-05-04 22:07:44,974] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104361: loss 3.8367
[2019-05-04 22:07:44,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104364: learning rate 0.0001
[2019-05-04 22:07:45,089] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 104406: loss 2.8838
[2019-05-04 22:07:45,090] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 104406: learning rate 0.0001
[2019-05-04 22:07:45,134] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104426: loss 3.5639
[2019-05-04 22:07:45,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104427: learning rate 0.0001
[2019-05-04 22:07:45,331] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104516: loss 0.5996
[2019-05-04 22:07:45,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104516: learning rate 0.0001
[2019-05-04 22:07:45,511] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6500, global step 104596: loss 1.4917
[2019-05-04 22:07:45,512] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6500, global step 104596: learning rate 0.0001
[2019-05-04 22:07:45,683] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 104673: loss 2.1312
[2019-05-04 22:07:45,687] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 104674: learning rate 0.0001
[2019-05-04 22:07:49,567] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3068107e-15 1.0000000e+00 1.8718079e-20 5.2766795e-20 5.0963003e-27
 9.2343661e-28 1.5684537e-19 1.0439916e-25 3.8086089e-17 1.2402256e-16
 3.5890086e-11], sum to 1.0000
[2019-05-04 22:07:49,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6072
[2019-05-04 22:07:49,581] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 86.0, 2.85, 17.5, 0.0, 0.0, 6.0, -0.2494588732454364, 20.0, 22.27825196703759, 19.4, 0.0, 5.8581063700807094], 
current ob forecast is [], 
actual action is [6.0, 20.0], 
sim time this is 5966100.0000, 
sim time next is 5967000.0000, 
raw observation next is [1.0, 86.0, 3.1, 15.0, 0.0, 0.0, 6.0, -0.2645994443820578, 20.0, 22.22293744588862, 19.4, 0.0, 5.918586633029178], 
processed observation next is [0.0, 0.043478260869565216, 0.358974358974359, 0.86, 0.2818181818181818, 0.041666666666666664, 0.0, 0.0, 0.6, 0.4118001852059807, 0.0, 0.6032767779840883, 0.1999999999999998, 0.0, 0.06963043097681386], 
reward next is 0.8804, 
noisyNet noise sample is [array([0.78875196], dtype=float32), 0.25993106]. 
=============================================
[2019-05-04 22:07:49,606] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.421486]
 [76.45935 ]
 [76.4922  ]
 [76.51272 ]
 [76.971375]], R is [[76.46203613]
 [76.57849884]
 [76.69441223]
 [76.80983734]
 [76.9249115 ]].
[2019-05-04 22:07:49,875] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9688094e-15 1.0000000e+00 1.0684518e-17 5.6740041e-19 1.8950598e-25
 1.3786860e-26 3.3283190e-18 1.3323794e-26 5.3922113e-15 4.1181238e-15
 1.3296296e-08], sum to 1.0000
[2019-05-04 22:07:49,876] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6579
[2019-05-04 22:07:49,898] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 93.0, 2.1, 350.0, 74.0, 0.0, 6.0, -0.6163069974005552, 20.0, 20.89819505660262, 19.4, 0.0, 10.182390515668347], 
current ob forecast is [], 
actual action is [6.0, 20.0], 
sim time this is 5990400.0000, 
sim time next is 5991300.0000, 
raw observation next is [1.0, 93.0, 2.475, 352.5, 84.5, 0.0, 6.0, -0.6234589992025497, 20.0, 20.8505465393485, 20.56, 1.0, 12.610442449266124], 
processed observation next is [0.0, 0.34782608695652173, 0.358974358974359, 0.93, 0.225, 0.9791666666666666, 0.22354497354497355, 0.0, 0.6, 0.29218033359915013, 0.0, 0.4072209341926428, 0.36571428571428555, 1.0, 0.1483581464619544], 
reward next is 0.5430, 
noisyNet noise sample is [array([0.59038377], dtype=float32), 0.39631483]. 
=============================================
[2019-05-04 22:07:50,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8479986e-11 9.9767464e-01 7.3140353e-14 4.9800574e-14 1.5845829e-17
 7.6002109e-19 3.9481724e-13 3.5701347e-18 1.9973620e-10 4.8081917e-11
 2.3253385e-03], sum to 1.0000
[2019-05-04 22:07:50,520] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0120
[2019-05-04 22:07:50,543] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.25, 98.25, 2.725, 345.0, 113.0, 0.0, 7.0, -0.6832391225775205, 20.0, 20.70777066735602, 20.56, 1.0, 12.042085127380902], 
current ob forecast is [], 
actual action is [7.25, 20.0], 
sim time this is 6002100.0000, 
sim time next is 6003000.0000, 
raw observation next is [2.5, 96.5, 2.85, 340.0, 117.0, 0.0, 7.25, -0.6881294111512256, 20.0, 20.6892500575363, 20.56, 1.0, 11.949610164224822], 
processed observation next is [0.0, 0.4782608695652174, 0.3974358974358974, 0.965, 0.2590909090909091, 0.9444444444444444, 0.30952380952380953, 0.0, 0.6208333333333333, 0.27062352961625813, 0.0, 0.3841785796480427, 0.36571428571428555, 1.0, 0.14058364899088027], 
reward next is 0.1039, 
noisyNet noise sample is [array([-1.5567526], dtype=float32), 0.50971746]. 
=============================================
[2019-05-04 22:07:50,571] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[63.887688]
 [63.591003]
 [62.99816 ]
 [62.37402 ]
 [62.102062]], R is [[63.39765167]
 [62.90819931]
 [62.46577072]
 [62.07052994]
 [61.72195053]].
[2019-05-04 22:07:52,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2866122e-04 9.0534067e-01 1.7522955e-06 2.9204105e-06 4.2011138e-08
 2.3075286e-08 5.3224940e-06 6.0129636e-08 9.0876347e-05 1.6345439e-04
 9.4266213e-02], sum to 1.0000
[2019-05-04 22:07:52,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7387
[2019-05-04 22:07:52,400] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 93.0, 2.225, 352.5, 24.0, 0.0, 10.0, -0.04919896370858229, 65.0, 23.36428086171963, 20.56, 1.0, 48.89445233746631], 
current ob forecast is [], 
actual action is [10.0, 20.0], 
sim time this is 6025500.0000, 
sim time next is 6026400.0000, 
raw observation next is [5.0, 93.0, 2.1, 350.0, 16.0, 0.0, 10.0, -0.06192655145492523, 20.0, 23.35817930799775, 20.56, 1.0, 35.22969814507052], 
processed observation next is [0.0, 0.782608695652174, 0.46153846153846156, 0.93, 0.19090909090909092, 0.9722222222222222, 0.042328042328042326, 0.0, 0.6666666666666666, 0.4793578161816916, 0.0, 0.7654541868568217, 0.36571428571428555, 1.0, 0.41446703700082965], 
reward next is 0.5355, 
noisyNet noise sample is [array([-0.5365515], dtype=float32), 0.40858784]. 
=============================================
[2019-05-04 22:08:00,009] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9422835e-18 7.0446207e-05 5.9676745e-20 1.9922207e-21 3.3096878e-26
 4.3817649e-26 1.7596870e-19 4.0873420e-25 4.1005531e-18 1.6177962e-17
 9.9992955e-01], sum to 1.0000
[2019-05-04 22:08:00,010] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1701
[2019-05-04 22:08:00,169] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.75, 65.0, 3.225, 225.0, 63.75, 208.5, 2.0, -1.075473753847148, 20.0, 19.32910157981189, 22.2, 1.0, 15.558448392457942], 
current ob forecast is [], 
actual action is [2.25, 65.0], 
sim time this is 6160500.0000, 
sim time next is 6161400.0000, 
raw observation next is [-2.5, 65.0, 3.35, 220.0, 85.0, 278.0, 2.25, -0.9440307235413984, 65.0, 19.57642217009477, 22.2, 1.0, 102.28308249378672], 
processed observation next is [1.0, 0.30434782608695654, 0.2692307692307692, 0.65, 0.30454545454545456, 0.6111111111111112, 0.22486772486772486, 0.278, 0.5375, 0.1853230921528672, 1.0, 0.22520316715639563, 0.5999999999999999, 1.0, 1.2033303822798438], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22930051], dtype=float32), 0.70244944]. 
=============================================
[2019-05-04 22:08:01,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4322326e-16 9.9681973e-01 1.5022123e-18 2.3732831e-20 8.0397175e-27
 3.9431963e-27 3.6674252e-19 4.2743294e-26 3.6596567e-18 2.2744335e-16
 3.1801956e-03], sum to 1.0000
[2019-05-04 22:08:01,015] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7088
[2019-05-04 22:08:01,160] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 65.0, 3.6, 210.0, 97.5, 442.5, 2.75, -0.7753535264416788, 20.0, 20.94536626469595, 22.2, 1.0, 45.57542368903629], 
current ob forecast is [], 
actual action is [3.0, 20.0], 
sim time this is 6163200.0000, 
sim time next is 6164100.0000, 
raw observation next is [-1.25, 62.75, 3.85, 212.5, 103.75, 524.75, 3.0, -0.7479380692677945, 20.0, 21.04672100322838, 22.2, 1.0, 32.88026569497256], 
processed observation next is [1.0, 0.34782608695652173, 0.30128205128205127, 0.6275, 0.35000000000000003, 0.5902777777777778, 0.2744708994708995, 0.52475, 0.55, 0.2506873102440685, 0.0, 0.43524585760405443, 0.5999999999999999, 1.0, 0.3868266552349713], 
reward next is 0.2742, 
noisyNet noise sample is [array([0.8690304], dtype=float32), -0.39354977]. 
=============================================
[2019-05-04 22:08:03,386] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7000, global step 110682: loss 0.2023
[2019-05-04 22:08:03,387] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7000, global step 110682: learning rate 0.0001
[2019-05-04 22:08:03,735] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.26003838e-14 9.99998689e-01 4.62941101e-18 2.07242476e-17
 1.54241287e-23 4.22544982e-24 4.62874017e-18 1.04068354e-22
 1.80550790e-15 1.72944519e-15 1.26063128e-06], sum to 1.0000
[2019-05-04 22:08:03,737] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5324
[2019-05-04 22:08:03,752] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.5, 56.0, 3.35, 190.0, 0.0, 0.0, 8.75, -0.5808958341770523, 20.0, 21.12356276119208, 19.4, 0.0, 4.778196804838983], 
current ob forecast is [], 
actual action is [8.5, 20.0], 
sim time this is 6211800.0000, 
sim time next is 6212700.0000, 
raw observation next is [3.25, 57.0, 3.225, 190.0, 0.0, 0.0, 8.5, -0.5897587927223278, 20.0, 21.08417628487128, 19.4, 0.0, 4.918632027475071], 
processed observation next is [1.0, 0.9130434782608695, 0.4166666666666667, 0.57, 0.2931818181818182, 0.5277777777777778, 0.0, 0.0, 0.6416666666666667, 0.3034137357592241, 0.0, 0.44059661212446855, 0.1999999999999998, 0.0, 0.05786625914676554], 
reward next is 0.8921, 
noisyNet noise sample is [array([1.5084867], dtype=float32), 2.1520848]. 
=============================================
[2019-05-04 22:08:03,950] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 110904: loss 0.2665
[2019-05-04 22:08:03,951] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 110904: learning rate 0.0001
[2019-05-04 22:08:06,050] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111610: loss 0.2347
[2019-05-04 22:08:06,051] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111610: learning rate 0.0001
[2019-05-04 22:08:06,261] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111678: loss 0.1961
[2019-05-04 22:08:06,306] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111689: learning rate 0.0001
[2019-05-04 22:08:06,781] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111880: loss 0.0257
[2019-05-04 22:08:06,783] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111882: learning rate 0.0001
[2019-05-04 22:08:06,823] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111905: loss 0.0079
[2019-05-04 22:08:06,826] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111907: learning rate 0.0001
[2019-05-04 22:08:07,130] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 112042: loss 0.1126
[2019-05-04 22:08:07,134] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 112043: learning rate 0.0001
[2019-05-04 22:08:07,256] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 112104: loss 0.1997
[2019-05-04 22:08:07,256] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 112104: learning rate 0.0001
[2019-05-04 22:08:07,445] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 112197: loss 0.3233
[2019-05-04 22:08:07,450] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 112199: learning rate 0.0001
[2019-05-04 22:08:07,473] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7000, global step 112210: loss 0.1315
[2019-05-04 22:08:07,473] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112210: loss 0.1616
[2019-05-04 22:08:07,474] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7000, global step 112210: learning rate 0.0001
[2019-05-04 22:08:07,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112210: learning rate 0.0001
[2019-05-04 22:08:07,843] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.8416550e-19 1.0000000e+00 7.9495517e-24 9.3711856e-24 1.2630537e-29
 9.6624718e-30 4.0423724e-22 1.0119344e-30 2.4521353e-19 6.8353585e-20
 2.6307505e-09], sum to 1.0000
[2019-05-04 22:08:07,844] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3302
[2019-05-04 22:08:07,867] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.0, 47.0, 5.1, 190.0, 0.0, 0.0, 19.0, -0.01969248844670891, 20.0, 23.36232847860632, 19.4, 0.0, 0.9789401305900318], 
current ob forecast is [], 
actual action is [19.0, 20.0], 
sim time this is 6404400.0000, 
sim time next is 6405300.0000, 
raw observation next is [14.0, 47.0, 4.85, 192.5, 0.0, 0.0, 19.0, -0.02642149971741864, 20.0, 23.33376986041759, 19.4, 0.0, 1.0258622417000127], 
processed observation next is [1.0, 0.13043478260869565, 0.6923076923076923, 0.47, 0.44090909090909086, 0.5347222222222222, 0.0, 0.0, 0.8166666666666667, 0.4911928334275271, 0.0, 0.7619671229167986, 0.1999999999999998, 0.0, 0.012068967549411915], 
reward next is 0.9379, 
noisyNet noise sample is [array([1.1403368], dtype=float32), -0.09764327]. 
=============================================
[2019-05-04 22:08:07,968] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 112429: loss 0.0036
[2019-05-04 22:08:07,970] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 112430: learning rate 0.0001
[2019-05-04 22:08:08,224] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112545: loss 0.0891
[2019-05-04 22:08:08,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112546: learning rate 0.0001
[2019-05-04 22:08:08,388] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112621: loss 0.1933
[2019-05-04 22:08:08,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112621: learning rate 0.0001
[2019-05-04 22:08:08,573] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 112703: loss 0.0233
[2019-05-04 22:08:08,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 112704: learning rate 0.0001
[2019-05-04 22:08:08,841] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112817: loss 0.2523
[2019-05-04 22:08:08,843] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112817: learning rate 0.0001
[2019-05-04 22:08:15,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5721112e-13 9.9884063e-01 1.7260073e-14 4.8974666e-15 5.9614267e-20
 1.1548334e-20 1.2172595e-14 2.3899639e-19 4.5494733e-13 8.4025479e-13
 1.1593733e-03], sum to 1.0000
[2019-05-04 22:08:15,541] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3228
[2019-05-04 22:08:15,559] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.5, 85.0, 6.7, 335.0, 0.0, 0.0, 3.75, -0.8263611815488084, 20.0, 19.64889961629166, 19.4, 0.0, 9.362232996678067], 
current ob forecast is [], 
actual action is [3.5, 20.0], 
sim time this is 6503400.0000, 
sim time next is 6504300.0000, 
raw observation next is [-1.75, 85.0, 6.45, 332.5, 22.25, 118.75, 3.5, -0.8352618497480448, 20.0, 19.59421106488609, 19.4, 0.0, 9.387233329855178], 
processed observation next is [1.0, 0.2608695652173913, 0.28846153846153844, 0.85, 0.5863636363636364, 0.9236111111111112, 0.05886243386243386, 0.11875, 0.5583333333333333, 0.2215793834173184, 0.0, 0.22774443784086987, 0.1999999999999998, 0.0, 0.1104380391747668], 
reward next is 0.8396, 
noisyNet noise sample is [array([-1.2463754], dtype=float32), 0.21595357]. 
=============================================
[2019-05-04 22:08:19,137] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1349042e-09 9.8801929e-01 2.0345630e-11 2.9313465e-11 3.7272101e-15
 2.7288727e-15 5.2767535e-10 1.7847919e-14 7.3417422e-10 2.9212583e-09
 1.1980691e-02], sum to 1.0000
[2019-05-04 22:08:19,140] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4039
[2019-05-04 22:08:19,159] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 64.0, 2.6, 90.0, 0.0, 0.0, -2.0, -1.31190809161934, 20.0, 18.13471830352019, 19.4, 0.0, 10.639187784820242], 
current ob forecast is [], 
actual action is [-2.0, 20.0], 
sim time this is 6588000.0000, 
sim time next is 6588900.0000, 
raw observation next is [-7.0, 64.0, 2.725, 82.5, 0.0, 0.0, -2.0, -1.328342601875451, 20.0, 18.07162639382844, 19.4, 0.0, 10.687767980784173], 
processed observation next is [0.0, 0.2608695652173913, 0.15384615384615385, 0.64, 0.24772727272727274, 0.22916666666666666, 0.0, 0.0, 0.4666666666666667, 0.05721913270818302, 0.0, 0.010232341975491213, 0.1999999999999998, 0.0, 0.12573844683275498], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3719617], dtype=float32), -1.4341251]. 
=============================================
[2019-05-04 22:08:23,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6296988e-14 9.9913007e-01 6.5970738e-16 5.9880319e-17 2.3210989e-22
 1.8290288e-24 1.7056234e-16 3.0041030e-22 5.8737683e-14 2.5421193e-15
 8.6999388e-04], sum to 1.0000
[2019-05-04 22:08:23,356] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3214
[2019-05-04 22:08:23,376] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.5, 29.0, 5.7, 115.0, 191.0, 789.0, 11.25, -0.5821696202442335, 20.0, 21.49365972292548, 20.56, 1.0, 7.630417484355284], 
current ob forecast is [], 
actual action is [11.5, 20.0], 
sim time this is 6615000.0000, 
sim time next is 6615900.0000, 
raw observation next is [6.75, 28.5, 5.7, 117.5, 205.5, 725.5, 11.5, -0.5735050197850161, 20.0, 21.53729028064208, 20.56, 1.0, 6.487867006755816], 
processed observation next is [0.0, 0.5652173913043478, 0.5064102564102564, 0.285, 0.5181818181818182, 0.3263888888888889, 0.5436507936507936, 0.7255, 0.6916666666666667, 0.3088316600716613, 0.0, 0.5053271829488685, 0.36571428571428555, 1.0, 0.07632784713830372], 
reward next is 0.8169, 
noisyNet noise sample is [array([-0.95682156], dtype=float32), -0.26520136]. 
=============================================
[2019-05-04 22:08:25,258] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7500, global step 118669: loss 0.2851
[2019-05-04 22:08:25,259] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7500, global step 118669: learning rate 0.0001
[2019-05-04 22:08:26,439] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 119043: loss 0.4515
[2019-05-04 22:08:26,440] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 119043: learning rate 0.0001
[2019-05-04 22:08:28,718] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 119331: loss 0.2707
[2019-05-04 22:08:28,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 119333: learning rate 0.0001
[2019-05-04 22:08:30,214] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8305297e-10 9.6902317e-01 6.7708683e-14 2.3130225e-15 2.5235938e-20
 2.1458915e-21 7.8025245e-16 2.5618502e-19 1.7592681e-12 1.1140010e-12
 3.0976817e-02], sum to 1.0000
[2019-05-04 22:08:30,219] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9681
[2019-05-04 22:08:30,243] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.75, 76.5, 5.1, 120.0, 0.0, 0.0, 10.5, -0.6829063476600559, 20.0, 20.98935755563934, 19.4, 0.0, 5.719143222785715], 
current ob forecast is [], 
actual action is [10.75, 20.0], 
sim time this is 6749100.0000, 
sim time next is 6750000.0000, 
raw observation next is [6.0, 75.0, 5.1, 120.0, 0.0, 0.0, 10.75, -0.6892212324697486, 20.0, 20.96041039388301, 19.4, 0.0, 5.743817760087187], 
processed observation next is [1.0, 0.13043478260869565, 0.48717948717948717, 0.75, 0.4636363636363636, 0.3333333333333333, 0.0, 0.0, 0.6791666666666667, 0.27025958917675047, 0.0, 0.42291577055471563, 0.1999999999999998, 0.0, 0.06757432658926102], 
reward next is 0.8824, 
noisyNet noise sample is [array([0.9158033], dtype=float32), 0.10966044]. 
=============================================
[2019-05-04 22:08:30,264] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[81.80525 ]
 [81.9214  ]
 [81.96767 ]
 [82.14583 ]
 [82.306076]], R is [[81.81015015]
 [81.87476349]
 [81.93917847]
 [82.00357056]
 [82.06808472]].
[2019-05-04 22:08:30,273] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119849: loss 0.0084
[2019-05-04 22:08:30,276] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119850: learning rate 0.0001
[2019-05-04 22:08:30,608] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120002: loss 1.3755
[2019-05-04 22:08:30,613] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120003: learning rate 0.0001
[2019-05-04 22:08:30,627] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 120012: loss 0.0236
[2019-05-04 22:08:30,627] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 120012: learning rate 0.0001
[2019-05-04 22:08:30,671] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 120026: loss 0.1224
[2019-05-04 22:08:30,673] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 120026: learning rate 0.0001
[2019-05-04 22:08:30,718] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 120055: loss 0.0270
[2019-05-04 22:08:30,721] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 120055: learning rate 0.0001
[2019-05-04 22:08:30,801] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120086: loss 0.0030
[2019-05-04 22:08:30,805] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120086: learning rate 0.0001
[2019-05-04 22:08:30,869] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120122: loss 0.0500
[2019-05-04 22:08:30,881] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120123: learning rate 0.0001
[2019-05-04 22:08:30,916] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7500, global step 120140: loss 0.0509
[2019-05-04 22:08:30,921] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7500, global step 120142: learning rate 0.0001
[2019-05-04 22:08:30,989] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120173: loss 0.0087
[2019-05-04 22:08:30,991] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120174: learning rate 0.0001
[2019-05-04 22:08:31,076] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120216: loss 0.2356
[2019-05-04 22:08:31,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120217: learning rate 0.0001
[2019-05-04 22:08:31,317] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 120313: loss 0.0036
[2019-05-04 22:08:31,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 120314: learning rate 0.0001
[2019-05-04 22:08:31,871] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120442: loss 0.1052
[2019-05-04 22:08:31,872] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120442: learning rate 0.0001
[2019-05-04 22:08:32,348] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120528: loss 0.0302
[2019-05-04 22:08:32,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120531: learning rate 0.0001
[2019-05-04 22:08:33,619] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3814348e-14 9.9999976e-01 2.1728483e-15 3.0430053e-16 2.9400733e-21
 4.5461418e-21 2.0550387e-15 9.3825724e-21 2.9958197e-14 6.0566612e-15
 1.8253249e-07], sum to 1.0000
[2019-05-04 22:08:33,619] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7603
[2019-05-04 22:08:33,642] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.0, 60.0, 7.2, 120.0, 148.0, 0.0, 11.25, -0.4289245131554618, 20.0, 22.09604379237013, 22.2, 1.0, 3.1252996759593388], 
current ob forecast is [], 
actual action is [11.0, 20.0], 
sim time this is 6789600.0000, 
sim time next is 6790500.0000, 
raw observation next is [6.0, 60.0, 6.550000000000001, 112.5, 160.0, 0.0, 11.0, -0.4126861683133742, 20.0, 22.21932726558331, 22.2, 1.0, 3.303195762504828], 
processed observation next is [1.0, 0.6086956521739131, 0.48717948717948717, 0.6, 0.5954545454545456, 0.3125, 0.42328042328042326, 0.0, 0.6833333333333333, 0.3624379438955419, 0.0, 0.6027610379404729, 0.5999999999999999, 1.0, 0.038861126617703856], 
reward next is 0.9111, 
noisyNet noise sample is [array([-0.2997689], dtype=float32), -0.7707877]. 
=============================================
[2019-05-04 22:08:33,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.93103]
 [70.83653]
 [70.87598]
 [71.15813]
 [71.50223]], R is [[71.10855865]
 [71.31070709]
 [71.51339722]
 [71.71723938]
 [71.92237854]].
[2019-05-04 22:08:33,668] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.5139292e-17 1.0000000e+00 3.4694697e-18 4.7072375e-18 2.8581141e-25
 4.8183692e-26 1.1717911e-18 1.3803451e-24 8.6794714e-16 3.2538877e-17
 1.4288662e-09], sum to 1.0000
[2019-05-04 22:08:33,672] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3126
[2019-05-04 22:08:33,708] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.0, 69.0, 5.7, 100.0, 44.0, 0.0, 9.25, -0.4579682193496434, 20.0, 21.89782244884983, 22.2, 1.0, 11.739042474999204], 
current ob forecast is [], 
actual action is [9.0, 20.0], 
sim time this is 6800400.0000, 
sim time next is 6801300.0000, 
raw observation next is [3.75, 70.25, 6.2, 97.5, 35.0, 0.0, 9.0, -0.5406596775271045, 20.0, 21.51078844006729, 22.2, 1.0, 11.905695152560677], 
processed observation next is [1.0, 0.7391304347826086, 0.42948717948717946, 0.7025, 0.5636363636363636, 0.2708333333333333, 0.09259259259259259, 0.0, 0.65, 0.31978010749096514, 0.0, 0.5015412057238987, 0.5999999999999999, 1.0, 0.1400670017948315], 
reward next is 0.8072, 
noisyNet noise sample is [array([-0.08331766], dtype=float32), 0.5157976]. 
=============================================
[2019-05-04 22:08:34,338] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2380132e-17 1.0000000e+00 5.7246172e-19 1.3412012e-19 1.6720473e-24
 2.1682259e-25 2.7015605e-19 2.0529143e-24 3.0400112e-17 1.1411680e-17
 6.2684826e-09], sum to 1.0000
[2019-05-04 22:08:34,349] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6182
[2019-05-04 22:08:34,374] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.0, 70.5, 2.475, 100.0, 0.0, 0.0, 9.0, -0.8244007635354212, 20.0, 20.06834804156427, 19.4, 0.0, 7.121068553953951], 
current ob forecast is [], 
actual action is [9.0, 20.0], 
sim time this is 6822900.0000, 
sim time next is 6823800.0000, 
raw observation next is [4.0, 72.0, 2.85, 100.0, 0.0, 0.0, 9.0, -0.8497334519878751, 20.0, 20.00124772893388, 19.4, 0.0, 8.576233888484778], 
processed observation next is [1.0, 1.0, 0.4358974358974359, 0.72, 0.2590909090909091, 0.2777777777777778, 0.0, 0.0, 0.65, 0.21675551600404164, 0.0, 0.28589253270483994, 0.1999999999999998, 0.0, 0.1008968692762915], 
reward next is 0.8491, 
noisyNet noise sample is [array([0.00902725], dtype=float32), 1.7874433]. 
=============================================
[2019-05-04 22:08:35,738] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8519529e-13 9.9999988e-01 2.2667938e-16 2.3820041e-16 3.4610814e-22
 1.0134140e-22 2.1192298e-15 1.2231810e-21 1.9563380e-14 1.5013883e-13
 1.7529941e-07], sum to 1.0000
[2019-05-04 22:08:35,739] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9691
[2019-05-04 22:08:35,760] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 87.0, 1.5, 35.0, 0.0, 0.0, 8.0, -1.095538495032948, 20.0, 18.89867191080214, 19.4, 0.0, 11.76487576434103], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 6846300.0000, 
sim time next is 6847200.0000, 
raw observation next is [3.0, 87.0, 1.5, 30.0, 0.0, 0.0, 8.0, -1.102362666376528, 20.0, 18.87496938752255, 19.4, 0.0, 11.279304990235444], 
processed observation next is [1.0, 0.2608695652173913, 0.41025641025641024, 0.87, 0.13636363636363635, 0.08333333333333333, 0.0, 0.0, 0.6333333333333333, 0.13254577787449065, 0.0, 0.12499562678893586, 0.1999999999999998, 0.0, 0.1326977057674758], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.23681766], dtype=float32), -1.5983225]. 
=============================================
[2019-05-04 22:08:40,609] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8650728e-19 9.9999964e-01 5.7494080e-20 2.3971591e-19 8.3146837e-29
 1.2035376e-28 1.5551013e-20 2.4998887e-26 7.8391749e-18 2.1779326e-17
 3.9039986e-07], sum to 1.0000
[2019-05-04 22:08:40,610] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2001
[2019-05-04 22:08:40,625] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.0, 85.5, 3.55, 80.0, 83.5, 4.25, 11.0, -0.3805130822365391, 20.0, 22.10570497140399, 22.2, 1.0, 2.4107490732211945], 
current ob forecast is [], 
actual action is [11.0, 20.0], 
sim time this is 6887700.0000, 
sim time next is 6888600.0000, 
raw observation next is [6.0, 84.0, 3.5, 80.0, 60.0, 0.0, 11.0, -0.4059812493317219, 20.0, 21.86146121524417, 22.2, 1.0, 2.2987735662663593], 
processed observation next is [1.0, 0.7391304347826086, 0.48717948717948717, 0.84, 0.3181818181818182, 0.2222222222222222, 0.15873015873015872, 0.0, 0.6833333333333333, 0.36467291688942605, 0.0, 0.5516373164634528, 0.5999999999999999, 1.0, 0.027044394897251287], 
reward next is 0.9230, 
noisyNet noise sample is [array([-0.65296805], dtype=float32), 0.09525633]. 
=============================================
[2019-05-04 22:08:45,820] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2452825e-15 1.0000000e+00 9.8647233e-19 4.5966865e-18 1.6429948e-23
 1.4271803e-24 1.8431858e-16 1.7281693e-21 2.1857750e-16 1.2544491e-14
 2.1999449e-09], sum to 1.0000
[2019-05-04 22:08:45,821] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8514
[2019-05-04 22:08:45,839] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.0, 87.0, 0.65, 67.5, 0.0, 0.0, 13.0, -0.6034470975122488, 20.0, 20.7441735403043, 19.4, 0.0, 3.721358261964417], 
current ob forecast is [], 
actual action is [13.0, 20.0], 
sim time this is 7011900.0000, 
sim time next is 7012800.0000, 
raw observation next is [8.0, 87.0, 0.0, 0.0, 0.0, 0.0, 13.0, -0.6093590513474665, 20.0, 20.72040052907442, 19.4, 0.0, 3.7076749147241537], 
processed observation next is [1.0, 0.17391304347826086, 0.5384615384615384, 0.87, 0.0, 0.0, 0.0, 0.0, 0.7166666666666667, 0.29688031621751115, 0.0, 0.3886286470106316, 0.1999999999999998, 0.0, 0.04361970487910769], 
reward next is 0.9064, 
noisyNet noise sample is [array([0.10658053], dtype=float32), -0.88738286]. 
=============================================
[2019-05-04 22:08:48,304] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7203466e-21 1.0000000e+00 2.9197614e-24 2.7352749e-25 6.2530613e-33
 9.0608436e-34 3.1747832e-22 2.3605932e-31 3.0769661e-21 2.0232508e-20
 2.7760291e-10], sum to 1.0000
[2019-05-04 22:08:48,306] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3400
[2019-05-04 22:08:48,324] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.0, 67.0, 2.1, 80.0, 252.5, 476.5, 18.0, -0.1395667335779545, 20.0, 22.82371313877682, 22.2, 1.0, 0.0], 
current ob forecast is [], 
actual action is [18.0, 20.0], 
sim time this is 7052400.0000, 
sim time next is 7053300.0000, 
raw observation next is [13.25, 66.0, 2.35, 77.5, 217.75, 568.75, 18.0, -0.1010414686601556, 20.0, 22.99796785819692, 22.2, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6730769230769231, 0.66, 0.21363636363636365, 0.2152777777777778, 0.576058201058201, 0.56875, 0.8, 0.46631951044661485, 0.0, 0.7139954083138456, 0.5999999999999999, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([0.35834628], dtype=float32), -1.0265956]. 
=============================================
[2019-05-04 22:08:49,675] A3C_AGENT_WORKER-Thread-7 INFO:Local step 8000, global step 126223: loss 1.6232
[2019-05-04 22:08:49,680] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 8000, global step 126223: learning rate 0.0001
[2019-05-04 22:08:50,560] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 126612: loss 1.1843
[2019-05-04 22:08:50,561] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 126614: learning rate 0.0001
[2019-05-04 22:08:53,366] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127760: loss 4.1363
[2019-05-04 22:08:53,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127760: learning rate 0.0001
[2019-05-04 22:08:53,726] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127930: loss 3.0980
[2019-05-04 22:08:53,729] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127931: learning rate 0.0001
[2019-05-04 22:08:53,826] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127983: loss 3.7316
[2019-05-04 22:08:53,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127983: learning rate 0.0001
[2019-05-04 22:08:53,865] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 127999: loss 3.9654
[2019-05-04 22:08:53,869] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 127999: learning rate 0.0001
[2019-05-04 22:08:54,281] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128188: loss 7.0641
[2019-05-04 22:08:54,285] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128190: learning rate 0.0001
[2019-05-04 22:08:54,322] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128208: loss 7.2063
[2019-05-04 22:08:54,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128208: learning rate 0.0001
[2019-05-04 22:08:54,385] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128232: loss 6.3865
[2019-05-04 22:08:54,390] A3C_AGENT_WORKER-Thread-8 INFO:Local step 8000, global step 128234: loss 6.4722
[2019-05-04 22:08:54,391] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 128234: loss 6.5176
[2019-05-04 22:08:54,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128234: learning rate 0.0001
[2019-05-04 22:08:54,392] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 8000, global step 128234: learning rate 0.0001
[2019-05-04 22:08:54,402] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 128240: learning rate 0.0001
[2019-05-04 22:08:54,485] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128281: loss 8.1095
[2019-05-04 22:08:54,489] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128283: learning rate 0.0001
[2019-05-04 22:08:54,494] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 128285: loss 8.1317
[2019-05-04 22:08:54,498] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 128285: learning rate 0.0001
[2019-05-04 22:08:54,532] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 128299: loss 8.1460
[2019-05-04 22:08:54,532] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 128299: learning rate 0.0001
[2019-05-04 22:08:55,357] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128671: loss 11.2384
[2019-05-04 22:08:55,360] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128672: learning rate 0.0001
[2019-05-04 22:08:55,395] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128684: loss 9.2342
[2019-05-04 22:08:55,396] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128684: learning rate 0.0001
[2019-05-04 22:08:55,420] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.68881739e-15 9.99999881e-01 5.11428170e-17 4.36657230e-18
 3.89942276e-22 4.79737587e-23 4.40431722e-16 1.80526931e-22
 8.37235402e-15 4.34895989e-14 1.04780526e-07], sum to 1.0000
[2019-05-04 22:08:55,423] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3085
[2019-05-04 22:08:55,437] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.25, 42.0, 6.675000000000001, 290.0, 0.0, 0.0, 20.5, 0.4905951805056628, 20.0, 25.09397309273961, 19.4, 0.0, 0.0], 
current ob forecast is [], 
actual action is [20.25, 20.0], 
sim time this is 7249500.0000, 
sim time next is 7250400.0000, 
raw observation next is [15.0, 41.0, 7.7, 290.0, 0.0, 0.0, 20.25, 0.4782933697778477, 20.0, 25.05159860279734, 19.4, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.717948717948718, 0.41, 0.7000000000000001, 0.8055555555555556, 0.0, 0.0, 0.8375, 0.6594311232592825, 0.0, 1.0073712289710488, 0.1999999999999998, 0.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-1.0157093], dtype=float32), -0.7146609]. 
=============================================
[2019-05-04 22:08:58,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2254294e-18 1.0000000e+00 2.6241867e-23 4.0866593e-27 5.3761240e-32
 1.4830533e-32 4.8659768e-23 1.6206358e-30 2.0275457e-21 2.6009213e-19
 1.7875545e-08], sum to 1.0000
[2019-05-04 22:08:58,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1990
[2019-05-04 22:08:58,085] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.0, 46.0, 6.825, 285.0, 138.75, 815.75, 16.0, 0.1568395454759886, 20.0, 23.93548316567377, 20.56, 1.0, 1.0215690447693513], 
current ob forecast is [], 
actual action is [16.0, 20.0], 
sim time this is 7294500.0000, 
sim time next is 7295400.0000, 
raw observation next is [11.0, 45.0, 6.45, 290.0, 139.0, 829.0, 16.0, 0.1679463784993303, 20.0, 23.98716510754915, 20.56, 1.0, 0.943986180576391], 
processed observation next is [0.0, 0.43478260869565216, 0.6153846153846154, 0.45, 0.5863636363636364, 0.8055555555555556, 0.36772486772486773, 0.829, 0.7666666666666667, 0.5559821261664434, 0.0, 0.8553093010784499, 0.36571428571428555, 1.0, 0.011105719771486954], 
reward next is 0.9389, 
noisyNet noise sample is [array([-1.8457936], dtype=float32), -1.1633333]. 
=============================================
[2019-05-04 22:08:58,576] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1756882e-15 9.9999535e-01 1.3554539e-17 7.3821362e-18 9.9972051e-24
 5.7945084e-24 9.5607870e-16 7.7341454e-24 2.6231055e-14 4.2803362e-16
 4.6074283e-06], sum to 1.0000
[2019-05-04 22:08:58,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1325
[2019-05-04 22:08:58,595] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.4428139e-16 9.9999774e-01 4.4859481e-18 1.6325665e-20 6.6416787e-24
 1.2778305e-24 6.0592707e-19 5.7879640e-23 1.7355776e-14 3.8954270e-16
 2.2938948e-06], sum to 1.0000
[2019-05-04 22:08:58,599] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8525
[2019-05-04 22:08:58,601] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.5, 38.0, 4.35, 290.0, 59.0, 472.0, 18.75, 0.4318764234412192, 20.0, 25.22746263811654, 20.56, 1.0, 0.0], 
current ob forecast is [], 
actual action is [18.5, 20.0], 
sim time this is 7320600.0000, 
sim time next is 7321500.0000, 
raw observation next is [13.25, 38.0, 4.475, 290.0, 45.0, 354.0, 18.5, 0.4211595514332904, 20.0, 25.17433775730072, 20.56, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.6730769230769231, 0.38, 0.4068181818181818, 0.8055555555555556, 0.11904761904761904, 0.354, 0.8083333333333333, 0.6403865171444302, 0.0, 1.0249053939001027, 0.36571428571428555, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-1.420061], dtype=float32), -0.12609133]. 
=============================================
[2019-05-04 22:08:58,626] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.66633]
 [83.18866]
 [83.70377]
 [84.22744]
 [84.70519]], R is [[81.97820282]
 [82.10842133]
 [82.23733521]
 [82.36495972]
 [82.49131012]].
[2019-05-04 22:08:58,631] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.75, 41.75, 6.2, 322.5, 148.25, 872.25, 17.5, 0.2419686191934476, 20.0, 24.26684668680999, 20.56, 1.0, 0.10220953997478682], 
current ob forecast is [], 
actual action is [17.75, 20.0], 
sim time this is 7303500.0000, 
sim time next is 7304400.0000, 
raw observation next is [13.0, 41.0, 5.7, 330.0, 147.5, 868.5, 17.75, 0.2518270132184433, 20.0, 24.31148158213448, 20.56, 1.0, 0.05954507580188328], 
processed observation next is [0.0, 0.5652173913043478, 0.6666666666666666, 0.41, 0.5181818181818182, 0.9166666666666666, 0.39021164021164023, 0.8685, 0.7958333333333333, 0.5839423377394811, 0.0, 0.9016402260192115, 0.36571428571428555, 1.0, 0.000700530303551568], 
reward next is 0.9493, 
noisyNet noise sample is [array([0.3946595], dtype=float32), -0.5644402]. 
=============================================
[2019-05-04 22:08:59,073] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6099014e-15 1.0000000e+00 5.7127931e-19 4.1369895e-21 1.6471413e-25
 2.4617520e-26 3.4069487e-18 2.2111602e-24 6.0637564e-16 2.4862360e-16
 5.0198075e-08], sum to 1.0000
[2019-05-04 22:08:59,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7949795e-15 9.9999976e-01 4.2019466e-20 5.3199479e-21 3.5441514e-26
 9.7417928e-28 4.3142817e-19 3.5912010e-25 6.9150635e-19 2.0288113e-16
 2.5939858e-07], sum to 1.0000
[2019-05-04 22:08:59,081] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9233
[2019-05-04 22:08:59,085] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9702
[2019-05-04 22:08:59,097] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.0, 40.0, 1.65, 220.0, 0.0, 0.0, 17.0, 0.3193894277015071, 20.0, 24.8094997841648, 19.4, 0.0, 0.0], 
current ob forecast is [], 
actual action is [17.0, 20.0], 
sim time this is 7328700.0000, 
sim time next is 7329600.0000, 
raw observation next is [12.0, 40.0, 1.5, 200.0, 0.0, 0.0, 17.0, 0.3087047280659708, 20.0, 24.77368334504686, 19.4, 0.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6410256410256411, 0.4, 0.13636363636363635, 0.5555555555555556, 0.0, 0.0, 0.7833333333333333, 0.6029015760219902, 0.0, 0.9676690492924084, 0.1999999999999998, 0.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([1.2333608], dtype=float32), 0.61671597]. 
=============================================
[2019-05-04 22:08:59,100] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.0, 41.0, 5.1, 282.5, 118.0, 749.25, 19.0, 0.4189432601023255, 20.0, 25.16539121426509, 20.56, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0, 20.0], 
sim time this is 7314300.0000, 
sim time next is 7315200.0000, 
raw observation next is [14.0, 41.0, 5.1, 280.0, 111.0, 725.5, 19.0, 0.4314476623055601, 20.0, 25.22941110110955, 20.56, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6923076923076923, 0.41, 0.4636363636363636, 0.7777777777777778, 0.29365079365079366, 0.7255, 0.8166666666666667, 0.6438158874351867, 0.0, 1.0327730144442218, 0.36571428571428555, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-0.21930009], dtype=float32), 0.8632352]. 
=============================================
[2019-05-04 22:08:59,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.7224172e-17 1.0000000e+00 7.0965137e-20 1.7140152e-20 5.1289705e-26
 2.4917479e-27 4.5485717e-21 5.5254252e-26 1.8159652e-18 7.1645256e-17
 1.5476310e-08], sum to 1.0000
[2019-05-04 22:08:59,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9516
[2019-05-04 22:08:59,214] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.0, 41.0, 5.1, 280.0, 111.0, 725.5, 19.0, 0.4559215482405308, 20.0, 25.33322716295937, 20.56, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0, 20.0], 
sim time this is 7315200.0000, 
sim time next is 7316100.0000, 
raw observation next is [14.0, 40.25, 4.85, 282.5, 104.0, 701.75, 19.0, 0.4666786694927452, 20.0, 25.39098908601601, 20.56, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6923076923076923, 0.4025, 0.44090909090909086, 0.7847222222222222, 0.2751322751322751, 0.70175, 0.8166666666666667, 0.6555595564975817, 0.0, 1.0558555837165728, 0.36571428571428555, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([0.3525062], dtype=float32), -0.53260916]. 
=============================================
[2019-05-04 22:08:59,808] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4417998e-19 1.0000000e+00 1.1880242e-21 4.8699669e-24 4.9679194e-29
 8.9376503e-31 5.8081133e-20 1.7170971e-28 1.7865014e-19 7.3960628e-19
 1.0718793e-09], sum to 1.0000
[2019-05-04 22:08:59,811] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7663
[2019-05-04 22:08:59,831] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.0, 69.0, 2.15, 220.0, 0.0, 0.0, 9.0, -0.5812451106278442, 20.0, 20.92995921168509, 19.4, 0.0, 5.849437955151871], 
current ob forecast is [], 
actual action is [9.0, 20.0], 
sim time this is 7451100.0000, 
sim time next is 7452000.0000, 
raw observation next is [4.0, 69.0, 1.5, 220.0, 0.0, 0.0, 9.0, -0.5900442876762816, 20.0, 20.89694130336031, 19.4, 0.0, 5.810473238186288], 
processed observation next is [1.0, 0.2608695652173913, 0.4358974358974359, 0.69, 0.13636363636363635, 0.6111111111111112, 0.0, 0.0, 0.65, 0.3033185707745728, 0.0, 0.4138487576229016, 0.1999999999999998, 0.0, 0.06835850868454456], 
reward next is 0.8816, 
noisyNet noise sample is [array([0.8334016], dtype=float32), -0.6505671]. 
=============================================
[2019-05-04 22:08:59,849] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[96.87136 ]
 [96.966415]
 [97.010864]
 [97.095406]
 [97.14303 ]], R is [[96.60243988]
 [96.51759338]
 [96.43340302]
 [96.35012054]
 [96.26798248]].
[2019-05-04 22:09:07,524] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2235153e-14 9.9998355e-01 9.0557235e-18 4.4735489e-17 2.6372999e-23
 5.4427261e-25 3.4566925e-17 3.0159610e-23 2.1941694e-15 2.3286986e-14
 1.6433385e-05], sum to 1.0000
[2019-05-04 22:09:07,525] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2793
[2019-05-04 22:09:07,554] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.25, 87.0, 6.075, 250.0, 14.25, 0.0, 13.0, -0.2092825296433519, 20.0, 22.42430817742728, 22.2, 1.0, 1.5708618064425313], 
current ob forecast is [], 
actual action is [12.25, 20.0], 
sim time this is 7496100.0000, 
sim time next is 7497000.0000, 
raw observation next is [6.5, 87.0, 5.95, 250.0, 1.0, 0.0, 12.25, -0.2224757860696959, 20.0, 22.23335039169396, 22.2, 1.0, 2.0395375530532154], 
processed observation next is [1.0, 0.782608695652174, 0.5, 0.87, 0.5409090909090909, 0.6944444444444444, 0.0026455026455026454, 0.0, 0.7041666666666667, 0.4258414046434347, 0.0, 0.6047643416705658, 0.5999999999999999, 1.0, 0.023994559447684888], 
reward next is 0.9260, 
noisyNet noise sample is [array([-0.14170381], dtype=float32), 0.8197942]. 
=============================================
[2019-05-04 22:09:07,568] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.09997 ]
 [79.38673 ]
 [79.568726]
 [79.74143 ]
 [79.85794 ]], R is [[78.84022522]
 [78.98334503]
 [79.12678528]
 [79.2687912 ]
 [79.40872192]].
[2019-05-04 22:09:09,284] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.03302847e-14 9.08220112e-01 1.92774746e-15 2.77068633e-16
 1.23588006e-23 3.52179599e-23 7.41349652e-16 7.97554547e-24
 2.89539324e-15 3.61977158e-14 9.17799175e-02], sum to 1.0000
[2019-05-04 22:09:09,284] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4926
[2019-05-04 22:09:09,301] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.75, 71.25, 8.85, 257.5, 323.75, 102.75, 11.5, -0.5569466072689858, 20.0, 21.38366990605117, 22.2, 1.0, 6.296926331758956], 
current ob forecast is [], 
actual action is [11.75, 20.0], 
sim time this is 7555500.0000, 
sim time next is 7556400.0000, 
raw observation next is [7.0, 70.0, 8.7, 260.0, 302.5, 72.5, 11.75, -0.5423063778953169, 20.0, 21.4665437272106, 22.2, 1.0, 5.8746876624506426], 
processed observation next is [1.0, 0.4782608695652174, 0.5128205128205128, 0.7, 0.7909090909090909, 0.7222222222222222, 0.8002645502645502, 0.0725, 0.6958333333333333, 0.3192312073682277, 0.0, 0.495220532458657, 0.5999999999999999, 1.0, 0.06911397249941932], 
reward next is 0.8923, 
noisyNet noise sample is [array([0.67955095], dtype=float32), 1.1849556]. 
=============================================
[2019-05-04 22:09:09,994] A3C_AGENT_WORKER-Thread-7 INFO:Local step 8500, global step 134494: loss 1.8182
[2019-05-04 22:09:09,994] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 8500, global step 134494: learning rate 0.0001
[2019-05-04 22:09:12,301] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8500, global step 135142: loss 1.2755
[2019-05-04 22:09:12,303] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8500, global step 135143: learning rate 0.0001
[2019-05-04 22:09:13,613] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 135714: loss 0.7017
[2019-05-04 22:09:13,617] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 135714: learning rate 0.0001
[2019-05-04 22:09:14,207] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 135898: loss 0.1531
[2019-05-04 22:09:14,208] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 135898: learning rate 0.0001
[2019-05-04 22:09:14,579] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136013: loss 0.2138
[2019-05-04 22:09:14,580] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136013: learning rate 0.0001
[2019-05-04 22:09:14,781] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136086: loss 0.1561
[2019-05-04 22:09:14,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136086: learning rate 0.0001
[2019-05-04 22:09:14,799] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1462026e-17 9.9999988e-01 1.2411838e-20 6.1941194e-23 4.4615455e-29
 5.5683620e-32 1.2762289e-19 1.7971053e-31 9.3200449e-20 1.4598990e-17
 1.3612302e-07], sum to 1.0000
[2019-05-04 22:09:14,801] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7501
[2019-05-04 22:09:14,821] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.0, 65.0, 3.4, 310.0, 227.0, 629.0, 10.75, -0.4328258106857447, 20.0, 21.91194724586848, 22.2, 1.0, 2.2830746696896913], 
current ob forecast is [], 
actual action is [11.0, 20.0], 
sim time this is 7639200.0000, 
sim time next is 7640100.0000, 
raw observation next is [6.25, 64.0, 3.45, 312.5, 239.5, 608.0, 11.0, -0.4088811331934746, 20.0, 22.01522402082075, 22.2, 1.0, 2.131982474389506], 
processed observation next is [1.0, 0.43478260869565216, 0.4935897435897436, 0.64, 0.31363636363636366, 0.8680555555555556, 0.6335978835978836, 0.608, 0.6833333333333333, 0.3637062889355085, 0.0, 0.5736034315458214, 0.5999999999999999, 1.0, 0.025082146757523602], 
reward next is 0.9249, 
noisyNet noise sample is [array([1.1077847], dtype=float32), -0.21203972]. 
=============================================
[2019-05-04 22:09:14,848] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 136112: loss 0.1253
[2019-05-04 22:09:14,856] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 136113: learning rate 0.0001
[2019-05-04 22:09:14,997] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.2502741e-19 9.9999988e-01 6.4323945e-23 1.9736376e-22 1.1493656e-29
 2.3356558e-31 9.2080414e-23 1.7232184e-29 9.1440640e-21 4.9567568e-19
 6.0047604e-08], sum to 1.0000
[2019-05-04 22:09:15,001] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6034
[2019-05-04 22:09:15,008] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.8054372e-19 9.9999571e-01 1.1710738e-21 3.6812503e-23 7.1242725e-31
 1.4816573e-30 5.6318749e-23 7.3594556e-31 6.7951662e-19 2.9385538e-17
 4.3357741e-06], sum to 1.0000
[2019-05-04 22:09:15,008] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6581
[2019-05-04 22:09:15,034] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.75, 62.0, 3.55, 317.5, 237.0, 634.5, 11.5, -0.2885294533929043, 20.0, 22.50024047398052, 22.2, 1.0, 2.5028916653316586], 
current ob forecast is [], 
actual action is [11.75, 20.0], 
sim time this is 7641900.0000, 
sim time next is 7642800.0000, 
raw observation next is [7.0, 61.0, 3.6, 320.0, 222.0, 682.0, 11.75, -0.319539809194282, 20.0, 22.61233239740189, 22.2, 1.0, 2.197297109479096], 
processed observation next is [1.0, 0.4782608695652174, 0.5128205128205128, 0.61, 0.32727272727272727, 0.8888888888888888, 0.5873015873015873, 0.682, 0.6958333333333333, 0.39348673026857267, 0.0, 0.65890462820027, 0.5999999999999999, 1.0, 0.025850554229165833], 
reward next is 0.9241, 
noisyNet noise sample is [array([-0.23714569], dtype=float32), 0.02825104]. 
=============================================
[2019-05-04 22:09:15,041] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.5, 63.0, 3.5, 315.0, 252.0, 587.0, 11.25, -0.2474485662383322, 20.0, 22.6199386612958, 22.2, 1.0, 2.818436010482818], 
current ob forecast is [], 
actual action is [11.5, 20.0], 
sim time this is 7641000.0000, 
sim time next is 7641900.0000, 
raw observation next is [6.75, 62.0, 3.55, 317.5, 237.0, 634.5, 11.5, -0.209351337279526, 20.0, 22.84779217835384, 22.2, 1.0, 2.510985887338544], 
processed observation next is [1.0, 0.43478260869565216, 0.5064102564102564, 0.62, 0.3227272727272727, 0.8819444444444444, 0.626984126984127, 0.6345, 0.6916666666666667, 0.4302162209068247, 0.0, 0.692541739764834, 0.5999999999999999, 1.0, 0.02954101043927699], 
reward next is 0.9205, 
noisyNet noise sample is [array([-0.29506415], dtype=float32), -0.45146954]. 
=============================================
[2019-05-04 22:09:15,078] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8500, global step 136202: loss 0.2155
[2019-05-04 22:09:15,079] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8500, global step 136202: learning rate 0.0001
[2019-05-04 22:09:15,326] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 136305: loss 0.2660
[2019-05-04 22:09:15,327] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 136305: learning rate 0.0001
[2019-05-04 22:09:15,337] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 136306: loss 0.2084
[2019-05-04 22:09:15,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 136309: learning rate 0.0001
[2019-05-04 22:09:15,346] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 136316: loss 0.3458
[2019-05-04 22:09:15,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 136316: learning rate 0.0001
[2019-05-04 22:09:15,350] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8500, global step 136316: loss 0.2631
[2019-05-04 22:09:15,351] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8500, global step 136316: learning rate 0.0001
[2019-05-04 22:09:15,411] A3C_AGENT_WORKER-Thread-8 INFO:Local step 8500, global step 136336: loss 0.5894
[2019-05-04 22:09:15,415] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 8500, global step 136338: learning rate 0.0001
[2019-05-04 22:09:15,435] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136350: loss 0.5259
[2019-05-04 22:09:15,436] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136350: learning rate 0.0001
[2019-05-04 22:09:15,567] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 136411: loss 0.4797
[2019-05-04 22:09:15,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 136411: learning rate 0.0001
[2019-05-04 22:09:15,759] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 136490: loss 0.3007
[2019-05-04 22:09:15,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 136490: learning rate 0.0001
[2019-05-04 22:09:15,950] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:16,102] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:16,593] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.4545861e-16 9.9999893e-01 1.7516132e-17 2.0196871e-19 3.1551315e-25
 1.2955495e-26 7.2683227e-18 1.3653443e-25 1.4918010e-17 9.1746162e-16
 1.0521867e-06], sum to 1.0000
[2019-05-04 22:09:16,595] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3614
[2019-05-04 22:09:16,611] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.0, 55.0, 2.725, 352.5, 0.0, 0.0, 11.0, 0.0009731806913176969, 20.0, 23.30123624080858, 19.4, 0.0, 1.7476725112490143], 
current ob forecast is [], 
actual action is [11.0, 20.0], 
sim time this is 7677900.0000, 
sim time next is 7678800.0000, 
raw observation next is [6.0, 55.0, 2.6, 350.0, 0.0, 0.0, 11.0, -0.008781680841362207, 20.0, 23.26612252002323, 19.4, 0.0, 1.8396217536517128], 
processed observation next is [1.0, 0.9130434782608695, 0.48717948717948717, 0.55, 0.23636363636363636, 0.9722222222222222, 0.0, 0.0, 0.6833333333333333, 0.4970727730528793, 0.0, 0.7523032171461759, 0.1999999999999998, 0.0, 0.021642608866490737], 
reward next is 0.9284, 
noisyNet noise sample is [array([2.075406], dtype=float32), -1.6301577]. 
=============================================
[2019-05-04 22:09:16,952] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:16,952] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:16,994] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res7/Eplus-env-sub_run2
[2019-05-04 22:09:17,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5568873e-20 1.0000000e+00 3.7475055e-23 3.6785671e-25 1.1697302e-29
 1.1552422e-30 2.3589746e-22 7.4055918e-30 3.7057595e-20 8.7579447e-20
 4.0733291e-08], sum to 1.0000
[2019-05-04 22:09:17,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1537
[2019-05-04 22:09:17,321] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.0, 47.0, 3.1, 260.0, 140.5, 853.0, 17.0, -0.1227922667388091, 20.0, 23.08757177027445, 22.2, 1.0, 0.1087308103236435], 
current ob forecast is [], 
actual action is [17.0, 20.0], 
sim time this is 7740000.0000, 
sim time next is 7740900.0000, 
raw observation next is [12.5, 45.5, 3.35, 282.5, 138.75, 843.0, 17.0, -0.0855791695284538, 20.0, 23.30155909855841, 22.2, 1.0, 0.01471572613738159], 
processed observation next is [1.0, 0.6086956521739131, 0.6538461538461539, 0.455, 0.30454545454545456, 0.7847222222222222, 0.36706349206349204, 0.843, 0.7833333333333333, 0.4714736101571821, 0.0, 0.7573655855083443, 0.5999999999999999, 1.0, 0.00017312618985154812], 
reward next is 0.9498, 
noisyNet noise sample is [array([-0.35211673], dtype=float32), 0.048703853]. 
=============================================
[2019-05-04 22:09:17,392] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:17,579] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:18,393] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:18,394] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:18,409] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res6/Eplus-env-sub_run2
[2019-05-04 22:09:18,794] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:18,984] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:19,384] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:19,556] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:19,627] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:19,736] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:19,794] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:19,796] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:19,796] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:19,798] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res10/Eplus-env-sub_run2
[2019-05-04 22:09:19,893] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,024] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,051] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,101] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6658595e-17 9.9999678e-01 1.4470519e-19 1.4876187e-20 1.7462042e-26
 1.5254611e-27 4.8152677e-18 3.0782163e-26 2.1030891e-18 8.2623866e-16
 3.2518078e-06], sum to 1.0000
[2019-05-04 22:09:20,101] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6111
[2019-05-04 22:09:20,119] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.725, 44.5, 2.725, 327.5, 0.0, 0.0, 16.0, 0.04141377454308578, 20.0, 23.46338419641434, 19.4, 0.0, 0.0], 
current ob forecast is [], 
actual action is [15.725, 20.0], 
sim time this is 7766100.0000, 
sim time next is 7767000.0000, 
raw observation next is [10.45, 46.0, 2.85, 335.0, 0.0, 0.0, 15.725, 0.0325399028883386, 20.0, 23.42947432660181, 19.4, 0.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6012820512820513, 0.46, 0.2590909090909091, 0.9305555555555556, 0.0, 0.0, 0.7620833333333333, 0.5108466342961129, 0.0, 0.7756391895145442, 0.1999999999999998, 0.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-0.34439364], dtype=float32), 1.7099315]. 
=============================================
[2019-05-04 22:09:20,142] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[93.051605]
 [92.97452 ]
 [93.00193 ]
 [93.368416]
 [93.77612 ]], R is [[93.10268402]
 [93.12165833]
 [93.14044189]
 [93.15903473]
 [93.17744446]].
[2019-05-04 22:09:20,188] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,213] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,264] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,264] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,272] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,335] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,355] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,369] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,374] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,384] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:20,384] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:20,386] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res11/Eplus-env-sub_run2
[2019-05-04 22:09:20,423] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,424] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,438] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,503] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,512] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,512] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:09:20,512] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,516] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,613] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:09:20,630] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:20,630] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:20,632] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res16/Eplus-env-sub_run2
[2019-05-04 22:09:20,738] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:20,738] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:20,740] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res15/Eplus-env-sub_run2
[2019-05-04 22:09:21,025] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,025] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,027] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res3/Eplus-env-sub_run2
[2019-05-04 22:09:21,052] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,053] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,055] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res4/Eplus-env-sub_run2
[2019-05-04 22:09:21,265] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,265] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,265] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,266] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,268] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res12/Eplus-env-sub_run2
[2019-05-04 22:09:21,289] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,290] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,292] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res5/Eplus-env-sub_run2
[2019-05-04 22:09:21,315] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res9/Eplus-env-sub_run2
[2019-05-04 22:09:21,343] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,343] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,345] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res14/Eplus-env-sub_run2
[2019-05-04 22:09:21,366] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,367] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,369] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res8/Eplus-env-sub_run2
[2019-05-04 22:09:21,393] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,393] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,394] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,394] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,396] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res17/Eplus-env-sub_run2
[2019-05-04 22:09:21,425] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res13/Eplus-env-sub_run2
[2019-05-04 22:09:21,557] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:09:21,557] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:09:21,559] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res2/Eplus-env-sub_run2
[2019-05-04 22:09:41,855] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2007043e-05 4.2310359e-07 2.6243131e-05 2.0119252e-05 1.6824186e-05
 5.5043015e-06 6.1102555e-06 1.8620582e-05 1.7764782e-05 4.1416257e-05
 9.9983490e-01], sum to 1.0000
[2019-05-04 22:09:41,855] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1803
[2019-05-04 22:09:41,879] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 61.0, 7.45, 262.5, 145.75, 200.5, -1.7, -0.0798554695761874, 65.0, 23.47179094673506, 22.7, 1.0, 54.33878922036229], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 137700.0000, 
sim time next is 138600.0000, 
raw observation next is [-6.7, 61.0, 7.699999999999999, 265.0, 148.0, 106.0, -1.7, -0.07709528732437772, 65.0, 23.49174490677502, 22.7, 1.0, 54.38320853216508], 
processed observation next is [1.0, 0.6086956521739131, 0.16153846153846155, 0.61, 0.7, 0.7361111111111112, 0.3915343915343915, 0.106, 0.4716666666666667, 0.4743015708918741, 1.0, 0.7845349866821455, 0.6714285714285714, 1.0, 0.6398024533195892], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6309951], dtype=float32), 1.1952299]. 
=============================================
[2019-05-04 22:09:48,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01699161 0.00290447 0.04495285 0.02240647 0.07621568 0.0141797
 0.01112806 0.03059863 0.02160228 0.02073432 0.73828596], sum to 1.0000
[2019-05-04 22:09:48,424] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3479
[2019-05-04 22:09:48,446] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 78.0, 3.6, 200.0, 0.0, 0.0, -3.9, -0.1906685397496132, 65.0, 22.98097259199981, 21.5, 0.0, 53.86937484739343], 
current ob forecast is [], 
actual action is [-3.9000000000000004, 65.0], 
sim time this is 187200.0000, 
sim time next is 188100.0000, 
raw observation next is [-8.9, 78.0, 3.85, 200.0, 0.0, 0.0, -3.9, -0.1983740839080659, 65.0, 22.95152867614459, 21.5, 0.0, 53.90512056912587], 
processed observation next is [1.0, 0.17391304347826086, 0.10512820512820512, 0.78, 0.35000000000000003, 0.5555555555555556, 0.0, 0.0, 0.435, 0.43387530536397806, 1.0, 0.7073612394492272, 0.5, 0.0, 0.6341778890485397], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2189078], dtype=float32), -1.585209]. 
=============================================
[2019-05-04 22:09:50,406] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.50840093e-09 1.00000000e+00 3.83990700e-10 2.80898482e-12
 1.49866449e-17 7.73234323e-19 7.07608546e-14 4.74537205e-17
 1.35356535e-11 9.95981769e-12 4.97021873e-08], sum to 1.0000
[2019-05-04 22:09:50,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1777
[2019-05-04 22:09:50,443] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.574999999999999, 75.75, 5.225, 197.5, 90.25, 0.0, -2.85, -0.4330342280042161, 20.0, 22.46526365300605, 22.7, 1.0, 12.49177507115219], 
current ob forecast is [], 
actual action is [-2.5749999999999993, 20.0], 
sim time this is 207900.0000, 
sim time next is 208800.0000, 
raw observation next is [-7.3, 75.0, 5.1, 200.0, 101.5, 0.0, -2.574999999999999, -0.4550769228834865, 20.0, 22.35339484097355, 22.7, 1.0, 10.432362418446148], 
processed observation next is [1.0, 0.43478260869565216, 0.14615384615384616, 0.75, 0.4636363636363636, 0.5555555555555556, 0.26851851851851855, 0.0, 0.45708333333333334, 0.34830769237217113, 0.0, 0.6219135487105072, 0.6714285714285714, 1.0, 0.12273367551113115], 
reward next is 0.8273, 
noisyNet noise sample is [array([1.4135654], dtype=float32), -1.442046]. 
=============================================
[2019-05-04 22:09:50,709] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1939759e-05 9.9798018e-01 3.9129141e-06 6.4241185e-08 2.8728557e-09
 1.8605907e-10 1.3235196e-07 3.4670888e-10 4.2228083e-07 5.7456543e-07
 1.9928466e-03], sum to 1.0000
[2019-05-04 22:09:50,710] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3627
[2019-05-04 22:09:50,733] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 65.0, 6.1, 220.0, 0.0, 0.0, 1.6, -1.002219087856737, 20.0, 19.74947877087761, 22.7, 1.0, 10.200936701336856], 
current ob forecast is [], 
actual action is [1.6, 20.0], 
sim time this is 239400.0000, 
sim time next is 240300.0000, 
raw observation next is [-3.4, 65.0, 5.85, 220.0, 0.0, 0.0, 1.6, -1.017704772815766, 20.0, 19.68370187643312, 22.7, 1.0, 10.23263332856331], 
processed observation next is [1.0, 0.782608695652174, 0.24615384615384614, 0.65, 0.5318181818181817, 0.6111111111111112, 0.0, 0.0, 0.5266666666666667, 0.160765075728078, 0.0, 0.2405288394904456, 0.6714285714285714, 1.0, 0.12038392151250953], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3394476], dtype=float32), -0.016788108]. 
=============================================
[2019-05-04 22:09:51,527] A3C_AGENT_WORKER-Thread-7 INFO:Local step 9000, global step 142885: loss 5.8123
[2019-05-04 22:09:51,528] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 9000, global step 142885: learning rate 0.0001
[2019-05-04 22:09:57,586] A3C_AGENT_WORKER-Thread-6 INFO:Local step 9000, global step 143807: loss 0.0206
[2019-05-04 22:09:57,586] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 9000, global step 143807: learning rate 0.0001
[2019-05-04 22:09:58,379] A3C_AGENT_WORKER-Thread-5 INFO:Local step 9000, global step 143892: loss 4.2641
[2019-05-04 22:09:58,380] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 9000, global step 143892: learning rate 0.0001
[2019-05-04 22:09:58,614] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 143914: loss 10.9896
[2019-05-04 22:09:58,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 143914: learning rate 0.0001
[2019-05-04 22:09:58,676] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 143919: loss 1.2444
[2019-05-04 22:09:58,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 143919: learning rate 0.0001
[2019-05-04 22:09:58,706] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 143926: loss 3.3518
[2019-05-04 22:09:58,706] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 143926: learning rate 0.0001
[2019-05-04 22:09:58,857] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 143938: loss -0.0668
[2019-05-04 22:09:58,867] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 143940: learning rate 0.0001
[2019-05-04 22:09:59,212] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 143974: loss -0.1433
[2019-05-04 22:09:59,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 143974: learning rate 0.0001
[2019-05-04 22:09:59,717] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 144024: loss -0.4605
[2019-05-04 22:09:59,757] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 144024: learning rate 0.0001
[2019-05-04 22:09:59,799] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144033: loss -0.4902
[2019-05-04 22:09:59,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144034: learning rate 0.0001
[2019-05-04 22:10:01,101] A3C_AGENT_WORKER-Thread-8 INFO:Local step 9000, global step 144250: loss -0.5174
[2019-05-04 22:10:01,103] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 9000, global step 144250: learning rate 0.0001
[2019-05-04 22:10:01,887] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 144428: loss -3.4941
[2019-05-04 22:10:01,888] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 144428: learning rate 0.0001
[2019-05-04 22:10:01,920] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 144436: loss 0.2651
[2019-05-04 22:10:01,920] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 144436: learning rate 0.0001
[2019-05-04 22:10:02,030] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 144463: loss 0.5545
[2019-05-04 22:10:02,031] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 144463: learning rate 0.0001
[2019-05-04 22:10:02,266] A3C_AGENT_WORKER-Thread-4 INFO:Local step 9000, global step 144511: loss -0.4493
[2019-05-04 22:10:02,266] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 9000, global step 144511: learning rate 0.0001
[2019-05-04 22:10:02,574] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144564: loss 0.5767
[2019-05-04 22:10:02,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144564: learning rate 0.0001
[2019-05-04 22:10:06,353] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.6477500e-05 2.9230127e-02 9.5564130e-05 6.6848861e-06 2.0249898e-08
 3.9038932e-09 9.5838845e-07 8.5256193e-09 4.1610410e-06 4.5159791e-06
 9.7059155e-01], sum to 1.0000
[2019-05-04 22:10:06,353] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1190
[2019-05-04 22:10:06,610] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.1, 41.5, 4.6, 185.0, 0.0, 0.0, -3.25, -1.232212549917292, 65.0, 19.17245789341347, 22.7, 1.0, 85.71457069261857], 
current ob forecast is [], 
actual action is [-3.0999999999999996, 65.0], 
sim time this is 459000.0000, 
sim time next is 459900.0000, 
raw observation next is [-7.949999999999999, 40.75, 4.85, 187.5, 0.0, 0.0, -3.1, -1.177887338581188, 65.0, 19.57011726273119, 22.7, 1.0, 79.01612041646375], 
processed observation next is [1.0, 0.30434782608695654, 0.1294871794871795, 0.4075, 0.44090909090909086, 0.5208333333333334, 0.0, 0.0, 0.4483333333333333, 0.10737088713960397, 1.0, 0.2243024661044559, 0.6714285714285714, 1.0, 0.9296014166642794], 
reward next is 0.5433, 
noisyNet noise sample is [array([-0.12474512], dtype=float32), -0.65068895]. 
=============================================
[2019-05-04 22:10:17,567] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.4663858e-06 2.4026047e-02 2.1396879e-06 2.2237150e-09 2.4150308e-13
 6.2294885e-13 9.8384662e-11 1.0532317e-12 5.7640182e-09 1.5626233e-08
 9.7596437e-01], sum to 1.0000
[2019-05-04 22:10:17,567] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5158
[2019-05-04 22:10:17,773] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.949999999999999, 40.75, 4.85, 187.5, 0.0, 0.0, -3.1, -1.159195631698079, 65.0, 19.52106031395731, 22.7, 1.0, 83.90428314894947], 
current ob forecast is [], 
actual action is [-2.9499999999999993, 65.0], 
sim time this is 459900.0000, 
sim time next is 460800.0000, 
raw observation next is [-7.8, 40.0, 5.1, 190.0, 11.5, 0.0, -2.949999999999999, -1.118349185770761, 65.0, 19.85583943376917, 22.7, 1.0, 76.57528345872794], 
processed observation next is [1.0, 0.34782608695652173, 0.13333333333333333, 0.4, 0.4636363636363636, 0.5277777777777778, 0.03042328042328042, 0.0, 0.45083333333333336, 0.127216938076413, 1.0, 0.26511991910988136, 0.6714285714285714, 1.0, 0.9008856877497404], 
reward next is 0.4085, 
noisyNet noise sample is [array([2.2293868], dtype=float32), 0.42189202]. 
=============================================
[2019-05-04 22:10:18,136] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.14019695e-05 3.21515530e-01 1.48716526e-05 2.75193681e-08
 2.38334287e-12 5.94370907e-13 7.77318321e-10 1.61286577e-12
 2.00186179e-07 1.40166456e-08 6.78457916e-01], sum to 1.0000
[2019-05-04 22:10:18,136] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8834
[2019-05-04 22:10:18,312] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 85.0, 6.35, 325.0, 77.0, 141.0, 4.4, -0.5905290877456257, 20.0, 21.81080768585511, 22.7, 1.0, 30.222920454116775], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 556200.0000, 
sim time next is 557100.0000, 
raw observation next is [-0.6, 84.0, 6.225, 322.5, 80.0, 139.5, 4.4, -0.5411391814551779, 65.0, 21.78339204291298, 22.7, 1.0, 74.87763576133067], 
processed observation next is [0.0, 0.43478260869565216, 0.317948717948718, 0.84, 0.5659090909090909, 0.8958333333333334, 0.21164021164021163, 0.1395, 0.5733333333333334, 0.31962027284827405, 1.0, 0.540484577558997, 0.6714285714285714, 1.0, 0.8809133618980078], 
reward next is 0.2057, 
noisyNet noise sample is [array([1.4956958], dtype=float32), -0.6791219]. 
=============================================
[2019-05-04 22:10:35,707] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-05-04 22:10:35,716] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation job starts!
[2019-05-04 22:10:35,717] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation job starts!
[2019-05-04 22:10:35,721] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:10:35,723] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Test-Repeat-Real97-v1-res1/Eplus-env-sub_run4
[2019-05-04 22:10:35,721] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:10:35,743] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res1/Eplus-env-sub_run4
[2019-05-04 22:11:18,695] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 DEBUG:NoisyNet noise sample: [array([0.00917689], dtype=float32), 0.13024472]
[2019-05-04 22:11:18,696] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 DEBUG:Observation this: [11.76111111, 97.6, 0.0, 135.0, 2.398959781, 1.062865678, 16.90555556, 79.39478724321172, 20.0, 14.55070730826683, 22.0, 1.0, 8.968660125426709]
[2019-05-04 22:11:18,696] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 DEBUG:Observation forecast: []
[2019-05-04 22:11:18,697] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Softmax [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], sampled 0.21022401946694347
[2019-05-04 22:12:10,051] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation: average rewards by now are 4595.7612 73651.5758 272758.3191
[2019-05-04 22:12:10,061] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:12:10,061] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:12:10,061] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:12:10,061] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:12:10,176] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:12:10,176] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:12:10,176] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:12:10,176] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:14:00,804] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:NoisyNet noise sample: [array([0.00917689], dtype=float32), 0.13024472]
[2019-05-04 22:14:00,804] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:Observation this: [21.5, 62.0, 7.050000000000001, 235.0, 368.25, 326.0, 27.0, 0.4647399539249024, 20.0, 24.99956778603299, 20.56, 1.0, 0.0]
[2019-05-04 22:14:00,804] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:Observation forecast: []
[2019-05-04 22:14:00,806] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Softmax [5.8969736e-14 9.9999988e-01 3.2038364e-14 9.6227930e-20 8.6274019e-26
 9.1672571e-28 1.2370242e-20 6.7808989e-27 7.4617025e-17 3.8892101e-18
 1.0432048e-07], sampled 0.8000461115745447
[2019-05-04 22:14:08,972] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation: average rewards by now are 5710.8603 150985.4714 -2369.3496
[2019-05-04 22:14:08,983] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:14:08,983] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:14:08,983] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:14:08,983] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:14:09,090] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:14:09,090] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:14:09,090] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:14:09,090] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:14:09,986] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 150000, evaluation results [150000.0, 5710.860345358241, 150985.4713748832, -2369.349631838523, 4595.761158783212, 73651.57584369082, 272758.3191094142]
[2019-05-04 22:14:10,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8990629e-12 9.9999976e-01 9.2617038e-13 3.0293369e-17 1.7890206e-23
 5.4762616e-25 6.1752366e-18 3.6074199e-23 2.4116559e-15 1.2303163e-15
 1.9080322e-07], sum to 1.0000
[2019-05-04 22:14:10,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7796
[2019-05-04 22:14:10,487] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 71.0, 3.0, 230.0, 0.0, 0.0, 1.225, -1.046045159828145, 20.0, 19.79825296068721, 21.5, 0.0, 7.56959187833867], 
current ob forecast is [], 
actual action is [1.1, 20.0], 
sim time this is 687600.0000, 
sim time next is 688500.0000, 
raw observation next is [-3.9, 71.0, 2.875, 232.5, 0.0, 0.0, 1.1, -1.062386682683615, 20.0, 19.72571085733859, 21.5, 0.0, 7.786940652945753], 
processed observation next is [0.0, 1.0, 0.23333333333333334, 0.71, 0.26136363636363635, 0.6458333333333334, 0.0, 0.0, 0.5183333333333333, 0.14587110577212833, 0.0, 0.24653012247694153, 0.5, 0.0, 0.09161106650524416], 
reward next is 0.8584, 
noisyNet noise sample is [array([-2.1853912], dtype=float32), -0.7891059]. 
=============================================
[2019-05-04 22:14:10,537] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[90.10896 ]
 [89.98434 ]
 [90.1215  ]
 [90.10522 ]
 [89.981514]], R is [[90.08634949]
 [90.0464325 ]
 [90.00981903]
 [89.97683716]
 [89.94788361]].
[2019-05-04 22:14:11,062] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.19033818e-12 9.99958158e-01 1.31486358e-12 7.13300580e-16
 8.77267853e-23 7.85888182e-25 1.04357784e-17 1.06128305e-23
 3.13582270e-14 4.91206053e-16 4.18653617e-05], sum to 1.0000
[2019-05-04 22:14:11,062] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0805
[2019-05-04 22:14:11,173] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.425, 67.5, 4.975, 260.0, 129.5, 63.25, 3.3, -0.7887754415145642, 65.0, 21.20285983655848, 22.7, 1.0, 73.7430614368782], 
current ob forecast is [], 
actual action is [3.575, 20.0], 
sim time this is 728100.0000, 
sim time next is 729000.0000, 
raw observation next is [-1.15, 67.0, 4.85, 260.0, 139.0, 68.0, 3.575, -0.7813508208105677, 20.0, 21.37590734499731, 22.7, 1.0, 44.255619673509734], 
processed observation next is [1.0, 0.43478260869565216, 0.3038461538461538, 0.67, 0.44090909090909086, 0.7222222222222222, 0.36772486772486773, 0.068, 0.5595833333333334, 0.23954972639647742, 0.0, 0.48227247785675836, 0.6714285714285714, 1.0, 0.5206543491001145], 
reward next is 0.0742, 
noisyNet noise sample is [array([0.09328092], dtype=float32), -1.6033964]. 
=============================================
[2019-05-04 22:14:11,182] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[92.33411]
 [92.9848 ]
 [92.88628]
 [92.72891]
 [91.5555 ]], R is [[92.50411224]
 [92.23480988]
 [91.84598541]
 [90.92752838]
 [90.11112976]].
[2019-05-04 22:14:11,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7831577e-09 9.9995828e-01 1.6289995e-09 1.8141422e-13 4.0823783e-18
 1.7252272e-19 3.1205192e-14 5.1341603e-18 9.2076437e-11 1.4449439e-12
 4.1769217e-05], sum to 1.0000
[2019-05-04 22:14:11,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8592
[2019-05-04 22:14:11,540] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 75.0, 3.6, 190.0, 0.0, 0.0, 1.6, -1.076921641382372, 20.0, 19.63942509752897, 21.5, 0.0, 41.74492974610484], 
current ob forecast is [], 
actual action is [1.6, 20.0], 
sim time this is 698400.0000, 
sim time next is 699300.0000, 
raw observation next is [-3.4, 75.0, 3.975, 197.5, 0.0, 0.0, 1.6, -1.096457553443233, 20.0, 19.62116088447417, 21.5, 0.0, 30.75599867631802], 
processed observation next is [1.0, 0.08695652173913043, 0.24615384615384614, 0.75, 0.3613636363636364, 0.5486111111111112, 0.0, 0.0, 0.5266666666666667, 0.13451414885225565, 0.0, 0.23159441206773831, 0.5, 0.0, 0.36183527854491787], 
reward next is 0.5882, 
noisyNet noise sample is [array([0.5719985], dtype=float32), -0.46016982]. 
=============================================
[2019-05-04 22:14:16,732] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.03011565e-07 9.99073625e-01 9.73821770e-08 4.57047594e-11
 5.09685506e-16 2.21884262e-15 3.83437813e-12 3.92146847e-16
 1.10982272e-08 3.93126115e-10 9.26078763e-04], sum to 1.0000
[2019-05-04 22:14:16,733] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4937
[2019-05-04 22:14:16,781] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.449999999999999, 60.25, 4.975, 350.0, 0.0, 0.0, -0.2999999999999998, -0.6455069551245202, 20.0, 21.5422800164659, 21.5, 0.0, 19.186258299388292], 
current ob forecast is [], 
actual action is [-0.4499999999999993, 20.0], 
sim time this is 765900.0000, 
sim time next is 766800.0000, 
raw observation next is [-5.6, 61.0, 5.1, 350.0, 0.0, 0.0, -0.4499999999999993, -0.6791374853289663, 20.0, 21.3888544726893, 21.5, 0.0, 16.126869927834566], 
processed observation next is [1.0, 0.9130434782608695, 0.18974358974358976, 0.61, 0.4636363636363636, 0.9722222222222222, 0.0, 0.0, 0.4925, 0.2736208382236779, 0.0, 0.4841220675270428, 0.5, 0.0, 0.18972788150393607], 
reward next is 0.7603, 
noisyNet noise sample is [array([-0.5642066], dtype=float32), 0.077019885]. 
=============================================
[2019-05-04 22:14:17,110] A3C_AGENT_WORKER-Thread-7 INFO:Local step 9500, global step 151069: loss 18.3700
[2019-05-04 22:14:17,112] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 9500, global step 151069: learning rate 0.0001
[2019-05-04 22:14:19,645] A3C_AGENT_WORKER-Thread-5 INFO:Local step 9500, global step 151511: loss 18.3064
[2019-05-04 22:14:19,645] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 9500, global step 151511: learning rate 0.0001
[2019-05-04 22:14:19,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3529642e-06 3.9698365e-01 3.9133745e-05 2.7025405e-07 2.0797444e-10
 6.3786761e-11 2.6567911e-08 8.4065178e-11 1.7320498e-06 1.5793123e-07
 6.0296869e-01], sum to 1.0000
[2019-05-04 22:14:19,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7902
[2019-05-04 22:14:20,136] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 54.5, 4.6, 335.0, 0.0, 0.0, 1.1, -0.2724091341531586, 65.0, 23.17890619222209, 22.7, 1.0, 66.91407065688692], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 757800.0000, 
sim time next is 758700.0000, 
raw observation next is [-3.9, 53.75, 4.85, 332.5, 0.0, 0.0, 1.1, -0.2383761038157091, 65.0, 23.36722808507328, 22.7, 1.0, 68.90973590702087], 
processed observation next is [1.0, 0.782608695652174, 0.23333333333333334, 0.5375, 0.44090909090909086, 0.9236111111111112, 0.0, 0.0, 0.5183333333333333, 0.420541298728097, 1.0, 0.7667468692961827, 0.6714285714285714, 1.0, 0.8107027753767162], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.72276425], dtype=float32), 0.40888193]. 
=============================================
[2019-05-04 22:14:20,383] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 151610: loss 14.0000
[2019-05-04 22:14:20,408] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 151610: learning rate 0.0001
[2019-05-04 22:14:20,408] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 151615: loss 14.4698
[2019-05-04 22:14:20,413] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 151616: learning rate 0.0001
[2019-05-04 22:14:20,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.67657765e-06 1.40134677e-01 1.50133365e-05 5.07416242e-09
 5.27942932e-11 1.03349595e-11 9.29649302e-10 2.52298031e-12
 7.88829368e-07 4.00126758e-08 8.59844804e-01], sum to 1.0000
[2019-05-04 22:14:20,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9057
[2019-05-04 22:14:20,606] A3C_AGENT_WORKER-Thread-6 INFO:Local step 9500, global step 151650: loss 11.7310
[2019-05-04 22:14:20,606] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 9500, global step 151650: learning rate 0.0001
[2019-05-04 22:14:20,658] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.125, 46.5, 4.85, 292.5, 81.25, 543.25, 5.25, -0.2892902101153844, 20.0, 23.34880493777723, 22.7, 1.0, 22.861966146474504], 
current ob forecast is [], 
actual action is [5.125, 65.0], 
sim time this is 744300.0000, 
sim time next is 745200.0000, 
raw observation next is [0.0, 47.0, 4.6, 300.0, 82.5, 372.5, 5.125, -0.2290144969918559, 65.0, 23.40318498902436, 22.7, 1.0, 71.154697634555], 
processed observation next is [1.0, 0.6521739130434783, 0.3333333333333333, 0.47, 0.41818181818181815, 0.8333333333333334, 0.21825396825396826, 0.3725, 0.5854166666666667, 0.42366183433604804, 1.0, 0.7718835698606229, 0.6714285714285714, 1.0, 0.8371140898182942], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41322866], dtype=float32), -1.3587925]. 
=============================================
[2019-05-04 22:14:21,475] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 151800: loss 8.7682
[2019-05-04 22:14:21,476] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 151800: learning rate 0.0001
[2019-05-04 22:14:21,543] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 151805: loss 8.6568
[2019-05-04 22:14:21,546] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 151805: learning rate 0.0001
[2019-05-04 22:14:22,387] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 151953: loss 8.2843
[2019-05-04 22:14:22,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 151953: learning rate 0.0001
[2019-05-04 22:14:22,467] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 151966: loss 8.3706
[2019-05-04 22:14:22,468] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 151966: learning rate 0.0001
[2019-05-04 22:14:24,425] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 152254: loss 3.5837
[2019-05-04 22:14:24,426] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 152254: learning rate 0.0001
[2019-05-04 22:14:26,265] A3C_AGENT_WORKER-Thread-8 INFO:Local step 9500, global step 152458: loss 8.3921
[2019-05-04 22:14:26,268] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 9500, global step 152458: learning rate 0.0001
[2019-05-04 22:14:26,834] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 152546: loss 4.3996
[2019-05-04 22:14:26,848] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 152547: loss 3.4438
[2019-05-04 22:14:26,848] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 152546: learning rate 0.0001
[2019-05-04 22:14:26,858] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 152547: learning rate 0.0001
[2019-05-04 22:14:27,024] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 152576: loss 3.6826
[2019-05-04 22:14:27,052] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 152576: learning rate 0.0001
[2019-05-04 22:14:27,164] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.76907485e-07 4.08198178e-01 3.37732018e-07 4.09995649e-10
 5.05411793e-13 2.22895856e-14 3.24522020e-10 1.36125708e-14
 1.00167705e-08 3.52613005e-09 5.91801226e-01], sum to 1.0000
[2019-05-04 22:14:27,167] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1457
[2019-05-04 22:14:27,198] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5806475e-08 9.9917161e-01 1.6698770e-07 1.0670650e-11 7.8474659e-16
 1.8960646e-16 3.8271404e-12 1.0793447e-15 5.9464172e-10 1.0878170e-10
 8.2814874e-04], sum to 1.0000
[2019-05-04 22:14:27,198] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7344
[2019-05-04 22:14:27,228] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 71.0, 2.375, 40.0, 0.0, 0.0, -2.3, -0.8056190251653567, 20.0, 20.84106922891734, 21.5, 0.0, 6.6719530412105215], 
current ob forecast is [], 
actual action is [-2.3, 20.0], 
sim time this is 780300.0000, 
sim time next is 781200.0000, 
raw observation next is [-7.3, 71.0, 2.5, 40.0, 0.0, 0.0, -2.3, -0.8272793777267191, 20.0, 20.74925434131329, 21.5, 0.0, 7.0199444786693865], 
processed observation next is [1.0, 0.043478260869565216, 0.14615384615384616, 0.71, 0.22727272727272727, 0.1111111111111111, 0.0, 0.0, 0.46166666666666667, 0.22424020742442696, 0.0, 0.39275062018761275, 0.5, 0.0, 0.08258758210199278], 
reward next is 0.8674, 
noisyNet noise sample is [array([0.13231094], dtype=float32), -0.49957725]. 
=============================================
[2019-05-04 22:14:27,327] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 84.25, 3.325, 82.5, 56.5, 0.0, 1.1, -0.384187528305221, 65.0, 23.08109984503603, 22.7, 1.0, 64.27302434927343], 
current ob forecast is [], 
actual action is [1.1, 20.0], 
sim time this is 830700.0000, 
sim time next is 831600.0000, 
raw observation next is [-3.9, 86.0, 3.6, 80.0, 54.0, 0.0, 1.1, -0.3833488163340307, 20.0, 23.17389067373631, 22.7, 1.0, 41.694476429690035], 
processed observation next is [1.0, 0.6521739130434783, 0.23333333333333334, 0.86, 0.32727272727272727, 0.2222222222222222, 0.14285714285714285, 0.0, 0.5183333333333333, 0.3722170612219898, 0.0, 0.7391272391051871, 0.6714285714285714, 1.0, 0.4905232521140004], 
reward next is 0.4595, 
noisyNet noise sample is [array([-1.2733241], dtype=float32), -1.0887992]. 
=============================================
[2019-05-04 22:14:27,609] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152695: loss 2.2345
[2019-05-04 22:14:27,616] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152696: learning rate 0.0001
[2019-05-04 22:14:27,788] A3C_AGENT_WORKER-Thread-4 INFO:Local step 9500, global step 152736: loss 1.9904
[2019-05-04 22:14:27,794] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 9500, global step 152737: learning rate 0.0001
[2019-05-04 22:14:32,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3459033e-11 9.9999785e-01 1.7759525e-11 2.7194272e-15 6.9992017e-20
 6.3653563e-22 5.9590601e-17 1.5528518e-20 1.5275525e-13 2.3902529e-14
 2.1191363e-06], sum to 1.0000
[2019-05-04 22:14:32,102] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4559
[2019-05-04 22:14:32,142] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.95, 80.0, 2.775, 72.5, 0.0, 0.0, 1.9, -0.7805238308524259, 20.0, 20.98913395546042, 21.5, 0.0, 7.428577000186054], 
current ob forecast is [], 
actual action is [2.05, 20.0], 
sim time this is 859500.0000, 
sim time next is 860400.0000, 
raw observation next is [-2.8, 79.0, 2.5, 80.0, 0.0, 0.0, 2.05, -0.8007616446950744, 20.0, 20.89969097334397, 21.5, 0.0, 6.488023077652337], 
processed observation next is [1.0, 1.0, 0.2615384615384615, 0.79, 0.22727272727272727, 0.2222222222222222, 0.0, 0.0, 0.5341666666666666, 0.23307945176830855, 0.0, 0.41424156762056696, 0.5, 0.0, 0.07632968326649808], 
reward next is 0.8737, 
noisyNet noise sample is [array([-0.62499684], dtype=float32), 0.67326367]. 
=============================================
[2019-05-04 22:14:37,898] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5314833e-11 9.9999189e-01 1.1512186e-11 1.9923737e-15 4.4163021e-20
 5.8116753e-21 1.0898832e-15 3.0840172e-20 9.6603400e-13 8.4677460e-15
 8.1380704e-06], sum to 1.0000
[2019-05-04 22:14:37,899] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5541
[2019-05-04 22:14:37,948] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.45, 72.0, 2.75, 105.0, 0.0, 0.0, 4.4, -1.138364299251771, 20.0, 19.3900825705067, 21.5, 0.0, 7.932379393783141], 
current ob forecast is [], 
actual action is [4.55, 20.0], 
sim time this is 882900.0000, 
sim time next is 883800.0000, 
raw observation next is [-0.3, 72.0, 2.5, 100.0, 0.0, 0.0, 4.55, -1.149394616202166, 20.0, 19.3390189351296, 21.5, 0.0, 7.958360020288045], 
processed observation next is [1.0, 0.21739130434782608, 0.32564102564102565, 0.72, 0.22727272727272727, 0.2777777777777778, 0.0, 0.0, 0.5758333333333333, 0.11686846126594468, 0.0, 0.1912884193042288, 0.5, 0.0, 0.09362776494456523], 
reward next is 0.8564, 
noisyNet noise sample is [array([-0.22750594], dtype=float32), -1.2738632]. 
=============================================
[2019-05-04 22:14:39,543] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6263524e-07 6.3891903e-02 1.4463135e-06 2.9438164e-11 1.4104030e-13
 6.6000227e-13 1.2947148e-10 3.3133601e-14 2.5542866e-08 5.9009073e-09
 9.3610632e-01], sum to 1.0000
[2019-05-04 22:14:39,543] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2684
[2019-05-04 22:14:39,695] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.55, 76.0, 3.3, 135.0, 29.0, 0.0, 5.275, -0.8158057014566001, 20.0, 21.20290823819765, 22.7, 1.0, 46.744283418388505], 
current ob forecast is [], 
actual action is [5.55, 65.0], 
sim time this is 894600.0000, 
sim time next is 895500.0000, 
raw observation next is [0.8250000000000001, 78.0, 3.45, 142.5, 33.75, 0.0, 5.55, -0.7878012934687981, 65.0, 21.18930862508031, 22.7, 1.0, 70.29740300593677], 
processed observation next is [1.0, 0.34782608695652173, 0.35448717948717945, 0.78, 0.31363636363636366, 0.3958333333333333, 0.08928571428571429, 0.0, 0.5924999999999999, 0.23739956884373395, 1.0, 0.4556155178686159, 0.6714285714285714, 1.0, 0.8270282706580797], 
reward next is 0.2800, 
noisyNet noise sample is [array([-0.45777133], dtype=float32), -0.16077985]. 
=============================================
[2019-05-04 22:14:39,709] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[57.350433]
 [57.101067]
 [56.85334 ]
 [56.56907 ]
 [57.39778 ]], R is [[57.44040298]
 [56.90213776]
 [56.98592377]
 [56.88957214]
 [57.32067871]].
[2019-05-04 22:14:43,229] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0633172e-06 9.1148245e-01 2.4110429e-07 9.7760910e-09 1.0665888e-11
 1.3862244e-13 4.0976098e-10 8.7047746e-13 8.6512514e-08 1.4717257e-07
 8.8515989e-02], sum to 1.0000
[2019-05-04 22:14:43,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3798
[2019-05-04 22:14:43,326] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.4, 93.0, 3.85, 145.0, 54.0, 0.0, 9.4, -0.1876748337686197, 20.0, 23.84783637931077, 22.7, 1.0, 27.510899981946768], 
current ob forecast is [], 
actual action is [9.4, 20.0], 
sim time this is 919800.0000, 
sim time next is 920700.0000, 
raw observation next is [4.4, 93.0, 3.725, 147.5, 45.0, 0.0, 9.4, -0.2139296328806433, 20.0, 23.6709991166558, 22.7, 1.0, 21.79103874934495], 
processed observation next is [1.0, 0.6521739130434783, 0.4461538461538461, 0.93, 0.3386363636363636, 0.4097222222222222, 0.11904761904761904, 0.0, 0.6566666666666666, 0.42869012237311893, 0.0, 0.8101427309508286, 0.6714285714285714, 1.0, 0.2563651617569994], 
reward next is 0.6936, 
noisyNet noise sample is [array([1.024178], dtype=float32), -0.9045261]. 
=============================================
[2019-05-04 22:14:49,527] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0306728e-13 9.9999988e-01 7.6560750e-15 1.4157857e-19 4.6041980e-25
 2.0633381e-26 9.1663462e-21 3.3034856e-27 4.8981728e-17 1.1460053e-18
 1.4533691e-07], sum to 1.0000
[2019-05-04 22:14:49,531] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7812
[2019-05-04 22:14:49,554] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.425, 55.5, 3.45, 200.0, 173.5, 79.25, 23.3, -0.2219433341558652, 20.0, 22.51308366099873, 22.7, 1.0, 0.031198683750477203], 
current ob forecast is [], 
actual action is [23.425, 20.0], 
sim time this is 1084500.0000, 
sim time next is 1085400.0000, 
raw observation next is [18.55, 55.0, 3.3, 190.0, 171.0, 0.0, 23.425, -0.2075736037559121, 20.0, 22.62597922787733, 22.7, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.808974358974359, 0.55, 0.3, 0.5277777777777778, 0.4523809523809524, 0.0, 0.8904166666666666, 0.4308087987480293, 0.0, 0.6608541754110472, 0.6714285714285714, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-0.705892], dtype=float32), -1.2803451]. 
=============================================
[2019-05-04 22:14:50,418] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8189719e-13 1.0000000e+00 8.7866547e-13 9.2066253e-18 1.9451066e-24
 1.0231240e-24 1.9470584e-17 2.5519027e-24 9.5386095e-16 1.6210872e-15
 1.2575073e-08], sum to 1.0000
[2019-05-04 22:14:50,421] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8935
[2019-05-04 22:14:50,437] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.75, 81.5, 3.85, 195.0, 0.0, 0.0, 18.025, -0.6471013605928001, 20.0, 20.54164053687627, 22.7, 1.0, 2.6231268719375147], 
current ob forecast is [], 
actual action is [17.75, 20.0], 
sim time this is 1063800.0000, 
sim time next is 1064700.0000, 
raw observation next is [12.475, 82.25, 3.975, 187.5, 0.0, 0.0, 17.75, -0.651464229374739, 20.0, 20.51869676223787, 22.7, 1.0, 2.677242032457509], 
processed observation next is [1.0, 0.30434782608695654, 0.6532051282051282, 0.8225, 0.3613636363636364, 0.5208333333333334, 0.0, 0.0, 0.7958333333333333, 0.282845256875087, 0.0, 0.3598138231768385, 0.6714285714285714, 1.0, 0.0314969650877354], 
reward next is 0.4067, 
noisyNet noise sample is [array([1.0004692], dtype=float32), -0.8987384]. 
=============================================
[2019-05-04 22:14:51,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6656195e-13 9.9999940e-01 2.1543680e-15 4.1189456e-19 8.3302189e-26
 2.7579434e-27 3.6229525e-20 3.4099218e-27 2.2454928e-15 4.2123190e-17
 5.7711543e-07], sum to 1.0000
[2019-05-04 22:14:51,633] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1694
[2019-05-04 22:14:51,655] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.925, 73.0, 6.374999999999999, 142.5, 70.0, 0.0, 20.5, -0.5212360177076204, 20.0, 21.09561602426795, 22.7, 1.0, 2.519623713518786], 
current ob forecast is [], 
actual action is [20.925, 20.0], 
sim time this is 1156500.0000, 
sim time next is 1157400.0000, 
raw observation next is [16.35, 71.0, 6.65, 145.0, 83.0, 0.0, 20.925, -0.5077885552801753, 20.0, 21.19885789122331, 22.7, 1.0, 2.297206892243468], 
processed observation next is [0.0, 0.391304347826087, 0.7525641025641026, 0.71, 0.6045454545454546, 0.4027777777777778, 0.21957671957671956, 0.0, 0.84875, 0.33073714823994155, 0.0, 0.456979698746187, 0.6714285714285714, 1.0, 0.027025963438158447], 
reward next is 0.9260, 
noisyNet noise sample is [array([1.9020367], dtype=float32), -1.1492993]. 
=============================================
[2019-05-04 22:14:51,806] A3C_AGENT_WORKER-Thread-7 INFO:Local step 10000, global step 158609: loss 0.1970
[2019-05-04 22:14:51,813] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 10000, global step 158610: learning rate 0.0001
[2019-05-04 22:14:52,288] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 158831: loss 0.1202
[2019-05-04 22:14:52,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 158831: learning rate 0.0001
[2019-05-04 22:14:52,916] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 159126: loss 0.2332
[2019-05-04 22:14:52,917] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 159126: learning rate 0.0001
[2019-05-04 22:14:53,014] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6280871e-13 1.0000000e+00 2.8928689e-14 2.7796934e-19 3.9797275e-24
 2.5103715e-27 2.6784606e-20 2.2037560e-26 7.9344663e-16 3.8647361e-17
 1.8856298e-08], sum to 1.0000
[2019-05-04 22:14:53,021] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7441
[2019-05-04 22:14:53,037] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.05, 67.25, 4.725, 132.5, 0.0, 0.0, 17.2, -0.4838061531760565, 20.0, 21.16095080201998, 21.5, 0.0, 2.2740326815655263], 
current ob forecast is [], 
actual action is [17.05, 20.0], 
sim time this is 1120500.0000, 
sim time next is 1121400.0000, 
raw observation next is [11.9, 68.5, 4.85, 135.0, 0.0, 0.0, 17.05, -0.4906330265178598, 20.0, 21.13437059193044, 21.5, 0.0, 2.369347751093802], 
processed observation next is [1.0, 1.0, 0.6384615384615384, 0.685, 0.44090909090909086, 0.375, 0.0, 0.0, 0.7841666666666666, 0.3364556578273801, 0.0, 0.44776722741863445, 0.5, 0.0, 0.027874679424632967], 
reward next is 0.9221, 
noisyNet noise sample is [array([-0.11215416], dtype=float32), 1.083345]. 
=============================================
[2019-05-04 22:14:53,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2873590e-15 1.0000000e+00 1.5116516e-14 7.8589566e-21 1.0291444e-24
 4.4997153e-28 3.9827994e-22 2.7435575e-28 1.3111943e-16 2.4373896e-20
 3.0611562e-09], sum to 1.0000
[2019-05-04 22:14:53,190] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4340
[2019-05-04 22:14:53,197] A3C_AGENT_WORKER-Thread-5 INFO:Local step 10000, global step 159263: loss 0.1528
[2019-05-04 22:14:53,201] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 10000, global step 159263: learning rate 0.0001
[2019-05-04 22:14:53,205] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.7, 67.0, 7.949999999999999, 145.0, 0.0, 0.0, 22.7, -0.3556781424881558, 20.0, 21.62294897079735, 21.5, 0.0, 0.14371844634373432], 
current ob forecast is [], 
actual action is [22.7, 20.0], 
sim time this is 1193400.0000, 
sim time next is 1194300.0000, 
raw observation next is [17.7, 67.0, 7.825, 147.5, 0.0, 0.0, 22.7, -0.3566440147331478, 20.0, 21.61663534651893, 21.5, 0.0, 0.16635193844309662], 
processed observation next is [0.0, 0.8260869565217391, 0.7871794871794872, 0.67, 0.7113636363636364, 0.4097222222222222, 0.0, 0.0, 0.8783333333333334, 0.3811186617556174, 0.0, 0.5166621923598471, 0.5, 0.0, 0.0019570816287423133], 
reward next is 0.9480, 
noisyNet noise sample is [array([-0.4011488], dtype=float32), 1.0449146]. 
=============================================
[2019-05-04 22:14:53,332] A3C_AGENT_WORKER-Thread-6 INFO:Local step 10000, global step 159322: loss 0.1627
[2019-05-04 22:14:53,334] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 10000, global step 159322: learning rate 0.0001
[2019-05-04 22:14:53,898] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 159591: loss 0.1000
[2019-05-04 22:14:53,901] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 159592: learning rate 0.0001
[2019-05-04 22:14:54,146] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 159701: loss 0.2790
[2019-05-04 22:14:54,147] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 159702: learning rate 0.0001
[2019-05-04 22:14:54,301] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 159766: loss 0.2873
[2019-05-04 22:14:54,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 159768: learning rate 0.0001
[2019-05-04 22:14:54,722] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9758379e-12 9.9988770e-01 8.3907658e-13 2.4982214e-15 1.0461406e-21
 2.7283729e-24 4.4399525e-17 2.6028272e-23 1.1181100e-14 3.3163290e-15
 1.1226407e-04], sum to 1.0000
[2019-05-04 22:14:54,726] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6886
[2019-05-04 22:14:54,746] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.8, 63.0, 9.7, 160.0, 165.5, 0.0, 23.675, -0.5198821078913155, 20.0, 21.14538376663168, 22.7, 1.0, 0.25488724886009395], 
current ob forecast is [], 
actual action is [23.8, 20.0], 
sim time this is 1166400.0000, 
sim time next is 1167300.0000, 
raw observation next is [18.675, 63.5, 9.825, 155.0, 168.25, 0.0, 23.8, -0.5279983360524406, 20.0, 21.09851116707729, 22.7, 1.0, 0.10431914671010066], 
processed observation next is [0.0, 0.5217391304347826, 0.8121794871794872, 0.635, 0.8931818181818181, 0.4305555555555556, 0.4451058201058201, 0.0, 0.8966666666666666, 0.3240005546491865, 0.0, 0.442644452439613, 0.6714285714285714, 1.0, 0.0012272840789423606], 
reward next is 0.9363, 
noisyNet noise sample is [array([-0.5043603], dtype=float32), -1.3016863]. 
=============================================
[2019-05-04 22:14:54,812] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 159995: loss 0.0312
[2019-05-04 22:14:54,814] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 159997: learning rate 0.0001
[2019-05-04 22:14:55,154] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 160144: loss 0.1249
[2019-05-04 22:14:55,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 160145: learning rate 0.0001
[2019-05-04 22:14:56,035] A3C_AGENT_WORKER-Thread-8 INFO:Local step 10000, global step 160539: loss 0.0321
[2019-05-04 22:14:56,038] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 10000, global step 160541: learning rate 0.0001
[2019-05-04 22:14:56,236] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 160633: loss 0.1758
[2019-05-04 22:14:56,242] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 160635: learning rate 0.0001
[2019-05-04 22:14:56,596] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 160802: loss 0.2036
[2019-05-04 22:14:56,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 160802: learning rate 0.0001
[2019-05-04 22:14:56,914] A3C_AGENT_WORKER-Thread-4 INFO:Local step 10000, global step 160942: loss 0.0275
[2019-05-04 22:14:56,914] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 10000, global step 160942: learning rate 0.0001
[2019-05-04 22:14:56,963] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 160957: loss 0.0694
[2019-05-04 22:14:56,965] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 160957: learning rate 0.0001
[2019-05-04 22:14:57,207] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 161053: loss 0.0241
[2019-05-04 22:14:57,208] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 161053: learning rate 0.0001
[2019-05-04 22:14:57,670] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8219236e-08 9.9990988e-01 8.7191332e-10 1.5225954e-13 5.0618627e-18
 1.2044213e-21 2.3315681e-13 2.8768744e-20 1.0075203e-10 5.7242714e-13
 9.0155991e-05], sum to 1.0000
[2019-05-04 22:14:57,671] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4473
[2019-05-04 22:14:57,689] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.5, 100.0, 5.625, 317.5, 0.0, 0.0, 10.5, -0.6864292410992441, 20.0, 19.94148496733023, 21.5, 0.0, 5.604568612439938], 
current ob forecast is [], 
actual action is [10.5, 20.0], 
sim time this is 1289700.0000, 
sim time next is 1290600.0000, 
raw observation next is [5.5, 100.0, 6.649999999999999, 295.0, 0.0, 0.0, 10.5, -0.696165179654963, 20.0, 19.90953847877083, 21.5, 0.0, 5.689818839761368], 
processed observation next is [0.0, 0.9565217391304348, 0.47435897435897434, 1.0, 0.6045454545454544, 0.8194444444444444, 0.0, 0.0, 0.675, 0.2679449401150123, 0.0, 0.2727912112529758, 0.5, 0.0, 0.06693904517366316], 
reward next is 0.8831, 
noisyNet noise sample is [array([-1.4182221], dtype=float32), 0.92921156]. 
=============================================
[2019-05-04 22:14:59,577] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7905802e-10 9.9999666e-01 4.2377638e-10 3.5928132e-14 5.4250965e-17
 2.6871006e-19 3.4745064e-13 1.5063389e-18 1.4641597e-11 4.9090140e-14
 3.3489794e-06], sum to 1.0000
[2019-05-04 22:14:59,578] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4805
[2019-05-04 22:14:59,610] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.4, 96.0, 7.2, 250.0, 0.0, 0.0, 9.675, -0.6504648002109237, 20.0, 20.21613839634949, 21.5, 0.0, 5.941698411941215], 
current ob forecast is [], 
actual action is [9.4, 20.0], 
sim time this is 1296000.0000, 
sim time next is 1296900.0000, 
raw observation next is [4.25, 95.25, 6.800000000000001, 250.0, 0.0, 0.0, 9.4, -0.6566094069442577, 20.0, 20.18542411598256, 21.5, 0.0, 6.0814791781867585], 
processed observation next is [1.0, 0.0, 0.4423076923076923, 0.9525, 0.6181818181818183, 0.6944444444444444, 0.0, 0.0, 0.6566666666666666, 0.28113019768524744, 0.0, 0.3122034451403657, 0.5, 0.0, 0.07154681386102069], 
reward next is 0.8785, 
noisyNet noise sample is [array([-0.02993802], dtype=float32), -0.6294531]. 
=============================================
[2019-05-04 22:15:02,963] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4306453e-05 9.7241962e-01 4.8505235e-06 5.8782586e-08 2.7495523e-11
 6.1317200e-13 2.5921330e-09 2.3223876e-11 3.0298793e-06 9.0961674e-07
 2.7547104e-02], sum to 1.0000
[2019-05-04 22:15:02,963] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4693
[2019-05-04 22:15:03,012] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 100.0, 2.65, 340.0, 0.0, 0.0, 4.4, -0.7475540498920591, 65.0, 20.25062414955148, 21.5, 0.0, 59.74631236629605], 
current ob forecast is [], 
actual action is [4.4, 20.0], 
sim time this is 1401300.0000, 
sim time next is 1402200.0000, 
raw observation next is [-0.6, 100.0, 2.8, 340.0, 0.0, 0.0, 4.4, -0.7365713503064478, 20.0, 20.43996398638846, 21.5, 0.0, 38.03923817587119], 
processed observation next is [1.0, 0.21739130434782608, 0.317948717948718, 1.0, 0.2545454545454545, 0.9444444444444444, 0.0, 0.0, 0.5733333333333334, 0.2544762165645174, 0.0, 0.34856628376977994, 0.5, 0.0, 0.44752044912789635], 
reward next is 0.5025, 
noisyNet noise sample is [array([-0.8250614], dtype=float32), -0.32730174]. 
=============================================
[2019-05-04 22:15:07,126] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7757935e-07 4.0588617e-01 1.7485293e-08 9.2121288e-10 7.7622969e-12
 2.1337073e-14 4.1538240e-10 5.7687758e-13 1.4240436e-08 1.2810875e-08
 5.9411353e-01], sum to 1.0000
[2019-05-04 22:15:07,126] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6908
[2019-05-04 22:15:07,291] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.375, 92.75, 2.75, 260.0, 94.5, 0.0, 5.25, -0.2877094867996517, 65.0, 22.58326509856652, 22.7, 1.0, 72.68097945023088], 
current ob forecast is [], 
actual action is [5.375, 65.0], 
sim time this is 1428300.0000, 
sim time next is 1429200.0000, 
raw observation next is [0.5, 92.0, 3.0, 340.0, 93.0, 0.0, 5.375, -0.2378671275602918, 65.0, 22.91551865510402, 22.7, 1.0, 66.430054488272], 
processed observation next is [1.0, 0.5652173913043478, 0.34615384615384615, 0.92, 0.2727272727272727, 0.9444444444444444, 0.24603174603174602, 0.0, 0.5895833333333333, 0.4207109574799028, 1.0, 0.7022169507291457, 0.6714285714285714, 1.0, 0.7815300528032001], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.08180411], dtype=float32), -0.624283]. 
=============================================
[2019-05-04 22:15:17,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.74733967e-12 9.99998093e-01 4.44123701e-14 4.60703702e-17
 1.04936246e-20 8.52363577e-23 4.09958705e-16 8.68191282e-22
 1.01385046e-12 6.91464377e-15 1.91800564e-06], sum to 1.0000
[2019-05-04 22:15:17,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8644
[2019-05-04 22:15:17,452] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.325, 73.25, 4.475, 102.5, 0.0, 0.0, 12.2, 0.06202249683067871, 20.0, 23.69801338231438, 21.5, 0.0, 9.80304836862344], 
current ob forecast is [], 
actual action is [12.325, 20.0], 
sim time this is 1541700.0000, 
sim time next is 1542600.0000, 
raw observation next is [7.45, 73.5, 4.35, 105.0, 0.0, 0.0, 12.325, 0.04629971106434624, 20.0, 23.60530783180398, 21.5, 0.0, 8.460519193656925], 
processed observation next is [1.0, 0.8695652173913043, 0.5243589743589744, 0.735, 0.39545454545454545, 0.2916666666666667, 0.0, 0.0, 0.7054166666666667, 0.5154332370214487, 0.0, 0.8007582616862828, 0.5, 0.0, 0.09953551992537558], 
reward next is 0.8505, 
noisyNet noise sample is [array([0.43839097], dtype=float32), 2.950853]. 
=============================================
[2019-05-04 22:15:18,718] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.87814754e-11 9.69881415e-01 4.58576094e-11 2.21608170e-12
 1.17077415e-17 2.39913935e-18 5.31557152e-14 4.46363254e-19
 2.49908150e-11 3.04408191e-13 3.01186088e-02], sum to 1.0000
[2019-05-04 22:15:18,718] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5236
[2019-05-04 22:15:18,755] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.6, 52.0, 2.5, 130.0, 76.0, 570.5, 16.2, 0.000447092296532244, 20.0, 23.67942401460202, 22.7, 1.0, 1.2037648834559282], 
current ob forecast is [], 
actual action is [16.6, 20.0], 
sim time this is 1522800.0000, 
sim time next is 1523700.0000, 
raw observation next is [11.75, 51.5, 2.5, 130.0, 76.5, 524.25, 16.6, 0.03626674633080868, 20.0, 23.98480177811499, 22.7, 1.0, 1.1283852064398856], 
processed observation next is [1.0, 0.6521739130434783, 0.6346153846153846, 0.515, 0.22727272727272727, 0.3611111111111111, 0.20238095238095238, 0.52425, 0.7766666666666667, 0.5120889154436029, 0.0, 0.8549716825878555, 0.6714285714285714, 1.0, 0.01327512007576336], 
reward next is 0.9367, 
noisyNet noise sample is [array([0.37149057], dtype=float32), 0.70628244]. 
=============================================
[2019-05-04 22:15:24,118] A3C_AGENT_WORKER-Thread-7 INFO:Local step 10500, global step 167044: loss 2.1177
[2019-05-04 22:15:24,123] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 10500, global step 167044: learning rate 0.0001
[2019-05-04 22:15:24,337] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 167119: loss 4.0282
[2019-05-04 22:15:24,339] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 167119: learning rate 0.0001
[2019-05-04 22:15:24,498] A3C_AGENT_WORKER-Thread-5 INFO:Local step 10500, global step 167182: loss 1.3320
[2019-05-04 22:15:24,506] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 10500, global step 167184: learning rate 0.0001
[2019-05-04 22:15:24,511] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 167186: loss 5.5050
[2019-05-04 22:15:24,512] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 167188: learning rate 0.0001
[2019-05-04 22:15:24,832] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2653512e-14 1.0000000e+00 1.6997582e-17 3.4420218e-19 3.6451555e-24
 1.4862414e-25 1.4413097e-20 3.5500400e-25 5.8483185e-17 2.0076238e-17
 1.0159319e-11], sum to 1.0000
[2019-05-04 22:15:24,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5406
[2019-05-04 22:15:24,850] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.9, 81.0, 7.45, 115.0, 0.0, 0.0, 12.05, -0.2740255469858515, 20.0, 22.09976925071157, 21.5, 0.0, 3.916382921995463], 
current ob forecast is [], 
actual action is [11.9, 20.0], 
sim time this is 1632600.0000, 
sim time next is 1633500.0000, 
raw observation next is [6.749999999999999, 83.5, 7.325, 112.5, 0.0, 0.0, 11.9, -0.2812769878162789, 20.0, 22.06626810480063, 21.5, 0.0, 4.071368173682074], 
processed observation next is [1.0, 0.9130434782608695, 0.5064102564102564, 0.835, 0.6659090909090909, 0.3125, 0.0, 0.0, 0.6983333333333334, 0.40624100406124036, 0.0, 0.5808954435429473, 0.5, 0.0, 0.04789844910214204], 
reward next is 0.9021, 
noisyNet noise sample is [array([0.16635653], dtype=float32), -1.0632949]. 
=============================================
[2019-05-04 22:15:24,874] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.10976 ]
 [82.93083 ]
 [82.80592 ]
 [82.846985]
 [82.7174  ]], R is [[83.37296295]
 [83.44315338]
 [83.51457977]
 [83.58742523]
 [83.66184235]].
[2019-05-04 22:15:25,115] A3C_AGENT_WORKER-Thread-6 INFO:Local step 10500, global step 167409: loss -4.2519
[2019-05-04 22:15:25,116] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 10500, global step 167409: learning rate 0.0001
[2019-05-04 22:15:25,289] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167472: loss 12.1308
[2019-05-04 22:15:25,291] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167472: learning rate 0.0001
[2019-05-04 22:15:25,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.8774684e-17 1.0000000e+00 1.4939497e-17 6.3982380e-21 4.4240811e-26
 4.2181371e-27 1.2989044e-21 6.6926258e-27 3.0907448e-19 3.5370351e-18
 4.7245402e-12], sum to 1.0000
[2019-05-04 22:15:25,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1397
[2019-05-04 22:15:25,339] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.9, 89.5, 3.3, 185.0, 0.0, 0.0, 12.05, -0.423935354617693, 20.0, 21.5536484381838, 21.5, 0.0, 4.848874399413657], 
current ob forecast is [], 
actual action is [11.9, 20.0], 
sim time this is 1643400.0000, 
sim time next is 1644300.0000, 
raw observation next is [6.749999999999999, 91.25, 2.9, 227.5, 0.0, 0.0, 11.9, -0.4319440407422071, 20.0, 21.52300712403849, 21.5, 0.0, 4.8171128156733145], 
processed observation next is [1.0, 0.0, 0.5064102564102564, 0.9125, 0.2636363636363636, 0.6319444444444444, 0.0, 0.0, 0.6983333333333334, 0.356018653085931, 0.0, 0.5032867320054984, 0.5, 0.0, 0.05667191547850958], 
reward next is 0.8933, 
noisyNet noise sample is [array([0.23868722], dtype=float32), -1.2132977]. 
=============================================
[2019-05-04 22:15:25,350] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 167494: loss 1.7926
[2019-05-04 22:15:25,351] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 167494: learning rate 0.0001
[2019-05-04 22:15:26,386] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 167961: loss 0.2584
[2019-05-04 22:15:26,391] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 167962: learning rate 0.0001
[2019-05-04 22:15:26,935] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168160: loss 0.7762
[2019-05-04 22:15:26,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168160: learning rate 0.0001
[2019-05-04 22:15:27,029] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 168193: loss 2.6805
[2019-05-04 22:15:27,031] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 168194: learning rate 0.0001
[2019-05-04 22:15:28,226] A3C_AGENT_WORKER-Thread-8 INFO:Local step 10500, global step 168554: loss 1.2523
[2019-05-04 22:15:28,228] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 10500, global step 168555: learning rate 0.0001
[2019-05-04 22:15:28,238] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168559: loss 1.3475
[2019-05-04 22:15:28,240] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168559: learning rate 0.0001
[2019-05-04 22:15:28,284] A3C_AGENT_WORKER-Thread-4 INFO:Local step 10500, global step 168571: loss 0.8129
[2019-05-04 22:15:28,284] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 10500, global step 168571: learning rate 0.0001
[2019-05-04 22:15:28,853] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 168656: loss 0.2446
[2019-05-04 22:15:28,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 168656: learning rate 0.0001
[2019-05-04 22:15:29,186] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168700: loss 1.0679
[2019-05-04 22:15:29,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168700: learning rate 0.0001
[2019-05-04 22:15:29,986] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 168802: loss 0.7901
[2019-05-04 22:15:29,987] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 168802: learning rate 0.0001
[2019-05-04 22:15:35,759] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0732922e-06 2.2225616e-02 8.5896869e-07 7.1615887e-09 4.7713059e-11
 1.7928454e-13 2.8428412e-10 6.5154067e-12 4.3203068e-09 1.2144874e-08
 9.7777241e-01], sum to 1.0000
[2019-05-04 22:15:35,760] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4924
[2019-05-04 22:15:35,915] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 83.0, 9.2, 250.0, 45.5, 0.0, 3.3, -0.8313875079275617, 65.0, 20.23351703974068, 22.7, 1.0, 83.46663349695685], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 1760400.0000, 
sim time next is 1761300.0000, 
raw observation next is [-1.85, 84.0, 9.2, 250.0, 55.25, 0.0, 3.3, -0.7512170053384342, 65.0, 20.75368396049722, 22.7, 1.0, 81.1361302938146], 
processed observation next is [0.0, 0.391304347826087, 0.2858974358974359, 0.84, 0.8363636363636363, 0.6944444444444444, 0.14616402116402116, 0.0, 0.5549999999999999, 0.24959433155385527, 1.0, 0.3933834229281743, 0.6714285714285714, 1.0, 0.9545427093389953], 
reward next is 0.8017, 
noisyNet noise sample is [array([0.06644791], dtype=float32), 1.8747977]. 
=============================================
[2019-05-04 22:15:54,229] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.7841990e-10 9.9999952e-01 3.6528201e-11 4.9046717e-12 2.9068595e-15
 1.9574401e-16 2.0701600e-12 3.4621885e-15 6.4617721e-11 1.7903087e-10
 4.6999276e-07], sum to 1.0000
[2019-05-04 22:15:54,244] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7836
[2019-05-04 22:15:54,257] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.749999999999999, 84.0, 3.325, 260.0, 0.0, 0.0, -0.9000000000000004, -1.033620056449237, 20.0, 19.62054237295349, 21.5, 0.0, 8.876836120338547], 
current ob forecast is [], 
actual action is [-0.7499999999999991, 20.0], 
sim time this is 1993500.0000, 
sim time next is 1994400.0000, 
raw observation next is [-5.6, 83.0, 3.6, 260.0, 0.0, 0.0, -0.7499999999999991, -1.050621224698644, 20.0, 19.5489176885912, 21.5, 0.0, 9.031161414763043], 
processed observation next is [1.0, 0.08695652173913043, 0.18974358974358976, 0.83, 0.32727272727272727, 0.7222222222222222, 0.0, 0.0, 0.4875, 0.14979292510045203, 0.0, 0.2212739555130285, 0.5, 0.0, 0.10624895782074167], 
reward next is 0.8438, 
noisyNet noise sample is [array([0.04378764], dtype=float32), 0.4216794]. 
=============================================
[2019-05-04 22:15:54,832] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.0719595e-08 5.2005559e-01 1.3165285e-07 9.4657535e-09 3.5740366e-11
 2.7777754e-12 1.1562255e-08 2.2672118e-11 2.0957625e-09 5.5252471e-08
 4.7994414e-01], sum to 1.0000
[2019-05-04 22:15:54,833] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2928
[2019-05-04 22:15:54,851] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.4, 78.0, 4.975, 247.5, 0.0, 0.0, -3.4, -1.316752470251235, 20.0, 18.60597702154302, 21.5, 0.0, 11.9995204504632], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 20.0], 
sim time this is 1912500.0000, 
sim time next is 1913400.0000, 
raw observation next is [-8.4, 78.0, 5.35, 245.0, 0.0, 0.0, -3.4, -1.334983356751196, 20.0, 18.52447102079952, 21.5, 0.0, 12.109743990244823], 
processed observation next is [1.0, 0.13043478260869565, 0.11794871794871795, 0.78, 0.48636363636363633, 0.6805555555555556, 0.0, 0.0, 0.44333333333333336, 0.05500554774960135, 0.0, 0.07492443154278863, 0.5, 0.0, 0.14246757635582144], 
reward next is 0.0861, 
noisyNet noise sample is [array([-1.9082199], dtype=float32), -1.134542]. 
=============================================
[2019-05-04 22:16:04,117] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.4647205e-08 7.3989886e-01 3.3086039e-08 1.4070909e-09 4.7936763e-12
 5.9429279e-13 1.0389198e-09 9.1089831e-13 7.1135280e-09 9.5133794e-09
 2.6010099e-01], sum to 1.0000
[2019-05-04 22:16:04,118] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3455
[2019-05-04 22:16:04,207] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 84.0, 4.35, 260.0, 0.0, 0.0, 1.1, -0.4164953038917331, 20.0, 22.31441227326643, 22.7, 1.0, 29.788108414495536], 
current ob forecast is [], 
actual action is [1.1, 20.0], 
sim time this is 2057400.0000, 
sim time next is 2058300.0000, 
raw observation next is [-3.9, 85.0, 4.225, 265.0, 0.0, 0.0, 1.1, -0.4649366237465602, 20.0, 22.0069236617526, 22.7, 1.0, 23.46672776868573], 
processed observation next is [1.0, 0.8260869565217391, 0.23333333333333334, 0.85, 0.38409090909090904, 0.7361111111111112, 0.0, 0.0, 0.5183333333333333, 0.34502112541781327, 0.0, 0.5724176659646574, 0.6714285714285714, 1.0, 0.2760791502198321], 
reward next is 0.6739, 
noisyNet noise sample is [array([0.45517737], dtype=float32), 1.7712395]. 
=============================================
[2019-05-04 22:16:05,760] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8163822e-08 5.4987252e-01 4.2806892e-08 1.7130850e-10 5.6411195e-12
 1.5327324e-13 1.1734279e-10 4.9239141e-12 1.4445743e-07 5.1646074e-09
 4.5012721e-01], sum to 1.0000
[2019-05-04 22:16:05,760] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1588
[2019-05-04 22:16:05,890] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 77.0, 4.6, 265.0, 156.0, 0.0, 0.5, -0.5482882446974818, 65.0, 21.91754159029824, 22.7, 1.0, 72.66162442240729], 
current ob forecast is [], 
actual action is [0.5, 20.0], 
sim time this is 2032200.0000, 
sim time next is 2033100.0000, 
raw observation next is [-4.5, 78.0, 4.6, 262.5, 154.0, 0.0, 0.5, -0.5359790511042734, 20.0, 22.14786539463343, 22.7, 1.0, 43.23253089731906], 
processed observation next is [1.0, 0.5217391304347826, 0.21794871794871795, 0.78, 0.41818181818181815, 0.7291666666666666, 0.4074074074074074, 0.0, 0.5083333333333333, 0.32134031629857557, 0.0, 0.5925521992333473, 0.6714285714285714, 1.0, 0.5086180105566948], 
reward next is 0.5317, 
noisyNet noise sample is [array([0.56207734], dtype=float32), 0.7458884]. 
=============================================
[2019-05-04 22:16:10,277] A3C_AGENT_WORKER-Thread-7 INFO:Local step 11000, global step 175171: loss 0.2018
[2019-05-04 22:16:10,277] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 11000, global step 175171: learning rate 0.0001
[2019-05-04 22:16:10,388] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 175186: loss 0.1746
[2019-05-04 22:16:10,389] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 175186: learning rate 0.0001
[2019-05-04 22:16:11,593] A3C_AGENT_WORKER-Thread-5 INFO:Local step 11000, global step 175362: loss 0.7888
[2019-05-04 22:16:11,596] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 11000, global step 175363: learning rate 0.0001
[2019-05-04 22:16:11,764] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 175394: loss 1.1210
[2019-05-04 22:16:11,765] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 175395: learning rate 0.0001
[2019-05-04 22:16:11,890] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.8159210e-11 9.9999881e-01 2.0265019e-11 1.3735706e-12 3.5773242e-15
 5.9437613e-18 4.6971991e-13 9.0680295e-18 2.4763466e-12 5.1555982e-13
 1.1406747e-06], sum to 1.0000
[2019-05-04 22:16:11,891] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9151
[2019-05-04 22:16:11,926] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 85.0, 3.6, 260.0, 0.0, 0.0, 1.1, -0.5894338443373089, 20.0, 21.35656191894766, 21.5, 0.0, 12.960552327702485], 
current ob forecast is [], 
actual action is [1.1, 20.0], 
sim time this is 2063700.0000, 
sim time next is 2064600.0000, 
raw observation next is [-3.9, 84.0, 3.6, 260.0, 0.0, 0.0, 1.1, -0.6131559305363409, 20.0, 21.24251263635819, 21.5, 0.0, 11.013304354003743], 
processed observation next is [1.0, 0.9130434782608695, 0.23333333333333334, 0.84, 0.32727272727272727, 0.7222222222222222, 0.0, 0.0, 0.5183333333333333, 0.2956146898212197, 0.0, 0.46321609090831267, 0.5, 0.0, 0.1295682865176911], 
reward next is 0.8204, 
noisyNet noise sample is [array([0.18664294], dtype=float32), 1.8147894]. 
=============================================
[2019-05-04 22:16:13,014] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.2643743e-08 9.4273204e-01 1.7254704e-08 3.8179855e-09 4.6499691e-13
 6.7835489e-13 2.9657229e-10 1.9618519e-12 1.4130395e-08 4.4435535e-09
 5.7267804e-02], sum to 1.0000
[2019-05-04 22:16:13,015] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9963
[2019-05-04 22:16:13,205] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 65.75, 4.975, 277.5, 41.0, 0.0, 0.5, -0.2619422899979315, 65.0, 23.18003173903639, 22.7, 1.0, 75.1972841407552], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2132100.0000, 
sim time next is 2133000.0000, 
raw observation next is [-4.5, 66.5, 4.85, 275.0, 26.0, 0.0, 0.5, -0.214490914739784, 65.0, 23.3948963968076, 22.7, 1.0, 71.53196760030046], 
processed observation next is [1.0, 0.6956521739130435, 0.21794871794871795, 0.665, 0.44090909090909086, 0.7638888888888888, 0.06878306878306878, 0.0, 0.5083333333333333, 0.428503028420072, 1.0, 0.7706994852582285, 0.6714285714285714, 1.0, 0.8415525600035348], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.20813386], dtype=float32), -0.4362938]. 
=============================================
[2019-05-04 22:16:13,220] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[53.45328 ]
 [53.427727]
 [53.209133]
 [53.245922]
 [52.776817]], R is [[52.89000702]
 [52.36110687]
 [52.43947983]
 [52.36021042]
 [51.83660889]].
[2019-05-04 22:16:13,651] A3C_AGENT_WORKER-Thread-6 INFO:Local step 11000, global step 175705: loss 0.0017
[2019-05-04 22:16:13,653] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 11000, global step 175705: learning rate 0.0001
[2019-05-04 22:16:13,806] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 175732: loss 0.3700
[2019-05-04 22:16:13,807] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 175732: learning rate 0.0001
[2019-05-04 22:16:15,662] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176000: loss 0.8018
[2019-05-04 22:16:15,663] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176000: learning rate 0.0001
[2019-05-04 22:16:16,065] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 176069: loss 1.1344
[2019-05-04 22:16:16,066] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 176069: learning rate 0.0001
[2019-05-04 22:16:17,094] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 176248: loss 0.8243
[2019-05-04 22:16:17,094] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 176248: learning rate 0.0001
[2019-05-04 22:16:17,204] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 176269: loss 0.6002
[2019-05-04 22:16:17,206] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 176269: learning rate 0.0001
[2019-05-04 22:16:17,433] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 176317: loss 1.1470
[2019-05-04 22:16:17,434] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 176317: learning rate 0.0001
[2019-05-04 22:16:18,210] A3C_AGENT_WORKER-Thread-4 INFO:Local step 11000, global step 176439: loss 3.1395
[2019-05-04 22:16:18,232] A3C_AGENT_WORKER-Thread-8 INFO:Local step 11000, global step 176441: loss 0.9473
[2019-05-04 22:16:18,232] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 11000, global step 176441: learning rate 0.0001
[2019-05-04 22:16:18,234] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 11000, global step 176439: learning rate 0.0001
[2019-05-04 22:16:18,838] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 176549: loss 2.1179
[2019-05-04 22:16:18,841] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 176549: learning rate 0.0001
[2019-05-04 22:16:19,289] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 176629: loss 0.2088
[2019-05-04 22:16:19,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 176629: learning rate 0.0001
[2019-05-04 22:16:21,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3359933e-09 9.8266739e-01 2.9628295e-09 4.4799572e-10 9.2532403e-15
 7.5446724e-15 4.9999917e-11 3.0709192e-15 1.4584385e-09 7.5011899e-09
 1.7332567e-02], sum to 1.0000
[2019-05-04 22:16:21,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2516
[2019-05-04 22:16:21,931] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 72.5, 2.25, 255.0, 0.0, 0.0, 0.0, -0.423698230459942, 20.0, 22.3371018387766, 22.7, 1.0, 41.67345447797289], 
current ob forecast is [], 
actual action is [0.0, 20.0], 
sim time this is 2140200.0000, 
sim time next is 2141100.0000, 
raw observation next is [-5.0, 73.25, 2.375, 257.5, 0.0, 0.0, 0.0, -0.4676370491202942, 20.0, 22.13171119177069, 22.7, 1.0, 29.27692791574582], 
processed observation next is [1.0, 0.782608695652174, 0.20512820512820512, 0.7325, 0.2159090909090909, 0.7152777777777778, 0.0, 0.0, 0.5, 0.3441209836265686, 0.0, 0.5902444559672416, 0.6714285714285714, 1.0, 0.3444344460675979], 
reward next is 0.6056, 
noisyNet noise sample is [array([0.45641342], dtype=float32), 0.4980135]. 
=============================================
[2019-05-04 22:16:22,082] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 177054: loss 3.2596
[2019-05-04 22:16:22,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 177054: learning rate 0.0001
[2019-05-04 22:16:24,513] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.6920826e-07 1.3817543e-01 1.9432223e-06 3.8358962e-08 4.5309206e-10
 8.2295094e-11 7.7561914e-08 5.4710281e-10 9.4263476e-07 8.2504761e-07
 8.6181974e-01], sum to 1.0000
[2019-05-04 22:16:24,515] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0284
[2019-05-04 22:16:24,681] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.15, 72.0, 6.225, 257.5, 105.75, 338.25, -0.2999999999999998, -0.648350411111795, 65.0, 21.65188349071853, 22.7, 1.0, 67.50880141837945], 
current ob forecast is [], 
actual action is [-0.15000000000000036, 20.0], 
sim time this is 2195100.0000, 
sim time next is 2196000.0000, 
raw observation next is [-5.0, 71.0, 6.6, 250.0, 109.5, 225.5, -0.1500000000000004, -0.6533725956401432, 20.0, 21.71542309341232, 22.7, 1.0, 44.541993416837556], 
processed observation next is [1.0, 0.43478260869565216, 0.20512820512820512, 0.71, 0.6, 0.6944444444444444, 0.2896825396825397, 0.2255, 0.49749999999999994, 0.28220913478661896, 0.0, 0.5307747276303313, 0.6714285714285714, 1.0, 0.5240234519627948], 
reward next is 0.2781, 
noisyNet noise sample is [array([-0.38140726], dtype=float32), -1.928815]. 
=============================================
[2019-05-04 22:16:24,704] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[39.567677]
 [39.461124]
 [39.220097]
 [39.174503]
 [39.21237 ]], R is [[39.89125061]
 [39.91230392]
 [39.99847031]
 [40.1601181 ]
 [40.05470657]].
[2019-05-04 22:16:33,632] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0075542e-11 9.9999917e-01 5.7338010e-12 3.2279970e-13 1.0452518e-14
 2.6219124e-16 2.3007627e-13 1.6459612e-15 4.9506978e-11 1.2162970e-12
 8.6095366e-07], sum to 1.0000
[2019-05-04 22:16:33,632] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4998
[2019-05-04 22:16:33,678] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 68.75, 6.749999999999999, 272.5, 0.0, 0.0, 0.0, -0.2649126531982971, 20.0, 22.90282622734114, 21.5, 0.0, 31.230115124674928], 
current ob forecast is [], 
actual action is [0.0, 20.0], 
sim time this is 2234700.0000, 
sim time next is 2235600.0000, 
raw observation next is [-5.0, 68.0, 6.6, 280.0, 0.0, 0.0, 0.0, -0.3079995010507731, 20.0, 22.63793584824169, 21.5, 0.0, 25.440846910311894], 
processed observation next is [1.0, 0.9130434782608695, 0.20512820512820512, 0.68, 0.6, 0.7777777777777778, 0.0, 0.0, 0.5, 0.39733349964974235, 0.0, 0.662562264034527, 0.5, 0.0, 0.29930408129778696], 
reward next is 0.6507, 
noisyNet noise sample is [array([2.6716137], dtype=float32), -0.1921123]. 
=============================================
[2019-05-04 22:16:41,204] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4701824e-07 1.3738601e-01 4.1848902e-09 9.7698416e-10 3.8296298e-12
 9.4986851e-14 3.4513756e-10 1.3635134e-12 1.7284221e-09 1.0526252e-08
 8.6261380e-01], sum to 1.0000
[2019-05-04 22:16:41,204] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2938
[2019-05-04 22:16:41,350] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.325, 54.5, 1.875, 247.5, 0.0, 0.0, 3.8, -0.2040777159396924, 20.0, 23.24133825484954, 22.7, 1.0, 27.158497872820437], 
current ob forecast is [], 
actual action is [3.675, 65.0], 
sim time this is 2315700.0000, 
sim time next is 2316600.0000, 
raw observation next is [-1.45, 55.0, 1.75, 245.0, 0.0, 0.0, 3.675, -0.1600757873423209, 65.0, 23.18919866071535, 22.7, 1.0, 71.93021660031422], 
processed observation next is [1.0, 0.8260869565217391, 0.29615384615384616, 0.55, 0.1590909090909091, 0.6805555555555556, 0.0, 0.0, 0.5612499999999999, 0.4466414042192264, 1.0, 0.741314094387907, 0.6714285714285714, 1.0, 0.8462378423566379], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9576337], dtype=float32), 1.1894641]. 
=============================================
[2019-05-04 22:16:47,123] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8522950e-09 9.8937732e-01 1.2005757e-09 4.9173110e-11 2.2283316e-13
 2.6110608e-15 2.8643386e-11 3.4892472e-13 1.7825728e-08 2.9163571e-11
 1.0622676e-02], sum to 1.0000
[2019-05-04 22:16:47,123] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4012
[2019-05-04 22:16:47,256] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.45, 33.5, 4.6, 70.0, 86.0, 829.0, 3.125, -0.7343585655356296, 65.0, 21.21099367588691, 22.7, 1.0, 71.48080850930006], 
current ob forecast is [], 
actual action is [3.55, 20.0], 
sim time this is 2460600.0000, 
sim time next is 2461500.0000, 
raw observation next is [-1.025, 32.25, 4.6, 65.0, 87.0, 833.0, 3.55, -0.7128527389370709, 20.0, 21.49214875286562, 22.7, 1.0, 39.770408381497305], 
processed observation next is [0.0, 0.4782608695652174, 0.307051282051282, 0.3225, 0.41818181818181815, 0.18055555555555555, 0.23015873015873015, 0.833, 0.5591666666666666, 0.2623824203543097, 0.0, 0.49887839326651723, 0.6714285714285714, 1.0, 0.46788715742938003], 
reward next is 0.2151, 
noisyNet noise sample is [array([-1.260325], dtype=float32), -0.85369086]. 
=============================================
[2019-05-04 22:16:47,267] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[53.46773 ]
 [52.350433]
 [51.71444 ]
 [51.167065]
 [50.327198]], R is [[53.64986801]
 [53.63843536]
 [53.10205078]
 [52.63602448]
 [52.67557526]].
[2019-05-04 22:16:51,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8381771e-06 9.7873771e-01 7.5956997e-08 5.4641053e-08 1.0629840e-10
 5.5052719e-11 1.6158213e-08 1.4876601e-10 6.5730795e-08 3.8146129e-07
 2.1259839e-02], sum to 1.0000
[2019-05-04 22:16:51,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3615
[2019-05-04 22:16:51,805] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.35, 60.75, 3.45, 62.5, 0.0, 0.0, -4.199999999999999, -1.261329314173573, 20.0, 18.77801610162241, 21.5, 0.0, 42.46956625493366], 
current ob forecast is [], 
actual action is [-4.35, 20.0], 
sim time this is 2443500.0000, 
sim time next is 2444400.0000, 
raw observation next is [-9.5, 61.0, 3.6, 60.0, 0.0, 0.0, -4.35, -1.291933686165279, 20.0, 18.7028008147137, 21.5, 0.0, 31.663755085658106], 
processed observation next is [0.0, 0.30434782608695654, 0.08974358974358974, 0.61, 0.32727272727272727, 0.16666666666666666, 0.0, 0.0, 0.4275, 0.06935543794490699, 0.0, 0.10040011638767155, 0.5, 0.0, 0.3725147657136248], 
reward next is 0.4753, 
noisyNet noise sample is [array([-0.56959987], dtype=float32), -0.08996563]. 
=============================================
[2019-05-04 22:16:53,613] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.48181899e-12 9.99102235e-01 1.65050102e-13 1.97061931e-13
 3.43401757e-18 1.45925969e-20 1.09593595e-14 5.54697703e-19
 4.25518588e-13 3.41047557e-13 8.97820923e-04], sum to 1.0000
[2019-05-04 22:16:53,613] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5445
[2019-05-04 22:16:53,744] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 56.0, 2.0, 20.0, 93.5, 25.5, 2.2, -0.8356830720740995, 65.0, 21.08885317495072, 22.7, 1.0, 73.98322259865097], 
current ob forecast is [], 
actual action is [2.2, 20.0], 
sim time this is 2538000.0000, 
sim time next is 2538900.0000, 
raw observation next is [-2.399999999999999, 54.25, 2.25, 105.0, 114.75, 29.25, 2.2, -0.8076638915920178, 20.0, 21.32211677688719, 22.7, 1.0, 46.50901014514678], 
processed observation next is [1.0, 0.391304347826087, 0.27179487179487183, 0.5425, 0.20454545454545456, 0.2916666666666667, 0.30357142857142855, 0.02925, 0.5366666666666667, 0.23077870280266075, 0.0, 0.4745881109838841, 0.6714285714285714, 1.0, 0.5471648252370209], 
reward next is 0.2802, 
noisyNet noise sample is [array([-0.94057935], dtype=float32), -0.40738878]. 
=============================================
[2019-05-04 22:16:58,784] A3C_AGENT_WORKER-Thread-7 INFO:Local step 11500, global step 183201: loss 0.1053
[2019-05-04 22:16:58,789] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 11500, global step 183201: learning rate 0.0001
[2019-05-04 22:16:58,894] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 183239: loss 0.1317
[2019-05-04 22:16:58,894] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 183239: learning rate 0.0001
[2019-05-04 22:16:59,498] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 183454: loss 0.5457
[2019-05-04 22:16:59,499] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 183454: learning rate 0.0001
[2019-05-04 22:16:59,582] A3C_AGENT_WORKER-Thread-5 INFO:Local step 11500, global step 183477: loss 0.3462
[2019-05-04 22:16:59,582] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 11500, global step 183477: learning rate 0.0001
[2019-05-04 22:16:59,937] A3C_AGENT_WORKER-Thread-6 INFO:Local step 11500, global step 183550: loss 0.0014
[2019-05-04 22:16:59,938] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 11500, global step 183550: learning rate 0.0001
[2019-05-04 22:17:00,273] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 183605: loss 2.4783
[2019-05-04 22:17:00,274] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 183605: learning rate 0.0001
[2019-05-04 22:17:00,785] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2145811e-10 9.8789269e-01 1.2471099e-10 2.2812138e-12 3.2695533e-15
 4.1061648e-16 5.8651320e-11 6.7582837e-16 2.1209412e-09 3.2279834e-10
 1.2107285e-02], sum to 1.0000
[2019-05-04 22:17:00,786] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8924
[2019-05-04 22:17:00,900] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.3, 29.0, 4.6, 350.0, 92.0, 256.5, 8.3, -0.1212166259519079, 20.0, 23.99726245435378, 22.7, 1.0, 38.84705655315003], 
current ob forecast is [], 
actual action is [8.3, 20.0], 
sim time this is 2563200.0000, 
sim time next is 2564100.0000, 
raw observation next is [3.149999999999999, 29.0, 4.725, 347.5, 81.0, 209.25, 8.3, -0.1507598030608669, 20.0, 23.82218173777794, 22.7, 1.0, 28.304752137681906], 
processed observation next is [1.0, 0.6956521739130435, 0.4141025641025641, 0.29, 0.4295454545454545, 0.9652777777777778, 0.21428571428571427, 0.20925, 0.6383333333333333, 0.44974673231304435, 0.0, 0.8317402482539913, 0.6714285714285714, 1.0, 0.3329970839727283], 
reward next is 0.6170, 
noisyNet noise sample is [array([-0.14372212], dtype=float32), 0.3003288]. 
=============================================
[2019-05-04 22:17:02,506] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 183964: loss 1.2193
[2019-05-04 22:17:02,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 183964: learning rate 0.0001
[2019-05-04 22:17:05,064] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184251: loss -0.5418
[2019-05-04 22:17:05,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184251: learning rate 0.0001
[2019-05-04 22:17:06,781] A3C_AGENT_WORKER-Thread-8 INFO:Local step 11500, global step 184431: loss -0.5101
[2019-05-04 22:17:06,786] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 11500, global step 184431: learning rate 0.0001
[2019-05-04 22:17:06,862] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 184445: loss -1.5224
[2019-05-04 22:17:06,863] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 184445: learning rate 0.0001
[2019-05-04 22:17:06,880] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3657428e-04 9.7220832e-01 8.6680528e-05 8.1522639e-06 5.0281717e-08
 7.1939972e-08 2.1381759e-06 7.9326043e-08 4.6160312e-05 7.7778468e-06
 2.7503991e-02], sum to 1.0000
[2019-05-04 22:17:06,880] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6601
[2019-05-04 22:17:06,964] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 29.0, 4.85, 345.0, 70.0, 162.0, 8.149999999999999, 0.0991891770630622, 20.0, 25.20948398380237, 22.7, 1.0, 20.76289045277079], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 2565000.0000, 
sim time next is 2565900.0000, 
raw observation next is [2.850000000000001, 29.0, 4.975, 342.5, 54.25, 122.75, 8.0, 0.1432542010697827, 20.0, 24.99260343754275, 22.7, 1.0, 16.616596889031566], 
processed observation next is [1.0, 0.6956521739130435, 0.40641025641025647, 0.29, 0.4522727272727272, 0.9513888888888888, 0.14351851851851852, 0.12275, 0.6333333333333333, 0.5477514003565943, 0.0, 0.998943348220393, 0.6714285714285714, 1.0, 0.19548937516507725], 
reward next is 0.7545, 
noisyNet noise sample is [array([1.1813569], dtype=float32), -0.3134121]. 
=============================================
[2019-05-04 22:17:07,238] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 184492: loss -0.1978
[2019-05-04 22:17:07,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 184492: learning rate 0.0001
[2019-05-04 22:17:07,352] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184506: loss -0.9508
[2019-05-04 22:17:07,354] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184507: learning rate 0.0001
[2019-05-04 22:17:08,077] A3C_AGENT_WORKER-Thread-4 INFO:Local step 11500, global step 184610: loss 0.2279
[2019-05-04 22:17:08,083] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 11500, global step 184611: learning rate 0.0001
[2019-05-04 22:17:08,078] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184610: loss 0.2080
[2019-05-04 22:17:08,107] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184614: learning rate 0.0001
[2019-05-04 22:17:08,174] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 184627: loss 0.0166
[2019-05-04 22:17:08,175] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 184627: learning rate 0.0001
[2019-05-04 22:17:09,160] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 184928: loss 0.0223
[2019-05-04 22:17:09,174] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 184933: learning rate 0.0001
[2019-05-04 22:17:12,107] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0176010e-16 1.0000000e+00 3.6306109e-17 2.6040088e-19 4.7883967e-24
 1.0070875e-27 8.2480635e-21 5.7723572e-26 1.5175338e-17 2.0528177e-19
 3.4931624e-10], sum to 1.0000
[2019-05-04 22:17:12,107] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0154
[2019-05-04 22:17:12,122] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.875, 52.25, 7.825, 232.5, 239.75, 153.0, 2.7, -0.5444387292653564, 20.0, 21.84498304001764, 22.7, 1.0, 7.4375015869490015], 
current ob forecast is [], 
actual action is [3.125, 20.0], 
sim time this is 2636100.0000, 
sim time next is 2637000.0000, 
raw observation next is [-1.45, 50.5, 7.949999999999999, 235.0, 245.0, 147.0, 3.125, -0.5550711130791955, 20.0, 21.74413630914694, 22.7, 1.0, 6.447485275285857], 
processed observation next is [1.0, 0.5217391304347826, 0.29615384615384616, 0.505, 0.7227272727272727, 0.6527777777777778, 0.6481481481481481, 0.147, 0.5520833333333334, 0.31497629564026813, 0.0, 0.5348766155924197, 0.6714285714285714, 1.0, 0.07585276794453949], 
reward next is 0.8330, 
noisyNet noise sample is [array([-0.478805], dtype=float32), -0.008323312]. 
=============================================
[2019-05-04 22:17:12,160] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[93.48009 ]
 [93.364655]
 [93.34647 ]
 [93.131874]
 [92.761   ]], R is [[93.57744598]
 [93.48535156]
 [93.39361572]
 [93.28978729]
 [93.16578674]].
[2019-05-04 22:17:12,460] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8370769e-14 9.9999988e-01 1.6845058e-16 1.4034792e-16 1.5135465e-22
 2.6741554e-25 9.9801334e-18 1.2889128e-23 1.5880096e-13 8.5276084e-18
 8.5749512e-08], sum to 1.0000
[2019-05-04 22:17:12,463] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7199
[2019-05-04 22:17:12,482] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 47.75, 8.7, 227.5, 170.25, 187.0, 5.5, -0.4706157158280443, 20.0, 22.16653395727043, 22.7, 1.0, 6.11539101025642], 
current ob forecast is [], 
actual action is [5.5, 20.0], 
sim time this is 2646900.0000, 
sim time next is 2647800.0000, 
raw observation next is [0.5, 48.5, 8.7, 225.0, 155.0, 206.0, 5.5, -0.4630202099333403, 20.0, 22.21643274520985, 22.7, 1.0, 6.153834937810089], 
processed observation next is [1.0, 0.6521739130434783, 0.34615384615384615, 0.485, 0.7909090909090909, 0.625, 0.41005291005291006, 0.206, 0.5916666666666667, 0.3456599300222199, 0.0, 0.6023475350299786, 0.6714285714285714, 1.0, 0.0723980580918834], 
reward next is 0.8776, 
noisyNet noise sample is [array([0.6140758], dtype=float32), 0.5105371]. 
=============================================
[2019-05-04 22:17:19,238] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.2218002e-09 9.9869823e-01 4.2015047e-10 2.8187707e-11 5.9773760e-14
 2.7115039e-15 4.3680194e-12 4.3469877e-14 4.6171883e-10 1.6424943e-11
 1.3017935e-03], sum to 1.0000
[2019-05-04 22:17:19,238] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9061
[2019-05-04 22:17:19,418] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 64.0, 2.1, 50.0, 0.0, 0.0, -2.0, -1.298852057046031, 65.0, 18.3205240559008, 21.5, 0.0, 79.16344248921216], 
current ob forecast is [], 
actual action is [-2.0, 20.0], 
sim time this is 2790000.0000, 
sim time next is 2790900.0000, 
raw observation next is [-6.75, 64.0, 1.95, 50.0, 0.0, 0.0, -2.0, -1.276109858743834, 20.0, 18.67961220943665, 22.7, 1.0, 61.96636731435348], 
processed observation next is [1.0, 0.30434782608695654, 0.16025641025641027, 0.64, 0.17727272727272728, 0.1388888888888889, 0.0, 0.0, 0.4666666666666667, 0.07463004708538867, 0.0, 0.09708745849095024, 0.6714285714285714, 1.0, 0.7290160860512174], 
reward next is 0.2274, 
noisyNet noise sample is [array([0.16890192], dtype=float32), 1.5367628]. 
=============================================
[2019-05-04 22:17:25,400] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9778478e-06 8.9926875e-01 3.4922988e-07 2.3354721e-08 6.6546289e-11
 1.6720856e-11 3.3800296e-09 6.9586209e-10 1.3338966e-07 1.6272139e-07
 1.0072561e-01], sum to 1.0000
[2019-05-04 22:17:25,400] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3457
[2019-05-04 22:17:25,438] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 59.0, 2.1, 60.0, 0.0, 0.0, -1.0, -1.17375211008789, 20.0, 18.97097143240121, 21.5, 0.0, 10.661329474257533], 
current ob forecast is [], 
actual action is [-1.0, 20.0], 
sim time this is 2772000.0000, 
sim time next is 2772900.0000, 
raw observation next is [-6.0, 59.0, 1.95, 62.5, 0.0, 0.0, -1.0, -1.190310159732833, 20.0, 18.90183744312474, 21.5, 0.0, 10.604952132083922], 
processed observation next is [1.0, 0.08695652173913043, 0.1794871794871795, 0.59, 0.17727272727272728, 0.1736111111111111, 0.0, 0.0, 0.48333333333333334, 0.10322994675572235, 0.0, 0.12883392044639155, 0.5, 0.0, 0.12476414273039908], 
reward next is 0.8210, 
noisyNet noise sample is [array([-0.65025496], dtype=float32), -1.04528]. 
=============================================
[2019-05-04 22:17:26,131] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.6773298e-08 9.9628276e-01 2.9913554e-08 6.4471428e-11 1.0580902e-12
 9.5101920e-15 6.5076056e-11 1.8230616e-13 2.4118467e-08 8.9690227e-10
 3.7171247e-03], sum to 1.0000
[2019-05-04 22:17:26,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5510
[2019-05-04 22:17:26,177] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 93.0, 3.975, 115.0, 0.0, 0.0, 6.0, -1.140199934472892, 20.0, 19.08617679416119, 21.5, 0.0, 23.334237478554424], 
current ob forecast is [], 
actual action is [6.0, 20.0], 
sim time this is 2868300.0000, 
sim time next is 2869200.0000, 
raw observation next is [1.0, 93.0, 4.1, 110.0, 0.0, 0.0, 6.0, -1.169713059962917, 20.0, 18.93217635189615, 21.5, 0.0, 19.24779515698542], 
processed observation next is [1.0, 0.21739130434782608, 0.358974358974359, 0.93, 0.3727272727272727, 0.3055555555555556, 0.0, 0.0, 0.6, 0.11009564667902765, 0.0, 0.13316805027087852, 0.5, 0.0, 0.2264446489057108], 
reward next is 0.7427, 
noisyNet noise sample is [array([0.38016647], dtype=float32), -0.17060784]. 
=============================================
[2019-05-04 22:17:30,675] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4810887e-06 9.9246514e-01 6.1748764e-08 6.4820483e-08 6.3043702e-11
 5.6936747e-12 3.1905806e-08 6.6099201e-12 2.7333735e-07 2.7958874e-08
 7.5328960e-03], sum to 1.0000
[2019-05-04 22:17:30,675] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0291
[2019-05-04 22:17:30,708] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 80.75, 6.300000000000001, 130.0, 0.0, 0.0, 6.0, -0.9515068868876501, 20.0, 19.8877066219364, 21.5, 0.0, 14.038767664790454], 
current ob forecast is [], 
actual action is [6.0, 20.0], 
sim time this is 2859300.0000, 
sim time next is 2860200.0000, 
raw observation next is [1.0, 82.5, 5.9, 130.0, 0.0, 0.0, 6.0, -0.9645662115310275, 20.0, 19.82878536355236, 21.5, 0.0, 13.995771538158207], 
processed observation next is [1.0, 0.08695652173913043, 0.358974358974359, 0.825, 0.5363636363636364, 0.3611111111111111, 0.0, 0.0, 0.6, 0.1784779294896575, 0.0, 0.26125505193605136, 0.5, 0.0, 0.16465613574303772], 
reward next is 0.7853, 
noisyNet noise sample is [array([-0.23691113], dtype=float32), -0.16430493]. 
=============================================
[2019-05-04 22:17:32,413] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.3569870e-12 1.0000000e+00 1.7418014e-12 1.0024334e-14 3.9122450e-18
 1.8825493e-20 4.3126154e-14 3.5806676e-18 1.7037224e-12 1.8976023e-14
 2.4919716e-08], sum to 1.0000
[2019-05-04 22:17:32,414] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2532
[2019-05-04 22:17:32,454] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 44.0, 4.1, 110.0, 0.0, 0.0, 7.0, -0.5733634489010555, 20.0, 21.48324890819985, 21.5, 0.0, 12.98828329227569], 
current ob forecast is [], 
actual action is [7.0, 20.0], 
sim time this is 2840400.0000, 
sim time next is 2841300.0000, 
raw observation next is [2.0, 44.0, 4.225, 112.5, 0.0, 0.0, 7.0, -0.591038919592041, 20.0, 21.40432910480207, 21.5, 0.0, 10.572578810196658], 
processed observation next is [1.0, 0.9130434782608695, 0.38461538461538464, 0.44, 0.38409090909090904, 0.3125, 0.0, 0.0, 0.6166666666666667, 0.302987026802653, 0.0, 0.48633272925743853, 0.5, 0.0, 0.12438328011996068], 
reward next is 0.8256, 
noisyNet noise sample is [array([-0.9137736], dtype=float32), -0.80985326]. 
=============================================
[2019-05-04 22:17:32,880] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4936528e-14 1.0000000e+00 3.8232420e-15 2.0023922e-17 1.1068324e-21
 3.5958351e-25 9.7492376e-19 1.6310363e-23 1.5855710e-16 3.7390941e-16
 8.2768709e-10], sum to 1.0000
[2019-05-04 22:17:32,881] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4266
[2019-05-04 22:17:32,894] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.25, 79.75, 7.824999999999999, 260.0, 0.0, 0.0, 4.0, -0.6529167763595521, 20.0, 20.91821457120721, 21.5, 0.0, 8.499090413523945], 
current ob forecast is [], 
actual action is [3.75, 20.0], 
sim time this is 2931300.0000, 
sim time next is 2932200.0000, 
raw observation next is [-1.5, 81.5, 7.449999999999999, 260.0, 0.0, 0.0, 3.75, -0.6768336826363476, 20.0, 20.78049436411924, 21.5, 0.0, 8.680632795559864], 
processed observation next is [1.0, 0.9565217391304348, 0.2948717948717949, 0.815, 0.6772727272727272, 0.7222222222222222, 0.0, 0.0, 0.5625, 0.2743887724545508, 0.0, 0.3972134805884627, 0.5, 0.0, 0.10212509171246899], 
reward next is 0.8479, 
noisyNet noise sample is [array([0.21468057], dtype=float32), 1.868543]. 
=============================================
[2019-05-04 22:17:33,296] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.3662852e-10 9.9997163e-01 3.0394370e-10 1.4499175e-10 5.0698742e-15
 5.2918575e-17 1.3154382e-12 3.3762727e-15 9.9889527e-11 1.5675425e-11
 2.8398990e-05], sum to 1.0000
[2019-05-04 22:17:33,297] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0774
[2019-05-04 22:17:33,315] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.25, 79.75, 7.824999999999999, 260.0, 0.0, 0.0, 4.0, -0.6110880946273237, 20.0, 21.1117010540528, 21.5, 0.0, 6.629478017738102], 
current ob forecast is [], 
actual action is [3.75, 20.0], 
sim time this is 2931300.0000, 
sim time next is 2932200.0000, 
raw observation next is [-1.5, 81.5, 7.449999999999999, 260.0, 0.0, 0.0, 3.75, -0.6362117725072783, 20.0, 20.96723726963452, 21.5, 0.0, 7.2806061770289405], 
processed observation next is [1.0, 0.9565217391304348, 0.2948717948717949, 0.815, 0.6772727272727272, 0.7222222222222222, 0.0, 0.0, 0.5625, 0.28792940916424054, 0.0, 0.4238910385192171, 0.5, 0.0, 0.08565419031798753], 
reward next is 0.8643, 
noisyNet noise sample is [array([-0.7527796], dtype=float32), 0.5424431]. 
=============================================
[2019-05-04 22:17:41,707] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 190876: loss 0.3414
[2019-05-04 22:17:41,715] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 190877: learning rate 0.0001
[2019-05-04 22:17:42,461] A3C_AGENT_WORKER-Thread-7 INFO:Local step 12000, global step 191041: loss 1.2492
[2019-05-04 22:17:42,461] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 12000, global step 191041: learning rate 0.0001
[2019-05-04 22:17:42,553] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 191070: loss 1.9052
[2019-05-04 22:17:42,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 191070: learning rate 0.0001
[2019-05-04 22:17:42,716] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.9136554e-07 1.7788337e-01 1.8916397e-06 6.8365753e-09 8.4729795e-10
 6.8030056e-12 5.9275450e-08 2.9905006e-11 3.0131969e-07 9.4333856e-09
 8.2211387e-01], sum to 1.0000
[2019-05-04 22:17:42,716] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1655
[2019-05-04 22:17:42,736] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 77.0, 4.475, 280.0, 0.0, 0.0, 1.0, -1.126663868855037, 20.0, 18.99462686698789, 21.5, 0.0, 17.70227941709425], 
current ob forecast is [], 
actual action is [1.0, 20.0], 
sim time this is 2963700.0000, 
sim time next is 2964600.0000, 
raw observation next is [-4.0, 77.0, 4.35, 280.0, 0.0, 0.0, 1.0, -1.144212210876633, 20.0, 18.92197599806956, 21.5, 0.0, 17.7321329630819], 
processed observation next is [0.0, 0.30434782608695654, 0.23076923076923078, 0.77, 0.39545454545454545, 0.7777777777777778, 0.0, 0.0, 0.5166666666666667, 0.11859592970778898, 0.0, 0.1317108568670799, 0.5, 0.0, 0.2086133289774341], 
reward next is 0.7574, 
noisyNet noise sample is [array([0.8567769], dtype=float32), 0.6385614]. 
=============================================
[2019-05-04 22:17:43,280] A3C_AGENT_WORKER-Thread-5 INFO:Local step 12000, global step 191363: loss 2.4717
[2019-05-04 22:17:43,280] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 12000, global step 191363: learning rate 0.0001
[2019-05-04 22:17:43,303] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191373: loss 3.4842
[2019-05-04 22:17:43,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191373: learning rate 0.0001
[2019-05-04 22:17:43,371] A3C_AGENT_WORKER-Thread-6 INFO:Local step 12000, global step 191395: loss 3.0204
[2019-05-04 22:17:43,374] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 12000, global step 191395: learning rate 0.0001
[2019-05-04 22:17:45,293] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 191762: loss 2.4475
[2019-05-04 22:17:45,299] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 191762: learning rate 0.0001
[2019-05-04 22:17:47,818] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192148: loss 1.2084
[2019-05-04 22:17:47,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192149: learning rate 0.0001
[2019-05-04 22:17:48,628] A3C_AGENT_WORKER-Thread-8 INFO:Local step 12000, global step 192293: loss 4.4424
[2019-05-04 22:17:48,631] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 12000, global step 192293: learning rate 0.0001
[2019-05-04 22:17:49,490] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192508: loss 3.1309
[2019-05-04 22:17:49,490] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192508: learning rate 0.0001
[2019-05-04 22:17:49,718] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 192569: loss 1.9574
[2019-05-04 22:17:49,719] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 192570: learning rate 0.0001
[2019-05-04 22:17:49,738] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 192573: loss 2.0136
[2019-05-04 22:17:49,739] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 192573: learning rate 0.0001
[2019-05-04 22:17:50,472] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 192789: loss 0.3203
[2019-05-04 22:17:50,472] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 192789: loss 0.4935
[2019-05-04 22:17:50,472] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 192789: learning rate 0.0001
[2019-05-04 22:17:50,481] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 192789: learning rate 0.0001
[2019-05-04 22:17:50,536] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 192811: loss 1.0252
[2019-05-04 22:17:50,537] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 192811: learning rate 0.0001
[2019-05-04 22:17:50,551] A3C_AGENT_WORKER-Thread-4 INFO:Local step 12000, global step 192816: loss 1.0801
[2019-05-04 22:17:50,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 12000, global step 192816: learning rate 0.0001
[2019-05-04 22:17:50,876] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1013928e-13 9.9999976e-01 2.6247788e-14 7.7429161e-17 4.6935840e-21
 7.3453482e-24 1.1689481e-16 3.7974855e-22 4.4151669e-16 2.7606743e-17
 1.8916461e-07], sum to 1.0000
[2019-05-04 22:17:50,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2813
[2019-05-04 22:17:50,903] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 100.0, 4.1, 90.0, 0.0, 0.0, 4.0, -0.6679864019793809, 20.0, 21.32070725682656, 21.5, 0.0, 7.280705079918167], 
current ob forecast is [], 
actual action is [4.0, 20.0], 
sim time this is 3101400.0000, 
sim time next is 3102300.0000, 
raw observation next is [-1.0, 100.0, 3.85, 85.0, 0.0, 0.0, 4.0, -0.6869534539488374, 20.0, 21.23833377256403, 21.5, 0.0, 6.4044865084736005], 
processed observation next is [0.0, 0.9130434782608695, 0.3076923076923077, 1.0, 0.35000000000000003, 0.2361111111111111, 0.0, 0.0, 0.5666666666666667, 0.27101551535038754, 0.0, 0.46261911036628994, 0.5, 0.0, 0.07534690009968942], 
reward next is 0.8747, 
noisyNet noise sample is [array([0.74279666], dtype=float32), 1.6143119]. 
=============================================
[2019-05-04 22:17:55,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.3599564e-12 9.9999845e-01 2.7476826e-11 1.0667134e-13 2.8725276e-17
 4.8134555e-19 9.7210558e-15 9.0467768e-18 9.5076629e-12 2.8739896e-14
 1.5275384e-06], sum to 1.0000
[2019-05-04 22:17:55,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8200
[2019-05-04 22:17:55,911] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.75, 100.0, 4.725, 275.0, 0.0, 0.0, 2.5, -0.9717693752835345, 20.0, 19.28342218759479, 20.8, 0.0, 8.651859626504125], 
current ob forecast is [], 
actual action is [2.25, 20.0], 
sim time this is 3217500.0000, 
sim time next is 3218400.0000, 
raw observation next is [-3.0, 100.0, 5.1, 270.0, 0.0, 0.0, 2.25, -0.9876878388216038, 20.0, 19.21803871712919, 20.8, 0.0, 8.788302627272612], 
processed observation next is [1.0, 0.2608695652173913, 0.2564102564102564, 1.0, 0.4636363636363636, 0.75, 0.0, 0.0, 0.5375, 0.17077072039279872, 0.0, 0.17400553101845567, 0.4000000000000001, 0.0, 0.10339179561497192], 
reward next is 0.8466, 
noisyNet noise sample is [array([-0.05551292], dtype=float32), 0.8948449]. 
=============================================
[2019-05-04 22:18:06,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3740446e-11 1.0000000e+00 6.3224794e-13 7.6606080e-14 2.2528694e-18
 3.0538760e-18 5.3602823e-14 3.6623802e-18 8.0087941e-14 9.0212560e-14
 8.1952356e-09], sum to 1.0000
[2019-05-04 22:18:06,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3386
[2019-05-04 22:18:06,882] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.25, 83.75, 3.825, 320.0, 0.0, 0.0, 0.0, -0.8531869618646097, 20.0, 19.84310072902133, 20.8, 0.0, 9.002456345976348], 
current ob forecast is [], 
actual action is [-0.25, 20.0], 
sim time this is 3273300.0000, 
sim time next is 3274200.0000, 
raw observation next is [-5.5, 86.5, 3.75, 320.0, 0.0, 0.0, -0.25, -0.8684593808734329, 20.0, 19.77869827728836, 20.8, 0.0, 9.114593051022004], 
processed observation next is [1.0, 0.9130434782608695, 0.19230769230769232, 0.865, 0.3409090909090909, 0.8888888888888888, 0.0, 0.0, 0.49583333333333335, 0.2105135397088557, 0.0, 0.2540997538983371, 0.4000000000000001, 0.0, 0.10723050648261181], 
reward next is 0.8428, 
noisyNet noise sample is [array([-0.83916885], dtype=float32), -0.73973054]. 
=============================================
[2019-05-04 22:18:14,365] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.0220067e-06 9.2150412e-02 5.9366807e-06 5.1961401e-08 7.5449362e-09
 7.0930556e-10 2.5262855e-07 2.3707165e-09 4.5899901e-07 3.5971698e-07
 9.0783942e-01], sum to 1.0000
[2019-05-04 22:18:14,366] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3192
[2019-05-04 22:18:14,416] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 84.25, 5.375, 242.5, 0.0, 0.0, 6.0, -0.533590970652329, 20.0, 21.51149011529718, 20.8, 0.0, 39.46446873145687], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3456900.0000, 
sim time next is 3457800.0000, 
raw observation next is [1.0, 82.5, 5.65, 245.0, 0.0, 0.0, 6.0, -0.5080467342821752, 65.0, 21.56831406556391, 20.8, 0.0, 58.88785203711509], 
processed observation next is [1.0, 0.0, 0.358974358974359, 0.825, 0.5136363636363637, 0.6805555555555556, 0.0, 0.0, 0.6, 0.3306510885726083, 1.0, 0.5097591522234158, 0.4000000000000001, 0.0, 0.6927982592601775], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2843577], dtype=float32), -1.2031984]. 
=============================================
[2019-05-04 22:18:14,665] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 198834: loss -0.0373
[2019-05-04 22:18:14,670] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 198834: learning rate 0.0001
[2019-05-04 22:18:14,996] A3C_AGENT_WORKER-Thread-6 INFO:Local step 12500, global step 198944: loss 0.6456
[2019-05-04 22:18:14,998] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 12500, global step 198944: learning rate 0.0001
[2019-05-04 22:18:15,177] A3C_AGENT_WORKER-Thread-7 INFO:Local step 12500, global step 198999: loss 0.2403
[2019-05-04 22:18:15,180] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 12500, global step 198999: learning rate 0.0001
[2019-05-04 22:18:15,379] A3C_AGENT_WORKER-Thread-5 INFO:Local step 12500, global step 199059: loss 1.6738
[2019-05-04 22:18:15,390] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 12500, global step 199060: learning rate 0.0001
[2019-05-04 22:18:15,398] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 199062: loss 0.3180
[2019-05-04 22:18:15,401] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 199063: learning rate 0.0001
[2019-05-04 22:18:15,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1018472e-06 9.9825615e-01 6.7576598e-06 3.5821287e-07 5.4279848e-08
 5.9830585e-09 9.5398980e-07 1.4636165e-08 4.5877619e-06 1.1726488e-06
 1.7237506e-03], sum to 1.0000
[2019-05-04 22:18:15,409] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0174
[2019-05-04 22:18:15,428] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 77.0, 3.1, 162.5, 0.0, 0.0, -1.0, -0.7341626533460617, 20.0, 20.73283363058404, 20.8, 0.0, 11.72856330228388], 
current ob forecast is [], 
actual action is [-1.0, 20.0], 
sim time this is 3374100.0000, 
sim time next is 3375000.0000, 
raw observation next is [-6.0, 77.0, 3.1, 165.0, 0.0, 0.0, -1.0, -0.7632531126605867, 20.0, 20.60043217521692, 20.8, 0.0, 10.054048001086107], 
processed observation next is [1.0, 0.043478260869565216, 0.1794871794871795, 0.77, 0.2818181818181818, 0.4583333333333333, 0.0, 0.0, 0.48333333333333334, 0.24558229577980442, 0.0, 0.3714903107452742, 0.4000000000000001, 0.0, 0.11828291765983655], 
reward next is 0.8317, 
noisyNet noise sample is [array([-0.09123126], dtype=float32), 0.004836103]. 
=============================================
[2019-05-04 22:18:15,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[29.351097]
 [29.6858  ]
 [29.984896]
 [30.160727]
 [29.295567]], R is [[29.52017784]
 [30.03699303]
 [30.523983  ]
 [30.97516441]
 [31.38175011]].
[2019-05-04 22:18:16,219] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199257: loss 0.3483
[2019-05-04 22:18:16,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199257: learning rate 0.0001
[2019-05-04 22:18:18,270] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-05-04 22:18:18,272] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation job starts!
[2019-05-04 22:18:18,277] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:18:18,279] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation job starts!
[2019-05-04 22:18:18,281] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:18:18,286] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res1/Eplus-env-sub_run5
[2019-05-04 22:18:18,301] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Test-Repeat-Real97-v1-res1/Eplus-env-sub_run5
[2019-05-04 22:19:52,961] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation: average rewards by now are 4595.7612 73651.5758 272758.3191
[2019-05-04 22:19:52,973] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:19:52,973] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:19:52,973] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:19:52,973] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:19:52,973] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:19:53,091] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:19:53,091] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:19:53,091] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:19:53,091] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:19:53,091] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:21:32,186] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation: average rewards by now are 4565.9788 145505.9348 -3050.0001
[2019-05-04 22:21:32,196] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:21:32,196] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:21:32,196] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:21:32,196] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:21:32,196] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:21:32,308] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:21:32,308] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:21:32,308] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:21:32,308] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:21:32,308] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:21:33,199] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 200000, evaluation results [200000.0, 4565.978831846878, 145505.93484757002, -3050.0000511666367, 4595.761158783212, 73651.57584369082, 272758.3191094142]
[2019-05-04 22:21:33,519] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 200115: loss 1.8425
[2019-05-04 22:21:33,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 200115: learning rate 0.0001
[2019-05-04 22:21:34,310] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 200418: loss 3.7459
[2019-05-04 22:21:34,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 200418: learning rate 0.0001
[2019-05-04 22:21:34,345] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200429: loss 0.2234
[2019-05-04 22:21:34,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200429: learning rate 0.0001
[2019-05-04 22:21:34,451] A3C_AGENT_WORKER-Thread-8 INFO:Local step 12500, global step 200468: loss 1.0993
[2019-05-04 22:21:34,452] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 12500, global step 200468: learning rate 0.0001
[2019-05-04 22:21:34,966] A3C_AGENT_WORKER-Thread-4 INFO:Local step 12500, global step 200613: loss 3.7405
[2019-05-04 22:21:34,967] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 12500, global step 200613: learning rate 0.0001
[2019-05-04 22:21:34,996] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 200619: loss 0.0844
[2019-05-04 22:21:34,997] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 200619: learning rate 0.0001
[2019-05-04 22:21:35,218] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200674: loss 0.7886
[2019-05-04 22:21:35,221] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200674: learning rate 0.0001
[2019-05-04 22:21:35,241] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 200676: loss -0.4310
[2019-05-04 22:21:35,244] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 200676: learning rate 0.0001
[2019-05-04 22:21:35,376] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 200704: loss 0.4440
[2019-05-04 22:21:35,390] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 200704: learning rate 0.0001
[2019-05-04 22:21:36,912] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 200879: loss -0.1952
[2019-05-04 22:21:36,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 200881: learning rate 0.0001
[2019-05-04 22:21:43,815] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0030965e-15 9.9999976e-01 8.4066807e-14 2.3916735e-16 1.7093714e-21
 5.5784217e-23 1.1108200e-15 2.2009454e-22 1.1144144e-16 4.5391177e-16
 2.6966831e-07], sum to 1.0000
[2019-05-04 22:21:43,815] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0905
[2019-05-04 22:21:43,881] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.0, 27.5, 4.725, 185.0, 103.5, 688.0, 16.0, -0.6124112195544714, 20.0, 21.78558415131247, 22.7, 1.0, 20.25481311154346], 
current ob forecast is [], 
actual action is [16.0, 20.0], 
sim time this is 3663900.0000, 
sim time next is 3664800.0000, 
raw observation next is [11.0, 28.0, 5.1, 190.0, 106.0, 713.0, 16.0, -0.6157681131277836, 20.0, 21.71554680923471, 22.7, 1.0, 15.581885264193389], 
processed observation next is [0.0, 0.43478260869565216, 0.6153846153846154, 0.28, 0.4636363636363636, 0.5277777777777778, 0.2804232804232804, 0.713, 0.7666666666666667, 0.2947439622907388, 0.0, 0.5307924013192442, 0.6714285714285714, 1.0, 0.1833162972258046], 
reward next is 0.5667, 
noisyNet noise sample is [array([-0.2870967], dtype=float32), 0.9110202]. 
=============================================
[2019-05-04 22:21:47,033] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.02766762e-11 9.91169453e-01 1.13950974e-10 3.07292982e-13
 2.26488123e-17 7.27203492e-20 1.83143377e-13 5.80156109e-18
 3.16787156e-12 3.06970215e-13 8.83049425e-03], sum to 1.0000
[2019-05-04 22:21:47,033] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2573
[2019-05-04 22:21:47,060] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 42.0, 3.825, 267.5, 98.0, 765.75, 4.0, -0.5595425647167501, 20.0, 21.61295166200197, 22.7, 1.0, 10.138978807020182], 
current ob forecast is [], 
actual action is [4.0, 20.0], 
sim time this is 3595500.0000, 
sim time next is 3596400.0000, 
raw observation next is [-1.0, 42.0, 4.6, 270.0, 94.0, 743.5, 4.0, -0.5676267524493573, 20.0, 21.55188825139881, 22.7, 1.0, 7.675734236200974], 
processed observation next is [0.0, 0.6521739130434783, 0.3076923076923077, 0.42, 0.41818181818181815, 0.75, 0.24867724867724866, 0.7435, 0.5666666666666667, 0.31079108251688087, 0.0, 0.5074126073426873, 0.6714285714285714, 1.0, 0.09030275572001145], 
reward next is 0.7928, 
noisyNet noise sample is [array([-0.70721924], dtype=float32), -1.0009184]. 
=============================================
[2019-05-04 22:21:59,761] A3C_AGENT_WORKER-Thread-7 INFO:Local step 13000, global step 206832: loss 0.1669
[2019-05-04 22:21:59,761] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 13000, global step 206832: learning rate 0.0001
[2019-05-04 22:22:00,001] A3C_AGENT_WORKER-Thread-6 INFO:Local step 13000, global step 206879: loss 1.0748
[2019-05-04 22:22:00,002] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 13000, global step 206879: learning rate 0.0001
[2019-05-04 22:22:00,125] A3C_AGENT_WORKER-Thread-5 INFO:Local step 13000, global step 206904: loss 1.2802
[2019-05-04 22:22:00,127] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 13000, global step 206905: learning rate 0.0001
[2019-05-04 22:22:00,155] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.47059889e-10 9.99945998e-01 1.77526868e-10 2.02673121e-11
 7.86053521e-15 1.85231957e-16 1.33514658e-11 7.86446057e-16
 1.43054379e-11 1.20879305e-11 5.40022265e-05], sum to 1.0000
[2019-05-04 22:22:00,155] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7212
[2019-05-04 22:22:00,171] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 74.0, 4.9, 235.0, 0.0, 0.0, 0.75, -1.258358192492145, 20.0, 18.49268306580446, 21.5, 0.0, 11.3149671161538], 
current ob forecast is [], 
actual action is [0.5, 20.0], 
sim time this is 3821400.0000, 
sim time next is 3822300.0000, 
raw observation next is [-4.75, 75.5, 4.25, 232.5, 0.0, 0.0, 0.5, -1.27357533523858, 20.0, 18.42780931844419, 21.5, 0.0, 11.306956931137465], 
processed observation next is [1.0, 0.21739130434782608, 0.21153846153846154, 0.755, 0.38636363636363635, 0.6458333333333334, 0.0, 0.0, 0.5083333333333333, 0.0754748882538067, 0.0, 0.061115616920598574, 0.5, 0.0, 0.1330230227192643], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.01518], dtype=float32), 0.12134577]. 
=============================================
[2019-05-04 22:22:00,349] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 206954: loss 3.9188
[2019-05-04 22:22:00,349] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 206954: learning rate 0.0001
[2019-05-04 22:22:00,498] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 206984: loss 0.9920
[2019-05-04 22:22:00,499] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 206984: learning rate 0.0001
[2019-05-04 22:22:01,557] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207224: loss 0.2121
[2019-05-04 22:22:01,557] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207224: learning rate 0.0001
[2019-05-04 22:22:04,390] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 208057: loss 1.5207
[2019-05-04 22:22:04,391] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 208057: learning rate 0.0001
[2019-05-04 22:22:05,554] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208372: loss 0.7786
[2019-05-04 22:22:05,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208372: learning rate 0.0001
[2019-05-04 22:22:05,614] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 208383: loss 0.0667
[2019-05-04 22:22:05,614] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 208383: learning rate 0.0001
[2019-05-04 22:22:05,637] A3C_AGENT_WORKER-Thread-8 INFO:Local step 13000, global step 208389: loss -0.0246
[2019-05-04 22:22:05,638] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 13000, global step 208389: learning rate 0.0001
[2019-05-04 22:22:06,699] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 208553: loss 0.8564
[2019-05-04 22:22:06,699] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 208553: learning rate 0.0001
[2019-05-04 22:22:07,488] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 208639: loss -0.0391
[2019-05-04 22:22:07,505] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 208639: learning rate 0.0001
[2019-05-04 22:22:08,082] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 208703: loss 0.4966
[2019-05-04 22:22:08,083] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 208703: learning rate 0.0001
[2019-05-04 22:22:08,093] A3C_AGENT_WORKER-Thread-4 INFO:Local step 13000, global step 208707: loss 0.0008
[2019-05-04 22:22:08,093] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 13000, global step 208707: learning rate 0.0001
[2019-05-04 22:22:08,548] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208757: loss -0.1428
[2019-05-04 22:22:08,569] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208759: learning rate 0.0001
[2019-05-04 22:22:10,190] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 209008: loss 2.7140
[2019-05-04 22:22:10,191] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 209008: learning rate 0.0001
[2019-05-04 22:22:11,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8009910e-09 9.9957174e-01 9.1291756e-09 3.0444863e-10 1.1777857e-11
 1.2933045e-12 4.0344741e-10 2.8480462e-12 4.6696219e-10 2.1568475e-10
 4.2826190e-04], sum to 1.0000
[2019-05-04 22:22:11,589] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7189
[2019-05-04 22:22:11,614] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.0, 63.0, 2.725, 350.0, 0.0, 0.0, -7.0, -1.102557738065979, 20.0, 19.25114112703496, 21.5, 0.0, 42.20081401953836], 
current ob forecast is [], 
actual action is [-7.0, 20.0], 
sim time this is 3986100.0000, 
sim time next is 3987000.0000, 
raw observation next is [-12.0, 63.0, 2.85, 350.0, 0.0, 0.0, -7.0, -1.134163635335299, 20.0, 19.19850038018842, 21.5, 0.0, 31.804291178278106], 
processed observation next is [1.0, 0.13043478260869565, 0.02564102564102564, 0.63, 0.2590909090909091, 0.9722222222222222, 0.0, 0.0, 0.38333333333333336, 0.12194545488823365, 0.0, 0.1712143400269172, 0.5, 0.0, 0.3741681315091542], 
reward next is 0.5758, 
noisyNet noise sample is [array([1.1549006], dtype=float32), 1.2565426]. 
=============================================
[2019-05-04 22:22:11,639] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[48.84493 ]
 [49.27274 ]
 [48.039783]
 [48.07831 ]
 [48.224247]], R is [[48.70949554]
 [48.67591858]
 [48.53248596]
 [48.69962692]
 [48.97912216]].
[2019-05-04 22:22:13,244] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.19208029e-11 3.23337713e-06 1.10342603e-11 2.14690006e-12
 1.19757076e-14 3.07830070e-17 6.63525170e-13 2.16785780e-15
 3.08526837e-12 1.49187856e-12 9.99996781e-01], sum to 1.0000
[2019-05-04 22:22:13,244] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8853
[2019-05-04 22:22:13,378] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 37.5, 0.75, 70.0, 0.0, 0.0, 0.25, -1.104670232968599, 65.0, 19.42403603531066, 22.7, 1.0, 89.5240618210073], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 4087800.0000, 
sim time next is 4088700.0000, 
raw observation next is [-4.25, 35.75, 0.375, 35.0, 23.0, 117.25, 0.5, -1.017969053109668, 65.0, 20.01104579529251, 22.7, 1.0, 80.61927856571423], 
processed observation next is [1.0, 0.30434782608695654, 0.22435897435897437, 0.3575, 0.03409090909090909, 0.09722222222222222, 0.06084656084656084, 0.11725, 0.5083333333333333, 0.16067698229677738, 1.0, 0.2872922564703586, 0.6714285714285714, 1.0, 0.9484621007731086], 
reward next is 0.8670, 
noisyNet noise sample is [array([-1.0505234], dtype=float32), 0.3455681]. 
=============================================
[2019-05-04 22:22:28,286] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6875692e-12 9.9998367e-01 5.1562548e-14 2.2849649e-14 7.9031243e-18
 2.1649572e-19 6.2837464e-14 3.7880982e-19 9.2180859e-15 7.8440570e-15
 1.6372458e-05], sum to 1.0000
[2019-05-04 22:22:28,286] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7335
[2019-05-04 22:22:28,434] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.75, 33.0, 2.475, 287.5, 142.0, 802.5, 6.5, -0.4506448438611441, 65.0, 22.11820361563911, 22.7, 1.0, 68.7760354938565], 
current ob forecast is [], 
actual action is [6.75, 20.0], 
sim time this is 4193100.0000, 
sim time next is 4194000.0000, 
raw observation next is [2.0, 34.0, 2.6, 280.0, 166.0, 758.0, 6.75, -0.4142346439440066, 20.0, 22.52080612460406, 22.7, 1.0, 35.404699423890115], 
processed observation next is [0.0, 0.5652173913043478, 0.38461538461538464, 0.34, 0.23636363636363636, 0.7777777777777778, 0.43915343915343913, 0.758, 0.6125, 0.3619217853519978, 0.0, 0.6458294463720087, 0.6714285714285714, 1.0, 0.41652587557517784], 
reward next is 0.5335, 
noisyNet noise sample is [array([-1.0185596], dtype=float32), 0.9051011]. 
=============================================
[2019-05-04 22:22:28,446] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[67.46743]
 [67.32361]
 [66.38411]
 [66.37205]
 [65.08491]], R is [[67.76301575]
 [67.08538818]
 [67.13101196]
 [67.08966064]
 [66.92042542]].
[2019-05-04 22:22:28,619] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8294194e-10 3.7417758e-02 2.6383979e-09 3.5473572e-11 5.4312718e-14
 1.8845215e-16 1.9813241e-11 1.2948767e-14 1.0050404e-11 1.5562668e-10
 9.6258229e-01], sum to 1.0000
[2019-05-04 22:22:28,619] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1036
[2019-05-04 22:22:28,695] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 53.0, 2.35, 352.5, 0.0, 0.0, 1.0, -1.038580906980237, 20.0, 19.61402931983981, 21.5, 0.0, 24.477507526220066], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 4166100.0000, 
sim time next is 4167000.0000, 
raw observation next is [-4.0, 52.0, 2.6, 345.0, 0.0, 0.0, 1.0, -0.9898867883966026, 65.0, 19.62096892646073, 21.5, 0.0, 67.51677851483738], 
processed observation next is [0.0, 0.21739130434782608, 0.23076923076923078, 0.52, 0.23636363636363636, 0.9583333333333334, 0.0, 0.0, 0.5166666666666667, 0.17003773720113247, 1.0, 0.23156698949438997, 0.5, 0.0, 0.794315041351028], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5146734], dtype=float32), -1.3899355]. 
=============================================
[2019-05-04 22:22:28,785] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[65.32548]
 [65.14704]
 [66.27544]
 [66.50781]
 [66.04562]], R is [[64.97702026]
 [64.9892807 ]
 [64.93507385]
 [64.76452637]
 [64.11688232]].
[2019-05-04 22:22:30,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9938426e-15 9.9365342e-01 6.0075148e-14 1.5962894e-15 5.1979442e-20
 1.4729613e-24 3.3721040e-17 2.4849221e-20 7.4318221e-16 1.7223303e-15
 6.3465913e-03], sum to 1.0000
[2019-05-04 22:22:30,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8795
[2019-05-04 22:22:30,899] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.25, 31.0, 2.225, 302.5, 118.25, 840.75, 6.0, -0.5975072601719619, 20.0, 21.69413655355567, 22.7, 1.0, 16.606412445860798], 
current ob forecast is [], 
actual action is [6.25, 20.0], 
sim time this is 4191300.0000, 
sim time next is 4192200.0000, 
raw observation next is [1.5, 32.0, 2.35, 295.0, 118.0, 847.0, 6.25, -0.6136268325718841, 20.0, 21.53508004888111, 22.7, 1.0, 13.326095792463684], 
processed observation next is [0.0, 0.5217391304347826, 0.3717948717948718, 0.32, 0.21363636363636365, 0.8194444444444444, 0.31216931216931215, 0.847, 0.6041666666666666, 0.29545772247603863, 0.0, 0.5050114355544443, 0.6714285714285714, 1.0, 0.1567775975583963], 
reward next is 0.5879, 
noisyNet noise sample is [array([-0.45735085], dtype=float32), -0.7017583]. 
=============================================
[2019-05-04 22:22:31,589] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5890235e-14 9.9999964e-01 3.9404135e-12 3.8192870e-13 4.4428893e-17
 3.4087250e-20 2.2394294e-14 1.8447941e-18 1.4777192e-13 2.9815282e-14
 3.2343934e-07], sum to 1.0000
[2019-05-04 22:22:31,592] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7618
[2019-05-04 22:22:31,615] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.5, 46.5, 6.2, 225.0, 0.0, 0.0, 7.25, -0.8191800045820227, 20.0, 20.74011799520147, 21.5, 0.0, 7.315247960879257], 
current ob forecast is [], 
actual action is [7.5, 20.0], 
sim time this is 4239000.0000, 
sim time next is 4239900.0000, 
raw observation next is [2.75, 45.75, 5.95, 227.5, 0.0, 0.0, 7.5, -0.8297799404172698, 20.0, 20.69520235015528, 21.5, 0.0, 7.364683372619777], 
processed observation next is [0.0, 0.043478260869565216, 0.40384615384615385, 0.4575, 0.5409090909090909, 0.6319444444444444, 0.0, 0.0, 0.625, 0.22340668652757675, 0.0, 0.3850289071650401, 0.5, 0.0, 0.08664333379552679], 
reward next is 0.8634, 
noisyNet noise sample is [array([-0.68098193], dtype=float32), -0.14283071]. 
=============================================
[2019-05-04 22:22:32,734] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.9265419e-23 1.0000000e+00 7.5433686e-21 2.1802090e-19 2.2088828e-25
 1.1919539e-31 4.2481977e-23 1.5685910e-27 3.2745121e-22 7.0777475e-21
 1.2957401e-09], sum to 1.0000
[2019-05-04 22:22:32,734] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9594
[2019-05-04 22:22:32,794] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.0, 28.0, 3.6, 230.0, 119.0, 840.5, 19.4, -0.3765842600224066, 20.0, 22.43452549828481, 22.7, 1.0, 0.6266325011026598], 
current ob forecast is [], 
actual action is [20.0, 20.0], 
sim time this is 4363200.0000, 
sim time next is 4364100.0000, 
raw observation next is [14.9, 28.25, 3.7, 232.5, 118.5, 846.75, 20.0, -0.4156801663608964, 20.0, 22.26091917292573, 22.7, 1.0, 0.5554733518733932], 
processed observation next is [1.0, 0.5217391304347826, 0.7153846153846154, 0.2825, 0.33636363636363636, 0.6458333333333334, 0.3134920634920635, 0.84675, 0.8333333333333334, 0.36143994454636785, 0.0, 0.6087027389893898, 0.6714285714285714, 1.0, 0.006534980610275214], 
reward next is 0.9435, 
noisyNet noise sample is [array([0.62957805], dtype=float32), 1.6655431]. 
=============================================
[2019-05-04 22:22:32,894] A3C_AGENT_WORKER-Thread-6 INFO:Local step 13500, global step 214413: loss 11.3323
[2019-05-04 22:22:32,894] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 13500, global step 214413: learning rate 0.0001
[2019-05-04 22:22:33,327] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13500, global step 214529: loss 12.2268
[2019-05-04 22:22:33,329] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13500, global step 214529: learning rate 0.0001
[2019-05-04 22:22:33,587] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13500, global step 214612: loss 12.6731
[2019-05-04 22:22:33,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13500, global step 214612: learning rate 0.0001
[2019-05-04 22:22:33,809] A3C_AGENT_WORKER-Thread-5 INFO:Local step 13500, global step 214681: loss 11.2592
[2019-05-04 22:22:33,814] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 13500, global step 214682: learning rate 0.0001
[2019-05-04 22:22:34,058] A3C_AGENT_WORKER-Thread-7 INFO:Local step 13500, global step 214762: loss 9.3986
[2019-05-04 22:22:34,058] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 13500, global step 214762: learning rate 0.0001
[2019-05-04 22:22:34,184] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13500, global step 214799: loss 10.0765
[2019-05-04 22:22:34,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13500, global step 214799: learning rate 0.0001
[2019-05-04 22:22:38,333] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13500, global step 216231: loss 5.9663
[2019-05-04 22:22:38,334] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13500, global step 216231: learning rate 0.0001
[2019-05-04 22:22:38,374] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13500, global step 216246: loss 6.6690
[2019-05-04 22:22:38,379] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13500, global step 216251: learning rate 0.0001
[2019-05-04 22:22:38,861] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13500, global step 216430: loss 8.3931
[2019-05-04 22:22:38,861] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13500, global step 216430: learning rate 0.0001
[2019-05-04 22:22:39,070] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.8980112e-12 9.9871969e-01 1.0675594e-11 3.6653460e-14 3.7696281e-17
 3.3025786e-21 2.7723092e-15 1.0382208e-19 4.8216165e-15 4.0943347e-14
 1.2803180e-03], sum to 1.0000
[2019-05-04 22:22:39,070] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1174
[2019-05-04 22:22:39,082] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13500, global step 216527: loss 12.9710
[2019-05-04 22:22:39,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13500, global step 216527: learning rate 0.0001
[2019-05-04 22:22:39,089] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.15, 65.25, 6.075, 237.5, 0.0, 0.0, 11.2, -0.7009705955516513, 20.0, 21.05019003777653, 22.7, 1.0, 3.0263134639961033], 
current ob forecast is [], 
actual action is [11.15, 20.0], 
sim time this is 4299300.0000, 
sim time next is 4300200.0000, 
raw observation next is [6.1, 66.5, 5.95, 235.0, 0.0, 0.0, 11.15, -0.7095650434603318, 20.0, 21.00552881585935, 22.7, 1.0, 3.5698613045134415], 
processed observation next is [0.0, 0.782608695652174, 0.4897435897435898, 0.665, 0.5409090909090909, 0.6527777777777778, 0.0, 0.0, 0.6858333333333333, 0.2634783188465561, 0.0, 0.4293612594084785, 0.6714285714285714, 1.0, 0.04199836828839343], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2142735], dtype=float32), 2.0384052]. 
=============================================
[2019-05-04 22:22:39,128] A3C_AGENT_WORKER-Thread-8 INFO:Local step 13500, global step 216544: loss 12.3980
[2019-05-04 22:22:39,128] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 13500, global step 216544: learning rate 0.0001
[2019-05-04 22:22:39,860] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2087653e-17 1.0000000e+00 7.2754315e-18 3.4080096e-21 1.4425445e-24
 3.9189655e-26 1.0795087e-18 1.1828846e-25 1.6972750e-19 3.6166185e-19
 1.9797766e-10], sum to 1.0000
[2019-05-04 22:22:39,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8972
[2019-05-04 22:22:39,883] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.95, 36.0, 3.6, 205.0, 116.25, 792.0, 16.3, -0.5330109468935992, 20.0, 21.83965916456049, 22.7, 1.0, 2.815582444604052], 
current ob forecast is [], 
actual action is [16.95, 20.0], 
sim time this is 4358700.0000, 
sim time next is 4359600.0000, 
raw observation next is [12.6, 34.0, 3.6, 210.0, 117.5, 804.0, 16.95, -0.5090170603215943, 20.0, 21.92364852419212, 22.7, 1.0, 2.4256327354973926], 
processed observation next is [1.0, 0.4782608695652174, 0.6564102564102564, 0.34, 0.32727272727272727, 0.5833333333333334, 0.31084656084656087, 0.804, 0.7825000000000001, 0.33032764655946856, 0.0, 0.5605212177417316, 0.6714285714285714, 1.0, 0.02853685571173403], 
reward next is 0.9250, 
noisyNet noise sample is [array([0.5147859], dtype=float32), -1.638391]. 
=============================================
[2019-05-04 22:22:40,552] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13500, global step 217133: loss 9.2376
[2019-05-04 22:22:40,553] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13500, global step 217133: learning rate 0.0001
[2019-05-04 22:22:40,955] A3C_AGENT_WORKER-Thread-4 INFO:Local step 13500, global step 217251: loss 8.6429
[2019-05-04 22:22:40,959] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 13500, global step 217251: learning rate 0.0001
[2019-05-04 22:22:41,289] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13500, global step 217326: loss 10.5099
[2019-05-04 22:22:41,289] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13500, global step 217326: learning rate 0.0001
[2019-05-04 22:22:42,096] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13500, global step 217515: loss 11.4455
[2019-05-04 22:22:42,106] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13500, global step 217517: learning rate 0.0001
[2019-05-04 22:22:42,803] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13500, global step 217688: loss 5.4538
[2019-05-04 22:22:42,804] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13500, global step 217688: learning rate 0.0001
[2019-05-04 22:22:46,757] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.11629923e-14 9.99999881e-01 2.88902927e-15 1.10778062e-17
 1.11292162e-19 5.46660136e-24 4.27222971e-17 3.21728300e-22
 2.27629093e-16 1.67348313e-16 1.11228545e-07], sum to 1.0000
[2019-05-04 22:22:46,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3798
[2019-05-04 22:22:46,777] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.2, 65.0, 0.0, 0.0, 0.0, 0.0, 4.95, -0.8960321981179344, 20.0, 20.00193963548217, 21.5, 0.0, 4.702709849052873], 
current ob forecast is [], 
actual action is [4.8, 20.0], 
sim time this is 4586400.0000, 
sim time next is 4587300.0000, 
raw observation next is [-0.425, 65.5, 0.0, 0.0, 0.0, 0.0, 4.8, -0.9095558224488745, 20.0, 19.94653692646064, 21.5, 0.0, 4.861203343608783], 
processed observation next is [1.0, 0.08695652173913043, 0.3224358974358974, 0.655, 0.0, 0.0, 0.0, 0.0, 0.58, 0.19681472585037518, 0.0, 0.27807670378009164, 0.5, 0.0, 0.05719062757186803], 
reward next is 0.8928, 
noisyNet noise sample is [array([-0.6019385], dtype=float32), 2.8057692]. 
=============================================
[2019-05-04 22:22:49,640] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8632976e-17 9.9999976e-01 4.1345065e-16 4.2556871e-17 1.9289765e-23
 3.0822691e-25 2.5497985e-18 2.2032774e-25 2.6889869e-18 1.7328189e-18
 2.3175561e-07], sum to 1.0000
[2019-05-04 22:22:49,642] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9441
[2019-05-04 22:22:49,677] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.25, 49.0, 2.1, 72.5, 120.5, 852.5, 8.0, -0.5162171609443984, 20.0, 21.82413342788294, 22.7, 1.0, 4.426643219921497], 
current ob forecast is [], 
actual action is [8.25, 20.0], 
sim time this is 4623300.0000, 
sim time next is 4624200.0000, 
raw observation next is [3.5, 49.0, 2.1, 65.0, 120.0, 859.0, 8.25, -0.5143407609438974, 20.0, 21.76583698613841, 22.7, 1.0, 3.0066511214925207], 
processed observation next is [1.0, 0.5217391304347826, 0.4230769230769231, 0.49, 0.19090909090909092, 0.18055555555555555, 0.31746031746031744, 0.859, 0.6375, 0.32855307968536757, 0.0, 0.5379767123054872, 0.6714285714285714, 1.0, 0.035372366135206126], 
reward next is 0.9170, 
noisyNet noise sample is [array([-0.8247109], dtype=float32), 0.041021418]. 
=============================================
[2019-05-04 22:22:57,432] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0021337e-12 9.9993753e-01 3.7072467e-12 3.9879506e-13 4.5770871e-16
 1.7712240e-19 2.4407302e-13 1.5300575e-18 4.1764816e-14 2.1422835e-14
 6.2475920e-05], sum to 1.0000
[2019-05-04 22:22:57,432] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4861
[2019-05-04 22:22:57,489] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.3, 68.0, 0.0, 0.0, 0.0, 0.0, 3.8, -1.012048505208766, 20.0, 19.48778272206842, 21.5, 0.0, 6.001891670152567], 
current ob forecast is [], 
actual action is [3.7, 20.0], 
sim time this is 4591800.0000, 
sim time next is 4592700.0000, 
raw observation next is [-1.4, 68.5, 0.0, 0.0, 0.0, 0.0, 3.7, -1.024884398113331, 20.0, 19.43411788507741, 21.5, 0.0, 6.095805161608475], 
processed observation next is [1.0, 0.13043478260869565, 0.29743589743589743, 0.685, 0.0, 0.0, 0.0, 0.0, 0.5616666666666668, 0.1583718672955563, 0.0, 0.20487398358248704, 0.5, 0.0, 0.07171535484245266], 
reward next is 0.8783, 
noisyNet noise sample is [array([1.0749362], dtype=float32), -0.43558642]. 
=============================================
[2019-05-04 22:23:02,679] A3C_AGENT_WORKER-Thread-6 INFO:Local step 14000, global step 222570: loss 4.7303
[2019-05-04 22:23:02,679] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 14000, global step 222570: learning rate 0.0001
[2019-05-04 22:23:04,933] A3C_AGENT_WORKER-Thread-3 INFO:Local step 14000, global step 223019: loss 10.9771
[2019-05-04 22:23:04,934] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 14000, global step 223019: learning rate 0.0001
[2019-05-04 22:23:05,259] A3C_AGENT_WORKER-Thread-5 INFO:Local step 14000, global step 223085: loss 9.5696
[2019-05-04 22:23:05,262] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 14000, global step 223085: learning rate 0.0001
[2019-05-04 22:23:05,567] A3C_AGENT_WORKER-Thread-7 INFO:Local step 14000, global step 223144: loss 12.9335
[2019-05-04 22:23:05,567] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 14000, global step 223144: learning rate 0.0001
[2019-05-04 22:23:05,581] A3C_AGENT_WORKER-Thread-2 INFO:Local step 14000, global step 223147: loss 9.8361
[2019-05-04 22:23:05,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 14000, global step 223147: learning rate 0.0001
[2019-05-04 22:23:05,626] A3C_AGENT_WORKER-Thread-12 INFO:Local step 14000, global step 223157: loss 11.5966
[2019-05-04 22:23:05,627] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 14000, global step 223158: learning rate 0.0001
[2019-05-04 22:23:05,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5811605e-10 9.2187548e-01 4.8678745e-09 1.5856243e-09 3.6278773e-12
 9.3967518e-14 2.5098842e-10 4.5601234e-13 5.2362392e-10 5.7271737e-10
 7.8124516e-02], sum to 1.0000
[2019-05-04 22:23:05,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1476
[2019-05-04 22:23:05,812] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 71.0, 3.6, 360.0, 174.0, 421.0, 0.5, -0.7932936143534484, 65.0, 20.74061166484916, 22.7, 1.0, 69.45900700354228], 
current ob forecast is [], 
actual action is [1.0, 20.0], 
sim time this is 4786200.0000, 
sim time next is 4787100.0000, 
raw observation next is [-3.5, 68.0, 3.6, 360.0, 168.75, 498.25, 1.0, -0.790038761941113, 20.0, 20.83441117728951, 22.7, 1.0, 45.2464585329453], 
processed observation next is [0.0, 0.391304347826087, 0.24358974358974358, 0.68, 0.32727272727272727, 1.0, 0.44642857142857145, 0.49825, 0.5166666666666667, 0.236653746019629, 0.0, 0.40491588246993004, 0.6714285714285714, 1.0, 0.5323112768581799], 
reward next is 0.0325, 
noisyNet noise sample is [array([-1.2865213], dtype=float32), 0.93004423]. 
=============================================
[2019-05-04 22:23:09,161] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8186650e-12 9.9744499e-01 1.7268470e-10 9.0767366e-11 1.3087431e-15
 2.6758641e-17 2.1103783e-12 9.1633354e-17 2.3980587e-12 1.1024109e-11
 2.5550434e-03], sum to 1.0000
[2019-05-04 22:23:09,161] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0205
[2019-05-04 22:23:09,198] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 92.0, 3.225, 332.5, 0.0, 0.0, -1.0, -0.9793471022734614, 20.0, 19.71367352452925, 21.5, 0.0, 30.86793832717109], 
current ob forecast is [], 
actual action is [-1.0, 20.0], 
sim time this is 4769100.0000, 
sim time next is 4770000.0000, 
raw observation next is [-6.0, 92.0, 3.1, 330.0, 0.0, 0.0, -1.0, -1.016727456285374, 20.0, 19.55109514369328, 21.5, 0.0, 24.88079157002731], 
processed observation next is [0.0, 0.21739130434782608, 0.1794871794871795, 0.92, 0.2818181818181818, 0.9166666666666666, 0.0, 0.0, 0.48333333333333334, 0.1610908479048753, 0.0, 0.22158502052761161, 0.5, 0.0, 0.2927151949414978], 
reward next is 0.6573, 
noisyNet noise sample is [array([-0.89028144], dtype=float32), 0.5480171]. 
=============================================
[2019-05-04 22:23:09,234] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[62.359474]
 [62.633408]
 [62.03883 ]
 [62.76441 ]
 [62.805714]], R is [[62.57940674]
 [62.54046249]
 [62.38535309]
 [61.76150131]
 [61.98188019]].
[2019-05-04 22:23:09,528] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9124402e-11 9.9999392e-01 3.7969831e-12 6.8502989e-13 6.5609262e-15
 4.7301619e-17 7.5386637e-13 8.2551715e-18 3.7903385e-13 2.1353041e-12
 6.1282926e-06], sum to 1.0000
[2019-05-04 22:23:09,528] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0427
[2019-05-04 22:23:09,574] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.5, 86.75, 2.6, 310.0, 0.0, 0.0, 0.0, -0.9970900524249338, 20.0, 19.5166535194497, 21.5, 0.0, 8.517829235378139], 
current ob forecast is [], 
actual action is [-0.5, 20.0], 
sim time this is 4761900.0000, 
sim time next is 4762800.0000, 
raw observation next is [-6.0, 92.0, 2.6, 310.0, 0.0, 0.0, -0.5, -1.017560756927236, 20.0, 19.43101010936605, 21.5, 0.0, 8.67811556523357], 
processed observation next is [0.0, 0.13043478260869565, 0.1794871794871795, 0.92, 0.23636363636363636, 0.8611111111111112, 0.0, 0.0, 0.49166666666666664, 0.16081308102425465, 0.0, 0.20443001562372132, 0.5, 0.0, 0.102095477238042], 
reward next is 0.8479, 
noisyNet noise sample is [array([-0.4875513], dtype=float32), -0.33452833]. 
=============================================
[2019-05-04 22:23:09,655] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4514352e-13 9.9999666e-01 9.9835973e-12 3.0239765e-13 6.9351818e-16
 3.7606633e-18 4.6431375e-13 2.8470834e-16 8.4087641e-12 2.9362878e-13
 3.3684673e-06], sum to 1.0000
[2019-05-04 22:23:09,658] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5779
[2019-05-04 22:23:09,677] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.5, 86.75, 2.6, 310.0, 0.0, 0.0, 0.0, -1.004642068864056, 20.0, 19.50244997523438, 21.5, 0.0, 8.993808030095494], 
current ob forecast is [], 
actual action is [-0.5, 20.0], 
sim time this is 4761900.0000, 
sim time next is 4762800.0000, 
raw observation next is [-6.0, 92.0, 2.6, 310.0, 0.0, 0.0, -0.5, -1.024928665778422, 20.0, 19.41713806890496, 21.5, 0.0, 9.108935240194638], 
processed observation next is [0.0, 0.13043478260869565, 0.1794871794871795, 0.92, 0.23636363636363636, 0.8611111111111112, 0.0, 0.0, 0.49166666666666664, 0.15835711140719266, 0.0, 0.20244829555785163, 0.5, 0.0, 0.10716394400228986], 
reward next is 0.8428, 
noisyNet noise sample is [array([-0.37046215], dtype=float32), -0.81672317]. 
=============================================
[2019-05-04 22:23:10,064] A3C_AGENT_WORKER-Thread-15 INFO:Local step 14000, global step 224007: loss 0.4922
[2019-05-04 22:23:10,070] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 14000, global step 224007: learning rate 0.0001
[2019-05-04 22:23:10,299] A3C_AGENT_WORKER-Thread-14 INFO:Local step 14000, global step 224058: loss 0.6689
[2019-05-04 22:23:10,300] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 14000, global step 224059: learning rate 0.0001
[2019-05-04 22:23:10,523] A3C_AGENT_WORKER-Thread-13 INFO:Local step 14000, global step 224125: loss 0.1186
[2019-05-04 22:23:10,525] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 14000, global step 224125: learning rate 0.0001
[2019-05-04 22:23:10,715] A3C_AGENT_WORKER-Thread-18 INFO:Local step 14000, global step 224189: loss 0.2726
[2019-05-04 22:23:10,718] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 14000, global step 224190: learning rate 0.0001
[2019-05-04 22:23:11,969] A3C_AGENT_WORKER-Thread-8 INFO:Local step 14000, global step 224672: loss 1.3561
[2019-05-04 22:23:11,971] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 14000, global step 224672: learning rate 0.0001
[2019-05-04 22:23:12,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1841125e-12 7.2691000e-01 1.0723765e-12 1.6736295e-13 1.7047810e-17
 1.0959789e-19 5.7503264e-14 2.2144039e-18 6.2536136e-13 2.2579437e-16
 2.7309006e-01], sum to 1.0000
[2019-05-04 22:23:12,905] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5882
[2019-05-04 22:23:13,040] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.75, 60.25, 3.475, 360.0, 158.25, 652.75, 2.0, -0.7280021571298994, 65.0, 21.04674955946044, 22.7, 1.0, 71.77044225508587], 
current ob forecast is [], 
actual action is [2.25, 20.0], 
sim time this is 4788900.0000, 
sim time next is 4789800.0000, 
raw observation next is [-2.5, 55.5, 3.35, 360.0, 153.0, 730.0, 2.25, -0.7159260084812326, 20.0, 21.2061667908171, 22.7, 1.0, 44.58912686152606], 
processed observation next is [0.0, 0.43478260869565216, 0.2692307692307692, 0.555, 0.30454545454545456, 1.0, 0.40476190476190477, 0.73, 0.5375, 0.26135799717292246, 0.0, 0.45802382725958574, 0.6714285714285714, 1.0, 0.5245779630767772], 
reward next is 0.1208, 
noisyNet noise sample is [array([0.20600659], dtype=float32), 0.10272049]. 
=============================================
[2019-05-04 22:23:14,450] A3C_AGENT_WORKER-Thread-19 INFO:Local step 14000, global step 225108: loss 4.3502
[2019-05-04 22:23:14,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 14000, global step 225108: learning rate 0.0001
[2019-05-04 22:23:16,850] A3C_AGENT_WORKER-Thread-17 INFO:Local step 14000, global step 225365: loss 2.1022
[2019-05-04 22:23:16,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 14000, global step 225366: learning rate 0.0001
[2019-05-04 22:23:17,298] A3C_AGENT_WORKER-Thread-4 INFO:Local step 14000, global step 225450: loss 2.1687
[2019-05-04 22:23:17,299] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 14000, global step 225450: learning rate 0.0001
[2019-05-04 22:23:17,838] A3C_AGENT_WORKER-Thread-11 INFO:Local step 14000, global step 225612: loss 5.7101
[2019-05-04 22:23:17,839] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 14000, global step 225612: learning rate 0.0001
[2019-05-04 22:23:17,903] A3C_AGENT_WORKER-Thread-16 INFO:Local step 14000, global step 225634: loss 6.7320
[2019-05-04 22:23:17,905] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 14000, global step 225634: learning rate 0.0001
[2019-05-04 22:23:21,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4127368e-20 9.9954468e-01 4.8509280e-20 4.4897601e-22 1.7899422e-28
 2.0239766e-32 1.8606123e-22 2.8493315e-28 2.7296165e-21 1.7240167e-23
 4.5529223e-04], sum to 1.0000
[2019-05-04 22:23:21,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1813
[2019-05-04 22:23:21,388] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.5, 25.5, 0.75, 105.0, 124.0, 865.0, 13.25, -0.4463935912935182, 20.0, 22.08213585763508, 22.2, 1.0, 1.0467820074901784], 
current ob forecast is [], 
actual action is [13.5, 20.0], 
sim time this is 5056200.0000, 
sim time next is 5057100.0000, 
raw observation next is [8.75, 25.25, 1.125, 157.5, 122.5, 863.75, 13.5, -0.4316919916699213, 20.0, 22.12598033518799, 22.2, 1.0, 0.9741780207691454], 
processed observation next is [1.0, 0.5217391304347826, 0.5576923076923077, 0.2525, 0.10227272727272728, 0.4375, 0.32407407407407407, 0.86375, 0.725, 0.35610266944335955, 0.0, 0.5894257621697128, 0.5999999999999999, 1.0, 0.01146091789140171], 
reward next is 0.9385, 
noisyNet noise sample is [array([2.2727954], dtype=float32), 0.19370925]. 
=============================================
[2019-05-04 22:23:27,778] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6006514e-16 1.0000000e+00 2.7124473e-17 9.8229716e-18 4.2132591e-22
 3.2645916e-25 1.6024858e-18 3.9926834e-24 1.0126626e-18 9.0078728e-19
 6.1281210e-13], sum to 1.0000
[2019-05-04 22:23:27,783] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5253
[2019-05-04 22:23:27,804] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.5, 56.0, 5.4, 290.0, 0.0, 0.0, 11.75, -0.4601401129968877, 20.0, 21.48237643978115, 19.4, 0.0, 4.71240777850652], 
current ob forecast is [], 
actual action is [11.5, 20.0], 
sim time this is 5182200.0000, 
sim time next is 5183100.0000, 
raw observation next is [6.25, 58.0, 5.25, 285.0, 0.0, 0.0, 11.5, -0.4719513192991955, 20.0, 21.43837360687986, 19.4, 0.0, 4.78981963999134], 
processed observation next is [1.0, 1.0, 0.4935897435897436, 0.58, 0.4772727272727273, 0.7916666666666666, 0.0, 0.0, 0.6916666666666667, 0.3426828935669348, 0.0, 0.4911962295542658, 0.1999999999999998, 0.0, 0.056350819294015767], 
reward next is 0.8936, 
noisyNet noise sample is [array([-1.0740814], dtype=float32), 0.38989148]. 
=============================================
[2019-05-04 22:23:28,075] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3310305e-15 1.0000000e+00 4.9089579e-13 1.8473890e-15 4.3417577e-18
 1.9349116e-21 1.9899603e-14 7.7505934e-19 5.6188669e-14 6.9652677e-17
 1.0673710e-08], sum to 1.0000
[2019-05-04 22:23:28,076] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4578
[2019-05-04 22:23:28,101] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.0, 56.0, 5.825, 290.0, 0.0, 0.0, 12.0, -0.4603705738285178, 20.0, 21.3281334934112, 19.4, 0.0, 4.187713138046857], 
current ob forecast is [], 
actual action is [12.0, 20.0], 
sim time this is 5175900.0000, 
sim time next is 5176800.0000, 
raw observation next is [7.0, 56.0, 5.7, 280.0, 0.0, 0.0, 12.0, -0.4668980155677183, 20.0, 21.30152571356318, 19.4, 0.0, 4.311176532122434], 
processed observation next is [1.0, 0.9565217391304348, 0.5128205128205128, 0.56, 0.5181818181818182, 0.7777777777777778, 0.0, 0.0, 0.7, 0.34436732814409393, 0.0, 0.4716465305090257, 0.1999999999999998, 0.0, 0.05071972390732275], 
reward next is 0.8993, 
noisyNet noise sample is [array([-2.1399992], dtype=float32), 0.53963983]. 
=============================================
[2019-05-04 22:23:32,202] A3C_AGENT_WORKER-Thread-6 INFO:Local step 14500, global step 230306: loss 0.9277
[2019-05-04 22:23:32,205] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 14500, global step 230307: learning rate 0.0001
[2019-05-04 22:23:32,719] A3C_AGENT_WORKER-Thread-3 INFO:Local step 14500, global step 230520: loss 1.5655
[2019-05-04 22:23:32,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 14500, global step 230520: learning rate 0.0001
[2019-05-04 22:23:32,722] A3C_AGENT_WORKER-Thread-12 INFO:Local step 14500, global step 230521: loss 1.1642
[2019-05-04 22:23:32,723] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 14500, global step 230521: learning rate 0.0001
[2019-05-04 22:23:33,271] A3C_AGENT_WORKER-Thread-5 INFO:Local step 14500, global step 230741: loss 1.8522
[2019-05-04 22:23:33,272] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 14500, global step 230741: learning rate 0.0001
[2019-05-04 22:23:33,276] A3C_AGENT_WORKER-Thread-7 INFO:Local step 14500, global step 230741: loss 1.4847
[2019-05-04 22:23:33,276] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 14500, global step 230741: learning rate 0.0001
[2019-05-04 22:23:34,017] A3C_AGENT_WORKER-Thread-2 INFO:Local step 14500, global step 230957: loss 0.2354
[2019-05-04 22:23:34,027] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 14500, global step 230957: learning rate 0.0001
[2019-05-04 22:23:37,557] A3C_AGENT_WORKER-Thread-15 INFO:Local step 14500, global step 231932: loss 0.2032
[2019-05-04 22:23:37,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 14500, global step 231932: learning rate 0.0001
[2019-05-04 22:23:37,828] A3C_AGENT_WORKER-Thread-18 INFO:Local step 14500, global step 232054: loss 0.4283
[2019-05-04 22:23:37,831] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 14500, global step 232054: learning rate 0.0001
[2019-05-04 22:23:37,833] A3C_AGENT_WORKER-Thread-14 INFO:Local step 14500, global step 232055: loss 0.2252
[2019-05-04 22:23:37,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 14500, global step 232056: learning rate 0.0001
[2019-05-04 22:23:38,149] A3C_AGENT_WORKER-Thread-13 INFO:Local step 14500, global step 232194: loss 0.6568
[2019-05-04 22:23:38,150] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 14500, global step 232194: learning rate 0.0001
[2019-05-04 22:23:39,306] A3C_AGENT_WORKER-Thread-8 INFO:Local step 14500, global step 232550: loss 0.0686
[2019-05-04 22:23:39,307] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 14500, global step 232550: learning rate 0.0001
[2019-05-04 22:23:42,817] A3C_AGENT_WORKER-Thread-19 INFO:Local step 14500, global step 233312: loss 4.6068
[2019-05-04 22:23:42,818] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 14500, global step 233312: learning rate 0.0001
[2019-05-04 22:23:43,979] A3C_AGENT_WORKER-Thread-4 INFO:Local step 14500, global step 233816: loss 0.2255
[2019-05-04 22:23:43,984] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 14500, global step 233817: learning rate 0.0001
[2019-05-04 22:23:44,241] A3C_AGENT_WORKER-Thread-17 INFO:Local step 14500, global step 233925: loss 0.3835
[2019-05-04 22:23:44,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 14500, global step 233925: learning rate 0.0001
[2019-05-04 22:23:44,494] A3C_AGENT_WORKER-Thread-16 INFO:Local step 14500, global step 234019: loss 0.8455
[2019-05-04 22:23:44,500] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 14500, global step 234022: learning rate 0.0001
[2019-05-04 22:23:44,821] A3C_AGENT_WORKER-Thread-11 INFO:Local step 14500, global step 234145: loss 0.1060
[2019-05-04 22:23:44,822] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 14500, global step 234145: learning rate 0.0001
[2019-05-04 22:23:46,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4626347e-19 1.0000000e+00 2.1569059e-19 2.2241322e-21 4.7029673e-25
 7.7683400e-30 5.5345130e-20 9.9223083e-28 2.7270096e-20 8.7104197e-23
 1.8692867e-15], sum to 1.0000
[2019-05-04 22:23:46,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4727
[2019-05-04 22:23:46,576] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.5, 42.5, 4.1, 260.0, 0.0, 0.0, 12.75, -0.6566108146915118, 20.0, 21.16147526125199, 19.4, 0.0, 3.2818513881601965], 
current ob forecast is [], 
actual action is [12.5, 20.0], 
sim time this is 5434200.0000, 
sim time next is 5435100.0000, 
raw observation next is [7.25, 43.25, 4.1, 260.0, 0.0, 0.0, 12.5, -0.6661692966642779, 20.0, 21.1217851438839, 19.4, 0.0, 3.4635638002329827], 
processed observation next is [0.0, 0.9130434782608695, 0.5192307692307693, 0.4325, 0.3727272727272727, 0.7222222222222222, 0.0, 0.0, 0.7083333333333334, 0.27794356777857404, 0.0, 0.44596930626912873, 0.1999999999999998, 0.0, 0.040747809414505676], 
reward next is 0.9093, 
noisyNet noise sample is [array([0.3425764], dtype=float32), -0.48743448]. 
=============================================
[2019-05-04 22:23:46,677] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.8446083e-15 1.0000000e+00 2.4584344e-15 2.3413997e-17 5.6656741e-20
 6.9234745e-22 1.4630387e-16 6.4563916e-22 6.5368832e-15 3.8934301e-16
 3.4424477e-12], sum to 1.0000
[2019-05-04 22:23:46,678] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3203
[2019-05-04 22:23:46,706] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.25, 67.0, 3.1, 350.0, 0.0, 0.0, 8.5, -0.7307188075752298, 20.0, 20.67304005234664, 19.4, 0.0, 5.1869712424183305], 
current ob forecast is [], 
actual action is [8.25, 20.0], 
sim time this is 5535900.0000, 
sim time next is 5536800.0000, 
raw observation next is [3.0, 68.0, 3.1, 350.0, 0.0, 0.0, 8.25, -0.7432053483011387, 20.0, 20.61706616698326, 19.4, 0.0, 5.335252628665615], 
processed observation next is [1.0, 0.08695652173913043, 0.41025641025641024, 0.68, 0.2818181818181818, 0.9722222222222222, 0.0, 0.0, 0.6375, 0.25226488389962043, 0.0, 0.37386659528332294, 0.1999999999999998, 0.0, 0.06276767798430136], 
reward next is 0.8872, 
noisyNet noise sample is [array([-0.02643088], dtype=float32), -0.67244714]. 
=============================================
[2019-05-04 22:23:47,323] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.4803926e-18 1.0000000e+00 4.5678749e-20 2.8539363e-21 3.3996111e-27
 1.7702373e-30 2.0046504e-21 3.0530350e-27 8.7650171e-23 1.4022389e-22
 1.2788117e-12], sum to 1.0000
[2019-05-04 22:23:47,324] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6212
[2019-05-04 22:23:47,350] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.0, 48.5, 3.1, 355.0, 145.0, 835.0, 15.5, -0.3541327489158272, 20.0, 22.18517230348103, 22.2, 1.0, 0.9198946293601019], 
current ob forecast is [], 
actual action is [16.0, 20.0], 
sim time this is 5574600.0000, 
sim time next is 5575500.0000, 
raw observation next is [11.5, 47.75, 3.1, 352.5, 143.75, 834.0, 16.0, -0.3296844963695231, 20.0, 22.29552513204362, 22.2, 1.0, 0.8501865691004373], 
processed observation next is [1.0, 0.5217391304347826, 0.6282051282051282, 0.4775, 0.2818181818181818, 0.9791666666666666, 0.3802910052910053, 0.834, 0.7666666666666667, 0.39010516787682564, 0.0, 0.6136464474348026, 0.5999999999999999, 1.0, 0.01000219493059338], 
reward next is 0.9400, 
noisyNet noise sample is [array([0.788714], dtype=float32), -0.3637613]. 
=============================================
[2019-05-04 22:23:47,373] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[96.51466]
 [96.15357]
 [96.16147]
 [95.92561]
 [95.58164]], R is [[97.02799225]
 [96.99689484]
 [96.96538544]
 [96.93350983]
 [96.90118408]].
[2019-05-04 22:23:47,575] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0979531e-16 1.0000000e+00 8.7521850e-16 4.1068088e-17 4.8433517e-21
 5.5487076e-24 2.0200303e-17 1.3155899e-21 6.3594638e-17 3.5109175e-17
 1.3145851e-12], sum to 1.0000
[2019-05-04 22:23:47,576] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5368
[2019-05-04 22:23:47,595] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.75, 48.75, 3.6, 350.0, 0.0, 0.0, 8.0, -0.7313090156886984, 20.0, 20.60152520332996, 19.4, 0.0, 5.440994148788672], 
current ob forecast is [], 
actual action is [7.75, 20.0], 
sim time this is 5350500.0000, 
sim time next is 5351400.0000, 
raw observation next is [2.5, 48.5, 3.6, 350.0, 0.0, 0.0, 7.75, -0.752664427329818, 20.0, 20.46988660351273, 19.4, 0.0, 5.6412484812790575], 
processed observation next is [1.0, 0.9565217391304348, 0.3974358974358974, 0.485, 0.32727272727272727, 0.9722222222222222, 0.0, 0.0, 0.6291666666666667, 0.24911185755672735, 0.0, 0.3528409433589615, 0.1999999999999998, 0.0, 0.06636762919151833], 
reward next is 0.8836, 
noisyNet noise sample is [array([-1.4190301], dtype=float32), 0.60891694]. 
=============================================
[2019-05-04 22:23:53,110] A3C_AGENT_WORKER-Thread-6 INFO:Local step 15000, global step 237488: loss 0.0013
[2019-05-04 22:23:53,112] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 15000, global step 237488: learning rate 0.0001
[2019-05-04 22:23:54,157] A3C_AGENT_WORKER-Thread-12 INFO:Local step 15000, global step 237948: loss 0.0141
[2019-05-04 22:23:54,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 15000, global step 237949: learning rate 0.0001
[2019-05-04 22:23:54,206] A3C_AGENT_WORKER-Thread-3 INFO:Local step 15000, global step 237973: loss 0.0019
[2019-05-04 22:23:54,208] A3C_AGENT_WORKER-Thread-5 INFO:Local step 15000, global step 237973: loss 0.0080
[2019-05-04 22:23:54,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 15000, global step 237973: learning rate 0.0001
[2019-05-04 22:23:54,212] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 15000, global step 237974: learning rate 0.0001
[2019-05-04 22:23:54,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5235896e-17 1.0000000e+00 1.6355281e-16 6.8204898e-19 3.1976913e-24
 2.7771928e-26 2.8968136e-20 4.7120382e-24 2.7794390e-19 1.6895103e-19
 2.0682076e-12], sum to 1.0000
[2019-05-04 22:23:54,257] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4918
[2019-05-04 22:23:54,267] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.25, 62.0, 5.25, 262.5, 0.0, 0.0, 21.0, 0.04596805352278378, 20.0, 23.27109482756742, 19.4, 0.0, 0.15323217433433642], 
current ob forecast is [], 
actual action is [21.25, 20.0], 
sim time this is 5724900.0000, 
sim time next is 5725800.0000, 
raw observation next is [16.5, 61.0, 5.4, 265.0, 0.0, 0.0, 21.25, 0.04285513270177945, 20.0, 23.2588097729936, 19.4, 0.0, 0.1662490068364215], 
processed observation next is [1.0, 0.2608695652173913, 0.7564102564102564, 0.61, 0.49090909090909096, 0.7361111111111112, 0.0, 0.0, 0.8541666666666666, 0.5142850442339265, 0.0, 0.7512585389990859, 0.1999999999999998, 0.0, 0.001955870668663782], 
reward next is 0.9480, 
noisyNet noise sample is [array([-2.0378592], dtype=float32), -0.14573352]. 
=============================================
[2019-05-04 22:23:54,604] A3C_AGENT_WORKER-Thread-7 INFO:Local step 15000, global step 238138: loss 0.0016
[2019-05-04 22:23:54,606] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 15000, global step 238139: learning rate 0.0001
[2019-05-04 22:23:55,209] A3C_AGENT_WORKER-Thread-2 INFO:Local step 15000, global step 238412: loss 0.1392
[2019-05-04 22:23:55,212] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 15000, global step 238414: learning rate 0.0001
[2019-05-04 22:23:58,258] A3C_AGENT_WORKER-Thread-15 INFO:Local step 15000, global step 239741: loss 0.0211
[2019-05-04 22:23:58,260] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 15000, global step 239741: learning rate 0.0001
[2019-05-04 22:23:58,626] A3C_AGENT_WORKER-Thread-14 INFO:Local step 15000, global step 239888: loss 0.0103
[2019-05-04 22:23:58,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 15000, global step 239889: learning rate 0.0001
[2019-05-04 22:23:58,682] A3C_AGENT_WORKER-Thread-18 INFO:Local step 15000, global step 239906: loss 0.0030
[2019-05-04 22:23:58,687] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 15000, global step 239909: learning rate 0.0001
[2019-05-04 22:23:58,949] A3C_AGENT_WORKER-Thread-13 INFO:Local step 15000, global step 240010: loss 0.0200
[2019-05-04 22:23:58,952] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 15000, global step 240011: learning rate 0.0001
[2019-05-04 22:24:00,593] A3C_AGENT_WORKER-Thread-8 INFO:Local step 15000, global step 240673: loss 0.0762
[2019-05-04 22:24:00,595] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 15000, global step 240674: learning rate 0.0001
[2019-05-04 22:24:02,204] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.8911185e-21 1.0000000e+00 1.6984391e-21 9.8084554e-23 6.3255279e-29
 1.4112054e-30 1.7164269e-23 1.2523740e-29 4.6395133e-25 2.9661596e-24
 8.2699537e-15], sum to 1.0000
[2019-05-04 22:24:02,205] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4024
[2019-05-04 22:24:02,219] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.0, 55.0, 4.975, 200.0, 0.0, 0.0, 22.0, 0.5552436493767093, 20.0, 25.35621601597278, 19.4, 0.0, 0.0], 
current ob forecast is [], 
actual action is [22.0, 20.0], 
sim time this is 5811300.0000, 
sim time next is 5812200.0000, 
raw observation next is [17.0, 55.0, 4.85, 200.0, 0.0, 0.0, 22.0, 0.54931223628725, 20.0, 25.33452107272518, 19.4, 0.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7692307692307693, 0.55, 0.44090909090909086, 0.5555555555555556, 0.0, 0.0, 0.8666666666666667, 0.6831040787624166, 0.0, 1.047788724675026, 0.1999999999999998, 0.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([0.458126], dtype=float32), -0.049687143]. 
=============================================
[2019-05-04 22:24:02,722] A3C_AGENT_WORKER-Thread-19 INFO:Local step 15000, global step 241601: loss 0.4136
[2019-05-04 22:24:02,722] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 15000, global step 241601: learning rate 0.0001
[2019-05-04 22:24:02,751] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1371017e-22 1.0000000e+00 1.7495671e-22 1.1590506e-24 2.9950989e-30
 4.8005119e-36 4.8268501e-26 1.3109328e-33 1.6358405e-23 5.0176558e-26
 2.2084340e-12], sum to 1.0000
[2019-05-04 22:24:02,757] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4262
[2019-05-04 22:24:02,790] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 51.5, 9.275, 235.0, 337.5, 116.25, 26.5, 0.9221177447428549, 20.0, 27.04736472152609, 22.2, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.75, 20.0], 
sim time this is 5831100.0000, 
sim time next is 5832000.0000, 
raw observation next is [22.0, 50.0, 9.8, 240.0, 341.0, 117.5, 26.75, 0.9108554038973793, 20.0, 27.04375762202576, 22.2, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8974358974358975, 0.5, 0.890909090909091, 0.6666666666666666, 0.9021164021164021, 0.1175, 0.9458333333333333, 0.8036184679657931, 0.0, 1.2919653745751083, 0.5999999999999999, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-1.2186422], dtype=float32), -0.15823585]. 
=============================================
[2019-05-04 22:24:02,814] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[115.71968 ]
 [115.18183 ]
 [114.67042 ]
 [114.225685]
 [113.909325]], R is [[115.68077087]
 [115.47396088]
 [115.26921844]
 [115.06652069]
 [114.86585236]].
[2019-05-04 22:24:04,059] A3C_AGENT_WORKER-Thread-4 INFO:Local step 15000, global step 242168: loss 0.0419
[2019-05-04 22:24:04,063] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 15000, global step 242170: learning rate 0.0001
[2019-05-04 22:24:05,221] A3C_AGENT_WORKER-Thread-17 INFO:Local step 15000, global step 242697: loss 0.0746
[2019-05-04 22:24:05,224] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 15000, global step 242698: learning rate 0.0001
[2019-05-04 22:24:05,575] A3C_AGENT_WORKER-Thread-16 INFO:Local step 15000, global step 242861: loss 0.2643
[2019-05-04 22:24:05,578] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 15000, global step 242861: learning rate 0.0001
[2019-05-04 22:24:05,821] A3C_AGENT_WORKER-Thread-11 INFO:Local step 15000, global step 242964: loss 0.0273
[2019-05-04 22:24:05,823] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 15000, global step 242965: learning rate 0.0001
[2019-05-04 22:24:05,923] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.3950338e-16 1.0000000e+00 4.3667890e-18 7.9800245e-17 1.2007822e-20
 9.7944990e-25 6.7165701e-18 1.1859265e-23 4.1354203e-17 4.2901130e-17
 3.9898702e-11], sum to 1.0000
[2019-05-04 22:24:05,937] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4064
[2019-05-04 22:24:05,953] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.5, 70.5, 7.949999999999999, 265.0, 141.0, 430.0, 21.75, 0.974286058946006, 20.0, 26.94950972208935, 22.2, 1.0, 0.6228977376186603], 
current ob forecast is [], 
actual action is [22.5, 20.0], 
sim time this is 5848200.0000, 
sim time next is 5849100.0000, 
raw observation next is [18.25, 67.25, 8.075, 267.5, 118.25, 407.5, 22.5, 1.036573218675427, 20.0, 27.39137566179831, 22.2, 1.0, 0.22840698910129775], 
processed observation next is [1.0, 0.6956521739130435, 0.8012820512820513, 0.6725, 0.734090909090909, 0.7430555555555556, 0.31283068783068785, 0.4075, 0.875, 0.8455244062251422, 0.0, 1.3416250945426158, 0.5999999999999999, 1.0, 0.0026871410482505616], 
reward next is 0.9473, 
noisyNet noise sample is [array([-1.230375], dtype=float32), 1.2137812]. 
=============================================
[2019-05-04 22:24:07,007] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4696550e-18 1.0000000e+00 3.6131729e-18 1.7831993e-19 2.8345770e-23
 1.5305956e-27 2.3090225e-19 1.5233458e-24 6.9402759e-19 2.0390416e-20
 6.6287496e-12], sum to 1.0000
[2019-05-04 22:24:07,011] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7564
[2019-05-04 22:24:07,041] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 56.0, 7.95, 220.0, 258.25, 497.5, 25.0, 0.8638824727677608, 20.0, 26.79178036620862, 22.2, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.5, 20.0], 
sim time this is 5827500.0000, 
sim time next is 5828400.0000, 
raw observation next is [21.0, 56.0, 7.7, 220.0, 283.5, 370.0, 25.5, 0.7531698503073901, 20.0, 26.90411787369769, 22.2, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8717948717948718, 0.56, 0.7000000000000001, 0.6111111111111112, 0.75, 0.37, 0.925, 0.75105661676913, 0.0, 1.27201683909967, 0.5999999999999999, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([1.8860415], dtype=float32), -0.3600761]. 
=============================================
[2019-05-04 22:24:09,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1008578e-20 1.0000000e+00 7.5361851e-19 1.0119125e-20 7.4410932e-25
 3.1254848e-26 3.6538083e-20 5.1340405e-26 4.7525727e-21 4.5080833e-20
 3.9309693e-13], sum to 1.0000
[2019-05-04 22:24:09,215] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0014
[2019-05-04 22:24:09,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9561851e-19 1.0000000e+00 6.2044233e-18 1.5313551e-19 3.5097668e-24
 6.9492482e-29 9.7024015e-22 7.1043434e-27 3.8797544e-19 6.6848198e-19
 1.1614467e-14], sum to 1.0000
[2019-05-04 22:24:09,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6970
[2019-05-04 22:24:09,241] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.5, 55.0, 4.6, 195.0, 0.0, 0.0, 21.25, 0.5677164625333346, 20.0, 25.3715849164993, 19.4, 0.0, 0.0], 
current ob forecast is [], 
actual action is [21.5, 20.0], 
sim time this is 5808600.0000, 
sim time next is 5809500.0000, 
raw observation next is [16.75, 55.0, 4.85, 197.5, 0.0, 0.0, 21.5, 0.5621273909122274, 20.0, 25.35216832026533, 19.4, 0.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.7628205128205128, 0.55, 0.44090909090909086, 0.5486111111111112, 0.0, 0.0, 0.8583333333333333, 0.6873757969707425, 0.0, 1.0503097600379045, 0.1999999999999998, 0.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-0.16968022], dtype=float32), 0.117787026]. 
=============================================
[2019-05-04 22:24:09,245] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 56.0, 8.45, 220.0, 232.75, 597.75, 24.0, 0.7538952902672532, 20.0, 26.70186224064643, 22.2, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5, 20.0], 
sim time this is 5825700.0000, 
sim time next is 5826600.0000, 
raw observation next is [20.0, 56.0, 8.2, 220.0, 233.0, 625.0, 24.5, 0.8354632453376021, 20.0, 26.57393523539649, 22.2, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8461538461538461, 0.56, 0.7454545454545454, 0.6111111111111112, 0.6164021164021164, 0.625, 0.9083333333333333, 0.7784877484458673, 0.0, 1.2248478907709273, 0.5999999999999999, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([1.3453207], dtype=float32), 1.8766531]. 
=============================================
[2019-05-04 22:24:09,269] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.89498 ]
 [83.78103 ]
 [83.798676]
 [83.87984 ]
 [83.893875]], R is [[84.18228149]
 [84.29045868]
 [84.39755249]
 [84.50357819]
 [84.6085434 ]].
[2019-05-04 22:24:12,868] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.3294501e-13 1.0000000e+00 4.2992571e-12 5.3684061e-14 3.4527958e-17
 6.6976587e-19 1.9503025e-14 1.8950460e-17 7.8666528e-13 4.7943617e-14
 1.8951575e-10], sum to 1.0000
[2019-05-04 22:24:12,869] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0684
[2019-05-04 22:24:12,887] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.25, 57.75, 4.199999999999999, 320.0, 0.0, 0.0, 5.5, -0.7404360827804681, 20.0, 20.74505120043701, 19.4, 0.0, 5.731739313247961], 
current ob forecast is [], 
actual action is [5.25, 20.0], 
sim time this is 6119100.0000, 
sim time next is 6120000.0000, 
raw observation next is [0.0, 60.0, 3.9, 320.0, 0.0, 0.0, 5.25, -0.7540571443819047, 20.0, 20.69114154606145, 19.4, 0.0, 5.985248190532762], 
processed observation next is [0.0, 0.8695652173913043, 0.3333333333333333, 0.6, 0.35454545454545455, 0.8888888888888888, 0.0, 0.0, 0.5875, 0.2486476185393651, 0.0, 0.38444879229449264, 0.1999999999999998, 0.0, 0.07041468459450309], 
reward next is 0.8796, 
noisyNet noise sample is [array([1.0028342], dtype=float32), 2.625099]. 
=============================================
[2019-05-04 22:24:12,910] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[54.903328]
 [56.429237]
 [58.034733]
 [57.224724]
 [57.09719 ]], R is [[55.18351746]
 [55.51425171]
 [55.84510803]
 [56.17659378]
 [55.61483002]].
[2019-05-04 22:24:13,224] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1064348e-12 9.9983895e-01 3.7449099e-11 7.4734442e-14 3.0146597e-17
 6.5719907e-20 1.0001494e-12 7.9120545e-18 2.1389571e-14 2.7297736e-14
 1.6100235e-04], sum to 1.0000
[2019-05-04 22:24:13,229] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0785
[2019-05-04 22:24:13,249] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.25, 91.5, 3.35, 360.0, 83.0, 0.0, 6.0, -0.6830362052839716, 20.0, 20.62092873075872, 20.56, 1.0, 13.003069400987167], 
current ob forecast is [], 
actual action is [6.25, 20.0], 
sim time this is 5994900.0000, 
sim time next is 5995800.0000, 
raw observation next is [1.5, 90.0, 3.1, 360.0, 79.0, 0.0, 6.25, -0.6834530765781762, 20.0, 20.65992279081882, 20.56, 1.0, 12.911235434772886], 
processed observation next is [0.0, 0.391304347826087, 0.3717948717948718, 0.9, 0.2818181818181818, 1.0, 0.20899470899470898, 0.0, 0.6041666666666666, 0.2721823078072746, 0.0, 0.379988970116974, 0.36571428571428555, 1.0, 0.1518968874679163], 
reward next is 0.1419, 
noisyNet noise sample is [array([0.05683555], dtype=float32), 0.8212919]. 
=============================================
[2019-05-04 22:24:13,410] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5584895e-13 9.9986994e-01 5.1695161e-13 6.8512915e-13 2.5478728e-18
 6.6383105e-20 6.2473461e-15 1.5566867e-19 2.9273380e-13 2.0575899e-13
 1.3009273e-04], sum to 1.0000
[2019-05-04 22:24:13,411] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0350
[2019-05-04 22:24:13,439] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.25, 98.25, 2.725, 345.0, 113.0, 0.0, 7.0, -0.721003238089969, 20.0, 20.54735963363562, 20.56, 1.0, 12.11122537923368], 
current ob forecast is [], 
actual action is [7.25, 20.0], 
sim time this is 6002100.0000, 
sim time next is 6003000.0000, 
raw observation next is [2.5, 96.5, 2.85, 340.0, 117.0, 0.0, 7.25, -0.7257869416070237, 20.0, 20.52937675464091, 20.56, 1.0, 12.018466940120708], 
processed observation next is [0.0, 0.4782608695652174, 0.3974358974358974, 0.965, 0.2590909090909091, 0.9444444444444444, 0.30952380952380953, 0.0, 0.6208333333333333, 0.2580710194643254, 0.0, 0.36133953637727295, 0.36571428571428555, 1.0, 0.14139372870730246], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.57661706], dtype=float32), 0.8859625]. 
=============================================
[2019-05-04 22:24:13,459] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.111626]
 [72.92275 ]
 [72.56216 ]
 [72.20234 ]
 [71.94109 ]], R is [[72.54569244]
 [71.82023621]
 [71.10203552]
 [70.3910141 ]
 [69.68710327]].
[2019-05-04 22:24:14,967] A3C_AGENT_WORKER-Thread-6 INFO:Local step 15500, global step 246434: loss 2.5058
[2019-05-04 22:24:14,968] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 15500, global step 246434: learning rate 0.0001
[2019-05-04 22:24:16,140] A3C_AGENT_WORKER-Thread-7 INFO:Local step 15500, global step 246688: loss 1.5928
[2019-05-04 22:24:16,141] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 15500, global step 246688: learning rate 0.0001
[2019-05-04 22:24:16,188] A3C_AGENT_WORKER-Thread-12 INFO:Local step 15500, global step 246698: loss 1.6931
[2019-05-04 22:24:16,189] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 15500, global step 246698: learning rate 0.0001
[2019-05-04 22:24:16,280] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.9642617e-16 9.9995637e-01 2.6163477e-14 1.9453561e-16 2.0258734e-20
 3.4058855e-21 1.7100319e-16 3.6772359e-21 1.5227162e-16 5.3867077e-17
 4.3616958e-05], sum to 1.0000
[2019-05-04 22:24:16,281] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7751
[2019-05-04 22:24:16,314] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.5, 47.5, 4.125, 262.5, 164.25, 811.5, 10.0, -0.6071124977972222, 20.0, 21.46041052915452, 22.2, 1.0, 6.589443764652873], 
current ob forecast is [], 
actual action is [10.5, 20.0], 
sim time this is 6176700.0000, 
sim time next is 6177600.0000, 
raw observation next is [6.0, 47.0, 3.6, 270.0, 161.5, 815.0, 10.5, -0.5914392413400665, 20.0, 21.51082610923625, 22.2, 1.0, 5.481477543277902], 
processed observation next is [1.0, 0.5217391304347826, 0.48717948717948717, 0.47, 0.32727272727272727, 0.75, 0.42724867724867727, 0.815, 0.675, 0.3028535862199778, 0.0, 0.5015465870337498, 0.5999999999999999, 1.0, 0.06448797109738708], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.48256743], dtype=float32), -0.0023678935]. 
=============================================
[2019-05-04 22:24:16,484] A3C_AGENT_WORKER-Thread-5 INFO:Local step 15500, global step 246772: loss 0.2711
[2019-05-04 22:24:16,485] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 15500, global step 246772: learning rate 0.0001
[2019-05-04 22:24:16,586] A3C_AGENT_WORKER-Thread-3 INFO:Local step 15500, global step 246789: loss 0.1237
[2019-05-04 22:24:16,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 15500, global step 246789: learning rate 0.0001
[2019-05-04 22:24:17,422] A3C_AGENT_WORKER-Thread-2 INFO:Local step 15500, global step 246927: loss 0.7444
[2019-05-04 22:24:17,423] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 15500, global step 246927: learning rate 0.0001
[2019-05-04 22:24:21,337] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.5103666e-16 9.9999630e-01 1.4216384e-16 5.1145338e-18 1.7849313e-23
 7.3843266e-29 4.1766205e-19 6.8640556e-26 5.4337670e-19 8.3239014e-19
 3.6993563e-06], sum to 1.0000
[2019-05-04 22:24:21,338] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6090
[2019-05-04 22:24:21,366] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.75, 54.5, 4.5, 197.5, 168.0, 792.25, 14.0, -0.3264388936705481, 20.0, 22.54287475163891, 22.2, 1.0, 9.515494796567312], 
current ob forecast is [], 
actual action is [14.75, 20.0], 
sim time this is 6261300.0000, 
sim time next is 6262200.0000, 
raw observation next is [10.5, 52.0, 5.4, 205.0, 144.0, 844.0, 14.75, -0.3084715132428702, 20.0, 22.6053738180032, 22.2, 1.0, 7.243110087699691], 
processed observation next is [1.0, 0.4782608695652174, 0.6025641025641025, 0.52, 0.49090909090909096, 0.5694444444444444, 0.38095238095238093, 0.844, 0.7458333333333333, 0.3971761622523766, 0.0, 0.6579105454290287, 0.5999999999999999, 1.0, 0.08521305985529048], 
reward next is 0.8648, 
noisyNet noise sample is [array([0.23707007], dtype=float32), -0.94206125]. 
=============================================
[2019-05-04 22:24:21,733] A3C_AGENT_WORKER-Thread-15 INFO:Local step 15500, global step 248163: loss 2.1705
[2019-05-04 22:24:21,733] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 15500, global step 248163: learning rate 0.0001
[2019-05-04 22:24:21,790] A3C_AGENT_WORKER-Thread-14 INFO:Local step 15500, global step 248174: loss 0.2253
[2019-05-04 22:24:21,791] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 15500, global step 248174: learning rate 0.0001
[2019-05-04 22:24:21,822] A3C_AGENT_WORKER-Thread-18 INFO:Local step 15500, global step 248180: loss 0.3260
[2019-05-04 22:24:21,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 15500, global step 248180: learning rate 0.0001
[2019-05-04 22:24:21,929] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.3867886e-18 9.9999940e-01 2.3044137e-16 5.0179030e-18 1.1847812e-24
 1.5436234e-27 3.4993193e-19 1.5620309e-23 2.5239206e-18 1.6485220e-19
 5.5478000e-07], sum to 1.0000
[2019-05-04 22:24:21,929] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9357
[2019-05-04 22:24:22,061] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 56.0, 4.6, 220.0, 119.5, 668.5, 5.25, -0.7285989814039282, 65.0, 21.02938855287981, 22.2, 1.0, 73.77496805236929], 
current ob forecast is [], 
actual action is [6.0, 20.0], 
sim time this is 6166800.0000, 
sim time next is 6167700.0000, 
raw observation next is [1.25, 55.0, 4.725, 225.0, 124.25, 699.25, 6.0, -0.6758664577164731, 20.0, 21.45196344858496, 22.2, 1.0, 40.259306707467836], 
processed observation next is [1.0, 0.391304347826087, 0.36538461538461536, 0.55, 0.4295454545454545, 0.625, 0.3287037037037037, 0.69925, 0.6, 0.2747111807611756, 0.0, 0.49313763551213696, 0.5999999999999999, 1.0, 0.4736389024407981], 
reward next is 0.6273, 
noisyNet noise sample is [array([0.7467407], dtype=float32), -2.548526]. 
=============================================
[2019-05-04 22:24:22,272] A3C_AGENT_WORKER-Thread-13 INFO:Local step 15500, global step 248288: loss 0.1313
[2019-05-04 22:24:22,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 15500, global step 248288: learning rate 0.0001
[2019-05-04 22:24:22,910] A3C_AGENT_WORKER-Thread-8 INFO:Local step 15500, global step 248483: loss 0.0241
[2019-05-04 22:24:22,911] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 15500, global step 248483: learning rate 0.0001
[2019-05-04 22:24:24,999] A3C_AGENT_WORKER-Thread-19 INFO:Local step 15500, global step 249275: loss 0.1988
[2019-05-04 22:24:25,002] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 15500, global step 249275: learning rate 0.0001
[2019-05-04 22:24:26,455] A3C_AGENT_WORKER-Thread-4 INFO:Local step 15500, global step 249714: loss 0.1395
[2019-05-04 22:24:26,455] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 15500, global step 249714: learning rate 0.0001
[2019-05-04 22:24:27,207] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-05-04 22:24:27,207] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation job starts!
[2019-05-04 22:24:27,207] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation job starts!
[2019-05-04 22:24:27,208] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:24:27,208] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:24:27,211] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res1/Eplus-env-sub_run6
[2019-05-04 22:24:27,234] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Test-Repeat-Real97-v1-res1/Eplus-env-sub_run6
[2019-05-04 22:26:01,257] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation: average rewards by now are 4595.7612 73651.5758 272758.3191
[2019-05-04 22:26:01,269] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:26:01,269] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:26:01,269] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:26:01,269] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:26:01,269] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:26:01,269] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:26:01,383] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:26:01,383] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:26:01,383] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:26:01,383] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:26:01,383] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:26:01,383] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:26:18,125] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:NoisyNet noise sample: [array([0.01660401], dtype=float32), 0.16768068]
[2019-05-04 22:26:18,125] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:Observation this: [-1.0, 81.5, 7.449999999999999, 260.0, 0.0, 0.0, 4.0, -0.6881250284802524, 20.0, 20.63168189699233, 21.5, 0.0, 10.984763604890666]
[2019-05-04 22:26:18,125] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:Observation forecast: []
[2019-05-04 22:26:18,127] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Softmax [5.5140382e-17 1.0000000e+00 1.6200911e-16 7.7859798e-18 2.2590868e-22
 2.4332215e-25 1.2467419e-18 9.8000727e-24 5.2735254e-18 1.3468673e-18
 1.2288165e-10], sampled 0.34887473559466464
[2019-05-04 22:26:32,563] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:NoisyNet noise sample: [array([0.01660401], dtype=float32), 0.16768068]
[2019-05-04 22:26:32,564] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:Observation this: [1.75, 70.0, 5.825, 217.5, 0.0, 0.0, 7.0, -0.5494310893665406, 20.0, 21.55535437570521, 22.0, 1.0, 40.260370931676846]
[2019-05-04 22:26:32,564] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 DEBUG:Observation forecast: []
[2019-05-04 22:26:32,566] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Softmax [1.11858917e-15 1.00000000e+00 3.46451553e-15 1.56127495e-16
 7.55292801e-21 1.14610745e-23 2.90509537e-17 3.87290214e-22
 1.20660704e-16 3.04910487e-17 1.13723608e-08], sampled 0.4624858759620579
[2019-05-04 22:27:56,803] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation: average rewards by now are 5702.3178 151104.8563 -2447.6395
[2019-05-04 22:27:56,814] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:27:56,814] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:27:56,814] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:27:56,814] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:27:56,814] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:27:56,814] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:27:56,932] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:27:56,932] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:27:56,932] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:27:56,932] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:27:56,932] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:27:56,932] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:27:57,817] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 250000, evaluation results [250000.0, 5702.317793996956, 151104.85630998635, -2447.6395374913786, 4595.761158783212, 73651.57584369082, 272758.3191094142]
[2019-05-04 22:27:58,150] A3C_AGENT_WORKER-Thread-17 INFO:Local step 15500, global step 250127: loss 0.0929
[2019-05-04 22:27:58,153] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 15500, global step 250129: learning rate 0.0001
[2019-05-04 22:27:58,900] A3C_AGENT_WORKER-Thread-11 INFO:Local step 15500, global step 250431: loss 1.9997
[2019-05-04 22:27:58,906] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 15500, global step 250431: learning rate 0.0001
[2019-05-04 22:27:59,695] A3C_AGENT_WORKER-Thread-16 INFO:Local step 15500, global step 250771: loss -0.2883
[2019-05-04 22:27:59,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 15500, global step 250771: learning rate 0.0001
[2019-05-04 22:28:06,871] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5638187e-14 1.0000000e+00 3.4915298e-14 1.3529180e-14 1.3588595e-20
 1.3555764e-21 1.3219211e-17 3.8534221e-21 1.5614060e-16 7.9953505e-17
 5.3648624e-11], sum to 1.0000
[2019-05-04 22:28:06,872] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5946
[2019-05-04 22:28:06,894] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.75, 100.0, 6.45, 330.0, 0.0, 0.0, 4.5, -0.5740767398766754, 20.0, 20.65845875387443, 19.4, 0.0, 8.648871121931345], 
current ob forecast is [], 
actual action is [4.25, 20.0], 
sim time this is 6493500.0000, 
sim time next is 6494400.0000, 
raw observation next is [-1.0, 100.0, 6.2, 330.0, 0.0, 0.0, 4.25, -0.5895733213819703, 20.0, 20.5994099091346, 19.4, 0.0, 8.684024384673998], 
processed observation next is [1.0, 0.17391304347826086, 0.3076923076923077, 1.0, 0.5636363636363636, 0.9166666666666666, 0.0, 0.0, 0.5708333333333333, 0.30347555953934324, 0.0, 0.3713442727335143, 0.1999999999999998, 0.0, 0.10216499276087057], 
reward next is 0.8478, 
noisyNet noise sample is [array([0.17014141], dtype=float32), 1.5090067]. 
=============================================
[2019-05-04 22:28:08,195] A3C_AGENT_WORKER-Thread-6 INFO:Local step 16000, global step 253919: loss 0.6344
[2019-05-04 22:28:08,197] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 16000, global step 253919: learning rate 0.0001
[2019-05-04 22:28:08,705] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.26037752e-22 1.00000000e+00 3.84982045e-22 2.94148435e-23
 2.41210977e-28 1.05391645e-30 1.96639989e-23 6.04761563e-29
 1.95010859e-22 5.67433897e-23 8.79099100e-18], sum to 1.0000
[2019-05-04 22:28:08,707] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6742
[2019-05-04 22:28:08,720] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.25, 33.5, 6.45, 222.5, 48.75, 181.5, 23.5, 0.2338658631837391, 20.0, 24.32194316967445, 22.2, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.25, 20.0], 
sim time this is 6371100.0000, 
sim time next is 6372000.0000, 
raw observation next is [18.0, 34.0, 5.7, 220.0, 32.5, 121.0, 23.25, 0.2369349960410009, 20.0, 24.35313608372954, 22.2, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7948717948717948, 0.34, 0.5181818181818182, 0.6111111111111112, 0.08597883597883597, 0.121, 0.8875, 0.578978332013667, 0.0, 0.9075908691042203, 0.5999999999999999, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([1.3056363], dtype=float32), 2.0540445]. 
=============================================
[2019-05-04 22:28:08,754] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[84.71649 ]
 [84.689926]
 [84.68457 ]
 [84.65315 ]
 [84.97554 ]], R is [[84.88049316]
 [84.98168945]
 [85.08187103]
 [85.18105316]
 [85.27924347]].
[2019-05-04 22:28:08,822] A3C_AGENT_WORKER-Thread-7 INFO:Local step 16000, global step 254142: loss 0.9665
[2019-05-04 22:28:08,825] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 16000, global step 254142: learning rate 0.0001
[2019-05-04 22:28:09,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.36125851e-16 1.00000000e+00 1.44184680e-15 3.76646369e-17
 3.37933621e-20 9.94862662e-25 1.08675704e-16 2.94202618e-21
 3.06501155e-17 2.96010023e-17 5.62209676e-13], sum to 1.0000
[2019-05-04 22:28:09,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9463
[2019-05-04 22:28:09,129] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 52.0, 5.7, 340.0, 128.0, 772.5, 6.75, -0.2881033305551662, 20.0, 22.37516171355175, 22.2, 1.0, 1.2414874647085299], 
current ob forecast is [], 
actual action is [7.0, 20.0], 
sim time this is 6534000.0000, 
sim time next is 6534900.0000, 
raw observation next is [2.0, 51.0, 5.550000000000001, 340.0, 124.0, 757.25, 7.0, -0.2541562263102567, 20.0, 22.60004153557455, 22.2, 1.0, 1.1379560832213442], 
processed observation next is [1.0, 0.6521739130434783, 0.38461538461538464, 0.51, 0.5045454545454546, 0.9444444444444444, 0.328042328042328, 0.75725, 0.6166666666666667, 0.41528125789658105, 0.0, 0.6571487907963645, 0.5999999999999999, 1.0, 0.013387718626133461], 
reward next is 0.9366, 
noisyNet noise sample is [array([1.2230365], dtype=float32), -1.4744841]. 
=============================================
[2019-05-04 22:28:09,533] A3C_AGENT_WORKER-Thread-2 INFO:Local step 16000, global step 254427: loss 2.5000
[2019-05-04 22:28:09,543] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 16000, global step 254427: learning rate 0.0001
[2019-05-04 22:28:10,090] A3C_AGENT_WORKER-Thread-12 INFO:Local step 16000, global step 254685: loss 0.1788
[2019-05-04 22:28:10,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 16000, global step 254685: learning rate 0.0001
[2019-05-04 22:28:10,126] A3C_AGENT_WORKER-Thread-3 INFO:Local step 16000, global step 254705: loss 2.2183
[2019-05-04 22:28:10,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 16000, global step 254705: learning rate 0.0001
[2019-05-04 22:28:10,146] A3C_AGENT_WORKER-Thread-5 INFO:Local step 16000, global step 254713: loss 1.4264
[2019-05-04 22:28:10,151] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 16000, global step 254713: learning rate 0.0001
[2019-05-04 22:28:10,333] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9225555e-26 1.0000000e+00 1.1751405e-24 4.7500048e-26 2.1175148e-32
 8.5961841e-36 1.9624169e-27 4.4190704e-34 2.4504035e-28 4.4154968e-27
 1.5470187e-21], sum to 1.0000
[2019-05-04 22:28:10,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8945
[2019-05-04 22:28:10,354] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.5, 59.5, 6.2, 210.0, 0.0, 0.0, 18.75, -0.2328911418754943, 20.0, 22.43413919305187, 19.4, 0.0, 1.5741549937202226], 
current ob forecast is [], 
actual action is [18.5, 20.0], 
sim time this is 6413400.0000, 
sim time next is 6414300.0000, 
raw observation next is [13.25, 65.75, 6.2, 210.0, 0.0, 0.0, 18.5, -0.2390128540521011, 20.0, 22.40375934745044, 19.4, 0.0, 1.618697411072727], 
processed observation next is [1.0, 0.21739130434782608, 0.6730769230769231, 0.6575, 0.5636363636363636, 0.5833333333333334, 0.0, 0.0, 0.8083333333333333, 0.42032904864929965, 0.0, 0.6291084782072056, 0.1999999999999998, 0.0, 0.019043498953796787], 
reward next is 0.9310, 
noisyNet noise sample is [array([-0.39473033], dtype=float32), 0.5636344]. 
=============================================
[2019-05-04 22:28:10,827] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.2469310e-17 1.0000000e+00 3.8100512e-15 9.2519417e-17 1.0868755e-19
 2.9069948e-22 1.1206038e-17 2.8128338e-20 4.6132035e-16 4.6944366e-17
 3.2222974e-13], sum to 1.0000
[2019-05-04 22:28:10,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1800
[2019-05-04 22:28:10,851] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.75, 41.75, 6.175000000000001, 130.0, 0.0, 0.0, 8.0, -0.7608995404483404, 20.0, 20.77464622958156, 19.4, 0.0, 5.123769122102683], 
current ob forecast is [], 
actual action is [7.75, 20.0], 
sim time this is 6639300.0000, 
sim time next is 6640200.0000, 
raw observation next is [2.5, 42.5, 5.65, 130.0, 0.0, 0.0, 7.75, -0.7723403026075379, 20.0, 20.72664668735598, 19.4, 0.0, 5.430965480060578], 
processed observation next is [0.0, 0.8695652173913043, 0.3974358974358974, 0.425, 0.5136363636363637, 0.3611111111111111, 0.0, 0.0, 0.6291666666666667, 0.24255323246415403, 0.0, 0.38952095533656866, 0.1999999999999998, 0.0, 0.06389371153012445], 
reward next is 0.8861, 
noisyNet noise sample is [array([0.3280378], dtype=float32), 1.1370277]. 
=============================================
[2019-05-04 22:28:11,311] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0068240e-18 1.0000000e+00 4.1236823e-18 1.8120389e-21 1.3972282e-25
 2.6217386e-28 1.2419891e-20 8.8258331e-26 6.1764982e-22 1.5693327e-20
 9.5350583e-13], sum to 1.0000
[2019-05-04 22:28:11,317] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9465
[2019-05-04 22:28:11,338] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.5, 96.5, 5.7, 350.0, 0.0, 0.0, 12.75, -0.1695098110560686, 20.0, 22.14112691871461, 22.2, 1.0, 7.585996021591484], 
current ob forecast is [], 
actual action is [12.5, 20.0], 
sim time this is 6463800.0000, 
sim time next is 6464700.0000, 
raw observation next is [7.25, 98.25, 5.7, 350.0, 0.0, 0.0, 12.5, -0.1786080263572375, 20.0, 22.09901584314831, 22.2, 1.0, 7.6786025445417225], 
processed observation next is [1.0, 0.8260869565217391, 0.5192307692307693, 0.9825, 0.5181818181818182, 0.9722222222222222, 0.0, 0.0, 0.7083333333333334, 0.4404639912142542, 0.0, 0.5855736918783302, 0.5999999999999999, 1.0, 0.09033650052402027], 
reward next is 0.8597, 
noisyNet noise sample is [array([-0.2930334], dtype=float32), -0.3672531]. 
=============================================
[2019-05-04 22:28:12,413] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.2605033e-07 9.0302914e-01 7.6083359e-07 4.6289763e-08 7.7581186e-10
 5.9366982e-12 9.7729771e-09 2.6273311e-10 1.7476739e-07 4.5284438e-08
 9.6968971e-02], sum to 1.0000
[2019-05-04 22:28:12,425] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5414
[2019-05-04 22:28:12,474] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.75, 64.0, 3.1, 77.5, 0.0, 0.0, -1.5, -1.157669884082653, 20.0, 18.78111918422723, 19.4, 0.0, 10.12681130090013], 
current ob forecast is [], 
actual action is [-1.75, 20.0], 
sim time this is 6583500.0000, 
sim time next is 6584400.0000, 
raw observation next is [-7.0, 64.0, 3.1, 80.0, 0.0, 0.0, -1.75, -1.176184513255003, 20.0, 18.70715716715631, 19.4, 0.0, 10.22357055296937], 
processed observation next is [0.0, 0.21739130434782608, 0.15384615384615385, 0.64, 0.2818181818181818, 0.2222222222222222, 0.0, 0.0, 0.4708333333333333, 0.10793849558166564, 0.0, 0.10102245245090151, 0.1999999999999998, 0.0, 0.12027730062316906], 
reward next is 0.5864, 
noisyNet noise sample is [array([-0.74841213], dtype=float32), -0.61979944]. 
=============================================
[2019-05-04 22:28:14,758] A3C_AGENT_WORKER-Thread-15 INFO:Local step 16000, global step 255912: loss 0.4782
[2019-05-04 22:28:14,759] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 16000, global step 255912: learning rate 0.0001
[2019-05-04 22:28:15,009] A3C_AGENT_WORKER-Thread-14 INFO:Local step 16000, global step 255979: loss 0.2386
[2019-05-04 22:28:15,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 16000, global step 255983: learning rate 0.0001
[2019-05-04 22:28:15,022] A3C_AGENT_WORKER-Thread-13 INFO:Local step 16000, global step 255983: loss 0.1858
[2019-05-04 22:28:15,023] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 16000, global step 255983: learning rate 0.0001
[2019-05-04 22:28:15,069] A3C_AGENT_WORKER-Thread-18 INFO:Local step 16000, global step 255996: loss 0.0439
[2019-05-04 22:28:15,070] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 16000, global step 255996: learning rate 0.0001
[2019-05-04 22:28:16,517] A3C_AGENT_WORKER-Thread-8 INFO:Local step 16000, global step 256492: loss 0.1689
[2019-05-04 22:28:16,520] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 16000, global step 256492: learning rate 0.0001
[2019-05-04 22:28:17,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0592846e-16 1.0000000e+00 8.4010991e-19 7.2424173e-20 2.8493864e-23
 3.2684795e-25 1.6408812e-19 2.2666441e-26 6.1292332e-20 6.0542016e-19
 1.3035012e-13], sum to 1.0000
[2019-05-04 22:28:17,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3265
[2019-05-04 22:28:17,507] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.0, 70.0, 5.7, 120.0, 0.0, 0.0, 11.0, -0.7453427412173186, 20.0, 20.7138207008992, 19.4, 0.0, 5.87230999982247], 
current ob forecast is [], 
actual action is [11.0, 20.0], 
sim time this is 6753600.0000, 
sim time next is 6754500.0000, 
raw observation next is [6.0, 70.0, 5.825, 120.0, 0.0, 0.0, 11.0, -0.7527852430081096, 20.0, 20.67876947641049, 19.4, 0.0, 5.896915611690747], 
processed observation next is [1.0, 0.17391304347826086, 0.48717948717948717, 0.7, 0.5295454545454545, 0.3333333333333333, 0.0, 0.0, 0.6833333333333333, 0.24907158566396348, 0.0, 0.3826813537729272, 0.1999999999999998, 0.0, 0.06937547778459702], 
reward next is 0.8806, 
noisyNet noise sample is [array([1.7393299], dtype=float32), 1.0159554]. 
=============================================
[2019-05-04 22:28:17,524] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.77568 ]
 [86.91493 ]
 [86.91065 ]
 [86.92427 ]
 [86.785706]], R is [[86.70541382]
 [86.71927643]
 [86.73326111]
 [86.74734497]
 [86.76148987]].
[2019-05-04 22:28:18,655] A3C_AGENT_WORKER-Thread-19 INFO:Local step 16000, global step 257270: loss 0.0913
[2019-05-04 22:28:18,656] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 16000, global step 257270: learning rate 0.0001
[2019-05-04 22:28:19,927] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.6264379e-18 1.0000000e+00 3.9083664e-18 3.5794778e-19 4.2243615e-23
 2.9056237e-27 9.5955859e-22 2.9192685e-26 6.7977328e-20 4.5917526e-22
 5.8783978e-11], sum to 1.0000
[2019-05-04 22:28:19,938] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5016
[2019-05-04 22:28:19,954] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.0, 75.0, 3.6, 100.0, 0.0, 0.0, 9.0, -0.8050057370117566, 20.0, 20.19023175374449, 19.4, 0.0, 12.098296558372823], 
current ob forecast is [], 
actual action is [9.0, 20.0], 
sim time this is 6825600.0000, 
sim time next is 6826500.0000, 
raw observation next is [3.75, 74.75, 3.6, 100.0, 0.0, 0.0, 9.0, -0.8146609038854692, 20.0, 20.14016402714151, 19.4, 0.0, 12.182708517624214], 
processed observation next is [1.0, 0.0, 0.42948717948717946, 0.7475, 0.32727272727272727, 0.2777777777777778, 0.0, 0.0, 0.65, 0.22844636537151028, 0.0, 0.3057377181630727, 0.1999999999999998, 0.0, 0.14332598256028486], 
reward next is 0.8067, 
noisyNet noise sample is [array([0.5343691], dtype=float32), -1.0209475]. 
=============================================
[2019-05-04 22:28:19,972] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[90.44484 ]
 [90.495255]
 [90.2284  ]
 [90.052155]
 [89.92895 ]], R is [[89.93661499]
 [89.8449173 ]
 [89.76141357]
 [89.72488403]
 [89.71121979]].
[2019-05-04 22:28:19,998] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8303352e-17 1.0000000e+00 1.6023042e-16 1.1996806e-17 3.7918411e-22
 7.1342479e-26 1.6884297e-17 3.1852367e-24 3.9829594e-18 5.7822510e-19
 1.7759283e-12], sum to 1.0000
[2019-05-04 22:28:19,999] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1286
[2019-05-04 22:28:20,022] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.75, 49.75, 4.85, 130.0, 0.0, 0.0, 8.0, -0.9795905910054775, 20.0, 19.87870919772654, 19.4, 0.0, 7.359813240614499], 
current ob forecast is [], 
actual action is [7.75, 20.0], 
sim time this is 6660900.0000, 
sim time next is 6661800.0000, 
raw observation next is [2.5, 50.5, 4.6, 130.0, 0.0, 0.0, 7.75, -0.9906771158276104, 20.0, 19.83105415952727, 19.4, 0.0, 7.378533476129046], 
processed observation next is [0.0, 0.08695652173913043, 0.3974358974358974, 0.505, 0.41818181818181815, 0.3611111111111111, 0.0, 0.0, 0.6291666666666667, 0.16977429472412986, 0.0, 0.2615791656467528, 0.1999999999999998, 0.0, 0.08680627618975349], 
reward next is 0.8632, 
noisyNet noise sample is [array([1.678265], dtype=float32), 0.2838582]. 
=============================================
[2019-05-04 22:28:20,284] A3C_AGENT_WORKER-Thread-4 INFO:Local step 16000, global step 257808: loss 0.3929
[2019-05-04 22:28:20,285] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 16000, global step 257808: learning rate 0.0001
[2019-05-04 22:28:20,679] A3C_AGENT_WORKER-Thread-17 INFO:Local step 16000, global step 257939: loss 0.3897
[2019-05-04 22:28:20,680] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 16000, global step 257939: learning rate 0.0001
[2019-05-04 22:28:22,101] A3C_AGENT_WORKER-Thread-16 INFO:Local step 16000, global step 258411: loss 0.2197
[2019-05-04 22:28:22,104] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 16000, global step 258411: learning rate 0.0001
[2019-05-04 22:28:22,165] A3C_AGENT_WORKER-Thread-11 INFO:Local step 16000, global step 258434: loss 0.4208
[2019-05-04 22:28:22,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 16000, global step 258434: learning rate 0.0001
[2019-05-04 22:28:26,240] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.7918735e-17 9.9997926e-01 5.2088294e-14 1.8793283e-16 4.4381822e-21
 4.0927117e-24 5.8894520e-19 1.5228226e-21 3.2959217e-16 1.3614580e-18
 2.0702735e-05], sum to 1.0000
[2019-05-04 22:28:26,240] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2738
[2019-05-04 22:28:26,266] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 87.0, 2.1, 60.0, 0.0, 0.0, 8.0, -0.8795719608084044, 20.0, 19.93052620707757, 19.4, 0.0, 10.862255849712822], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 6840000.0000, 
sim time next is 6840900.0000, 
raw observation next is [3.0, 87.0, 1.95, 57.5, 0.0, 0.0, 8.0, -0.893654757135681, 20.0, 19.84993310185426, 19.4, 0.0, 12.209927015569157], 
processed observation next is [1.0, 0.17391304347826086, 0.41025641025641024, 0.87, 0.17727272727272728, 0.1597222222222222, 0.0, 0.0, 0.6333333333333333, 0.202115080954773, 0.0, 0.2642761574077512, 0.1999999999999998, 0.0, 0.14364620018316657], 
reward next is 0.8064, 
noisyNet noise sample is [array([0.35221776], dtype=float32), -1.6285803]. 
=============================================
[2019-05-04 22:28:28,133] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1109016e-16 1.0000000e+00 1.1877087e-18 1.2449082e-17 1.0937324e-22
 8.0416548e-26 1.2645265e-18 2.5422726e-25 1.7154364e-18 8.5003556e-19
 2.9825761e-10], sum to 1.0000
[2019-05-04 22:28:28,133] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9215
[2019-05-04 22:28:28,145] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 81.0, 2.55, 70.0, 0.0, 0.0, 8.0, -0.6807584313709413, 20.0, 20.60292705741038, 19.4, 0.0, 4.540558515089675], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 6913800.0000, 
sim time next is 6914700.0000, 
raw observation next is [3.0, 81.0, 3.075, 75.0, 0.0, 0.0, 8.0, -0.691610288675168, 20.0, 20.56312086557162, 19.4, 0.0, 4.714252129295203], 
processed observation next is [1.0, 0.0, 0.41025641025641024, 0.81, 0.27954545454545454, 0.20833333333333334, 0.0, 0.0, 0.6333333333333333, 0.2694632371082773, 0.0, 0.3661601236530885, 0.1999999999999998, 0.0, 0.05546178975641415], 
reward next is 0.8945, 
noisyNet noise sample is [array([-0.12973416], dtype=float32), -1.4927144]. 
=============================================
[2019-05-04 22:28:30,623] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.8210983e-16 9.9997890e-01 7.5676802e-13 7.5721701e-17 1.0145232e-20
 2.4266537e-25 5.9399029e-18 8.7207418e-23 1.2014740e-16 2.9043029e-17
 2.1140348e-05], sum to 1.0000
[2019-05-04 22:28:30,624] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8184
[2019-05-04 22:28:30,715] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.25, 71.5, 0.0, 0.0, 324.25, 25.5, 12.0, -0.3154774934335402, 20.0, 22.57878368867649, 22.2, 1.0, 20.046253972229483], 
current ob forecast is [], 
actual action is [12.25, 20.0], 
sim time this is 6956100.0000, 
sim time next is 6957000.0000, 
raw observation next is [7.5, 73.0, 0.0, 0.0, 337.0, 30.0, 12.25, -0.3767568399131974, 20.0, 22.42597018999992, 22.2, 1.0, 15.317168122759865], 
processed observation next is [1.0, 0.5217391304347826, 0.5256410256410257, 0.73, 0.0, 0.0, 0.8915343915343915, 0.03, 0.7041666666666667, 0.37441438669560084, 0.0, 0.6322814557142742, 0.5999999999999999, 1.0, 0.18020197791482195], 
reward next is 0.7698, 
noisyNet noise sample is [array([-0.77164626], dtype=float32), 0.92131364]. 
=============================================
[2019-05-04 22:28:30,733] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[89.75849 ]
 [89.386284]
 [88.43423 ]
 [88.25656 ]
 [88.8529  ]], R is [[89.61396027]
 [89.43198395]
 [89.17997742]
 [88.81794739]
 [87.92977142]].
[2019-05-04 22:28:34,381] A3C_AGENT_WORKER-Thread-6 INFO:Local step 16500, global step 261840: loss 0.5856
[2019-05-04 22:28:34,385] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 16500, global step 261840: learning rate 0.0001
[2019-05-04 22:28:35,249] A3C_AGENT_WORKER-Thread-7 INFO:Local step 16500, global step 262201: loss 3.0188
[2019-05-04 22:28:35,250] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 16500, global step 262201: learning rate 0.0001
[2019-05-04 22:28:35,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4631246e-16 1.0000000e+00 2.2166768e-15 9.1779818e-17 3.5799265e-20
 1.5561565e-22 2.1244720e-18 9.0095438e-22 2.5545235e-16 1.2399838e-17
 2.5276528e-09], sum to 1.0000
[2019-05-04 22:28:35,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3627
[2019-05-04 22:28:35,314] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.0, 66.0, 0.0, 0.0, 247.5, 327.5, 14.75, -0.3197315961859998, 20.0, 22.65351071246721, 22.2, 1.0, 0.015414948047374291], 
current ob forecast is [], 
actual action is [15.0, 20.0], 
sim time this is 6966000.0000, 
sim time next is 6966900.0000, 
raw observation next is [10.5, 64.0, 0.0, 0.0, 228.75, 455.75, 15.0, -0.2675595067226915, 20.0, 22.53697969626457, 22.2, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6025641025641025, 0.64, 0.0, 0.0, 0.6051587301587301, 0.45575, 0.75, 0.41081349775910286, 0.0, 0.6481399566092243, 0.5999999999999999, 1.0, 0.0], 
reward next is 0.9500, 
noisyNet noise sample is [array([-0.5036918], dtype=float32), -0.09294015]. 
=============================================
[2019-05-04 22:28:35,890] A3C_AGENT_WORKER-Thread-2 INFO:Local step 16500, global step 262440: loss 0.6230
[2019-05-04 22:28:35,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 16500, global step 262440: learning rate 0.0001
[2019-05-04 22:28:36,188] A3C_AGENT_WORKER-Thread-12 INFO:Local step 16500, global step 262573: loss 0.0394
[2019-05-04 22:28:36,189] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 16500, global step 262573: learning rate 0.0001
[2019-05-04 22:28:36,192] A3C_AGENT_WORKER-Thread-5 INFO:Local step 16500, global step 262573: loss 0.1279
[2019-05-04 22:28:36,197] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 16500, global step 262574: learning rate 0.0001
[2019-05-04 22:28:36,358] A3C_AGENT_WORKER-Thread-3 INFO:Local step 16500, global step 262634: loss 0.0614
[2019-05-04 22:28:36,359] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 16500, global step 262634: learning rate 0.0001
[2019-05-04 22:28:38,190] A3C_AGENT_WORKER-Thread-15 INFO:Local step 16500, global step 263393: loss 0.8926
[2019-05-04 22:28:38,194] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 16500, global step 263395: learning rate 0.0001
[2019-05-04 22:28:39,269] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0329893e-26 1.0000000e+00 2.0448618e-26 5.3113408e-25 2.3712507e-32
 2.2901337e-36 3.5030888e-26 2.9717493e-35 5.0001317e-27 7.5608034e-27
 5.6190961e-20], sum to 1.0000
[2019-05-04 22:28:39,271] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9941
[2019-05-04 22:28:39,301] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.0, 67.0, 4.1, 190.0, 57.5, 54.5, 19.0, 0.1167546611548314, 20.0, 23.55858353815024, 19.4, 0.0, 0.8020386856328043], 
current ob forecast is [], 
actual action is [19.0, 20.0], 
sim time this is 7196400.0000, 
sim time next is 7197300.0000, 
raw observation next is [14.5, 67.25, 4.35, 195.0, 82.75, 67.75, 19.0, 0.1162385376615628, 20.0, 23.54965042237883, 19.4, 0.0, 0.7111279664238693], 
processed observation next is [0.0, 0.30434782608695654, 0.7051282051282052, 0.6725, 0.39545454545454545, 0.5416666666666666, 0.21891534391534392, 0.06775, 0.8166666666666667, 0.5387461792205209, 0.0, 0.7928072031969755, 0.1999999999999998, 0.0, 0.00836621136969258], 
reward next is 0.9416, 
noisyNet noise sample is [array([-0.19607335], dtype=float32), -0.20343961]. 
=============================================
[2019-05-04 22:28:39,345] A3C_AGENT_WORKER-Thread-13 INFO:Local step 16500, global step 263889: loss 4.2897
[2019-05-04 22:28:39,348] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 16500, global step 263890: learning rate 0.0001
[2019-05-04 22:28:39,426] A3C_AGENT_WORKER-Thread-18 INFO:Local step 16500, global step 263934: loss 3.5873
[2019-05-04 22:28:39,428] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 16500, global step 263934: learning rate 0.0001
[2019-05-04 22:28:39,458] A3C_AGENT_WORKER-Thread-14 INFO:Local step 16500, global step 263946: loss 5.2543
[2019-05-04 22:28:39,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 16500, global step 263947: learning rate 0.0001
[2019-05-04 22:28:41,216] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8346659e-20 1.0000000e+00 3.4635851e-18 4.9027295e-19 7.7219360e-27
 1.6784455e-30 2.8427354e-20 3.5837105e-28 8.3977220e-22 8.8094412e-21
 3.0682543e-08], sum to 1.0000
[2019-05-04 22:28:41,216] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5606
[2019-05-04 22:28:41,254] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.0, 76.0, 3.1, 120.0, 135.0, 701.0, 15.25, -0.32656649161809, 20.0, 21.9162778729445, 22.2, 1.0, 1.7034818047056788], 
current ob forecast is [], 
actual action is [16.0, 20.0], 
sim time this is 7117200.0000, 
sim time next is 7118100.0000, 
raw observation next is [11.5, 73.75, 2.85, 122.5, 141.0, 726.5, 16.0, -0.2776540090806771, 20.0, 22.14901537531125, 22.2, 1.0, 1.5576353089226924], 
processed observation next is [1.0, 0.391304347826087, 0.6282051282051282, 0.7375, 0.2590909090909091, 0.3402777777777778, 0.373015873015873, 0.7265, 0.7666666666666667, 0.4074486636397743, 0.0, 0.5927164821873215, 0.5999999999999999, 1.0, 0.01832512128144344], 
reward next is 0.9317, 
noisyNet noise sample is [array([-1.2885346], dtype=float32), 1.8093064]. 
=============================================
[2019-05-04 22:28:41,515] A3C_AGENT_WORKER-Thread-8 INFO:Local step 16500, global step 264815: loss 0.5333
[2019-05-04 22:28:41,520] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 16500, global step 264816: learning rate 0.0001
[2019-05-04 22:28:42,551] A3C_AGENT_WORKER-Thread-19 INFO:Local step 16500, global step 265233: loss 2.7178
[2019-05-04 22:28:42,552] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 16500, global step 265233: learning rate 0.0001
[2019-05-04 22:28:43,993] A3C_AGENT_WORKER-Thread-4 INFO:Local step 16500, global step 265853: loss 0.0766
[2019-05-04 22:28:43,994] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 16500, global step 265853: learning rate 0.0001
[2019-05-04 22:28:45,074] A3C_AGENT_WORKER-Thread-17 INFO:Local step 16500, global step 266297: loss 4.1315
[2019-05-04 22:28:45,076] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 16500, global step 266297: learning rate 0.0001
[2019-05-04 22:28:47,242] A3C_AGENT_WORKER-Thread-11 INFO:Local step 16500, global step 267216: loss 1.7253
[2019-05-04 22:28:47,244] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 16500, global step 267217: learning rate 0.0001
[2019-05-04 22:28:47,566] A3C_AGENT_WORKER-Thread-16 INFO:Local step 16500, global step 267362: loss 2.3076
[2019-05-04 22:28:47,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 16500, global step 267362: learning rate 0.0001
[2019-05-04 22:28:47,988] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1475128e-18 1.0000000e+00 2.1780830e-17 3.3999863e-20 4.6243200e-24
 4.5389181e-28 1.2687191e-20 3.5732986e-26 4.3784133e-19 1.0676435e-20
 1.7743286e-10], sum to 1.0000
[2019-05-04 22:28:48,004] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7133
[2019-05-04 22:28:48,020] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.5, 61.5, 3.1, 325.0, 0.0, 0.0, 14.75, -0.1108019504368669, 20.0, 22.67155649451301, 22.2, 1.0, 8.092040512259553], 
current ob forecast is [], 
actual action is [14.5, 20.0], 
sim time this is 7414200.0000, 
sim time next is 7415100.0000, 
raw observation next is [9.25, 61.25, 2.85, 322.5, 0.0, 0.0, 14.5, -0.1086488785318405, 20.0, 22.64470096744685, 22.2, 1.0, 8.120394564634482], 
processed observation next is [1.0, 0.8260869565217391, 0.5705128205128205, 0.6125, 0.2590909090909091, 0.8958333333333334, 0.0, 0.0, 0.7416666666666667, 0.46378370715605316, 0.0, 0.663528709635264, 0.5999999999999999, 1.0, 0.09553405370158215], 
reward next is 0.8545, 
noisyNet noise sample is [array([0.7901498], dtype=float32), 0.3004445]. 
=============================================
[2019-05-04 22:28:52,197] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3542605e-19 1.0000000e+00 2.0514098e-18 1.9892569e-19 3.5568058e-24
 1.3364314e-26 1.6481574e-20 3.5587141e-27 1.5472024e-20 2.8863236e-20
 5.7972622e-14], sum to 1.0000
[2019-05-04 22:28:52,205] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9356
[2019-05-04 22:28:52,221] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 87.0, 6.075, 250.0, 0.0, 0.0, 8.0, -0.3357500047369675, 20.0, 21.73328152463366, 19.4, 0.0, 4.660945581012764], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 7503300.0000, 
sim time next is 7504200.0000, 
raw observation next is [3.0, 87.0, 6.45, 250.0, 0.0, 0.0, 8.0, -0.3444215782230484, 20.0, 21.69631865821622, 19.4, 0.0, 4.8977079068107905], 
processed observation next is [1.0, 0.8695652173913043, 0.41025641025641024, 0.87, 0.5863636363636364, 0.6944444444444444, 0.0, 0.0, 0.6333333333333333, 0.38519280725898386, 0.0, 0.5280455226023173, 0.1999999999999998, 0.0, 0.057620093021303415], 
reward next is 0.8924, 
noisyNet noise sample is [array([0.13228717], dtype=float32), -1.0912403]. 
=============================================
[2019-05-04 22:28:53,011] A3C_AGENT_WORKER-Thread-6 INFO:Local step 17000, global step 269666: loss 1.4445
[2019-05-04 22:28:53,018] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 17000, global step 269666: learning rate 0.0001
[2019-05-04 22:28:53,546] A3C_AGENT_WORKER-Thread-7 INFO:Local step 17000, global step 269900: loss 1.8629
[2019-05-04 22:28:53,549] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 17000, global step 269902: learning rate 0.0001
[2019-05-04 22:28:54,596] A3C_AGENT_WORKER-Thread-3 INFO:Local step 17000, global step 270351: loss 1.1562
[2019-05-04 22:28:54,597] A3C_AGENT_WORKER-Thread-5 INFO:Local step 17000, global step 270351: loss 0.9853
[2019-05-04 22:28:54,601] A3C_AGENT_WORKER-Thread-2 INFO:Local step 17000, global step 270352: loss 1.1635
[2019-05-04 22:28:54,603] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 17000, global step 270352: learning rate 0.0001
[2019-05-04 22:28:54,604] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 17000, global step 270352: learning rate 0.0001
[2019-05-04 22:28:54,604] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 17000, global step 270352: learning rate 0.0001
[2019-05-04 22:28:54,770] A3C_AGENT_WORKER-Thread-12 INFO:Local step 17000, global step 270425: loss 1.0324
[2019-05-04 22:28:54,773] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 17000, global step 270425: learning rate 0.0001
[2019-05-04 22:28:56,953] A3C_AGENT_WORKER-Thread-15 INFO:Local step 17000, global step 271182: loss 0.9859
[2019-05-04 22:28:56,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 17000, global step 271182: learning rate 0.0001
[2019-05-04 22:28:57,685] A3C_AGENT_WORKER-Thread-13 INFO:Local step 17000, global step 271498: loss 0.8137
[2019-05-04 22:28:57,688] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 17000, global step 271499: learning rate 0.0001
[2019-05-04 22:28:58,388] A3C_AGENT_WORKER-Thread-18 INFO:Local step 17000, global step 271788: loss 1.0174
[2019-05-04 22:28:58,389] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 17000, global step 271788: learning rate 0.0001
[2019-05-04 22:28:58,428] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6495494e-18 1.0000000e+00 2.8078621e-18 5.0965741e-19 9.7719583e-25
 4.1084642e-27 8.2327550e-21 8.4228257e-26 3.4245637e-19 1.0221699e-21
 4.3878292e-14], sum to 1.0000
[2019-05-04 22:28:58,432] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6398
[2019-05-04 22:28:58,463] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.0, 49.0, 3.1, 190.0, 0.0, 0.0, 14.25, 0.1990512418555118, 20.0, 24.37560306322704, 19.4, 0.0, 0.681080827215704], 
current ob forecast is [], 
actual action is [14.0, 20.0], 
sim time this is 7340400.0000, 
sim time next is 7341300.0000, 
raw observation next is [8.25, 51.75, 3.225, 185.0, 0.0, 0.0, 14.0, 0.1836636944440966, 20.0, 24.31626471339545, 19.4, 0.0, 1.1473397884036922], 
processed observation next is [0.0, 1.0, 0.5448717948717948, 0.5175, 0.2931818181818182, 0.5138888888888888, 0.0, 0.0, 0.7333333333333333, 0.5612212314813655, 0.0, 0.9023235304850645, 0.1999999999999998, 0.0, 0.013498115157690496], 
reward next is 0.9365, 
noisyNet noise sample is [array([-1.425755], dtype=float32), -1.4174956]. 
=============================================
[2019-05-04 22:28:58,562] A3C_AGENT_WORKER-Thread-14 INFO:Local step 17000, global step 271858: loss 0.7654
[2019-05-04 22:28:58,563] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 17000, global step 271858: learning rate 0.0001
[2019-05-04 22:28:59,933] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0771531e-13 9.9999821e-01 2.1069187e-14 2.8685548e-15 1.0079003e-20
 3.4367589e-24 6.2691545e-16 4.7623230e-23 4.6252123e-16 4.6131601e-16
 1.7543767e-06], sum to 1.0000
[2019-05-04 22:28:59,936] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9248
[2019-05-04 22:28:59,947] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.0, 69.0, 4.1, 260.0, 0.0, 0.0, 9.5, -0.4415321098136996, 20.0, 21.45786768466705, 19.4, 0.0, 5.1991904873106884], 
current ob forecast is [], 
actual action is [9.0, 20.0], 
sim time this is 7437600.0000, 
sim time next is 7438500.0000, 
raw observation next is [4.0, 69.0, 3.975, 257.5, 0.0, 0.0, 9.0, -0.4544204875085154, 20.0, 21.40163199273427, 19.4, 0.0, 5.313785983883911], 
processed observation next is [1.0, 0.08695652173913043, 0.4358974358974359, 0.69, 0.3613636363636364, 0.7152777777777778, 0.0, 0.0, 0.65, 0.3485265041638282, 0.0, 0.48594742753346715, 0.1999999999999998, 0.0, 0.06251512922216367], 
reward next is 0.8875, 
noisyNet noise sample is [array([0.44363165], dtype=float32), 1.8102485]. 
=============================================
[2019-05-04 22:28:59,973] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.978004]
 [86.43804 ]
 [86.6748  ]
 [86.85264 ]
 [86.88852 ]], R is [[85.66844177]
 [85.70059204]
 [85.73361969]
 [85.76722717]
 [85.80110931]].
[2019-05-04 22:29:00,792] A3C_AGENT_WORKER-Thread-8 INFO:Local step 17000, global step 272597: loss 0.7833
[2019-05-04 22:29:00,796] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 17000, global step 272598: learning rate 0.0001
[2019-05-04 22:29:01,444] A3C_AGENT_WORKER-Thread-19 INFO:Local step 17000, global step 272890: loss 1.0584
[2019-05-04 22:29:01,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 17000, global step 272890: learning rate 0.0001
[2019-05-04 22:29:01,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5045369e-16 1.0000000e+00 3.0625471e-16 3.3308482e-17 4.2286845e-22
 1.0188243e-24 1.1094513e-17 5.5995518e-23 8.7382318e-18 2.4599959e-19
 1.8969240e-12], sum to 1.0000
[2019-05-04 22:29:01,898] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0194
[2019-05-04 22:29:01,924] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.0, 56.0, 2.6, 305.0, 0.0, 0.0, 12.0, -0.2420710837814934, 20.0, 22.26487132935373, 19.4, 0.0, 5.315941332506399], 
current ob forecast is [], 
actual action is [12.0, 20.0], 
sim time this is 7424100.0000, 
sim time next is 7425000.0000, 
raw observation next is [7.0, 56.0, 2.6, 300.0, 0.0, 0.0, 12.0, -0.2604751994075339, 20.0, 22.14566943198996, 19.4, 0.0, 5.254842768028417], 
processed observation next is [1.0, 0.9565217391304348, 0.5128205128205128, 0.56, 0.23636363636363636, 0.8333333333333334, 0.0, 0.0, 0.7, 0.4131749335308221, 0.0, 0.5922384902842798, 0.1999999999999998, 0.0, 0.06182167962386373], 
reward next is 0.8882, 
noisyNet noise sample is [array([1.719113], dtype=float32), 0.030422617]. 
=============================================
[2019-05-04 22:29:01,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.546974]
 [80.78634 ]
 [81.20083 ]
 [81.66241 ]
 [81.99673 ]], R is [[80.53740692]
 [80.61949158]
 [80.70000458]
 [80.77781677]
 [80.85253143]].
[2019-05-04 22:29:02,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.1959498e-20 1.0000000e+00 1.8768937e-18 1.5617431e-19 2.9988579e-24
 2.1667116e-26 1.4606043e-20 2.0672788e-25 4.0594677e-20 6.1461345e-21
 1.3723920e-11], sum to 1.0000
[2019-05-04 22:29:02,568] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6302
[2019-05-04 22:29:02,591] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.0, 75.0, 3.225, 255.0, 0.0, 0.0, 9.0, -0.5607392142229595, 20.0, 20.99797838891433, 19.4, 0.0, 5.222435475096405], 
current ob forecast is [], 
actual action is [9.0, 20.0], 
sim time this is 7602300.0000, 
sim time next is 7603200.0000, 
raw observation next is [4.0, 75.0, 3.1, 250.0, 0.0, 0.0, 9.0, -0.5734734389649389, 20.0, 20.95115783695947, 19.4, 0.0, 5.318907456433811], 
processed observation next is [1.0, 0.0, 0.4358974358974359, 0.75, 0.2818181818181818, 0.6944444444444444, 0.0, 0.0, 0.65, 0.30884218701168703, 0.0, 0.42159397670849585, 0.1999999999999998, 0.0, 0.06257538184039778], 
reward next is 0.8874, 
noisyNet noise sample is [array([-1.4070886], dtype=float32), -0.60055614]. 
=============================================
[2019-05-04 22:29:02,793] A3C_AGENT_WORKER-Thread-4 INFO:Local step 17000, global step 273473: loss 0.6057
[2019-05-04 22:29:02,794] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 17000, global step 273473: learning rate 0.0001
[2019-05-04 22:29:03,407] A3C_AGENT_WORKER-Thread-17 INFO:Local step 17000, global step 273735: loss 0.7435
[2019-05-04 22:29:03,408] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 17000, global step 273735: learning rate 0.0001
[2019-05-04 22:29:05,249] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:05,423] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:05,790] A3C_AGENT_WORKER-Thread-11 INFO:Local step 17000, global step 274645: loss 1.0880
[2019-05-04 22:29:05,793] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 17000, global step 274646: learning rate 0.0001
[2019-05-04 22:29:05,845] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:06,023] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:06,042] A3C_AGENT_WORKER-Thread-16 INFO:Local step 17000, global step 274754: loss 0.7227
[2019-05-04 22:29:06,045] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 17000, global step 274756: learning rate 0.0001
[2019-05-04 22:29:06,256] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:06,258] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:06,293] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res6/Eplus-env-sub_run3
[2019-05-04 22:29:06,472] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:06,573] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:06,576] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:06,662] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:06,760] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:06,785] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:06,833] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:06,848] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:06,850] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:06,867] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res7/Eplus-env-sub_run3
[2019-05-04 22:29:07,006] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:07,472] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:07,473] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:07,476] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res5/Eplus-env-sub_run3
[2019-05-04 22:29:07,574] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:07,574] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:07,577] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:07,578] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:07,582] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res2/Eplus-env-sub_run3
[2019-05-04 22:29:07,618] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res3/Eplus-env-sub_run3
[2019-05-04 22:29:07,833] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:07,833] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:07,835] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res10/Eplus-env-sub_run3
[2019-05-04 22:29:08,796] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:08,968] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:09,416] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:09,462] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:09,582] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:09,669] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:09,797] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:09,797] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:09,800] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res13/Eplus-env-sub_run3
[2019-05-04 22:29:09,800] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:09,955] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:10,414] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:10,414] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:10,416] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res11/Eplus-env-sub_run3
[2019-05-04 22:29:10,463] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:10,463] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:10,465] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res16/Eplus-env-sub_run3
[2019-05-04 22:29:10,797] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:10,798] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:10,801] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res12/Eplus-env-sub_run3
[2019-05-04 22:29:11,704] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2147039e-18 1.0000000e+00 7.3922948e-16 3.4992813e-19 2.0011664e-21
 2.4775239e-25 1.8775084e-19 5.9042225e-25 6.7405178e-18 5.6160803e-18
 2.0543451e-08], sum to 1.0000
[2019-05-04 22:29:11,704] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6577
[2019-05-04 22:29:11,750] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.5, 65.75, 0.225, 2.5, 117.25, 641.0, 8.0, -0.7108417297032054, 20.0, 20.49775470614166, 22.2, 1.0, 4.245413327563156], 
current ob forecast is [], 
actual action is [8.5, 20.0], 
sim time this is 7719300.0000, 
sim time next is 7720200.0000, 
raw observation next is [4.0, 63.5, 0.45, 5.0, 121.0, 696.0, 8.5, -0.6773689934039453, 20.0, 20.76277253039692, 22.2, 1.0, 3.4628954846792115], 
processed observation next is [1.0, 0.34782608695652173, 0.4358974358974359, 0.635, 0.04090909090909091, 0.013888888888888888, 0.3201058201058201, 0.696, 0.6416666666666667, 0.27421033553201823, 0.0, 0.39468179005670273, 0.5999999999999999, 1.0, 0.04073994687857896], 
reward next is 0.5001, 
noisyNet noise sample is [array([0.07624104], dtype=float32), -0.39229983]. 
=============================================
[2019-05-04 22:29:11,917] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:12,130] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:12,913] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:12,913] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:12,915] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res8/Eplus-env-sub_run3
[2019-05-04 22:29:14,793] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:15,085] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:15,769] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:15,769] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:15,771] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res17/Eplus-env-sub_run3
[2019-05-04 22:29:16,425] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:16,750] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:17,290] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:17,438] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:17,438] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:17,440] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res4/Eplus-env-sub_run3
[2019-05-04 22:29:17,537] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:18,296] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:18,296] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:18,298] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res15/Eplus-env-sub_run3
[2019-05-04 22:29:20,778] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:20,851] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:29:20,979] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:21,045] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-05-04 22:29:21,781] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:21,781] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:21,783] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res9/Eplus-env-sub_run3
[2019-05-04 22:29:21,825] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-05-04 22:29:21,825] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:29:21,833] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res14/Eplus-env-sub_run3
[2019-05-04 22:29:33,292] A3C_AGENT_WORKER-Thread-6 INFO:Local step 17500, global step 278857: loss 0.1060
[2019-05-04 22:29:33,296] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 17500, global step 278858: learning rate 0.0001
[2019-05-04 22:29:33,507] A3C_AGENT_WORKER-Thread-7 INFO:Local step 17500, global step 278951: loss 0.3726
[2019-05-04 22:29:33,512] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 17500, global step 278951: learning rate 0.0001
[2019-05-04 22:29:33,577] A3C_AGENT_WORKER-Thread-2 INFO:Local step 17500, global step 278985: loss 0.1057
[2019-05-04 22:29:33,579] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 17500, global step 278985: learning rate 0.0001
[2019-05-04 22:29:33,613] A3C_AGENT_WORKER-Thread-5 INFO:Local step 17500, global step 279000: loss 0.2197
[2019-05-04 22:29:33,614] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 17500, global step 279000: learning rate 0.0001
[2019-05-04 22:29:33,702] A3C_AGENT_WORKER-Thread-3 INFO:Local step 17500, global step 279034: loss 0.0509
[2019-05-04 22:29:33,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 17500, global step 279034: learning rate 0.0001
[2019-05-04 22:29:33,922] A3C_AGENT_WORKER-Thread-12 INFO:Local step 17500, global step 279110: loss 0.0938
[2019-05-04 22:29:33,933] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 17500, global step 279110: learning rate 0.0001
[2019-05-04 22:29:34,196] A3C_AGENT_WORKER-Thread-14 INFO:Local step 17500, global step 279185: loss 1.5800
[2019-05-04 22:29:34,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 17500, global step 279185: learning rate 0.0001
[2019-05-04 22:29:35,463] A3C_AGENT_WORKER-Thread-18 INFO:Local step 17500, global step 279458: loss -1.1731
[2019-05-04 22:29:35,464] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 17500, global step 279458: learning rate 0.0001
[2019-05-04 22:29:35,656] A3C_AGENT_WORKER-Thread-15 INFO:Local step 17500, global step 279491: loss -0.3412
[2019-05-04 22:29:35,656] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 17500, global step 279491: learning rate 0.0001
[2019-05-04 22:29:36,278] A3C_AGENT_WORKER-Thread-13 INFO:Local step 17500, global step 279593: loss 0.0688
[2019-05-04 22:29:36,279] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 17500, global step 279593: learning rate 0.0001
[2019-05-04 22:29:37,625] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5174502e-06 1.6408819e-01 1.3271858e-05 3.4627483e-08 1.7911930e-10
 2.2089939e-11 2.6543774e-09 7.8278301e-11 1.6239647e-07 6.1556158e-08
 8.3589578e-01], sum to 1.0000
[2019-05-04 22:29:37,625] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1075
[2019-05-04 22:29:37,745] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 75.0, 5.1, 200.0, 101.5, 0.0, -2.574999999999999, -0.663974077041673, 65.0, 21.79442388273281, 22.7, 1.0, 71.47966276825066], 
current ob forecast is [], 
actual action is [-2.3, 20.0], 
sim time this is 208800.0000, 
sim time next is 209700.0000, 
raw observation next is [-7.024999999999999, 74.25, 5.1, 197.5, 112.75, 0.0, -2.3, -0.6726989145896107, 20.0, 21.84130694892512, 22.7, 1.0, 46.964722298860785], 
processed observation next is [1.0, 0.43478260869565216, 0.15320512820512824, 0.7425, 0.4636363636363636, 0.5486111111111112, 0.29828042328042326, 0.0, 0.46166666666666667, 0.27576702847012974, 0.0, 0.5487581355607313, 0.6714285714285714, 1.0, 0.5525261446924798], 
reward next is 0.1721, 
noisyNet noise sample is [array([0.4087248], dtype=float32), -0.32770702]. 
=============================================
[2019-05-04 22:29:40,132] A3C_AGENT_WORKER-Thread-8 INFO:Local step 17500, global step 280041: loss -0.1270
[2019-05-04 22:29:40,154] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 17500, global step 280044: learning rate 0.0001
[2019-05-04 22:29:42,496] A3C_AGENT_WORKER-Thread-19 INFO:Local step 17500, global step 280456: loss -0.0095
[2019-05-04 22:29:42,497] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 17500, global step 280456: learning rate 0.0001
[2019-05-04 22:29:43,273] A3C_AGENT_WORKER-Thread-4 INFO:Local step 17500, global step 280636: loss 2.8314
[2019-05-04 22:29:43,274] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 17500, global step 280636: learning rate 0.0001
[2019-05-04 22:29:45,314] A3C_AGENT_WORKER-Thread-17 INFO:Local step 17500, global step 281010: loss 0.1165
[2019-05-04 22:29:45,326] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 17500, global step 281010: learning rate 0.0001
[2019-05-04 22:29:46,857] A3C_AGENT_WORKER-Thread-11 INFO:Local step 17500, global step 281214: loss 0.2122
[2019-05-04 22:29:46,861] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 17500, global step 281214: learning rate 0.0001
[2019-05-04 22:29:47,904] A3C_AGENT_WORKER-Thread-16 INFO:Local step 17500, global step 281337: loss -0.1480
[2019-05-04 22:29:47,905] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 17500, global step 281337: learning rate 0.0001
[2019-05-04 22:29:53,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9385912e-05 9.8102462e-01 3.7616966e-05 1.0175088e-06 7.7284240e-10
 1.2347744e-10 3.0692439e-07 1.4656570e-10 2.2614530e-07 1.2446650e-07
 1.8916687e-02], sum to 1.0000
[2019-05-04 22:29:53,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8869
[2019-05-04 22:29:53,137] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-13.525, 79.0, 4.6, 260.0, 0.0, 0.0, -8.4, -0.9569258777410402, 20.0, 20.34866431834431, 21.5, 0.0, 8.3878182267241], 
current ob forecast is [], 
actual action is [-8.525, 20.0], 
sim time this is 339300.0000, 
sim time next is 340200.0000, 
raw observation next is [-13.65, 76.0, 4.6, 260.0, 0.0, 0.0, -8.525, -0.9855593086669828, 20.0, 20.22354945127529, 21.5, 0.0, 9.340963819386664], 
processed observation next is [1.0, 0.9565217391304348, -0.016666666666666677, 0.76, 0.41818181818181815, 0.7222222222222222, 0.0, 0.0, 0.3579166666666667, 0.17148023044433905, 0.0, 0.3176499216107556, 0.5, 0.0, 0.10989369199278427], 
reward next is 0.8401, 
noisyNet noise sample is [array([0.09976759], dtype=float32), -0.8375084]. 
=============================================
[2019-05-04 22:29:55,433] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6646105e-04 4.2142086e-02 5.7456229e-04 1.1718247e-05 6.2764815e-07
 2.1945306e-07 3.7681559e-06 5.2674181e-07 4.0186223e-05 1.2405959e-05
 9.5704746e-01], sum to 1.0000
[2019-05-04 22:29:55,433] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7884
[2019-05-04 22:29:55,488] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.3, 71.0, 5.35, 250.0, 0.0, 0.0, -10.15, -1.026145995768674, 20.0, 19.96549498040372, 21.5, 0.0, 43.962035313524495], 
current ob forecast is [], 
actual action is [-10.3, 65.0], 
sim time this is 358200.0000, 
sim time next is 359100.0000, 
raw observation next is [-15.45, 72.0, 5.225, 245.0, 0.0, 0.0, -10.3, -1.027104241329662, 65.0, 19.85735615899686, 21.5, 0.0, 62.605323410903445], 
processed observation next is [1.0, 0.13043478260869565, -0.0628205128205128, 0.72, 0.475, 0.6805555555555556, 0.0, 0.0, 0.3283333333333333, 0.15763191955677933, 1.0, 0.26533659414240873, 0.5, 0.0, 0.7365332165988641], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.010094], dtype=float32), -1.0571766]. 
=============================================
[2019-05-04 22:30:01,751] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2317335e-09 9.9999976e-01 1.0539676e-09 3.1179819e-11 2.0214473e-14
 4.4402322e-16 7.2600931e-12 5.0030611e-15 1.0018211e-11 3.3371513e-12
 2.3420566e-07], sum to 1.0000
[2019-05-04 22:30:01,775] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3900
[2019-05-04 22:30:01,831] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.6, 47.5, 3.0, 200.0, 0.0, 0.0, -5.6, -0.8898744282860562, 20.0, 20.69336690247726, 21.5, 0.0, 20.752456786976897], 
current ob forecast is [], 
actual action is [-5.6, 20.0], 
sim time this is 422100.0000, 
sim time next is 423000.0000, 
raw observation next is [-10.6, 48.0, 3.0, 200.0, 0.0, 0.0, -5.6, -0.9272841759034768, 20.0, 20.52258897608842, 21.5, 0.0, 17.230198502579327], 
processed observation next is [1.0, 0.9130434782608695, 0.06153846153846155, 0.48, 0.2727272727272727, 0.5555555555555556, 0.0, 0.0, 0.4066666666666666, 0.19090527469884108, 0.0, 0.3603698537269174, 0.5, 0.0, 0.20270821767740385], 
reward next is 0.7473, 
noisyNet noise sample is [array([0.5220532], dtype=float32), 1.304346]. 
=============================================
[2019-05-04 22:30:01,929] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[46.209877]
 [46.329536]
 [45.701515]
 [46.348083]
 [45.921814]], R is [[46.28156281]
 [46.52460098]
 [46.71221924]
 [46.83141708]
 [46.84041595]].
[2019-05-04 22:30:17,267] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1702921e-09 9.9999130e-01 6.3187127e-09 1.1529121e-11 3.8229409e-14
 5.8392393e-17 4.8394559e-11 3.1024022e-14 3.4187979e-11 1.5592536e-11
 8.7284552e-06], sum to 1.0000
[2019-05-04 22:30:17,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7951
[2019-05-04 22:30:17,314] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.425, 82.75, 6.1, 295.0, 0.0, 0.0, 7.7, -0.9917719349292436, 20.0, 19.97082350011128, 21.5, 0.0, 7.137244988993397], 
current ob forecast is [], 
actual action is [7.425, 20.0], 
sim time this is 533700.0000, 
sim time next is 534600.0000, 
raw observation next is [2.15, 83.5, 6.1, 290.0, 0.0, 0.0, 7.425, -1.004389021735888, 20.0, 19.90876504716508, 21.5, 0.0, 7.2766936615510565], 
processed observation next is [0.0, 0.17391304347826086, 0.38846153846153847, 0.835, 0.5545454545454546, 0.8055555555555556, 0.0, 0.0, 0.6237499999999999, 0.1652036594213707, 0.0, 0.2726807210235828, 0.5, 0.0, 0.08560816072413008], 
reward next is 0.8644, 
noisyNet noise sample is [array([2.5522604], dtype=float32), -1.0467685]. 
=============================================
[2019-05-04 22:30:25,804] A3C_AGENT_WORKER-Thread-2 INFO:Local step 18000, global step 287088: loss 0.3333
[2019-05-04 22:30:25,807] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 18000, global step 287090: learning rate 0.0001
[2019-05-04 22:30:25,854] A3C_AGENT_WORKER-Thread-5 INFO:Local step 18000, global step 287099: loss 1.0599
[2019-05-04 22:30:25,855] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 18000, global step 287099: learning rate 0.0001
[2019-05-04 22:30:26,125] A3C_AGENT_WORKER-Thread-6 INFO:Local step 18000, global step 287149: loss 0.6063
[2019-05-04 22:30:26,133] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 18000, global step 287149: learning rate 0.0001
[2019-05-04 22:30:26,734] A3C_AGENT_WORKER-Thread-14 INFO:Local step 18000, global step 287249: loss 0.0914
[2019-05-04 22:30:26,757] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 18000, global step 287249: learning rate 0.0001
[2019-05-04 22:30:26,847] A3C_AGENT_WORKER-Thread-12 INFO:Local step 18000, global step 287269: loss 2.1025
[2019-05-04 22:30:26,848] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 18000, global step 287269: learning rate 0.0001
[2019-05-04 22:30:26,881] A3C_AGENT_WORKER-Thread-7 INFO:Local step 18000, global step 287273: loss 0.9223
[2019-05-04 22:30:26,883] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 18000, global step 287273: learning rate 0.0001
[2019-05-04 22:30:26,950] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0105789e-11 3.5824552e-08 3.0154093e-10 3.9161539e-12 9.9620070e-15
 2.2065998e-18 3.6653924e-14 4.8459671e-17 9.6624403e-14 2.7783873e-13
 1.0000000e+00], sum to 1.0000
[2019-05-04 22:30:26,950] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1142
[2019-05-04 22:30:27,115] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 80.0, 5.999999999999999, 315.0, 135.25, 563.0, 3.8, -0.4552821478113112, 65.0, 22.20667732973295, 22.7, 1.0, 72.46991787791626], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 567900.0000, 
sim time next is 568800.0000, 
raw observation next is [-1.2, 80.0, 5.6, 310.0, 132.5, 531.0, 3.8, -0.3846528586082505, 65.0, 22.66858795352514, 22.7, 1.0, 70.07962269429395], 
processed observation next is [0.0, 0.6086956521739131, 0.3025641025641026, 0.8, 0.509090909090909, 0.8611111111111112, 0.3505291005291005, 0.531, 0.5633333333333332, 0.3717823804639165, 1.0, 0.6669411362178772, 0.6714285714285714, 1.0, 0.8244661493446347], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2354134], dtype=float32), -0.49116954]. 
=============================================
[2019-05-04 22:30:28,119] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5583982e-12 7.4175138e-10 5.3639926e-11 2.2351770e-13 4.9168590e-16
 9.8167909e-18 2.6475463e-14 2.2441046e-17 2.8488178e-14 1.2230361e-14
 1.0000000e+00], sum to 1.0000
[2019-05-04 22:30:28,119] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5474
[2019-05-04 22:30:28,130] A3C_AGENT_WORKER-Thread-3 INFO:Local step 18000, global step 287426: loss 0.4099
[2019-05-04 22:30:28,131] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 18000, global step 287426: learning rate 0.0001
[2019-05-04 22:30:28,310] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.375, 91.75, 5.475, 295.0, 25.75, 79.25, 5.5, -0.89038186707916, 20.0, 20.42464900264278, 22.7, 1.0, 51.040771742792174], 
current ob forecast is [], 
actual action is [5.375, 65.0], 
sim time this is 548100.0000, 
sim time next is 549000.0000, 
raw observation next is [0.25, 91.5, 5.85, 300.0, 34.0, 104.0, 5.375, -0.8214838140274788, 65.0, 20.64664487653015, 22.7, 1.0, 81.96207563345536], 
processed observation next is [0.0, 0.34782608695652173, 0.33974358974358976, 0.915, 0.5318181818181817, 0.8333333333333334, 0.08994708994708994, 0.104, 0.5895833333333333, 0.2261720619908404, 1.0, 0.37809212521859287, 0.6714285714285714, 1.0, 0.9642597133347689], 
reward next is 0.6890, 
noisyNet noise sample is [array([-0.8587317], dtype=float32), 0.36110327]. 
=============================================
[2019-05-04 22:30:28,349] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[54.817757]
 [57.09168 ]
 [56.013493]
 [55.956097]
 [56.163418]], R is [[55.92570496]
 [55.79929352]
 [55.24130249]
 [55.28691101]
 [55.20964813]].
[2019-05-04 22:30:29,055] A3C_AGENT_WORKER-Thread-15 INFO:Local step 18000, global step 287527: loss 0.1169
[2019-05-04 22:30:29,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 18000, global step 287527: learning rate 0.0001
[2019-05-04 22:30:29,659] A3C_AGENT_WORKER-Thread-18 INFO:Local step 18000, global step 287601: loss 0.4314
[2019-05-04 22:30:29,660] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 18000, global step 287601: learning rate 0.0001
[2019-05-04 22:30:30,960] A3C_AGENT_WORKER-Thread-13 INFO:Local step 18000, global step 287791: loss 1.2213
[2019-05-04 22:30:30,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 18000, global step 287791: learning rate 0.0001
[2019-05-04 22:30:31,149] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0276036e-06 3.6294633e-01 2.1356075e-06 5.1115336e-08 4.3137690e-12
 4.6938164e-13 4.9064164e-10 3.4038592e-12 1.1024667e-09 5.9677721e-09
 6.3704747e-01], sum to 1.0000
[2019-05-04 22:30:31,149] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9693
[2019-05-04 22:30:31,298] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.1, 80.25, 6.925000000000001, 330.0, 132.0, 445.75, 4.0, -0.3992179234523127, 65.0, 22.53840778284692, 22.7, 1.0, 64.88050989501365], 
current ob forecast is [], 
actual action is [3.9, 65.0], 
sim time this is 564300.0000, 
sim time next is 565200.0000, 
raw observation next is [-1.2, 80.0, 7.2, 330.0, 134.0, 495.5, 3.9, -0.3568352892774212, 65.0, 22.72412365974554, 22.7, 1.0, 67.18235833966432], 
processed observation next is [0.0, 0.5652173913043478, 0.3025641025641026, 0.8, 0.6545454545454545, 0.9166666666666666, 0.3544973544973545, 0.4955, 0.565, 0.38105490357419297, 1.0, 0.6748748085350772, 0.6714285714285714, 1.0, 0.790380686348992], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.12254011], dtype=float32), -0.8844656]. 
=============================================
[2019-05-04 22:30:33,654] A3C_AGENT_WORKER-Thread-8 INFO:Local step 18000, global step 288361: loss 0.9524
[2019-05-04 22:30:33,655] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 18000, global step 288361: learning rate 0.0001
[2019-05-04 22:30:34,537] A3C_AGENT_WORKER-Thread-4 INFO:Local step 18000, global step 288482: loss 0.3016
[2019-05-04 22:30:34,539] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 18000, global step 288482: learning rate 0.0001
[2019-05-04 22:30:34,693] A3C_AGENT_WORKER-Thread-19 INFO:Local step 18000, global step 288509: loss 0.2699
[2019-05-04 22:30:34,694] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 18000, global step 288509: learning rate 0.0001
[2019-05-04 22:30:39,885] A3C_AGENT_WORKER-Thread-17 INFO:Local step 18000, global step 289215: loss 1.1796
[2019-05-04 22:30:39,888] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 18000, global step 289215: learning rate 0.0001
[2019-05-04 22:30:39,980] A3C_AGENT_WORKER-Thread-11 INFO:Local step 18000, global step 289227: loss 1.0745
[2019-05-04 22:30:39,990] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 18000, global step 289228: learning rate 0.0001
[2019-05-04 22:30:41,626] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6733053e-07 3.7669807e-03 2.5084034e-07 3.3219305e-08 1.1909637e-11
 8.3779381e-13 6.7405320e-10 1.8931441e-11 3.8009751e-09 1.4839733e-09
 9.9623245e-01], sum to 1.0000
[2019-05-04 22:30:41,627] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2560
[2019-05-04 22:30:41,763] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.15, 47.25, 5.35, 300.0, 72.25, 10.0, 4.4, -0.1165072332175228, 20.0, 24.13491935788267, 22.7, 1.0, 23.195941310927726], 
current ob forecast is [], 
actual action is [3.85, 65.0], 
sim time this is 749700.0000, 
sim time next is 750600.0000, 
raw observation next is [-1.7, 49.5, 5.1, 310.0, 68.0, 3.0, 3.85, -0.06162871160125752, 65.0, 24.14401731597452, 22.7, 1.0, 72.02899989515413], 
processed observation next is [1.0, 0.6956521739130435, 0.28974358974358977, 0.495, 0.4636363636363636, 0.8611111111111112, 0.17989417989417988, 0.003, 0.5641666666666667, 0.47945709613291415, 1.0, 0.8777167594249313, 0.6714285714285714, 1.0, 0.8473999987665192], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.56313074], dtype=float32), 0.26220948]. 
=============================================
[2019-05-04 22:30:42,400] A3C_AGENT_WORKER-Thread-16 INFO:Local step 18000, global step 289624: loss -0.0155
[2019-05-04 22:30:42,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 18000, global step 289624: learning rate 0.0001
[2019-05-04 22:30:42,489] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.1614012e-13 1.0000000e+00 4.3820606e-13 7.0084285e-14 7.1106079e-19
 2.3583291e-20 1.1695104e-16 1.3427164e-19 3.7876044e-15 2.6547445e-16
 1.2622754e-11], sum to 1.0000
[2019-05-04 22:30:42,491] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3701
[2019-05-04 22:30:42,527] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.8, 74.0, 1.5, 70.0, 0.0, 0.0, -2.8, -0.9053074097618006, 20.0, 20.47760030960585, 21.5, 0.0, 8.527395739006684], 
current ob forecast is [], 
actual action is [-2.8, 20.0], 
sim time this is 788400.0000, 
sim time next is 789300.0000, 
raw observation next is [-7.675, 74.25, 1.875, 72.5, 0.0, 0.0, -2.8, -0.9248250900998539, 20.0, 20.3901229208198, 21.5, 0.0, 8.6614069760088], 
processed observation next is [1.0, 0.13043478260869565, 0.13653846153846155, 0.7425, 0.17045454545454544, 0.2013888888888889, 0.0, 0.0, 0.4533333333333333, 0.19172496996671537, 0.0, 0.34144613154568554, 0.5, 0.0, 0.10189890560010353], 
reward next is 0.8481, 
noisyNet noise sample is [array([-0.11737034], dtype=float32), 0.9485249]. 
=============================================
[2019-05-04 22:30:50,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2088773e-10 9.9999607e-01 1.6242969e-11 2.6435814e-13 4.0060920e-17
 2.6606113e-18 1.6357738e-13 2.0098150e-17 1.2345278e-13 2.7331804e-12
 3.8940511e-06], sum to 1.0000
[2019-05-04 22:30:50,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8206
[2019-05-04 22:30:50,531] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 85.0, 3.275, 72.5, 21.75, 0.0, 1.1, -0.3563051804844128, 20.0, 23.2822264710426, 22.7, 1.0, 27.78366321075406], 
current ob forecast is [], 
actual action is [1.1, 20.0], 
sim time this is 837900.0000, 
sim time next is 838800.0000, 
raw observation next is [-3.9, 86.0, 3.0, 70.0, 14.5, 0.0, 1.1, -0.3966761877451035, 20.0, 23.02703076466418, 22.7, 1.0, 21.968629366770447], 
processed observation next is [1.0, 0.7391304347826086, 0.23333333333333334, 0.86, 0.2727272727272727, 0.19444444444444445, 0.03835978835978836, 0.0, 0.5183333333333333, 0.3677746040849655, 0.0, 0.7181472520948827, 0.6714285714285714, 1.0, 0.25845446313847587], 
reward next is 0.6915, 
noisyNet noise sample is [array([1.5243523], dtype=float32), 1.7680165]. 
=============================================
[2019-05-04 22:30:54,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.03102260e-15 1.00000000e+00 2.43237899e-16 5.54878793e-18
 2.25998350e-22 1.15363445e-23 7.70198790e-20 3.87851858e-23
 5.88252677e-18 1.32748416e-18 3.28582799e-12], sum to 1.0000
[2019-05-04 22:30:54,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4502
[2019-05-04 22:30:54,407] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 83.0, 3.6, 62.5, 0.0, 0.0, 1.6, -0.6085359552098979, 20.0, 21.77142258033823, 21.5, 0.0, 14.595025436285757], 
current ob forecast is [], 
actual action is [1.6, 20.0], 
sim time this is 852300.0000, 
sim time next is 853200.0000, 
raw observation next is [-3.4, 83.0, 3.6, 60.0, 0.0, 0.0, 1.6, -0.6361667621955023, 20.0, 21.6417265904863, 21.5, 0.0, 12.36910257878843], 
processed observation next is [1.0, 0.9130434782608695, 0.24615384615384614, 0.83, 0.32727272727272727, 0.16666666666666666, 0.0, 0.0, 0.5266666666666667, 0.2879444126014992, 0.0, 0.5202466557837572, 0.5, 0.0, 0.14551885386809918], 
reward next is 0.8045, 
noisyNet noise sample is [array([0.44101524], dtype=float32), -0.31930256]. 
=============================================
[2019-05-04 22:30:58,528] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0438701e-12 9.9990439e-01 1.6575592e-11 8.4011142e-15 8.4485043e-18
 1.1801785e-20 5.8429302e-14 8.1811719e-18 1.4571609e-14 6.1019786e-14
 9.5575459e-05], sum to 1.0000
[2019-05-04 22:30:58,529] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9105
[2019-05-04 22:30:58,621] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.1, 93.0, 3.3, 135.0, 90.0, 0.0, 8.95, -0.3549073155224873, 20.0, 23.12790342361529, 22.7, 1.0, 27.556856643391374], 
current ob forecast is [], 
actual action is [9.1, 20.0], 
sim time this is 916200.0000, 
sim time next is 917100.0000, 
raw observation next is [4.25, 93.0, 3.7, 137.5, 81.0, 0.0, 9.1, -0.3784163151668475, 20.0, 22.96049042737556, 22.7, 1.0, 21.472357416969164], 
processed observation next is [1.0, 0.6086956521739131, 0.4423076923076923, 0.93, 0.33636363636363636, 0.3819444444444444, 0.21428571428571427, 0.0, 0.6516666666666667, 0.37386122827771756, 0.0, 0.7086414896250801, 0.6714285714285714, 1.0, 0.2526159696114019], 
reward next is 0.6974, 
noisyNet noise sample is [array([-0.82042176], dtype=float32), -0.120767355]. 
=============================================
[2019-05-04 22:31:03,716] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7932331e-14 9.9996626e-01 1.8153000e-12 2.0846392e-17 7.1903826e-23
 3.0332919e-26 3.5156471e-19 6.6413461e-23 1.2619401e-17 1.1790068e-17
 3.3732718e-05], sum to 1.0000
[2019-05-04 22:31:03,718] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7720
[2019-05-04 22:31:03,740] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.7, 86.0, 7.2, 190.0, 123.5, 0.0, 17.575, -0.4230269736183541, 20.0, 22.06989773134235, 22.7, 1.0, 3.409379013109152], 
current ob forecast is [], 
actual action is [17.7, 20.0], 
sim time this is 997200.0000, 
sim time next is 998100.0000, 
raw observation next is [13.125, 84.75, 6.925000000000001, 197.5, 121.25, 0.0, 17.7, -0.4009333550512304, 20.0, 22.16696629206829, 22.7, 1.0, 2.9920863159005457], 
processed observation next is [1.0, 0.5652173913043478, 0.6698717948717948, 0.8475, 0.6295454545454546, 0.5486111111111112, 0.32076719576719576, 0.0, 0.795, 0.36635554831625655, 0.0, 0.5952808988668988, 0.6714285714285714, 1.0, 0.035201015481182894], 
reward next is 0.9148, 
noisyNet noise sample is [array([0.70189613], dtype=float32), -0.33053684]. 
=============================================
[2019-05-04 22:31:04,641] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9636456e-20 1.0000000e+00 4.9287009e-19 2.2426464e-20 4.6173500e-27
 9.0415652e-29 1.5160314e-22 3.4146357e-27 6.8951931e-22 2.2888836e-22
 3.2876393e-16], sum to 1.0000
[2019-05-04 22:31:04,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9895
[2019-05-04 22:31:04,655] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 80.0, 2.0, 60.0, 0.0, 0.0, 2.7, -0.9482595725435784, 20.0, 20.27955018641618, 21.5, 0.0, 5.480047753121734], 
current ob forecast is [], 
actual action is [2.7, 20.0], 
sim time this is 865800.0000, 
sim time next is 866700.0000, 
raw observation next is [-2.3, 80.0, 2.25, 60.0, 0.0, 0.0, 2.7, -0.9637895115654175, 20.0, 20.20869551284412, 21.5, 0.0, 5.741680084907908], 
processed observation next is [1.0, 0.0, 0.27435897435897433, 0.8, 0.20454545454545456, 0.16666666666666666, 0.0, 0.0, 0.545, 0.17873682947819416, 0.0, 0.31552793040630284, 0.5, 0.0, 0.0675491774695048], 
reward next is 0.8825, 
noisyNet noise sample is [array([0.17857409], dtype=float32), -1.2037668]. 
=============================================
[2019-05-04 22:31:07,641] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7062378e-23 1.0000000e+00 1.2868011e-22 2.1852564e-25 4.6014806e-31
 1.3501177e-34 5.7510974e-28 2.0046284e-35 6.4597780e-26 7.1643491e-28
 4.5552765e-20], sum to 1.0000
[2019-05-04 22:31:07,647] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7582
[2019-05-04 22:31:07,660] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 77.0, 5.35, 210.0, 0.0, 0.0, 19.4, -0.5575278244268416, 20.0, 21.01265825755545, 21.5, 0.0, 2.072396976873904], 
current ob forecast is [], 
actual action is [19.4, 20.0], 
sim time this is 1049400.0000, 
sim time next is 1050300.0000, 
raw observation next is [14.4, 77.0, 5.475, 210.0, 0.0, 0.0, 19.4, -0.5597715586367785, 20.0, 21.00065149402601, 21.5, 0.0, 2.094970899068243], 
processed observation next is [1.0, 0.13043478260869565, 0.7025641025641025, 0.77, 0.4977272727272727, 0.5833333333333334, 0.0, 0.0, 0.8233333333333334, 0.31340948045440714, 0.0, 0.42866449914657273, 0.5, 0.0, 0.024646716459626388], 
reward next is 0.9254, 
noisyNet noise sample is [array([-0.5291367], dtype=float32), -0.32497394]. 
=============================================
[2019-05-04 22:31:07,912] A3C_AGENT_WORKER-Thread-6 INFO:Local step 18500, global step 294639: loss 2.5326
[2019-05-04 22:31:07,917] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 18500, global step 294640: learning rate 0.0001
[2019-05-04 22:31:07,998] A3C_AGENT_WORKER-Thread-5 INFO:Local step 18500, global step 294677: loss 1.1119
[2019-05-04 22:31:07,999] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 18500, global step 294678: learning rate 0.0001
[2019-05-04 22:31:08,021] A3C_AGENT_WORKER-Thread-12 INFO:Local step 18500, global step 294690: loss 1.0575
[2019-05-04 22:31:08,027] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 18500, global step 294691: learning rate 0.0001
[2019-05-04 22:31:08,046] A3C_AGENT_WORKER-Thread-2 INFO:Local step 18500, global step 294700: loss 0.7055
[2019-05-04 22:31:08,047] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 18500, global step 294700: learning rate 0.0001
[2019-05-04 22:31:08,316] A3C_AGENT_WORKER-Thread-14 INFO:Local step 18500, global step 294820: loss 0.3127
[2019-05-04 22:31:08,316] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 18500, global step 294820: learning rate 0.0001
[2019-05-04 22:31:08,721] A3C_AGENT_WORKER-Thread-7 INFO:Local step 18500, global step 294997: loss 2.5020
[2019-05-04 22:31:08,725] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 18500, global step 294999: learning rate 0.0001
[2019-05-04 22:31:09,170] A3C_AGENT_WORKER-Thread-15 INFO:Local step 18500, global step 295172: loss 0.4149
[2019-05-04 22:31:09,173] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 18500, global step 295172: learning rate 0.0001
[2019-05-04 22:31:09,319] A3C_AGENT_WORKER-Thread-3 INFO:Local step 18500, global step 295232: loss 0.8214
[2019-05-04 22:31:09,320] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 18500, global step 295232: learning rate 0.0001
[2019-05-04 22:31:09,782] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1179736e-17 1.0000000e+00 4.7513339e-16 1.1903389e-19 1.2819633e-25
 3.1751143e-27 1.7498091e-20 1.7995993e-27 7.5967120e-19 8.4758116e-19
 4.3724410e-10], sum to 1.0000
[2019-05-04 22:31:09,783] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9699
[2019-05-04 22:31:09,799] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.45, 86.0, 6.4, 200.0, 128.0, 0.0, 17.325, -0.5956648812034516, 20.0, 21.3767992355728, 22.7, 1.0, 2.171114823646232], 
current ob forecast is [], 
actual action is [17.45, 20.0], 
sim time this is 995400.0000, 
sim time next is 996300.0000, 
raw observation next is [12.575, 86.0, 6.800000000000001, 195.0, 125.75, 0.0, 17.45, -0.5947780521311812, 20.0, 21.35518135724945, 22.7, 1.0, 1.91920234593702], 
processed observation next is [1.0, 0.5217391304347826, 0.6557692307692308, 0.86, 0.6181818181818183, 0.5416666666666666, 0.3326719576719577, 0.0, 0.7908333333333334, 0.30174064928960626, 0.0, 0.47931162246420705, 0.6714285714285714, 1.0, 0.022578851128670824], 
reward next is 0.7414, 
noisyNet noise sample is [array([-0.3432855], dtype=float32), 0.350738]. 
=============================================
[2019-05-04 22:31:09,806] A3C_AGENT_WORKER-Thread-18 INFO:Local step 18500, global step 295451: loss 1.4347
[2019-05-04 22:31:09,808] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 18500, global step 295451: learning rate 0.0001
[2019-05-04 22:31:10,741] A3C_AGENT_WORKER-Thread-13 INFO:Local step 18500, global step 295911: loss 1.2413
[2019-05-04 22:31:10,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 18500, global step 295911: learning rate 0.0001
[2019-05-04 22:31:11,416] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.0726741e-23 1.0000000e+00 1.5552901e-22 4.0471073e-23 7.2776560e-32
 4.0009743e-33 2.5967779e-26 3.0528408e-32 3.4495582e-24 9.2412818e-26
 1.5011705e-17], sum to 1.0000
[2019-05-04 22:31:11,419] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3918
[2019-05-04 22:31:11,445] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.05, 67.5, 3.85, 185.0, 254.0, 215.0, 20.775, -0.43436132521497, 20.0, 21.66214794396665, 22.7, 1.0, 0.6790785662592779], 
current ob forecast is [], 
actual action is [21.05, 20.0], 
sim time this is 1078200.0000, 
sim time next is 1079100.0000, 
raw observation next is [16.325, 66.25, 3.725, 192.5, 235.75, 240.5, 21.05, -0.419464445267643, 20.0, 21.72812326260244, 22.7, 1.0, 0.36337372688586894], 
processed observation next is [1.0, 0.4782608695652174, 0.7519230769230769, 0.6625, 0.3386363636363636, 0.5347222222222222, 0.6236772486772487, 0.2405, 0.8508333333333333, 0.360178518244119, 0.0, 0.5325890375146345, 0.6714285714285714, 1.0, 0.004274985022186694], 
reward next is 0.9457, 
noisyNet noise sample is [array([-0.0430441], dtype=float32), 0.8819118]. 
=============================================
[2019-05-04 22:31:12,131] A3C_AGENT_WORKER-Thread-8 INFO:Local step 18500, global step 296540: loss 0.8853
[2019-05-04 22:31:12,134] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 18500, global step 296540: learning rate 0.0001
[2019-05-04 22:31:13,175] A3C_AGENT_WORKER-Thread-19 INFO:Local step 18500, global step 297021: loss 1.0190
[2019-05-04 22:31:13,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 18500, global step 297021: learning rate 0.0001
[2019-05-04 22:31:13,537] A3C_AGENT_WORKER-Thread-4 INFO:Local step 18500, global step 297180: loss 0.0730
[2019-05-04 22:31:13,538] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 18500, global step 297180: learning rate 0.0001
[2019-05-04 22:31:16,051] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1181683e-11 9.9986017e-01 3.5772038e-11 8.4683948e-14 9.8922124e-18
 2.1809354e-19 1.4137385e-13 5.0926764e-18 2.4794142e-13 5.1987606e-14
 1.3984101e-04], sum to 1.0000
[2019-05-04 22:31:16,052] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6905
[2019-05-04 22:31:16,079] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.05, 86.0, 5.85, 205.0, 122.0, 0.0, 16.9, -0.636101389524796, 20.0, 21.2971629219857, 22.7, 1.0, 7.177362224622385], 
current ob forecast is [], 
actual action is [17.05, 20.0], 
sim time this is 992700.0000, 
sim time next is 993600.0000, 
raw observation next is [12.2, 86.0, 5.6, 210.0, 124.0, 0.0, 17.05, -0.6328291303636567, 20.0, 21.29535004221269, 22.7, 1.0, 6.09808560465526], 
processed observation next is [1.0, 0.5217391304347826, 0.6461538461538462, 0.86, 0.509090909090909, 0.5833333333333334, 0.328042328042328, 0.0, 0.7841666666666666, 0.2890569565454478, 0.0, 0.4707642917446699, 0.6714285714285714, 1.0, 0.07174218358417954], 
reward next is 0.5398, 
noisyNet noise sample is [array([-0.14961165], dtype=float32), 0.14326186]. 
=============================================
[2019-05-04 22:31:16,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.4082518e-14 1.0000000e+00 7.6616895e-14 6.8361811e-16 8.3696233e-19
 1.0288004e-21 3.2618307e-16 1.7743297e-20 1.3362343e-16 6.3937890e-16
 1.7374059e-11], sum to 1.0000
[2019-05-04 22:31:16,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3894
[2019-05-04 22:31:16,711] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.35, 92.0, 6.35, 255.0, 0.0, 0.0, 6.475, -0.8901568155459872, 20.0, 19.37351541357697, 21.5, 0.0, 8.044307003804713], 
current ob forecast is [], 
actual action is [6.35, 20.0], 
sim time this is 1319400.0000, 
sim time next is 1320300.0000, 
raw observation next is [1.225, 92.0, 6.475, 252.5, 0.0, 0.0, 6.35, -0.900980975407586, 20.0, 19.33364483893526, 21.5, 0.0, 8.111308309157323], 
processed observation next is [1.0, 0.2608695652173913, 0.3647435897435897, 0.92, 0.5886363636363636, 0.7013888888888888, 0.0, 0.0, 0.6058333333333333, 0.19967300819747133, 0.0, 0.1905206912764658, 0.5, 0.0, 0.09542715657832145], 
reward next is 0.8546, 
noisyNet noise sample is [array([1.7482733], dtype=float32), -0.70420116]. 
=============================================
[2019-05-04 22:31:17,726] A3C_AGENT_WORKER-Thread-17 INFO:Local step 18500, global step 298799: loss 0.4769
[2019-05-04 22:31:17,727] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 18500, global step 298799: learning rate 0.0001
[2019-05-04 22:31:17,825] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2995371e-16 1.0000000e+00 8.8287786e-15 5.1398422e-18 8.2637353e-24
 7.3086080e-28 1.1823469e-19 5.3265762e-26 3.7482138e-19 3.7265983e-20
 4.2624761e-11], sum to 1.0000
[2019-05-04 22:31:17,828] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9832
[2019-05-04 22:31:17,847] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.0, 100.0, 1.5, 300.0, 76.0, 0.0, 20.0, -0.4662723639948443, 20.0, 21.0379576606624, 22.7, 1.0, 1.2990431318811841], 
current ob forecast is [], 
actual action is [20.0, 20.0], 
sim time this is 1245600.0000, 
sim time next is 1246500.0000, 
raw observation next is [14.85, 100.0, 2.275, 302.5, 77.0, 0.0, 20.0, -0.4627259675694883, 20.0, 21.05359873882283, 22.7, 1.0, 1.2305762085221816], 
processed observation next is [0.0, 0.43478260869565216, 0.7141025641025641, 1.0, 0.20681818181818182, 0.8402777777777778, 0.2037037037037037, 0.0, 0.8333333333333334, 0.34575801081017055, 0.0, 0.43622839126040447, 0.6714285714285714, 1.0, 0.01447736715908449], 
reward next is 0.9355, 
noisyNet noise sample is [array([0.6600021], dtype=float32), 0.18125477]. 
=============================================
[2019-05-04 22:31:17,886] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[95.73419 ]
 [95.88757 ]
 [95.92236 ]
 [95.839676]
 [95.8911  ]], R is [[95.56350708]
 [95.54258728]
 [95.52093506]
 [95.49845123]
 [95.47506714]].
[2019-05-04 22:31:18,350] A3C_AGENT_WORKER-Thread-11 INFO:Local step 18500, global step 298966: loss 0.4182
[2019-05-04 22:31:18,351] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 18500, global step 298966: learning rate 0.0001
[2019-05-04 22:31:18,670] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2308960e-21 1.0000000e+00 6.0666858e-19 4.2579178e-22 1.4919992e-28
 2.7256957e-30 2.9264758e-23 2.1933279e-29 1.6837990e-24 1.3811938e-23
 7.5785352e-19], sum to 1.0000
[2019-05-04 22:31:18,670] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9018
[2019-05-04 22:31:18,684] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.0, 96.0, 4.35, 180.0, 0.0, 0.0, 20.0, -0.5534910546819866, 20.0, 20.57678620607572, 21.5, 0.0, 1.8427063375966781], 
current ob forecast is [], 
actual action is [20.0, 20.0], 
sim time this is 1236600.0000, 
sim time next is 1237500.0000, 
raw observation next is [15.0, 96.0, 4.475, 215.0, 0.0, 0.0, 20.0, -0.5544371065804352, 20.0, 20.56907712903595, 21.5, 0.0, 1.844454627237921], 
processed observation next is [0.0, 0.30434782608695654, 0.717948717948718, 0.96, 0.4068181818181818, 0.5972222222222222, 0.0, 0.0, 0.8333333333333334, 0.31518763113985493, 0.0, 0.36701101843370715, 0.5, 0.0, 0.02169946620279907], 
reward next is 0.9283, 
noisyNet noise sample is [array([-1.3960917], dtype=float32), 1.2678934]. 
=============================================
[2019-05-04 22:31:18,703] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[92.06431 ]
 [92.32512 ]
 [92.625275]
 [92.6819  ]
 [92.702965]], R is [[91.66281128]
 [91.67450714]
 [91.6860733 ]
 [91.69749451]
 [91.70880127]].
[2019-05-04 22:31:18,807] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8349383e-20 1.0000000e+00 4.2124395e-22 1.7002468e-22 7.9292356e-30
 3.4898599e-32 3.2505528e-23 1.2265216e-29 2.5353989e-23 4.4672094e-25
 5.2548245e-20], sum to 1.0000
[2019-05-04 22:31:18,808] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8182
[2019-05-04 22:31:18,838] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.5, 77.0, 4.6, 140.0, 0.0, 0.0, 15.775, -0.4803052992418209, 20.0, 21.23647524600155, 21.5, 0.0, 2.814099766277201], 
current ob forecast is [], 
actual action is [15.5, 20.0], 
sim time this is 1126800.0000, 
sim time next is 1127700.0000, 
raw observation next is [10.375, 77.5, 4.6, 135.0, 0.0, 0.0, 15.5, -0.4878605414459329, 20.0, 21.20406664322109, 21.5, 0.0, 2.9089764177341917], 
processed observation next is [0.0, 0.043478260869565216, 0.5993589743589743, 0.775, 0.41818181818181815, 0.375, 0.0, 0.0, 0.7583333333333333, 0.3373798195180224, 0.0, 0.4577238061744414, 0.5, 0.0, 0.03422325197334343], 
reward next is 0.9158, 
noisyNet noise sample is [array([0.53719985], dtype=float32), 1.1986326]. 
=============================================
[2019-05-04 22:31:19,291] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8139768e-14 1.0000000e+00 2.5798506e-14 5.7246695e-16 7.0967573e-20
 3.2864221e-24 2.5388909e-17 5.9790986e-21 1.1375476e-17 3.3958843e-16
 1.4832913e-09], sum to 1.0000
[2019-05-04 22:31:19,291] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3283
[2019-05-04 22:31:19,319] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 6.1, 280.0, 73.5, 0.0, 5.5, -0.6108926626505734, 20.0, 21.0568595137775, 22.7, 1.0, 10.697088613897938], 
current ob forecast is [], 
actual action is [5.5, 20.0], 
sim time this is 1332000.0000, 
sim time next is 1332900.0000, 
raw observation next is [0.65, 92.0, 5.475, 282.5, 87.75, 0.0, 5.5, -0.613824093410653, 20.0, 21.04062824478238, 22.7, 1.0, 9.095374153499328], 
processed observation next is [1.0, 0.43478260869565216, 0.35000000000000003, 0.92, 0.4977272727272727, 0.7847222222222222, 0.23214285714285715, 0.0, 0.5916666666666667, 0.29539196886311564, 0.0, 0.4343754635403399, 0.6714285714285714, 1.0, 0.10700440180587445], 
reward next is 0.6085, 
noisyNet noise sample is [array([-0.15105885], dtype=float32), -0.63591796]. 
=============================================
[2019-05-04 22:31:19,556] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.26483312e-19 1.00000000e+00 4.53714323e-17 1.61154848e-23
 5.62580796e-28 1.04126597e-27 1.02179115e-22 1.05842188e-28
 1.12742652e-23 1.94643619e-23 8.14361846e-19], sum to 1.0000
[2019-05-04 22:31:19,568] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7632
[2019-05-04 22:31:19,588] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.8, 100.0, 4.1, 345.0, 81.5, 0.0, 18.8, -0.452826591542986, 20.0, 21.02231815939959, 22.7, 1.0, 1.2104406176352656], 
current ob forecast is [], 
actual action is [18.8, 20.0], 
sim time this is 1260900.0000, 
sim time next is 1261800.0000, 
raw observation next is [13.8, 100.0, 4.1, 340.0, 77.0, 0.0, 18.8, -0.4508029585016834, 20.0, 21.03143791222429, 22.7, 1.0, 1.2350046031644517], 
processed observation next is [0.0, 0.6086956521739131, 0.6871794871794872, 1.0, 0.3727272727272727, 0.9444444444444444, 0.2037037037037037, 0.0, 0.8133333333333332, 0.3497323471661056, 0.0, 0.43306255888918443, 0.6714285714285714, 1.0, 0.014529465919581784], 
reward next is 0.9355, 
noisyNet noise sample is [array([0.6236447], dtype=float32), -1.4939054]. 
=============================================
[2019-05-04 22:31:20,217] A3C_AGENT_WORKER-Thread-16 INFO:Local step 18500, global step 299618: loss 0.5041
[2019-05-04 22:31:20,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 18500, global step 299618: learning rate 0.0001
[2019-05-04 22:31:21,767] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-05-04 22:31:21,769] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation job starts!
[2019-05-04 22:31:21,770] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation job starts!
[2019-05-04 22:31:21,772] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:31:21,772] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:31:21,776] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res1/Eplus-env-sub_run7
[2019-05-04 22:31:21,793] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Test-Repeat-Real97-v1-res1/Eplus-env-sub_run7
[2019-05-04 22:32:56,863] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation: average rewards by now are 4595.7612 73651.5758 272758.3191
[2019-05-04 22:32:56,873] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:32:56,873] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:32:56,873] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:32:56,873] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:32:56,873] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:32:56,873] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:32:56,873] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:32:56,984] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:32:56,984] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:32:56,984] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:32:56,984] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:32:56,984] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:32:56,984] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:32:56,984] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:34:55,185] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation: average rewards by now are 5538.5561 148025.8893 -2555.3968
[2019-05-04 22:34:55,197] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:34:55,197] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:34:55,197] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:34:55,197] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:34:55,197] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:34:55,197] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:34:55,197] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-05-04 22:34:55,313] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:34:55,313] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:34:55,313] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:34:55,313] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:34:55,313] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:34:55,313] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:34:55,313] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-05-04 22:34:56,199] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 300000, evaluation results [300000.0, 5538.5560787718605, 148025.88925359788, -2555.396843440928, 4595.761158783212, 73651.57584369082, 272758.3191094142]
[2019-05-04 22:34:59,999] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5263981e-09 1.6093215e-02 8.7295672e-07 2.2810320e-11 6.5825973e-14
 1.2073799e-15 4.1513928e-11 1.2755690e-14 5.2127167e-11 2.4641641e-11
 9.8390597e-01], sum to 1.0000
[2019-05-04 22:34:59,999] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7993
[2019-05-04 22:35:00,207] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 100.0, 2.375, 275.0, 13.5, 0.0, 4.4, -0.6455626810688201, 20.0, 21.29062823059778, 22.7, 1.0, 46.126805324485815], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 1412100.0000, 
sim time next is 1413000.0000, 
raw observation next is [-0.6, 100.0, 2.25, 190.0, 18.0, 0.0, 4.4, -0.6059610108988155, 65.0, 21.39265318645261, 22.7, 1.0, 71.1700536974869], 
processed observation next is [1.0, 0.34782608695652173, 0.317948717948718, 1.0, 0.20454545454545456, 0.5277777777777778, 0.047619047619047616, 0.0, 0.5733333333333334, 0.2980129963670615, 1.0, 0.4846647409218013, 0.6714285714285714, 1.0, 0.8372947493821989], 
reward next is 0.4589, 
noisyNet noise sample is [array([0.33470473], dtype=float32), 1.0003546]. 
=============================================
[2019-05-04 22:35:00,263] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[59.265133]
 [58.739426]
 [58.394283]
 [58.41986 ]
 [57.78044 ]], R is [[60.15827942]
 [60.1671257 ]
 [60.07499313]
 [60.47424316]
 [60.86950302]].
[2019-05-04 22:35:03,360] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6998231e-17 1.0000000e+00 2.1034949e-20 3.1214060e-21 3.1542551e-26
 2.7500423e-29 7.7123879e-22 1.3243529e-26 1.2111417e-21 2.7332541e-23
 2.4131014e-15], sum to 1.0000
[2019-05-04 22:35:03,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8928
[2019-05-04 22:35:03,394] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 4.1, 90.0, 32.0, 0.0, 6.1, -0.5142492121924984, 20.0, 21.80883334228369, 22.7, 1.0, 3.2745283814972184], 
current ob forecast is [], 
actual action is [6.1, 20.0], 
sim time this is 1440000.0000, 
sim time next is 1440900.0000, 
raw observation next is [1.1, 92.0, 3.975, 90.0, 25.0, 0.0, 6.1, -0.5202240069761715, 20.0, 21.65223204539776, 22.7, 1.0, 3.10854881530618], 
processed observation next is [1.0, 0.6956521739130435, 0.36153846153846153, 0.92, 0.3613636363636364, 0.25, 0.06613756613756613, 0.0, 0.6016666666666667, 0.3265919976746095, 0.0, 0.5217474350568229, 0.6714285714285714, 1.0, 0.03657116253301389], 
reward next is 0.9120, 
noisyNet noise sample is [array([-2.5746706], dtype=float32), 1.2166495]. 
=============================================
[2019-05-04 22:35:06,893] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3836354e-10 9.9125624e-01 5.1806334e-11 5.0407259e-13 1.5422007e-17
 4.2159289e-20 1.4151512e-14 1.2607974e-18 1.3759101e-12 5.2132495e-13
 8.7437769e-03], sum to 1.0000
[2019-05-04 22:35:06,893] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5950
[2019-05-04 22:35:06,940] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 3.0, 340.0, 93.0, 0.0, 5.375, -0.548413376261384, 20.0, 21.51792389125603, 22.7, 1.0, 9.31622841454985], 
current ob forecast is [], 
actual action is [5.5, 20.0], 
sim time this is 1429200.0000, 
sim time next is 1430100.0000, 
raw observation next is [0.65, 92.0, 3.0, 265.0, 91.5, 0.0, 5.5, -0.5627114274889002, 20.0, 21.47199821046264, 22.7, 1.0, 7.479871575746717], 
processed observation next is [1.0, 0.5652173913043478, 0.35000000000000003, 0.92, 0.2727272727272727, 0.7361111111111112, 0.24206349206349206, 0.0, 0.5916666666666667, 0.31242952417036657, 0.0, 0.49599974435180577, 0.6714285714285714, 1.0, 0.08799848912643196], 
reward next is 0.8070, 
noisyNet noise sample is [array([0.80803746], dtype=float32), 0.5281128]. 
=============================================
[2019-05-04 22:35:08,266] A3C_AGENT_WORKER-Thread-6 INFO:Local step 19000, global step 302851: loss 0.2672
[2019-05-04 22:35:08,271] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 19000, global step 302853: learning rate 0.0001
[2019-05-04 22:35:08,378] A3C_AGENT_WORKER-Thread-2 INFO:Local step 19000, global step 302898: loss 0.1393
[2019-05-04 22:35:08,381] A3C_AGENT_WORKER-Thread-12 INFO:Local step 19000, global step 302898: loss 0.1543
[2019-05-04 22:35:08,382] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 19000, global step 302898: learning rate 0.0001
[2019-05-04 22:35:08,383] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 19000, global step 302898: learning rate 0.0001
[2019-05-04 22:35:08,427] A3C_AGENT_WORKER-Thread-5 INFO:Local step 19000, global step 302909: loss 0.0849
[2019-05-04 22:35:08,428] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 19000, global step 302909: learning rate 0.0001
[2019-05-04 22:35:08,456] A3C_AGENT_WORKER-Thread-14 INFO:Local step 19000, global step 302922: loss 0.0611
[2019-05-04 22:35:08,458] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 19000, global step 302922: learning rate 0.0001
[2019-05-04 22:35:08,659] A3C_AGENT_WORKER-Thread-7 INFO:Local step 19000, global step 303008: loss 0.0618
[2019-05-04 22:35:08,665] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 19000, global step 303010: learning rate 0.0001
[2019-05-04 22:35:09,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.5521227e-15 1.0000000e+00 2.1637772e-13 2.1286356e-16 2.1790282e-19
 5.1032027e-21 2.4875723e-17 7.4082501e-22 3.4075909e-15 1.2679338e-15
 3.4546808e-09], sum to 1.0000
[2019-05-04 22:35:09,487] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5762
[2019-05-04 22:35:09,507] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 0.0, 0.0, 6.6, -0.7891529600708688, 20.0, 20.24013839801835, 21.5, 0.0, 2.9400458676916075], 
current ob forecast is [], 
actual action is [6.6, 20.0], 
sim time this is 1468800.0000, 
sim time next is 1469700.0000, 
raw observation next is [1.6, 92.0, 0.625, 70.0, 0.0, 0.0, 6.6, -0.798345141112807, 20.0, 20.19572121111266, 21.5, 0.0, 2.931975565630761], 
processed observation next is [1.0, 0.0, 0.37435897435897436, 0.92, 0.056818181818181816, 0.19444444444444445, 0.0, 0.0, 0.61, 0.23388495296239767, 0.0, 0.3136744587303798, 0.5, 0.0, 0.03449383018389131], 
reward next is 0.9155, 
noisyNet noise sample is [array([-0.980673], dtype=float32), 1.2095594]. 
=============================================
[2019-05-04 22:35:10,485] A3C_AGENT_WORKER-Thread-15 INFO:Local step 19000, global step 303488: loss 0.2797
[2019-05-04 22:35:10,487] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 19000, global step 303488: learning rate 0.0001
[2019-05-04 22:35:10,544] A3C_AGENT_WORKER-Thread-3 INFO:Local step 19000, global step 303502: loss 0.0765
[2019-05-04 22:35:10,546] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 19000, global step 303502: learning rate 0.0001
[2019-05-04 22:35:11,678] A3C_AGENT_WORKER-Thread-18 INFO:Local step 19000, global step 303832: loss 0.1710
[2019-05-04 22:35:11,679] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 19000, global step 303832: learning rate 0.0001
[2019-05-04 22:35:12,822] A3C_AGENT_WORKER-Thread-13 INFO:Local step 19000, global step 304202: loss 0.0835
[2019-05-04 22:35:12,824] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 19000, global step 304203: learning rate 0.0001
[2019-05-04 22:35:13,342] A3C_AGENT_WORKER-Thread-8 INFO:Local step 19000, global step 304420: loss 0.2842
[2019-05-04 22:35:13,348] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 19000, global step 304420: learning rate 0.0001
[2019-05-04 22:35:14,063] A3C_AGENT_WORKER-Thread-19 INFO:Local step 19000, global step 304728: loss 0.0490
[2019-05-04 22:35:14,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 19000, global step 304728: learning rate 0.0001
[2019-05-04 22:35:15,021] A3C_AGENT_WORKER-Thread-4 INFO:Local step 19000, global step 304997: loss 0.1585
[2019-05-04 22:35:15,022] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 19000, global step 304998: learning rate 0.0001
[2019-05-04 22:35:19,479] A3C_AGENT_WORKER-Thread-17 INFO:Local step 19000, global step 305906: loss 0.4307
[2019-05-04 22:35:19,480] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 19000, global step 305906: learning rate 0.0001
[2019-05-04 22:35:19,824] A3C_AGENT_WORKER-Thread-11 INFO:Local step 19000, global step 305985: loss 0.1778
[2019-05-04 22:35:19,828] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 19000, global step 305985: learning rate 0.0001
[2019-05-04 22:35:23,214] A3C_AGENT_WORKER-Thread-16 INFO:Local step 19000, global step 306924: loss 0.1699
[2019-05-04 22:35:23,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 19000, global step 306926: learning rate 0.0001
[2019-05-04 22:35:28,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9354326e-13 1.0000000e+00 5.7576360e-14 4.9279991e-14 2.8085048e-18
 8.7747949e-21 1.5279323e-16 8.9011923e-20 5.0550487e-16 4.7039780e-15
 1.7271690e-12], sum to 1.0000
[2019-05-04 22:35:28,194] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2451
[2019-05-04 22:35:28,220] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 86.0, 8.575, 242.5, 0.0, 0.0, 0.0, -0.9052447900371859, 20.0, 20.17432456018803, 21.5, 0.0, 8.488994272148407], 
current ob forecast is [], 
actual action is [0.0, 20.0], 
sim time this is 1804500.0000, 
sim time next is 1805400.0000, 
raw observation next is [-5.0, 86.0, 8.45, 245.0, 0.0, 0.0, 0.0, -0.9280716728246952, 20.0, 20.08097597565006, 21.5, 0.0, 7.561042866308419], 
processed observation next is [0.0, 0.9130434782608695, 0.20512820512820512, 0.86, 0.7681818181818181, 0.6805555555555556, 0.0, 0.0, 0.5, 0.1906427757251016, 0.0, 0.29728228223572273, 0.5, 0.0, 0.0889534454859814], 
reward next is 0.8610, 
noisyNet noise sample is [array([0.47328928], dtype=float32), 1.6360036]. 
=============================================
[2019-05-04 22:35:30,798] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3847696e-07 8.2412732e-01 1.0508728e-06 2.9006895e-08 3.5599978e-11
 6.0641722e-12 4.9330627e-08 3.5469711e-11 2.2383764e-09 6.1449863e-09
 1.7587142e-01], sum to 1.0000
[2019-05-04 22:35:30,798] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4453
[2019-05-04 22:35:30,925] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.25, 87.0, 9.575, 250.0, 56.5, 0.0, 1.9, -0.5265834318592598, 65.0, 21.82474878152327, 22.7, 1.0, 74.93256462980436], 
current ob forecast is [], 
actual action is [1.75, 20.0], 
sim time this is 1784700.0000, 
sim time next is 1785600.0000, 
raw observation next is [-3.4, 87.0, 9.7, 250.0, 47.0, 0.0, 1.75, -0.5144015264390579, 20.0, 22.09741877161738, 22.7, 1.0, 43.53983251541353], 
processed observation next is [0.0, 0.6956521739130435, 0.24615384615384614, 0.87, 0.8818181818181817, 0.6944444444444444, 0.12433862433862433, 0.0, 0.5291666666666667, 0.328532824520314, 0.0, 0.585345538802483, 0.6714285714285714, 1.0, 0.5122333237107474], 
reward next is 0.4783, 
noisyNet noise sample is [array([1.1492664], dtype=float32), 0.97806567]. 
=============================================
[2019-05-04 22:35:39,929] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.6815480e-06 5.6915033e-01 4.6135588e-06 8.6967236e-07 2.9633924e-09
 1.6692214e-11 7.5923680e-08 3.0245201e-10 1.3921593e-07 4.7188554e-08
 4.3083927e-01], sum to 1.0000
[2019-05-04 22:35:39,929] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5813
[2019-05-04 22:35:40,067] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 71.0, 7.825, 240.0, 174.0, 51.0, 0.5, -0.5861997174848728, 65.0, 21.95865312567053, 22.7, 1.0, 69.18916443389817], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 1863900.0000, 
sim time next is 1864800.0000, 
raw observation next is [-4.5, 71.0, 7.7, 240.0, 178.0, 62.0, 0.5, -0.5649724071473533, 65.0, 22.05579309803802, 22.7, 1.0, 68.9739696393952], 
processed observation next is [0.0, 0.6086956521739131, 0.21794871794871795, 0.71, 0.7000000000000001, 0.6666666666666666, 0.4708994708994709, 0.062, 0.5083333333333333, 0.31167586428421556, 1.0, 0.5793990140054314, 0.6714285714285714, 1.0, 0.8114584663458257], 
reward next is 0.2883, 
noisyNet noise sample is [array([2.0909772], dtype=float32), 0.49328637]. 
=============================================
[2019-05-04 22:35:47,704] A3C_AGENT_WORKER-Thread-6 INFO:Local step 19500, global step 311050: loss 0.2352
[2019-05-04 22:35:47,705] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 19500, global step 311050: learning rate 0.0001
[2019-05-04 22:35:47,788] A3C_AGENT_WORKER-Thread-7 INFO:Local step 19500, global step 311066: loss 0.2036
[2019-05-04 22:35:47,796] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 19500, global step 311066: learning rate 0.0001
[2019-05-04 22:35:47,870] A3C_AGENT_WORKER-Thread-5 INFO:Local step 19500, global step 311083: loss 0.5703
[2019-05-04 22:35:47,875] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 19500, global step 311083: learning rate 0.0001
[2019-05-04 22:35:47,926] A3C_AGENT_WORKER-Thread-12 INFO:Local step 19500, global step 311092: loss 0.3091
[2019-05-04 22:35:47,926] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 19500, global step 311092: learning rate 0.0001
[2019-05-04 22:35:48,464] A3C_AGENT_WORKER-Thread-14 INFO:Local step 19500, global step 311202: loss 0.1144
[2019-05-04 22:35:48,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 19500, global step 311202: learning rate 0.0001
[2019-05-04 22:35:48,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1487050e-08 9.9825031e-01 7.2247959e-09 5.3137406e-10 3.6099368e-13
 1.7153151e-14 9.8313635e-11 9.0686041e-15 2.9692013e-11 2.6183724e-11
 1.7496119e-03], sum to 1.0000
[2019-05-04 22:35:48,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2836
[2019-05-04 22:35:48,773] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 75.0, 6.1, 240.0, 50.5, 0.0, 0.5, -0.5163664490986112, 65.0, 22.27946620450902, 22.7, 1.0, 71.39912573926996], 
current ob forecast is [], 
actual action is [0.5, 20.0], 
sim time this is 1872000.0000, 
sim time next is 1872900.0000, 
raw observation next is [-4.5, 77.0, 5.85, 237.5, 39.75, 0.0, 0.5, -0.5174493945628608, 20.0, 22.39237853947737, 22.7, 1.0, 44.19222029465061], 
processed observation next is [0.0, 0.6956521739130435, 0.21794871794871795, 0.77, 0.5318181818181817, 0.6597222222222222, 0.10515873015873016, 0.0, 0.5083333333333333, 0.32751686847904643, 0.0, 0.6274826484967672, 0.6714285714285714, 1.0, 0.5199084740547131], 
reward next is 0.4722, 
noisyNet noise sample is [array([-1.0123206], dtype=float32), 0.26146793]. 
=============================================
[2019-05-04 22:35:49,181] A3C_AGENT_WORKER-Thread-2 INFO:Local step 19500, global step 311294: loss 0.1224
[2019-05-04 22:35:49,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 19500, global step 311294: learning rate 0.0001
[2019-05-04 22:35:49,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1041284e-09 1.5446024e-04 7.5597363e-08 2.4575730e-10 1.6289591e-12
 1.6823338e-14 1.8623587e-10 1.6228490e-13 7.1887205e-11 8.5908010e-11
 9.9984539e-01], sum to 1.0000
[2019-05-04 22:35:49,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8859
[2019-05-04 22:35:49,947] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.2, 87.0, 2.75, 260.0, 0.0, 0.0, -1.2, -1.232458419104162, 20.0, 18.74139322123492, 21.5, 0.0, 9.825588476316137], 
current ob forecast is [], 
actual action is [-1.2000000000000002, 65.0], 
sim time this is 2007000.0000, 
sim time next is 2007900.0000, 
raw observation next is [-6.2, 87.0, 2.875, 260.0, 0.0, 0.0, -1.2, -1.13678125369635, 65.0, 18.82971327360313, 21.5, 0.0, 81.03218352761044], 
processed observation next is [1.0, 0.21739130434782608, 0.17435897435897435, 0.87, 0.26136363636363635, 0.7222222222222222, 0.0, 0.0, 0.48000000000000004, 0.12107291543455001, 1.0, 0.11853046765758991, 0.5, 0.0, 0.9533198062071817], 
reward next is 0.3406, 
noisyNet noise sample is [array([1.4282855], dtype=float32), 2.1349149]. 
=============================================
[2019-05-04 22:35:51,409] A3C_AGENT_WORKER-Thread-15 INFO:Local step 19500, global step 311568: loss 0.3041
[2019-05-04 22:35:51,409] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 19500, global step 311568: learning rate 0.0001
[2019-05-04 22:35:52,814] A3C_AGENT_WORKER-Thread-3 INFO:Local step 19500, global step 311776: loss 0.1656
[2019-05-04 22:35:52,827] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 19500, global step 311776: learning rate 0.0001
[2019-05-04 22:35:53,492] A3C_AGENT_WORKER-Thread-18 INFO:Local step 19500, global step 311879: loss 0.1360
[2019-05-04 22:35:53,503] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 19500, global step 311880: learning rate 0.0001
[2019-05-04 22:35:53,866] A3C_AGENT_WORKER-Thread-13 INFO:Local step 19500, global step 311924: loss 0.0902
[2019-05-04 22:35:53,874] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 19500, global step 311924: learning rate 0.0001
[2019-05-04 22:35:55,697] A3C_AGENT_WORKER-Thread-8 INFO:Local step 19500, global step 312165: loss 0.1070
[2019-05-04 22:35:55,718] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 19500, global step 312165: learning rate 0.0001
[2019-05-04 22:35:56,383] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1844381e-06 1.0607212e-01 1.1345422e-06 1.2815082e-07 2.9410499e-10
 2.7722221e-12 4.3542507e-09 1.5908166e-10 2.0217398e-08 6.3341172e-09
 8.9392543e-01], sum to 1.0000
[2019-05-04 22:35:56,388] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4816
[2019-05-04 22:35:56,431] A3C_AGENT_WORKER-Thread-19 INFO:Local step 19500, global step 312277: loss 0.1648
[2019-05-04 22:35:56,433] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 19500, global step 312277: learning rate 0.0001
[2019-05-04 22:35:56,537] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 79.0, 4.6, 260.0, 152.0, 0.0, 0.5, -0.4695195780722378, 65.0, 22.15481261617854, 22.7, 1.0, 73.61434526082463], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2034000.0000, 
sim time next is 2034900.0000, 
raw observation next is [-4.35, 79.0, 4.975, 260.0, 150.0, 0.0, 0.5, -0.387779290857991, 65.0, 22.69388586465647, 22.7, 1.0, 69.41153675142935], 
processed observation next is [1.0, 0.5652173913043478, 0.2217948717948718, 0.79, 0.4522727272727272, 0.7222222222222222, 0.3968253968253968, 0.0, 0.5083333333333333, 0.3707402363806696, 1.0, 0.6705551235223527, 0.6714285714285714, 1.0, 0.8166063147226983], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5449929], dtype=float32), -0.15907358]. 
=============================================
[2019-05-04 22:35:57,106] A3C_AGENT_WORKER-Thread-4 INFO:Local step 19500, global step 312367: loss 0.2274
[2019-05-04 22:35:57,106] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 19500, global step 312367: learning rate 0.0001
[2019-05-04 22:35:59,232] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.7650688e-07 2.8723241e-03 4.8407619e-06 1.1561443e-07 1.4973785e-10
 5.1164524e-12 2.3636340e-09 1.0156589e-11 9.9873239e-09 6.2461680e-08
 9.9712235e-01], sum to 1.0000
[2019-05-04 22:35:59,233] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7541
[2019-05-04 22:35:59,365] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.324999999999999, 81.0, 4.975, 255.0, 135.75, 0.0, -0.5999999999999996, -0.5633256342557272, 20.0, 22.09920781346591, 22.7, 1.0, 41.98184352139009], 
current ob forecast is [], 
actual action is [-0.3249999999999993, 65.0], 
sim time this is 2027700.0000, 
sim time next is 2028600.0000, 
raw observation next is [-5.05, 79.0, 4.85, 260.0, 147.0, 0.0, -0.3249999999999993, -0.5125205992617309, 65.0, 22.26016270233391, 22.7, 1.0, 68.68716921597645], 
processed observation next is [1.0, 0.4782608695652174, 0.20384615384615384, 0.79, 0.44090909090909086, 0.7222222222222222, 0.3888888888888889, 0.0, 0.4945833333333333, 0.3291598002460897, 1.0, 0.6085946717619873, 0.6714285714285714, 1.0, 0.80808434371737], 
reward next is 0.0626, 
noisyNet noise sample is [array([0.8959241], dtype=float32), 0.5417012]. 
=============================================
[2019-05-04 22:36:00,991] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0863247e-09 5.8056335e-06 1.6567678e-09 5.3723292e-10 1.6878284e-12
 1.0324702e-14 2.0998285e-11 3.0389444e-13 6.9296027e-11 7.8160135e-12
 9.9999416e-01], sum to 1.0000
[2019-05-04 22:36:00,991] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3161
[2019-05-04 22:36:01,127] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.55, 80.5, 2.25, 230.0, 72.0, 37.0, -2.425, -0.7829992831324265, 65.0, 21.04094935726801, 22.7, 1.0, 73.45454645993853], 
current ob forecast is [], 
actual action is [-2.55, 65.0], 
sim time this is 2104200.0000, 
sim time next is 2105100.0000, 
raw observation next is [-7.675, 81.25, 2.375, 230.0, 97.5, 57.25, -2.55, -0.7541463357190477, 65.0, 21.16747785471554, 22.7, 1.0, 72.4551905912234], 
processed observation next is [1.0, 0.34782608695652173, 0.13653846153846155, 0.8125, 0.2159090909090909, 0.6388888888888888, 0.25793650793650796, 0.05725, 0.45749999999999996, 0.2486178880936508, 1.0, 0.45249683638793414, 0.6714285714285714, 1.0, 0.8524140069555693], 
reward next is 0.2885, 
noisyNet noise sample is [array([-1.0965778], dtype=float32), -0.3280073]. 
=============================================
[2019-05-04 22:36:02,798] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6551245e-13 1.0000000e+00 3.2647734e-13 3.6133439e-15 6.0938495e-18
 4.9307671e-20 6.7775307e-14 4.0776927e-18 6.0829339e-14 2.3115589e-13
 1.2882996e-10], sum to 1.0000
[2019-05-04 22:36:02,798] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1801
[2019-05-04 22:36:02,877] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 86.0, 3.975, 267.5, 0.0, 0.0, 1.1, -0.3415486115650826, 20.0, 22.6076092535025, 21.5, 0.0, 26.502523844769936], 
current ob forecast is [], 
actual action is [1.1, 20.0], 
sim time this is 2060100.0000, 
sim time next is 2061000.0000, 
raw observation next is [-3.9, 86.0, 3.85, 265.0, 0.0, 0.0, 1.1, -0.3872550515372459, 20.0, 22.31874936917164, 21.5, 0.0, 21.795078334812626], 
processed observation next is [1.0, 0.8695652173913043, 0.23333333333333334, 0.86, 0.35000000000000003, 0.7361111111111112, 0.0, 0.0, 0.5183333333333333, 0.370914982820918, 0.0, 0.6169641955959487, 0.5, 0.0, 0.25641268629191327], 
reward next is 0.6936, 
noisyNet noise sample is [array([0.10926707], dtype=float32), -0.52853656]. 
=============================================
[2019-05-04 22:36:02,920] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[55.924927]
 [49.152363]
 [49.52499 ]
 [48.747746]
 [49.023075]], R is [[58.12158966]
 [58.17858124]
 [58.06281281]
 [57.48218536]
 [57.58403397]].
[2019-05-04 22:36:03,115] A3C_AGENT_WORKER-Thread-17 INFO:Local step 19500, global step 313231: loss 0.0366
[2019-05-04 22:36:03,116] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 19500, global step 313231: learning rate 0.0001
[2019-05-04 22:36:03,706] A3C_AGENT_WORKER-Thread-11 INFO:Local step 19500, global step 313346: loss 0.1294
[2019-05-04 22:36:03,707] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 19500, global step 313346: learning rate 0.0001
[2019-05-04 22:36:05,658] A3C_AGENT_WORKER-Thread-16 INFO:Local step 19500, global step 313755: loss 0.4462
[2019-05-04 22:36:05,661] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 19500, global step 313755: learning rate 0.0001
[2019-05-04 22:36:09,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.08842066e-16 1.09435030e-14 2.79413638e-15 4.93579664e-18
 8.27396778e-21 2.04951263e-22 3.14397108e-19 1.64811168e-22
 5.34854415e-19 1.11840816e-18 1.00000000e+00], sum to 1.0000
[2019-05-04 22:36:09,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7571
[2019-05-04 22:36:10,113] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 75.0, 3.325, 252.5, 0.0, 0.0, -0.5999999999999996, -1.150221283202295, 20.0, 19.23377539075048, 22.7, 1.0, 41.68090367650296], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 65.0], 
sim time this is 2186100.0000, 
sim time next is 2187000.0000, 
raw observation next is [-5.6, 75.0, 3.05, 255.0, 0.0, 0.0, -0.5999999999999996, -1.077602298947018, 65.0, 19.37093601480285, 22.7, 1.0, 89.42617058555943], 
processed observation next is [1.0, 0.30434782608695654, 0.18974358974358976, 0.75, 0.2772727272727273, 0.7083333333333334, 0.0, 0.0, 0.49, 0.14079923368432734, 1.0, 0.19584800211469297, 0.6714285714285714, 1.0, 1.0520725951242287], 
reward next is 0.7262, 
noisyNet noise sample is [array([0.16935794], dtype=float32), 1.0815215]. 
=============================================
[2019-05-04 22:36:10,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[55.959602]
 [54.656624]
 [54.39377 ]
 [53.99505 ]
 [54.120125]], R is [[56.48190308]
 [55.91708374]
 [55.80842209]
 [55.25033951]
 [55.27078629]].
[2019-05-04 22:36:15,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5801165e-08 6.3869536e-01 2.2003526e-09 1.3765036e-10 2.4223874e-13
 1.4144592e-14 1.0170432e-11 1.5493465e-13 1.2700892e-10 6.2275789e-11
 3.6130470e-01], sum to 1.0000
[2019-05-04 22:36:15,379] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.5395614e-12 9.9999976e-01 2.0733625e-11 5.3003110e-13 1.9684402e-16
 4.1888116e-18 3.0961464e-13 4.3564724e-18 1.4400751e-13 7.4328910e-14
 2.6896535e-07], sum to 1.0000
[2019-05-04 22:36:15,387] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2789
[2019-05-04 22:36:15,382] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2416
[2019-05-04 22:36:15,489] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.55, 69.0, 6.4, 275.0, 0.0, 0.0, 0.4749999999999996, -0.5381297267146277, 20.0, 21.71906157686495, 22.7, 1.0, 42.597150472494825], 
current ob forecast is [], 
actual action is [0.4500000000000002, 20.0], 
sim time this is 2226600.0000, 
sim time next is 2227500.0000, 
raw observation next is [-4.574999999999999, 69.5, 6.550000000000001, 272.5, 0.0, 0.0, 0.4500000000000002, -0.5711301227838751, 20.0, 21.61954556839367, 22.7, 1.0, 30.88258948516138], 
processed observation next is [1.0, 0.782608695652174, 0.21602564102564104, 0.695, 0.5954545454545456, 0.7569444444444444, 0.0, 0.0, 0.5075, 0.30962329240537495, 0.0, 0.5170779383419527, 0.6714285714285714, 1.0, 0.36332458217836916], 
reward next is 0.6072, 
noisyNet noise sample is [array([-0.6806322], dtype=float32), -1.4108161]. 
=============================================
[2019-05-04 22:36:15,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[55.673588]
 [55.43373 ]
 [54.375103]
 [54.52982 ]
 [55.682087]], R is [[54.95878983]
 [54.96313477]
 [54.7077446 ]
 [54.64186859]
 [54.66511536]].
[2019-05-04 22:36:15,555] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 71.0, 7.2, 250.0, 0.0, 0.0, 0.09999999999999964, -0.5083873790383978, 65.0, 21.53610768896084, 22.7, 1.0, 83.12891903667894], 
current ob forecast is [], 
actual action is [0.0, 20.0], 
sim time this is 2232000.0000, 
sim time next is 2232900.0000, 
raw observation next is [-5.0, 70.25, 7.050000000000001, 257.5, 0.0, 0.0, 0.0, -0.4957660556843717, 20.0, 21.92243047967347, 21.5, 0.0, 38.655022244835045], 
processed observation next is [1.0, 0.8695652173913043, 0.20512820512820512, 0.7025, 0.640909090909091, 0.7152777777777778, 0.0, 0.0, 0.5, 0.33474464810520943, 0.0, 0.5603472113819242, 0.5, 0.0, 0.45476496758629464], 
reward next is 0.4952, 
noisyNet noise sample is [array([1.0155048], dtype=float32), -0.13960542]. 
=============================================
[2019-05-04 22:36:23,043] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0428648e-12 1.0000000e+00 1.1247839e-13 1.4461165e-14 5.5184333e-18
 2.3762118e-18 1.3976279e-14 5.5079333e-20 7.1487982e-14 4.8644378e-15
 4.0803388e-10], sum to 1.0000
[2019-05-04 22:36:23,044] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3129
[2019-05-04 22:36:23,158] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.875, 46.5, 2.125, 212.5, 196.0, 66.75, 3.85, -0.4861410471532817, 20.0, 22.37819126314201, 22.7, 1.0, 38.676540752047124], 
current ob forecast is [], 
actual action is [4.125, 20.0], 
sim time this is 2295900.0000, 
sim time next is 2296800.0000, 
raw observation next is [-0.6, 45.0, 2.0, 210.0, 171.0, 64.5, 4.125, -0.5031999454029503, 20.0, 22.30469654462313, 22.7, 1.0, 28.099501612412237], 
processed observation next is [1.0, 0.6086956521739131, 0.317948717948718, 0.45, 0.18181818181818182, 0.5833333333333334, 0.4523809523809524, 0.0645, 0.56875, 0.3322666848656832, 0.0, 0.6149566492318758, 0.6714285714285714, 1.0, 0.3305823719107322], 
reward next is 0.6253, 
noisyNet noise sample is [array([-0.10121694], dtype=float32), -1.3798977]. 
=============================================
[2019-05-04 22:36:35,175] A3C_AGENT_WORKER-Thread-6 INFO:Local step 20000, global step 318546: loss 1.0132
[2019-05-04 22:36:35,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 20000, global step 318546: learning rate 0.0001
[2019-05-04 22:36:35,958] A3C_AGENT_WORKER-Thread-12 INFO:Local step 20000, global step 318732: loss 2.1724
[2019-05-04 22:36:35,958] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 20000, global step 318732: learning rate 0.0001
[2019-05-04 22:36:37,254] A3C_AGENT_WORKER-Thread-5 INFO:Local step 20000, global step 318934: loss 1.0473
[2019-05-04 22:36:37,262] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 20000, global step 318934: learning rate 0.0001
[2019-05-04 22:36:37,405] A3C_AGENT_WORKER-Thread-14 INFO:Local step 20000, global step 318955: loss 1.7535
[2019-05-04 22:36:37,406] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 20000, global step 318955: learning rate 0.0001
[2019-05-04 22:36:37,413] A3C_AGENT_WORKER-Thread-7 INFO:Local step 20000, global step 318955: loss 1.3345
[2019-05-04 22:36:37,413] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 20000, global step 318955: learning rate 0.0001
[2019-05-04 22:36:37,950] A3C_AGENT_WORKER-Thread-2 INFO:Local step 20000, global step 319016: loss 1.2799
[2019-05-04 22:36:37,951] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 20000, global step 319016: learning rate 0.0001
[2019-05-04 22:36:41,275] A3C_AGENT_WORKER-Thread-15 INFO:Local step 20000, global step 319444: loss 0.2527
[2019-05-04 22:36:41,277] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 20000, global step 319444: learning rate 0.0001
[2019-05-04 22:36:42,551] A3C_AGENT_WORKER-Thread-8 INFO:Local step 20000, global step 319722: loss 2.1486
[2019-05-04 22:36:42,573] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 20000, global step 319726: learning rate 0.0001
[2019-05-04 22:36:42,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4934435e-09 9.9939370e-01 1.4976626e-08 3.9538803e-10 1.8486106e-13
 9.5242172e-15 4.7826626e-10 1.4344367e-13 1.2579492e-11 5.2911991e-10
 6.0628454e-04], sum to 1.0000
[2019-05-04 22:36:42,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8728
[2019-05-04 22:36:42,948] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.3, 26.0, 3.6, 100.0, 55.0, 479.5, 8.3, -0.4397006807243785, 20.0, 22.60970279606361, 22.7, 1.0, 16.80556853648932], 
current ob forecast is [], 
actual action is [8.3, 20.0], 
sim time this is 2476800.0000, 
sim time next is 2477700.0000, 
raw observation next is [3.3, 25.75, 3.975, 102.5, 51.5, 379.25, 8.3, -0.4633393677822355, 20.0, 22.45384528472157, 22.7, 1.0, 13.71720027697332], 
processed observation next is [0.0, 0.6956521739130435, 0.41794871794871796, 0.2575, 0.3613636363636364, 0.2847222222222222, 0.13624338624338625, 0.37925, 0.6383333333333333, 0.34555354407258815, 0.0, 0.6362636121030815, 0.6714285714285714, 1.0, 0.16137882678792143], 
reward next is 0.7886, 
noisyNet noise sample is [array([0.61851776], dtype=float32), 0.3123681]. 
=============================================
[2019-05-04 22:36:43,081] A3C_AGENT_WORKER-Thread-13 INFO:Local step 20000, global step 319849: loss 3.2991
[2019-05-04 22:36:43,082] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 20000, global step 319849: learning rate 0.0001
[2019-05-04 22:36:43,134] A3C_AGENT_WORKER-Thread-3 INFO:Local step 20000, global step 319860: loss 1.0426
[2019-05-04 22:36:43,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 20000, global step 319860: learning rate 0.0001
[2019-05-04 22:36:43,313] A3C_AGENT_WORKER-Thread-18 INFO:Local step 20000, global step 319896: loss 1.2925
[2019-05-04 22:36:43,317] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 20000, global step 319896: learning rate 0.0001
[2019-05-04 22:36:44,727] A3C_AGENT_WORKER-Thread-19 INFO:Local step 20000, global step 320219: loss 1.0912
[2019-05-04 22:36:44,728] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 20000, global step 320219: learning rate 0.0001
[2019-05-04 22:36:45,547] A3C_AGENT_WORKER-Thread-4 INFO:Local step 20000, global step 320383: loss 1.7811
[2019-05-04 22:36:45,589] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 20000, global step 320385: learning rate 0.0001
[2019-05-04 22:36:51,209] A3C_AGENT_WORKER-Thread-17 INFO:Local step 20000, global step 321376: loss 2.0982
[2019-05-04 22:36:51,210] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 20000, global step 321376: learning rate 0.0001
[2019-05-04 22:36:52,347] A3C_AGENT_WORKER-Thread-11 INFO:Local step 20000, global step 321611: loss 3.8389
[2019-05-04 22:36:52,347] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 20000, global step 321611: learning rate 0.0001
[2019-05-04 22:36:54,377] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.9235686e-13 7.2050912e-09 7.5676200e-14 6.2090768e-14 2.4568111e-16
 3.9152736e-19 4.7311881e-15 1.2400222e-16 8.5470693e-14 1.1041163e-14
 1.0000000e+00], sum to 1.0000
[2019-05-04 22:36:54,377] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8467
[2019-05-04 22:36:54,552] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 79.0, 5.725, 230.0, 24.5, 6.0, -2.3, -0.9249249771928248, 65.0, 20.2449354483148, 22.7, 1.0, 79.84796286021533], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 2619900.0000, 
sim time next is 2620800.0000, 
raw observation next is [-7.3, 79.0, 5.6, 230.0, 42.0, 4.0, -2.3, -0.8912690759557084, 65.0, 20.48318250460648, 22.7, 1.0, 75.66749135821452], 
processed observation next is [1.0, 0.34782608695652173, 0.14615384615384616, 0.79, 0.509090909090909, 0.6388888888888888, 0.1111111111111111, 0.004, 0.46166666666666667, 0.20291030801476387, 1.0, 0.3547403578009255, 0.6714285714285714, 1.0, 0.8902057806848767], 
reward next is 0.3366, 
noisyNet noise sample is [array([-0.09707616], dtype=float32), -0.39069083]. 
=============================================
[2019-05-04 22:36:56,142] A3C_AGENT_WORKER-Thread-16 INFO:Local step 20000, global step 322227: loss 3.1453
[2019-05-04 22:36:56,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 20000, global step 322227: learning rate 0.0001
[2019-05-04 22:37:00,527] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2595362e-10 9.9971730e-01 3.8579921e-11 1.6135907e-11 1.7394032e-16
 2.3924733e-18 5.2967120e-13 1.2514546e-17 2.2601564e-13 3.8000524e-13
 2.8267581e-04], sum to 1.0000
[2019-05-04 22:37:00,529] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1252
[2019-05-04 22:37:00,553] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.3, 25.25, 4.725, 107.5, 37.5, 220.0, 8.3, -0.625946923705722, 20.0, 21.68692907212081, 22.7, 1.0, 4.453211019787509], 
current ob forecast is [], 
actual action is [8.3, 20.0], 
sim time this is 2479500.0000, 
sim time next is 2480400.0000, 
raw observation next is [3.3, 25.0, 5.1, 110.0, 27.0, 161.0, 8.3, -0.6355371082243705, 20.0, 21.65272025036543, 22.7, 1.0, 4.472916188975416], 
processed observation next is [0.0, 0.7391304347826086, 0.41794871794871796, 0.25, 0.4636363636363636, 0.3055555555555556, 0.07142857142857142, 0.161, 0.6383333333333333, 0.2881542972585432, 0.0, 0.5218171786236329, 0.6714285714285714, 1.0, 0.05262254339971078], 
reward next is 0.5077, 
noisyNet noise sample is [array([-0.450355], dtype=float32), -0.22084853]. 
=============================================
[2019-05-04 22:37:11,080] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.8546388e-12 9.9999976e-01 8.8968853e-11 2.1429217e-12 5.3258912e-16
 7.5626285e-17 1.7066288e-13 1.2766544e-16 2.2341291e-12 4.9706317e-13
 2.6938582e-07], sum to 1.0000
[2019-05-04 22:37:11,080] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1454
[2019-05-04 22:37:11,122] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.75, 62.75, 2.975, 60.0, 0.0, 0.0, -1.5, -1.281392330173752, 20.0, 18.54629906039791, 21.5, 0.0, 10.13493173765847], 
current ob forecast is [], 
actual action is [-1.75, 20.0], 
sim time this is 2781900.0000, 
sim time next is 2782800.0000, 
raw observation next is [-7.0, 64.0, 3.1, 60.0, 0.0, 0.0, -1.75, -1.300095432188791, 20.0, 18.46487035466757, 21.5, 0.0, 10.244696770178557], 
processed observation next is [1.0, 0.21739130434782608, 0.15384615384615385, 0.64, 0.2818181818181818, 0.16666666666666666, 0.0, 0.0, 0.4708333333333333, 0.06663485593706968, 0.0, 0.06641005066679588, 0.5, 0.0, 0.12052584435504185], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0955846], dtype=float32), -1.6123387]. 
=============================================
[2019-05-04 22:37:15,786] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9152568e-08 9.9831641e-01 1.6569706e-07 1.1237930e-08 3.9138354e-12
 9.8493479e-13 7.8611684e-10 2.2159130e-12 3.4421350e-09 1.2613209e-08
 1.6834103e-03], sum to 1.0000
[2019-05-04 22:37:15,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3912
[2019-05-04 22:37:15,920] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.75, 53.0, 1.125, 255.0, 0.0, 0.0, 1.5, -0.4680400585396264, 65.0, 21.83437836167899, 22.7, 1.0, 92.98648293190648], 
current ob forecast is [], 
actual action is [1.25, 20.0], 
sim time this is 2742300.0000, 
sim time next is 2743200.0000, 
raw observation next is [-4.0, 54.0, 1.5, 340.0, 0.0, 0.0, 1.25, -0.4460382022544211, 20.0, 22.34914753746208, 22.7, 1.0, 52.253671742337225], 
processed observation next is [1.0, 0.782608695652174, 0.23076923076923078, 0.54, 0.13636363636363635, 0.9444444444444444, 0.0, 0.0, 0.5208333333333334, 0.35132059924852627, 0.0, 0.6213067910660115, 0.6714285714285714, 1.0, 0.6147490793216144], 
reward next is 0.3353, 
noisyNet noise sample is [array([0.52238536], dtype=float32), -1.6456612]. 
=============================================
[2019-05-04 22:37:17,703] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.10789315e-10 9.99907136e-01 1.47603541e-10 4.28039632e-10
 9.70590321e-15 3.69664131e-16 8.36305157e-12 2.95016934e-14
 9.82280056e-12 2.46362774e-11 9.28318477e-05], sum to 1.0000
[2019-05-04 22:37:17,703] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1903
[2019-05-04 22:37:17,731] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-15.5, 83.0, 0.75, 30.0, 0.0, 0.0, -10.75, -1.210998456917612, 65.0, 18.59522496801604, 21.5, 0.0, 68.5193145028199], 
current ob forecast is [], 
actual action is [-10.5, 20.0], 
sim time this is 2701800.0000, 
sim time next is 2702700.0000, 
raw observation next is [-15.25, 83.0, 1.125, 45.0, 0.0, 0.0, -10.5, -1.207150341512814, 20.0, 18.79763372536111, 21.5, 0.0, 42.66185887103422], 
processed observation next is [1.0, 0.2608695652173913, -0.057692307692307696, 0.83, 0.10227272727272728, 0.125, 0.0, 0.0, 0.325, 0.09761655282906201, 0.0, 0.11394767505158702, 0.5, 0.0, 0.5019042220121673], 
reward next is 0.6715, 
noisyNet noise sample is [array([0.0900909], dtype=float32), 0.18630056]. 
=============================================
[2019-05-04 22:37:22,463] A3C_AGENT_WORKER-Thread-6 INFO:Local step 20500, global step 326835: loss -1.4042
[2019-05-04 22:37:22,464] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 20500, global step 326835: learning rate 0.0001
[2019-05-04 22:37:23,053] A3C_AGENT_WORKER-Thread-12 INFO:Local step 20500, global step 326930: loss 0.0409
[2019-05-04 22:37:23,055] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 20500, global step 326930: learning rate 0.0001
[2019-05-04 22:37:23,056] A3C_AGENT_WORKER-Thread-7 INFO:Local step 20500, global step 326930: loss 0.0113
[2019-05-04 22:37:23,057] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 20500, global step 326930: learning rate 0.0001
[2019-05-04 22:37:23,321] A3C_AGENT_WORKER-Thread-14 INFO:Local step 20500, global step 326977: loss 0.8959
[2019-05-04 22:37:23,323] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 20500, global step 326977: learning rate 0.0001
[2019-05-04 22:37:23,788] A3C_AGENT_WORKER-Thread-2 INFO:Local step 20500, global step 327058: loss 0.9238
[2019-05-04 22:37:23,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 20500, global step 327058: learning rate 0.0001
[2019-05-04 22:37:24,403] A3C_AGENT_WORKER-Thread-5 INFO:Local step 20500, global step 327135: loss 0.3874
[2019-05-04 22:37:24,405] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 20500, global step 327135: learning rate 0.0001
[2019-05-04 22:37:25,567] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.05413005e-14 1.00000000e+00 4.73911758e-15 1.17235687e-14
 3.40014374e-19 4.02514043e-21 1.27199340e-15 5.53624488e-20
 1.21567287e-14 2.72590237e-16 6.36307465e-11], sum to 1.0000
[2019-05-04 22:37:25,584] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7150
[2019-05-04 22:37:25,628] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 64.0, 1.5, 60.0, 0.0, 0.0, -0.75, -0.5486686356972283, 20.0, 21.6207192897478, 21.5, 0.0, 27.669695951378284], 
current ob forecast is [], 
actual action is [-1.0, 20.0], 
sim time this is 2754000.0000, 
sim time next is 2754900.0000, 
raw observation next is [-6.0, 62.75, 1.65, 57.5, 0.0, 0.0, -1.0, -0.586712679922714, 20.0, 21.42121522532352, 21.5, 0.0, 22.039220207884696], 
processed observation next is [1.0, 0.9130434782608695, 0.1794871794871795, 0.6275, 0.15, 0.1597222222222222, 0.0, 0.0, 0.48333333333333334, 0.3044291066924287, 0.0, 0.48874503218907456, 0.5, 0.0, 0.2592849436221729], 
reward next is 0.6907, 
noisyNet noise sample is [array([-1.9273756], dtype=float32), 0.35160816]. 
=============================================
[2019-05-04 22:37:27,318] A3C_AGENT_WORKER-Thread-15 INFO:Local step 20500, global step 327634: loss 0.0403
[2019-05-04 22:37:27,321] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 20500, global step 327634: learning rate 0.0001
[2019-05-04 22:37:28,374] A3C_AGENT_WORKER-Thread-3 INFO:Local step 20500, global step 327849: loss 0.0211
[2019-05-04 22:37:28,375] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 20500, global step 327849: learning rate 0.0001
[2019-05-04 22:37:28,602] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1246542e-11 5.1076352e-09 1.0910153e-11 8.4120556e-14 1.3344012e-15
 3.0082853e-18 2.8964024e-14 4.2155228e-16 4.6380353e-14 1.6182159e-13
 1.0000000e+00], sum to 1.0000
[2019-05-04 22:37:28,602] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5180
[2019-05-04 22:37:28,810] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 93.0, 5.5, 130.0, 0.0, 0.0, 7.0, -0.9497115466908066, 20.0, 20.00403477855136, 22.7, 1.0, 41.780977888246326], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2877300.0000, 
sim time next is 2878200.0000, 
raw observation next is [2.0, 93.0, 5.9, 130.0, 0.0, 0.0, 7.0, -0.8650888389194393, 65.0, 20.17297932233168, 22.7, 1.0, 94.65411943703653], 
processed observation next is [1.0, 0.30434782608695654, 0.38461538461538464, 0.93, 0.5363636363636364, 0.3611111111111111, 0.0, 0.0, 0.6166666666666667, 0.21163705369352023, 1.0, 0.31042561747595443, 0.6714285714285714, 1.0, 1.1135778757298416], 
reward next is 0.8462, 
noisyNet noise sample is [array([0.22254497], dtype=float32), -0.4674034]. 
=============================================
[2019-05-04 22:37:30,014] A3C_AGENT_WORKER-Thread-13 INFO:Local step 20500, global step 328116: loss 0.1706
[2019-05-04 22:37:30,014] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 20500, global step 328116: learning rate 0.0001
[2019-05-04 22:37:30,262] A3C_AGENT_WORKER-Thread-8 INFO:Local step 20500, global step 328148: loss 0.0808
[2019-05-04 22:37:30,262] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 20500, global step 328148: learning rate 0.0001
[2019-05-04 22:37:30,725] A3C_AGENT_WORKER-Thread-18 INFO:Local step 20500, global step 328207: loss 0.0754
[2019-05-04 22:37:30,726] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 20500, global step 328207: learning rate 0.0001
[2019-05-04 22:37:32,093] A3C_AGENT_WORKER-Thread-19 INFO:Local step 20500, global step 328348: loss 0.0448
[2019-05-04 22:37:32,094] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 20500, global step 328348: learning rate 0.0001
[2019-05-04 22:37:32,275] A3C_AGENT_WORKER-Thread-4 INFO:Local step 20500, global step 328367: loss 0.1058
[2019-05-04 22:37:32,277] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 20500, global step 328367: learning rate 0.0001
[2019-05-04 22:37:34,033] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4045797e-10 9.9988592e-01 3.2209763e-10 1.0664769e-10 6.5396038e-15
 2.5579582e-15 1.3529128e-11 1.4209401e-15 5.4865813e-11 8.4259780e-11
 1.1409192e-04], sum to 1.0000
[2019-05-04 22:37:34,035] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9786
[2019-05-04 22:37:34,173] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.5, 53.75, 0.525, 15.0, 168.5, 439.25, 2.0, -0.5258877441357469, 65.0, 22.2493067403587, 22.7, 1.0, 72.71659434794853], 
current ob forecast is [], 
actual action is [2.5, 20.0], 
sim time this is 2801700.0000, 
sim time next is 2802600.0000, 
raw observation next is [-2.0, 52.5, 1.05, 30.0, 174.0, 508.0, 2.5, -0.5239745846102669, 20.0, 22.31748796429434, 22.7, 1.0, 43.61642154208662], 
processed observation next is [1.0, 0.43478260869565216, 0.28205128205128205, 0.525, 0.09545454545454546, 0.08333333333333333, 0.4603174603174603, 0.508, 0.5416666666666666, 0.325341805129911, 0.0, 0.6167839948991914, 0.6714285714285714, 1.0, 0.513134371083372], 
reward next is 0.4923, 
noisyNet noise sample is [array([0.05686534], dtype=float32), -0.63350624]. 
=============================================
[2019-05-04 22:37:34,785] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.13985421e-10 9.99997377e-01 1.16258073e-10 2.08944754e-11
 5.26796319e-14 1.20256994e-15 1.85552424e-11 1.33200240e-15
 4.84345376e-12 1.08044485e-11 2.61921446e-06], sum to 1.0000
[2019-05-04 22:37:34,805] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4190
[2019-05-04 22:37:34,870] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.25, 82.25, 4.6, 287.5, 0.0, 0.0, 2.0, -0.9060729984416361, 20.0, 19.95112911943492, 21.5, 0.0, 9.729248033889736], 
current ob forecast is [], 
actual action is [1.75, 20.0], 
sim time this is 2956500.0000, 
sim time next is 2957400.0000, 
raw observation next is [-3.5, 80.5, 4.6, 285.0, 0.0, 0.0, 1.75, -0.9239219036687163, 20.0, 19.87804203135331, 21.5, 0.0, 9.751639011140298], 
processed observation next is [0.0, 0.21739130434782608, 0.24358974358974358, 0.805, 0.41818181818181815, 0.7916666666666666, 0.0, 0.0, 0.5291666666666667, 0.1920260321104279, 0.0, 0.26829171876475855, 0.5, 0.0, 0.11472516483694468], 
reward next is 0.8353, 
noisyNet noise sample is [array([0.9286256], dtype=float32), 0.9021774]. 
=============================================
[2019-05-04 22:37:35,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.9773070e-11 9.9999976e-01 6.6331489e-12 5.5624737e-12 1.7818415e-16
 2.7351602e-17 2.4072191e-13 7.2664197e-17 1.9292955e-13 2.9358337e-12
 2.5361470e-07], sum to 1.0000
[2019-05-04 22:37:35,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6651
[2019-05-04 22:37:35,909] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 85.0, 6.45, 257.5, 0.0, 0.0, 3.0, -0.6597739305285965, 20.0, 20.97489354146846, 21.5, 0.0, 8.573394772858707], 
current ob forecast is [], 
actual action is [3.0, 20.0], 
sim time this is 2943900.0000, 
sim time next is 2944800.0000, 
raw observation next is [-2.0, 85.0, 6.2, 260.0, 0.0, 0.0, 3.0, -0.6771010619580305, 20.0, 20.90416223481854, 21.5, 0.0, 8.69722385276176], 
processed observation next is [0.0, 0.08695652173913043, 0.28205128205128205, 0.85, 0.5636363636363636, 0.7222222222222222, 0.0, 0.0, 0.55, 0.2742996460139898, 0.0, 0.4148803192597913, 0.5, 0.0, 0.10232028062072658], 
reward next is 0.8477, 
noisyNet noise sample is [array([1.4823669], dtype=float32), 1.8776463]. 
=============================================
[2019-05-04 22:37:38,020] A3C_AGENT_WORKER-Thread-17 INFO:Local step 20500, global step 329436: loss -0.2475
[2019-05-04 22:37:38,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 20500, global step 329436: learning rate 0.0001
[2019-05-04 22:37:40,768] A3C_AGENT_WORKER-Thread-11 INFO:Local step 20500, global step 329838: loss -0.2187
[2019-05-04 22:37:40,769] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 20500, global step 329838: learning rate 0.0001
[2019-05-04 22:37:43,502] A3C_AGENT_WORKER-Thread-16 INFO:Local step 20500, global step 330541: loss 0.6774
[2019-05-04 22:37:43,505] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 20500, global step 330543: learning rate 0.0001
[2019-05-04 22:37:44,324] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2878594e-09 8.1190884e-01 3.3401363e-08 8.7505309e-11 8.1761963e-13
 4.1389902e-14 1.2961462e-10 6.7004941e-14 7.9520740e-10 8.3746682e-10
 1.8809113e-01], sum to 1.0000
[2019-05-04 22:37:44,325] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6835
[2019-05-04 22:37:44,347] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 73.5, 2.6, 25.0, 0.0, 0.0, -1.0, -1.245612671198853, 20.0, 18.87384492592862, 21.5, 0.0, 25.076080183671966], 
current ob forecast is [], 
actual action is [-1.0, 20.0], 
sim time this is 3047400.0000, 
sim time next is 3048300.0000, 
raw observation next is [-6.0, 75.25, 2.85, 22.5, 0.0, 0.0, -1.0, -1.276012951371405, 20.0, 18.73019005080059, 21.5, 0.0, 20.3503701311723], 
processed observation next is [0.0, 0.2608695652173913, 0.1794871794871795, 0.7525, 0.2590909090909091, 0.0625, 0.0, 0.0, 0.48333333333333334, 0.07466234954286503, 0.0, 0.10431286440008439, 0.5, 0.0, 0.23941611919026237], 
reward next is 0.5756, 
noisyNet noise sample is [array([0.27203393], dtype=float32), 0.9448282]. 
=============================================
[2019-05-04 22:37:47,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7274126e-17 1.0000000e+00 1.6446069e-15 1.8808797e-17 1.0911784e-21
 9.3129905e-24 3.5739248e-19 2.3899613e-22 3.0507072e-19 9.7391646e-19
 2.9646778e-09], sum to 1.0000
[2019-05-04 22:37:47,961] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7304
[2019-05-04 22:37:48,008] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 85.0, 6.2, 260.0, 0.0, 0.0, 3.0, -0.8984452897225362, 20.0, 19.89275621612695, 21.5, 0.0, 9.183066079340632], 
current ob forecast is [], 
actual action is [3.0, 20.0], 
sim time this is 2944800.0000, 
sim time next is 2945700.0000, 
raw observation next is [-2.25, 84.75, 5.800000000000001, 265.0, 0.0, 0.0, 3.0, -0.9146872386780505, 20.0, 19.82981385979824, 21.5, 0.0, 9.270795881751884], 
processed observation next is [0.0, 0.08695652173913043, 0.27564102564102566, 0.8475, 0.5272727272727273, 0.7361111111111112, 0.0, 0.0, 0.55, 0.1951042537739832, 0.0, 0.26140197997117703, 0.5, 0.0, 0.10906818684413981], 
reward next is 0.8409, 
noisyNet noise sample is [array([0.1896228], dtype=float32), 1.182727]. 
=============================================
[2019-05-04 22:37:50,027] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.7622038e-16 1.0000000e+00 4.8208203e-16 4.2194139e-17 1.0959590e-21
 2.1739178e-23 4.1625755e-19 9.2948282e-22 5.2272397e-18 5.9889005e-18
 1.3249701e-10], sum to 1.0000
[2019-05-04 22:37:50,028] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6937
[2019-05-04 22:37:50,053] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.75, 100.0, 0.375, 12.5, 0.0, 0.0, 6.5, -0.9659648727552882, 20.0, 19.98785347071834, 21.5, 0.0, 6.697185474191007], 
current ob forecast is [], 
actual action is [6.75, 20.0], 
sim time this is 3120300.0000, 
sim time next is 3121200.0000, 
raw observation next is [2.0, 100.0, 0.0, 0.0, 0.0, 0.0, 6.75, -0.9871289240078536, 20.0, 19.91205229040404, 21.5, 0.0, 9.981773457442058], 
processed observation next is [1.0, 0.13043478260869565, 0.38461538461538464, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6125, 0.17095702533071547, 0.0, 0.27315032720057736, 0.5, 0.0, 0.11743262891108304], 
reward next is 0.8326, 
noisyNet noise sample is [array([0.9583651], dtype=float32), -1.4941132]. 
=============================================
[2019-05-04 22:37:52,220] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0485295e-15 9.9981707e-01 9.6843608e-14 8.6869542e-17 1.2720687e-19
 9.6685128e-24 1.6189282e-18 2.8573135e-21 3.2711377e-17 9.9607161e-15
 1.8296106e-04], sum to 1.0000
[2019-05-04 22:37:52,220] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0090
[2019-05-04 22:37:52,271] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.5, 96.5, 5.15, 195.0, 114.0, 805.0, 12.25, -0.4556325930628995, 20.0, 22.03464174744325, 22.7, 1.0, 3.103804181971892], 
current ob forecast is [], 
actual action is [12.5, 20.0], 
sim time this is 3151800.0000, 
sim time next is 3152700.0000, 
raw observation next is [7.75, 94.75, 4.875, 192.5, 113.75, 809.5, 12.5, -0.4380396554739821, 20.0, 22.0886624060133, 22.7, 1.0, 2.8442030820246735], 
processed observation next is [1.0, 0.4782608695652174, 0.532051282051282, 0.9475, 0.4431818181818182, 0.5347222222222222, 0.30092592592592593, 0.8095, 0.7083333333333334, 0.35398678150867263, 0.0, 0.5840946294304713, 0.6714285714285714, 1.0, 0.03346121272970204], 
reward next is 0.9165, 
noisyNet noise sample is [array([-0.43073034], dtype=float32), -0.33604062]. 
=============================================
[2019-05-04 22:38:00,022] A3C_AGENT_WORKER-Thread-6 INFO:Local step 21000, global step 334523: loss 1.1725
[2019-05-04 22:38:00,022] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 21000, global step 334523: learning rate 0.0001
[2019-05-04 22:38:00,735] A3C_AGENT_WORKER-Thread-12 INFO:Local step 21000, global step 334767: loss 1.1032
[2019-05-04 22:38:00,737] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 21000, global step 334767: learning rate 0.0001
[2019-05-04 22:38:01,048] A3C_AGENT_WORKER-Thread-14 INFO:Local step 21000, global step 334861: loss 0.5224
[2019-05-04 22:38:01,048] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 21000, global step 334861: learning rate 0.0001
[2019-05-04 22:38:01,125] A3C_AGENT_WORKER-Thread-7 INFO:Local step 21000, global step 334888: loss 0.3280
[2019-05-04 22:38:01,126] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 21000, global step 334888: learning rate 0.0001
[2019-05-04 22:38:02,369] A3C_AGENT_WORKER-Thread-2 INFO:Local step 21000, global step 335207: loss 2.4297
[2019-05-04 22:38:02,370] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 21000, global step 335207: learning rate 0.0001
[2019-05-04 22:38:02,566] A3C_AGENT_WORKER-Thread-5 INFO:Local step 21000, global step 335254: loss 2.6701
[2019-05-04 22:38:02,566] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 21000, global step 335254: learning rate 0.0001
[2019-05-04 22:38:05,652] A3C_AGENT_WORKER-Thread-3 INFO:Local step 21000, global step 335844: loss 0.1118
[2019-05-04 22:38:05,653] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 21000, global step 335844: learning rate 0.0001
[2019-05-04 22:38:05,726] A3C_AGENT_WORKER-Thread-15 INFO:Local step 21000, global step 335859: loss 1.7293
[2019-05-04 22:38:05,749] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 21000, global step 335859: learning rate 0.0001
[2019-05-04 22:38:06,080] A3C_AGENT_WORKER-Thread-8 INFO:Local step 21000, global step 335933: loss 0.9419
[2019-05-04 22:38:06,085] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 21000, global step 335933: learning rate 0.0001
[2019-05-04 22:38:06,324] A3C_AGENT_WORKER-Thread-13 INFO:Local step 21000, global step 335972: loss 1.8048
[2019-05-04 22:38:06,331] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 21000, global step 335972: learning rate 0.0001
[2019-05-04 22:38:06,374] A3C_AGENT_WORKER-Thread-18 INFO:Local step 21000, global step 335980: loss 0.6550
[2019-05-04 22:38:06,392] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 21000, global step 335983: learning rate 0.0001
[2019-05-04 22:38:07,575] A3C_AGENT_WORKER-Thread-19 INFO:Local step 21000, global step 336260: loss 0.1777
[2019-05-04 22:38:07,576] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 21000, global step 336260: learning rate 0.0001
[2019-05-04 22:38:07,810] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5116829e-10 8.8235450e-01 3.4637029e-09 6.3547466e-11 2.1689769e-13
 3.0655472e-16 1.1233108e-11 6.3364550e-16 2.9171554e-11 1.7561881e-11
 1.1764547e-01], sum to 1.0000
[2019-05-04 22:38:07,811] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0192
[2019-05-04 22:38:07,834] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.25, 72.5, 3.1, 162.5, 0.0, 0.0, 0.0, -0.9449421833299092, 20.0, 19.84550443435825, 20.8, 0.0, 7.018642127267312], 
current ob forecast is [], 
actual action is [-0.25, 20.0], 
sim time this is 3366900.0000, 
sim time next is 3367800.0000, 
raw observation next is [-5.5, 74.0, 3.1, 165.0, 0.0, 0.0, -0.25, -0.9649280006080506, 20.0, 19.76518716840732, 20.8, 0.0, 7.334905057350653], 
processed observation next is [1.0, 1.0, 0.19230769230769232, 0.74, 0.2818181818181818, 0.4583333333333333, 0.0, 0.0, 0.49583333333333335, 0.1783573331306498, 0.0, 0.2521695954867599, 0.4000000000000001, 0.0, 0.08629300067471357], 
reward next is 0.8637, 
noisyNet noise sample is [array([-1.4452362], dtype=float32), -0.90978867]. 
=============================================
[2019-05-04 22:38:08,314] A3C_AGENT_WORKER-Thread-4 INFO:Local step 21000, global step 336453: loss 0.5281
[2019-05-04 22:38:08,315] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 21000, global step 336453: learning rate 0.0001
[2019-05-04 22:38:14,323] A3C_AGENT_WORKER-Thread-17 INFO:Local step 21000, global step 337803: loss 0.3961
[2019-05-04 22:38:14,326] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 21000, global step 337803: learning rate 0.0001
[2019-05-04 22:38:16,730] A3C_AGENT_WORKER-Thread-11 INFO:Local step 21000, global step 338393: loss 1.5031
[2019-05-04 22:38:16,734] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 21000, global step 338393: learning rate 0.0001
[2019-05-04 22:38:19,149] A3C_AGENT_WORKER-Thread-16 INFO:Local step 21000, global step 338901: loss 0.4828
[2019-05-04 22:38:19,171] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 21000, global step 338902: learning rate 0.0001
[2019-05-04 22:38:19,893] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3305216e-11 9.9991691e-01 1.0082272e-10 5.2865173e-12 3.4346998e-15
 6.2483453e-18 1.1541122e-13 8.0899746e-17 8.6896956e-13 1.9759511e-12
 8.3051273e-05], sum to 1.0000
[2019-05-04 22:38:19,894] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7309
[2019-05-04 22:38:19,910] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 42.0, 0.375, 75.0, 0.0, 0.0, 4.0, -0.6858989936169673, 20.0, 21.15695063025925, 22.7, 1.0, 2.83636515291006], 
current ob forecast is [], 
actual action is [4.0, 20.0], 
sim time this is 3609900.0000, 
sim time next is 3610800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 0.0, 0.0, 4.0, -0.7001900376658633, 20.0, 21.10120650497285, 22.7, 1.0, 3.3176849793650227], 
processed observation next is [0.0, 0.8260869565217391, 0.3076923076923077, 0.42, 0.0, 0.0, 0.0, 0.0, 0.5666666666666667, 0.26660332077804555, 0.0, 0.4430295007104072, 0.6714285714285714, 1.0, 0.039031587992529676], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33544704], dtype=float32), 2.0210757]. 
=============================================
[2019-05-04 22:38:24,045] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7461627e-12 1.0000000e+00 1.5542897e-12 3.3810099e-14 1.0413672e-16
 2.8171197e-19 1.8941310e-14 2.8132014e-18 8.4619737e-14 3.0823634e-14
 5.0122782e-08], sum to 1.0000
[2019-05-04 22:38:24,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8182
[2019-05-04 22:38:24,126] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 55.75, 6.45, 230.0, 98.25, 757.25, 8.0, -0.6468263592203758, 20.0, 21.2632522868516, 22.0, 1.0, 8.199439146828446], 
current ob forecast is [], 
actual action is [8.0, 20.0], 
sim time this is 3422700.0000, 
sim time next is 3423600.0000, 
raw observation next is [3.0, 58.0, 6.7, 230.0, 93.5, 739.5, 8.0, -0.6187193582709996, 20.0, 21.44744325875689, 22.0, 1.0, 7.520109644246522], 
processed observation next is [1.0, 0.6521739130434783, 0.41025641025641024, 0.58, 0.6090909090909091, 0.6388888888888888, 0.24735449735449735, 0.7395, 0.6333333333333333, 0.29376021390966683, 0.0, 0.4924918941081273, 0.5714285714285714, 1.0, 0.08847187816760614], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.07683101], dtype=float32), 0.045056857]. 
=============================================
[2019-05-04 22:38:30,909] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1632942e-10 9.9905294e-01 8.6497215e-10 7.4811780e-12 2.3477464e-17
 2.1938882e-19 7.3582484e-14 2.1115578e-18 6.4156563e-13 6.1898288e-14
 9.4705360e-04], sum to 1.0000
[2019-05-04 22:38:30,909] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9904
[2019-05-04 22:38:31,033] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.75, 69.5, 6.325, 242.5, 114.0, 808.75, 2.0, -0.3200454245888038, 65.0, 22.82754259053493, 22.7, 1.0, 70.57551763415829], 
current ob forecast is [], 
actual action is [2.25, 20.0], 
sim time this is 3755700.0000, 
sim time next is 3756600.0000, 
raw observation next is [-2.5, 68.0, 6.45, 245.0, 115.0, 822.0, 2.25, -0.2900996124328736, 20.0, 23.12804552084152, 22.7, 1.0, 39.41492316945723], 
processed observation next is [1.0, 0.4782608695652174, 0.2692307692307692, 0.68, 0.5863636363636364, 0.6805555555555556, 0.30423280423280424, 0.822, 0.5375, 0.4033001291890421, 0.0, 0.7325779315487887, 0.6714285714285714, 1.0, 0.4637049784642027], 
reward next is 0.4863, 
noisyNet noise sample is [array([1.1544983], dtype=float32), -0.7143597]. 
=============================================
[2019-05-04 22:38:32,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.88456628e-12 9.99996066e-01 1.71333461e-13 6.78650103e-16
 3.22603670e-20 1.04721696e-20 1.75446538e-15 2.79896141e-20
 1.88649960e-16 4.72860484e-16 3.95688267e-06], sum to 1.0000
[2019-05-04 22:38:32,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9901
[2019-05-04 22:38:32,348] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 42.0, 6.2, 220.0, 115.0, 823.5, 9.75, -0.1553971887711334, 20.0, 23.59925441531801, 22.7, 1.0, 34.68811657424033], 
current ob forecast is [], 
actual action is [10.0, 20.0], 
sim time this is 3675600.0000, 
sim time next is 3676500.0000, 
raw observation next is [5.25, 42.25, 5.925000000000001, 225.0, 114.0, 820.75, 10.0, -0.1456602857337216, 20.0, 23.6668849933717, 22.7, 1.0, 25.535431675739122], 
processed observation next is [0.0, 0.5652173913043478, 0.46794871794871795, 0.4225, 0.5386363636363637, 0.625, 0.30158730158730157, 0.82075, 0.6666666666666666, 0.45144657142209277, 0.0, 0.8095549990530999, 0.6714285714285714, 1.0, 0.30041684324398965], 
reward next is 0.6496, 
noisyNet noise sample is [array([-0.69744164], dtype=float32), -0.34670302]. 
=============================================
[2019-05-04 22:38:32,391] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.68545 ]
 [76.708916]
 [76.99943 ]
 [77.5517  ]
 [78.07169 ]], R is [[76.70513916]
 [76.47999573]
 [75.7151947 ]
 [75.70639801]
 [75.64835358]].
[2019-05-04 22:38:32,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.40192455e-15 1.00000000e+00 4.41918053e-15 1.54982910e-15
 4.64884148e-21 3.36527229e-22 6.25594795e-18 1.34380725e-20
 8.40749180e-16 7.10522312e-16 9.48327306e-10], sum to 1.0000
[2019-05-04 22:38:32,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0510
[2019-05-04 22:38:32,527] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.75, 42.75, 5.375, 235.0, 110.75, 807.5, 10.5, -0.07725830429233858, 20.0, 24.02984121664864, 22.7, 1.0, 26.570833052121582], 
current ob forecast is [], 
actual action is [10.75, 20.0], 
sim time this is 3678300.0000, 
sim time next is 3679200.0000, 
raw observation next is [6.0, 43.0, 5.1, 240.0, 108.5, 797.0, 10.75, -0.0863858312882788, 20.0, 23.94851904271107, 22.7, 1.0, 21.01148652536127], 
processed observation next is [0.0, 0.6086956521739131, 0.48717948717948717, 0.43, 0.4636363636363636, 0.6666666666666666, 0.28703703703703703, 0.797, 0.6791666666666667, 0.4712047229039071, 0.0, 0.8497884346730099, 0.6714285714285714, 1.0, 0.2471939591218973], 
reward next is 0.7028, 
noisyNet noise sample is [array([-0.69880587], dtype=float32), -0.028997239]. 
=============================================
[2019-05-04 22:38:33,859] A3C_AGENT_WORKER-Thread-6 INFO:Local step 21500, global step 342280: loss 0.4477
[2019-05-04 22:38:33,865] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 21500, global step 342280: learning rate 0.0001
[2019-05-04 22:38:33,987] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1892712e-14 1.0000000e+00 1.7906005e-13 4.0734567e-14 5.6561783e-18
 6.7480970e-21 2.5987447e-15 3.1886611e-19 3.3143491e-15 4.8947439e-14
 3.9168974e-10], sum to 1.0000
[2019-05-04 22:38:33,987] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1800
[2019-05-04 22:38:34,010] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.25, 66.25, 2.975, 287.5, 0.0, 0.0, 0.0, -0.9805911408623945, 20.0, 19.65635826958878, 21.5, 0.0, 9.416479309665695], 
current ob forecast is [], 
actual action is [-0.25, 20.0], 
sim time this is 3561300.0000, 
sim time next is 3562200.0000, 
raw observation next is [-5.5, 67.5, 2.85, 285.0, 0.0, 0.0, -0.25, -0.9991791448752912, 20.0, 19.57903781858417, 21.5, 0.0, 9.488668258758231], 
processed observation next is [0.0, 0.21739130434782608, 0.19230769230769232, 0.675, 0.2590909090909091, 0.7916666666666666, 0.0, 0.0, 0.49583333333333335, 0.16694028504156957, 0.0, 0.22557683122630984, 0.5, 0.0, 0.11163139127950861], 
reward next is 0.8384, 
noisyNet noise sample is [array([0.55672455], dtype=float32), 0.08890241]. 
=============================================
[2019-05-04 22:38:34,453] A3C_AGENT_WORKER-Thread-12 INFO:Local step 21500, global step 342534: loss 0.0232
[2019-05-04 22:38:34,456] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 21500, global step 342537: learning rate 0.0001
[2019-05-04 22:38:34,791] A3C_AGENT_WORKER-Thread-14 INFO:Local step 21500, global step 342680: loss 0.0620
[2019-05-04 22:38:34,792] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 21500, global step 342680: learning rate 0.0001
[2019-05-04 22:38:34,897] A3C_AGENT_WORKER-Thread-7 INFO:Local step 21500, global step 342725: loss 0.4261
[2019-05-04 22:38:34,900] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 21500, global step 342726: learning rate 0.0001
[2019-05-04 22:38:36,020] A3C_AGENT_WORKER-Thread-5 INFO:Local step 21500, global step 343010: loss 0.0599
[2019-05-04 22:38:36,020] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 21500, global step 343010: learning rate 0.0001
[2019-05-04 22:38:37,180] A3C_AGENT_WORKER-Thread-2 INFO:Local step 21500, global step 343171: loss 0.0759
[2019-05-04 22:38:37,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 21500, global step 343171: learning rate 0.0001
[2019-05-04 22:38:39,274] A3C_AGENT_WORKER-Thread-3 INFO:Local step 21500, global step 343784: loss 0.2802
[2019-05-04 22:38:39,275] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 21500, global step 343784: learning rate 0.0001
[2019-05-04 22:38:39,795] A3C_AGENT_WORKER-Thread-15 INFO:Local step 21500, global step 343905: loss 0.1320
[2019-05-04 22:38:39,797] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 21500, global step 343905: learning rate 0.0001
[2019-05-04 22:38:39,959] A3C_AGENT_WORKER-Thread-8 INFO:Local step 21500, global step 343953: loss 0.0639
[2019-05-04 22:38:39,961] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 21500, global step 343953: learning rate 0.0001
[2019-05-04 22:38:39,965] A3C_AGENT_WORKER-Thread-13 INFO:Local step 21500, global step 343953: loss 0.0374
[2019-05-04 22:38:39,969] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 21500, global step 343954: learning rate 0.0001
[2019-05-04 22:38:40,129] A3C_AGENT_WORKER-Thread-18 INFO:Local step 21500, global step 344000: loss 0.0143
[2019-05-04 22:38:40,133] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 21500, global step 344000: learning rate 0.0001
[2019-05-04 22:38:40,458] A3C_AGENT_WORKER-Thread-19 INFO:Local step 21500, global step 344098: loss 0.0284
[2019-05-04 22:38:40,460] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 21500, global step 344098: learning rate 0.0001
[2019-05-04 22:38:40,571] A3C_AGENT_WORKER-Thread-4 INFO:Local step 21500, global step 344133: loss 0.0214
[2019-05-04 22:38:40,572] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 21500, global step 344133: learning rate 0.0001
[2019-05-04 22:38:41,883] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2769694e-09 4.1698858e-01 5.9314159e-10 1.2776351e-11 1.1185671e-13
 1.9902511e-16 4.3530599e-11 4.8820557e-14 3.8100242e-11 7.1390553e-11
 5.8301139e-01], sum to 1.0000
[2019-05-04 22:38:41,883] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7270
[2019-05-04 22:38:42,039] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.5, 49.5, 3.1, 270.0, 0.0, 0.0, 6.75, -0.5010560369445044, 20.0, 21.57688526980029, 22.7, 1.0, 2.7022449173413325], 
current ob forecast is [], 
actual action is [6.5, 65.0], 
sim time this is 3868200.0000, 
sim time next is 3869100.0000, 
raw observation next is [1.25, 50.25, 2.6, 270.0, 0.0, 0.0, 6.5, -0.3790221484416583, 65.0, 21.74607898975711, 22.7, 1.0, 93.33457764176626], 
processed observation next is [1.0, 0.782608695652174, 0.36538461538461536, 0.5025, 0.23636363636363636, 0.75, 0.0, 0.0, 0.6083333333333333, 0.3736592838527806, 1.0, 0.5351541413938727, 0.6714285714285714, 1.0, 1.0980538546090148], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4531472], dtype=float32), 0.21579117]. 
=============================================
[2019-05-04 22:38:43,971] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.08863166e-13 1.53365429e-04 7.04716504e-13 3.88338145e-14
 3.33456874e-18 5.06907298e-20 1.60879572e-15 3.49054792e-18
 3.67562282e-15 1.17645516e-14 9.99846578e-01], sum to 1.0000
[2019-05-04 22:38:43,971] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6907
[2019-05-04 22:38:44,115] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 60.0, 4.9, 245.0, 117.0, 822.0, 4.0, -0.6507942770631773, 20.0, 21.27561531112293, 22.7, 1.0, 7.196565902065669], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3843000.0000, 
sim time next is 3843900.0000, 
raw observation next is [-1.0, 60.0, 4.5, 252.5, 117.0, 825.25, 4.0, -0.5059419953134784, 65.0, 21.67846879637656, 22.7, 1.0, 85.55921535690543], 
processed observation next is [1.0, 0.4782608695652174, 0.3076923076923077, 0.6, 0.4090909090909091, 0.7013888888888888, 0.30952380952380953, 0.82525, 0.5666666666666667, 0.3313526682288405, 1.0, 0.5254955423395086, 0.6714285714285714, 1.0, 1.0065790041988874], 
reward next is 0.0297, 
noisyNet noise sample is [array([0.6268558], dtype=float32), -0.7348076]. 
=============================================
[2019-05-04 22:38:48,388] A3C_AGENT_WORKER-Thread-17 INFO:Local step 21500, global step 346031: loss 0.0433
[2019-05-04 22:38:48,390] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 21500, global step 346032: learning rate 0.0001
[2019-05-04 22:38:49,200] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.32090094e-09 9.94175017e-01 1.54434188e-10 4.22006569e-10
 2.08397769e-14 1.34208638e-16 2.27883858e-11 8.70428336e-14
 1.13726334e-10 8.20072177e-10 5.82499430e-03], sum to 1.0000
[2019-05-04 22:38:49,214] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9488
[2019-05-04 22:38:49,303] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 45.0, 4.6, 360.0, 117.5, 829.5, -1.0, -0.5153577437476241, 20.0, 21.83362584408691, 22.7, 1.0, 36.676829947953955], 
current ob forecast is [], 
actual action is [-1.0, 20.0], 
sim time this is 3934800.0000, 
sim time next is 3935700.0000, 
raw observation next is [-5.75, 43.25, 4.6, 272.5, 116.75, 826.75, -1.0, -0.4848797606450672, 20.0, 22.06508981228635, 22.7, 1.0, 26.990789179840238], 
processed observation next is [1.0, 0.5652173913043478, 0.1858974358974359, 0.4325, 0.41818181818181815, 0.7569444444444444, 0.30886243386243384, 0.82675, 0.48333333333333334, 0.33837341311831093, 0.0, 0.580727116040907, 0.6714285714285714, 1.0, 0.3175386962334146], 
reward next is 0.6325, 
noisyNet noise sample is [array([0.24621266], dtype=float32), 2.3673518]. 
=============================================
[2019-05-04 22:38:49,428] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8338916e-09 9.8149800e-01 1.8374534e-08 3.7520559e-10 7.1866954e-13
 1.5144387e-14 1.9159534e-10 1.2694565e-13 3.2270135e-09 8.5992946e-10
 1.8501969e-02], sum to 1.0000
[2019-05-04 22:38:49,428] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6394
[2019-05-04 22:38:49,443] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.0, 63.0, 2.975, 350.0, 0.0, 0.0, -7.0, -1.135169328076168, 20.0, 19.12995252001791, 21.5, 0.0, 42.545509026872494], 
current ob forecast is [], 
actual action is [-7.0, 20.0], 
sim time this is 3987900.0000, 
sim time next is 3988800.0000, 
raw observation next is [-12.0, 63.0, 3.1, 350.0, 0.0, 0.0, -7.0, -1.166540286891871, 20.0, 19.07607688957092, 21.5, 0.0, 32.060784807481866], 
processed observation next is [1.0, 0.17391304347826086, 0.02564102564102564, 0.63, 0.2818181818181818, 0.9722222222222222, 0.0, 0.0, 0.38333333333333336, 0.11115323770270964, 0.0, 0.15372526993870292, 0.5, 0.0, 0.3771857036174337], 
reward next is 0.5728, 
noisyNet noise sample is [array([3.1251874], dtype=float32), -0.48970675]. 
=============================================
[2019-05-04 22:38:50,567] A3C_AGENT_WORKER-Thread-11 INFO:Local step 21500, global step 346418: loss 0.0274
[2019-05-04 22:38:50,579] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 21500, global step 346418: learning rate 0.0001
[2019-05-04 22:38:51,882] A3C_AGENT_WORKER-Thread-16 INFO:Local step 21500, global step 346698: loss 0.0249
[2019-05-04 22:38:51,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 21500, global step 346698: learning rate 0.0001
[2019-05-04 22:38:53,766] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0498642e-11 9.9856085e-01 2.2645091e-11 1.9035445e-13 1.2910111e-16
 5.9531243e-19 1.1227145e-11 2.1177850e-17 7.5942780e-15 7.0749309e-12
 1.4391231e-03], sum to 1.0000
[2019-05-04 22:38:53,767] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5863
[2019-05-04 22:38:53,900] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.25, 26.75, 0.375, 12.5, 111.5, 821.0, 1.5, -0.4080383216565591, 65.0, 22.28519112882821, 22.7, 1.0, 75.81572173317129], 
current ob forecast is [], 
actual action is [1.75, 20.0], 
sim time this is 4023900.0000, 
sim time next is 4024800.0000, 
raw observation next is [-3.0, 26.0, 0.0, 0.0, 109.0, 812.0, 1.75, -0.3595711212647325, 20.0, 22.80623025245212, 22.7, 1.0, 35.860015415868126], 
processed observation next is [1.0, 0.6086956521739131, 0.2564102564102564, 0.26, 0.0, 0.0, 0.28835978835978837, 0.812, 0.5291666666666667, 0.3801429595784225, 0.0, 0.6866043217788744, 0.6714285714285714, 1.0, 0.4218825343043309], 
reward next is 0.5281, 
noisyNet noise sample is [array([-0.48466113], dtype=float32), -0.76187986]. 
=============================================
[2019-05-04 22:38:55,799] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1546688e-10 9.9999917e-01 2.0575429e-12 3.4945688e-12 4.8853097e-18
 3.6023783e-19 1.1986888e-14 1.9190875e-17 4.5050162e-14 1.2253847e-13
 8.4377575e-07], sum to 1.0000
[2019-05-04 22:38:55,801] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9770
[2019-05-04 22:38:55,854] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.75, 25.5, 2.225, 65.0, 15.0, 144.75, 2.5, -0.5650613324186519, 20.0, 21.67463497844141, 22.7, 1.0, 3.183789407852629], 
current ob forecast is [], 
actual action is [2.25, 20.0], 
sim time this is 4038300.0000, 
sim time next is 4039200.0000, 
raw observation next is [-3.0, 26.0, 2.1, 70.0, 0.0, 0.0, 2.25, -0.5657144487645644, 20.0, 21.43366566798645, 22.7, 1.0, 3.9661575064547954], 
processed observation next is [1.0, 0.782608695652174, 0.2564102564102564, 0.26, 0.19090909090909092, 0.19444444444444445, 0.0, 0.0, 0.5375, 0.3114285170784785, 0.0, 0.4905236668552071, 0.6714285714285714, 1.0, 0.046660676546527], 
reward next is 0.8271, 
noisyNet noise sample is [array([-0.92369276], dtype=float32), 1.5955981]. 
=============================================
[2019-05-04 22:39:02,737] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5264557e-11 9.9999571e-01 9.8704801e-11 9.8800898e-14 1.8225182e-15
 2.6406722e-17 3.1686394e-12 5.8695677e-18 2.5285687e-12 1.9268909e-12
 4.2703336e-06], sum to 1.0000
[2019-05-04 22:39:02,738] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5830
[2019-05-04 22:39:02,758] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.0, 58.0, 3.1, 360.0, 0.0, 0.0, -5.75, -1.008470024805539, 20.0, 19.66067721240024, 21.5, 0.0, 10.542595131939917], 
current ob forecast is [], 
actual action is [-6.0, 20.0], 
sim time this is 3978000.0000, 
sim time next is 3978900.0000, 
raw observation next is [-11.25, 59.25, 3.1, 357.5, 0.0, 0.0, -6.0, -1.031098155421451, 20.0, 19.56850283094056, 21.5, 0.0, 10.72370030776328], 
processed observation next is [1.0, 0.043478260869565216, 0.04487179487179487, 0.5925, 0.2818181818181818, 0.9930555555555556, 0.0, 0.0, 0.4, 0.15630061485951638, 0.0, 0.2240718329915085, 0.5, 0.0, 0.1261611800913327], 
reward next is 0.8238, 
noisyNet noise sample is [array([-0.815328], dtype=float32), 1.3402503]. 
=============================================
[2019-05-04 22:39:06,667] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-05-04 22:39:06,668] A3C_EVAL-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1 INFO:Evaluation job starts!
[2019-05-04 22:39:06,668] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:39:06,669] A3C_EVAL-Part4-Heavy-Pit-Test-Repeat-Real97-v1 INFO:Evaluation job starts!
[2019-05-04 22:39:06,670] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-05-04 22:39:06,675] EPLUS_ENV_Part4-Heavy-Pit-Test-Repeat-Real97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Test-Repeat-Real97-v1-res1/Eplus-env-sub_run8
[2019-05-04 22:39:06,691] EPLUS_ENV_Part4-Heavy-Pit-Train-Repeat-Cmp97-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_heavy_cmp97/1/Eplus-env-Part4-Heavy-Pit-Train-Repeat-Cmp97-v1-res1/Eplus-env-sub_run8
