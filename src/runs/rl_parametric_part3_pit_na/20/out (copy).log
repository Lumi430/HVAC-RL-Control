Using TensorFlow backend.
[2019-03-08 09:37:26,191] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Pit-Train-v1', eval_act_func='part3_v3', eval_env_res_max_keep=20, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part3_v1', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part3-NA-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=7.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=16, test_env=None, test_mode='Multiple', train_act_func='part3_v3', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=1)
[2019-03-08 09:37:26,191] A3C_AGENT_MAIN INFO:Start compiling...
2019-03-08 09:37:26.229366: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-03-08 09:37:42,252] A3C_AGENT_MAIN INFO:Start the learning...
[2019-03-08 09:37:42,252] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Pit-Train-v1'] ...
[2019-03-08 09:37:42,261] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-03-08 09:37:42,261] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:42,262] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-03-08 09:37:42,324] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:42,325] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-03-08 09:37:43,218] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-08 09:37:43,219] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation job starts!
[2019-03-08 09:37:43,219] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:43,221] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-03-08 09:37:43,263] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:43,265] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-03-08 09:37:43,327] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:43,327] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-03-08 09:37:44,266] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:44,267] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-03-08 09:37:44,353] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:44,355] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-03-08 09:37:45,268] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:45,269] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-03-08 09:37:45,372] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:45,374] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-03-08 09:37:46,270] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:46,272] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-03-08 09:37:46,338] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:46,339] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-03-08 09:37:47,272] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:47,273] A3C_AGENT_WORKER-Thread-8 INFO:Local worker starts!
[2019-03-08 09:37:47,349] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:47,351] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-03-08 09:37:48,274] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:48,275] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-03-08 09:37:48,368] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:48,370] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-03-08 09:37:49,276] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:49,277] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-03-08 09:37:49,359] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:49,360] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-03-08 09:37:50,278] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:50,279] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-03-08 09:37:50,338] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:50,339] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-03-08 09:37:51,280] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:51,281] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-03-08 09:37:51,349] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:51,352] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-03-08 09:37:52,282] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:52,284] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-03-08 09:37:52,374] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:52,376] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-03-08 09:37:53,284] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:53,286] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-03-08 09:37:53,387] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:53,389] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-03-08 09:37:54,287] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:54,289] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-03-08 09:37:54,375] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:54,376] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-03-08 09:37:55,290] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:55,291] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-03-08 09:37:55,380] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:55,382] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-03-08 09:37:55,798] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-08 09:37:55,799] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation this: [24.0, 55.00000000000001, 1.0, 1.0, 0.4413914048669169, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 342515.5838102086, 342515.5838102086, 63597.95547570824]
[2019-03-08 09:37:55,799] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-03-08 09:37:55,799] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Softmax [0.2754734  0.2052288  0.16960452 0.15184772 0.19784552], sampled 0.6659556183195423
[2019-03-08 09:37:56,292] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:56,293] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-03-08 09:37:56,383] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:56,385] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-03-08 09:37:57,294] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-08 09:37:57,297] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-03-08 09:37:57,369] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:37:57,370] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-03-08 09:38:01,735] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-08 09:38:01,735] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation this: [21.33333333333334, 76.5, 1.0, 2.0, 0.4647904494279615, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8717678220666439, 6.911199999999999, 6.9112, 708889.5035145418, 708889.5035145419, 155392.7208681409]
[2019-03-08 09:38:01,735] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-03-08 09:38:01,736] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Softmax [0.23221184 0.22705945 0.19330303 0.16138215 0.18604355], sampled 0.14866168051039907
[2019-03-08 09:38:13,697] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation: average rewards by now are 3769.5409 1452263592.0497 183.0000
[2019-03-08 09:38:14,711] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3769.540902791828, 1452263592.0496583, 183.0]
[2019-03-08 09:38:18,808] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.13028988 0.26715448 0.25333157 0.17709641 0.17212766], sum to 1.0000
[2019-03-08 09:38:18,821] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9794
[2019-03-08 09:38:18,927] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.66666666666667, 79.66666666666667, 1.0, 2.0, 0.6224993517478028, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 6.911199999999999, 6.9112, 859222.6841205533, 859222.6841205535, 194094.5520176649], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 38400.0000, 
sim time next is 39000.0000, 
raw observation next is [21.83333333333334, 78.83333333333333, 1.0, 2.0, 0.3820499277322752, 1.0, 1.0, 0.3820499277322752, 1.0, 2.0, 0.7040773400773398, 6.911199999999999, 6.9112, 858549.3770559513, 858549.3770559514, 184862.5394015668], 
processed observation next is [1.0, 0.43478260869565216, 0.628787878787879, 0.7883333333333333, 1.0, 1.0, 0.3820499277322752, 1.0, 0.5, 0.3820499277322752, 1.0, 1.0, 0.7040773400773398, -8.881784197001253e-17, 0.0, 0.35772890710664634, 0.3577289071066464, 0.462156348503917], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1204157], dtype=float32), 0.89507747]. 
=============================================
[2019-03-08 09:38:18,944] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[2.3310905]
 [2.4320383]
 [2.429354 ]
 [2.381831 ]
 [2.3445096]], R is [[2.67688346]
 [3.28251815]
 [3.99936581]
 [4.70894289]
 [4.66185331]].
[2019-03-08 09:38:21,972] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0581256  0.06196168 0.13885546 0.05865333 0.6824039 ], sum to 1.0000
[2019-03-08 09:38:21,981] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9790
[2019-03-08 09:38:22,078] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.0, 77.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 2.0, 0.25, 6.911200000000001, 6.9112, 222752.3800619967, 222752.3800619965, 82405.45720508312], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 90000.0000, 
sim time next is 90600.0000, 
raw observation next is [15.66666666666667, 77.83333333333334, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 2.0, 0.25, 6.911199999999999, 6.9112, 232906.7213459284, 232906.7213459286, 83736.7862228123], 
processed observation next is [1.0, 0.043478260869565216, 0.3484848484848486, 0.7783333333333334, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 1.0, 1.0, 0.25, -8.881784197001253e-17, 0.0, 0.09704446722747018, 0.09704446722747026, 0.20934196555703075], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.75003016], dtype=float32), 0.73572385]. 
=============================================
[2019-03-08 09:38:22,497] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.09532084 0.07598127 0.08450793 0.04280988 0.7013801 ], sum to 1.0000
[2019-03-08 09:38:22,508] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4027
[2019-03-08 09:38:22,613] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.5, 85.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 2.0, 0.25, 6.911199999999999, 6.9112, 176568.1767427903, 176568.1767427905, 70227.91962386477], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 99000.0000, 
sim time next is 99600.0000, 
raw observation next is [13.33333333333333, 86.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 2.0, 0.25, 6.911199999999999, 6.9112, 174406.2932725977, 174406.2932725979, 69688.3250032756], 
processed observation next is [1.0, 0.13043478260869565, 0.2424242424242423, 0.86, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 1.0, 1.0, 0.25, -8.881784197001253e-17, 0.0, 0.07266928886358237, 0.07266928886358245, 0.17422081250818902], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9629991], dtype=float32), 0.46976918]. 
=============================================
[2019-03-08 09:38:24,149] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0014444e-03 3.5846435e-02 6.9013294e-03 3.3811049e-04 9.5591271e-01], sum to 1.0000
[2019-03-08 09:38:24,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7206
[2019-03-08 09:38:24,260] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [19.66666666666667, 58.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 2.0, 0.3383465775822554, 6.9112, 6.9112, 412313.9858129123, 412313.9858129123, 119650.6263258229], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 127200.0000, 
sim time next is 127800.0000, 
raw observation next is [20.0, 55.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 2.0, 0.2707243833990702, 6.911200000000001, 6.9112, 329867.0898732082, 329867.089873208, 106069.6819661868], 
processed observation next is [1.0, 0.4782608695652174, 0.5454545454545454, 0.55, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 1.0, 1.0, 0.2707243833990702, 8.881784197001253e-17, 0.0, 0.13744462078050343, 0.13744462078050335, 0.265174204915467], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7171923], dtype=float32), 0.58332163]. 
=============================================
[2019-03-08 09:38:24,956] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9854128e-07 6.1129707e-05 5.7395355e-06 2.1339689e-07 9.9993241e-01], sum to 1.0000
[2019-03-08 09:38:24,962] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3112
[2019-03-08 09:38:25,066] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.0, 42.5, 1.0, 2.0, 0.2840257153219458, 1.0, 2.0, 0.2840257153219458, 1.0, 2.0, 0.5421170941224475, 6.911200000000001, 6.9112, 660882.457493725, 660882.4574937248, 151446.857675382], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 138600.0000, 
sim time next is 139200.0000, 
raw observation next is [23.0, 42.0, 1.0, 2.0, 0.3965402592521988, 1.0, 2.0, 0.3965402592521988, 1.0, 2.0, 0.7550279923158758, 6.911200000000003, 6.9112, 920753.4178858678, 920753.4178858672, 187455.7309222884], 
processed observation next is [1.0, 0.6086956521739131, 0.6818181818181818, 0.42, 1.0, 1.0, 0.3965402592521988, 1.0, 1.0, 0.3965402592521988, 1.0, 1.0, 0.7550279923158758, 2.6645352591003756e-16, 0.0, 0.3836472574524449, 0.38364725745244466, 0.468639327305721], 
reward next is 0.7017, 
noisyNet noise sample is [array([1.1877084], dtype=float32), 0.27293807]. 
=============================================
[2019-03-08 09:38:25,097] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4987741e-07 1.1070689e-05 2.0517786e-05 3.2071316e-08 9.9996805e-01], sum to 1.0000
[2019-03-08 09:38:25,097] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2804
[2019-03-08 09:38:25,100] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.0, 40.0, 1.0, 2.0, 0.287131191777361, 1.0, 2.0, 0.287131191777361, 1.0, 2.0, 0.5491162743694953, 6.9112, 6.9112, 669423.7295472332, 669423.7295472332, 152105.5255451482], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 141600.0000, 
sim time next is 142200.0000, 
raw observation next is [23.0, 39.5, 1.0, 2.0, 0.2908933460841092, 1.0, 2.0, 0.2908933460841092, 1.0, 2.0, 0.5566521122979848, 6.9112, 6.9112, 678619.6944857567, 678619.6944857567, 153130.1485049785], 
processed observation next is [1.0, 0.6521739130434783, 0.6818181818181818, 0.395, 1.0, 1.0, 0.2908933460841092, 1.0, 1.0, 0.2908933460841092, 1.0, 1.0, 0.5566521122979848, 0.0, 0.0, 0.28275820603573193, 0.28275820603573193, 0.38282537126244626], 
reward next is 0.6331, 
noisyNet noise sample is [array([-0.21477324], dtype=float32), -0.3741808]. 
=============================================
[2019-03-08 09:38:34,105] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7909: loss 0.0497
[2019-03-08 09:38:34,169] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7912: learning rate 0.0010
[2019-03-08 09:38:34,185] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7924: loss 0.0135
[2019-03-08 09:38:34,186] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7925: learning rate 0.0010
[2019-03-08 09:38:34,204] A3C_AGENT_WORKER-Thread-8 INFO:Local step 500, global step 7936: loss 0.0056
[2019-03-08 09:38:34,205] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 500, global step 7936: learning rate 0.0010
[2019-03-08 09:38:34,221] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7944: loss 0.0222
[2019-03-08 09:38:34,223] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7944: loss 0.0021
[2019-03-08 09:38:34,224] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7944: learning rate 0.0010
[2019-03-08 09:38:34,225] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7944: learning rate 0.0010
[2019-03-08 09:38:34,269] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7965: loss 0.0341
[2019-03-08 09:38:34,270] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7965: learning rate 0.0010
[2019-03-08 09:38:34,296] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7976: loss 0.0913
[2019-03-08 09:38:34,299] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7976: learning rate 0.0010
[2019-03-08 09:38:34,304] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7979: loss 0.1836
[2019-03-08 09:38:34,306] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7980: learning rate 0.0010
[2019-03-08 09:38:34,321] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7987: loss 0.1362
[2019-03-08 09:38:34,323] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7988: learning rate 0.0010
[2019-03-08 09:38:34,352] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7999: loss 0.2027
[2019-03-08 09:38:34,354] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7999: learning rate 0.0010
[2019-03-08 09:38:34,375] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8012: loss 0.2011
[2019-03-08 09:38:34,386] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8015: learning rate 0.0010
[2019-03-08 09:38:34,412] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8030: loss 0.0855
[2019-03-08 09:38:34,417] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8031: learning rate 0.0010
[2019-03-08 09:38:34,418] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 8031: loss 0.1352
[2019-03-08 09:38:34,423] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 8033: learning rate 0.0010
[2019-03-08 09:38:34,437] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8039: loss 0.0814
[2019-03-08 09:38:34,441] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8040: learning rate 0.0010
[2019-03-08 09:38:34,460] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8050: loss 0.0463
[2019-03-08 09:38:34,461] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8050: learning rate 0.0010
[2019-03-08 09:38:34,477] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8058: loss 0.0327
[2019-03-08 09:38:34,479] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8058: learning rate 0.0010
[2019-03-08 09:38:47,043] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9788541e-01 2.1145095e-03 6.5871913e-10 2.8165292e-08 1.1582671e-08], sum to 1.0000
[2019-03-08 09:38:47,049] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0287
[2019-03-08 09:38:47,158] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4652293591642785, 6.9112, 6.9112, 189031.5867738654, 189031.5867738654, 53783.49088576768], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 520800.0000, 
sim time next is 521400.0000, 
raw observation next is [14.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4645615172935174, 6.9112, 6.9112, 188759.9827241922, 188759.9827241922, 53718.15942749201], 
processed observation next is [1.0, 0.0, 0.2727272727272727, 0.94, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4645615172935174, 0.0, 0.0, 0.07864999280174675, 0.07864999280174675, 0.13429539856873005], 
reward next is 0.5020, 
noisyNet noise sample is [array([0.07672077], dtype=float32), -2.6555398]. 
=============================================
[2019-03-08 09:38:49,619] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9999988e-01 8.9321141e-08 1.6950623e-11 2.2865661e-11 2.0928762e-10], sum to 1.0000
[2019-03-08 09:38:49,629] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1521
[2019-03-08 09:38:49,640] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.0, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 8.363520871167118, 6.9112, 501526.7545793731, 388336.4192830817, 101356.7793023404], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 563400.0000, 
sim time next is 564000.0000, 
raw observation next is [20.33333333333334, 70.33333333333334, 1.0, 1.0, 0.4243059034156563, 1.0, 1.0, 0.4243059034156563, 0.0, 1.0, 0.0, 6.9112, 6.9112, 657968.1157906583, 657968.1157906583, 117140.6528241628], 
processed observation next is [1.0, 0.5217391304347826, 0.5606060606060609, 0.7033333333333335, 1.0, 0.5, 0.4243059034156563, 1.0, 0.5, 0.4243059034156563, 0.0, 0.5, 0.0, 0.0, 0.0, 0.27415338157944097, 0.27415338157944097, 0.292851632060407], 
reward next is 0.8024, 
noisyNet noise sample is [array([0.6816925], dtype=float32), 0.3954389]. 
=============================================
[2019-03-08 09:38:49,650] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[52.986614]
 [53.118893]
 [52.213596]
 [51.963657]
 [43.984905]], R is [[43.67753983]
 [43.24076462]
 [43.34032059]
 [43.42288971]
 [43.50125885]].
[2019-03-08 09:38:51,545] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15921: loss 0.0080
[2019-03-08 09:38:51,551] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15924: learning rate 0.0010
[2019-03-08 09:38:51,551] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1000, global step 15924: loss 0.0039
[2019-03-08 09:38:51,557] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1000, global step 15925: learning rate 0.0010
[2019-03-08 09:38:51,565] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15929: loss 0.0036
[2019-03-08 09:38:51,571] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15931: learning rate 0.0010
[2019-03-08 09:38:51,596] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15941: loss 0.0085
[2019-03-08 09:38:51,601] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15942: learning rate 0.0010
[2019-03-08 09:38:51,614] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15952: loss 0.0346
[2019-03-08 09:38:51,617] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15953: learning rate 0.0010
[2019-03-08 09:38:51,629] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15956: loss 0.0367
[2019-03-08 09:38:51,630] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15956: learning rate 0.0010
[2019-03-08 09:38:51,642] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 15964: loss 0.0616
[2019-03-08 09:38:51,648] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 15965: learning rate 0.0010
[2019-03-08 09:38:51,654] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15968: loss 0.0935
[2019-03-08 09:38:51,657] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15968: learning rate 0.0010
[2019-03-08 09:38:51,668] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 15971: loss 0.1364
[2019-03-08 09:38:51,674] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 15971: learning rate 0.0010
[2019-03-08 09:38:51,711] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15991: loss 0.2677
[2019-03-08 09:38:51,715] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15992: learning rate 0.0010
[2019-03-08 09:38:51,726] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15997: loss 0.3493
[2019-03-08 09:38:51,729] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15998: learning rate 0.0010
[2019-03-08 09:38:51,819] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16048: loss 0.1875
[2019-03-08 09:38:51,820] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16048: learning rate 0.0010
[2019-03-08 09:38:51,820] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 16048: loss 0.0297
[2019-03-08 09:38:51,825] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 16050: learning rate 0.0010
[2019-03-08 09:38:51,850] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16060: loss 0.0500
[2019-03-08 09:38:51,852] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16060: loss 0.0064
[2019-03-08 09:38:51,854] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16060: learning rate 0.0010
[2019-03-08 09:38:51,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16060: learning rate 0.0010
[2019-03-08 09:38:51,895] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16076: loss 0.0043
[2019-03-08 09:38:51,901] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16077: learning rate 0.0010
[2019-03-08 09:38:54,153] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0000000e+00 3.0251475e-16 4.8067068e-13 1.3649125e-19 1.9873465e-18], sum to 1.0000
[2019-03-08 09:38:54,163] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2203
[2019-03-08 09:38:54,181] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.5, 63.0, 1.0, 2.0, 0.4002811280726452, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7539676042259956, 6.911199999999999, 6.9112, 612981.7470825744, 612981.7470825745, 138754.2567523992], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 642600.0000, 
sim time next is 643200.0000, 
raw observation next is [22.66666666666666, 61.0, 1.0, 2.0, 0.3665649269403601, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6941691339978501, 6.911199999999998, 6.9112, 564310.4786975677, 564310.478697568, 130365.0524581371], 
processed observation next is [1.0, 0.43478260869565216, 0.6666666666666664, 0.61, 1.0, 1.0, 0.3665649269403601, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6941691339978501, -1.7763568394002506e-16, 0.0, 0.23512936612398655, 0.23512936612398663, 0.32591263114534275], 
reward next is 0.6184, 
noisyNet noise sample is [array([-0.1695757], dtype=float32), 1.6711614]. 
=============================================
[2019-03-08 09:38:55,116] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0000000e+00 7.0678706e-14 5.3986487e-13 1.6438580e-18 6.3530902e-16], sum to 1.0000
[2019-03-08 09:38:55,125] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9037
[2019-03-08 09:38:55,135] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.0, 50.0, 1.0, 2.0, 0.5349819675859062, 1.0, 2.0, 0.5349819675859062, 0.0, 2.0, 0.0, 6.9112, 6.9112, 820208.897719442, 820208.897719442, 144602.065661705], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 658800.0000, 
sim time next is 659400.0000, 
raw observation next is [25.16666666666667, 50.16666666666667, 1.0, 2.0, 0.3162226486764985, 1.0, 2.0, 0.3162226486764985, 1.0, 1.0, 0.5914345762782299, 6.911199999999999, 6.9112, 721063.3812639123, 721063.3812639125, 162832.2260958679], 
processed observation next is [1.0, 0.6521739130434783, 0.7803030303030305, 0.5016666666666667, 1.0, 1.0, 0.3162226486764985, 1.0, 1.0, 0.3162226486764985, 1.0, 0.5, 0.5914345762782299, -8.881784197001253e-17, 0.0, 0.30044307552663013, 0.30044307552663024, 0.40708056523966973], 
reward next is 0.6326, 
noisyNet noise sample is [array([0.23055767], dtype=float32), -0.9138678]. 
=============================================
[2019-03-08 09:39:01,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9998951e-01 7.1632849e-07 6.7603258e-07 7.7174464e-06 1.3421338e-06], sum to 1.0000
[2019-03-08 09:39:01,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6537
[2019-03-08 09:39:01,593] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 69.0, 1.0, 2.0, 0.6236880265133623, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 475099.0249060782, 475099.0249060782, 95733.7663302069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 771000.0000, 
sim time next is 771600.0000, 
raw observation next is [24.0, 69.0, 1.0, 2.0, 0.6210789354419852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 474151.1967596122, 474151.1967596121, 95124.26968409895], 
processed observation next is [1.0, 0.9565217391304348, 0.7272727272727273, 0.69, 1.0, 1.0, 0.6210789354419852, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.19756299864983842, 0.1975629986498384, 0.23781067421024737], 
reward next is 0.7121, 
noisyNet noise sample is [array([0.9144086], dtype=float32), -1.5098672]. 
=============================================
[2019-03-08 09:39:03,296] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9969840e-01 5.4495881e-06 2.3927075e-04 5.3134907e-05 3.6870099e-06], sum to 1.0000
[2019-03-08 09:39:03,305] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3444
[2019-03-08 09:39:03,414] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.0, 94.0, 1.0, 2.0, 0.19, 1.0, 1.0, 0.19, 1.0, 2.0, 0.3536449751702038, 6.911199999999999, 6.9112, 430969.106749465, 430969.1067494652, 125801.0585680217], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 799200.0000, 
sim time next is 799800.0000, 
raw observation next is [19.33333333333334, 93.00000000000001, 1.0, 2.0, 0.2445485318629461, 1.0, 2.0, 0.2445485318629461, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 374931.7836857125, 374931.7836857126, 89918.05449486834], 
processed observation next is [0.0, 0.2608695652173913, 0.5151515151515155, 0.9300000000000002, 1.0, 1.0, 0.2445485318629461, 1.0, 1.0, 0.2445485318629461, 0.0, 0.5, 0.0, -8.881784197001253e-17, 0.0, 0.15622157653571353, 0.15622157653571359, 0.22479513623717085], 
reward next is 0.5957, 
noisyNet noise sample is [array([-0.7333535], dtype=float32), -0.7986558]. 
=============================================
[2019-03-08 09:39:04,166] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9613369e-01 2.7609050e-07 3.7597488e-03 5.6486042e-05 4.9847142e-05], sum to 1.0000
[2019-03-08 09:39:04,174] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4360
[2019-03-08 09:39:04,185] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.66666666666666, 81.66666666666667, 1.0, 2.0, 0.3785451024473079, 1.0, 2.0, 0.3785451024473079, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 555986.4776757789, 555986.4776757788, 114714.8947026567], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 811200.0000, 
sim time next is 811800.0000, 
raw observation next is [25.0, 81.0, 1.0, 2.0, 0.3031409241019946, 1.0, 2.0, 0.3031409241019946, 1.0, 1.0, 0.5439073466378049, 6.911199999999999, 6.9112, 662092.6763836867, 662092.676383687, 160083.7742036666], 
processed observation next is [0.0, 0.391304347826087, 0.7727272727272727, 0.81, 1.0, 1.0, 0.3031409241019946, 1.0, 1.0, 0.3031409241019946, 1.0, 0.5, 0.5439073466378049, -8.881784197001253e-17, 0.0, 0.2758719484932028, 0.2758719484932029, 0.4002094355091665], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.23506045], dtype=float32), 1.0764039]. 
=============================================
[2019-03-08 09:39:04,892] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9314249e-01 5.8613864e-10 1.9880945e-05 6.8366602e-03 9.0669545e-07], sum to 1.0000
[2019-03-08 09:39:04,898] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2678
[2019-03-08 09:39:04,997] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 56.0, 1.0, 2.0, 0.4132625166087803, 0.0, 2.0, 0.0, 1.0, 1.0, 0.7487608171942226, 6.911200000000001, 6.9112, 608716.8279566703, 608716.8279566702, 142831.8587126072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 826800.0000, 
sim time next is 827400.0000, 
raw observation next is [29.0, 55.5, 1.0, 2.0, 0.7873089983745724, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 584795.4921085187, 584795.4921085187, 125136.4655740182], 
processed observation next is [0.0, 0.5652173913043478, 0.9545454545454546, 0.555, 1.0, 1.0, 0.7873089983745724, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.24366478837854946, 0.24366478837854946, 0.3128411639350455], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.35773325], dtype=float32), -0.9076767]. 
=============================================
[2019-03-08 09:39:06,546] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3087424e-03 9.3544582e-12 8.8215735e-08 9.9769115e-01 1.5369471e-08], sum to 1.0000
[2019-03-08 09:39:06,556] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8516
[2019-03-08 09:39:06,658] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.66666666666667, 73.66666666666667, 1.0, 2.0, 0.3255451444108969, 1.0, 2.0, 0.3255451444108969, 0.0, 2.0, 0.0, 6.9112, 6.9112, 490805.0888592408, 490805.0888592408, 103351.5561635755], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 854400.0000, 
sim time next is 855000.0000, 
raw observation next is [23.0, 76.0, 1.0, 2.0, 0.3172419660820555, 1.0, 2.0, 0.3172419660820555, 0.0, 2.0, 0.0, 6.9112, 6.9112, 479730.265783043, 479730.265783043, 101737.7344705204], 
processed observation next is [0.0, 0.9130434782608695, 0.6818181818181818, 0.76, 1.0, 1.0, 0.3172419660820555, 1.0, 1.0, 0.3172419660820555, 0.0, 1.0, 0.0, 0.0, 0.0, 0.19988761074293457, 0.19988761074293457, 0.254344336176301], 
reward next is 0.6736, 
noisyNet noise sample is [array([-0.9114535], dtype=float32), 0.18343216]. 
=============================================
[2019-03-08 09:39:06,670] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[52.107388]
 [52.312862]
 [52.50791 ]
 [52.475956]
 [52.542862]], R is [[52.04442978]
 [52.20240021]
 [52.36138153]
 [52.52013016]
 [52.67869568]].
[2019-03-08 09:39:09,187] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23907: loss 0.1481
[2019-03-08 09:39:09,193] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23910: learning rate 0.0010
[2019-03-08 09:39:09,212] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23922: loss 0.0323
[2019-03-08 09:39:09,216] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23922: learning rate 0.0010
[2019-03-08 09:39:09,242] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23933: loss 0.0569
[2019-03-08 09:39:09,247] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23934: learning rate 0.0010
[2019-03-08 09:39:09,256] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23941: loss 0.0319
[2019-03-08 09:39:09,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23942: learning rate 0.0010
[2019-03-08 09:39:09,282] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23952: loss 0.0052
[2019-03-08 09:39:09,286] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23953: learning rate 0.0010
[2019-03-08 09:39:09,292] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23954: loss 0.0047
[2019-03-08 09:39:09,294] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23955: learning rate 0.0010
[2019-03-08 09:39:09,298] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1500, global step 23956: loss 0.0251
[2019-03-08 09:39:09,300] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1500, global step 23956: learning rate 0.0010
[2019-03-08 09:39:09,307] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23959: loss 0.0116
[2019-03-08 09:39:09,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23959: learning rate 0.0010
[2019-03-08 09:39:09,354] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23979: loss 0.0362
[2019-03-08 09:39:09,358] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23982: learning rate 0.0010
[2019-03-08 09:39:09,363] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 23984: loss 0.0772
[2019-03-08 09:39:09,365] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 23984: learning rate 0.0010
[2019-03-08 09:39:09,460] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 24027: loss 0.1402
[2019-03-08 09:39:09,463] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 24028: learning rate 0.0010
[2019-03-08 09:39:09,463] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24028: loss 0.2459
[2019-03-08 09:39:09,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24034: learning rate 0.0010
[2019-03-08 09:39:09,476] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 24037: loss 0.2594
[2019-03-08 09:39:09,483] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 24037: learning rate 0.0010
[2019-03-08 09:39:09,495] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 24041: loss 0.1197
[2019-03-08 09:39:09,498] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 24042: learning rate 0.0010
[2019-03-08 09:39:09,563] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24070: loss 0.0362
[2019-03-08 09:39:09,568] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24071: learning rate 0.0010
[2019-03-08 09:39:09,608] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24090: loss 0.0070
[2019-03-08 09:39:09,613] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24090: learning rate 0.0010
[2019-03-08 09:39:16,216] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1350826e-05 2.7733296e-10 1.7138358e-08 9.9998844e-01 1.9707637e-07], sum to 1.0000
[2019-03-08 09:39:16,223] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1587
[2019-03-08 09:39:16,229] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.0, 95.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 222476.4858874617, 222476.4858874617, 63028.91110305785], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1018200.0000, 
sim time next is 1018800.0000, 
raw observation next is [14.0, 94.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 219049.9211290783, 219049.9211290783, 62227.38330430552], 
processed observation next is [1.0, 0.8260869565217391, 0.2727272727272727, 0.94, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0912708004704493, 0.0912708004704493, 0.1555684582607638], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.757785], dtype=float32), 1.4180753]. 
=============================================
[2019-03-08 09:39:17,828] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.9433153e-13 1.1437357e-12 1.0047387e-09 8.2253491e-13], sum to 1.0000
[2019-03-08 09:39:17,838] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3569
[2019-03-08 09:39:17,843] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [13.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4256659174662694, 6.9112, 6.9112, 172942.7916595861, 172942.7916595861, 49440.16592002802], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1048800.0000, 
sim time next is 1049400.0000, 
raw observation next is [13.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4276371768912102, 6.911199999999999, 6.9112, 173744.3612104715, 173744.3612104716, 49622.15698403822], 
processed observation next is [1.0, 0.13043478260869565, 0.22727272727272727, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4276371768912102, -8.881784197001253e-17, 0.0, 0.07239348383769646, 0.0723934838376965, 0.12405539246009556], 
reward next is 0.5002, 
noisyNet noise sample is [array([-0.77889794], dtype=float32), 2.100029]. 
=============================================
[2019-03-08 09:39:22,982] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.1004041e-16 1.5074170e-13 2.0463153e-09 1.1208065e-14], sum to 1.0000
[2019-03-08 09:39:22,993] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6027
[2019-03-08 09:39:23,100] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 89.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7712016022626178, 6.9112, 6.9112, 313491.1854014878, 313491.1854014878, 87708.7029624882], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1134600.0000, 
sim time next is 1135200.0000, 
raw observation next is [18.0, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7771046298155713, 6.9112, 6.9112, 315893.6587817639, 315893.6587817639, 88206.40111615679], 
processed observation next is [1.0, 0.13043478260869565, 0.45454545454545453, 0.9, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7771046298155713, 0.0, 0.0, 0.13162235782573498, 0.13162235782573498, 0.22051600279039196], 
reward next is 0.5116, 
noisyNet noise sample is [array([-0.45140582], dtype=float32), 0.4604001]. 
=============================================
[2019-03-08 09:39:25,136] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.9400117e-13 1.2198260e-10 2.4073085e-08 1.1934713e-12], sum to 1.0000
[2019-03-08 09:39:25,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2510
[2019-03-08 09:39:25,162] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.66666666666667, 67.33333333333334, 1.0, 2.0, 0.6561410593486413, 1.0, 2.0, 0.6561410593486413, 0.0, 2.0, 0.0, 6.9112, 6.9112, 963225.2596057642, 963225.2596057642, 186855.4012780084], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1172400.0000, 
sim time next is 1173000.0000, 
raw observation next is [26.83333333333333, 66.66666666666666, 1.0, 2.0, 0.8395112080012307, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 6.911200000000001, 6.9112, 1003516.265960579, 1003516.265960579, 228002.9729562265], 
processed observation next is [1.0, 0.5652173913043478, 0.8560606060606059, 0.6666666666666665, 1.0, 1.0, 0.8395112080012307, 0.0, 0.5, 0.0, 1.0, 0.5, 0.9547116395713053, 8.881784197001253e-17, 0.0, 0.4181317774835746, 0.4181317774835746, 0.5700074323905663], 
reward next is 0.6288, 
noisyNet noise sample is [array([1.2788556], dtype=float32), -0.46896988]. 
=============================================
[2019-03-08 09:39:25,175] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[55.84542 ]
 [55.631676]
 [55.76627 ]
 [57.433414]
 [56.77531 ]], R is [[57.46411514]
 [57.62588882]
 [57.74555206]
 [57.16809845]
 [57.22582245]].
[2019-03-08 09:39:26,778] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31916: loss 77.3610
[2019-03-08 09:39:26,781] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31916: learning rate 0.0010
[2019-03-08 09:39:26,814] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31929: loss -24.3968
[2019-03-08 09:39:26,816] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31929: learning rate 0.0010
[2019-03-08 09:39:26,823] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31933: loss -4.2269
[2019-03-08 09:39:26,825] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31934: learning rate 0.0010
[2019-03-08 09:39:26,827] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31935: loss 55.4432
[2019-03-08 09:39:26,828] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31935: learning rate 0.0010
[2019-03-08 09:39:26,834] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31939: loss -42.6424
[2019-03-08 09:39:26,836] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31940: learning rate 0.0010
[2019-03-08 09:39:26,870] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31955: loss -70.2185
[2019-03-08 09:39:26,873] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31956: learning rate 0.0010
[2019-03-08 09:39:26,907] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31977: loss 35.2551
[2019-03-08 09:39:26,909] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31977: learning rate 0.0010
[2019-03-08 09:39:26,912] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31979: loss -22.6955
[2019-03-08 09:39:26,915] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31979: learning rate 0.0010
[2019-03-08 09:39:26,928] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31985: loss -23.1464
[2019-03-08 09:39:26,932] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31985: learning rate 0.0010
[2019-03-08 09:39:26,945] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2000, global step 31992: loss -103.1345
[2019-03-08 09:39:26,950] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2000, global step 31992: learning rate 0.0010
[2019-03-08 09:39:26,953] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 31992: loss 55.8006
[2019-03-08 09:39:26,956] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 31993: learning rate 0.0010
[2019-03-08 09:39:26,983] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32000: loss -62.4099
[2019-03-08 09:39:26,986] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32000: learning rate 0.0010
[2019-03-08 09:39:27,108] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 32063: loss 54.7797
[2019-03-08 09:39:27,110] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32063: loss 38.0243
[2019-03-08 09:39:27,112] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 32063: learning rate 0.0010
[2019-03-08 09:39:27,113] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32064: learning rate 0.0010
[2019-03-08 09:39:27,124] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32067: loss -11.5519
[2019-03-08 09:39:27,132] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32068: learning rate 0.0010
[2019-03-08 09:39:27,140] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32074: loss -67.5952
[2019-03-08 09:39:27,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32075: learning rate 0.0010
[2019-03-08 09:39:29,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9543081e-07 5.1326964e-14 1.3607467e-12 9.9999905e-01 6.3593374e-13], sum to 1.0000
[2019-03-08 09:39:29,887] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6136
[2019-03-08 09:39:29,993] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 65.0, 1.0, 2.0, 0.6202844030377154, 1.0, 2.0, 0.6202844030377154, 0.0, 2.0, 0.0, 6.9112, 6.9112, 922124.1336680235, 922124.1336680235, 173511.822772414], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1249200.0000, 
sim time next is 1249800.0000, 
raw observation next is [26.0, 65.0, 1.0, 2.0, 0.9671450140051946, 1.0, 2.0, 0.9671450140051946, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 1430474.758705556, 1430474.758705556, 312226.9277611337], 
processed observation next is [1.0, 0.4782608695652174, 0.8181818181818182, 0.65, 1.0, 1.0, 0.9671450140051946, 1.0, 1.0, 0.9671450140051946, 0.0, 1.0, 0.0, -1.7763568394002506e-16, 0.0, 0.5960311494606483, 0.5960311494606483, 0.7805673194028343], 
reward next is 0.6545, 
noisyNet noise sample is [array([0.3747149], dtype=float32), 0.39356044]. 
=============================================
[2019-03-08 09:39:30,561] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3433736e-14 1.0680030e-18 7.2631551e-16 1.0000000e+00 1.3478644e-18], sum to 1.0000
[2019-03-08 09:39:30,573] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8181
[2019-03-08 09:39:30,683] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 60.0, 1.0, 2.0, 0.8627462159271772, 1.0, 2.0, 0.8627462159271772, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1262048.849883445, 1262048.849883445, 269655.5866750145], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1261800.0000, 
sim time next is 1262400.0000, 
raw observation next is [28.0, 60.66666666666666, 1.0, 2.0, 0.9499061841646709, 1.0, 2.0, 0.9499061841646709, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1386179.194480378, 1386179.194480378, 307233.2830025068], 
processed observation next is [1.0, 0.6086956521739131, 0.9090909090909091, 0.6066666666666666, 1.0, 1.0, 0.9499061841646709, 1.0, 1.0, 0.9499061841646709, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.5775746643668241, 0.5775746643668241, 0.768083207506267], 
reward next is 0.6445, 
noisyNet noise sample is [array([-0.81547356], dtype=float32), 1.4612099]. 
=============================================
[2019-03-08 09:39:36,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0448078e-11 4.3797228e-16 4.3296950e-13 1.0000000e+00 3.9957509e-14], sum to 1.0000
[2019-03-08 09:39:36,833] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2642
[2019-03-08 09:39:36,839] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.66666666666667, 81.33333333333334, 1.0, 2.0, 0.3364848028395691, 1.0, 2.0, 0.3364848028395691, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 507099.6520001739, 507099.652000174, 105205.6332463007], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1362000.0000, 
sim time next is 1362600.0000, 
raw observation next is [22.5, 83.0, 1.0, 2.0, 0.3445416268949296, 1.0, 2.0, 0.3445416268949296, 0.0, 2.0, 0.0, 6.9112, 6.9112, 518907.0003391707, 518907.0003391707, 106634.2066861844], 
processed observation next is [1.0, 0.782608695652174, 0.6590909090909091, 0.83, 1.0, 1.0, 0.3445416268949296, 1.0, 1.0, 0.3445416268949296, 0.0, 1.0, 0.0, 0.0, 0.0, 0.21621125014132112, 0.21621125014132112, 0.266585516715461], 
reward next is 0.6952, 
noisyNet noise sample is [array([-0.6240856], dtype=float32), 0.672687]. 
=============================================
[2019-03-08 09:39:44,314] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39880: loss 0.1494
[2019-03-08 09:39:44,320] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39880: learning rate 0.0010
[2019-03-08 09:39:44,455] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39940: loss 0.2395
[2019-03-08 09:39:44,458] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39941: learning rate 0.0010
[2019-03-08 09:39:44,469] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39947: loss 0.2473
[2019-03-08 09:39:44,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39947: learning rate 0.0010
[2019-03-08 09:39:44,481] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39952: loss 0.2572
[2019-03-08 09:39:44,482] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39954: learning rate 0.0010
[2019-03-08 09:39:44,490] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 39957: loss 0.2105
[2019-03-08 09:39:44,494] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 39958: learning rate 0.0010
[2019-03-08 09:39:44,504] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39962: loss 0.2397
[2019-03-08 09:39:44,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39962: learning rate 0.0010
[2019-03-08 09:39:44,521] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2500, global step 39966: loss 0.2458
[2019-03-08 09:39:44,525] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2500, global step 39968: learning rate 0.0010
[2019-03-08 09:39:44,534] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39972: loss 0.2132
[2019-03-08 09:39:44,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39973: learning rate 0.0010
[2019-03-08 09:39:44,565] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39983: loss 0.1627
[2019-03-08 09:39:44,568] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39984: learning rate 0.0010
[2019-03-08 09:39:44,576] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39989: loss 0.1292
[2019-03-08 09:39:44,580] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39989: learning rate 0.0010
[2019-03-08 09:39:44,606] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40000: loss 0.1439
[2019-03-08 09:39:44,610] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40000: learning rate 0.0010
[2019-03-08 09:39:44,622] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 40008: loss 0.2310
[2019-03-08 09:39:44,625] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 40008: learning rate 0.0010
[2019-03-08 09:39:44,693] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 40041: loss 0.0951
[2019-03-08 09:39:44,696] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 40041: learning rate 0.0010
[2019-03-08 09:39:44,718] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 40050: loss 0.0324
[2019-03-08 09:39:44,721] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 40050: learning rate 0.0010
[2019-03-08 09:39:44,738] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40058: loss 0.0141
[2019-03-08 09:39:44,740] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40058: learning rate 0.0010
[2019-03-08 09:39:44,782] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40080: loss 0.0276
[2019-03-08 09:39:44,786] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40082: learning rate 0.0010
[2019-03-08 09:39:48,848] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3708158e-09 1.4663794e-13 1.7065373e-14 1.0000000e+00 1.6881038e-14], sum to 1.0000
[2019-03-08 09:39:48,859] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3363
[2019-03-08 09:39:48,957] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.0, 100.0, 1.0, 2.0, 0.2662156625144156, 1.0, 2.0, 0.2662156625144156, 0.0, 2.0, 0.0, 6.9112, 6.9112, 406852.6977344343, 406852.6977344343, 93130.55526616023], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1572600.0000, 
sim time next is 1573200.0000, 
raw observation next is [19.0, 100.0, 1.0, 2.0, 0.2671812010998848, 1.0, 2.0, 0.2671812010998848, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 408325.1177345121, 408325.1177345123, 93264.82273859976], 
processed observation next is [1.0, 0.21739130434782608, 0.5, 1.0, 1.0, 1.0, 0.2671812010998848, 1.0, 1.0, 0.2671812010998848, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.17013546572271337, 0.17013546572271346, 0.2331620568464994], 
reward next is 0.6254, 
noisyNet noise sample is [array([-1.3073195], dtype=float32), 0.0095871]. 
=============================================
[2019-03-08 09:39:50,032] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2340431e-09 5.3384780e-14 8.6475022e-11 1.0000000e+00 1.2039823e-15], sum to 1.0000
[2019-03-08 09:39:50,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0925
[2019-03-08 09:39:50,054] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.66666666666667, 79.0, 1.0, 2.0, 0.524931302503218, 1.0, 2.0, 0.524931302503218, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 782052.7473237069, 782052.747323707, 145378.9089070342], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1593600.0000, 
sim time next is 1594200.0000, 
raw observation next is [23.83333333333333, 76.5, 1.0, 2.0, 0.5241314080849716, 1.0, 2.0, 0.5241314080849716, 0.0, 2.0, 0.0, 6.9112, 6.9112, 782418.5221551087, 782418.5221551087, 144949.7160341421], 
processed observation next is [1.0, 0.43478260869565216, 0.7196969696969695, 0.765, 1.0, 1.0, 0.5241314080849716, 1.0, 1.0, 0.5241314080849716, 0.0, 1.0, 0.0, 0.0, 0.0, 0.32600771756462865, 0.32600771756462865, 0.36237429008535527], 
reward next is 0.7711, 
noisyNet noise sample is [array([0.05118775], dtype=float32), 0.03269548]. 
=============================================
[2019-03-08 09:39:59,464] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.8970258e-01 1.9784471e-04 9.8681990e-03 1.8711100e-04 4.4209835e-05], sum to 1.0000
[2019-03-08 09:39:59,474] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6245
[2019-03-08 09:39:59,483] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [8.833333333333332, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.25, 6.9112, 6.9112, 91155.95424310317, 91155.95424310317, 27218.19880560312], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [9.0, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.25, 6.9112, 6.9112, 93287.54849821958, 93287.54849821958, 27680.32756504889], 
processed observation next is [1.0, 0.2608695652173913, 0.045454545454545456, 0.81, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.038869811874258156, 0.038869811874258156, 0.06920081891262223], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3970016], dtype=float32), 0.615322]. 
=============================================
[2019-03-08 09:40:01,946] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47902: loss 3.6991
[2019-03-08 09:40:01,947] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47902: loss 0.1441
[2019-03-08 09:40:01,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47903: learning rate 0.0010
[2019-03-08 09:40:01,952] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47903: learning rate 0.0010
[2019-03-08 09:40:01,988] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47917: loss -0.0498
[2019-03-08 09:40:01,992] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47917: learning rate 0.0010
[2019-03-08 09:40:02,023] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47935: loss 2.8324
[2019-03-08 09:40:02,026] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47935: learning rate 0.0010
[2019-03-08 09:40:02,049] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 47947: loss 0.1147
[2019-03-08 09:40:02,053] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 47948: learning rate 0.0010
[2019-03-08 09:40:02,062] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 47954: loss 0.1088
[2019-03-08 09:40:02,064] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 47955: learning rate 0.0010
[2019-03-08 09:40:02,076] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47960: loss 3.4846
[2019-03-08 09:40:02,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47960: learning rate 0.0010
[2019-03-08 09:40:02,093] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 47969: loss 0.0556
[2019-03-08 09:40:02,094] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 47969: learning rate 0.0010
[2019-03-08 09:40:02,114] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47977: loss 0.0482
[2019-03-08 09:40:02,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47977: learning rate 0.0010
[2019-03-08 09:40:02,148] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47991: loss 0.0366
[2019-03-08 09:40:02,151] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47992: learning rate 0.0010
[2019-03-08 09:40:02,163] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3000, global step 47998: loss 1.8417
[2019-03-08 09:40:02,167] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3000, global step 47999: learning rate 0.0010
[2019-03-08 09:40:02,252] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48036: loss 2.5431
[2019-03-08 09:40:02,255] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48036: learning rate 0.0010
[2019-03-08 09:40:02,275] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 48047: loss 0.0114
[2019-03-08 09:40:02,279] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 48047: learning rate 0.0010
[2019-03-08 09:40:02,326] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 48075: loss 0.0005
[2019-03-08 09:40:02,330] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 48076: learning rate 0.0010
[2019-03-08 09:40:02,340] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48082: loss 0.0004
[2019-03-08 09:40:02,345] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48082: learning rate 0.0010
[2019-03-08 09:40:02,389] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 48107: loss 0.0011
[2019-03-08 09:40:02,392] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 48107: learning rate 0.0010
[2019-03-08 09:40:06,315] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.7486418e-01 1.0228747e-05 2.3998071e-02 2.3809429e-04 8.8933244e-04], sum to 1.0000
[2019-03-08 09:40:06,325] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8261
[2019-03-08 09:40:06,428] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.66666666666667, 43.0, 1.0, 2.0, 0.8323784896398005, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 650885.6698301418, 650885.6698301418, 121719.2290050185], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1870800.0000, 
sim time next is 1871400.0000, 
raw observation next is [23.83333333333333, 43.5, 1.0, 2.0, 0.4512711595788687, 1.0, 1.0, 0.4512711595788687, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 702259.9822277429, 702259.9822277427, 121546.4835301585], 
processed observation next is [1.0, 0.6521739130434783, 0.7196969696969695, 0.435, 1.0, 1.0, 0.4512711595788687, 1.0, 0.5, 0.4512711595788687, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.2926083259282262, 0.2926083259282261, 0.3038662088253962], 
reward next is 0.8254, 
noisyNet noise sample is [array([0.970422], dtype=float32), -0.7887355]. 
=============================================
[2019-03-08 09:40:12,862] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 6.4410365e-16 1.0186562e-11 7.1957304e-15 1.5346280e-14], sum to 1.0000
[2019-03-08 09:40:12,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7968
[2019-03-08 09:40:12,875] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 61.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7472617659955484, 6.9112, 6.9112, 303748.3649144675, 303748.3649144675, 85693.93374679111], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1981200.0000, 
sim time next is 1981800.0000, 
raw observation next is [21.0, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7514774892258064, 6.9112, 6.9112, 305463.9893889349, 305463.9893889349, 86048.24371743482], 
processed observation next is [1.0, 0.9565217391304348, 0.5909090909090909, 0.62, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7514774892258064, 0.0, 0.0, 0.12727666224538955, 0.12727666224538955, 0.21512060929358703], 
reward next is 0.5071, 
noisyNet noise sample is [array([0.08627979], dtype=float32), 0.42589125]. 
=============================================
[2019-03-08 09:40:19,415] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55897: loss 0.1332
[2019-03-08 09:40:19,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55897: learning rate 0.0010
[2019-03-08 09:40:19,432] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55902: loss 0.1042
[2019-03-08 09:40:19,432] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 55902: loss 0.1191
[2019-03-08 09:40:19,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55902: learning rate 0.0010
[2019-03-08 09:40:19,436] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 55902: learning rate 0.0010
[2019-03-08 09:40:19,466] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55920: loss 0.1271
[2019-03-08 09:40:19,468] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55921: learning rate 0.0010
[2019-03-08 09:40:19,516] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55940: loss 0.1048
[2019-03-08 09:40:19,521] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55941: learning rate 0.0010
[2019-03-08 09:40:19,539] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55955: loss 0.0824
[2019-03-08 09:40:19,543] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55955: learning rate 0.0010
[2019-03-08 09:40:19,555] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55962: loss 0.0684
[2019-03-08 09:40:19,564] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55962: learning rate 0.0010
[2019-03-08 09:40:19,575] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55969: loss 0.0618
[2019-03-08 09:40:19,580] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55970: learning rate 0.0010
[2019-03-08 09:40:19,585] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3500, global step 55973: loss 0.0603
[2019-03-08 09:40:19,588] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3500, global step 55973: learning rate 0.0010
[2019-03-08 09:40:19,593] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55974: loss 0.0715
[2019-03-08 09:40:19,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55976: learning rate 0.0010
[2019-03-08 09:40:19,645] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55998: loss 0.0530
[2019-03-08 09:40:19,647] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55998: learning rate 0.0010
[2019-03-08 09:40:19,726] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56033: loss 0.0550
[2019-03-08 09:40:19,729] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56034: learning rate 0.0010
[2019-03-08 09:40:19,756] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56050: loss 0.0631
[2019-03-08 09:40:19,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56050: learning rate 0.0010
[2019-03-08 09:40:19,838] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 56086: loss 0.0548
[2019-03-08 09:40:19,840] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 56087: learning rate 0.0010
[2019-03-08 09:40:19,880] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56103: loss 0.0573
[2019-03-08 09:40:19,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56104: learning rate 0.0010
[2019-03-08 09:40:19,912] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 56113: loss 0.0558
[2019-03-08 09:40:19,915] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 56113: learning rate 0.0010
[2019-03-08 09:40:24,411] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0000000e+00 2.1225899e-13 9.2283053e-10 7.7352069e-11 1.9742164e-12], sum to 1.0000
[2019-03-08 09:40:24,422] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5458
[2019-03-08 09:40:24,430] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.43333333333333, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5087189471302896, 6.9112, 6.9112, 206719.8634749244, 206719.8634749244, 58266.27441719995], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2182800.0000, 
sim time next is 2183400.0000, 
raw observation next is [14.55, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.512824011606209, 6.9112, 6.9112, 208389.6493371949, 208389.6493371949, 58738.6538412918], 
processed observation next is [1.0, 0.2608695652173913, 0.2977272727272728, 0.92, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.512824011606209, 0.0, 0.0, 0.08682902055716454, 0.08682902055716454, 0.1468466346032295], 
reward next is 0.5068, 
noisyNet noise sample is [array([-0.06825896], dtype=float32), 1.8763627]. 
=============================================
[2019-03-08 09:40:26,664] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9033414e-11 5.8571037e-15 4.1161300e-05 9.9995887e-01 2.7854921e-11], sum to 1.0000
[2019-03-08 09:40:26,676] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9913
[2019-03-08 09:40:26,680] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.0, 69.66666666666667, 1.0, 2.0, 0.4707329735928353, 1.0, 2.0, 0.4707329735928353, 0.0, 2.0, 0.0, 6.9112, 6.9112, 720752.8725750352, 720752.8725750352, 129077.8501842961], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2214600.0000, 
sim time next is 2215200.0000, 
raw observation next is [22.0, 70.33333333333334, 1.0, 2.0, 0.4659692950193954, 1.0, 2.0, 0.4659692950193954, 0.0, 2.0, 0.0, 6.9112, 6.9112, 713260.9779351561, 713260.9779351561, 128100.5202185275], 
processed observation next is [1.0, 0.6521739130434783, 0.6363636363636364, 0.7033333333333335, 1.0, 1.0, 0.4659692950193954, 1.0, 1.0, 0.4659692950193954, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2971920741396484, 0.2971920741396484, 0.32025130054631873], 
reward next is 0.7954, 
noisyNet noise sample is [array([0.15138708], dtype=float32), -1.0811781]. 
=============================================
[2019-03-08 09:40:36,926] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63919: loss 0.0090
[2019-03-08 09:40:36,928] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63919: learning rate 0.0010
[2019-03-08 09:40:36,931] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63920: loss 0.0090
[2019-03-08 09:40:36,932] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63920: learning rate 0.0010
[2019-03-08 09:40:36,969] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63937: loss 0.0022
[2019-03-08 09:40:36,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63938: learning rate 0.0010
[2019-03-08 09:40:36,982] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63940: loss 0.0063
[2019-03-08 09:40:36,984] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63940: loss 0.0121
[2019-03-08 09:40:36,987] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63940: learning rate 0.0010
[2019-03-08 09:40:36,989] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63940: learning rate 0.0010
[2019-03-08 09:40:37,003] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 63945: loss 0.0028
[2019-03-08 09:40:37,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 63945: learning rate 0.0010
[2019-03-08 09:40:37,038] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 63962: loss 0.0723
[2019-03-08 09:40:37,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 63963: learning rate 0.0010
[2019-03-08 09:40:37,074] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 63980: loss 0.0249
[2019-03-08 09:40:37,079] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 63984: learning rate 0.0010
[2019-03-08 09:40:37,111] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63999: loss 0.1133
[2019-03-08 09:40:37,112] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63999: loss 0.0483
[2019-03-08 09:40:37,116] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64000: learning rate 0.0010
[2019-03-08 09:40:37,120] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64000: learning rate 0.0010
[2019-03-08 09:40:37,141] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4000, global step 64007: loss 0.1440
[2019-03-08 09:40:37,143] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4000, global step 64007: learning rate 0.0010
[2019-03-08 09:40:37,150] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 64007: loss 0.2285
[2019-03-08 09:40:37,155] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 64010: learning rate 0.0010
[2019-03-08 09:40:37,198] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64033: loss 0.2810
[2019-03-08 09:40:37,208] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64035: learning rate 0.0010
[2019-03-08 09:40:37,216] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4000, global step 64042: loss 0.1661
[2019-03-08 09:40:37,222] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4000, global step 64043: learning rate 0.0010
[2019-03-08 09:40:37,280] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 64072: loss 0.1093
[2019-03-08 09:40:37,285] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 64073: learning rate 0.0010
[2019-03-08 09:40:37,394] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64126: loss 0.0039
[2019-03-08 09:40:37,397] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64126: learning rate 0.0010
[2019-03-08 09:40:40,660] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.9998593e-01 1.6126780e-09 9.8241298e-06 4.0802992e-09 4.2518463e-06], sum to 1.0000
[2019-03-08 09:40:40,668] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9925
[2019-03-08 09:40:40,680] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.0, 86.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 1.0, 0.3530333935314084, 6.911199999999999, 6.9112, 430223.3135029891, 430223.3135029892, 123014.5432686807], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2460000.0000, 
sim time next is 2460600.0000, 
raw observation next is [17.0, 85.0, 1.0, 2.0, 0.3270927723472306, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 255580.4477950918, 255580.4477950918, 50877.91450643303], 
processed observation next is [1.0, 0.4782608695652174, 0.4090909090909091, 0.85, 1.0, 1.0, 0.3270927723472306, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.10649185324795492, 0.10649185324795492, 0.12719478626608258], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.43500912], dtype=float32), -0.20386007]. 
=============================================
[2019-03-08 09:40:53,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 8.3488853e-14 3.9241399e-08 3.9557828e-09 2.2436847e-10], sum to 1.0000
[2019-03-08 09:40:53,376] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4167
[2019-03-08 09:40:53,484] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.05, 79.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.853266096846741, 6.9112, 6.9112, 346894.592258675, 346894.592258675, 94661.10783210908], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2681400.0000, 
sim time next is 2682000.0000, 
raw observation next is [20.0, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.857781955659532, 6.9112, 6.9112, 348732.9693028957, 348732.9693028957, 95045.83862833522], 
processed observation next is [0.0, 0.043478260869565216, 0.5454545454545454, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.857781955659532, 0.0, 0.0, 0.14530540387620655, 0.14530540387620655, 0.23761459657083803], 
reward next is 0.5242, 
noisyNet noise sample is [array([-1.4434326], dtype=float32), 1.0854115]. 
=============================================
[2019-03-08 09:40:53,501] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[54.36882 ]
 [54.26815 ]
 [54.165974]
 [54.07623 ]
 [54.008087]], R is [[54.39904785]
 [54.37857056]
 [54.35742569]
 [54.33555984]
 [54.31334305]].
[2019-03-08 09:40:53,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9999976e-01 5.9731456e-14 2.8725515e-07 1.1101778e-12 7.0378581e-10], sum to 1.0000
[2019-03-08 09:40:53,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6871
[2019-03-08 09:40:53,983] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.88333333333333, 88.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7706497169177251, 6.9112, 6.9112, 313266.5758318976, 313266.5758318976, 87662.00552187704], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2689800.0000, 
sim time next is 2690400.0000, 
raw observation next is [17.76666666666667, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7709772224599485, 6.9112, 6.9112, 313399.8659324037, 313399.8659324037, 87689.63776542885], 
processed observation next is [0.0, 0.13043478260869565, 0.4439393939393941, 0.8966666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7709772224599485, 0.0, 0.0, 0.13058327747183487, 0.13058327747183487, 0.21922409441357213], 
reward next is 0.5106, 
noisyNet noise sample is [array([-0.5927107], dtype=float32), 0.013459229]. 
=============================================
[2019-03-08 09:40:54,372] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71892: loss 0.0001
[2019-03-08 09:40:54,379] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71893: learning rate 0.0010
[2019-03-08 09:40:54,467] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71940: loss 0.0274
[2019-03-08 09:40:54,470] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 71940: loss 0.0374
[2019-03-08 09:40:54,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 71941: learning rate 0.0010
[2019-03-08 09:40:54,473] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71940: learning rate 0.0010
[2019-03-08 09:40:54,482] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 71944: loss 0.0001
[2019-03-08 09:40:54,484] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 71944: learning rate 0.0010
[2019-03-08 09:40:54,489] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71947: loss 0.0140
[2019-03-08 09:40:54,491] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71947: learning rate 0.0010
[2019-03-08 09:40:54,495] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71949: loss 0.0102
[2019-03-08 09:40:54,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71949: learning rate 0.0010
[2019-03-08 09:40:54,519] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71958: loss 0.0051
[2019-03-08 09:40:54,522] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71958: learning rate 0.0010
[2019-03-08 09:40:54,538] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71966: loss 0.0010
[2019-03-08 09:40:54,544] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71967: learning rate 0.0010
[2019-03-08 09:40:54,552] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71972: loss 0.0002
[2019-03-08 09:40:54,557] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71973: learning rate 0.0010
[2019-03-08 09:40:54,603] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 71995: loss 0.0086
[2019-03-08 09:40:54,606] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 71995: learning rate 0.0010
[2019-03-08 09:40:54,608] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71995: loss 0.0112
[2019-03-08 09:40:54,615] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71997: learning rate 0.0010
[2019-03-08 09:40:54,653] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72018: loss 0.0199
[2019-03-08 09:40:54,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72018: learning rate 0.0010
[2019-03-08 09:40:54,686] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4500, global step 72033: loss 0.0372
[2019-03-08 09:40:54,689] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4500, global step 72035: learning rate 0.0010
[2019-03-08 09:40:54,742] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 72062: loss 0.0963
[2019-03-08 09:40:54,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 72063: learning rate 0.0010
[2019-03-08 09:40:54,762] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4500, global step 72070: loss 0.0816
[2019-03-08 09:40:54,765] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4500, global step 72071: learning rate 0.0010
[2019-03-08 09:40:54,799] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 72084: loss 0.0864
[2019-03-08 09:40:54,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 72084: learning rate 0.0010
[2019-03-08 09:40:55,498] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0000000e+00 1.5606342e-13 9.9297905e-16 2.2026052e-18 1.3248461e-14], sum to 1.0000
[2019-03-08 09:40:55,505] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5280
[2019-03-08 09:40:55,517] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333334, 83.33333333333334, 1.0, 2.0, 0.2759220398832803, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5137749191396703, 6.911199999999999, 6.9112, 417537.068703455, 417537.0687034552, 109405.6742796155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2712000.0000, 
sim time next is 2712600.0000, 
raw observation next is [22.15, 82.0, 1.0, 2.0, 0.478683959517136, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 364721.0383128223, 364721.0383128223, 69249.40989473806], 
processed observation next is [0.0, 0.391304347826087, 0.6431818181818181, 0.82, 1.0, 1.0, 0.478683959517136, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, -8.881784197001253e-17, 0.0, 0.1519670992970093, 0.1519670992970093, 0.17312352473684514], 
reward next is 0.7524, 
noisyNet noise sample is [array([-0.33482578], dtype=float32), 0.24749485]. 
=============================================
[2019-03-08 09:40:56,019] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6006254e-01 4.7189135e-07 8.3626056e-01 3.0359311e-04 3.3729288e-03], sum to 1.0000
[2019-03-08 09:40:56,029] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2037
[2019-03-08 09:40:56,035] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.66666666666667, 63.0, 1.0, 2.0, 1.011924188334657, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.981986299399203, 6.9112, 768121.4746369368, 759104.5744407129, 167769.3466238233], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2724000.0000, 
sim time next is 2724600.0000, 
raw observation next is [26.83333333333333, 62.5, 1.0, 2.0, 0.8587980985999608, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9547116395713053, 6.9112, 6.9112, 1024172.113823415, 1024172.113823415, 229255.3053397022], 
processed observation next is [0.0, 0.5217391304347826, 0.8560606060606059, 0.625, 1.0, 1.0, 0.8587980985999608, 0.0, 1.0, 0.0, 1.0, 0.5, 0.9547116395713053, 0.0, 0.0, 0.42673838075975623, 0.42673838075975623, 0.5731382633492556], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.59375805], dtype=float32), 0.42103767]. 
=============================================
[2019-03-08 09:40:56,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1680789e-05 6.7785783e-10 9.5851874e-01 3.7177645e-02 4.2520179e-03], sum to 1.0000
[2019-03-08 09:40:56,208] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0995
[2019-03-08 09:40:56,212] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.66666666666667, 63.0, 1.0, 2.0, 0.5355043470181993, 1.0, 1.0, 0.5355043470181993, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 795916.4877366379, 795916.487736638, 148273.5055197836], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2724000.0000, 
sim time next is 2724600.0000, 
raw observation next is [26.83333333333333, 62.5, 1.0, 2.0, 0.8154562068509881, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 6.911199999999998, 6.9112, 991470.7221746662, 991470.7221746666, 221194.1691166205], 
processed observation next is [0.0, 0.5217391304347826, 0.8560606060606059, 0.625, 1.0, 1.0, 0.8154562068509881, 0.0, 0.5, 0.0, 1.0, 0.5, 0.9547116395713053, -1.7763568394002506e-16, 0.0, 0.4131128009061109, 0.4131128009061111, 0.5529854227915513], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.01265123], dtype=float32), 0.07529439]. 
=============================================
[2019-03-08 09:40:57,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9750358e-01 2.5825210e-07 2.4745031e-03 1.4955682e-05 6.6444727e-06], sum to 1.0000
[2019-03-08 09:40:57,147] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6316
[2019-03-08 09:40:57,167] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.66666666666667, 52.00000000000001, 1.0, 1.0, 0.573930853346032, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 439865.4456610365, 439865.4456610365, 80033.68090286569], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2740800.0000, 
sim time next is 2741400.0000, 
raw observation next is [26.5, 52.5, 1.0, 2.0, 0.3386651760156039, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6335795268452846, 6.911199999999999, 6.9112, 515004.9061586655, 515004.9061586657, 123791.5762821454], 
processed observation next is [0.0, 0.7391304347826086, 0.8409090909090909, 0.525, 1.0, 1.0, 0.3386651760156039, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6335795268452846, -8.881784197001253e-17, 0.0, 0.21458537756611062, 0.2145853775661107, 0.30947894070536347], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.23052342], dtype=float32), -1.2724788]. 
=============================================
[2019-03-08 09:41:04,300] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.7607721e-14 5.6570764e-13 4.9428570e-05 9.9995029e-01 2.2892152e-07], sum to 1.0000
[2019-03-08 09:41:04,312] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5634
[2019-03-08 09:41:04,317] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.16666666666666, 87.16666666666667, 1.0, 2.0, 0.3064876189408647, 1.0, 2.0, 0.3064876189408647, 0.0, 2.0, 0.0, 6.9112, 6.9112, 465169.0913211411, 465169.0913211411, 99703.74979670363], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2865000.0000, 
sim time next is 2865600.0000, 
raw observation next is [21.0, 88.0, 1.0, 2.0, 0.3023335893987703, 1.0, 2.0, 0.3023335893987703, 0.0, 2.0, 0.0, 6.9112, 6.9112, 459162.0257174587, 459162.0257174587, 99001.37327862052], 
processed observation next is [1.0, 0.17391304347826086, 0.5909090909090909, 0.88, 1.0, 1.0, 0.3023335893987703, 1.0, 1.0, 0.3023335893987703, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1913175107156078, 0.1913175107156078, 0.2475034331965513], 
reward next is 0.6626, 
noisyNet noise sample is [array([2.6238394], dtype=float32), 0.2666137]. 
=============================================
[2019-03-08 09:41:11,842] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 79890: loss 0.8526
[2019-03-08 09:41:11,843] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 79890: learning rate 0.0010
[2019-03-08 09:41:11,962] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79917: loss 0.7818
[2019-03-08 09:41:11,965] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79917: learning rate 0.0010
[2019-03-08 09:41:12,058] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79930: loss 0.6795
[2019-03-08 09:41:12,059] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79931: loss 0.6468
[2019-03-08 09:41:12,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79931: learning rate 0.0010
[2019-03-08 09:41:12,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79931: learning rate 0.0010
[2019-03-08 09:41:12,218] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79941: loss 0.5779
[2019-03-08 09:41:12,221] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79942: learning rate 0.0010
[2019-03-08 09:41:12,296] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 79950: loss 0.6535
[2019-03-08 09:41:12,303] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 79951: learning rate 0.0010
[2019-03-08 09:41:12,394] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 79961: loss 0.6052
[2019-03-08 09:41:12,395] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 79961: learning rate 0.0010
[2019-03-08 09:41:12,475] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79974: loss 0.4120
[2019-03-08 09:41:12,482] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 79976: loss 0.4640
[2019-03-08 09:41:12,482] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79974: learning rate 0.0010
[2019-03-08 09:41:12,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 79976: learning rate 0.0010
[2019-03-08 09:41:12,627] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79983: loss 0.3933
[2019-03-08 09:41:12,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79984: learning rate 0.0010
[2019-03-08 09:41:12,709] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5000, global step 79991: loss 0.3033
[2019-03-08 09:41:12,713] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5000, global step 79992: learning rate 0.0010
[2019-03-08 09:41:12,822] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80012: loss 0.1940
[2019-03-08 09:41:12,825] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80013: learning rate 0.0010
[2019-03-08 09:41:12,948] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80050: loss 0.1711
[2019-03-08 09:41:12,950] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 80050: loss 0.1393
[2019-03-08 09:41:12,952] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80051: learning rate 0.0010
[2019-03-08 09:41:13,029] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 80051: learning rate 0.0010
[2019-03-08 09:41:13,138] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 80079: loss 0.0604
[2019-03-08 09:41:13,140] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 80079: learning rate 0.0010
[2019-03-08 09:41:13,321] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5000, global step 80144: loss 0.0564
[2019-03-08 09:41:13,332] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5000, global step 80144: learning rate 0.0010
[2019-03-08 09:41:13,487] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1739980e-07 9.2420811e-12 1.7284392e-05 2.4839077e-04 9.9973410e-01], sum to 1.0000
[2019-03-08 09:41:13,494] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6939
[2019-03-08 09:41:13,499] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.83333333333333, 69.66666666666667, 1.0, 2.0, 0.216434151460761, 1.0, 2.0, 0.216434151460761, 1.0, 2.0, 0.4004290813538193, 6.9112, 6.9112, 488025.0248123017, 488025.0248123017, 134349.0542744762], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3010200.0000, 
sim time next is 3010800.0000, 
raw observation next is [23.66666666666666, 70.33333333333334, 1.0, 2.0, 0.2146792562741006, 1.0, 2.0, 0.2146792562741006, 1.0, 2.0, 0.3975024652587524, 6.9112, 6.9112, 484455.5558433873, 484455.5558433873, 133850.5814079357], 
processed observation next is [1.0, 0.8695652173913043, 0.7121212121212118, 0.7033333333333335, 1.0, 1.0, 0.2146792562741006, 1.0, 1.0, 0.2146792562741006, 1.0, 1.0, 0.3975024652587524, 0.0, 0.0, 0.2018564816014114, 0.2018564816014114, 0.3346264535198393], 
reward next is 0.5171, 
noisyNet noise sample is [array([-0.61471105], dtype=float32), 0.20062433]. 
=============================================
[2019-03-08 09:41:19,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.12413037e-15 1.12185025e-14 8.83743911e-11 1.00000000e+00
 1.96566763e-09], sum to 1.0000
[2019-03-08 09:41:19,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1247
[2019-03-08 09:41:19,310] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.0, 89.0, 1.0, 2.0, 0.3906090765942357, 1.0, 2.0, 0.3906090765942357, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 579476.8246613917, 579476.8246613919, 116306.1859890512], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3109200.0000, 
sim time next is 3109800.0000, 
raw observation next is [23.0, 89.0, 1.0, 2.0, 0.3998985287736587, 1.0, 2.0, 0.3998985287736587, 0.0, 2.0, 0.0, 6.9112, 6.9112, 593249.7428142424, 593249.7428142424, 118118.8463588852], 
processed observation next is [1.0, 1.0, 0.6818181818181818, 0.89, 1.0, 1.0, 0.3998985287736587, 1.0, 1.0, 0.3998985287736587, 0.0, 1.0, 0.0, 0.0, 0.0, 0.24718739283926766, 0.24718739283926766, 0.295297115897213], 
reward next is 0.7175, 
noisyNet noise sample is [array([-0.7687023], dtype=float32), 0.7540207]. 
=============================================
[2019-03-08 09:41:22,910] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.9288516e-12 8.3770794e-12 3.1685974e-09 1.0000000e+00 9.7211093e-11], sum to 1.0000
[2019-03-08 09:41:22,918] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9915
[2019-03-08 09:41:22,925] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.0, 74.0, 1.0, 2.0, 0.344179553232327, 1.0, 2.0, 0.344179553232327, 0.0, 2.0, 0.0, 6.9112, 6.9112, 517304.1457710395, 517304.1457710395, 106751.8067647972], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3171000.0000, 
sim time next is 3171600.0000, 
raw observation next is [24.0, 74.0, 1.0, 2.0, 0.3467043926100132, 1.0, 2.0, 0.3467043926100132, 0.0, 2.0, 0.0, 6.9112, 6.9112, 521068.9894848707, 521068.9894848707, 107192.6407806829], 
processed observation next is [1.0, 0.7391304347826086, 0.7272727272727273, 0.74, 1.0, 1.0, 0.3467043926100132, 1.0, 1.0, 0.3467043926100132, 0.0, 1.0, 0.0, 0.0, 0.0, 0.21711207895202944, 0.21711207895202944, 0.2679816019517072], 
reward next is 0.6944, 
noisyNet noise sample is [array([1.6052637], dtype=float32), 1.1874388]. 
=============================================
[2019-03-08 09:41:23,944] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.3438648e-15 2.3075075e-12 1.8058636e-09 1.0000000e+00 1.4254392e-09], sum to 1.0000
[2019-03-08 09:41:23,957] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8708
[2019-03-08 09:41:23,964] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.5, 85.5, 1.0, 2.0, 0.3189491094974862, 1.0, 2.0, 0.3189491094974862, 0.0, 2.0, 0.0, 6.9112, 6.9112, 483484.8426340871, 483484.8426340871, 101786.68223098], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3184200.0000, 
sim time next is 3184800.0000, 
raw observation next is [21.33333333333334, 86.33333333333334, 1.0, 2.0, 0.3167148785944884, 1.0, 2.0, 0.3167148785944884, 0.0, 2.0, 0.0, 6.9112, 6.9112, 480364.6596691271, 480364.6596691271, 101376.7406628115], 
processed observation next is [1.0, 0.8695652173913043, 0.6060606060606063, 0.8633333333333334, 1.0, 1.0, 0.3167148785944884, 1.0, 1.0, 0.3167148785944884, 0.0, 1.0, 0.0, 0.0, 0.0, 0.20015194152880295, 0.20015194152880295, 0.25344185165702876], 
reward next is 0.6769, 
noisyNet noise sample is [array([1.178625], dtype=float32), 0.7963188]. 
=============================================
[2019-03-08 09:41:30,260] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 87866: loss 0.3444
[2019-03-08 09:41:30,264] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 87867: learning rate 0.0010
[2019-03-08 09:41:30,310] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87889: loss 12.6239
[2019-03-08 09:41:30,312] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87889: learning rate 0.0010
[2019-03-08 09:41:30,348] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87907: loss -0.0199
[2019-03-08 09:41:30,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87908: learning rate 0.0010
[2019-03-08 09:41:30,367] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 87913: loss 5.7716
[2019-03-08 09:41:30,368] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 87914: learning rate 0.0010
[2019-03-08 09:41:30,386] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87921: loss 2.1510
[2019-03-08 09:41:30,389] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87922: learning rate 0.0010
[2019-03-08 09:41:30,406] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 87932: loss -0.6601
[2019-03-08 09:41:30,407] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 87932: learning rate 0.0010
[2019-03-08 09:41:30,487] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87968: loss 25.5825
[2019-03-08 09:41:30,490] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87968: loss 28.1252
[2019-03-08 09:41:30,492] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87968: learning rate 0.0010
[2019-03-08 09:41:30,494] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87969: learning rate 0.0010
[2019-03-08 09:41:30,509] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5500, global step 87976: loss 1.2128
[2019-03-08 09:41:30,512] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5500, global step 87978: learning rate 0.0010
[2019-03-08 09:41:30,523] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87984: loss 1.2221
[2019-03-08 09:41:30,525] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87985: learning rate 0.0010
[2019-03-08 09:41:30,604] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88019: loss 0.9828
[2019-03-08 09:41:30,610] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88020: learning rate 0.0010
[2019-03-08 09:41:30,619] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88026: loss 0.9882
[2019-03-08 09:41:30,624] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88026: learning rate 0.0010
[2019-03-08 09:41:30,641] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 88035: loss 1.0461
[2019-03-08 09:41:30,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 88035: learning rate 0.0010
[2019-03-08 09:41:30,667] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88046: loss 0.8515
[2019-03-08 09:41:30,671] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88046: learning rate 0.0010
[2019-03-08 09:41:30,909] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88158: loss 0.1248
[2019-03-08 09:41:30,910] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88159: learning rate 0.0010
[2019-03-08 09:41:30,946] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5500, global step 88174: loss 0.1376
[2019-03-08 09:41:30,953] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5500, global step 88175: learning rate 0.0010
[2019-03-08 09:41:32,901] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 7.5698574e-29 1.0445633e-21 5.0569555e-32 1.8256997e-22], sum to 1.0000
[2019-03-08 09:41:32,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7156
[2019-03-08 09:41:32,910] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 50.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8447875887966207, 6.911200000000001, 6.9112, 343478.4481616591, 343478.4481616591, 93317.42729259825], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3333600.0000, 
sim time next is 3334200.0000, 
raw observation next is [25.0, 50.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8460308538635822, 6.911200000000001, 6.9112, 343984.6617174405, 343984.6617174405, 93421.5590194107], 
processed observation next is [0.0, 0.6086956521739131, 0.7727272727272727, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8460308538635822, 8.881784197001253e-17, 0.0, 0.1433269423822669, 0.1433269423822669, 0.23355389754852676], 
reward next is 0.5260, 
noisyNet noise sample is [array([-0.24802668], dtype=float32), 0.7181347]. 
=============================================
[2019-03-08 09:41:36,075] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.5489284e-18 1.5982777e-09 5.0080575e-18 2.3836911e-08], sum to 1.0000
[2019-03-08 09:41:36,088] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5129
[2019-03-08 09:41:36,102] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.0, 78.0, 1.0, 2.0, 0.2341921161170985, 1.0, 2.0, 0.2341921161170985, 1.0, 1.0, 0.4390834999797321, 6.911200000000001, 6.9112, 535173.8264040332, 535173.826404033, 138638.2108314254], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3398400.0000, 
sim time next is 3399000.0000, 
raw observation next is [21.16666666666667, 78.0, 1.0, 2.0, 0.2314240007026638, 1.0, 2.0, 0.2314240007026638, 1.0, 2.0, 0.4334046925381792, 6.911199999999999, 6.9112, 528246.6643112133, 528246.6643112135, 137932.0861621049], 
processed observation next is [1.0, 0.34782608695652173, 0.5984848484848487, 0.78, 1.0, 1.0, 0.2314240007026638, 1.0, 1.0, 0.2314240007026638, 1.0, 1.0, 0.4334046925381792, -8.881784197001253e-17, 0.0, 0.22010277679633888, 0.22010277679633897, 0.34483021540526226], 
reward next is 0.5471, 
noisyNet noise sample is [array([1.2277443], dtype=float32), -0.92883414]. 
=============================================
[2019-03-08 09:41:36,120] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[52.2767  ]
 [31.212782]
 [69.82598 ]
 [69.68822 ]
 [69.19321 ]], R is [[52.77602005]
 [52.2482605 ]
 [52.5103302 ]
 [51.98522568]
 [52.00014877]].
[2019-03-08 09:41:45,020] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 3.9382369e-19 8.3739154e-16 1.0264578e-18 1.6464181e-11], sum to 1.0000
[2019-03-08 09:41:45,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5594
[2019-03-08 09:41:45,046] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.0, 89.0, 1.0, 2.0, 0.2726077813073908, 1.0, 2.0, 0.2726077813073908, 1.0, 2.0, 0.4921772710289249, 6.911200000000001, 6.9112, 599946.1340628611, 599946.1340628609, 150663.7879885061], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3548400.0000, 
sim time next is 3549000.0000, 
raw observation next is [23.0, 89.0, 1.0, 2.0, 0.3792430714524963, 1.0, 2.0, 0.3792430714524963, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 561754.6168892832, 561754.6168892831, 114252.1406753819], 
processed observation next is [1.0, 0.043478260869565216, 0.6818181818181818, 0.89, 1.0, 1.0, 0.3792430714524963, 1.0, 1.0, 0.3792430714524963, 0.0, 0.5, 0.0, 8.881784197001253e-17, 0.0, 0.23406442370386799, 0.23406442370386793, 0.28563035168845474], 
reward next is 0.7024, 
noisyNet noise sample is [array([0.2856137], dtype=float32), 0.3665589]. 
=============================================
[2019-03-08 09:41:45,052] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[35.37268 ]
 [35.63039 ]
 [36.572636]
 [35.628162]
 [36.523922]], R is [[36.91112137]
 [37.11087036]
 [36.73976135]
 [36.37236404]
 [36.00864029]].
[2019-03-08 09:41:47,864] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95921: loss 0.0021
[2019-03-08 09:41:47,869] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95923: learning rate 0.0010
[2019-03-08 09:41:47,877] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 95924: loss 0.0065
[2019-03-08 09:41:47,880] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 95925: learning rate 0.0010
[2019-03-08 09:41:47,905] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95937: loss 0.0371
[2019-03-08 09:41:47,908] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95937: learning rate 0.0010
[2019-03-08 09:41:47,908] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95938: loss 0.0080
[2019-03-08 09:41:47,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95939: learning rate 0.0010
[2019-03-08 09:41:47,914] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 95939: loss 0.0471
[2019-03-08 09:41:47,918] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 95939: learning rate 0.0010
[2019-03-08 09:41:47,928] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95943: loss 0.0313
[2019-03-08 09:41:47,931] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95943: learning rate 0.0010
[2019-03-08 09:41:47,940] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95949: loss 0.0030
[2019-03-08 09:41:47,944] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95949: learning rate 0.0010
[2019-03-08 09:41:47,953] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 95956: loss 0.0028
[2019-03-08 09:41:47,954] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 95956: learning rate 0.0010
[2019-03-08 09:41:47,983] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6000, global step 95971: loss 0.0166
[2019-03-08 09:41:47,987] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6000, global step 95971: learning rate 0.0010
[2019-03-08 09:41:47,989] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95971: loss 0.0022
[2019-03-08 09:41:47,995] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95973: learning rate 0.0010
[2019-03-08 09:41:48,021] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95985: loss 0.0266
[2019-03-08 09:41:48,024] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95985: learning rate 0.0010
[2019-03-08 09:41:48,079] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96009: loss 0.0326
[2019-03-08 09:41:48,081] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96009: learning rate 0.0010
[2019-03-08 09:41:48,177] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 96051: loss 0.1812
[2019-03-08 09:41:48,182] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 96052: learning rate 0.0010
[2019-03-08 09:41:48,237] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96078: loss 0.2412
[2019-03-08 09:41:48,240] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96079: learning rate 0.0010
[2019-03-08 09:41:48,254] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96084: loss 0.1881
[2019-03-08 09:41:48,258] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96084: learning rate 0.0010
[2019-03-08 09:41:48,379] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6000, global step 96141: loss 0.0023
[2019-03-08 09:41:48,382] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6000, global step 96141: learning rate 0.0010
[2019-03-08 09:41:50,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5214232e-08 7.5577276e-17 5.7931672e-12 3.2944043e-17 1.0000000e+00], sum to 1.0000
[2019-03-08 09:41:50,950] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5220
[2019-03-08 09:41:50,958] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.2432613380445339, 1.0, 2.0, 0.2432613380445339, 1.0, 2.0, 0.4436140623816842, 6.9112, 6.9112, 540700.4334909408, 540700.4334909408, 142096.6460967719], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3645600.0000, 
sim time next is 3646200.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.2397245937351852, 1.0, 2.0, 0.2397245937351852, 1.0, 2.0, 0.437280524718719, 6.911200000000001, 6.9112, 532974.4927848756, 532974.4927848754, 141102.3409312093], 
processed observation next is [1.0, 0.17391304347826086, 0.5909090909090909, 1.0, 1.0, 1.0, 0.2397245937351852, 1.0, 1.0, 0.2397245937351852, 1.0, 1.0, 0.437280524718719, 8.881784197001253e-17, 0.0, 0.2220727053270315, 0.22207270532703144, 0.35275585232802326], 
reward next is 0.5396, 
noisyNet noise sample is [array([0.42821115], dtype=float32), 1.345062]. 
=============================================
[2019-03-08 09:41:52,975] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5330883e-15 7.8031423e-24 2.0707095e-20 1.3834620e-22 1.0000000e+00], sum to 1.0000
[2019-03-08 09:41:52,985] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2874
[2019-03-08 09:41:52,990] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 52.5, 1.0, 2.0, 0.7470623650186831, 1.0, 2.0, 0.6399714941526377, 1.0, 2.0, 0.9638808491382345, 6.9112, 6.9112, 1398873.680675055, 1398873.680675055, 296068.5826319836], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3685800.0000, 
sim time next is 3686400.0000, 
raw observation next is [29.0, 52.0, 1.0, 2.0, 0.7379530588309545, 1.0, 2.0, 0.6354168410587732, 1.0, 2.0, 0.9631794843836698, 6.911199999999999, 6.9112, 1388910.98383136, 1388910.98383136, 293523.1673952881], 
processed observation next is [1.0, 0.6956521739130435, 0.9545454545454546, 0.52, 1.0, 1.0, 0.7379530588309545, 1.0, 1.0, 0.6354168410587732, 1.0, 1.0, 0.9631794843836698, -8.881784197001253e-17, 0.0, 0.5787129099297333, 0.5787129099297333, 0.7338079184882204], 
reward next is 0.6760, 
noisyNet noise sample is [array([1.070491], dtype=float32), 0.80873805]. 
=============================================
[2019-03-08 09:41:56,825] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-08 09:41:56,828] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation job starts!
[2019-03-08 09:41:56,829] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:41:56,846] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-03-08 09:42:06,622] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11087208], dtype=float32), 0.43888527]
[2019-03-08 09:42:06,623] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation this: [21.0, 88.00000000000001, 1.0, 2.0, 0.2027625847916268, 1.0, 2.0, 0.2027625847916268, 1.0, 2.0, 0.3764843634181984, 6.911200000000001, 6.9112, 458821.816322686, 458821.8163226857, 130584.2004558312]
[2019-03-08 09:42:06,623] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-03-08 09:42:06,623] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Softmax [5.19791155e-10 1.05685995e-14 4.91728658e-10 3.29928735e-15
 1.00000000e+00], sampled 0.3971648575221567
[2019-03-08 09:42:22,953] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation: average rewards by now are 3508.8636 1705022357.1662 6.0000
[2019-03-08 09:42:23,968] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 100000, evaluation results [100000.0, 3508.8635650796773, 1705022357.1662278, 6.0]
[2019-03-08 09:42:25,647] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6036367e-07 1.8930781e-10 8.8689910e-07 1.4835889e-10 9.9999893e-01], sum to 1.0000
[2019-03-08 09:42:25,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8916
[2019-03-08 09:42:25,661] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.0, 66.5, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 2.0, 0.2845457266938985, 6.9112, 6.9112, 346716.775068625, 346716.775068625, 110284.4866944416], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3781800.0000, 
sim time next is 3782400.0000, 
raw observation next is [21.0, 67.33333333333334, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 2.0, 0.2869073123475338, 6.911199999999999, 6.9112, 349595.885517306, 349595.8855173062, 110850.3628857222], 
processed observation next is [1.0, 0.782608695652174, 0.5909090909090909, 0.6733333333333335, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 1.0, 1.0, 0.2869073123475338, -8.881784197001253e-17, 0.0, 0.1456649522988775, 0.1456649522988776, 0.2771259072143055], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4775897], dtype=float32), 0.844521]. 
=============================================
[2019-03-08 09:42:32,486] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 103902: loss 0.0535
[2019-03-08 09:42:32,491] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 103904: learning rate 0.0010
[2019-03-08 09:42:32,492] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103904: loss 0.0659
[2019-03-08 09:42:32,500] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103905: learning rate 0.0010
[2019-03-08 09:42:32,500] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103905: loss 0.0758
[2019-03-08 09:42:32,506] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103906: learning rate 0.0010
[2019-03-08 09:42:32,526] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 103914: loss 0.0658
[2019-03-08 09:42:32,529] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 103916: learning rate 0.0010
[2019-03-08 09:42:32,566] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 103936: loss 0.0887
[2019-03-08 09:42:32,569] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 103937: learning rate 0.0010
[2019-03-08 09:42:32,607] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103957: loss 0.0161
[2019-03-08 09:42:32,608] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 103958: loss 0.0098
[2019-03-08 09:42:32,611] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 103958: learning rate 0.0010
[2019-03-08 09:42:32,612] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103958: learning rate 0.0010
[2019-03-08 09:42:32,622] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103965: loss 0.0059
[2019-03-08 09:42:32,624] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6500, global step 103966: loss 0.0018
[2019-03-08 09:42:32,626] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6500, global step 103966: learning rate 0.0010
[2019-03-08 09:42:32,626] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103966: learning rate 0.0010
[2019-03-08 09:42:32,631] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 103966: loss 0.0023
[2019-03-08 09:42:32,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 103968: learning rate 0.0010
[2019-03-08 09:42:32,641] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103971: loss 0.0017
[2019-03-08 09:42:32,643] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103971: learning rate 0.0010
[2019-03-08 09:42:32,798] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104042: loss 0.0319
[2019-03-08 09:42:32,801] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104042: learning rate 0.0010
[2019-03-08 09:42:32,832] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104057: loss 0.0272
[2019-03-08 09:42:32,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104058: learning rate 0.0010
[2019-03-08 09:42:32,888] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 104078: loss 0.0162
[2019-03-08 09:42:32,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 104079: learning rate 0.0010
[2019-03-08 09:42:32,939] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104107: loss 0.0231
[2019-03-08 09:42:32,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104107: learning rate 0.0010
[2019-03-08 09:42:33,071] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6500, global step 104158: loss 0.0025
[2019-03-08 09:42:33,071] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6500, global step 104158: learning rate 0.0010
[2019-03-08 09:42:49,782] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111846: loss -81.7419
[2019-03-08 09:42:49,787] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111848: learning rate 0.0010
[2019-03-08 09:42:49,848] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111874: loss -5.9955
[2019-03-08 09:42:49,852] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111877: learning rate 0.0010
[2019-03-08 09:42:49,915] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 111907: loss 0.0766
[2019-03-08 09:42:49,917] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 111908: learning rate 0.0010
[2019-03-08 09:42:49,932] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111914: loss 0.7098
[2019-03-08 09:42:49,937] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 111916: loss 0.1240
[2019-03-08 09:42:49,938] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111914: learning rate 0.0010
[2019-03-08 09:42:49,938] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 111916: learning rate 0.0010
[2019-03-08 09:42:49,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7724133e-07 1.5798388e-19 9.9999988e-01 8.9090967e-22 1.6123104e-16], sum to 1.0000
[2019-03-08 09:42:49,959] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1203
[2019-03-08 09:42:49,967] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.83333333333333, 61.66666666666666, 1.0, 2.0, 0.4824888413269935, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9045985764509279, 6.911199999999999, 6.9112, 735625.39095231, 735625.3909523101, 160113.5624662024], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4193400.0000, 
sim time next is 4194000.0000, 
raw observation next is [24.0, 61.0, 1.0, 2.0, 0.5189754818076083, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 6.911200000000001, 6.9112, 783385.26576589, 783385.2657658899, 170151.5831040683], 
processed observation next is [1.0, 0.5652173913043478, 0.7272727272727273, 0.61, 1.0, 1.0, 0.5189754818076083, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 8.881784197001253e-17, 0.0, 0.3264105274024542, 0.32641052740245413, 0.42537895776017076], 
reward next is 0.6577, 
noisyNet noise sample is [array([-1.4954838], dtype=float32), -0.04633456]. 
=============================================
[2019-03-08 09:42:49,974] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[64.016884]
 [63.955856]
 [63.869896]
 [63.732983]
 [63.492233]], R is [[64.17165375]
 [64.1862793 ]
 [64.1980896 ]
 [64.2042923 ]
 [64.19798279]].
[2019-03-08 09:42:49,997] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 111941: loss 0.0912
[2019-03-08 09:42:50,003] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 111941: learning rate 0.0010
[2019-03-08 09:42:50,044] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7000, global step 111966: loss 0.0550
[2019-03-08 09:42:50,045] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7000, global step 111966: learning rate 0.0010
[2019-03-08 09:42:50,051] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111968: loss 0.0521
[2019-03-08 09:42:50,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111968: learning rate 0.0010
[2019-03-08 09:42:50,070] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 111977: loss 0.0524
[2019-03-08 09:42:50,075] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 111977: learning rate 0.0010
[2019-03-08 09:42:50,090] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111985: loss 0.0692
[2019-03-08 09:42:50,092] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111985: learning rate 0.0010
[2019-03-08 09:42:50,121] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111994: loss 0.0881
[2019-03-08 09:42:50,124] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111994: learning rate 0.0010
[2019-03-08 09:42:50,252] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112055: loss 0.1335
[2019-03-08 09:42:50,257] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112056: learning rate 0.0010
[2019-03-08 09:42:50,323] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 112081: loss 0.0499
[2019-03-08 09:42:50,325] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 112083: learning rate 0.0010
[2019-03-08 09:42:50,359] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7000, global step 112102: loss 0.0445
[2019-03-08 09:42:50,360] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7000, global step 112102: learning rate 0.0010
[2019-03-08 09:42:50,373] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112109: loss 0.0412
[2019-03-08 09:42:50,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112109: learning rate 0.0010
[2019-03-08 09:42:50,446] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112140: loss 0.0436
[2019-03-08 09:42:50,453] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112142: learning rate 0.0010
[2019-03-08 09:43:04,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.99509454e-01 1.28672565e-20 4.90544946e-04 2.67556295e-21
 5.22242875e-16], sum to 1.0000
[2019-03-08 09:43:04,758] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1884
[2019-03-08 09:43:04,773] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.16666666666667, 73.33333333333334, 1.0, 2.0, 0.3767250739412277, 0.0, 2.0, 0.0, 1.0, 2.0, 0.687620270604974, 6.911199999999999, 6.9112, 558958.342446586, 558958.3424465861, 133867.8105825701], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4456200.0000, 
sim time next is 4456800.0000, 
raw observation next is [25.0, 74.0, 1.0, 2.0, 0.2650746958177472, 1.0, 1.0, 0.2650746958177472, 1.0, 2.0, 0.4802877261396397, 6.911200000000001, 6.9112, 585440.2435455061, 585440.2435455059, 148425.0058281917], 
processed observation next is [0.0, 0.6086956521739131, 0.7727272727272727, 0.74, 1.0, 1.0, 0.2650746958177472, 1.0, 0.5, 0.2650746958177472, 1.0, 1.0, 0.4802877261396397, 8.881784197001253e-17, 0.0, 0.24393343481062757, 0.24393343481062746, 0.37106251457047923], 
reward next is 0.5635, 
noisyNet noise sample is [array([-0.4586293], dtype=float32), -1.945023]. 
=============================================
[2019-03-08 09:43:06,970] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119734: loss 0.0378
[2019-03-08 09:43:06,974] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119734: learning rate 0.0010
[2019-03-08 09:43:07,256] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119864: loss 0.1958
[2019-03-08 09:43:07,259] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119865: learning rate 0.0010
[2019-03-08 09:43:07,294] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 119879: loss 0.5519
[2019-03-08 09:43:07,298] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 119879: learning rate 0.0010
[2019-03-08 09:43:07,334] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 119895: loss 0.4972
[2019-03-08 09:43:07,339] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 119896: learning rate 0.0010
[2019-03-08 09:43:07,402] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 119936: loss 0.6682
[2019-03-08 09:43:07,405] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 119936: learning rate 0.0010
[2019-03-08 09:43:07,425] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7500, global step 119947: loss 0.8171
[2019-03-08 09:43:07,428] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7500, global step 119948: learning rate 0.0010
[2019-03-08 09:43:07,434] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119951: loss 0.7952
[2019-03-08 09:43:07,440] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119954: learning rate 0.0010
[2019-03-08 09:43:07,451] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 119958: loss 1.0168
[2019-03-08 09:43:07,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 119959: learning rate 0.0010
[2019-03-08 09:43:07,545] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119993: loss -27.2593
[2019-03-08 09:43:07,552] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119995: learning rate 0.0010
[2019-03-08 09:43:07,565] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120000: loss -24.6796
[2019-03-08 09:43:07,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120001: learning rate 0.0010
[2019-03-08 09:43:07,617] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120030: loss 0.4470
[2019-03-08 09:43:07,620] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120030: learning rate 0.0010
[2019-03-08 09:43:07,622] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120031: loss 0.3592
[2019-03-08 09:43:07,627] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120032: learning rate 0.0010
[2019-03-08 09:43:07,693] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 120062: loss 0.1910
[2019-03-08 09:43:07,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 120062: learning rate 0.0010
[2019-03-08 09:43:07,706] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7500, global step 120067: loss 0.0897
[2019-03-08 09:43:07,709] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7500, global step 120067: learning rate 0.0010
[2019-03-08 09:43:07,957] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120180: loss 0.3545
[2019-03-08 09:43:07,961] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120180: learning rate 0.0010
[2019-03-08 09:43:08,169] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120273: loss 0.4082
[2019-03-08 09:43:08,171] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120274: learning rate 0.0010
[2019-03-08 09:43:12,158] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 8.5799825e-25 2.9485577e-11 3.9603649e-28 8.4229750e-21], sum to 1.0000
[2019-03-08 09:43:12,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7836
[2019-03-08 09:43:12,179] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.0, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6044473307205149, 6.911200000000001, 6.9112, 245642.1576790088, 245642.1576790088, 70787.91955748755], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4579200.0000, 
sim time next is 4579800.0000, 
raw observation next is [16.83333333333334, 83.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6048010471095583, 6.9112, 6.9112, 245786.0405204308, 245786.0405204308, 70659.70892501576], 
processed observation next is [1.0, 0.0, 0.40151515151515177, 0.8300000000000002, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6048010471095583, 0.0, 0.0, 0.10241085021684616, 0.10241085021684616, 0.1766492723125394], 
reward next is 0.4969, 
noisyNet noise sample is [array([2.3614464], dtype=float32), 1.3441447]. 
=============================================
[2019-03-08 09:43:13,525] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.00000000e+00 9.55339113e-24 1.27869715e-08 4.84711481e-24
 1.07801384e-20], sum to 1.0000
[2019-03-08 09:43:13,538] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1953
[2019-03-08 09:43:13,543] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.33333333333333, 93.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4594411339531743, 6.9112, 6.9112, 186677.6019726274, 186677.6019726274, 53684.37731959667], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4601400.0000, 
sim time next is 4602000.0000, 
raw observation next is [14.66666666666667, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4751455643971606, 6.911199999999999, 6.9112, 193064.4837077549, 193064.483707755, 55570.86982725822], 
processed observation next is [1.0, 0.2608695652173913, 0.30303030303030315, 0.92, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4751455643971606, -8.881784197001253e-17, 0.0, 0.08044353487823121, 0.08044353487823125, 0.13892717456814554], 
reward next is 0.4963, 
noisyNet noise sample is [array([0.61733526], dtype=float32), 0.13389125]. 
=============================================
[2019-03-08 09:43:13,554] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[47.285828]
 [47.217743]
 [47.228996]
 [47.23059 ]
 [47.238007]], R is [[47.39776611]
 [47.4205513 ]
 [47.4454689 ]
 [47.46988297]
 [47.49415588]].
[2019-03-08 09:43:19,828] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.2308637e-01 2.7536982e-15 4.7691369e-01 1.6961187e-12 2.5129239e-12], sum to 1.0000
[2019-03-08 09:43:19,842] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1959
[2019-03-08 09:43:19,856] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.0, 52.5, 1.0, 2.0, 0.3156817619683926, 1.0, 1.0, 0.3156817619683926, 1.0, 2.0, 0.5910987307761016, 6.9112, 6.9112, 720653.5394971803, 720653.5394971803, 162605.6826284759], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4710600.0000, 
sim time next is 4711200.0000, 
raw observation next is [25.33333333333333, 52.0, 1.0, 2.0, 0.4331949261357234, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8135783472194973, 6.911199999999999, 6.9112, 661509.6126345725, 661509.6126345728, 147135.4072201415], 
processed observation next is [1.0, 0.5217391304347826, 0.7878787878787876, 0.52, 1.0, 1.0, 0.4331949261357234, 0.0, 0.5, 0.0, 1.0, 1.0, 0.8135783472194973, -8.881784197001253e-17, 0.0, 0.2756290052644052, 0.2756290052644053, 0.3678385180503537], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.1568682], dtype=float32), -0.72190446]. 
=============================================
[2019-03-08 09:43:20,524] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1785260e-02 1.2679831e-15 9.8821473e-01 9.5436645e-11 5.9779964e-10], sum to 1.0000
[2019-03-08 09:43:20,533] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2136
[2019-03-08 09:43:20,538] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.0, 54.0, 1.0, 2.0, 0.8228784252502972, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 6.911199999999999, 6.9112, 1008826.581552762, 1008826.581552762, 225389.4353623648], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4722600.0000, 
sim time next is 4723200.0000, 
raw observation next is [26.0, 54.0, 1.0, 2.0, 0.6532495784046042, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 6.911199999999999, 6.9112, 881629.4426380898, 881629.4426380899, 193701.6100936247], 
processed observation next is [1.0, 0.6956521739130435, 0.8181818181818182, 0.54, 1.0, 1.0, 0.6532495784046042, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, -8.881784197001253e-17, 0.0, 0.36734560109920406, 0.3673456010992041, 0.48425402523406175], 
reward next is 0.6502, 
noisyNet noise sample is [array([0.8336916], dtype=float32), 0.68712]. 
=============================================
[2019-03-08 09:43:21,756] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7967730e-04 2.3190039e-20 9.9982029e-01 2.9202734e-16 1.9047569e-17], sum to 1.0000
[2019-03-08 09:43:21,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8720
[2019-03-08 09:43:21,768] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.33333333333334, 74.66666666666667, 1.0, 2.0, 0.2872028928026114, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5386564866022379, 6.911199999999999, 6.9112, 437778.4948984918, 437778.4948984919, 111909.0310201471], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4738800.0000, 
sim time next is 4739400.0000, 
raw observation next is [22.0, 75.5, 1.0, 2.0, 0.2818920049131752, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5296372654154136, 6.911199999999999, 6.9112, 430441.0403590286, 430441.0403590287, 110705.1517276485], 
processed observation next is [1.0, 0.8695652173913043, 0.6363636363636364, 0.755, 1.0, 1.0, 0.2818920049131752, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5296372654154136, -8.881784197001253e-17, 0.0, 0.1793504334829286, 0.17935043348292862, 0.27676287931912125], 
reward next is 0.5555, 
noisyNet noise sample is [array([0.04050358], dtype=float32), -0.44360948]. 
=============================================
[2019-03-08 09:43:24,571] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127786: loss -59.5760
[2019-03-08 09:43:24,572] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127786: learning rate 0.0010
[2019-03-08 09:43:24,696] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127843: loss -69.2343
[2019-03-08 09:43:24,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127844: learning rate 0.0010
[2019-03-08 09:43:24,865] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 127919: loss -23.9840
[2019-03-08 09:43:24,868] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 127920: learning rate 0.0010
[2019-03-08 09:43:24,895] A3C_AGENT_WORKER-Thread-8 INFO:Local step 8000, global step 127931: loss 0.5037
[2019-03-08 09:43:24,899] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 8000, global step 127931: learning rate 0.0010
[2019-03-08 09:43:24,903] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127934: loss 0.6014
[2019-03-08 09:43:24,904] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127934: learning rate 0.0010
[2019-03-08 09:43:24,915] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 127938: loss 10.1809
[2019-03-08 09:43:24,919] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 127938: learning rate 0.0010
[2019-03-08 09:43:24,947] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 127949: loss 9.0437
[2019-03-08 09:43:24,947] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 127949: loss 0.6021
[2019-03-08 09:43:24,950] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 127950: learning rate 0.0010
[2019-03-08 09:43:24,952] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 127950: learning rate 0.0010
[2019-03-08 09:43:24,981] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127961: loss -63.3290
[2019-03-08 09:43:24,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127961: learning rate 0.0010
[2019-03-08 09:43:25,032] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127983: loss -60.4052
[2019-03-08 09:43:25,034] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127984: learning rate 0.0010
[2019-03-08 09:43:25,097] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128015: loss 1.4413
[2019-03-08 09:43:25,099] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128016: learning rate 0.0010
[2019-03-08 09:43:25,137] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 128032: loss -2.3622
[2019-03-08 09:43:25,140] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 128033: learning rate 0.0010
[2019-03-08 09:43:25,274] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 128098: loss 44.9324
[2019-03-08 09:43:25,277] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 128098: learning rate 0.0010
[2019-03-08 09:43:25,293] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128106: loss 0.5282
[2019-03-08 09:43:25,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128106: learning rate 0.0010
[2019-03-08 09:43:25,403] A3C_AGENT_WORKER-Thread-7 INFO:Local step 8000, global step 128157: loss 0.6994
[2019-03-08 09:43:25,406] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 8000, global step 128157: learning rate 0.0010
[2019-03-08 09:43:25,529] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128210: loss -37.5155
[2019-03-08 09:43:25,539] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128212: learning rate 0.0010
[2019-03-08 09:43:26,646] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0000000e+00 3.5340017e-25 3.1909145e-10 5.4045097e-21 1.1150088e-18], sum to 1.0000
[2019-03-08 09:43:26,651] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3605
[2019-03-08 09:43:26,663] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.0, 96.0, 1.0, 2.0, 0.344616654371953, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6366392645274193, 6.9112, 6.9112, 517475.3387050125, 517475.3387050125, 126260.5920296065], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4830000.0000, 
sim time next is 4830600.0000, 
raw observation next is [21.0, 95.0, 1.0, 2.0, 0.3354684055719281, 1.0, 1.0, 0.3354684055719281, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 504811.5986950776, 504811.5986950777, 105165.5825801237], 
processed observation next is [1.0, 0.9130434782608695, 0.5909090909090909, 0.95, 1.0, 1.0, 0.3354684055719281, 1.0, 0.5, 0.3354684055719281, 0.0, 0.5, 0.0, -8.881784197001253e-17, 0.0, 0.210338166122949, 0.21033816612294903, 0.26291395645030924], 
reward next is 0.6857, 
noisyNet noise sample is [array([-0.42284405], dtype=float32), 0.10150184]. 
=============================================
[2019-03-08 09:43:28,342] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0882616e-07 2.7828475e-13 9.9999988e-01 5.0794027e-09 8.2826255e-12], sum to 1.0000
[2019-03-08 09:43:28,353] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9801
[2019-03-08 09:43:28,358] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [19.0, 100.0, 1.0, 2.0, 0.4487767705177932, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 344570.6236291663, 344570.6236291663, 65427.03760227454], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4862400.0000, 
sim time next is 4863000.0000, 
raw observation next is [19.0, 100.0, 1.0, 2.0, 0.3286852133120608, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6177219384949012, 6.911199999999999, 6.9112, 502102.1875405699, 502102.18754057, 121423.6322708887], 
processed observation next is [1.0, 0.2608695652173913, 0.5, 1.0, 1.0, 1.0, 0.3286852133120608, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6177219384949012, -8.881784197001253e-17, 0.0, 0.2092092448085708, 0.20920924480857084, 0.30355908067722176], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.13174057], dtype=float32), 1.4055145]. 
=============================================
[2019-03-08 09:43:28,375] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[36.239716]
 [36.4244  ]
 [36.37445 ]
 [36.24025 ]
 [36.43095 ]], R is [[36.01490402]
 [36.40710831]
 [36.59739685]
 [36.23142242]
 [36.62163544]].
[2019-03-08 09:43:42,105] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 135782: loss 0.0840
[2019-03-08 09:43:42,110] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 135784: learning rate 0.0010
[2019-03-08 09:43:42,237] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135843: loss 0.1279
[2019-03-08 09:43:42,241] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135844: learning rate 0.0010
[2019-03-08 09:43:42,289] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8500, global step 135867: loss 0.0087
[2019-03-08 09:43:42,298] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8500, global step 135867: learning rate 0.0010
[2019-03-08 09:43:42,354] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 135895: loss 0.0064
[2019-03-08 09:43:42,358] A3C_AGENT_WORKER-Thread-8 INFO:Local step 8500, global step 135896: loss 0.0006
[2019-03-08 09:43:42,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 135896: learning rate 0.0010
[2019-03-08 09:43:42,360] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 8500, global step 135896: learning rate 0.0010
[2019-03-08 09:43:42,466] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8500, global step 135954: loss 0.0177
[2019-03-08 09:43:42,469] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8500, global step 135955: learning rate 0.0010
[2019-03-08 09:43:42,534] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 135986: loss 0.0061
[2019-03-08 09:43:42,536] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 135986: loss 0.0173
[2019-03-08 09:43:42,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 135986: learning rate 0.0010
[2019-03-08 09:43:42,543] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 135986: learning rate 0.0010
[2019-03-08 09:43:42,582] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136007: loss 0.0157
[2019-03-08 09:43:42,584] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136008: loss 0.0519
[2019-03-08 09:43:42,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136009: learning rate 0.0010
[2019-03-08 09:43:42,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136009: learning rate 0.0010
[2019-03-08 09:43:42,669] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8500, global step 136044: loss 0.0174
[2019-03-08 09:43:42,671] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 136045: loss 0.0011
[2019-03-08 09:43:42,671] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8500, global step 136045: learning rate 0.0010
[2019-03-08 09:43:42,674] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 136047: learning rate 0.0010
[2019-03-08 09:43:42,745] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 136077: loss -106.6611
[2019-03-08 09:43:42,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 136078: learning rate 0.0010
[2019-03-08 09:43:42,803] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 136105: loss 0.0693
[2019-03-08 09:43:42,807] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 136107: learning rate 0.0010
[2019-03-08 09:43:42,822] A3C_AGENT_WORKER-Thread-7 INFO:Local step 8500, global step 136112: loss 0.0787
[2019-03-08 09:43:42,823] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 8500, global step 136112: learning rate 0.0010
[2019-03-08 09:43:42,930] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 136161: loss 0.1319
[2019-03-08 09:43:42,933] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 136162: learning rate 0.0010
[2019-03-08 09:43:49,125] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.13179737e-02 4.22275824e-21 9.78682101e-01 1.24268865e-17
 1.05557848e-16], sum to 1.0000
[2019-03-08 09:43:49,133] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2594
[2019-03-08 09:43:49,137] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.66666666666667, 90.33333333333334, 1.0, 2.0, 0.451856852658439, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8327869430473424, 6.9112, 6.9112, 677115.9726908186, 677115.9726908186, 153201.6944561575], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5215200.0000, 
sim time next is 5215800.0000, 
raw observation next is [21.33333333333333, 92.16666666666667, 1.0, 2.0, 0.450987108912682, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8320410769263326, 6.9112, 6.9112, 676508.7419735618, 676508.7419735618, 152989.45793621], 
processed observation next is [1.0, 0.34782608695652173, 0.6060606060606059, 0.9216666666666667, 1.0, 1.0, 0.450987108912682, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8320410769263326, 0.0, 0.0, 0.2818786424889841, 0.2818786424889841, 0.382473644840525], 
reward next is 0.6317, 
noisyNet noise sample is [array([-0.7700616], dtype=float32), 1.139375]. 
=============================================
[2019-03-08 09:43:50,028] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.9998927e-01 3.9001974e-23 1.0754310e-05 1.6515518e-19 9.3981636e-21], sum to 1.0000
[2019-03-08 09:43:50,036] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1503
[2019-03-08 09:43:50,047] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.4061138589047979, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7436765316756943, 6.911200000000001, 6.9112, 604578.6821054226, 604578.6821054225, 141259.400436492], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5226600.0000, 
sim time next is 5227200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3082309785640387, 1.0, 1.0, 0.3082309785640387, 1.0, 2.0, 0.5588331520215236, 6.9112, 6.9112, 681280.9936221857, 681280.9936221857, 161318.8746136171], 
processed observation next is [1.0, 0.5217391304347826, 0.6363636363636364, 0.94, 1.0, 1.0, 0.3082309785640387, 1.0, 0.5, 0.3082309785640387, 1.0, 1.0, 0.5588331520215236, 0.0, 0.0, 0.2838670806759107, 0.2838670806759107, 0.4032971865340428], 
reward next is 0.6033, 
noisyNet noise sample is [array([-0.46898305], dtype=float32), 0.80010164]. 
=============================================
[2019-03-08 09:43:55,651] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.8154098e-09 9.3181929e-18 9.9999845e-01 1.5489591e-06 1.8160718e-12], sum to 1.0000
[2019-03-08 09:43:55,660] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6647
[2019-03-08 09:43:55,666] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 51.0, 1.0, 2.0, 0.5122299662248431, 1.0, 2.0, 0.5122299662248431, 0.0, 1.0, 0.0, 6.911199999999997, 6.9112, 748233.1036686741, 748233.1036686747, 153628.1850460821], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5328600.0000, 
sim time next is 5329200.0000, 
raw observation next is [30.0, 51.00000000000001, 1.0, 2.0, 0.3983622502014467, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7211740942458539, 6.911199999999999, 6.9112, 586044.2543379525, 586044.2543379527, 146878.5643246149], 
processed observation next is [1.0, 0.6956521739130435, 1.0, 0.5100000000000001, 1.0, 1.0, 0.3983622502014467, 0.0, 0.5, 0.0, 1.0, 0.5, 0.7211740942458539, -8.881784197001253e-17, 0.0, 0.24418510597414686, 0.24418510597414697, 0.36719641081153725], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3698108], dtype=float32), 0.61504614]. 
=============================================
[2019-03-08 09:43:57,972] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.8662628e-03 3.8372821e-29 9.9013370e-01 5.0694785e-20 2.2959504e-27], sum to 1.0000
[2019-03-08 09:43:57,981] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4239
[2019-03-08 09:43:57,986] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.55, 76.5, 1.0, 2.0, 0.4782668153635348, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8827846899842294, 6.911199999999999, 6.9112, 717860.8269513139, 717860.826951314, 158870.1216311631], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5358600.0000, 
sim time next is 5359200.0000, 
raw observation next is [23.46666666666667, 77.33333333333333, 1.0, 2.0, 0.3747558951263351, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6917856301268168, 6.911199999999999, 6.9112, 562370.6888305781, 562370.6888305782, 132392.5041429107], 
processed observation next is [1.0, 0.0, 0.7030303030303031, 0.7733333333333333, 1.0, 1.0, 0.3747558951263351, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6917856301268168, -8.881784197001253e-17, 0.0, 0.23432112034607422, 0.23432112034607427, 0.33098126035727676], 
reward next is 0.6068, 
noisyNet noise sample is [array([-1.2868623], dtype=float32), 0.86752725]. 
=============================================
[2019-03-08 09:43:59,668] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143787: loss 17.0899
[2019-03-08 09:43:59,670] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143787: learning rate 0.0010
[2019-03-08 09:43:59,709] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 143804: loss 8.8508
[2019-03-08 09:43:59,714] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 143805: learning rate 0.0010
[2019-03-08 09:43:59,732] A3C_AGENT_WORKER-Thread-8 INFO:Local step 9000, global step 143815: loss 10.4360
[2019-03-08 09:43:59,734] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 9000, global step 143816: learning rate 0.0010
[2019-03-08 09:43:59,851] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 143869: loss 1.3153
[2019-03-08 09:43:59,854] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 143870: learning rate 0.0010
[2019-03-08 09:43:59,973] A3C_AGENT_WORKER-Thread-6 INFO:Local step 9000, global step 143926: loss 0.9008
[2019-03-08 09:43:59,976] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 9000, global step 143926: learning rate 0.0010
[2019-03-08 09:44:00,073] A3C_AGENT_WORKER-Thread-4 INFO:Local step 9000, global step 143969: loss 4.9418
[2019-03-08 09:44:00,076] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 9000, global step 143970: learning rate 0.0010
[2019-03-08 09:44:00,128] A3C_AGENT_WORKER-Thread-5 INFO:Local step 9000, global step 143993: loss 0.5433
[2019-03-08 09:44:00,129] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 9000, global step 143993: learning rate 0.0010
[2019-03-08 09:44:00,156] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144005: loss 34.9027
[2019-03-08 09:44:00,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144007: learning rate 0.0010
[2019-03-08 09:44:00,169] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 144011: loss 2.3606
[2019-03-08 09:44:00,171] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 144011: learning rate 0.0010
[2019-03-08 09:44:00,188] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144019: loss 0.2229
[2019-03-08 09:44:00,193] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144021: learning rate 0.0010
[2019-03-08 09:44:00,211] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 144031: loss 0.2793
[2019-03-08 09:44:00,214] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 144031: learning rate 0.0010
[2019-03-08 09:44:00,270] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 144058: loss 17.2994
[2019-03-08 09:44:00,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 144058: learning rate 0.0010
[2019-03-08 09:44:00,288] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 144068: loss 27.6600
[2019-03-08 09:44:00,293] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 144069: learning rate 0.0010
[2019-03-08 09:44:00,299] A3C_AGENT_WORKER-Thread-7 INFO:Local step 9000, global step 144071: loss 5.3596
[2019-03-08 09:44:00,302] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 9000, global step 144071: learning rate 0.0010
[2019-03-08 09:44:00,357] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 144097: loss 0.1133
[2019-03-08 09:44:00,359] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 144097: learning rate 0.0010
[2019-03-08 09:44:00,543] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3391123e-06 1.9022829e-24 2.4748059e-09 9.9999666e-01 2.2738725e-22], sum to 1.0000
[2019-03-08 09:44:00,552] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9055
[2019-03-08 09:44:00,564] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.8, 62.66666666666666, 1.0, 2.0, 0.8915703075628219, 1.0, 2.0, 0.712225465424707, 1.0, 1.0, 0.9739780311643539, 6.911200000000001, 6.9112, 1556908.790689915, 1556908.790689914, 339739.9980712194], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5404200.0000, 
sim time next is 5404800.0000, 
raw observation next is [28.8, 62.33333333333334, 1.0, 2.0, 0.8777965945868849, 1.0, 2.0, 0.7053386089367386, 1.0, 2.0, 0.9752706629357468, 6.9112, 6.9112, 1541859.490941103, 1541859.490941103, 335702.9946497816], 
processed observation next is [1.0, 0.5652173913043478, 0.9454545454545454, 0.6233333333333334, 1.0, 1.0, 0.8777965945868849, 1.0, 1.0, 0.7053386089367386, 1.0, 1.0, 0.9752706629357468, 0.0, 0.0, 0.642441454558793, 0.642441454558793, 0.839257486624454], 
reward next is 0.6561, 
noisyNet noise sample is [array([0.6198961], dtype=float32), 1.4234585]. 
=============================================
[2019-03-08 09:44:00,589] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 144199: loss 0.3869
[2019-03-08 09:44:00,592] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 144200: learning rate 0.0010
[2019-03-08 09:44:00,975] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.9540487e-14 1.5246823e-25 4.2320626e-12 1.0000000e+00 1.1392896e-16], sum to 1.0000
[2019-03-08 09:44:00,983] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0808
[2019-03-08 09:44:00,988] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.4, 71.0, 1.0, 2.0, 0.7162024691121511, 1.0, 2.0, 0.7162024691121511, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1066786.389786641, 1066786.389786641, 212167.9263231912], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5410800.0000, 
sim time next is 5411400.0000, 
raw observation next is [23.66666666666666, 74.16666666666667, 1.0, 2.0, 0.693680411375107, 1.0, 2.0, 0.693680411375107, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1036699.089063037, 1036699.089063038, 204125.7028017236], 
processed observation next is [1.0, 0.6521739130434783, 0.7121212121212118, 0.7416666666666667, 1.0, 1.0, 0.693680411375107, 1.0, 1.0, 0.693680411375107, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.4319579537762654, 0.43195795377626583, 0.510314257004309], 
reward next is 0.7255, 
noisyNet noise sample is [array([0.45539922], dtype=float32), 0.41482836]. 
=============================================
[2019-03-08 09:44:01,082] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2342711e-04 1.4084421e-14 7.0487231e-04 9.9907172e-01 1.5332767e-10], sum to 1.0000
[2019-03-08 09:44:01,091] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0576
[2019-03-08 09:44:01,099] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.0, 90.0, 1.0, 2.0, 0.28434230641252, 1.0, 2.0, 0.28434230641252, 0.0, 2.0, 0.0, 6.9112, 6.9112, 434879.9549101539, 434879.9549101539, 95618.32190658842], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5416800.0000, 
sim time next is 5417400.0000, 
raw observation next is [20.0, 90.0, 1.0, 2.0, 0.2438037361648127, 1.0, 2.0, 0.2438037361648127, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 373110.9960738525, 373110.9960738524, 90002.16075891344], 
processed observation next is [1.0, 0.6956521739130435, 0.5454545454545454, 0.9, 1.0, 1.0, 0.2438037361648127, 1.0, 1.0, 0.2438037361648127, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.1554629150307719, 0.15546291503077184, 0.2250054018972836], 
reward next is 0.5922, 
noisyNet noise sample is [array([-1.0425432], dtype=float32), -0.23376204]. 
=============================================
[2019-03-08 09:44:02,062] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6133693e-01 1.2141208e-22 4.3866310e-01 2.2752489e-10 1.5236020e-17], sum to 1.0000
[2019-03-08 09:44:02,072] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2531
[2019-03-08 09:44:02,200] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.8, 93.0, 1.0, 2.0, 0.2633145521741847, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4986381039610442, 6.911199999999999, 6.9112, 405224.0353899832, 405224.0353899834, 106480.9658966207], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5432400.0000, 
sim time next is 5433000.0000, 
raw observation next is [18.71666666666667, 93.5, 1.0, 2.0, 0.190239446942769, 1.0, 1.0, 0.190239446942769, 1.0, 2.0, 0.3591370533466678, 6.9112, 6.9112, 437666.4977939702, 437666.4977939702, 126514.7640261369], 
processed observation next is [1.0, 0.9130434782608695, 0.48712121212121223, 0.935, 1.0, 1.0, 0.190239446942769, 1.0, 0.5, 0.190239446942769, 1.0, 1.0, 0.3591370533466678, 0.0, 0.0, 0.1823610407474876, 0.1823610407474876, 0.3162869100653422], 
reward next is 0.4942, 
noisyNet noise sample is [array([0.38380373], dtype=float32), -0.3083681]. 
=============================================
[2019-03-08 09:44:02,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[59.563656]
 [58.79294 ]
 [59.830994]
 [59.41375 ]
 [59.953987]], R is [[60.23051453]
 [60.17186356]
 [60.11290359]
 [60.14423752]
 [60.17884827]].
[2019-03-08 09:44:03,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4187682e-06 6.3191106e-20 6.9679432e-02 9.3031812e-01 2.0100958e-14], sum to 1.0000
[2019-03-08 09:44:04,008] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8459
[2019-03-08 09:44:04,013] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.36666666666667, 99.0, 1.0, 2.0, 0.2324095424821884, 1.0, 2.0, 0.2324095424821884, 0.0, 2.0, 0.0, 6.9112, 6.9112, 359810.2365528789, 359810.2365528789, 87268.97109861337], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5456400.0000, 
sim time next is 5457000.0000, 
raw observation next is [17.28333333333333, 99.5, 1.0, 2.0, 0.2312385401892395, 1.0, 2.0, 0.2312385401892395, 0.0, 2.0, 0.0, 6.9112, 6.9112, 358065.7591275827, 358065.7591275827, 87101.10859750872], 
processed observation next is [1.0, 0.13043478260869565, 0.4219696969696969, 0.995, 1.0, 1.0, 0.2312385401892395, 1.0, 1.0, 0.2312385401892395, 0.0, 1.0, 0.0, 0.0, 0.0, 0.14919406630315946, 0.14919406630315946, 0.21775277149377179], 
reward next is 0.5873, 
noisyNet noise sample is [array([0.20125954], dtype=float32), 0.25866017]. 
=============================================
[2019-03-08 09:44:04,021] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[55.94547 ]
 [55.92551 ]
 [55.92217 ]
 [55.93349 ]
 [55.884945]], R is [[55.98699188]
 [56.01612473]
 [56.0447731 ]
 [56.07808304]
 [56.1204567 ]].
[2019-03-08 09:44:09,068] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.5101159e-16 5.9500278e-26 2.4637681e-14 1.0000000e+00 2.9566523e-21], sum to 1.0000
[2019-03-08 09:44:09,076] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5627
[2019-03-08 09:44:09,085] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.0, 93.0, 1.0, 2.0, 0.2861454613065559, 1.0, 2.0, 0.2861454613065559, 0.0, 2.0, 0.0, 6.9112, 6.9112, 436423.845612977, 436423.845612977, 96164.80500625618], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5547600.0000, 
sim time next is 5548200.0000, 
raw observation next is [20.08333333333334, 92.5, 1.0, 2.0, 0.2861100686049413, 1.0, 2.0, 0.2861100686049413, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 436303.5385784139, 436303.538578414, 96174.70167917856], 
processed observation next is [1.0, 0.21739130434782608, 0.5492424242424245, 0.925, 1.0, 1.0, 0.2861100686049413, 1.0, 1.0, 0.2861100686049413, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.1817931410743391, 0.18179314107433917, 0.24043675419794638], 
reward next is 0.6481, 
noisyNet noise sample is [array([0.18473835], dtype=float32), 0.7510391]. 
=============================================
[2019-03-08 09:44:09,744] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4946181e-19 2.0755240e-29 3.2066995e-13 1.0000000e+00 6.6271581e-19], sum to 1.0000
[2019-03-08 09:44:09,755] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0143
[2019-03-08 09:44:09,763] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.8, 76.0, 1.0, 2.0, 0.6765029780403563, 1.0, 2.0, 0.6765029780403563, 0.0, 2.0, 0.0, 6.9112, 6.9112, 1008843.432005722, 1008843.432005722, 195747.9713424872], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5562000.0000, 
sim time next is 5562600.0000, 
raw observation next is [24.18333333333334, 74.5, 1.0, 2.0, 0.7290387535842112, 1.0, 2.0, 0.7290387535842112, 0.0, 2.0, 0.0, 6.9112, 6.9112, 1084594.051045913, 1084594.051045913, 216217.495716662], 
processed observation next is [1.0, 0.391304347826087, 0.7356060606060609, 0.745, 1.0, 1.0, 0.7290387535842112, 1.0, 1.0, 0.7290387535842112, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4519141879357971, 0.4519141879357971, 0.540543739291655], 
reward next is 0.7166, 
noisyNet noise sample is [array([-1.4629756], dtype=float32), 0.39797908]. 
=============================================
[2019-03-08 09:44:17,282] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151796: loss 0.0198
[2019-03-08 09:44:17,285] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151797: learning rate 0.0010
[2019-03-08 09:44:17,328] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 151817: loss 0.0297
[2019-03-08 09:44:17,332] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 151818: learning rate 0.0010
[2019-03-08 09:44:17,344] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 151825: loss 0.0231
[2019-03-08 09:44:17,346] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 151826: learning rate 0.0010
[2019-03-08 09:44:17,450] A3C_AGENT_WORKER-Thread-8 INFO:Local step 9500, global step 151884: loss 0.0299
[2019-03-08 09:44:17,452] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 9500, global step 151885: learning rate 0.0010
[2019-03-08 09:44:17,511] A3C_AGENT_WORKER-Thread-4 INFO:Local step 9500, global step 151913: loss 0.0509
[2019-03-08 09:44:17,514] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 9500, global step 151913: learning rate 0.0010
[2019-03-08 09:44:17,518] A3C_AGENT_WORKER-Thread-6 INFO:Local step 9500, global step 151915: loss 0.0516
[2019-03-08 09:44:17,520] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 9500, global step 151915: learning rate 0.0010
[2019-03-08 09:44:17,629] A3C_AGENT_WORKER-Thread-5 INFO:Local step 9500, global step 151966: loss 0.0356
[2019-03-08 09:44:17,632] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 9500, global step 151966: learning rate 0.0010
[2019-03-08 09:44:17,682] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 151993: loss 0.0318
[2019-03-08 09:44:17,684] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 151993: learning rate 0.0010
[2019-03-08 09:44:17,719] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 152007: loss 0.0221
[2019-03-08 09:44:17,721] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 152007: learning rate 0.0010
[2019-03-08 09:44:17,751] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152018: loss 0.0225
[2019-03-08 09:44:17,753] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152019: learning rate 0.0010
[2019-03-08 09:44:17,754] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 152020: loss 0.0175
[2019-03-08 09:44:17,757] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 152021: learning rate 0.0010
[2019-03-08 09:44:17,915] A3C_AGENT_WORKER-Thread-7 INFO:Local step 9500, global step 152095: loss 0.0155
[2019-03-08 09:44:17,919] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 9500, global step 152095: learning rate 0.0010
[2019-03-08 09:44:17,921] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152097: loss 0.0142
[2019-03-08 09:44:17,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152097: learning rate 0.0010
[2019-03-08 09:44:17,939] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 152103: loss 0.0153
[2019-03-08 09:44:17,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 152104: learning rate 0.0010
[2019-03-08 09:44:18,026] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 152141: loss 0.0411
[2019-03-08 09:44:18,030] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 152141: learning rate 0.0010
[2019-03-08 09:44:18,154] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 152204: loss 0.0465
[2019-03-08 09:44:18,158] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 152205: learning rate 0.0010
[2019-03-08 09:44:19,254] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.9615312e-22 1.6459063e-11 4.9403442e-15 4.1444589e-22], sum to 1.0000
[2019-03-08 09:44:19,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5908
[2019-03-08 09:44:19,268] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [10.33333333333333, 89.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.25, 6.9112, 6.9112, 92204.11477509364, 92204.11477509364, 27993.87660275432], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5726400.0000, 
sim time next is 5727000.0000, 
raw observation next is [10.71666666666667, 87.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.25, 6.9112, 6.9112, 96310.83374933443, 96310.83374933443, 29097.17370118663], 
processed observation next is [0.0, 0.2608695652173913, 0.12348484848484866, 0.8783333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.040129514062222676, 0.040129514062222676, 0.07274293425296657], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8238591], dtype=float32), 0.9247204]. 
=============================================
[2019-03-08 09:44:19,282] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[32.157578]
 [32.23855 ]
 [32.321053]
 [32.38883 ]
 [32.437477]], R is [[31.75538063]
 [31.43782806]
 [31.12344933]
 [30.81221581]
 [30.50409317]].
[2019-03-08 09:44:24,256] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0000000e+00 2.1989044e-24 2.2102582e-16 3.9406138e-14 1.3354207e-21], sum to 1.0000
[2019-03-08 09:44:24,268] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3828
[2019-03-08 09:44:24,274] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [13.0, 80.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.296452379627943, 6.9112, 6.9112, 120414.4446490808, 120414.4446490808, 35460.37545131436], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5812200.0000, 
sim time next is 5812800.0000, 
raw observation next is [13.46666666666667, 78.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3122852409926677, 6.9112, 6.9112, 126849.4453124405, 126849.4453124405, 37104.39606085195], 
processed observation next is [1.0, 0.2608695652173913, 0.24848484848484864, 0.7866666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3122852409926677, 0.0, 0.0, 0.05285393554685021, 0.05285393554685021, 0.09276099015212987], 
reward next is 0.4884, 
noisyNet noise sample is [array([1.9122341], dtype=float32), -0.058947317]. 
=============================================
[2019-03-08 09:44:29,086] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0000000e+00 1.2384012e-23 6.9257222e-10 1.3902082e-13 7.3261173e-26], sum to 1.0000
[2019-03-08 09:44:29,097] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3253
[2019-03-08 09:44:29,106] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.2, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5576725753140941, 6.911199999999999, 6.9112, 226632.1429070641, 226632.1429070642, 64701.4939836801], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5895600.0000, 
sim time next is 5896200.0000, 
raw observation next is [17.2, 75.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5602901663117112, 6.9112, 6.9112, 227696.9056252262, 227696.9056252262, 64786.53262037619], 
processed observation next is [1.0, 0.21739130434782608, 0.41818181818181815, 0.755, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5602901663117112, 0.0, 0.0, 0.09487371067717759, 0.09487371067717759, 0.16196633155094048], 
reward next is 0.5021, 
noisyNet noise sample is [array([1.4991708], dtype=float32), 0.16752201]. 
=============================================
[2019-03-08 09:44:30,664] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0278080e-20 2.6042864e-27 5.8636776e-13 1.0000000e+00 6.9825047e-25], sum to 1.0000
[2019-03-08 09:44:30,674] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6485
[2019-03-08 09:44:30,680] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.43333333333333, 62.0, 1.0, 2.0, 0.4379527479005525, 1.0, 2.0, 0.4379527479005525, 0.0, 2.0, 0.0, 6.9112, 6.9112, 665916.0190145151, 665916.0190145151, 123220.2087897096], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5913600.0000, 
sim time next is 5914200.0000, 
raw observation next is [24.71666666666667, 61.0, 1.0, 2.0, 0.4549597116900763, 1.0, 2.0, 0.4549597116900763, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 690947.3545439004, 690947.3545439002, 126940.1693653654], 
processed observation next is [1.0, 0.43478260869565216, 0.7598484848484849, 0.61, 1.0, 1.0, 0.4549597116900763, 1.0, 1.0, 0.4549597116900763, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.2878947310599585, 0.28789473105995844, 0.31735042341341346], 
reward next is 0.7776, 
noisyNet noise sample is [array([1.1144818], dtype=float32), -0.7117134]. 
=============================================
[2019-03-08 09:44:34,717] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 159770: loss 1.2414
[2019-03-08 09:44:34,723] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 159770: learning rate 0.0010
[2019-03-08 09:44:34,774] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159789: loss 1.2323
[2019-03-08 09:44:34,776] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159790: learning rate 0.0010
[2019-03-08 09:44:34,864] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 159835: loss 0.6629
[2019-03-08 09:44:34,870] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 159836: learning rate 0.0010
[2019-03-08 09:44:35,018] A3C_AGENT_WORKER-Thread-8 INFO:Local step 10000, global step 159906: loss 0.5135
[2019-03-08 09:44:35,018] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 10000, global step 159906: learning rate 0.0010
[2019-03-08 09:44:35,108] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 159941: loss 0.4413
[2019-03-08 09:44:35,112] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 159942: learning rate 0.0010
[2019-03-08 09:44:35,182] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 159974: loss 0.2208
[2019-03-08 09:44:35,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 159974: learning rate 0.0010
[2019-03-08 09:44:35,221] A3C_AGENT_WORKER-Thread-4 INFO:Local step 10000, global step 159992: loss 0.2409
[2019-03-08 09:44:35,224] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 10000, global step 159993: learning rate 0.0010
[2019-03-08 09:44:35,242] A3C_AGENT_WORKER-Thread-5 INFO:Local step 10000, global step 160000: loss 0.5650
[2019-03-08 09:44:35,245] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 10000, global step 160000: learning rate 0.0010
[2019-03-08 09:44:35,268] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 160008: loss 0.2490
[2019-03-08 09:44:35,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 160009: learning rate 0.0010
[2019-03-08 09:44:35,298] A3C_AGENT_WORKER-Thread-6 INFO:Local step 10000, global step 160020: loss 0.1441
[2019-03-08 09:44:35,299] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 10000, global step 160020: learning rate 0.0010
[2019-03-08 09:44:35,351] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 160041: loss 0.2339
[2019-03-08 09:44:35,354] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 160041: learning rate 0.0010
[2019-03-08 09:44:35,368] A3C_AGENT_WORKER-Thread-7 INFO:Local step 10000, global step 160045: loss 0.1960
[2019-03-08 09:44:35,374] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 10000, global step 160048: learning rate 0.0010
[2019-03-08 09:44:35,413] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160068: loss 0.1609
[2019-03-08 09:44:35,414] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160068: learning rate 0.0010
[2019-03-08 09:44:35,472] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 160089: loss 0.0521
[2019-03-08 09:44:35,477] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 160092: learning rate 0.0010
[2019-03-08 09:44:35,492] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 160099: loss 0.0686
[2019-03-08 09:44:35,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 160102: learning rate 0.0010
[2019-03-08 09:44:35,772] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 160228: loss 0.0843
[2019-03-08 09:44:35,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 160229: learning rate 0.0010
[2019-03-08 09:44:40,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9904662e-01 2.2483594e-07 9.4457791e-04 8.4733283e-06 8.0436600e-08], sum to 1.0000
[2019-03-08 09:44:40,436] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2866
[2019-03-08 09:44:40,452] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.33333333333333, 67.33333333333333, 1.0, 2.0, 0.2519820686900503, 0.0, 1.0, 0.0, 1.0, 2.0, 0.4851701405644058, 6.911199999999999, 6.9112, 394269.1203819884, 394269.1203819885, 100096.6592250568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6079200.0000, 
sim time next is 6079800.0000, 
raw observation next is [17.51666666666667, 66.16666666666667, 1.0, 2.0, 0.536457108503856, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 419767.011608938, 419767.0116089379, 73961.3820539479], 
processed observation next is [1.0, 0.34782608695652173, 0.43257575757575767, 0.6616666666666667, 1.0, 1.0, 0.536457108503856, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 8.881784197001253e-17, 0.0, 0.17490292150372416, 0.1749029215037241, 0.18490345513486978], 
reward next is 0.8108, 
noisyNet noise sample is [array([-0.11158133], dtype=float32), 1.3085583]. 
=============================================
[2019-03-08 09:44:41,731] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3021983e-14 1.9986035e-13 2.2419586e-08 1.0000000e+00 7.5532545e-11], sum to 1.0000
[2019-03-08 09:44:41,743] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3416
[2019-03-08 09:44:41,749] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.8, 53.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 290543.859935901, 290543.859935901, 80255.04152067921], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6115200.0000, 
sim time next is 6115800.0000, 
raw observation next is [21.7, 53.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 288147.9900055893, 288147.9900055893, 79835.23656920083], 
processed observation next is [1.0, 0.782608695652174, 0.6227272727272727, 0.53, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.12006166250232889, 0.12006166250232889, 0.19958809142300207], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00691278], dtype=float32), -0.231754]. 
=============================================
[2019-03-08 09:44:43,987] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 3.4461552e-37 6.7406839e-22 1.1256374e-33 3.1150929e-34], sum to 1.0000
[2019-03-08 09:44:43,996] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0932
[2019-03-08 09:44:44,005] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.53333333333333, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6672783318801423, 6.9112, 6.9112, 271224.7239408256, 271224.7239408256, 76806.35421773841], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6150000.0000, 
sim time next is 6150600.0000, 
raw observation next is [17.45, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6672780385772568, 6.9112, 6.9112, 271224.6045899232, 271224.6045899232, 76938.83394936389], 
processed observation next is [1.0, 0.17391304347826086, 0.4295454545454545, 0.81, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6672780385772568, 0.0, 0.0, 0.11301025191246801, 0.11301025191246801, 0.19234708487340973], 
reward next is 0.5036, 
noisyNet noise sample is [array([-0.84072655], dtype=float32), 0.55379784]. 
=============================================
[2019-03-08 09:44:46,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2733529e-10 3.2077395e-08 5.2103247e-03 8.1301081e-01 1.8177888e-01], sum to 1.0000
[2019-03-08 09:44:46,870] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0463
[2019-03-08 09:44:46,876] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.7, 61.66666666666667, 1.0, 2.0, 0.5212783068327297, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 403365.9420232949, 403365.9420232949, 80278.5133425226], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6193200.0000, 
sim time next is 6193800.0000, 
raw observation next is [22.7, 60.5, 1.0, 2.0, 0.2083998348731824, 1.0, 1.0, 0.2083998348731824, 0.0, 2.0, 0.0, 6.9112, 6.9112, 322295.037105723, 322295.037105723, 84586.38207940303], 
processed observation next is [1.0, 0.6956521739130435, 0.6681818181818181, 0.605, 1.0, 1.0, 0.2083998348731824, 1.0, 0.5, 0.2083998348731824, 0.0, 1.0, 0.0, 0.0, 0.0, 0.13428959879405125, 0.13428959879405125, 0.21146595519850758], 
reward next is 0.5443, 
noisyNet noise sample is [array([0.88556236], dtype=float32), -2.2483459]. 
=============================================
[2019-03-08 09:44:50,535] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.824645e-08 2.153632e-07 9.919973e-01 7.931725e-03 7.062249e-05], sum to 1.0000
[2019-03-08 09:44:50,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7426
[2019-03-08 09:44:50,552] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.05, 71.16666666666666, 1.0, 2.0, 0.7717869012381197, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 574594.1173067203, 574594.1173067203, 122096.466231365], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6263400.0000, 
sim time next is 6264000.0000, 
raw observation next is [26.6, 69.0, 1.0, 2.0, 0.3868045218175464, 0.0, 2.0, 0.0, 1.0, 1.0, 0.7015118773395471, 6.9112, 6.9112, 570263.0222744647, 570263.0222744647, 136219.9537790293], 
processed observation next is [0.0, 0.5217391304347826, 0.8454545454545456, 0.69, 1.0, 1.0, 0.3868045218175464, 0.0, 1.0, 0.0, 1.0, 0.5, 0.7015118773395471, 0.0, 0.0, 0.2376095926143603, 0.2376095926143603, 0.3405498844475733], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6735099], dtype=float32), -1.3360952]. 
=============================================
[2019-03-08 09:44:50,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[51.77851 ]
 [49.85662 ]
 [50.159103]
 [50.62551 ]
 [51.400627]], R is [[50.15091705]
 [50.32170486]
 [49.81848907]
 [49.90499878]
 [50.04860306]].
[2019-03-08 09:44:52,378] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167809: loss 0.0464
[2019-03-08 09:44:52,380] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167809: learning rate 0.0010
[2019-03-08 09:44:52,415] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 167825: loss 0.0416
[2019-03-08 09:44:52,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 167825: learning rate 0.0010
[2019-03-08 09:44:52,536] A3C_AGENT_WORKER-Thread-8 INFO:Local step 10500, global step 167884: loss 0.1917
[2019-03-08 09:44:52,538] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 10500, global step 167884: learning rate 0.0010
[2019-03-08 09:44:52,551] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 167888: loss 7.0184
[2019-03-08 09:44:52,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 167891: learning rate 0.0010
[2019-03-08 09:44:52,625] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 167917: loss 2.4652
[2019-03-08 09:44:52,627] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 167918: learning rate 0.0010
[2019-03-08 09:44:52,679] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167943: loss 0.0127
[2019-03-08 09:44:52,684] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167944: learning rate 0.0010
[2019-03-08 09:44:52,737] A3C_AGENT_WORKER-Thread-6 INFO:Local step 10500, global step 167970: loss -1.1633
[2019-03-08 09:44:52,743] A3C_AGENT_WORKER-Thread-4 INFO:Local step 10500, global step 167972: loss -108.3716
[2019-03-08 09:44:52,743] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 10500, global step 167971: learning rate 0.0010
[2019-03-08 09:44:52,748] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 10500, global step 167972: learning rate 0.0010
[2019-03-08 09:44:52,781] A3C_AGENT_WORKER-Thread-5 INFO:Local step 10500, global step 167988: loss -1.0836
[2019-03-08 09:44:52,785] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 10500, global step 167988: learning rate 0.0010
[2019-03-08 09:44:52,812] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 167999: loss -0.5189
[2019-03-08 09:44:52,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 167999: learning rate 0.0010
[2019-03-08 09:44:52,933] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168056: loss 0.1195
[2019-03-08 09:44:52,934] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168056: learning rate 0.0010
[2019-03-08 09:44:52,936] A3C_AGENT_WORKER-Thread-7 INFO:Local step 10500, global step 168058: loss 0.1039
[2019-03-08 09:44:52,937] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 10500, global step 168058: learning rate 0.0010
[2019-03-08 09:44:52,977] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168076: loss 20.1193
[2019-03-08 09:44:52,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168076: learning rate 0.0010
[2019-03-08 09:44:53,048] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 168108: loss -23.9448
[2019-03-08 09:44:53,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 168108: learning rate 0.0010
[2019-03-08 09:44:53,058] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168113: loss 0.0829
[2019-03-08 09:44:53,066] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168115: learning rate 0.0010
[2019-03-08 09:44:53,185] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 168168: loss 0.0828
[2019-03-08 09:44:53,186] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 168168: learning rate 0.0010
[2019-03-08 09:45:01,803] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4296680e-20 6.2690332e-20 7.7668401e-12 1.0000000e+00 3.4532425e-14], sum to 1.0000
[2019-03-08 09:45:01,807] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4403
[2019-03-08 09:45:01,812] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.86666666666667, 65.66666666666666, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 278673.7419105116, 278673.7419105116, 76926.94905718243], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6460800.0000, 
sim time next is 6461400.0000, 
raw observation next is [18.58333333333334, 65.33333333333334, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 267389.575642487, 267389.575642487, 73678.08428796922], 
processed observation next is [1.0, 0.782608695652174, 0.48106060606060635, 0.6533333333333334, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.11141232318436957, 0.11141232318436957, 0.18419521071992306], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.59003234], dtype=float32), -1.2829183]. 
=============================================
[2019-03-08 09:45:04,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 1.5163591e-32 5.9232266e-22 9.9562851e-27 1.9530349e-29], sum to 1.0000
[2019-03-08 09:45:04,369] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2431
[2019-03-08 09:45:04,374] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [12.56666666666667, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3487040505816291, 6.9112, 6.9112, 141652.7748883642, 141652.7748883642, 40611.23723817479], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6502800.0000, 
sim time next is 6503400.0000, 
raw observation next is [12.75, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3538296205844909, 6.9112, 6.9112, 143736.3597704071, 143736.3597704071, 41243.82478011316], 
processed observation next is [1.0, 0.2608695652173913, 0.2159090909090909, 0.89, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3538296205844909, 0.0, 0.0, 0.0598901499043363, 0.0598901499043363, 0.1031095619502829], 
reward next is 0.4979, 
noisyNet noise sample is [array([-0.14559576], dtype=float32), -0.24845794]. 
=============================================
[2019-03-08 09:45:06,381] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.9999976e-01 4.8453846e-28 2.6886636e-07 1.1209381e-15 5.0704772e-24], sum to 1.0000
[2019-03-08 09:45:06,389] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0311
[2019-03-08 09:45:06,404] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.31666666666667, 51.16666666666666, 1.0, 2.0, 0.3063691649412187, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5899619752378653, 6.911199999999999, 6.9112, 479516.5531014088, 479516.5531014089, 114807.2426221556], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6533400.0000, 
sim time next is 6534000.0000, 
raw observation next is [20.5, 51.0, 1.0, 2.0, 0.4176944848833734, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 326977.2088357017, 326977.2088357017, 55041.47268324952], 
processed observation next is [1.0, 0.6521739130434783, 0.5681818181818182, 0.51, 1.0, 1.0, 0.4176944848833734, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.13624050368154236, 0.13624050368154236, 0.13760368170812382], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.15575737], dtype=float32), 1.0255682]. 
=============================================
[2019-03-08 09:45:06,421] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[55.4326  ]
 [44.677692]
 [55.207897]
 [43.709248]
 [50.916348]], R is [[44.36883926]
 [43.92515182]
 [43.48590088]
 [43.05104065]
 [42.62052917]].
[2019-03-08 09:45:09,744] A3C_AGENT_WORKER-Thread-8 INFO:Local step 11000, global step 175766: loss 129.2792
[2019-03-08 09:45:09,749] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 11000, global step 175767: learning rate 0.0010
[2019-03-08 09:45:09,806] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175795: loss 195.6305
[2019-03-08 09:45:09,810] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175795: learning rate 0.0010
[2019-03-08 09:45:09,934] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 175851: loss 134.4844
[2019-03-08 09:45:09,938] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 175852: learning rate 0.0010
[2019-03-08 09:45:09,951] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 175860: loss 131.2596
[2019-03-08 09:45:09,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 175862: learning rate 0.0010
[2019-03-08 09:45:10,077] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 175923: loss 156.0292
[2019-03-08 09:45:10,080] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 175923: learning rate 0.0010
[2019-03-08 09:45:10,131] A3C_AGENT_WORKER-Thread-6 INFO:Local step 11000, global step 175944: loss 68.8520
[2019-03-08 09:45:10,133] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 11000, global step 175946: learning rate 0.0010
[2019-03-08 09:45:10,169] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 175960: loss 92.4723
[2019-03-08 09:45:10,170] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 175960: learning rate 0.0010
[2019-03-08 09:45:10,216] A3C_AGENT_WORKER-Thread-4 INFO:Local step 11000, global step 175983: loss 93.6101
[2019-03-08 09:45:10,223] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 11000, global step 175984: learning rate 0.0010
[2019-03-08 09:45:10,260] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176001: loss 61.4277
[2019-03-08 09:45:10,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176002: learning rate 0.0010
[2019-03-08 09:45:10,272] A3C_AGENT_WORKER-Thread-5 INFO:Local step 11000, global step 176007: loss 92.9792
[2019-03-08 09:45:10,275] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 11000, global step 176008: learning rate 0.0010
[2019-03-08 09:45:10,324] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 176031: loss 99.1767
[2019-03-08 09:45:10,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 176031: learning rate 0.0010
[2019-03-08 09:45:10,365] A3C_AGENT_WORKER-Thread-7 INFO:Local step 11000, global step 176049: loss 94.7393
[2019-03-08 09:45:10,366] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 11000, global step 176049: learning rate 0.0010
[2019-03-08 09:45:10,407] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176062: loss 92.7965
[2019-03-08 09:45:10,409] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176063: learning rate 0.0010
[2019-03-08 09:45:10,529] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 176126: loss 269.4921
[2019-03-08 09:45:10,531] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 176127: learning rate 0.0010
[2019-03-08 09:45:10,549] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 176131: loss 225.4876
[2019-03-08 09:45:10,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 176131: learning rate 0.0010
[2019-03-08 09:45:10,680] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 176194: loss 217.4119
[2019-03-08 09:45:10,684] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 176195: learning rate 0.0010
[2019-03-08 09:45:14,548] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.8460299e-37 9.6696550e-24 9.9195430e-36 4.6407867e-33], sum to 1.0000
[2019-03-08 09:45:14,557] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4366
[2019-03-08 09:45:14,571] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.63333333333333, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8380285013964284, 6.9112, 6.9112, 340691.6568079992, 340691.6568079992, 93364.46149415801], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6672000.0000, 
sim time next is 6672600.0000, 
raw observation next is [18.55, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8347607548257138, 6.911200000000001, 6.9112, 339361.4577749089, 339361.4577749088, 93086.7676531521], 
processed observation next is [1.0, 0.21739130434782608, 0.47954545454545455, 0.885, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8347607548257138, 8.881784197001253e-17, 0.0, 0.14140060740621205, 0.141400607406212, 0.23271691913288026], 
reward next is 0.5208, 
noisyNet noise sample is [array([-0.55788857], dtype=float32), 0.19935971]. 
=============================================
[2019-03-08 09:45:15,399] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9999940e-01 2.0485980e-21 6.1818338e-07 7.9683055e-10 5.0823139e-14], sum to 1.0000
[2019-03-08 09:45:15,410] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9437
[2019-03-08 09:45:15,422] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.55, 90.0, 1.0, 2.0, 0.4050881367765011, 1.0, 1.0, 0.4050881367765011, 0.0, 2.0, 0.0, 6.9112, 6.9112, 625454.3172598148, 625454.3172598148, 114370.4987494885], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6690600.0000, 
sim time next is 6691200.0000, 
raw observation next is [18.46666666666667, 91.0, 1.0, 2.0, 0.2453417741130528, 1.0, 2.0, 0.2453417741130528, 1.0, 1.0, 0.4635509718623746, 6.911199999999999, 6.9112, 565021.5843208092, 565021.5843208095, 141274.9285910393], 
processed observation next is [1.0, 0.43478260869565216, 0.4757575757575758, 0.91, 1.0, 1.0, 0.2453417741130528, 1.0, 1.0, 0.2453417741130528, 1.0, 0.5, 0.4635509718623746, -8.881784197001253e-17, 0.0, 0.2354256601336705, 0.2354256601336706, 0.35318732147759824], 
reward next is 0.5713, 
noisyNet noise sample is [array([-0.8451964], dtype=float32), 0.4958984]. 
=============================================
[2019-03-08 09:45:27,246] A3C_AGENT_WORKER-Thread-8 INFO:Local step 11500, global step 183780: loss 19.7498
[2019-03-08 09:45:27,249] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 11500, global step 183781: learning rate 0.0010
[2019-03-08 09:45:27,298] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183802: loss 27.9476
[2019-03-08 09:45:27,301] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183802: learning rate 0.0010
[2019-03-08 09:45:27,437] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 183873: loss 35.2002
[2019-03-08 09:45:27,442] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 183875: learning rate 0.0010
[2019-03-08 09:45:27,535] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 183911: loss -18.2366
[2019-03-08 09:45:27,536] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 183912: learning rate 0.0010
[2019-03-08 09:45:27,613] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 183954: loss -125.7963
[2019-03-08 09:45:27,615] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 183954: learning rate 0.0010
[2019-03-08 09:45:27,618] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 183954: loss -52.8638
[2019-03-08 09:45:27,620] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 183954: learning rate 0.0010
[2019-03-08 09:45:27,643] A3C_AGENT_WORKER-Thread-6 INFO:Local step 11500, global step 183964: loss -104.8657
[2019-03-08 09:45:27,647] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 11500, global step 183964: learning rate 0.0010
[2019-03-08 09:45:27,654] A3C_AGENT_WORKER-Thread-4 INFO:Local step 11500, global step 183967: loss 42.4917
[2019-03-08 09:45:27,659] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 11500, global step 183967: learning rate 0.0010
[2019-03-08 09:45:27,685] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 183975: loss -30.2492
[2019-03-08 09:45:27,687] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 183975: learning rate 0.0010
[2019-03-08 09:45:27,789] A3C_AGENT_WORKER-Thread-7 INFO:Local step 11500, global step 184029: loss -1.2859
[2019-03-08 09:45:27,792] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 11500, global step 184030: learning rate 0.0010
[2019-03-08 09:45:27,805] A3C_AGENT_WORKER-Thread-5 INFO:Local step 11500, global step 184037: loss -98.8516
[2019-03-08 09:45:27,810] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 11500, global step 184038: learning rate 0.0010
[2019-03-08 09:45:27,830] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184044: loss -132.7724
[2019-03-08 09:45:27,831] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184044: learning rate 0.0010
[2019-03-08 09:45:27,879] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184060: loss -80.5904
[2019-03-08 09:45:27,882] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184060: learning rate 0.0010
[2019-03-08 09:45:27,951] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 184100: loss 99.2029
[2019-03-08 09:45:27,954] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 184101: learning rate 0.0010
[2019-03-08 09:45:28,021] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 184131: loss -34.9547
[2019-03-08 09:45:28,022] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 184131: learning rate 0.0010
[2019-03-08 09:45:28,210] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 184216: loss -18.1055
[2019-03-08 09:45:28,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 184216: learning rate 0.0010
[2019-03-08 09:45:38,168] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 09:45:38,177] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4611
[2019-03-08 09:45:38,182] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8312077125195049, 6.911199999999999, 6.9112, 337915.1390501519, 337915.139050152, 92784.77672238616], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7083000.0000, 
sim time next is 7083600.0000, 
raw observation next is [17.9, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8271273979960213, 6.9112, 6.9112, 336254.206142529, 336254.206142529, 92438.33916250773], 
processed observation next is [1.0, 1.0, 0.44999999999999996, 0.92, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8271273979960213, 0.0, 0.0, 0.14010591922605373, 0.14010591922605373, 0.23109584790626933], 
reward next is 0.5197, 
noisyNet noise sample is [array([-0.74213433], dtype=float32), -0.06543005]. 
=============================================
[2019-03-08 09:45:41,555] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0000000e+00 9.9356405e-28 6.6126274e-22 5.6512159e-26 1.6308704e-28], sum to 1.0000
[2019-03-08 09:45:41,561] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1372
[2019-03-08 09:45:41,573] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.3, 50.0, 1.0, 2.0, 0.3166999654712188, 1.0, 2.0, 0.3166999654712188, 1.0, 2.0, 0.5955193183768744, 6.9112, 6.9112, 726048.1428364457, 726048.1428364457, 162658.4307032826], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7138200.0000, 
sim time next is 7138800.0000, 
raw observation next is [24.4, 50.0, 1.0, 2.0, 0.3200144696036489, 1.0, 2.0, 0.3200144696036489, 1.0, 2.0, 0.6014822846197639, 6.911199999999999, 6.9112, 733325.0875975918, 733325.0875975919, 163718.6864807945], 
processed observation next is [1.0, 0.6521739130434783, 0.7454545454545454, 0.5, 1.0, 1.0, 0.3200144696036489, 1.0, 1.0, 0.3200144696036489, 1.0, 1.0, 0.6014822846197639, -8.881784197001253e-17, 0.0, 0.30555211983232994, 0.30555211983233, 0.40929671620198627], 
reward next is 0.6399, 
noisyNet noise sample is [array([-0.715852], dtype=float32), 1.466165]. 
=============================================
[2019-03-08 09:45:44,640] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191731: loss 1.1816
[2019-03-08 09:45:44,642] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191731: learning rate 0.0010
[2019-03-08 09:45:44,774] A3C_AGENT_WORKER-Thread-8 INFO:Local step 12000, global step 191793: loss 1.2804
[2019-03-08 09:45:44,778] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 12000, global step 191795: learning rate 0.0010
[2019-03-08 09:45:44,832] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 191819: loss 1.4044
[2019-03-08 09:45:44,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 191819: learning rate 0.0010
[2019-03-08 09:45:44,958] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 191875: loss 1.2836
[2019-03-08 09:45:44,960] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 191875: learning rate 0.0010
[2019-03-08 09:45:45,041] A3C_AGENT_WORKER-Thread-6 INFO:Local step 12000, global step 191915: loss 1.0750
[2019-03-08 09:45:45,046] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 12000, global step 191915: learning rate 0.0010
[2019-03-08 09:45:45,052] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 191919: loss 1.1359
[2019-03-08 09:45:45,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 191919: learning rate 0.0010
[2019-03-08 09:45:45,113] A3C_AGENT_WORKER-Thread-4 INFO:Local step 12000, global step 191940: loss 1.0563
[2019-03-08 09:45:45,116] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 12000, global step 191943: learning rate 0.0010
[2019-03-08 09:45:45,181] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191974: loss 1.0663
[2019-03-08 09:45:45,183] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191974: learning rate 0.0010
[2019-03-08 09:45:45,252] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 192006: loss 1.1699
[2019-03-08 09:45:45,255] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 192006: learning rate 0.0010
[2019-03-08 09:45:45,276] A3C_AGENT_WORKER-Thread-7 INFO:Local step 12000, global step 192015: loss 1.1118
[2019-03-08 09:45:45,277] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 12000, global step 192015: learning rate 0.0010
[2019-03-08 09:45:45,328] A3C_AGENT_WORKER-Thread-5 INFO:Local step 12000, global step 192037: loss 1.0566
[2019-03-08 09:45:45,331] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 12000, global step 192038: learning rate 0.0010
[2019-03-08 09:45:45,370] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192057: loss 0.9915
[2019-03-08 09:45:45,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192058: learning rate 0.0010
[2019-03-08 09:45:45,450] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 192094: loss 1.1050
[2019-03-08 09:45:45,451] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 192094: learning rate 0.0010
[2019-03-08 09:45:45,475] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 192104: loss 1.0436
[2019-03-08 09:45:45,479] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 192104: learning rate 0.0010
[2019-03-08 09:45:45,559] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 192144: loss 0.9442
[2019-03-08 09:45:45,561] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 192145: learning rate 0.0010
[2019-03-08 09:45:45,702] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 192210: loss 1.0122
[2019-03-08 09:45:45,703] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 192210: learning rate 0.0010
[2019-03-08 09:45:48,466] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 3.5906665e-33 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:45:48,471] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8572
[2019-03-08 09:45:48,477] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.2, 69.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6079098103737539, 6.9112, 6.9112, 247068.8808888051, 247068.8808888051, 69436.75763598648], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7254600.0000, 
sim time next is 7255200.0000, 
raw observation next is [18.1, 70.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.608376965528944, 6.9112, 6.9112, 247258.9379682487, 247258.9379682487, 69630.6742289008], 
processed observation next is [1.0, 1.0, 0.45909090909090916, 0.7033333333333335, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.608376965528944, 0.0, 0.0, 0.1030245574867703, 0.1030245574867703, 0.17407668557225198], 
reward next is 0.5073, 
noisyNet noise sample is [array([1.0345714], dtype=float32), -1.6035963]. 
=============================================
[2019-03-08 09:45:54,647] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 9.7699376e-34 0.0000000e+00 1.4116304e-38], sum to 1.0000
[2019-03-08 09:45:54,662] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1593
[2019-03-08 09:45:54,668] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.7, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7588805791215903, 6.911199999999999, 6.9112, 308476.7945613996, 308476.7945613997, 86670.8094308997], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7362600.0000, 
sim time next is 7363200.0000, 
raw observation next is [17.7, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7575169045259346, 6.9112, 6.9112, 307921.8202604235, 307921.8202604235, 86556.10059399642], 
processed observation next is [1.0, 0.21739130434782608, 0.44090909090909086, 0.87, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7575169045259346, 0.0, 0.0, 0.12830075844184313, 0.12830075844184313, 0.21639025148499105], 
reward next is 0.5082, 
noisyNet noise sample is [array([0.8518768], dtype=float32), -0.56402797]. 
=============================================
[2019-03-08 09:45:56,027] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 6.927659e-34 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-08 09:45:56,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8278
[2019-03-08 09:45:56,050] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.63333333333333, 57.66666666666667, 1.0, 2.0, 0.415086366205756, 1.0, 1.0, 0.415086366205756, 1.0, 2.0, 0.7537473191449647, 6.911200000000001, 6.9112, 919189.760178253, 919189.7601782529, 196686.6970817578], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7384800.0000, 
sim time next is 7385400.0000, 
raw observation next is [26.9, 56.5, 1.0, 2.0, 0.5825044113404452, 1.0, 2.0, 0.5825044113404452, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 869862.725021689, 869862.7250216888, 162941.3604797606], 
processed observation next is [1.0, 0.4782608695652174, 0.859090909090909, 0.565, 1.0, 1.0, 0.5825044113404452, 1.0, 1.0, 0.5825044113404452, 0.0, 0.5, 0.0, 8.881784197001253e-17, 0.0, 0.3624428020923704, 0.36244280209237034, 0.4073534011994015], 
reward next is 0.7626, 
noisyNet noise sample is [array([-0.14032494], dtype=float32), -1.5147554]. 
=============================================
[2019-03-08 09:46:02,103] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0000000e+00 2.7122625e-31 1.5431910e-24 3.5849493e-31 3.0561011e-26], sum to 1.0000
[2019-03-08 09:46:02,112] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6871
[2019-03-08 09:46:02,120] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.63333333333333, 53.66666666666666, 1.0, 2.0, 0.2501711271268883, 1.0, 2.0, 0.2501711271268883, 1.0, 2.0, 0.4537073172529447, 6.9112, 6.9112, 553013.0273302607, 553013.0273302607, 144140.7576540286], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7486800.0000, 
sim time next is 7487400.0000, 
raw observation next is [28.71666666666667, 53.33333333333334, 1.0, 2.0, 0.2453921470057955, 1.0, 2.0, 0.2453921470057955, 1.0, 2.0, 0.4449633154586871, 6.9112, 6.9112, 542346.3380596417, 542346.3380596417, 142794.3258212406], 
processed observation next is [0.0, 0.6521739130434783, 0.9416666666666668, 0.5333333333333334, 1.0, 1.0, 0.2453921470057955, 1.0, 1.0, 0.2453921470057955, 1.0, 1.0, 0.4449633154586871, 0.0, 0.0, 0.22597764085818403, 0.22597764085818403, 0.3569858145531015], 
reward next is 0.5426, 
noisyNet noise sample is [array([0.6440134], dtype=float32), 0.68321085]. 
=============================================
[2019-03-08 09:46:02,288] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199799: loss 202.6731
[2019-03-08 09:46:02,290] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199799: learning rate 0.0010
[2019-03-08 09:46:02,335] A3C_AGENT_WORKER-Thread-8 INFO:Local step 12500, global step 199820: loss 188.9424
[2019-03-08 09:46:02,338] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 12500, global step 199821: learning rate 0.0010
[2019-03-08 09:46:02,375] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 199835: loss 254.4544
[2019-03-08 09:46:02,377] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 199835: learning rate 0.0010
[2019-03-08 09:46:02,426] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199868: loss 271.5888
[2019-03-08 09:46:02,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199869: learning rate 0.0010
[2019-03-08 09:46:02,485] A3C_AGENT_WORKER-Thread-6 INFO:Local step 12500, global step 199893: loss 250.3208
[2019-03-08 09:46:02,489] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 12500, global step 199895: learning rate 0.0010
[2019-03-08 09:46:02,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.00000000e+00 1.53791995e-31 2.26119816e-23 7.69167218e-30
 1.00304115e-30], sum to 1.0000
[2019-03-08 09:46:02,543] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4696
[2019-03-08 09:46:02,556] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.63333333333333, 60.33333333333333, 1.0, 2.0, 0.3378145511038478, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6227607023673801, 6.911199999999999, 6.9112, 506183.5617409357, 506183.5617409358, 124618.166499985], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7494000.0000, 
sim time next is 7494600.0000, 
raw observation next is [26.36666666666667, 61.16666666666667, 1.0, 2.0, 0.3436005823281758, 1.0, 1.0, 0.3436005823281758, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 515124.1917078426, 515124.1917078427, 106866.8473131357], 
processed observation next is [0.0, 0.7391304347826086, 0.8348484848484851, 0.6116666666666667, 1.0, 1.0, 0.3436005823281758, 1.0, 0.5, 0.3436005823281758, 0.0, 0.5, 0.0, -8.881784197001253e-17, 0.0, 0.21463507987826774, 0.2146350798782678, 0.26716711828283923], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1041316], dtype=float32), 0.62955624]. 
=============================================
[2019-03-08 09:46:02,578] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 199932: loss 245.0682
[2019-03-08 09:46:02,581] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 199933: learning rate 0.0010
[2019-03-08 09:46:02,607] A3C_AGENT_WORKER-Thread-4 INFO:Local step 12500, global step 199947: loss 214.8782
[2019-03-08 09:46:02,608] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 12500, global step 199947: learning rate 0.0010
[2019-03-08 09:46:02,727] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-08 09:46:02,731] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation job starts!
[2019-03-08 09:46:02,732] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:02,743] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-03-08 09:46:06,977] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11087208], dtype=float32), 0.382618]
[2019-03-08 09:46:06,977] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation this: [22.5, 67.0, 1.0, 2.0, 0.3246014804128867, 1.0, 2.0, 0.3246014804128867, 1.0, 2.0, 0.6025984886201561, 6.911200000000001, 6.9112, 734687.2698543038, 734687.2698543036, 165798.4913743659]
[2019-03-08 09:46:06,977] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-03-08 09:46:06,978] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Softmax [1.00000000e+00 1.11591744e-29 1.45180304e-23 3.90462098e-28
 2.36281295e-28], sampled 0.5789583743754954
[2019-03-08 09:46:25,210] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11087208], dtype=float32), 0.382618]
[2019-03-08 09:46:25,211] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation this: [18.3, 93.0, 1.0, 2.0, 0.4357129581449704, 1.0, 2.0, 0.4357129581449704, 0.0, 1.0, 0.0, 6.9112, 6.9112, 669240.708416241, 669240.708416241, 121240.2352225937]
[2019-03-08 09:46:25,211] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-03-08 09:46:25,212] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Softmax [1.0000000e+00 1.5069946e-24 2.9954371e-18 1.1431188e-19 8.8774120e-22], sampled 0.7145150610757405
[2019-03-08 09:46:29,249] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5553.6978 1458672152.7962 430.0000
[2019-03-08 09:46:30,263] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 200000, evaluation results [200000.0, 5553.697785150822, 1458672152.7962258, 430.0]
[2019-03-08 09:46:30,301] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 200022: loss -66.3982
[2019-03-08 09:46:30,303] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 200022: loss 173.6660
[2019-03-08 09:46:30,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 200022: learning rate 0.0010
[2019-03-08 09:46:30,307] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 200023: learning rate 0.0010
[2019-03-08 09:46:30,343] A3C_AGENT_WORKER-Thread-5 INFO:Local step 12500, global step 200040: loss 160.6136
[2019-03-08 09:46:30,345] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 12500, global step 200040: learning rate 0.0010
[2019-03-08 09:46:30,364] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200048: loss 182.3991
[2019-03-08 09:46:30,369] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200050: learning rate 0.0010
[2019-03-08 09:46:30,386] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200057: loss 131.6100
[2019-03-08 09:46:30,388] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200058: learning rate 0.0010
[2019-03-08 09:46:30,417] A3C_AGENT_WORKER-Thread-7 INFO:Local step 12500, global step 200068: loss -102.9764
[2019-03-08 09:46:30,420] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 12500, global step 200069: learning rate 0.0010
[2019-03-08 09:46:30,505] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200112: loss 86.8131
[2019-03-08 09:46:30,506] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200112: learning rate 0.0010
[2019-03-08 09:46:30,563] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 200136: loss 144.9672
[2019-03-08 09:46:30,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 200138: learning rate 0.0010
[2019-03-08 09:46:30,619] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 200163: loss 63.7522
[2019-03-08 09:46:30,620] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 200163: learning rate 0.0010
[2019-03-08 09:46:33,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 6.1436560e-19 1.2489521e-12 2.1704618e-16 4.1933233e-17], sum to 1.0000
[2019-03-08 09:46:33,032] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6606
[2019-03-08 09:46:33,045] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.2, 84.0, 1.0, 2.0, 0.383977070278115, 1.0, 1.0, 0.383977070278115, 0.0, 2.0, 0.0, 6.9112, 6.9112, 579638.6243956886, 579638.6243956886, 113481.5618158394], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7549200.0000, 
sim time next is 7549800.0000, 
raw observation next is [22.56666666666667, 82.66666666666667, 1.0, 2.0, 0.4487968112479958, 1.0, 2.0, 0.4487968112479958, 0.0, 2.0, 0.0, 6.9112, 6.9112, 675046.5525736522, 675046.5525736522, 126826.9672914074], 
processed observation next is [0.0, 0.391304347826087, 0.6621212121212122, 0.8266666666666667, 1.0, 1.0, 0.4487968112479958, 1.0, 1.0, 0.4487968112479958, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2812693969056884, 0.2812693969056884, 0.3170674182285185], 
reward next is 0.7604, 
noisyNet noise sample is [array([-0.3870632], dtype=float32), 0.5614585]. 
=============================================
[2019-03-08 09:46:43,498] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0000000e+00 1.3404173e-36 1.1269410e-30 5.4317715e-36 2.8881423e-34], sum to 1.0000
[2019-03-08 09:46:43,504] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8281
[2019-03-08 09:46:43,518] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.31666666666667, 64.16666666666666, 1.0, 1.0, 0.4500508906255968, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8610428292136376, 6.911199999999999, 6.9112, 700156.1696103542, 700156.1696103544, 150971.4344919726], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7725000.0000, 
sim time next is 7725600.0000, 
raw observation next is [20.5, 63.0, 1.0, 2.0, 0.5896067855353208, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 460591.8171067578, 460591.8171067578, 80821.31346447906], 
processed observation next is [1.0, 0.43478260869565216, 0.5681818181818182, 0.63, 1.0, 1.0, 0.5896067855353208, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.19191325712781573, 0.19191325712781573, 0.20205328366119762], 
reward next is 0.8141, 
noisyNet noise sample is [array([0.28387704], dtype=float32), 0.18527189]. 
=============================================
[2019-03-08 09:46:47,298] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207792: loss 0.0100
[2019-03-08 09:46:47,304] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207792: learning rate 0.0010
[2019-03-08 09:46:47,304] A3C_AGENT_WORKER-Thread-8 INFO:Local step 13000, global step 207793: loss 0.0015
[2019-03-08 09:46:47,309] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 13000, global step 207794: learning rate 0.0010
[2019-03-08 09:46:47,327] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 207800: loss 0.0068
[2019-03-08 09:46:47,328] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 207800: learning rate 0.0010
[2019-03-08 09:46:47,407] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 207845: loss 0.0015
[2019-03-08 09:46:47,410] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 207846: learning rate 0.0010
[2019-03-08 09:46:47,579] A3C_AGENT_WORKER-Thread-4 INFO:Local step 13000, global step 207922: loss 0.0044
[2019-03-08 09:46:47,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 13000, global step 207923: learning rate 0.0010
[2019-03-08 09:46:47,609] A3C_AGENT_WORKER-Thread-6 INFO:Local step 13000, global step 207936: loss 0.0134
[2019-03-08 09:46:47,610] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 13000, global step 207936: learning rate 0.0010
[2019-03-08 09:46:47,677] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 207965: loss 0.0095
[2019-03-08 09:46:47,684] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 207968: learning rate 0.0010
[2019-03-08 09:46:47,691] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 207973: loss 0.0182
[2019-03-08 09:46:47,693] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 207974: learning rate 0.0010
[2019-03-08 09:46:47,699] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207977: loss 0.0169
[2019-03-08 09:46:47,700] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207977: learning rate 0.0010
[2019-03-08 09:46:47,775] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208010: loss 0.0185
[2019-03-08 09:46:47,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208010: learning rate 0.0010
[2019-03-08 09:46:47,798] A3C_AGENT_WORKER-Thread-5 INFO:Local step 13000, global step 208019: loss 0.0034
[2019-03-08 09:46:47,801] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 13000, global step 208019: learning rate 0.0010
[2019-03-08 09:46:47,887] A3C_AGENT_WORKER-Thread-7 INFO:Local step 13000, global step 208065: loss 0.0026
[2019-03-08 09:46:47,892] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 13000, global step 208066: learning rate 0.0010
[2019-03-08 09:46:47,899] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208070: loss 0.0044
[2019-03-08 09:46:47,904] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208071: learning rate 0.0010
[2019-03-08 09:46:48,030] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 208134: loss 0.0169
[2019-03-08 09:46:48,034] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 208134: learning rate 0.0010
[2019-03-08 09:46:48,074] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208149: loss 0.0069
[2019-03-08 09:46:48,076] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208150: learning rate 0.0010
[2019-03-08 09:46:48,236] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 208224: loss 0.0237
[2019-03-08 09:46:48,238] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 208224: learning rate 0.0010
[2019-03-08 09:46:56,991] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:56,992] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:56,993] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-03-08 09:46:57,012] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,012] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,015] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-03-08 09:46:57,030] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,031] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,034] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-03-08 09:46:57,069] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,069] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,071] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-03-08 09:46:57,245] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,246] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,247] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-03-08 09:46:57,265] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,266] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,269] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-03-08 09:46:57,288] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,288] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,289] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,290] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,292] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-03-08 09:46:57,308] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,309] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,310] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-03-08 09:46:57,323] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,323] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,324] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-03-08 09:46:57,340] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,341] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,345] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-03-08 09:46:57,345] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-03-08 09:46:57,392] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,393] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,393] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,394] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,394] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,394] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,395] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,394] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:46:57,394] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,395] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:46:57,398] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-03-08 09:46:57,428] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-03-08 09:46:57,462] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-03-08 09:46:57,479] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-03-08 09:46:57,501] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-03-08 09:46:59,401] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 09:46:59,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3158
[2019-03-08 09:46:59,417] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.16666666666667, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8532450320054515, 6.911199999999999, 6.9112, 346922.0617407624, 346922.0617407625, 94025.39518255962], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 22200.0000, 
sim time next is 22800.0000, 
raw observation next is [17.33333333333334, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8594708406942884, 6.911200000000001, 6.9112, 349457.0843850409, 349457.0843850407, 94547.65786050943], 
processed observation next is [1.0, 0.2608695652173913, 0.42424242424242453, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8594708406942884, 8.881784197001253e-17, 0.0, 0.14560711849376703, 0.14560711849376695, 0.23636914465127357], 
reward next is 0.5280, 
noisyNet noise sample is [array([1.3577366], dtype=float32), 0.46998283]. 
=============================================
[2019-03-08 09:46:59,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 09:46:59,630] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2580
[2019-03-08 09:46:59,638] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.83333333333333, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9377855052116493, 6.911200000000001, 6.9112, 381349.7687074026, 381349.7687074025, 101146.183255954], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 24600.0000, 
sim time next is 25200.0000, 
raw observation next is [18.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 9.910638550665794, 6.9112, 820066.6280714953, 388254.2243955427, 106652.455513204], 
processed observation next is [1.0, 0.30434782608695654, 0.45454545454545453, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 0.29994385506657945, 0.0, 0.3416944283631231, 0.1617725934981428, 0.26663113878301], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.62500376], dtype=float32), -0.23604158]. 
=============================================
[2019-03-08 09:47:10,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.4247044e-34 1.1025229e-26 8.8564543e-19 8.6625733e-30], sum to 1.0000
[2019-03-08 09:47:10,972] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7299
[2019-03-08 09:47:10,980] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.5, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7120216981164046, 6.9112, 6.9112, 289433.0804544789, 289433.0804544789, 82278.02798987442], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 214200.0000, 
sim time next is 214800.0000, 
raw observation next is [20.33333333333333, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6993600633853777, 6.9112, 6.9112, 284280.1374726148, 284280.1374726148, 81233.77761691346], 
processed observation next is [0.0, 0.4782608695652174, 0.5606060606060604, 0.64, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6993600633853777, 0.0, 0.0, 0.11845005728025618, 0.11845005728025618, 0.20308444404228365], 
reward next is 0.4999, 
noisyNet noise sample is [array([1.6285503], dtype=float32), -1.0869365]. 
=============================================
[2019-03-08 09:47:14,976] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.00000000e+00 7.35574314e-34 9.95489017e-25 1.13480666e-20
 3.87910335e-34], sum to 1.0000
[2019-03-08 09:47:14,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2332
[2019-03-08 09:47:14,994] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [15.16666666666667, 92.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5146874691376826, 6.9112, 6.9112, 209147.6426677692, 209147.6426677692, 60377.48150262558], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 288600.0000, 
sim time next is 289200.0000, 
raw observation next is [15.33333333333334, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5148544671210344, 6.911199999999999, 6.9112, 209215.5722341624, 209215.5722341624, 60280.95097009766], 
processed observation next is [0.0, 0.34782608695652173, 0.3333333333333336, 0.9, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5148544671210344, -8.881784197001253e-17, 0.0, 0.08717315509756766, 0.08717315509756766, 0.15070237742524417], 
reward next is 0.4958, 
noisyNet noise sample is [array([-0.02877102], dtype=float32), -0.2102014]. 
=============================================
[2019-03-08 09:47:15,067] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 6.0729606e-34 3.0601693e-28 5.5995359e-36], sum to 1.0000
[2019-03-08 09:47:15,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2133
[2019-03-08 09:47:15,083] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.66666666666667, 66.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5249413911197054, 6.911199999999999, 6.9112, 213318.7006166223, 213318.7006166223, 62313.59441535528], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 298200.0000, 
sim time next is 298800.0000, 
raw observation next is [19.0, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5260396736114349, 6.9112, 6.9112, 213765.4664503252, 213765.4664503252, 62474.12749090393], 
processed observation next is [0.0, 0.4782608695652174, 0.5, 0.64, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5260396736114349, 0.0, 0.0, 0.08906894435430217, 0.08906894435430217, 0.15618531872725983], 
reward next is 0.4888, 
noisyNet noise sample is [array([-0.12281888], dtype=float32), -0.38943976]. 
=============================================
[2019-03-08 09:47:24,029] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 4.438131e-32 5.634522e-30 0.000000e+00], sum to 1.0000
[2019-03-08 09:47:24,039] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7543
[2019-03-08 09:47:24,049] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [13.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4078925612563318, 6.9112, 6.9112, 165715.9247906748, 165715.9247906748, 47831.9870611571], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 444600.0000, 
sim time next is 445200.0000, 
raw observation next is [13.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4067890298266285, 6.911199999999999, 6.9112, 165267.231849991, 165267.231849991, 47731.16032538719], 
processed observation next is [1.0, 0.13043478260869565, 0.22727272727272727, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4067890298266285, -8.881784197001253e-17, 0.0, 0.06886134660416292, 0.06886134660416292, 0.11932790081346797], 
reward next is 0.4946, 
noisyNet noise sample is [array([0.7706786], dtype=float32), 2.1373992]. 
=============================================
[2019-03-08 09:47:27,003] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0000000e+00 2.6942254e-32 7.5947106e-27 1.3482237e-19 3.2882724e-35], sum to 1.0000
[2019-03-08 09:47:27,015] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6254
[2019-03-08 09:47:27,024] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6169817704288939, 6.9112, 6.9112, 250740.9462719174, 250740.9462719174, 70925.32623067325], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 496800.0000, 
sim time next is 497400.0000, 
raw observation next is [15.83333333333333, 89.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.614745659211944, 6.9112, 6.9112, 249831.3211189098, 249831.3211189098, 70563.38922075328], 
processed observation next is [1.0, 0.782608695652174, 0.3560606060606059, 0.8900000000000001, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.614745659211944, 0.0, 0.0, 0.10409638379954575, 0.10409638379954575, 0.1764084730518832], 
reward next is 0.5058, 
noisyNet noise sample is [array([-0.28463575], dtype=float32), -0.93610007]. 
=============================================
[2019-03-08 09:47:30,690] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.0533826e-36 1.5434140e-25 1.2210925e-21 0.0000000e+00], sum to 1.0000
[2019-03-08 09:47:30,701] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5182
[2019-03-08 09:47:30,707] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8665005305176976, 6.9112, 6.9112, 352319.4950413223, 352319.4950413223, 95135.83754461976], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 554400.0000, 
sim time next is 555000.0000, 
raw observation next is [17.16666666666667, 87.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 7.164835510568011, 6.9112, 406904.2003094293, 388235.5295316086, 102763.5596228698], 
processed observation next is [1.0, 0.43478260869565216, 0.4166666666666669, 0.8716666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 0.025363551056801104, 0.0, 0.16954341679559554, 0.16176480397150358, 0.2569088990571745], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9491624], dtype=float32), 1.9202517]. 
=============================================
[2019-03-08 09:47:30,714] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[57.32525 ]
 [57.18167 ]
 [57.151253]
 [57.10096 ]
 [57.00078 ]], R is [[56.71488953]
 [56.67678833]
 [56.64012146]
 [56.61210251]
 [56.59017181]].
[2019-03-08 09:47:33,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 1.8221938e-36 8.3774470e-27 1.9333740e-24 4.3523141e-36], sum to 1.0000
[2019-03-08 09:47:33,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8027
[2019-03-08 09:47:33,798] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.5, 82.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6831828468547493, 6.9112, 6.9112, 277673.6846485384, 277673.6846485384, 79616.54320402262], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 606600.0000, 
sim time next is 607200.0000, 
raw observation next is [17.33333333333333, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6677240935531744, 6.9112, 6.9112, 271384.0657173157, 271384.0657173157, 77587.13649745057], 
processed observation next is [1.0, 0.0, 0.42424242424242403, 0.8233333333333335, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6677240935531744, 0.0, 0.0, 0.11307669404888154, 0.11307669404888154, 0.19396784124362643], 
reward next is 0.4997, 
noisyNet noise sample is [array([-0.7633123], dtype=float32), -0.3992805]. 
=============================================
[2019-03-08 09:47:37,421] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.9544003e-34 3.4101710e-27 6.8644400e-28 8.1492391e-35], sum to 1.0000
[2019-03-08 09:47:37,430] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4350
[2019-03-08 09:47:37,438] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 60.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8529817305792153, 6.9112, 6.9112, 346778.8294509309, 346778.8294509309, 94636.57256499439], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 682200.0000, 
sim time next is 682800.0000, 
raw observation next is [22.33333333333334, 61.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8588908711182279, 6.911200000000001, 6.9112, 349184.4056150665, 349184.4056150664, 95139.97727955118], 
processed observation next is [1.0, 0.9130434782608695, 0.6515151515151518, 0.6166666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8588908711182279, 8.881784197001253e-17, 0.0, 0.14549350233961106, 0.145493502339611, 0.23784994319887795], 
reward next is 0.5243, 
noisyNet noise sample is [array([2.1399488], dtype=float32), 0.8515066]. 
=============================================
[2019-03-08 09:47:41,893] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9999642e-01 4.2009259e-26 1.4239541e-19 3.6315328e-06 1.6279022e-25], sum to 1.0000
[2019-03-08 09:47:41,900] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1975
[2019-03-08 09:47:41,909] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.5, 71.0, 1.0, 2.0, 0.305310078671666, 1.0, 2.0, 0.305310078671666, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 463091.5494920358, 463091.5494920359, 99579.78626961366], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 775800.0000, 
sim time next is 776400.0000, 
raw observation next is [23.33333333333333, 71.66666666666667, 1.0, 2.0, 0.3034675685245759, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5664200788896445, 6.911200000000001, 6.9112, 460348.9664222174, 460348.9664222173, 116662.2280305175], 
processed observation next is [1.0, 1.0, 0.6969696969696968, 0.7166666666666667, 1.0, 1.0, 0.3034675685245759, 0.0, 0.5, 0.0, 1.0, 0.5, 0.5664200788896445, 8.881784197001253e-17, 0.0, 0.19181206934259057, 0.19181206934259054, 0.2916555700762937], 
reward next is 0.5637, 
noisyNet noise sample is [array([0.29662758], dtype=float32), -0.6145559]. 
=============================================
[2019-03-08 09:47:44,948] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9997008e-01 1.2575827e-15 7.6001094e-11 2.9905890e-05 1.4706369e-14], sum to 1.0000
[2019-03-08 09:47:44,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7940
[2019-03-08 09:47:44,967] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333333, 82.33333333333334, 1.0, 2.0, 0.4625793870490669, 1.0, 2.0, 0.4625793870490669, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 681506.4074880795, 681506.4074880794, 131856.6394704536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 810600.0000, 
sim time next is 811200.0000, 
raw observation next is [24.66666666666666, 81.66666666666667, 1.0, 2.0, 0.8071938037475856, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 598932.5071561266, 598932.5071561266, 128939.9787786132], 
processed observation next is [0.0, 0.391304347826087, 0.7575757575757573, 0.8166666666666668, 1.0, 1.0, 0.8071938037475856, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.24955521131505273, 0.24955521131505273, 0.32234994694653296], 
reward next is 0.6636, 
noisyNet noise sample is [array([-1.2033684], dtype=float32), 2.4544225]. 
=============================================
[2019-03-08 09:47:49,509] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 5.9467288e-29 1.1157517e-21 7.8100436e-11 7.0992354e-31], sum to 1.0000
[2019-03-08 09:47:49,518] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0612
[2019-03-08 09:47:49,533] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.0, 88.0, 1.0, 2.0, 0.1959972594981351, 1.0, 2.0, 0.1959972594981351, 1.0, 2.0, 0.3644398727037546, 6.9112, 6.9112, 444133.2245999508, 444133.2245999508, 128760.5377424032], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 892200.0000, 
sim time next is 892800.0000, 
raw observation next is [21.0, 88.0, 1.0, 2.0, 0.2710427019929121, 1.0, 2.0, 0.2710427019929121, 0.0, 1.0, 0.0, 6.9112, 6.9112, 411578.6421561681, 411578.6421561681, 94401.03236324455], 
processed observation next is [0.0, 0.34782608695652173, 0.5909090909090909, 0.88, 1.0, 1.0, 0.2710427019929121, 1.0, 1.0, 0.2710427019929121, 0.0, 0.5, 0.0, 0.0, 0.0, 0.1714911008984034, 0.1714911008984034, 0.23600258090811138], 
reward next is 0.6228, 
noisyNet noise sample is [array([0.902271], dtype=float32), 1.9598361]. 
=============================================
[2019-03-08 09:47:51,084] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.8964429e-01 6.8552748e-16 1.0341019e-12 1.0355678e-02 1.4800547e-15], sum to 1.0000
[2019-03-08 09:47:51,087] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0449
[2019-03-08 09:47:51,099] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 75.33333333333333, 1.0, 2.0, 0.31938655209896, 1.0, 1.0, 0.31938655209896, 0.0, 2.0, 0.0, 6.9112, 6.9112, 481293.1848752278, 481293.1848752278, 102388.5349056766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 913200.0000, 
sim time next is 913800.0000, 
raw observation next is [23.83333333333333, 74.66666666666667, 1.0, 2.0, 0.651643062814429, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 493283.9155264084, 493283.9155264084, 100528.4665144919], 
processed observation next is [0.0, 0.5652173913043478, 0.7196969696969695, 0.7466666666666667, 1.0, 1.0, 0.651643062814429, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.20553496480267017, 0.20553496480267017, 0.25132116628622975], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.1029823], dtype=float32), -0.73511225]. 
=============================================
[2019-03-08 09:47:51,646] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0000000e+00 9.1301550e-29 6.7468600e-29 2.3250402e-10 9.0092826e-34], sum to 1.0000
[2019-03-08 09:47:51,654] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9582
[2019-03-08 09:47:51,662] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 78.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 1.0, 2.0, 0.3027619139486065, 6.9112, 6.9112, 368925.5666255929, 368925.5666255929, 116314.6851101869], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 918600.0000, 
sim time next is 919200.0000, 
raw observation next is [22.0, 78.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7477810041830779, 6.9112, 6.9112, 303959.672097073, 303959.672097073, 85741.00975482102], 
processed observation next is [0.0, 0.6521739130434783, 0.6363636363636364, 0.78, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.7477810041830779, 0.0, 0.0, 0.12664986337378042, 0.12664986337378042, 0.21435252438705255], 
reward next is 0.5064, 
noisyNet noise sample is [array([-0.7487291], dtype=float32), 0.788286]. 
=============================================
[2019-03-08 09:47:52,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 3.9894831e-33 1.0062064e-23 2.6124163e-19 5.4723382e-32], sum to 1.0000
[2019-03-08 09:47:52,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3178
[2019-03-08 09:47:52,350] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.0, 94.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9547116395713053, 7.600305736243961, 6.9112, 439748.8915889883, 388275.4597642067, 102182.4667126931], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 934200.0000, 
sim time next is 934800.0000, 
raw observation next is [19.66666666666667, 96.0, 1.0, 1.0, 0.3569366067503484, 1.0, 1.0, 0.3569366067503484, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 544048.7091804276, 544048.7091804277, 107552.7706319948], 
processed observation next is [0.0, 0.8260869565217391, 0.5303030303030305, 0.96, 1.0, 0.5, 0.3569366067503484, 1.0, 0.5, 0.3569366067503484, 0.0, 0.5, 0.0, -8.881784197001253e-17, 0.0, 0.2266869621585115, 0.22668696215851156, 0.268881926579987], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2089722], dtype=float32), 0.5708564]. 
=============================================
[2019-03-08 09:47:52,418] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.75289464e-01 1.52779323e-23 1.92223209e-18 5.24710536e-01
 1.06874244e-23], sum to 1.0000
[2019-03-08 09:47:52,425] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0158
[2019-03-08 09:47:52,436] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [19.0, 99.00000000000001, 1.0, 2.0, 0.2679703733305757, 1.0, 2.0, 0.2679703733305757, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 409802.7103414045, 409802.7103414044, 93307.7821274685], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 951000.0000, 
sim time next is 951600.0000, 
raw observation next is [19.0, 98.0, 1.0, 2.0, 0.2635513097110041, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4962476737306949, 6.911199999999999, 6.9112, 403279.6057049151, 403279.6057049152, 106616.2731203001], 
processed observation next is [1.0, 0.0, 0.5, 0.98, 1.0, 1.0, 0.2635513097110041, 0.0, 0.5, 0.0, 1.0, 0.5, 0.4962476737306949, -8.881784197001253e-17, 0.0, 0.16803316904371463, 0.16803316904371465, 0.2665406828007503], 
reward next is 0.5404, 
noisyNet noise sample is [array([0.2856333], dtype=float32), -0.5447704]. 
=============================================
[2019-03-08 09:47:53,130] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.7676342e-01 4.0003547e-24 4.4601041e-21 3.2323653e-01 1.6401622e-27], sum to 1.0000
[2019-03-08 09:47:53,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7494
[2019-03-08 09:47:53,151] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.0, 95.0, 1.0, 2.0, 0.2588788836475909, 1.0, 1.0, 0.2588788836475909, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 397284.2652750605, 397284.2652750604, 91703.56763093769], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 953400.0000, 
sim time next is 954000.0000, 
raw observation next is [19.0, 94.0, 1.0, 2.0, 0.2530953669863013, 1.0, 2.0, 0.2530953669863013, 0.0, 2.0, 0.0, 6.9112, 6.9112, 388808.4059134796, 388808.4059134796, 90822.8582294215], 
processed observation next is [1.0, 0.043478260869565216, 0.5, 0.94, 1.0, 1.0, 0.2530953669863013, 1.0, 1.0, 0.2530953669863013, 0.0, 1.0, 0.0, 0.0, 0.0, 0.16200350246394984, 0.16200350246394984, 0.22705714557355375], 
reward next is 0.6116, 
noisyNet noise sample is [array([2.4458444], dtype=float32), 0.49354044]. 
=============================================
[2019-03-08 09:47:53,157] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[49.553726]
 [49.12322 ]
 [48.70624 ]
 [50.086338]
 [49.314068]], R is [[49.44573975]
 [49.57017899]
 [49.61251831]
 [49.11639404]
 [48.62522888]].
[2019-03-08 09:47:57,751] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 4.9790372e-29 4.9156650e-21 6.1238339e-09 8.7401493e-29], sum to 1.0000
[2019-03-08 09:47:57,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5909
[2019-03-08 09:47:57,767] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [13.0, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4697615058401629, 6.9112, 6.9112, 190874.7805064487, 190874.7805064487, 53038.55315985542], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1027800.0000, 
sim time next is 1028400.0000, 
raw observation next is [13.0, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4599049873634178, 6.9112, 6.9112, 186866.2422606278, 186866.2422606278, 52024.16774507926], 
processed observation next is [1.0, 0.9130434782608695, 0.22727272727272727, 0.96, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4599049873634178, 0.0, 0.0, 0.07786093427526158, 0.07786093427526158, 0.13006041936269816], 
reward next is 0.5131, 
noisyNet noise sample is [array([0.5541739], dtype=float32), 0.49059793]. 
=============================================
[2019-03-08 09:48:02,351] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3489874e-01 2.8149644e-24 5.5759247e-19 8.6510122e-01 1.4391973e-24], sum to 1.0000
[2019-03-08 09:48:02,362] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4945
[2019-03-08 09:48:02,368] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.0, 69.0, 1.0, 2.0, 0.2457135328467387, 1.0, 2.0, 0.2457135328467387, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 378370.3859833414, 378370.3859833413, 89595.69748045898], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1105200.0000, 
sim time next is 1105800.0000, 
raw observation next is [21.83333333333334, 69.66666666666667, 1.0, 2.0, 0.2448857274317458, 1.0, 2.0, 0.2448857274317458, 0.0, 2.0, 0.0, 6.9112, 6.9112, 377216.6309300001, 377216.6309300001, 89452.64626146831], 
processed observation next is [1.0, 0.8260869565217391, 0.628787878787879, 0.6966666666666668, 1.0, 1.0, 0.2448857274317458, 1.0, 1.0, 0.2448857274317458, 0.0, 1.0, 0.0, 0.0, 0.0, 0.15717359622083338, 0.15717359622083338, 0.2236316156536708], 
reward next is 0.6024, 
noisyNet noise sample is [array([1.5834923], dtype=float32), -0.80844426]. 
=============================================
[2019-03-08 09:48:04,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 8.7678572e-31 8.4943785e-22 3.4392063e-08 1.0925878e-33], sum to 1.0000
[2019-03-08 09:48:04,657] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8262
[2019-03-08 09:48:04,662] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8357186296554853, 6.9112, 6.9112, 339751.3777226421, 339751.3777226421, 93168.2362264762], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1140000.0000, 
sim time next is 1140600.0000, 
raw observation next is [18.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.836123340226331, 6.9112, 6.9112, 339916.1226932983, 339916.1226932983, 93202.62358090648], 
processed observation next is [1.0, 0.17391304347826086, 0.45454545454545453, 0.94, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.836123340226331, 0.0, 0.0, 0.1416317177888743, 0.1416317177888743, 0.2330065589522662], 
reward next is 0.5210, 
noisyNet noise sample is [array([-0.29789126], dtype=float32), 1.2701287]. 
=============================================
[2019-03-08 09:48:09,391] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.5386562e-35 3.3045147e-30 4.7262755e-10 5.4311857e-38], sum to 1.0000
[2019-03-08 09:48:09,402] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0313
[2019-03-08 09:48:09,419] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [19.83333333333334, 95.0, 1.0, 2.0, 0.2854417671583817, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5348543710904942, 6.911199999999999, 6.9112, 434685.3093466265, 434685.3093466266, 111518.0691033716], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1230600.0000, 
sim time next is 1231200.0000, 
raw observation next is [20.0, 94.0, 1.0, 2.0, 0.2837588877974564, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5317146221556008, 6.911199999999999, 6.9112, 432131.0211730556, 432131.0211730557, 111140.3534611748], 
processed observation next is [1.0, 0.2608695652173913, 0.5454545454545454, 0.94, 1.0, 1.0, 0.2837588877974564, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5317146221556008, -8.881784197001253e-17, 0.0, 0.1800545921554398, 0.18005459215543987, 0.27785088365293703], 
reward next is 0.5555, 
noisyNet noise sample is [array([-1.3309132], dtype=float32), 1.4141338]. 
=============================================
[2019-03-08 09:48:15,426] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.2129704e-35 5.8937799e-33 2.5288867e-17 3.4711174e-37], sum to 1.0000
[2019-03-08 09:48:15,439] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9029
[2019-03-08 09:48:15,444] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9318125982346546, 6.9112, 6.9112, 378874.0685836957, 378874.0685836957, 101384.1800295195], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1314000.0000, 
sim time next is 1314600.0000, 
raw observation next is [18.16666666666667, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9356355310782947, 6.911199999999999, 6.9112, 380430.7405685271, 380430.7405685271, 101713.2833058797], 
processed observation next is [1.0, 0.21739130434782608, 0.4621212121212123, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9356355310782947, -8.881784197001253e-17, 0.0, 0.15851280857021963, 0.15851280857021963, 0.25428320826469925], 
reward next is 0.5343, 
noisyNet noise sample is [array([0.85295725], dtype=float32), 1.198348]. 
=============================================
[2019-03-08 09:48:15,789] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7248983e-03 3.2250482e-28 9.7569354e-25 9.9727505e-01 3.1259432e-28], sum to 1.0000
[2019-03-08 09:48:15,795] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4459
[2019-03-08 09:48:15,803] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.5, 85.83333333333334, 1.0, 2.0, 0.840626545918165, 1.0, 2.0, 0.840626545918165, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1224919.545269862, 1224919.545269863, 261271.8627506749], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1349400.0000, 
sim time next is 1350000.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.9082437183301182, 1.0, 2.0, 0.9082437183301182, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1323572.816161053, 1323572.816161053, 289128.4264285017], 
processed observation next is [1.0, 0.6521739130434783, 0.7727272727272727, 0.89, 1.0, 1.0, 0.9082437183301182, 1.0, 1.0, 0.9082437183301182, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.5514886734004387, 0.5514886734004387, 0.7228210660712543], 
reward next is 0.6540, 
noisyNet noise sample is [array([1.0865208], dtype=float32), 0.68897825]. 
=============================================
[2019-03-08 09:48:15,819] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.37722 ]
 [71.302925]
 [70.87527 ]
 [72.66717 ]
 [72.469734]], R is [[71.37828064]
 [71.3342514 ]
 [71.28546143]
 [71.22000122]
 [71.18318939]].
[2019-03-08 09:48:19,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.1800632e-23 9.2836755e-20 6.3328647e-15 1.3973963e-21], sum to 1.0000
[2019-03-08 09:48:19,564] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7316
[2019-03-08 09:48:19,581] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3340022848437673, 1.0, 2.0, 0.3340022848437673, 0.0, 1.0, 0.0, 6.9112, 6.9112, 498939.892599548, 498939.892599548, 105511.6083561925], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1395000.0000, 
sim time next is 1395600.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.2539994678293862, 1.0, 2.0, 0.2539994678293862, 1.0, 1.0, 0.4636097898007977, 6.9112, 6.9112, 565093.3393381034, 565093.3393381034, 145116.5956430672], 
processed observation next is [0.0, 0.13043478260869565, 0.5909090909090909, 1.0, 1.0, 1.0, 0.2539994678293862, 1.0, 1.0, 0.2539994678293862, 1.0, 0.5, 0.4636097898007977, 0.0, 0.0, 0.2354555580575431, 0.2354555580575431, 0.362791489107668], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6918335], dtype=float32), -1.9385151]. 
=============================================
[2019-03-08 09:48:26,520] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 2.6764451e-30 2.0266755e-25 1.4165291e-22 8.8347663e-32], sum to 1.0000
[2019-03-08 09:48:26,527] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0601
[2019-03-08 09:48:26,538] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4289547506705355, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7689736159658102, 6.911200000000001, 6.9112, 625168.8574081712, 625168.857408171, 146535.5937846267], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [23.66666666666667, 88.66666666666666, 1.0, 2.0, 0.4092133790800126, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7389050991074722, 6.911200000000001, 6.9112, 600695.2298836287, 600695.2298836285, 141717.2605149039], 
processed observation next is [0.0, 0.5652173913043478, 0.7121212121212124, 0.8866666666666666, 1.0, 1.0, 0.4092133790800126, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7389050991074722, 8.881784197001253e-17, 0.0, 0.2502896791181786, 0.25028967911817857, 0.35429315128725974], 
reward next is 0.6055, 
noisyNet noise sample is [array([0.23021445], dtype=float32), 0.57285357]. 
=============================================
[2019-03-08 09:48:26,552] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[48.082134]
 [48.326366]
 [48.258633]
 [48.154846]
 [46.99849 ]], R is [[47.93045807]
 [48.06062698]
 [48.18959045]
 [48.30993271]
 [47.82683563]].
[2019-03-08 09:48:30,458] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.22947389e-19 2.99041813e-30 1.30651655e-30 1.00000000e+00
 1.30454182e-34], sum to 1.0000
[2019-03-08 09:48:30,469] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4731
[2019-03-08 09:48:30,477] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.86666666666667, 62.33333333333333, 1.0, 2.0, 0.5846767269895019, 1.0, 2.0, 0.5846767269895019, 0.0, 2.0, 0.0, 6.9112, 6.9112, 866237.0592713162, 866237.0592713162, 161468.3151652093], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1604400.0000, 
sim time next is 1605000.0000, 
raw observation next is [26.93333333333333, 62.16666666666667, 1.0, 2.0, 0.6067813281881018, 1.0, 2.0, 0.6067813281881018, 0.0, 2.0, 0.0, 6.9112, 6.9112, 898185.2072115813, 898185.2072115813, 167668.1032993398], 
processed observation next is [1.0, 0.5652173913043478, 0.8606060606060605, 0.6216666666666667, 1.0, 1.0, 0.6067813281881018, 1.0, 1.0, 0.6067813281881018, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3742438363381589, 0.3742438363381589, 0.41917025824834947], 
reward next is 0.7653, 
noisyNet noise sample is [array([2.4622118], dtype=float32), -0.353519]. 
=============================================
[2019-03-08 09:48:30,493] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[56.104416]
 [55.976433]
 [55.943954]
 [55.977554]
 [55.995266]], R is [[56.39149475]
 [56.59397125]
 [56.79331589]
 [56.99068832]
 [57.18028259]].
[2019-03-08 09:48:35,076] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4696234e-07 4.9749216e-27 3.9477687e-21 9.9999988e-01 4.5403688e-22], sum to 1.0000
[2019-03-08 09:48:35,090] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1783
[2019-03-08 09:48:35,096] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.5, 88.5, 1.0, 2.0, 0.3101209982645836, 1.0, 2.0, 0.3101209982645836, 0.0, 2.0, 0.0, 6.9112, 6.9112, 476433.7323578797, 476433.7323578797, 98947.34053912756], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1668600.0000, 
sim time next is 1669200.0000, 
raw observation next is [19.66666666666666, 86.66666666666666, 1.0, 2.0, 0.3163940158674132, 1.0, 2.0, 0.3163940158674132, 0.0, 2.0, 0.0, 6.9112, 6.9112, 486229.4015241114, 486229.4015241114, 99874.89880530549], 
processed observation next is [1.0, 0.30434782608695654, 0.53030303030303, 0.8666666666666666, 1.0, 1.0, 0.3163940158674132, 1.0, 1.0, 0.3163940158674132, 0.0, 1.0, 0.0, 0.0, 0.0, 0.20259558396837976, 0.20259558396837976, 0.24968724701326372], 
reward next is 0.6955, 
noisyNet noise sample is [array([-2.1095626], dtype=float32), 0.4250844]. 
=============================================
[2019-03-08 09:48:47,857] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 4.6290864e-31 6.4970603e-29 9.7293614e-19 7.2261464e-32], sum to 1.0000
[2019-03-08 09:48:47,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3004
[2019-03-08 09:48:47,878] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 44.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7214348788237154, 6.9112, 6.9112, 293238.3864527613, 293238.3864527613, 81499.09491045945], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [23.0, 44.50000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7201208402094574, 6.911200000000001, 6.9112, 292703.6750119854, 292703.6750119854, 81655.90880865896], 
processed observation next is [1.0, 0.7391304347826086, 0.6818181818181818, 0.44500000000000006, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7201208402094574, 8.881784197001253e-17, 0.0, 0.12195986458832725, 0.12195986458832725, 0.2041397720216474], 
reward next is 0.5121, 
noisyNet noise sample is [array([-1.7651873], dtype=float32), -0.669799]. 
=============================================
[2019-03-08 09:48:50,112] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.9626768e-14 0.0000000e+00 1.1910436e-33 1.0000000e+00 4.0654296e-37], sum to 1.0000
[2019-03-08 09:48:50,124] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1515
[2019-03-08 09:48:50,128] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.66666666666666, 69.66666666666667, 1.0, 2.0, 0.7473537475698733, 1.0, 2.0, 0.7473537475698733, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 1114768.475769842, 1114768.475769842, 222613.6216828217], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1945200.0000, 
sim time next is 1945800.0000, 
raw observation next is [25.0, 67.5, 1.0, 2.0, 0.7605485009182253, 1.0, 2.0, 0.7605485009182253, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1133077.604233171, 1133077.604233171, 227534.6368619363], 
processed observation next is [1.0, 0.5217391304347826, 0.7727272727272727, 0.675, 1.0, 1.0, 0.7605485009182253, 1.0, 1.0, 0.7605485009182253, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.47211566843048797, 0.47211566843048797, 0.5688365921548407], 
reward next is 0.7114, 
noisyNet noise sample is [array([0.49785954], dtype=float32), 0.30861253]. 
=============================================
[2019-03-08 09:49:00,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 2.2788546e-38 7.9374165e-35 3.7042215e-26 0.0000000e+00], sum to 1.0000
[2019-03-08 09:49:00,640] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9319
[2019-03-08 09:49:00,644] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.0, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4838517564778458, 6.911199999999999, 6.9112, 196605.4005797205, 196605.4005797205, 55727.52353308034], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2098800.0000, 
sim time next is 2099400.0000, 
raw observation next is [16.5, 75.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4885672041054847, 6.911200000000001, 6.9112, 198523.2824378129, 198523.2824378129, 56756.34198808565], 
processed observation next is [0.0, 0.30434782608695654, 0.38636363636363635, 0.755, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4885672041054847, 8.881784197001253e-17, 0.0, 0.0827180343490887, 0.0827180343490887, 0.14189085497021414], 
reward next is 0.4997, 
noisyNet noise sample is [array([-1.2576703], dtype=float32), -0.8837106]. 
=============================================
[2019-03-08 09:49:06,774] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9998546e-01 1.8866991e-12 7.9651048e-15 1.4516347e-05 6.3596968e-14], sum to 1.0000
[2019-03-08 09:49:06,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9545
[2019-03-08 09:49:06,788] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.83333333333333, 73.66666666666667, 1.0, 2.0, 0.2349004948280328, 1.0, 2.0, 0.2349004948280328, 0.0, 1.0, 0.0, 6.9112, 6.9112, 365994.0175710432, 365994.0175710432, 86575.43857302908], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2199000.0000, 
sim time next is 2199600.0000, 
raw observation next is [19.0, 73.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8446627810187255, 6.911199999999999, 6.9112, 343392.3096091868, 343392.3096091868, 93926.98219525328], 
processed observation next is [1.0, 0.4782608695652174, 0.5, 0.73, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.8446627810187255, -8.881784197001253e-17, 0.0, 0.14308012900382783, 0.14308012900382783, 0.2348174554881332], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11245043], dtype=float32), -0.33733058]. 
=============================================
[2019-03-08 09:49:08,686] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.2043715e-32 3.6866898e-31 2.4054933e-21 1.9256965e-32], sum to 1.0000
[2019-03-08 09:49:08,696] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2970
[2019-03-08 09:49:08,703] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4861245682731488, 6.9112, 6.9112, 197529.801391313, 197529.801391313, 55662.77530405683], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2257200.0000, 
sim time next is 2257800.0000, 
raw observation next is [14.0, 92.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4802129645956325, 6.9112, 6.9112, 195125.4438345018, 195125.4438345018, 54784.19294766512], 
processed observation next is [1.0, 0.13043478260869565, 0.2727272727272727, 0.9200000000000002, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4802129645956325, 0.0, 0.0, 0.08130226826437575, 0.08130226826437575, 0.1369604823691628], 
reward next is 0.5088, 
noisyNet noise sample is [array([-1.5536511], dtype=float32), 0.7400636]. 
=============================================
[2019-03-08 09:49:09,567] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 2.7959318e-27 3.4580428e-24 4.1249785e-15 1.9199371e-26], sum to 1.0000
[2019-03-08 09:49:09,578] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7037
[2019-03-08 09:49:09,585] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.16666666666667, 58.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7290123014247943, 6.9112, 6.9112, 296348.1419419667, 296348.1419419667, 75863.83261603248], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2279400.0000, 
sim time next is 2280000.0000, 
raw observation next is [17.33333333333334, 58.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.72753933268485, 6.9112, 6.9112, 295748.6378837702, 295748.6378837702, 75835.42738165084], 
processed observation next is [1.0, 0.391304347826087, 0.42424242424242453, 0.58, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.72753933268485, 0.0, 0.0, 0.1232285991182376, 0.1232285991182376, 0.18958856845412708], 
reward next is 0.5571, 
noisyNet noise sample is [array([-0.2217034], dtype=float32), -0.14324817]. 
=============================================
[2019-03-08 09:49:09,606] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[38.3987  ]
 [38.358196]
 [38.30855 ]
 [38.21379 ]
 [38.118004]], R is [[38.61304092]
 [38.78495407]
 [38.95664978]
 [39.12713242]
 [39.29748535]].
[2019-03-08 09:49:13,050] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.00000000e+00 3.82726377e-33 8.63825802e-29 1.31963973e-23
 1.22658986e-32], sum to 1.0000
[2019-03-08 09:49:13,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9132
[2019-03-08 09:49:13,068] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.5, 49.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5922455299440803, 6.911200000000001, 6.9112, 240678.8723537633, 240678.8723537632, 65027.37447133145], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2313000.0000, 
sim time next is 2313600.0000, 
raw observation next is [19.33333333333334, 49.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5887212180888707, 6.9112, 6.9112, 239245.3350987331, 239245.3350987331, 64477.94749616431], 
processed observation next is [1.0, 0.782608695652174, 0.5151515151515155, 0.49, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5887212180888707, 0.0, 0.0, 0.09968555629113879, 0.09968555629113879, 0.16119486874041078], 
reward next is 0.5301, 
noisyNet noise sample is [array([0.1517655], dtype=float32), -1.6821418]. 
=============================================
[2019-03-08 09:49:13,253] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 4.4650091e-26 1.8237079e-23 2.4922781e-10 1.3549453e-29], sum to 1.0000
[2019-03-08 09:49:13,259] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3408
[2019-03-08 09:49:13,263] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [15.5, 67.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4491913985037849, 6.9112, 6.9112, 182509.3187121094, 182509.3187121094, 50278.51065230609], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2334600.0000, 
sim time next is 2335200.0000, 
raw observation next is [15.33333333333333, 67.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.443756467165479, 6.9112, 6.9112, 180299.1508180991, 180299.1508180991, 49598.30368927485], 
processed observation next is [1.0, 0.0, 0.3333333333333332, 0.6733333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.443756467165479, 0.0, 0.0, 0.07512464617420796, 0.07512464617420796, 0.12399575922318712], 
reward next is 0.5193, 
noisyNet noise sample is [array([0.87120986], dtype=float32), 0.045231935]. 
=============================================
[2019-03-08 09:49:14,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.9980408e-34 1.2219931e-34 5.8287773e-20 2.8149361e-36], sum to 1.0000
[2019-03-08 09:49:14,095] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0732
[2019-03-08 09:49:14,100] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [15.33333333333333, 67.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.443756467165479, 6.9112, 6.9112, 180299.1508180991, 180299.1508180991, 49598.30368927485], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2335200.0000, 
sim time next is 2335800.0000, 
raw observation next is [15.16666666666667, 67.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4384339257103176, 6.911200000000001, 6.9112, 178134.733045834, 178134.7330458339, 48937.89247258303], 
processed observation next is [1.0, 0.0, 0.3257575757575759, 0.6716666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4384339257103176, 8.881784197001253e-17, 0.0, 0.07422280543576416, 0.07422280543576414, 0.12234473118145757], 
reward next is 0.5200, 
noisyNet noise sample is [array([0.63455546], dtype=float32), 0.9225395]. 
=============================================
[2019-03-08 09:49:15,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 8.3369039e-25 8.8329064e-26 4.4437058e-09 5.5014519e-25], sum to 1.0000
[2019-03-08 09:49:15,092] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8818
[2019-03-08 09:49:15,107] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 49.0, 1.0, 2.0, 0.2634676083657382, 1.0, 2.0, 0.2634676083657382, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 412294.9322567701, 412294.93225677, 85331.41947811162], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2373000.0000, 
sim time next is 2373600.0000, 
raw observation next is [20.0, 49.0, 1.0, 2.0, 0.3779522086166553, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 295831.0616104309, 295831.0616104308, 49605.96263650108], 
processed observation next is [1.0, 0.4782608695652174, 0.5454545454545454, 0.49, 1.0, 1.0, 0.3779522086166553, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.12326294233767954, 0.12326294233767951, 0.1240149065912527], 
reward next is 0.8519, 
noisyNet noise sample is [array([0.83283705], dtype=float32), -0.48968422]. 
=============================================
[2019-03-08 09:49:27,200] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.3939870e-03 2.2658733e-18 1.9323398e-13 9.9660599e-01 1.2607798e-15], sum to 1.0000
[2019-03-08 09:49:27,210] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4761
[2019-03-08 09:49:27,217] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.66666666666667, 61.33333333333334, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 265364.4857804121, 265364.4857804121, 75391.79968452043], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2578800.0000, 
sim time next is 2579400.0000, 
raw observation next is [19.5, 62.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 267504.6270420383, 267504.6270420383, 75416.73036191946], 
processed observation next is [1.0, 0.8695652173913043, 0.5227272727272727, 0.62, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.11146026126751596, 0.11146026126751596, 0.18854182590479865], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.23171486], dtype=float32), 0.31941372]. 
=============================================
[2019-03-08 09:49:28,260] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9955589e-01 3.2225352e-22 1.9584258e-21 4.4405830e-04 1.1179690e-22], sum to 1.0000
[2019-03-08 09:49:28,268] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9429
[2019-03-08 09:49:28,272] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.66666666666667, 57.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6734094039574602, 6.9112, 6.9112, 273697.1817584389, 273697.1817584389, 77548.24838548606], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2575200.0000, 
sim time next is 2575800.0000, 
raw observation next is [20.5, 58.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6673279371651778, 6.911200000000001, 6.9112, 271222.8877113309, 271222.8877113308, 76845.45056615693], 
processed observation next is [1.0, 0.8260869565217391, 0.5681818181818182, 0.58, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6673279371651778, 8.881784197001253e-17, 0.0, 0.11300953654638787, 0.11300953654638783, 0.1921136264153923], 
reward next is 0.5042, 
noisyNet noise sample is [array([-0.21353997], dtype=float32), 0.03432922]. 
=============================================
[2019-03-08 09:49:32,981] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0000000e+00 1.8960852e-34 9.7787795e-33 3.5605791e-17 0.0000000e+00], sum to 1.0000
[2019-03-08 09:49:32,994] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3344
[2019-03-08 09:49:33,000] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 39.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8760930973164016, 6.911200000000001, 6.9112, 356187.5831826903, 356187.5831826902, 96608.50775673552], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2648400.0000, 
sim time next is 2649000.0000, 
raw observation next is [28.0, 39.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8858513299095495, 6.911200000000001, 6.9112, 360160.4129422518, 360160.4129422517, 97442.5609033844], 
processed observation next is [0.0, 0.6521739130434783, 0.9090909090909091, 0.395, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8858513299095495, 8.881784197001253e-17, 0.0, 0.15006683872593823, 0.1500668387259382, 0.24360640225846097], 
reward next is 0.5280, 
noisyNet noise sample is [array([-0.5866577], dtype=float32), -1.5677384]. 
=============================================
[2019-03-08 09:49:33,011] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[53.655293]
 [53.58655 ]
 [53.517643]
 [53.448456]
 [53.37882 ]], R is [[53.71479797]
 [53.70435333]
 [53.69264221]
 [53.67985916]
 [53.66646194]].
[2019-03-08 09:49:33,686] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0000000e+00 5.4174683e-30 6.8687390e-27 1.4698582e-08 2.0175209e-31], sum to 1.0000
[2019-03-08 09:49:33,696] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9978
[2019-03-08 09:49:33,700] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.16666666666667, 47.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9232992260257058, 6.911200000000001, 6.9112, 375407.5488021025, 375407.5488021025, 100652.6110706332], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2659800.0000, 
sim time next is 2660400.0000, 
raw observation next is [26.0, 48.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9170828110792778, 6.911200000000001, 6.9112, 372876.3744983825, 372876.3744983824, 100118.6231452557], 
processed observation next is [0.0, 0.8260869565217391, 0.8181818181818182, 0.48, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9170828110792778, 8.881784197001253e-17, 0.0, 0.1553651560409927, 0.15536515604099266, 0.25029655786313926], 
reward next is 0.5320, 
noisyNet noise sample is [array([-0.8597965], dtype=float32), 0.37244472]. 
=============================================
[2019-03-08 09:49:34,209] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.00000000e+00 1.02874526e-29 3.77133252e-27 7.82725528e-14
 5.15735210e-29], sum to 1.0000
[2019-03-08 09:49:34,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7989
[2019-03-08 09:49:34,225] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9185478638534879, 6.9112, 6.9112, 373472.9042366599, 373472.9042366599, 100244.3820886835], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2673000.0000, 
sim time next is 2673600.0000, 
raw observation next is [22.33333333333334, 67.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9151902952535574, 6.911200000000001, 6.9112, 372105.7974500192, 372105.7974500191, 99956.05804131879], 
processed observation next is [0.0, 0.9565217391304348, 0.6515151515151518, 0.6766666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9151902952535574, 8.881784197001253e-17, 0.0, 0.15504408227084135, 0.1550440822708413, 0.24989014510329696], 
reward next is 0.5318, 
noisyNet noise sample is [array([0.00484354], dtype=float32), -0.29357836]. 
=============================================
[2019-03-08 09:49:39,198] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4526114e-05 1.9853370e-22 3.1166772e-19 9.9995542e-01 2.6411891e-18], sum to 1.0000
[2019-03-08 09:49:39,208] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8187
[2019-03-08 09:49:39,215] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.83333333333334, 73.83333333333334, 1.0, 2.0, 0.2837965150818172, 1.0, 2.0, 0.2837965150818172, 0.0, 2.0, 0.0, 6.9112, 6.9112, 431794.149637949, 431794.149637949, 96055.34246579172], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2758200.0000, 
sim time next is 2758800.0000, 
raw observation next is [22.66666666666667, 74.66666666666667, 1.0, 2.0, 0.2834652811797833, 1.0, 2.0, 0.2834652811797833, 0.0, 2.0, 0.0, 6.9112, 6.9112, 431403.6783773209, 431403.6783773209, 95982.32907920369], 
processed observation next is [0.0, 0.9565217391304348, 0.6666666666666669, 0.7466666666666667, 1.0, 1.0, 0.2834652811797833, 1.0, 1.0, 0.2834652811797833, 0.0, 1.0, 0.0, 0.0, 0.0, 0.17975153265721702, 0.17975153265721702, 0.23995582269800922], 
reward next is 0.6421, 
noisyNet noise sample is [array([0.02439226], dtype=float32), 2.2206583]. 
=============================================
[2019-03-08 09:49:44,719] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.99089837e-01 1.32729692e-20 1.30230015e-20 9.10159259e-04
 2.82938704e-21], sum to 1.0000
[2019-03-08 09:49:44,725] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2223
[2019-03-08 09:49:44,744] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.0, 83.0, 1.0, 2.0, 0.6178316331302879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 471491.8752828139, 471491.8752828139, 94696.034534371], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2862000.0000, 
sim time next is 2862600.0000, 
raw observation next is [21.83333333333334, 83.83333333333334, 1.0, 2.0, 0.3201627817708533, 1.0, 1.0, 0.3201627817708533, 0.0, 2.0, 0.0, 6.9112, 6.9112, 485323.252397066, 485323.252397066, 101981.9623006804], 
processed observation next is [1.0, 0.13043478260869565, 0.628787878787879, 0.8383333333333334, 1.0, 1.0, 0.3201627817708533, 1.0, 0.5, 0.3201627817708533, 0.0, 1.0, 0.0, 0.0, 0.0, 0.20221802183211082, 0.20221802183211082, 0.254954905751701], 
reward next is 0.6798, 
noisyNet noise sample is [array([1.1271654], dtype=float32), -0.42754856]. 
=============================================
[2019-03-08 09:49:44,813] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.3342678e-06 4.2177682e-31 2.6482305e-28 9.9999464e-01 1.0792756e-27], sum to 1.0000
[2019-03-08 09:49:44,821] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0860
[2019-03-08 09:49:44,831] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.0, 83.0, 1.0, 2.0, 0.3113639714523102, 1.0, 2.0, 0.3113639714523102, 0.0, 2.0, 0.0, 6.9112, 6.9112, 471398.8907396861, 471398.8907396861, 100696.3218051912], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2857200.0000, 
sim time next is 2857800.0000, 
raw observation next is [22.0, 83.0, 1.0, 2.0, 0.3118885303989494, 1.0, 2.0, 0.3118885303989494, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 472197.3909856345, 472197.3909856346, 100778.2535073397], 
processed observation next is [1.0, 0.043478260869565216, 0.6363636363636364, 0.83, 1.0, 1.0, 0.3118885303989494, 1.0, 1.0, 0.3118885303989494, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.19674891291068106, 0.19674891291068108, 0.2519456337683493], 
reward next is 0.6694, 
noisyNet noise sample is [array([2.4453368], dtype=float32), -1.7544429]. 
=============================================
[2019-03-08 09:49:46,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 4.7776332e-31 3.4362783e-25 5.5013523e-08 2.8502444e-31], sum to 1.0000
[2019-03-08 09:49:46,046] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1121
[2019-03-08 09:49:46,058] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 80.0, 1.0, 2.0, 0.3911391144192437, 1.0, 2.0, 0.3911391144192437, 1.0, 1.0, 0.704189166471605, 6.9112, 6.9112, 858685.8912574503, 858685.8912574503, 188432.0410366145], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2881200.0000, 
sim time next is 2881800.0000, 
raw observation next is [24.5, 78.5, 1.0, 2.0, 0.5064239762559991, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9186622455630449, 6.9112, 6.9112, 747038.9549606021, 747038.9549606021, 167547.2556744758], 
processed observation next is [1.0, 0.34782608695652173, 0.75, 0.785, 1.0, 1.0, 0.5064239762559991, 0.0, 0.5, 0.0, 1.0, 1.0, 0.9186622455630449, 0.0, 0.0, 0.3112662312335842, 0.3112662312335842, 0.41886813918618954], 
reward next is 0.6370, 
noisyNet noise sample is [array([0.49805346], dtype=float32), -0.11835938]. 
=============================================
[2019-03-08 09:49:46,714] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9998939e-01 1.2760780e-36 9.5905638e-33 1.0589197e-05 1.3228275e-36], sum to 1.0000
[2019-03-08 09:49:46,723] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9026
[2019-03-08 09:49:46,738] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.66666666666666, 71.33333333333333, 1.0, 2.0, 0.4397650546037498, 1.0, 2.0, 0.4397650546037498, 1.0, 1.0, 0.7960624425655487, 6.911199999999999, 6.9112, 960873.2633993606, 960873.2633993608, 207871.6249361263], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2893200.0000, 
sim time next is 2893800.0000, 
raw observation next is [27.83333333333334, 70.66666666666667, 1.0, 2.0, 0.4320825270176322, 1.0, 2.0, 0.4320825270176322, 1.0, 2.0, 0.7868607418072939, 6.911200000000003, 6.9112, 944066.3946860825, 944066.3946860819, 206308.367550633], 
processed observation next is [1.0, 0.4782608695652174, 0.9015151515151518, 0.7066666666666667, 1.0, 1.0, 0.4320825270176322, 1.0, 1.0, 0.4320825270176322, 1.0, 1.0, 0.7868607418072939, 2.6645352591003756e-16, 0.0, 0.3933609977858677, 0.39336099778586747, 0.5157709188765826], 
reward next is 0.6537, 
noisyNet noise sample is [array([0.45282778], dtype=float32), -1.2147945]. 
=============================================
[2019-03-08 09:49:55,009] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 1.3515588e-29 3.6062164e-27 2.1166296e-14 7.5860455e-29], sum to 1.0000
[2019-03-08 09:49:55,016] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3235
[2019-03-08 09:49:55,032] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [20.33333333333334, 79.66666666666667, 1.0, 2.0, 0.8201249394195415, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 6.911199999999999, 6.9112, 1015613.815032615, 1015613.815032615, 222726.0197911077], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3058800.0000, 
sim time next is 3059400.0000, 
raw observation next is [20.66666666666666, 78.83333333333333, 1.0, 2.0, 0.9967587704230085, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 7.568760535706012, 6.9112, 1289293.974213949, 1147377.451081344, 258532.456326566], 
processed observation next is [1.0, 0.391304347826087, 0.5757575757575755, 0.7883333333333333, 1.0, 1.0, 0.9967587704230085, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 0.06575605357060121, 0.0, 0.5372058225891454, 0.47807393795055997, 0.646331140816415], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2786117], dtype=float32), 1.4713001]. 
=============================================
[2019-03-08 09:49:59,371] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9962711e-01 2.0297147e-28 4.9132835e-23 3.7286509e-04 1.7756666e-26], sum to 1.0000
[2019-03-08 09:49:59,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8784
[2019-03-08 09:49:59,393] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 70.33333333333333, 1.0, 2.0, 0.436397099235838, 1.0, 1.0, 0.436397099235838, 0.0, 2.0, 0.0, 6.9112, 6.9112, 639110.1514585321, 639110.1514585321, 126600.1403480691], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3087600.0000, 
sim time next is 3088200.0000, 
raw observation next is [26.93333333333333, 70.16666666666667, 1.0, 2.0, 0.9092385038379893, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 670557.0373763826, 670557.0373763826, 149914.9347071825], 
processed observation next is [1.0, 0.7391304347826086, 0.8606060606060605, 0.7016666666666667, 1.0, 1.0, 0.9092385038379893, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.27939876557349275, 0.27939876557349275, 0.3747873367679562], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6861409], dtype=float32), 0.24112098]. 
=============================================
[2019-03-08 09:50:03,634] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2572150e-04 2.0143348e-26 1.2373643e-20 9.9987423e-01 1.7217823e-26], sum to 1.0000
[2019-03-08 09:50:03,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6049
[2019-03-08 09:50:03,648] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.66666666666667, 70.5, 1.0, 2.0, 0.9512247861589017, 1.0, 2.0, 0.9512247861589017, 0.0, 2.0, 0.0, 6.9112, 6.9112, 1411473.249538734, 1411473.249538734, 304763.7654494724], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3161400.0000, 
sim time next is 3162000.0000, 
raw observation next is [24.33333333333334, 72.0, 1.0, 2.0, 0.905076273974747, 1.0, 2.0, 0.905076273974747, 0.0, 2.0, 0.0, 6.9112, 6.9112, 1344013.119967076, 1344013.119967076, 284416.1870713413], 
processed observation next is [1.0, 0.6086956521739131, 0.7424242424242427, 0.72, 1.0, 1.0, 0.905076273974747, 1.0, 1.0, 0.905076273974747, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5600054666529484, 0.5600054666529484, 0.7110404676783533], 
reward next is 0.6751, 
noisyNet noise sample is [array([-1.3318032], dtype=float32), -1.6216648]. 
=============================================
[2019-03-08 09:50:03,656] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[60.645473]
 [61.013493]
 [61.220726]
 [61.41839 ]
 [61.621754]], R is [[60.7651062 ]
 [60.81908035]
 [60.91350174]
 [60.99324036]
 [61.06801605]].
[2019-03-08 09:50:10,745] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-08 09:50:10,749] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation job starts!
[2019-03-08 09:50:10,749] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:50:10,765] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-03-08 09:50:11,831] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11087208], dtype=float32), 0.61328024]
[2019-03-08 09:50:11,831] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation this: [20.5, 57.33333333333334, 1.0, 2.0, 0.3930104893447204, 1.0, 1.0, 0.3930104893447204, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 613539.9115450622, 613539.9115450621, 109616.4469916658]
[2019-03-08 09:50:11,831] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-03-08 09:50:11,832] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Softmax [9.9995410e-01 3.3369388e-25 3.4492848e-22 4.5900226e-05 3.3406847e-24], sampled 0.38540865159620297
[2019-03-08 09:50:37,313] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5601.6180 1459187640.4966 449.0000
[2019-03-08 09:50:38,330] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 300000, evaluation results [300000.0, 5601.617985977011, 1459187640.496634, 449.0]
[2019-03-08 09:50:42,406] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1156052e-26 0.0000000e+00], sum to 1.0000
[2019-03-08 09:50:42,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8787
[2019-03-08 09:50:42,424] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7740314027894734, 6.9112, 6.9112, 314672.5280237009, 314672.5280237009, 87413.56556771942], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3376200.0000, 
sim time next is 3376800.0000, 
raw observation next is [18.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7728002331033101, 6.911200000000001, 6.9112, 314171.3609110726, 314171.3609110726, 87311.26929589237], 
processed observation next is [1.0, 0.08695652173913043, 0.45454545454545453, 0.88, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7728002331033101, 8.881784197001253e-17, 0.0, 0.13090473371294692, 0.13090473371294692, 0.21827817323973092], 
reward next is 0.5140, 
noisyNet noise sample is [array([0.61285037], dtype=float32), 0.6356974]. 
=============================================
[2019-03-08 09:50:46,435] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 2.0092634e-32 6.2536403e-31 2.9347105e-11 1.3402025e-36], sum to 1.0000
[2019-03-08 09:50:46,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8866
[2019-03-08 09:50:46,462] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.66666666666666, 89.0, 1.0, 2.0, 0.4218095146500249, 0.0, 2.0, 0.0, 1.0, 1.0, 0.762644600070934, 6.9112, 6.9112, 620017.2989499737, 620017.2989499737, 144954.5867384667], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3442800.0000, 
sim time next is 3443400.0000, 
raw observation next is [23.5, 89.0, 1.0, 2.0, 0.3115047686936054, 1.0, 1.0, 0.3115047686936054, 1.0, 2.0, 0.5581032432345302, 6.911199999999998, 6.9112, 680378.5073957895, 680378.5073957898, 162446.2282788825], 
processed observation next is [1.0, 0.8695652173913043, 0.7045454545454546, 0.89, 1.0, 1.0, 0.3115047686936054, 1.0, 0.5, 0.3115047686936054, 1.0, 1.0, 0.5581032432345302, -1.7763568394002506e-16, 0.0, 0.2834910447482456, 0.28349104474824577, 0.4061155706972063], 
reward next is 0.5983, 
noisyNet noise sample is [array([0.20540863], dtype=float32), 0.16323265]. 
=============================================
[2019-03-08 09:50:47,800] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0000000e+00 8.2915918e-30 2.9549052e-28 1.6847147e-20 9.6762897e-33], sum to 1.0000
[2019-03-08 09:50:47,809] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9201
[2019-03-08 09:50:47,826] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.2536252752319236, 1.0, 2.0, 0.2536252752319236, 1.0, 2.0, 0.4605622971080269, 6.9112, 6.9112, 561375.5675464463, 561375.5675464463, 145101.2207195799], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3456000.0000, 
sim time next is 3456600.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.2479039588176536, 1.0, 2.0, 0.2479039588176536, 1.0, 2.0, 0.4503330772977281, 6.911200000000001, 6.9112, 548896.7965358417, 548896.7965358414, 143472.9688520239], 
processed observation next is [1.0, 0.0, 0.6363636363636364, 0.9400000000000002, 1.0, 1.0, 0.2479039588176536, 1.0, 1.0, 0.2479039588176536, 1.0, 1.0, 0.4503330772977281, 8.881784197001253e-17, 0.0, 0.22870699855660068, 0.2287069985566006, 0.3586824221300598], 
reward next is 0.5465, 
noisyNet noise sample is [array([0.05807283], dtype=float32), -0.16643819]. 
=============================================
[2019-03-08 09:50:49,950] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3923386e-10 9.5394364e-33 3.0609213e-25 1.0000000e+00 1.3405734e-32], sum to 1.0000
[2019-03-08 09:50:49,957] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0220
[2019-03-08 09:50:49,961] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.66666666666667, 60.66666666666667, 1.0, 2.0, 0.3917220128795533, 1.0, 2.0, 0.3917220128795533, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 579857.9702777099, 579857.9702777098, 116691.118241936], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3522000.0000, 
sim time next is 3522600.0000, 
raw observation next is [27.5, 62.0, 1.0, 2.0, 0.4060357823290525, 1.0, 2.0, 0.4060357823290525, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 600433.3426217783, 600433.3426217781, 119589.2049005425], 
processed observation next is [1.0, 0.782608695652174, 0.8863636363636364, 0.62, 1.0, 1.0, 0.4060357823290525, 1.0, 1.0, 0.4060357823290525, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.25018055942574097, 0.2501805594257409, 0.29897301225135625], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.93337125], dtype=float32), -0.69669473]. 
=============================================
[2019-03-08 09:50:50,474] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6933658e-07 0.0000000e+00 0.0000000e+00 9.9999952e-01 0.0000000e+00], sum to 1.0000
[2019-03-08 09:50:50,482] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9994
[2019-03-08 09:50:50,487] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 70.0, 1.0, 2.0, 0.6911187588642744, 1.0, 2.0, 0.6911187588642744, 0.0, 2.0, 0.0, 6.9112, 6.9112, 1007162.477237733, 1007162.477237733, 201759.2320496325], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3499800.0000, 
sim time next is 3500400.0000, 
raw observation next is [27.0, 70.0, 1.0, 2.0, 0.9593087733562228, 1.0, 2.0, 0.9593087733562228, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1398072.92700763, 1398072.927007631, 311465.8697194903], 
processed observation next is [1.0, 0.5217391304347826, 0.8636363636363636, 0.7, 1.0, 1.0, 0.9593087733562228, 1.0, 1.0, 0.9593087733562228, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.5825303862531792, 0.5825303862531795, 0.7786646742987258], 
reward next is 0.6412, 
noisyNet noise sample is [array([0.164249], dtype=float32), -1.5957224]. 
=============================================
[2019-03-08 09:51:07,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.9652225e-29 0.0000000e+00], sum to 1.0000
[2019-03-08 09:51:07,415] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9956
[2019-03-08 09:51:07,420] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.16666666666667, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8105520276499297, 6.9112, 6.9112, 329507.253391247, 329507.253391247, 91032.4076135478], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3797400.0000, 
sim time next is 3798000.0000, 
raw observation next is [17.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8076568658010174, 6.9112, 6.9112, 328328.8226591542, 328328.8226591542, 90787.20254862694], 
processed observation next is [1.0, 1.0, 0.4090909090909091, 0.94, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8076568658010174, 0.0, 0.0, 0.13680367610798091, 0.13680367610798091, 0.22696800637156736], 
reward next is 0.5166, 
noisyNet noise sample is [array([0.9167524], dtype=float32), 0.07083149]. 
=============================================
[2019-03-08 09:51:07,443] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[51.23814 ]
 [51.226448]
 [51.214737]
 [51.203003]
 [51.19126 ]], R is [[51.27830887]
 [51.28262329]
 [51.28734589]
 [51.29263687]
 [51.29868317]].
[2019-03-08 09:51:08,996] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000e+00 0.0000e+00 0.0000e+00 2.7995e-32 0.0000e+00], sum to 1.0000
[2019-03-08 09:51:09,005] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2110
[2019-03-08 09:51:09,009] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.0, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7488454464470732, 6.9112, 6.9112, 304392.8545126225, 304392.8545126225, 85826.77782968155], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3814800.0000, 
sim time next is 3815400.0000, 
raw observation next is [17.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7373747259152027, 6.911200000000001, 6.9112, 299724.8383874244, 299724.8383874242, 84863.65944286446], 
processed observation next is [0.0, 0.13043478260869565, 0.4090909090909091, 0.89, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7373747259152027, 8.881784197001253e-17, 0.0, 0.1248853493280935, 0.1248853493280934, 0.21215914860716115], 
reward next is 0.5045, 
noisyNet noise sample is [array([-0.2560346], dtype=float32), 1.9277023]. 
=============================================
[2019-03-08 09:51:11,761] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9999702e-01 0.0000000e+00 3.0529857e-38 2.9572975e-06 0.0000000e+00], sum to 1.0000
[2019-03-08 09:51:11,767] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8817
[2019-03-08 09:51:11,774] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7488673170771684, 6.9112, 6.9112, 304401.7549375421, 304401.7549375421, 85829.19676706233], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3877200.0000, 
sim time next is 3877800.0000, 
raw observation next is [20.83333333333333, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7390131109657978, 6.9112, 6.9112, 300391.569857818, 300391.569857818, 85001.6629737324], 
processed observation next is [0.0, 0.9130434782608695, 0.5833333333333331, 0.64, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7390131109657978, 0.0, 0.0, 0.12516315410742418, 0.12516315410742418, 0.212504157434331], 
reward next is 0.5048, 
noisyNet noise sample is [array([1.3089812], dtype=float32), 0.08459032]. 
=============================================
[2019-03-08 09:51:13,584] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.5682359e-37 9.2275765e-34 5.1731494e-16 0.0000000e+00], sum to 1.0000
[2019-03-08 09:51:13,591] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8952
[2019-03-08 09:51:13,597] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 45.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8224395168060077, 6.911199999999999, 6.9112, 334345.9832873257, 334345.9832873258, 92041.34640611021], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3934800.0000, 
sim time next is 3935400.0000, 
raw observation next is [26.0, 45.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.823899517447953, 6.9112, 6.9112, 334940.2800336065, 334940.2800336065, 92165.1958389119], 
processed observation next is [0.0, 0.5652173913043478, 0.8181818181818182, 0.45, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.823899517447953, 0.0, 0.0, 0.13955845001400272, 0.13955845001400272, 0.23041298959727974], 
reward next is 0.5192, 
noisyNet noise sample is [array([0.5249775], dtype=float32), 1.4172899]. 
=============================================
[2019-03-08 09:51:17,702] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 8.141177e-31 0.000000e+00], sum to 1.0000
[2019-03-08 09:51:17,712] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3870
[2019-03-08 09:51:17,721] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 84.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8854620112622409, 6.9112, 6.9112, 360040.7351586747, 360040.7351586747, 96729.97778018628], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4004400.0000, 
sim time next is 4005000.0000, 
raw observation next is [18.0, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9181760674817805, 6.9112, 6.9112, 373363.2756476359, 373363.2756476359, 99487.19633748815], 
processed observation next is [1.0, 0.34782608695652173, 0.45454545454545453, 0.855, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9181760674817805, 0.0, 0.0, 0.1555680315198483, 0.1555680315198483, 0.24871799084372037], 
reward next is 0.5361, 
noisyNet noise sample is [array([-0.6746337], dtype=float32), 3.254429]. 
=============================================
[2019-03-08 09:51:17,738] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[53.87304 ]
 [53.761246]
 [53.65178 ]
 [51.930077]
 [53.02969 ]], R is [[53.9762764 ]
 [53.96824646]
 [53.95821762]
 [53.94470596]
 [53.89941406]].
[2019-03-08 09:51:20,011] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.1208088e-31 2.7977668e-27 4.4544968e-14 6.7113998e-32], sum to 1.0000
[2019-03-08 09:51:20,020] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2059
[2019-03-08 09:51:20,041] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [17.0, 100.0, 1.0, 2.0, 0.1927513639755447, 1.0, 2.0, 0.1927513639755447, 1.0, 2.0, 0.3660006756792215, 6.911200000000001, 6.9112, 446036.6300971964, 446036.6300971962, 126746.4150508902], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4014000.0000, 
sim time next is 4014600.0000, 
raw observation next is [17.16666666666667, 99.00000000000001, 1.0, 2.0, 0.2615680851627805, 0.0, 1.0, 0.0, 1.0, 2.0, 0.4980713703990498, 6.911199999999999, 6.9112, 404763.0399830603, 404763.0399830604, 105963.6427267306], 
processed observation next is [1.0, 0.4782608695652174, 0.4166666666666669, 0.9900000000000001, 1.0, 1.0, 0.2615680851627805, 0.0, 0.5, 0.0, 1.0, 1.0, 0.4980713703990498, -8.881784197001253e-17, 0.0, 0.16865126665960847, 0.1686512666596085, 0.2649091068168265], 
reward next is 0.5457, 
noisyNet noise sample is [array([0.72086376], dtype=float32), 0.87301767]. 
=============================================
[2019-03-08 09:51:21,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.7235546e-30 1.9697583e-28 8.7143706e-09 1.7816421e-34], sum to 1.0000
[2019-03-08 09:51:21,484] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6341
[2019-03-08 09:51:21,500] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.83333333333333, 100.0, 1.0, 2.0, 0.3645192781528145, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6898477172915334, 6.911199999999998, 6.9112, 560793.5496431768, 560793.5496431772, 129881.2397822351], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4024200.0000, 
sim time next is 4024800.0000, 
raw observation next is [18.0, 100.0, 1.0, 2.0, 0.3860773354825884, 1.0, 1.0, 0.3860773354825884, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 593329.3511832362, 593329.3511832363, 111638.891266601], 
processed observation next is [1.0, 0.6086956521739131, 0.45454545454545453, 1.0, 1.0, 1.0, 0.3860773354825884, 1.0, 0.5, 0.3860773354825884, 0.0, 0.5, 0.0, -8.881784197001253e-17, 0.0, 0.2472205629930151, 0.24722056299301515, 0.2790972281665025], 
reward next is 0.7592, 
noisyNet noise sample is [array([0.28020746], dtype=float32), 0.4796961]. 
=============================================
[2019-03-08 09:51:23,261] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0000000e+00 1.3131306e-32 8.8130687e-29 4.7364074e-11 1.1634759e-32], sum to 1.0000
[2019-03-08 09:51:23,270] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2299
[2019-03-08 09:51:23,276] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.16666666666667, 99.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7049353477394963, 6.9112, 6.9112, 286524.5207454138, 286524.5207454138, 82148.3118260429], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4068600.0000, 
sim time next is 4069200.0000, 
raw observation next is [16.33333333333334, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7090009119482993, 6.9112, 6.9112, 288178.8175800201, 288178.8175800201, 82488.0926672754], 
processed observation next is [1.0, 0.08695652173913043, 0.37878787878787906, 0.98, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7090009119482993, 0.0, 0.0, 0.12007450732500838, 0.12007450732500838, 0.2062202316681885], 
reward next is 0.4991, 
noisyNet noise sample is [array([-0.37173584], dtype=float32), -2.4908054]. 
=============================================
[2019-03-08 09:51:28,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.0348779e-36 2.2937884e-21 7.2654361e-36], sum to 1.0000
[2019-03-08 09:51:28,574] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5050
[2019-03-08 09:51:28,580] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8206617422694547, 6.911200000000001, 6.9112, 333622.3395806627, 333622.3395806627, 91889.78809780694], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4164600.0000, 
sim time next is 4165200.0000, 
raw observation next is [17.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8151643916320338, 6.911199999999999, 6.9112, 331384.6666484196, 331384.6666484196, 91423.77675178676], 
processed observation next is [1.0, 0.21739130434782608, 0.4090909090909091, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8151643916320338, -8.881784197001253e-17, 0.0, 0.1380769444368415, 0.1380769444368415, 0.2285594418794669], 
reward next is 0.5178, 
noisyNet noise sample is [array([-0.25318632], dtype=float32), -1.2929848]. 
=============================================
[2019-03-08 09:51:34,664] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.8269182e-01 1.2729302e-23 2.5860830e-18 1.7308187e-02 1.3323017e-21], sum to 1.0000
[2019-03-08 09:51:34,675] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4625
[2019-03-08 09:51:34,691] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.83333333333333, 94.0, 1.0, 2.0, 0.3563380034267954, 1.0, 2.0, 0.3563380034267954, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 550139.098991817, 550139.0989918169, 105694.3193680743], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4261800.0000, 
sim time next is 4262400.0000, 
raw observation next is [18.0, 94.0, 1.0, 2.0, 0.6872246447590992, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 533513.6847941306, 533513.6847941305, 98175.33941170425], 
processed observation next is [1.0, 0.34782608695652173, 0.45454545454545453, 0.94, 1.0, 1.0, 0.6872246447590992, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.2222973686642211, 0.22229736866422103, 0.2454383485292606], 
reward next is 0.7763, 
noisyNet noise sample is [array([1.4522673], dtype=float32), 2.333081]. 
=============================================
[2019-03-08 09:51:43,549] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0000000e+00 5.4460763e-25 1.2510565e-21 1.2964038e-11 1.8349561e-24], sum to 1.0000
[2019-03-08 09:51:43,560] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9672
[2019-03-08 09:51:43,566] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.83333333333334, 78.83333333333333, 1.0, 2.0, 0.4975171492238155, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 378310.1872832618, 378310.1872832618, 71521.37053165174], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4438200.0000, 
sim time next is 4438800.0000, 
raw observation next is [23.0, 78.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9547116395713053, 8.419154249980888, 6.9112, 501002.6827783573, 388366.7361085787, 100897.7279388336], 
processed observation next is [0.0, 0.391304347826087, 0.6818181818181818, 0.78, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.9547116395713053, 0.15079542499808882, 0.0, 0.20875111782431555, 0.16181947337857444, 0.252244319847084], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33105764], dtype=float32), -0.6403196]. 
=============================================
[2019-03-08 09:51:43,882] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 2.2677962e-32 2.8963790e-30 1.8702796e-21 8.6496030e-36], sum to 1.0000
[2019-03-08 09:51:43,890] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5781
[2019-03-08 09:51:43,899] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 83.83333333333334, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9547116395713053, 7.714735532484044, 6.9112, 448308.9259765206, 388288.2125953536, 101995.228648039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4420200.0000, 
sim time next is 4420800.0000, 
raw observation next is [21.0, 83.0, 1.0, 1.0, 0.589681140068378, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 453852.6394922661, 453852.6394922661, 81556.68009187489], 
processed observation next is [0.0, 0.17391304347826086, 0.5909090909090909, 0.83, 1.0, 0.5, 0.589681140068378, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 8.881784197001253e-17, 0.0, 0.1891052664551109, 0.1891052664551109, 0.20389170022968722], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.14645293], dtype=float32), -0.39064133]. 
=============================================
[2019-03-08 09:51:49,273] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.9463463e-01 1.6463614e-24 1.3882514e-20 5.3654206e-03 8.0536885e-24], sum to 1.0000
[2019-03-08 09:51:49,285] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6938
[2019-03-08 09:51:49,301] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 83.83333333333334, 1.0, 1.0, 0.3782352960969291, 1.0, 1.0, 0.3782352960969291, 0.0, 1.0, 0.0, 6.9112, 6.9112, 573222.1676851786, 573222.1676851786, 112001.3044153844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4535400.0000, 
sim time next is 4536000.0000, 
raw observation next is [22.0, 83.0, 1.0, 2.0, 0.481361214216745, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 366665.7082793224, 366665.7082793224, 69565.72539217398], 
processed observation next is [0.0, 0.5217391304347826, 0.6363636363636364, 0.83, 1.0, 1.0, 0.481361214216745, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.15277737844971767, 0.15277737844971767, 0.17391431348043493], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2613142], dtype=float32), 1.3236283]. 
=============================================
[2019-03-08 09:51:49,315] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[38.518578]
 [38.856216]
 [38.889526]
 [37.942417]
 [38.07759 ]], R is [[37.69724655]
 [38.0514183 ]
 [37.67090607]
 [37.81518936]
 [37.43703842]].
[2019-03-08 09:51:53,361] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 9.155851e-16 0.000000e+00], sum to 1.0000
[2019-03-08 09:51:53,373] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3277
[2019-03-08 09:51:53,379] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4660373797424143, 6.911199999999999, 6.9112, 189360.200935953, 189360.200935953, 53866.26041604331], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4597800.0000, 
sim time next is 4598400.0000, 
raw observation next is [14.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4601864372227466, 6.9112, 6.9112, 186980.7026892627, 186980.7026892627, 53337.41288746387], 
processed observation next is [1.0, 0.21739130434782608, 0.2727272727272727, 0.94, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4601864372227466, 0.0, 0.0, 0.07790862612052613, 0.07790862612052613, 0.1333435322186597], 
reward next is 0.5008, 
noisyNet noise sample is [array([1.134194], dtype=float32), -1.8395038]. 
=============================================
[2019-03-08 09:51:59,354] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 2.942294e-22 0.000000e+00], sum to 1.0000
[2019-03-08 09:51:59,360] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3752
[2019-03-08 09:51:59,365] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.0, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4222198401628088, 6.911199999999999, 6.9112, 171541.5345499099, 171541.53454991, 48436.48953046484], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4685400.0000, 
sim time next is 4686000.0000, 
raw observation next is [14.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4255906141541534, 6.9112, 6.9112, 172912.1713368639, 172912.1713368639, 48602.05096432038], 
processed observation next is [1.0, 0.21739130434782608, 0.2727272727272727, 0.84, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4255906141541534, 0.0, 0.0, 0.07204673805702662, 0.07204673805702662, 0.12150512741080094], 
reward next is 0.5082, 
noisyNet noise sample is [array([-1.0934107], dtype=float32), -0.6199973]. 
=============================================
[2019-03-08 09:51:59,383] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[50.991238]
 [51.118073]
 [51.245514]
 [51.37365 ]
 [51.36142 ]], R is [[50.86574936]
 [50.86303329]
 [50.85838699]
 [50.85329056]
 [50.84744263]].
[2019-03-08 09:52:14,530] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.9973291e-01 2.9321848e-31 1.8681688e-36 2.6706667e-04 5.9892040e-38], sum to 1.0000
[2019-03-08 09:52:14,538] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3411
[2019-03-08 09:52:14,542] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.0, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8097352902351781, 6.911200000000001, 6.9112, 329174.8120128125, 329174.8120128124, 90963.53680202918], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4941000.0000, 
sim time next is 4941600.0000, 
raw observation next is [17.0, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7994757225300657, 6.911200000000001, 6.9112, 324998.8728055235, 324998.8728055235, 90095.20944221405], 
processed observation next is [1.0, 0.17391304347826086, 0.4090909090909091, 0.96, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7994757225300657, 8.881784197001253e-17, 0.0, 0.13541619700230145, 0.13541619700230145, 0.22523802360553513], 
reward next is 0.5153, 
noisyNet noise sample is [array([0.38259706], dtype=float32), -2.0595307]. 
=============================================
[2019-03-08 09:52:14,977] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.9999964e-01 1.4487343e-20 1.1239190e-21 3.2077787e-07 1.3463158e-20], sum to 1.0000
[2019-03-08 09:52:14,985] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5586
[2019-03-08 09:52:14,999] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.33333333333334, 90.33333333333334, 1.0, 2.0, 0.3671269919063037, 1.0, 1.0, 0.3671269919063037, 0.0, 1.0, 0.0, 6.9112, 6.9112, 569865.843867974, 569865.843867974, 106539.7708441277], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4958400.0000, 
sim time next is 4959000.0000, 
raw observation next is [17.5, 88.5, 1.0, 2.0, 0.3639194787260857, 1.0, 2.0, 0.3639194787260857, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 565199.7609961096, 565199.7609961095, 105879.1859074693], 
processed observation next is [1.0, 0.391304347826087, 0.4318181818181818, 0.885, 1.0, 1.0, 0.3639194787260857, 1.0, 1.0, 0.3639194787260857, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.23549990041504565, 0.2354999004150456, 0.26469796476867324], 
reward next is 0.7626, 
noisyNet noise sample is [array([0.84521824], dtype=float32), 1.5890151]. 
=============================================
[2019-03-08 09:52:15,012] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[51.600925]
 [49.883076]
 [49.624382]
 [51.875603]
 [51.76017 ]], R is [[51.93495941]
 [52.17973328]
 [52.29948807]
 [52.38039398]
 [52.60033798]].
[2019-03-08 09:52:18,383] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1574952e-04 5.7660758e-38 4.1377817e-36 9.9968421e-01 1.4502108e-37], sum to 1.0000
[2019-03-08 09:52:18,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1803
[2019-03-08 09:52:18,399] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.5, 70.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6593774908675042, 6.9112, 6.9112, 267988.2550766598, 267988.2550766598, 75731.1608064312], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4995000.0000, 
sim time next is 4995600.0000, 
raw observation next is [18.33333333333334, 71.33333333333333, 1.0, 1.0, 0.2081260342202293, 1.0, 1.0, 0.2081260342202293, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 325493.9248993282, 325493.9248993283, 82721.46182628341], 
processed observation next is [1.0, 0.8260869565217391, 0.46969696969696995, 0.7133333333333333, 1.0, 0.5, 0.2081260342202293, 1.0, 0.5, 0.2081260342202293, 0.0, 0.5, 0.0, -8.881784197001253e-17, 0.0, 0.13562246870805342, 0.13562246870805347, 0.20680365456570854], 
reward next is 0.5621, 
noisyNet noise sample is [array([2.341115], dtype=float32), 0.13227588]. 
=============================================
[2019-03-08 09:52:35,337] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0000000e+00 1.1015629e-36 0.0000000e+00 7.0178225e-28 0.0000000e+00], sum to 1.0000
[2019-03-08 09:52:35,343] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9327
[2019-03-08 09:52:35,358] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.23333333333333, 55.33333333333333, 1.0, 2.0, 0.4984655040666466, 1.0, 1.0, 0.4984655040666466, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 753505.9217614895, 753505.9217614896, 137233.2703264104], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5305800.0000, 
sim time next is 5306400.0000, 
raw observation next is [26.6, 54.0, 1.0, 2.0, 0.5028347881379905, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9328217853716118, 6.911200000000001, 6.9112, 758570.0016857525, 758570.0016857524, 167068.8831065467], 
processed observation next is [1.0, 0.43478260869565216, 0.8454545454545456, 0.54, 1.0, 1.0, 0.5028347881379905, 0.0, 0.5, 0.0, 1.0, 0.5, 0.9328217853716118, 8.881784197001253e-17, 0.0, 0.3160708340357302, 0.31607083403573016, 0.41767220776636677], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2733212], dtype=float32), 0.2547043]. 
=============================================
[2019-03-08 09:52:40,992] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 6.9992594e-38 0.0000000e+00 1.2075602e-16 0.0000000e+00], sum to 1.0000
[2019-03-08 09:52:41,003] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7806
[2019-03-08 09:52:41,008] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.0, 95.66666666666666, 1.0, 2.0, 0.4371140316530426, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 336890.2094713455, 336890.2094713455, 63899.03246562902], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5427600.0000, 
sim time next is 5428200.0000, 
raw observation next is [18.9, 96.33333333333334, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9547116395713053, 7.306634204981756, 6.9112, 417780.2900037753, 388242.7348700223, 102672.1649043554], 
processed observation next is [1.0, 0.8260869565217391, 0.49545454545454537, 0.9633333333333334, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.9547116395713053, 0.03954342049817559, 0.0, 0.1740751208349064, 0.1617678061958426, 0.2566804122608885], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5958479], dtype=float32), -1.6041522]. 
=============================================
[2019-03-08 09:52:51,361] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.0732715e-01 1.7647837e-31 3.0500404e-31 1.9267282e-01 2.8873679e-31], sum to 1.0000
[2019-03-08 09:52:51,368] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0137
[2019-03-08 09:52:51,384] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [19.6, 93.0, 1.0, 1.0, 0.3673944018341444, 1.0, 1.0, 0.3673944018341444, 0.0, 1.0, 0.0, 6.9112, 6.9112, 561848.6607512438, 561848.6607512438, 108977.4820044209], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5604000.0000, 
sim time next is 5604600.0000, 
raw observation next is [19.5, 93.0, 1.0, 2.0, 0.2071014156525825, 1.0, 2.0, 0.2071014156525825, 1.0, 1.0, 0.3880958413560542, 6.911200000000001, 6.9112, 472982.9611125616, 472982.9611125613, 131336.1714232622], 
processed observation next is [1.0, 0.8695652173913043, 0.5227272727272727, 0.93, 1.0, 1.0, 0.2071014156525825, 1.0, 1.0, 0.2071014156525825, 1.0, 0.5, 0.3880958413560542, 8.881784197001253e-17, 0.0, 0.19707623379690065, 0.19707623379690054, 0.3283404285581555], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.75895983], dtype=float32), -1.9993948]. 
=============================================
[2019-03-08 09:52:56,480] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 5.3465818e-28 6.6752587e-28 6.3255741e-11 1.4463648e-29], sum to 1.0000
[2019-03-08 09:52:56,488] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0108
[2019-03-08 09:52:56,496] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [12.0, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3221282085053541, 6.9112, 6.9112, 130850.155494649, 130850.155494649, 37081.25723264521], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5696400.0000, 
sim time next is 5697000.0000, 
raw observation next is [11.9, 78.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3181928199495948, 6.9112, 6.9112, 129250.583888381, 129250.583888381, 36699.58294636993], 
processed observation next is [0.0, 0.9565217391304348, 0.17727272727272728, 0.785, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3181928199495948, 0.0, 0.0, 0.05385440995349208, 0.05385440995349208, 0.09174895736592481], 
reward next is 0.5031, 
noisyNet noise sample is [array([1.272672], dtype=float32), 0.45391536]. 
=============================================
[2019-03-08 09:52:56,506] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[41.07294 ]
 [41.06879 ]
 [41.061607]
 [41.067028]
 [41.063618]], R is [[41.16799927]
 [41.26042557]
 [41.35314178]
 [41.44680023]
 [41.54128647]].
[2019-03-08 09:52:56,514] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7259522e-10 1.4684470e-29 8.7459441e-31 1.0000000e+00 2.1145997e-33], sum to 1.0000
[2019-03-08 09:52:56,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5334
[2019-03-08 09:52:56,530] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.6, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6347212896597227, 6.911200000000001, 6.9112, 257957.410253983, 257957.410253983, 72898.3820391907], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5655000.0000, 
sim time next is 5655600.0000, 
raw observation next is [15.5, 93.0, 1.0, 1.0, 0.1988711193737338, 1.0, 1.0, 0.1988711193737338, 0.0, 1.0, 0.0, 6.9112, 6.9112, 311138.8865409755, 311138.8865409755, 81247.16893743952], 
processed observation next is [0.0, 0.4782608695652174, 0.3409090909090909, 0.93, 1.0, 0.5, 0.1988711193737338, 1.0, 0.5, 0.1988711193737338, 0.0, 0.5, 0.0, 0.0, 0.0, 0.12964120272540644, 0.12964120272540644, 0.2031179223435988], 
reward next is 0.5471, 
noisyNet noise sample is [array([1.3498179], dtype=float32), 1.0839468]. 
=============================================
[2019-03-08 09:53:06,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.00000e+00 0.00000e+00 0.00000e+00 5.59218e-33 0.00000e+00], sum to 1.0000
[2019-03-08 09:53:06,115] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0185
[2019-03-08 09:53:06,121] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.4, 52.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6716261773630797, 6.911200000000001, 6.9112, 272971.6566983885, 272971.6566983884, 79373.49843132788], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5832000.0000, 
sim time next is 5832600.0000, 
raw observation next is [24.4, 51.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.866620134622813, 6.911200000000001, 6.9112, 352331.0101607537, 352331.0101607537, 95799.21865747868], 
processed observation next is [1.0, 0.5217391304347826, 0.7454545454545454, 0.5166666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.866620134622813, 8.881784197001253e-17, 0.0, 0.1468045875669807, 0.1468045875669807, 0.2394980466436967], 
reward next is 0.5254, 
noisyNet noise sample is [array([-0.16234757], dtype=float32), -0.6053453]. 
=============================================
[2019-03-08 09:53:11,307] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7132316e-26 1.5791015e-37 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:53:11,316] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2368
[2019-03-08 09:53:11,320] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.3, 66.0, 1.0, 2.0, 0.4621826004999237, 1.0, 2.0, 0.4621826004999237, 0.0, 2.0, 0.0, 6.9112, 6.9112, 704836.3771496115, 704836.3771496115, 127882.4231551438], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5911200.0000, 
sim time next is 5911800.0000, 
raw observation next is [23.58333333333334, 65.0, 1.0, 2.0, 0.4212503189319911, 1.0, 2.0, 0.4212503189319911, 0.0, 2.0, 0.0, 6.9112, 6.9112, 642066.8195987334, 642066.8195987334, 119518.3945042093], 
processed observation next is [1.0, 0.43478260869565216, 0.7083333333333336, 0.65, 1.0, 1.0, 0.4212503189319911, 1.0, 1.0, 0.4212503189319911, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2675278414994723, 0.2675278414994723, 0.2987959862605232], 
reward next is 0.7674, 
noisyNet noise sample is [array([1.1244183], dtype=float32), 0.40121374]. 
=============================================
[2019-03-08 09:53:22,812] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.42269952e-04 4.99387428e-31 3.77149758e-33 9.99657750e-01
 1.11256316e-32], sum to 1.0000
[2019-03-08 09:53:22,820] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5419
[2019-03-08 09:53:22,826] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.9, 53.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 293272.8260408673, 293272.8260408673, 80724.60706731485], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6114600.0000, 
sim time next is 6115200.0000, 
raw observation next is [21.8, 53.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 290543.8599359365, 290543.8599359365, 80255.0486605104], 
processed observation next is [1.0, 0.782608695652174, 0.6272727272727273, 0.53, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.12105994163997355, 0.12105994163997355, 0.200637621651276], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3222101], dtype=float32), -0.9220364]. 
=============================================
[2019-03-08 09:53:29,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9008918e-01 4.7501461e-20 2.0499806e-23 9.9108387e-03 9.6300504e-23], sum to 1.0000
[2019-03-08 09:53:29,878] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4320
[2019-03-08 09:53:29,895] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.96666666666667, 67.0, 1.0, 2.0, 0.402119641213163, 1.0, 2.0, 0.402119641213163, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 592276.9835806682, 592276.9835806683, 119107.6453244543], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6264600.0000, 
sim time next is 6265200.0000, 
raw observation next is [27.33333333333334, 65.0, 1.0, 2.0, 0.2837531556159354, 1.0, 2.0, 0.2837531556159354, 1.0, 1.0, 0.5088300155240976, 6.9112, 6.9112, 620264.4683405047, 620264.4683405047, 154017.6530641816], 
processed observation next is [0.0, 0.5217391304347826, 0.878787878787879, 0.65, 1.0, 1.0, 0.2837531556159354, 1.0, 1.0, 0.2837531556159354, 1.0, 0.5, 0.5088300155240976, 0.0, 0.0, 0.2584435284752103, 0.2584435284752103, 0.38504413266045395], 
reward next is 0.5753, 
noisyNet noise sample is [array([-0.9046274], dtype=float32), 0.65302217]. 
=============================================
[2019-03-08 09:53:35,461] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0938259e-19 0.0000000e+00], sum to 1.0000
[2019-03-08 09:53:35,471] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2521
[2019-03-08 09:53:35,487] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.85, 81.5, 1.0, 2.0, 0.3423235612749588, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6254699686799583, 6.911199999999999, 6.9112, 508387.8172819368, 508387.8172819369, 125567.3516671148], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6337800.0000, 
sim time next is 6338400.0000, 
raw observation next is [24.03333333333333, 80.66666666666667, 1.0, 2.0, 0.2607996984789191, 1.0, 1.0, 0.2607996984789191, 1.0, 2.0, 0.4727924673647173, 6.911199999999999, 6.9112, 576295.9513182368, 576295.9513182369, 147183.2225135768], 
processed observation next is [0.0, 0.34782608695652173, 0.7287878787878787, 0.8066666666666668, 1.0, 1.0, 0.2607996984789191, 1.0, 0.5, 0.2607996984789191, 1.0, 1.0, 0.4727924673647173, -8.881784197001253e-17, 0.0, 0.24012331304926532, 0.24012331304926537, 0.367958056283942], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.19278084], dtype=float32), -0.036710963]. 
=============================================
[2019-03-08 09:53:42,626] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 9.3398389e-34 4.6688064e-33 1.5919926e-12 1.9282569e-37], sum to 1.0000
[2019-03-08 09:53:42,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1639
[2019-03-08 09:53:42,644] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.15, 81.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.614192655798134, 6.9112, 6.9112, 249606.3664649785, 249606.3664649785, 74613.80815159681], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6453000.0000, 
sim time next is 6453600.0000, 
raw observation next is [19.43333333333333, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7930206959968371, 6.9112, 6.9112, 322371.5599324429, 322371.5599324429, 89549.97248609415], 
processed observation next is [1.0, 0.6956521739130435, 0.5196969696969695, 0.79, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7930206959968371, 0.0, 0.0, 0.13432148330518454, 0.13432148330518454, 0.22387493121523538], 
reward next is 0.5143, 
noisyNet noise sample is [array([-0.4484617], dtype=float32), -1.9204457]. 
=============================================
[2019-03-08 09:53:48,615] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1223535e-37 0.0000000e+00], sum to 1.0000
[2019-03-08 09:53:48,626] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3319
[2019-03-08 09:53:48,635] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.6, 54.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5914847730938131, 6.9112, 6.9112, 240369.4281174658, 240369.4281174658, 66197.96166508015], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6547200.0000, 
sim time next is 6547800.0000, 
raw observation next is [19.5, 54.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5982124375786384, 6.9112, 6.9112, 243105.9876175773, 243105.9876175773, 66835.26148288828], 
processed observation next is [1.0, 0.782608695652174, 0.5227272727272727, 0.54, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5982124375786384, 0.0, 0.0, 0.10129416150732388, 0.10129416150732388, 0.1670881537072207], 
reward next is 0.5196, 
noisyNet noise sample is [array([0.72946995], dtype=float32), 0.78997797]. 
=============================================
[2019-03-08 09:53:57,511] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.4831773e-25 6.2183810e-25 1.2494688e-20 3.1301144e-26], sum to 1.0000
[2019-03-08 09:53:57,521] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9170
[2019-03-08 09:53:57,528] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.2, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8038102846454437, 6.9112, 6.9112, 326763.1479488002, 326763.1479488002, 90462.08954724662], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6738600.0000, 
sim time next is 6739200.0000, 
raw observation next is [17.2, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8038112294061706, 6.9112, 6.9112, 326763.53249264, 326763.53249264, 90462.16935451033], 
processed observation next is [1.0, 0.0, 0.41818181818181815, 0.96, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8038112294061706, 0.0, 0.0, 0.13615147187193335, 0.13615147187193335, 0.22615542338627584], 
reward next is 0.5160, 
noisyNet noise sample is [array([1.028563], dtype=float32), -0.6766778]. 
=============================================
[2019-03-08 09:53:57,793] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0880730e-20 6.6481488e-34 1.0762352e-34 1.0000000e+00 2.5899979e-36], sum to 1.0000
[2019-03-08 09:53:57,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4152
[2019-03-08 09:53:57,810] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 97.0, 1.0, 2.0, 0.2345508376005443, 1.0, 2.0, 0.2345508376005443, 0.0, 2.0, 0.0, 6.9112, 6.9112, 362869.7259315499, 362869.7259315499, 87623.83132168587], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6727200.0000, 
sim time next is 6727800.0000, 
raw observation next is [17.7, 97.0, 1.0, 2.0, 0.2344638039373098, 1.0, 2.0, 0.2344638039373098, 0.0, 2.0, 0.0, 6.9112, 6.9112, 362737.6637679611, 362737.6637679611, 87612.21813758093], 
processed observation next is [1.0, 0.8695652173913043, 0.44090909090909086, 0.97, 1.0, 1.0, 0.2344638039373098, 1.0, 1.0, 0.2344638039373098, 0.0, 1.0, 0.0, 0.0, 0.0, 0.15114069323665044, 0.15114069323665044, 0.21903054534395233], 
reward next is 0.5915, 
noisyNet noise sample is [array([0.32159522], dtype=float32), 0.7964371]. 
=============================================
[2019-03-08 09:54:04,195] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1129423e-04 2.5415007e-34 4.7468651e-33 9.9978870e-01 6.2517052e-29], sum to 1.0000
[2019-03-08 09:54:04,196] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3883
[2019-03-08 09:54:04,203] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.7, 70.0, 1.0, 2.0, 0.2779588292977551, 1.0, 2.0, 0.2779588292977551, 0.0, 2.0, 0.0, 6.9112, 6.9112, 425238.4632059155, 425238.4632059155, 94671.85715212834], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6812400.0000, 
sim time next is 6813000.0000, 
raw observation next is [22.7, 69.5, 1.0, 2.0, 0.2756731662206126, 1.0, 2.0, 0.2756731662206126, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 421996.1684225851, 421996.168422585, 94284.4890463995], 
processed observation next is [1.0, 0.8695652173913043, 0.6681818181818181, 0.695, 1.0, 1.0, 0.2756731662206126, 1.0, 1.0, 0.2756731662206126, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.1758317368427438, 0.17583173684274375, 0.23571122261599875], 
reward next is 0.6394, 
noisyNet noise sample is [array([-0.11133993], dtype=float32), 0.82402474]. 
=============================================
[2019-03-08 09:54:04,243] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[51.689884]
 [51.77579 ]
 [51.846622]
 [51.822952]
 [51.77364 ]], R is [[51.73182297]
 [51.85617828]
 [51.98257446]
 [52.10772705]
 [52.22770309]].
[2019-03-08 09:54:17,255] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-08 09:54:17,257] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation job starts!
[2019-03-08 09:54:17,258] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:54:17,271] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-03-08 09:54:43,945] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5562.0216 1461573057.3078 450.0000
[2019-03-08 09:54:44,960] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 400000, evaluation results [400000.0, 5562.0215818873785, 1461573057.307816, 450.0]
[2019-03-08 09:54:48,354] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.7043552e-15 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:54:48,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4175
[2019-03-08 09:54:48,366] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.4, 48.33333333333333, 1.0, 2.0, 0.4096662799577363, 1.0, 2.0, 0.4096662799577363, 0.0, 2.0, 0.0, 6.9112, 6.9112, 632767.8297292019, 632767.8297292019, 115159.8554403592], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7141800.0000, 
sim time next is 7142400.0000, 
raw observation next is [24.4, 48.0, 1.0, 2.0, 0.4492413103276612, 1.0, 2.0, 0.4492413103276612, 0.0, 2.0, 0.0, 6.9112, 6.9112, 693988.3083263017, 693988.3083263017, 122905.7724518341], 
processed observation next is [1.0, 0.6956521739130435, 0.7454545454545454, 0.48, 1.0, 1.0, 0.4492413103276612, 1.0, 1.0, 0.4492413103276612, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2891617951359591, 0.2891617951359591, 0.30726443112958524], 
reward next is 0.8066, 
noisyNet noise sample is [array([-0.00841307], dtype=float32), 0.22308965]. 
=============================================
[2019-03-08 09:54:48,583] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 09:54:48,590] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7088
[2019-03-08 09:54:48,595] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.7, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8454364489333493, 6.9112, 6.9112, 343707.2545217108, 343707.2545217108, 93994.46146987373], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7102800.0000, 
sim time next is 7103400.0000, 
raw observation next is [17.7, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8461477289607576, 6.9112, 6.9112, 343996.8032308652, 343996.8032308652, 94054.9737979312], 
processed observation next is [1.0, 0.21739130434782608, 0.44090909090909086, 0.97, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8461477289607576, 0.0, 0.0, 0.14333200134619384, 0.14333200134619384, 0.23513743449482802], 
reward next is 0.5225, 
noisyNet noise sample is [array([-0.26776403], dtype=float32), -1.7787863]. 
=============================================
[2019-03-08 09:54:52,927] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.757117e-12 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-08 09:54:52,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2480
[2019-03-08 09:54:52,940] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.0, 43.0, 1.0, 2.0, 0.4659489319430531, 1.0, 2.0, 0.4659489319430531, 0.0, 2.0, 0.0, 6.9112, 6.9112, 723609.4269579346, 723609.4269579346, 125104.2409330596], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7224000.0000, 
sim time next is 7224600.0000, 
raw observation next is [23.9, 43.0, 1.0, 2.0, 0.4628559561547962, 1.0, 2.0, 0.4628559561547962, 0.0, 2.0, 0.0, 6.9112, 6.9112, 719318.5385483037, 719318.5385483037, 124274.0600649632], 
processed observation next is [1.0, 0.6086956521739131, 0.7227272727272727, 0.43, 1.0, 1.0, 0.4628559561547962, 1.0, 1.0, 0.4628559561547962, 0.0, 1.0, 0.0, 0.0, 0.0, 0.29971605772845983, 0.29971605772845983, 0.310685150162408], 
reward next is 0.8269, 
noisyNet noise sample is [array([-1.2776228], dtype=float32), -0.5016237]. 
=============================================
[2019-03-08 09:54:57,935] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0229991e-02 4.4618160e-22 6.8808124e-24 9.5977002e-01 6.1031087e-25], sum to 1.0000
[2019-03-08 09:54:57,948] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7384
[2019-03-08 09:54:57,955] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.9, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5976572633915753, 6.911200000000001, 6.9112, 242897.8170358462, 242897.8170358462, 67687.98808400569], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7260000.0000, 
sim time next is 7260600.0000, 
raw observation next is [15.45, 87.5, 1.0, 1.0, 0.1912303222113943, 1.0, 1.0, 0.1912303222113943, 0.0, 1.0, 0.0, 6.9112, 6.9112, 299176.6682854344, 299176.6682854344, 77826.639052674], 
processed observation next is [1.0, 0.0, 0.3386363636363636, 0.875, 1.0, 0.5, 0.1912303222113943, 1.0, 0.5, 0.1912303222113943, 0.0, 0.5, 0.0, 0.0, 0.0, 0.12465694511893101, 0.12465694511893101, 0.194566597631685], 
reward next is 0.5492, 
noisyNet noise sample is [array([1.5343198], dtype=float32), 0.49605733]. 
=============================================
[2019-03-08 09:54:58,629] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.0611693e-12 8.8454788e-27 1.8523476e-25 1.0000000e+00 3.0059527e-28], sum to 1.0000
[2019-03-08 09:54:58,635] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5198
[2019-03-08 09:54:58,642] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 77.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 174865.2602397565, 174865.2602397565, 52540.2689465852], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7272000.0000, 
sim time next is 7272600.0000, 
raw observation next is [14.3, 78.16666666666667, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 174966.8550003265, 174966.8550003265, 52592.97142770077], 
processed observation next is [1.0, 0.17391304347826086, 0.2863636363636364, 0.7816666666666667, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.07290285625013604, 0.07290285625013604, 0.13148242856925194], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3997363], dtype=float32), 0.54274374]. 
=============================================
[2019-03-08 09:55:04,551] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.157963e-26 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-08 09:55:04,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3712
[2019-03-08 09:55:04,564] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.8, 51.66666666666667, 1.0, 2.0, 0.8545107192650213, 1.0, 2.0, 0.8545107192650213, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 1262163.027690373, 1262163.027690373, 264679.1031482504], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7395600.0000, 
sim time next is 7396200.0000, 
raw observation next is [28.8, 52.0, 1.0, 2.0, 0.8858280616592585, 1.0, 2.0, 0.8858280616592585, 0.0, 2.0, 0.0, 6.9112, 6.9112, 1306796.040973113, 1306796.040973113, 277637.73618679], 
processed observation next is [1.0, 0.6086956521739131, 0.9454545454545454, 0.52, 1.0, 1.0, 0.8858280616592585, 1.0, 1.0, 0.8858280616592585, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5444983504054637, 0.5444983504054637, 0.6940943404669749], 
reward next is 0.6724, 
noisyNet noise sample is [array([0.45827675], dtype=float32), 0.8366477]. 
=============================================
[2019-03-08 09:55:07,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7830972e-34 0.0000000e+00], sum to 1.0000
[2019-03-08 09:55:07,255] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2939
[2019-03-08 09:55:07,262] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.45, 96.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8774271248982533, 6.9112, 6.9112, 356730.6932819557, 356730.6932819557, 96721.23536199192], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7435800.0000, 
sim time next is 7436400.0000, 
raw observation next is [17.36666666666667, 96.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8682428864090417, 6.9112, 6.9112, 352991.6465588163, 352991.6465588163, 95937.05266780026], 
processed observation next is [0.0, 0.043478260869565216, 0.42575757575757595, 0.9633333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8682428864090417, 0.0, 0.0, 0.14707985273284013, 0.14707985273284013, 0.23984263166950065], 
reward next is 0.5256, 
noisyNet noise sample is [array([-0.01726055], dtype=float32), 0.6617305]. 
=============================================
[2019-03-08 09:55:08,570] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8942735e-31 0.0000000e+00], sum to 1.0000
[2019-03-08 09:55:08,580] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6837
[2019-03-08 09:55:08,583] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.2, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8271946693930327, 6.9112, 6.9112, 336281.5894669195, 336281.5894669195, 92443.77228809727], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7444200.0000, 
sim time next is 7444800.0000, 
raw observation next is [17.2, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8236712381782141, 6.9112, 6.9112, 334847.3582330802, 334847.3582330802, 92144.80028457814], 
processed observation next is [0.0, 0.17391304347826086, 0.41818181818181815, 0.96, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8236712381782141, 0.0, 0.0, 0.13951973259711675, 0.13951973259711675, 0.23036200071144536], 
reward next is 0.5191, 
noisyNet noise sample is [array([0.25035727], dtype=float32), -2.5909004]. 
=============================================
[2019-03-08 09:55:24,718] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.2216838e-06 1.3769282e-34 6.6443110e-34 9.9999475e-01 3.6736671e-33], sum to 1.0000
[2019-03-08 09:55:24,730] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6604
[2019-03-08 09:55:24,735] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.53333333333333, 88.0, 1.0, 2.0, 0.3069239463098196, 1.0, 2.0, 0.3069239463098196, 0.0, 2.0, 0.0, 6.9112, 6.9112, 477296.213741486, 477296.213741486, 96588.09310396359], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7717200.0000, 
sim time next is 7717800.0000, 
raw observation next is [17.61666666666667, 86.0, 1.0, 2.0, 0.3109666355041926, 1.0, 2.0, 0.3109666355041926, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 483789.0624012344, 483789.0624012343, 97102.53758963701], 
processed observation next is [1.0, 0.30434782608695654, 0.4371212121212123, 0.86, 1.0, 1.0, 0.3109666355041926, 1.0, 1.0, 0.3109666355041926, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.20157877600051433, 0.2015787760005143, 0.24275634397409251], 
reward next is 0.7117, 
noisyNet noise sample is [array([-0.78196335], dtype=float32), 0.6148836]. 
=============================================
[2019-03-08 09:55:25,213] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.7159177e-24 0.0000000e+00], sum to 1.0000
[2019-03-08 09:55:25,224] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6962
[2019-03-08 09:55:25,244] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.36666666666667, 56.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 10.04448588011503, 6.9112, 726825.3929067896, 388369.1060326987, 101712.2102884382], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7733400.0000, 
sim time next is 7734000.0000, 
raw observation next is [21.63333333333334, 55.00000000000001, 1.0, 1.0, 0.5512124537817946, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 6.911199999999999, 6.9112, 816442.6485562118, 816442.648556212, 170220.1959521685], 
processed observation next is [1.0, 0.5217391304347826, 0.61969696969697, 0.55, 1.0, 0.5, 0.5512124537817946, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, -8.881784197001253e-17, 0.0, 0.3401844368984216, 0.34018443689842165, 0.4255504898804212], 
reward next is 0.6852, 
noisyNet noise sample is [array([0.13599303], dtype=float32), -0.26112545]. 
=============================================
[2019-03-08 09:55:25,254] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[48.27284 ]
 [47.999554]
 [47.908817]
 [48.69258 ]
 [49.60057 ]], R is [[49.96539307]
 [49.4657402 ]
 [49.48402023]
 [49.48524094]
 [48.99039078]].
[2019-03-08 09:55:29,037] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9621552e-01 7.5197961e-32 8.3563167e-35 3.7845229e-03 1.7683809e-38], sum to 1.0000
[2019-03-08 09:55:29,046] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8558
[2019-03-08 09:55:29,054] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [13.3, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4453060470448597, 6.911200000000001, 6.9112, 180929.2978024752, 180929.2978024751, 51083.16061969151], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7797600.0000, 
sim time next is 7798200.0000, 
raw observation next is [13.48333333333333, 95.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4532297029927342, 6.911199999999999, 6.9112, 184151.5651829136, 184151.5651829136, 52015.77624586289], 
processed observation next is [1.0, 0.2608695652173913, 0.24924242424242413, 0.955, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4532297029927342, -8.881784197001253e-17, 0.0, 0.07672981882621399, 0.07672981882621399, 0.13003944061465722], 
reward next is 0.5058, 
noisyNet noise sample is [array([0.6816232], dtype=float32), 0.21339355]. 
=============================================
[2019-03-08 09:55:29,296] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.368172e-32 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-08 09:55:29,305] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2513
[2019-03-08 09:55:29,311] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.03333333333333, 85.0, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 6.9112, 6.9112, 222699.5469687973, 222699.5469687973, 61609.31123825414], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7791600.0000, 
sim time next is 7792200.0000, 
raw observation next is [13.85, 87.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 180660.5954635322, 180660.5954635322, 54370.09010003586], 
processed observation next is [1.0, 0.17391304347826086, 0.2659090909090909, 0.87, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.07527524810980508, 0.07527524810980508, 0.13592522525008965], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6014765], dtype=float32), -0.09527667]. 
=============================================
[2019-03-08 09:55:30,706] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0000000e+00 9.8728574e-26 2.9061559e-27 2.0332742e-09 5.8876857e-27], sum to 1.0000
[2019-03-08 09:55:30,712] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3387
[2019-03-08 09:55:30,718] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.3, 48.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8952479071973816, 6.911199999999999, 6.9112, 363986.1180162246, 363986.1180162247, 98244.13850297629], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7819200.0000, 
sim time next is 7819800.0000, 
raw observation next is [23.38333333333333, 47.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 8.031808208588483, 6.9112, 486346.0508011053, 388238.5217950017, 102966.0408677538], 
processed observation next is [1.0, 0.5217391304347826, 0.6992424242424241, 0.475, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 0.11206082085884832, 0.0, 0.20264418783379387, 0.16176605074791736, 0.2574151021693845], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6264733], dtype=float32), 1.9821353]. 
=============================================
[2019-03-08 09:55:30,959] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 1.9758762e-36 0.0000000e+00 1.8883924e-32 0.0000000e+00], sum to 1.0000
[2019-03-08 09:55:30,969] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7833
[2019-03-08 09:55:30,976] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.33333333333334, 63.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7034620537177315, 6.9112, 6.9112, 285949.5125598348, 285949.5125598348, 81571.70564030245], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7849200.0000, 
sim time next is 7849800.0000, 
raw observation next is [20.25, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7025159623774047, 6.9112, 6.9112, 285564.4824695307, 285564.4824695307, 81184.02879287773], 
processed observation next is [1.0, 0.8695652173913043, 0.5568181818181818, 0.63, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7025159623774047, 0.0, 0.0, 0.11898520102897113, 0.11898520102897113, 0.20296007198219432], 
reward next is 0.5025, 
noisyNet noise sample is [array([2.0272381], dtype=float32), 0.079818204]. 
=============================================
[2019-03-08 09:55:35,348] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.3276851e-03 7.7728554e-35 1.9214532e-34 9.9267232e-01 8.0112415e-36], sum to 1.0000
[2019-03-08 09:55:35,354] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2324
[2019-03-08 09:55:35,361] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.2, 80.0, 1.0, 2.0, 0.2239202685505836, 1.0, 2.0, 0.2239202685505836, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 347351.1120117097, 347351.1120117098, 85980.09180528678], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7946400.0000, 
sim time next is 7947000.0000, 
raw observation next is [19.1, 79.5, 1.0, 2.0, 0.2183049314197597, 1.0, 2.0, 0.2183049314197597, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 338957.3401677141, 338957.3401677142, 85189.49433187624], 
processed observation next is [1.0, 1.0, 0.5045454545454546, 0.795, 1.0, 1.0, 0.2183049314197597, 1.0, 1.0, 0.2183049314197597, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.14123222506988087, 0.14123222506988092, 0.2129737358296906], 
reward next is 0.5684, 
noisyNet noise sample is [array([0.04347197], dtype=float32), -0.2836479]. 
=============================================
[2019-03-08 09:55:35,385] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[52.269535]
 [52.472637]
 [52.685215]
 [53.25023 ]
 [53.74882 ]], R is [[52.11837769]
 [52.17432404]
 [52.23955154]
 [52.31653595]
 [52.40694427]].
[2019-03-08 09:55:36,268] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:36,270] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:36,289] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-03-08 09:55:36,500] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:36,501] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:36,544] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-03-08 09:55:37,646] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:37,647] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:37,654] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:37,654] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:37,662] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:37,663] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:37,666] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-03-08 09:55:37,689] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-03-08 09:55:37,708] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-03-08 09:55:37,753] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:37,754] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:37,763] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-03-08 09:55:37,829] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:37,830] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:37,835] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-03-08 09:55:37,876] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:37,876] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:37,883] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-03-08 09:55:37,987] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:37,988] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:37,996] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-03-08 09:55:38,034] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:38,034] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:38,035] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:38,036] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:38,039] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-03-08 09:55:38,041] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:38,041] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:38,068] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-03-08 09:55:38,087] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:38,087] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:38,088] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-03-08 09:55:38,113] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:38,115] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:38,118] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-03-08 09:55:38,140] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:38,142] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:38,142] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-08 09:55:38,143] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:55:38,144] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-03-08 09:55:38,163] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-03-08 09:55:38,187] EPLUS_ENV_Part3-NA-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-03-08 09:55:44,753] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2304177e-22 0.0000000e+00], sum to 1.0000
[2019-03-08 09:55:44,760] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1904
[2019-03-08 09:55:44,766] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [13.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3931037409003761, 6.9112, 6.9112, 159702.9911989571, 159702.9911989571, 44502.65487428222], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 103800.0000, 
sim time next is 104400.0000, 
raw observation next is [13.0, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3924220862815433, 6.9112, 6.9112, 159425.84809314, 159425.84809314, 44355.82877867598], 
processed observation next is [1.0, 0.21739130434782608, 0.22727272727272727, 0.82, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3924220862815433, 0.0, 0.0, 0.066427436705475, 0.066427436705475, 0.11088957194668995], 
reward next is 0.5135, 
noisyNet noise sample is [array([-0.9589859], dtype=float32), 0.028758401]. 
=============================================
[2019-03-08 09:55:55,653] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3830906e-27 0.0000000e+00], sum to 1.0000
[2019-03-08 09:55:55,661] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1967
[2019-03-08 09:55:55,669] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.83333333333334, 57.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.52629678474397, 6.9112, 6.9112, 213870.05591249, 213870.05591249, 62014.80011184017], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 301800.0000, 
sim time next is 302400.0000, 
raw observation next is [20.0, 56.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5255324283317482, 6.911199999999999, 6.9112, 213559.1259824478, 213559.1259824479, 61832.53433136403], 
processed observation next is [0.0, 0.5217391304347826, 0.5454545454545454, 0.56, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5255324283317482, -8.881784197001253e-17, 0.0, 0.08898296915935325, 0.08898296915935329, 0.15458133582841008], 
reward next is 0.4934, 
noisyNet noise sample is [array([-1.5738041], dtype=float32), 0.33823866]. 
=============================================
[2019-03-08 09:55:58,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 2.234327e-22 0.000000e+00], sum to 1.0000
[2019-03-08 09:55:58,267] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0955
[2019-03-08 09:55:58,274] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.66666666666667, 43.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5418248332953703, 6.9112, 6.9112, 220185.9254999337, 220185.9254999337, 60311.46764682449], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 325200.0000, 
sim time next is 325800.0000, 
raw observation next is [20.5, 43.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5362281208749259, 6.911199999999999, 6.9112, 217909.4924644073, 217909.4924644073, 59591.86725036109], 
processed observation next is [0.0, 0.782608695652174, 0.5681818181818182, 0.43, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5362281208749259, -8.881784197001253e-17, 0.0, 0.0907956218601697, 0.0907956218601697, 0.14897966812590271], 
reward next is 0.5224, 
noisyNet noise sample is [array([-0.24946307], dtype=float32), 1.2370399]. 
=============================================
[2019-03-08 09:55:59,493] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 09:55:59,502] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5353
[2019-03-08 09:55:59,510] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [12.33333333333333, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2865394012662104, 6.9112, 6.9112, 116385.6842949622, 116385.6842949622, 33792.19611404248], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 355200.0000, 
sim time next is 355800.0000, 
raw observation next is [12.16666666666667, 74.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2818246709788179, 6.9112, 6.9112, 114469.6129225573, 114469.6129225573, 33363.0620034706], 
processed observation next is [1.0, 0.08695652173913043, 0.18939393939393953, 0.745, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2818246709788179, 0.0, 0.0, 0.04769567205106554, 0.04769567205106554, 0.0834076550086765], 
reward next is 0.4901, 
noisyNet noise sample is [array([-0.95008904], dtype=float32), 1.4368776]. 
=============================================
[2019-03-08 09:56:03,034] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 4.3978721e-38 9.7059178e-37 2.9115746e-32 0.0000000e+00], sum to 1.0000
[2019-03-08 09:56:03,049] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6086
[2019-03-08 09:56:03,055] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.0, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4530488058371174, 6.9112, 6.9112, 184077.9996643955, 184077.9996643955, 51976.63533024309], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 414000.0000, 
sim time next is 414600.0000, 
raw observation next is [16.0, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4449786092832717, 6.9112, 6.9112, 180796.1427354204, 180796.1427354204, 51243.0629900699], 
processed observation next is [1.0, 0.8260869565217391, 0.36363636363636365, 0.72, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4449786092832717, 0.0, 0.0, 0.0753317261397585, 0.0753317261397585, 0.12810765747517475], 
reward next is 0.5040, 
noisyNet noise sample is [array([-1.0133893], dtype=float32), 0.37243378]. 
=============================================
[2019-03-08 09:56:13,881] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 09:56:13,888] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8614
[2019-03-08 09:56:13,890] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7124621629903298, 6.9112, 6.9112, 289587.2331553659, 289587.2331553659, 82777.43996159462], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 603600.0000, 
sim time next is 604200.0000, 
raw observation next is [18.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7131092091775599, 6.9112, 6.9112, 289850.5240188264, 289850.5240188264, 82831.54730488826], 
processed observation next is [1.0, 1.0, 0.45454545454545453, 0.83, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7131092091775599, 0.0, 0.0, 0.120771051674511, 0.120771051674511, 0.20707886826222063], 
reward next is 0.4999, 
noisyNet noise sample is [array([0.14014128], dtype=float32), -0.81398314]. 
=============================================
[2019-03-08 09:56:27,241] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.2332969e-17 6.9752156e-31 5.3768033e-31 1.0000000e+00 1.2056270e-31], sum to 1.0000
[2019-03-08 09:56:27,251] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3697
[2019-03-08 09:56:27,256] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.5, 63.5, 1.0, 2.0, 0.3660079830159792, 1.0, 2.0, 0.3660079830159792, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 545571.7350194019, 545571.735019402, 111309.315541137], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 847800.0000, 
sim time next is 848400.0000, 
raw observation next is [26.33333333333334, 64.0, 1.0, 2.0, 0.3580567105623235, 1.0, 2.0, 0.3580567105623235, 0.0, 2.0, 0.0, 6.9112, 6.9112, 534135.2201520834, 534135.2201520834, 109809.9943819927], 
processed observation next is [0.0, 0.8260869565217391, 0.8333333333333336, 0.64, 1.0, 1.0, 0.3580567105623235, 1.0, 1.0, 0.3580567105623235, 0.0, 1.0, 0.0, 0.0, 0.0, 0.22255634173003477, 0.22255634173003477, 0.27452498595498176], 
reward next is 0.6949, 
noisyNet noise sample is [array([-0.2650555], dtype=float32), -0.5293261]. 
=============================================
[2019-03-08 09:56:31,764] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0366652e-11 0.0000000e+00], sum to 1.0000
[2019-03-08 09:56:31,775] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6974
[2019-03-08 09:56:31,786] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.0, 71.0, 1.0, 2.0, 0.4237334728197477, 1.0, 2.0, 0.4237334728197477, 1.0, 2.0, 0.7640065827832269, 6.911199999999999, 6.9112, 931716.1467792974, 931716.1467792976, 199857.992376506], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 906000.0000, 
sim time next is 906600.0000, 
raw observation next is [25.5, 68.0, 1.0, 2.0, 0.3371301032716242, 1.0, 2.0, 0.3371301032716242, 1.0, 2.0, 0.6086111733553179, 6.9112, 6.9112, 742025.0556361905, 742025.0556361905, 170454.9991544621], 
processed observation next is [0.0, 0.4782608695652174, 0.7954545454545454, 0.68, 1.0, 1.0, 0.3371301032716242, 1.0, 1.0, 0.3371301032716242, 1.0, 1.0, 0.6086111733553179, 0.0, 0.0, 0.30917710651507935, 0.30917710651507935, 0.42613749788615524], 
reward next is 0.6219, 
noisyNet noise sample is [array([2.229256], dtype=float32), 0.18981071]. 
=============================================
[2019-03-08 09:56:35,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0863945e-02 2.4983850e-23 2.4559896e-27 9.7913605e-01 1.1466764e-28], sum to 1.0000
[2019-03-08 09:56:35,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8309
[2019-03-08 09:56:35,525] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.33333333333333, 100.0, 1.0, 2.0, 0.3356920220256466, 1.0, 2.0, 0.3356920220256466, 0.0, 2.0, 0.0, 6.9112, 6.9112, 521259.2277965112, 521259.2277965112, 101285.9602719292], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 988800.0000, 
sim time next is 989400.0000, 
raw observation next is [16.16666666666667, 100.0, 1.0, 2.0, 0.3276874009830344, 1.0, 2.0, 0.3276874009830344, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 509363.8467743148, 509363.8467743147, 99820.28430010228], 
processed observation next is [1.0, 0.43478260869565216, 0.37121212121212144, 1.0, 1.0, 1.0, 0.3276874009830344, 1.0, 1.0, 0.3276874009830344, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.2122349361559645, 0.21223493615596445, 0.2495507107502557], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.29942822], dtype=float32), 0.98978424]. 
=============================================
[2019-03-08 09:56:38,263] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2825964e-34 0.0000000e+00], sum to 1.0000
[2019-03-08 09:56:38,277] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7889
[2019-03-08 09:56:38,281] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [12.16666666666667, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3727733611615918, 6.9112, 6.9112, 151437.5040417837, 151437.5040417837, 43394.41078352774], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1039800.0000, 
sim time next is 1040400.0000, 
raw observation next is [12.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3666304027692445, 6.911199999999999, 6.9112, 148940.1621927722, 148940.1621927722, 42748.11488700129], 
processed observation next is [1.0, 0.043478260869565216, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3666304027692445, -8.881784197001253e-17, 0.0, 0.06205840091365508, 0.06205840091365508, 0.10687028721750323], 
reward next is 0.4977, 
noisyNet noise sample is [array([-0.5731127], dtype=float32), 0.7411579]. 
=============================================
[2019-03-08 09:56:39,142] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9999881e-01 0.0000000e+00 0.0000000e+00 1.1811587e-06 0.0000000e+00], sum to 1.0000
[2019-03-08 09:56:39,156] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7638
[2019-03-08 09:56:39,174] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.33333333333334, 83.33333333333334, 1.0, 2.0, 0.6618261322843036, 1.0, 1.0, 0.6618261322843036, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 980163.2880878424, 980163.2880878425, 190195.4239415608], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1244400.0000, 
sim time next is 1245000.0000, 
raw observation next is [23.66666666666666, 80.66666666666666, 1.0, 2.0, 0.6742133094667059, 1.0, 2.0, 0.6742133094667059, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 999257.9827094146, 999257.9827094143, 194750.3035736045], 
processed observation next is [1.0, 0.391304347826087, 0.7121212121212118, 0.8066666666666665, 1.0, 1.0, 0.6742133094667059, 1.0, 1.0, 0.6742133094667059, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.4163574927955894, 0.4163574927955893, 0.48687575893401125], 
reward next is 0.7330, 
noisyNet noise sample is [array([-0.6158219], dtype=float32), 2.8394265]. 
=============================================
[2019-03-08 09:56:39,182] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[61.689457]
 [60.643444]
 [62.354473]
 [61.982864]
 [60.372826]], R is [[62.41031647]
 [61.78621292]
 [61.78862381]
 [61.9378891 ]
 [61.31851196]].
[2019-03-08 09:56:46,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 9.395999e-15 0.000000e+00], sum to 1.0000
[2019-03-08 09:56:46,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9795
[2019-03-08 09:56:46,378] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 66.0, 1.0, 2.0, 0.77998860614283, 1.0, 1.0, 0.77998860614283, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 1141936.24752356, 1141936.24752356, 236926.6319524545], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1173600.0000, 
sim time next is 1174200.0000, 
raw observation next is [27.0, 65.33333333333333, 1.0, 2.0, 0.8382086973211981, 1.0, 2.0, 0.8382086973211981, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 1226906.922248759, 1226906.92224876, 259636.1584030516], 
processed observation next is [1.0, 0.6086956521739131, 0.8636363636363636, 0.6533333333333333, 1.0, 1.0, 0.8382086973211981, 1.0, 1.0, 0.8382086973211981, 0.0, 1.0, 0.0, -1.7763568394002506e-16, 0.0, 0.5112112176036496, 0.51121121760365, 0.6490903960076291], 
reward next is 0.6751, 
noisyNet noise sample is [array([-1.2895136], dtype=float32), 0.57513094]. 
=============================================
[2019-03-08 09:56:59,678] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9999964e-01 4.4346823e-37 1.8030479e-35 3.0565957e-07 1.6680994e-36], sum to 1.0000
[2019-03-08 09:56:59,688] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6051
[2019-03-08 09:56:59,698] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.2371486857878257, 1.0, 2.0, 0.2371486857878257, 1.0, 2.0, 0.4325670288795977, 6.9112, 6.9112, 527224.8720576542, 527224.8720576542, 140385.9515509567], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1402800.0000, 
sim time next is 1403400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3376151361826166, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6202091010157876, 6.9112, 6.9112, 504107.5992547862, 504107.5992547862, 124528.4483529582], 
processed observation next is [0.0, 0.21739130434782608, 0.5909090909090909, 1.0, 1.0, 1.0, 0.3376151361826166, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6202091010157876, 0.0, 0.0, 0.2100448330228276, 0.2100448330228276, 0.3113211208823955], 
reward next is 0.5783, 
noisyNet noise sample is [array([0.01097584], dtype=float32), -0.73496705]. 
=============================================
[2019-03-08 09:57:01,728] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2872014e-18 5.3526729e-32 3.5076192e-35 1.0000000e+00 2.7455775e-36], sum to 1.0000
[2019-03-08 09:57:01,734] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8386
[2019-03-08 09:57:01,741] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.0, 80.5, 1.0, 2.0, 0.3763524815516044, 1.0, 2.0, 0.3763524815516044, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 559450.3689562684, 559450.3689562686, 113437.9012465375], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1423800.0000, 
sim time next is 1424400.0000, 
raw observation next is [24.0, 79.66666666666667, 1.0, 2.0, 0.3659453484375202, 1.0, 2.0, 0.3659453484375202, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 544728.8482867054, 544728.8482867056, 111406.2564840917], 
processed observation next is [0.0, 0.4782608695652174, 0.7272727272727273, 0.7966666666666667, 1.0, 1.0, 0.3659453484375202, 1.0, 1.0, 0.3659453484375202, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.22697035345279393, 0.22697035345279398, 0.27851564121022926], 
reward next is 0.6985, 
noisyNet noise sample is [array([1.6396326], dtype=float32), -0.74013233]. 
=============================================
[2019-03-08 09:57:02,170] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 6.6188763e-25 5.0266550e-28 2.6196158e-23 1.1784549e-29], sum to 1.0000
[2019-03-08 09:57:02,181] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5325
[2019-03-08 09:57:02,199] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.4074853041154001, 1.0, 2.0, 0.4074853041154001, 1.0, 1.0, 0.7300831679373371, 6.911199999999999, 6.9112, 890297.8026106656, 890297.8026106657, 194138.9564836972], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1584000.0000, 
sim time next is 1584600.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.4444333127743686, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8093024696204416, 6.911199999999999, 6.9112, 657997.2777314273, 657997.2777314275, 150993.2982648286], 
processed observation next is [1.0, 0.34782608695652173, 0.6363636363636364, 0.9400000000000002, 1.0, 1.0, 0.4444333127743686, 0.0, 0.5, 0.0, 1.0, 1.0, 0.8093024696204416, -8.881784197001253e-17, 0.0, 0.2741655323880947, 0.27416553238809477, 0.3774832456620715], 
reward next is 0.6225, 
noisyNet noise sample is [array([0.23273212], dtype=float32), -1.5127339]. 
=============================================
[2019-03-08 09:57:17,931] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 6.2677596e-37 0.0000000e+00 1.5785968e-28 0.0000000e+00], sum to 1.0000
[2019-03-08 09:57:17,937] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2086
[2019-03-08 09:57:17,944] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [13.83333333333333, 63.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4367864739272953, 6.9112, 6.9112, 177464.8039917276, 177464.8039917276, 47504.4393018059], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1714200.0000, 
sim time next is 1714800.0000, 
raw observation next is [13.66666666666667, 64.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4265197962526689, 6.9112, 6.9112, 173290.0020401753, 173290.0020401753, 46534.99784897462], 
processed observation next is [1.0, 0.8695652173913043, 0.25757575757575774, 0.6433333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4265197962526689, 0.0, 0.0, 0.07220416751673972, 0.07220416751673972, 0.11633749462243655], 
reward next is 0.5320, 
noisyNet noise sample is [array([-0.7972577], dtype=float32), 0.48608035]. 
=============================================
[2019-03-08 09:57:20,146] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.0110613e-32 2.2569886e-34 6.5496829e-24 9.1483468e-38], sum to 1.0000
[2019-03-08 09:57:20,150] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2137
[2019-03-08 09:57:20,157] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [9.0, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2546178192047517, 6.9112, 6.9112, 103413.4110816798, 103413.4110816798, 29851.13548322799], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1733400.0000, 
sim time next is 1734000.0000, 
raw observation next is [9.0, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2533537788268214, 6.9112, 6.9112, 102899.7645121076, 102899.7645121076, 29738.70311454919], 
processed observation next is [1.0, 0.043478260869565216, 0.045454545454545456, 0.71, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2533537788268214, 0.0, 0.0, 0.042874901880044836, 0.042874901880044836, 0.07434675778637297], 
reward next is 0.4943, 
noisyNet noise sample is [array([3.1248238], dtype=float32), 1.649425]. 
=============================================
[2019-03-08 09:57:20,191] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[41.10775 ]
 [41.10294 ]
 [41.09366 ]
 [41.075676]
 [41.131863]], R is [[41.19403076]
 [41.27698898]
 [41.3599968 ]
 [41.44383621]
 [41.53018951]].
[2019-03-08 09:57:25,633] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 9.026974e-32 0.000000e+00], sum to 1.0000
[2019-03-08 09:57:25,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0894
[2019-03-08 09:57:25,651] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.0, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5429549769570391, 6.9112, 6.9112, 220645.6106306437, 220645.6106306437, 61785.75254508023], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2002200.0000, 
sim time next is 2002800.0000, 
raw observation next is [17.0, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5428931091501641, 6.9112, 6.9112, 220620.4458989674, 220620.4458989674, 61772.23768073304], 
processed observation next is [0.0, 0.17391304347826086, 0.4090909090909091, 0.72, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5428931091501641, 0.0, 0.0, 0.09192518579123642, 0.09192518579123642, 0.15443059420183258], 
reward next is 0.5102, 
noisyNet noise sample is [array([-0.6770255], dtype=float32), -0.35718784]. 
=============================================
[2019-03-08 09:57:32,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1409862e-01 4.5846892e-34 0.0000000e+00 8.5901387e-02 0.0000000e+00], sum to 1.0000
[2019-03-08 09:57:32,270] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9673
[2019-03-08 09:57:32,276] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 61.0, 1.0, 2.0, 0.6669682232261475, 1.0, 1.0, 0.6669682232261475, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 995735.1707501883, 995735.1707501882, 192649.5665607967], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1948200.0000, 
sim time next is 1948800.0000, 
raw observation next is [26.0, 61.0, 1.0, 2.0, 0.6894602409153624, 1.0, 2.0, 0.6894602409153624, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1029823.201266172, 1029823.201266172, 202082.7146006964], 
processed observation next is [1.0, 0.5652173913043478, 0.8181818181818182, 0.61, 1.0, 1.0, 0.6894602409153624, 1.0, 1.0, 0.6894602409153624, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.42909300052757166, 0.42909300052757166, 0.505206786501741], 
reward next is 0.7280, 
noisyNet noise sample is [array([1.295444], dtype=float32), 2.2153318]. 
=============================================
[2019-03-08 09:57:33,823] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 2.1024403e-31 2.4572547e-36 3.5759292e-19 0.0000000e+00], sum to 1.0000
[2019-03-08 09:57:33,832] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9220
[2019-03-08 09:57:33,845] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9547116395713053, 7.105805662071615, 6.9112, 402756.7925864523, 388220.3589956423, 103017.1260555404], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2148000.0000, 
sim time next is 2148600.0000, 
raw observation next is [23.0, 69.0, 1.0, 1.0, 0.5333224076149006, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 410534.0667920238, 410534.0667920238, 74549.66240143371], 
processed observation next is [0.0, 0.8695652173913043, 0.6818181818181818, 0.69, 1.0, 0.5, 0.5333224076149006, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.17105586116334323, 0.17105586116334323, 0.18637415600358428], 
reward next is 0.7867, 
noisyNet noise sample is [array([1.4028718], dtype=float32), 0.70397836]. 
=============================================
[2019-03-08 09:57:39,471] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0000000e+00 1.6714516e-27 1.3764386e-31 3.6532509e-20 3.4341118e-35], sum to 1.0000
[2019-03-08 09:57:39,481] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8668
[2019-03-08 09:57:39,487] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.0, 78.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5223741808113835, 6.911200000000001, 6.9112, 212274.4031708283, 212274.4031708282, 59577.37421978131], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2079600.0000, 
sim time next is 2080200.0000, 
raw observation next is [16.0, 77.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5138590122924075, 6.911200000000001, 6.9112, 208810.6529438056, 208810.6529438055, 58633.40374927539], 
processed observation next is [0.0, 0.043478260869565216, 0.36363636363636365, 0.7783333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5138590122924075, 8.881784197001253e-17, 0.0, 0.08700443872658566, 0.08700443872658563, 0.14658350937318848], 
reward next is 0.5088, 
noisyNet noise sample is [array([-0.16255796], dtype=float32), 2.3461487]. 
=============================================
[2019-03-08 09:57:39,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 5.5188490e-35 0.0000000e+00 1.7073025e-10 0.0000000e+00], sum to 1.0000
[2019-03-08 09:57:40,007] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0805
[2019-03-08 09:57:40,012] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.0, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4435482383028286, 6.9112, 6.9112, 180214.4734554708, 180214.4734554708, 51421.19282595206], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2094000.0000, 
sim time next is 2094600.0000, 
raw observation next is [14.0, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4510985008386035, 6.9112, 6.9112, 183284.8716680682, 183284.8716680682, 52282.19884645707], 
processed observation next is [0.0, 0.21739130434782608, 0.2727272727272727, 0.93, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4510985008386035, 0.0, 0.0, 0.07636869652836174, 0.07636869652836174, 0.13070549711614268], 
reward next is 0.5008, 
noisyNet noise sample is [array([2.4321916], dtype=float32), -0.122699946]. 
=============================================
[2019-03-08 09:57:42,506] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 09:57:42,515] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9677
[2019-03-08 09:57:42,518] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.7, 51.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9089127528051858, 6.9112, 6.9112, 369590.7148652674, 369590.7148652674, 98707.8604260128], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2122800.0000, 
sim time next is 2123400.0000, 
raw observation next is [25.85, 51.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9186752649120103, 6.9112, 6.9112, 373566.5812376538, 373566.5812376538, 99531.82567676272], 
processed observation next is [0.0, 0.5652173913043478, 0.8113636363636364, 0.5116666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9186752649120103, 0.0, 0.0, 0.15565274218235575, 0.15565274218235575, 0.2488295641919068], 
reward next is 0.5362, 
noisyNet noise sample is [array([-0.11866431], dtype=float32), -0.8754115]. 
=============================================
[2019-03-08 09:57:50,484] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 3.5503000e-31 6.7403632e-33 1.1417263e-08 6.8168671e-36], sum to 1.0000
[2019-03-08 09:57:50,498] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5553
[2019-03-08 09:57:50,505] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [15.0, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4514467364553359, 6.911200000000001, 6.9112, 183426.487748137, 183426.4877481369, 49933.93995359411], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2273400.0000, 
sim time next is 2274000.0000, 
raw observation next is [15.33333333333333, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5190393308846779, 6.911200000000001, 6.9112, 210917.8587143322, 210917.8587143321, 55908.37906361281], 
processed observation next is [1.0, 0.30434782608695654, 0.3333333333333332, 0.65, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5190393308846779, 8.881784197001253e-17, 0.0, 0.08788244113097175, 0.08788244113097171, 0.13977094765903203], 
reward next is 0.5389, 
noisyNet noise sample is [array([-2.3581092], dtype=float32), 0.02857946]. 
=============================================
[2019-03-08 09:57:50,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[43.35871 ]
 [43.529537]
 [43.65427 ]
 [43.75853 ]
 [43.69114 ]], R is [[43.23699188]
 [43.32939148]
 [43.41085815]
 [43.48679733]
 [43.5602951 ]].
[2019-03-08 09:57:52,200] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.6498456e-02 2.2751624e-30 0.0000000e+00 9.2350155e-01 0.0000000e+00], sum to 1.0000
[2019-03-08 09:57:52,210] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2781
[2019-03-08 09:57:52,219] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.33333333333334, 58.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7275393326845415, 6.9112, 6.9112, 295748.6378836447, 295748.6378836447, 75835.42738161267], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2280000.0000, 
sim time next is 2280600.0000, 
raw observation next is [17.5, 57.5, 1.0, 1.0, 0.2454103038184719, 1.0, 1.0, 0.2454103038184719, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 384013.1577244821, 384013.1577244822, 81451.18149340386], 
processed observation next is [1.0, 0.391304347826087, 0.4318181818181818, 0.575, 1.0, 0.5, 0.2454103038184719, 1.0, 0.5, 0.2454103038184719, 0.0, 0.5, 0.0, -8.881784197001253e-17, 0.0, 0.16000548238520088, 0.1600054823852009, 0.20362795373350964], 
reward next is 0.6735, 
noisyNet noise sample is [array([0.79902333], dtype=float32), 0.17156851]. 
=============================================
[2019-03-08 09:57:54,279] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0038054e-10 3.9300029e-24 1.5423900e-25 1.0000000e+00 6.5176705e-27], sum to 1.0000
[2019-03-08 09:57:54,288] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3012
[2019-03-08 09:57:54,292] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.0, 53.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 205649.6158745836, 205649.6158745836, 57100.04684439913], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2328000.0000, 
sim time next is 2328600.0000, 
raw observation next is [17.0, 52.5, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 205570.2238335447, 205570.2238335447, 57016.8457778656], 
processed observation next is [1.0, 0.9565217391304348, 0.4090909090909091, 0.525, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.08565425993064363, 0.08565425993064363, 0.142542114444664], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9661396], dtype=float32), -0.9448659]. 
=============================================
[2019-03-08 09:57:55,193] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 8.1116733e-29 3.1116216e-36 6.5968958e-10 0.0000000e+00], sum to 1.0000
[2019-03-08 09:57:55,201] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7078
[2019-03-08 09:57:55,206] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [13.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3625734090835648, 6.9112, 6.9112, 147290.875830956, 147290.875830956, 42194.52056320726], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2354400.0000, 
sim time next is 2355000.0000, 
raw observation next is [13.5, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3660602912336511, 6.9112, 6.9112, 148708.3936268603, 148708.3936268603, 42746.25497029525], 
processed observation next is [1.0, 0.2608695652173913, 0.25, 0.8533333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3660602912336511, 0.0, 0.0, 0.061961830677858466, 0.061961830677858466, 0.10686563742573814], 
reward next is 0.4970, 
noisyNet noise sample is [array([-0.16075666], dtype=float32), 1.1704175]. 
=============================================
[2019-03-08 09:57:55,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[48.180176]
 [48.077583]
 [47.98654 ]
 [47.89122 ]
 [47.792168]], R is [[48.21829987]
 [48.23479843]
 [48.25148773]
 [48.26811981]
 [48.28520584]].
[2019-03-08 09:58:10,692] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 3.983916e-11 0.000000e+00], sum to 1.0000
[2019-03-08 09:58:10,698] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5399
[2019-03-08 09:58:10,706] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [15.66666666666667, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.669946603506998, 6.9112, 6.9112, 272288.3075728046, 272288.3075728046, 78825.6713188389], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2612400.0000, 
sim time next is 2613000.0000, 
raw observation next is [15.83333333333333, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6837981825266793, 6.9112, 6.9112, 277924.0492128195, 277924.0492128195, 80384.77070701018], 
processed observation next is [0.0, 0.21739130434782608, 0.3560606060606059, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6837981825266793, 0.0, 0.0, 0.11580168717200812, 0.11580168717200812, 0.20096192676752545], 
reward next is 0.4939, 
noisyNet noise sample is [array([-0.0335291], dtype=float32), 1.2963313]. 
=============================================
[2019-03-08 09:58:10,719] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[55.981567]
 [55.887783]
 [55.789143]
 [55.682587]
 [55.56356 ]], R is [[56.01799774]
 [55.95129013]
 [55.88718033]
 [55.82611847]
 [55.76900482]].
[2019-03-08 09:58:10,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0721304e-32 0.0000000e+00], sum to 1.0000
[2019-03-08 09:58:10,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3268
[2019-03-08 09:58:10,935] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.36666666666667, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6712882386227873, 6.911199999999999, 6.9112, 272834.1631159424, 272834.1631159424, 79343.40356936428], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2595000.0000, 
sim time next is 2595600.0000, 
raw observation next is [17.3, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6932584889663072, 6.9112, 6.9112, 281773.2698493629, 281773.2698493629, 81173.54489940012], 
processed observation next is [0.0, 0.043478260869565216, 0.4227272727272728, 0.88, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6932584889663072, 0.0, 0.0, 0.1174055291039012, 0.1174055291039012, 0.2029338622485003], 
reward next is 0.4959, 
noisyNet noise sample is [array([-0.4771456], dtype=float32), 0.95098424]. 
=============================================
[2019-03-08 09:58:12,064] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0000000e+00 2.0236827e-28 1.7425074e-29 1.4904761e-11 1.4939505e-31], sum to 1.0000
[2019-03-08 09:58:12,073] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1652
[2019-03-08 09:58:12,088] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.83333333333334, 42.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7978589800159107, 6.9112, 6.9112, 324340.8243043384, 324340.8243043384, 89959.20819840889], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2634600.0000, 
sim time next is 2635200.0000, 
raw observation next is [26.0, 42.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7963768273989419, 6.911199999999999, 6.9112, 323737.5596679035, 323737.5596679036, 89833.88295440456], 
processed observation next is [0.0, 0.5217391304347826, 0.8181818181818182, 0.42, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7963768273989419, -8.881784197001253e-17, 0.0, 0.13489064986162647, 0.1348906498616265, 0.22458470738601138], 
reward next is 0.5148, 
noisyNet noise sample is [array([-0.0230614], dtype=float32), -0.5497479]. 
=============================================
[2019-03-08 09:58:13,752] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5393875e-16 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:58:13,760] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3108
[2019-03-08 09:58:13,768] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.1, 51.5, 1.0, 2.0, 0.647329378860975, 1.0, 2.0, 0.647329378860975, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 957340.9899935393, 957340.9899935394, 182828.2039742444], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2820600.0000, 
sim time next is 2821200.0000, 
raw observation next is [29.13333333333333, 51.33333333333334, 1.0, 2.0, 0.7970301117719396, 1.0, 2.0, 0.7970301117719396, 0.0, 2.0, 0.0, 6.911200000000002, 6.9112, 1175981.005789834, 1175981.005789833, 242406.8291484939], 
processed observation next is [1.0, 0.6521739130434783, 0.9606060606060605, 0.5133333333333334, 1.0, 1.0, 0.7970301117719396, 1.0, 1.0, 0.7970301117719396, 0.0, 1.0, 0.0, 1.7763568394002506e-16, 0.0, 0.4899920857457642, 0.4899920857457637, 0.6060170728712347], 
reward next is 0.6930, 
noisyNet noise sample is [array([0.82916147], dtype=float32), 0.067174464]. 
=============================================
[2019-03-08 09:58:17,614] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.5258287e-36 3.9034347e-33 9.0082352e-15 5.7091740e-35], sum to 1.0000
[2019-03-08 09:58:17,621] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9682
[2019-03-08 09:58:17,634] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.16666666666667, 59.33333333333334, 1.0, 2.0, 0.2114311445788048, 1.0, 2.0, 0.2114311445788048, 1.0, 2.0, 0.3891927573478203, 6.911199999999999, 6.9112, 474320.7713491382, 474320.7713491384, 133162.784353098], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2736600.0000, 
sim time next is 2737200.0000, 
raw observation next is [26.33333333333334, 57.66666666666667, 1.0, 2.0, 0.2888271330336108, 1.0, 2.0, 0.2888271330336108, 0.0, 1.0, 0.0, 6.9112, 6.9112, 435585.7849308783, 435585.7849308783, 97562.34727997963], 
processed observation next is [0.0, 0.6956521739130435, 0.8333333333333336, 0.5766666666666667, 1.0, 1.0, 0.2888271330336108, 1.0, 1.0, 0.2888271330336108, 0.0, 0.5, 0.0, 0.0, 0.0, 0.18149407705453263, 0.18149407705453263, 0.24390586819994908], 
reward next is 0.6378, 
noisyNet noise sample is [array([0.8844289], dtype=float32), -1.5073886]. 
=============================================
[2019-03-08 09:58:18,123] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3824206e-10 6.8229980e-31 2.4945471e-32 1.0000000e+00 9.8070215e-33], sum to 1.0000
[2019-03-08 09:58:18,134] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9835
[2019-03-08 09:58:18,140] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 59.83333333333334, 1.0, 2.0, 0.295716343325941, 1.0, 2.0, 0.295716343325941, 0.0, 2.0, 0.0, 6.9112, 6.9112, 446826.8706549358, 446826.8706549358, 98445.53672259364], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2746200.0000, 
sim time next is 2746800.0000, 
raw observation next is [26.0, 61.0, 1.0, 2.0, 0.3014662422923429, 1.0, 2.0, 0.3014662422923429, 0.0, 2.0, 0.0, 6.9112, 6.9112, 454612.405537051, 454612.405537051, 99492.70557648531], 
processed observation next is [0.0, 0.8260869565217391, 0.8181818181818182, 0.61, 1.0, 1.0, 0.3014662422923429, 1.0, 1.0, 0.3014662422923429, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1894218356404379, 0.1894218356404379, 0.2487317639412133], 
reward next is 0.6528, 
noisyNet noise sample is [array([-1.0650403], dtype=float32), -0.28665525]. 
=============================================
[2019-03-08 09:58:18,605] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9602216e-01 3.0653595e-20 6.9174979e-26 3.9778980e-03 9.3151847e-27], sum to 1.0000
[2019-03-08 09:58:18,613] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6095
[2019-03-08 09:58:18,629] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.0, 61.0, 1.0, 2.0, 0.2829331311350869, 1.0, 2.0, 0.2829331311350869, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 425834.9063853861, 425834.9063853862, 96843.13781944397], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2746800.0000, 
sim time next is 2747400.0000, 
raw observation next is [25.5, 65.66666666666667, 1.0, 2.0, 0.3060024460951001, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5658530570777136, 6.911199999999999, 6.9112, 459903.0732842255, 459903.0732842256, 116160.8629054674], 
processed observation next is [0.0, 0.8260869565217391, 0.7954545454545454, 0.6566666666666667, 1.0, 1.0, 0.3060024460951001, 0.0, 0.5, 0.0, 1.0, 0.5, 0.5658530570777136, -8.881784197001253e-17, 0.0, 0.19162628053509395, 0.191626280535094, 0.29040215726366847], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.82402956], dtype=float32), 0.49551725]. 
=============================================
[2019-03-08 09:58:20,189] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.999548e-01 0.000000e+00 0.000000e+00 4.516193e-05 0.000000e+00], sum to 1.0000
[2019-03-08 09:58:20,199] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3652
[2019-03-08 09:58:20,205] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8232574479784001, 6.911200000000001, 6.9112, 334678.9237893148, 334678.9237893148, 92110.01512811439], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2772000.0000, 
sim time next is 2772600.0000, 
raw observation next is [18.83333333333334, 83.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8137334768258302, 6.9112, 6.9112, 330802.2252377734, 330802.2252377734, 91302.5376597719], 
processed observation next is [1.0, 0.08695652173913043, 0.4924242424242427, 0.8383333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8137334768258302, 0.0, 0.0, 0.1378342605157389, 0.1378342605157389, 0.22825634414942975], 
reward next is 0.5176, 
noisyNet noise sample is [array([0.6855748], dtype=float32), 1.4857739]. 
=============================================
[2019-03-08 09:58:21,459] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2595196e-37 0.0000000e+00], sum to 1.0000
[2019-03-08 09:58:21,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2227
[2019-03-08 09:58:21,478] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7547354681078478, 6.9112, 6.9112, 306789.8671240242, 306789.8671240242, 86322.63605729255], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2785200.0000, 
sim time next is 2785800.0000, 
raw observation next is [18.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7557169513335, 6.9112, 6.9112, 307189.2973911471, 307189.2973911471, 86405.16400433729], 
processed observation next is [1.0, 0.21739130434782608, 0.45454545454545453, 0.88, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7557169513335, 0.0, 0.0, 0.12799554057964463, 0.12799554057964463, 0.21601291001084322], 
reward next is 0.5079, 
noisyNet noise sample is [array([-0.6039438], dtype=float32), 0.98950934]. 
=============================================
[2019-03-08 09:58:21,679] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5505943e-33 0.0000000e+00], sum to 1.0000
[2019-03-08 09:58:21,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2831
[2019-03-08 09:58:21,692] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7692781061462883, 6.911200000000001, 6.9112, 312708.3512259975, 312708.3512259974, 87546.77273289132], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2787600.0000, 
sim time next is 2788200.0000, 
raw observation next is [18.0, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7849437734661153, 6.9112, 6.9112, 319084.1806232072, 319084.1806232072, 88867.88725870807], 
processed observation next is [1.0, 0.2608695652173913, 0.45454545454545453, 0.91, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7849437734661153, 0.0, 0.0, 0.13295174192633633, 0.13295174192633633, 0.22216971814677017], 
reward next is 0.5129, 
noisyNet noise sample is [array([0.35908294], dtype=float32), -1.7480562]. 
=============================================
[2019-03-08 09:58:22,024] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.9606508e-04 4.4091011e-23 5.0189421e-28 9.9930394e-01 7.7958547e-27], sum to 1.0000
[2019-03-08 09:58:22,032] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8447
[2019-03-08 09:58:22,040] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 58.0, 1.0, 2.0, 0.8422436103865096, 1.0, 2.0, 0.8422436103865096, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 1232862.941061689, 1232862.941061689, 261214.3797342026], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2811600.0000, 
sim time next is 2812200.0000, 
raw observation next is [28.0, 58.0, 1.0, 2.0, 0.7158923180218626, 1.0, 2.0, 0.7158923180218626, 0.0, 2.0, 0.0, 6.911199999999998, 6.9112, 1053364.312654133, 1053364.312654134, 212698.2523414945], 
processed observation next is [1.0, 0.5652173913043478, 0.9090909090909091, 0.58, 1.0, 1.0, 0.7158923180218626, 1.0, 1.0, 0.7158923180218626, 0.0, 1.0, 0.0, -1.7763568394002506e-16, 0.0, 0.43890179693922204, 0.43890179693922243, 0.5317456308537363], 
reward next is 0.7075, 
noisyNet noise sample is [array([0.6157804], dtype=float32), 0.5914424]. 
=============================================
[2019-03-08 09:58:23,276] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.3650555e-14 0.0000000e+00], sum to 1.0000
[2019-03-08 09:58:23,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2992
[2019-03-08 09:58:23,300] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.0, 58.0, 1.0, 2.0, 0.7140183880187945, 1.0, 2.0, 0.7140183880187945, 0.0, 1.0, 0.0, 6.911199999999997, 6.9112, 1048996.757753016, 1048996.757753017, 212765.0970147065], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2814000.0000, 
sim time next is 2814600.0000, 
raw observation next is [28.0, 58.0, 1.0, 2.0, 0.961983977832568, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 6.911199999999999, 6.9112, 1095336.073756116, 1095336.073756116, 255933.1059251093], 
processed observation next is [1.0, 0.5652173913043478, 0.9090909090909091, 0.58, 1.0, 1.0, 0.961983977832568, 0.0, 0.5, 0.0, 1.0, 0.5, 0.9547116395713053, -8.881784197001253e-17, 0.0, 0.45639003073171497, 0.45639003073171497, 0.6398327648127733], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.31787458], dtype=float32), -0.551119]. 
=============================================
[2019-03-08 09:58:23,566] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3540844e-22 0.0000000e+00], sum to 1.0000
[2019-03-08 09:58:23,578] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9284
[2019-03-08 09:58:23,591] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.33333333333334, 53.0, 1.0, 2.0, 0.2318440800121527, 1.0, 2.0, 0.2318440800121527, 1.0, 1.0, 0.4272929946704147, 6.9112, 6.9112, 520791.6146886736, 520791.6146886736, 138667.7193822763], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2832000.0000, 
sim time next is 2832600.0000, 
raw observation next is [27.16666666666666, 53.5, 1.0, 2.0, 0.2172858247774337, 1.0, 2.0, 0.2172858247774337, 1.0, 2.0, 0.4004586705415603, 6.9112, 6.9112, 488061.1136828328, 488061.1136828328, 134698.3727086668], 
processed observation next is [1.0, 0.782608695652174, 0.871212121212121, 0.535, 1.0, 1.0, 0.2172858247774337, 1.0, 1.0, 0.2172858247774337, 1.0, 1.0, 0.4004586705415603, 0.0, 0.0, 0.203358797367847, 0.203358797367847, 0.336745931771667], 
reward next is 0.5176, 
noisyNet noise sample is [array([1.8801479], dtype=float32), -1.073706]. 
=============================================
[2019-03-08 09:58:23,592] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-08 09:58:23,594] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation job starts!
[2019-03-08 09:58:23,595] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-08 09:58:23,602] EPLUS_ENV_Part3-NA-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_pit_na/20/Eplus-env-Part3-NA-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-03-08 09:58:34,609] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.11087208], dtype=float32), 0.34560716]
[2019-03-08 09:58:34,610] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation this: [23.5, 83.83333333333334, 1.0, 2.0, 0.6613412675595826, 1.0, 2.0, 0.5971109454230873, 1.0, 1.0, 0.959245036814763, 6.911200000000003, 6.9112, 1305195.3605282, 1305195.3605282, 270826.5678222812]
[2019-03-08 09:58:34,610] A3C_EVAL-Part3-NA-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-03-08 09:58:34,611] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Softmax [9.9997747e-01 6.2090080e-29 1.5820131e-31 2.2489867e-05 1.3630315e-31], sampled 0.5503894581728965
[2019-03-08 09:58:50,205] A3C_EVAL-Part3-NA-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5614.4027 1457512489.0581 415.0000
[2019-03-08 09:58:51,220] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 500000, evaluation results [500000.0, 5614.4027380462385, 1457512489.058057, 415.0]
[2019-03-08 09:58:53,200] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0000000e+00 1.2406225e-32 2.2772951e-34 3.6116323e-09 9.1586616e-35], sum to 1.0000
[2019-03-08 09:58:53,209] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8348
[2019-03-08 09:58:53,224] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 81.33333333333333, 1.0, 2.0, 0.3148531676481602, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5854953542428579, 6.9112, 6.9112, 475866.4147531019, 475866.4147531019, 119288.3173522122], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2853600.0000, 
sim time next is 2854200.0000, 
raw observation next is [22.16666666666667, 82.16666666666667, 1.0, 2.0, 0.6255144993319238, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 475938.946800743, 475938.946800743, 96118.28814348236], 
processed observation next is [1.0, 0.0, 0.6439393939393941, 0.8216666666666668, 1.0, 1.0, 0.6255144993319238, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.19830789450030958, 0.19830789450030958, 0.24029572035870592], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32632002], dtype=float32), 1.217415]. 
=============================================
[2019-03-08 09:58:54,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 5.353823e-37 0.000000e+00 3.623398e-12 0.000000e+00], sum to 1.0000
[2019-03-08 09:58:54,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8763
[2019-03-08 09:58:54,206] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 88.0, 1.0, 1.0, 0.6153972320805843, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 471447.3337032222, 471447.3337032221, 85534.05604635077], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2870400.0000, 
sim time next is 2871000.0000, 
raw observation next is [21.0, 88.0, 1.0, 2.0, 0.5917626077708604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 453328.3486913617, 453328.3486913618, 82388.00346789783], 
processed observation next is [1.0, 0.21739130434782608, 0.5909090909090909, 0.88, 1.0, 1.0, 0.5917626077708604, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.18888681195473406, 0.18888681195473409, 0.20597000866974458], 
reward next is 0.7861, 
noisyNet noise sample is [array([1.9271169], dtype=float32), -1.846897]. 
=============================================
[2019-03-08 09:58:54,217] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[48.079067]
 [47.5887  ]
 [47.774242]
 [47.75645 ]
 [48.47716 ]], R is [[48.42265701]
 [47.93843079]
 [47.45904541]
 [46.98445511]
 [47.07528687]].
[2019-03-08 09:59:01,328] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 9.9477458e-34 2.5661564e-38 4.2901121e-25 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:01,341] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9586
[2019-03-08 09:59:01,351] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 60.0, 1.0, 2.0, 0.692219111061168, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 518886.834436472, 518886.834436472, 107823.6389192953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3004200.0000, 
sim time next is 3004800.0000, 
raw observation next is [26.66666666666666, 61.66666666666666, 1.0, 2.0, 0.722267176271844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 544650.0114246503, 544650.0114246503, 112235.5368248263], 
processed observation next is [1.0, 0.782608695652174, 0.8484848484848482, 0.6166666666666666, 1.0, 1.0, 0.722267176271844, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.22693750476027097, 0.22693750476027097, 0.28058884206206575], 
reward next is 0.6932, 
noisyNet noise sample is [array([-0.04162559], dtype=float32), -0.47899386]. 
=============================================
[2019-03-08 09:59:02,262] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9828388e-20 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:02,275] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8158
[2019-03-08 09:59:02,288] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.66666666666666, 73.66666666666667, 1.0, 2.0, 0.3370346409961044, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6248546188304727, 6.911200000000001, 6.9112, 507887.1678099511, 507887.1678099509, 124484.5915188669], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3172800.0000, 
sim time next is 3173400.0000, 
raw observation next is [23.5, 73.5, 1.0, 2.0, 0.3314403077780534, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6155747097034772, 6.9112, 6.9112, 500337.1377800948, 500337.1377800948, 123170.5503043439], 
processed observation next is [1.0, 0.7391304347826086, 0.7045454545454546, 0.735, 1.0, 1.0, 0.3314403077780534, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6155747097034772, 0.0, 0.0, 0.20847380740837285, 0.20847380740837285, 0.30792637576085974], 
reward next is 0.5803, 
noisyNet noise sample is [array([-1.3730326], dtype=float32), -0.28349298]. 
=============================================
[2019-03-08 09:59:02,617] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0260184e-06 3.7328813e-34 2.3008020e-37 9.9999797e-01 2.5081328e-37], sum to 1.0000
[2019-03-08 09:59:02,624] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7878
[2019-03-08 09:59:02,630] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.5, 88.0, 1.0, 2.0, 0.2396405367517999, 1.0, 2.0, 0.2396405367517999, 0.0, 2.0, 0.0, 6.9112, 6.9112, 371069.5271728078, 371069.5271728078, 88135.1075290889], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3025800.0000, 
sim time next is 3026400.0000, 
raw observation next is [18.33333333333334, 88.0, 1.0, 2.0, 0.2354734968679992, 1.0, 2.0, 0.2354734968679992, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 364980.1345607349, 364980.1345607351, 87482.39504420577], 
processed observation next is [1.0, 0.0, 0.46969696969696995, 0.88, 1.0, 1.0, 0.2354734968679992, 1.0, 1.0, 0.2354734968679992, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.15207505606697289, 0.15207505606697294, 0.2187059876105144], 
reward next is 0.5960, 
noisyNet noise sample is [array([-1.4079134], dtype=float32), 1.2573503]. 
=============================================
[2019-03-08 09:59:07,955] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.4144596e-26 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:07,965] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6522
[2019-03-08 09:59:07,970] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.0, 83.0, 1.0, 2.0, 0.3333303900675086, 1.0, 2.0, 0.3333303900675086, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 504400.2136423616, 504400.2136423617, 104302.2373965044], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3124200.0000, 
sim time next is 3124800.0000, 
raw observation next is [22.0, 83.0, 1.0, 2.0, 0.3319843442232982, 1.0, 2.0, 0.3319843442232982, 0.0, 2.0, 0.0, 6.9112, 6.9112, 502388.5696694488, 502388.5696694488, 104073.9181241457], 
processed observation next is [1.0, 0.17391304347826086, 0.6363636363636364, 0.83, 1.0, 1.0, 0.3319843442232982, 1.0, 1.0, 0.3319843442232982, 0.0, 1.0, 0.0, 0.0, 0.0, 0.20932857069560368, 0.20932857069560368, 0.26018479531036426], 
reward next is 0.6896, 
noisyNet noise sample is [array([-0.21890254], dtype=float32), -1.2047552]. 
=============================================
[2019-03-08 09:59:08,957] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2542187e-11 2.0278230e-37 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:08,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5520
[2019-03-08 09:59:08,972] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.0, 73.0, 1.0, 2.0, 0.6961178198715343, 1.0, 2.0, 0.6961178198715343, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 1049148.501463077, 1049148.501463077, 203205.7708232056], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3141600.0000, 
sim time next is 3142200.0000, 
raw observation next is [23.0, 73.0, 1.0, 2.0, 0.6896719121115369, 1.0, 2.0, 0.6896719121115369, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1039336.122701063, 1039336.122701063, 201146.3110691433], 
processed observation next is [1.0, 0.34782608695652173, 0.6818181818181818, 0.73, 1.0, 1.0, 0.6896719121115369, 1.0, 1.0, 0.6896719121115369, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.4330567177921096, 0.4330567177921096, 0.5028657776728582], 
reward next is 0.7382, 
noisyNet noise sample is [array([0.39132214], dtype=float32), 0.870359]. 
=============================================
[2019-03-08 09:59:15,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2059430e-05 1.3802891e-38 2.1317951e-34 9.9997795e-01 1.7403985e-34], sum to 1.0000
[2019-03-08 09:59:15,094] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5528
[2019-03-08 09:59:15,100] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.0, 83.0, 1.0, 2.0, 0.2202449758846415, 1.0, 2.0, 0.2202449758846415, 0.0, 2.0, 0.0, 6.9112, 6.9112, 341499.6867155232, 341499.6867155232, 85610.73853200627], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3222000.0000, 
sim time next is 3222600.0000, 
raw observation next is [19.33333333333334, 81.33333333333334, 1.0, 2.0, 0.2224603503807074, 1.0, 2.0, 0.2224603503807074, 0.0, 2.0, 0.0, 6.9112, 6.9112, 344723.3354377618, 344723.3354377618, 85954.38650461055], 
processed observation next is [0.0, 0.30434782608695654, 0.5151515151515155, 0.8133333333333335, 1.0, 1.0, 0.2224603503807074, 1.0, 1.0, 0.2224603503807074, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1436347230990674, 0.1436347230990674, 0.21488596626152637], 
reward next is 0.5729, 
noisyNet noise sample is [array([1.0509207], dtype=float32), 0.02724212]. 
=============================================
[2019-03-08 09:59:15,487] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.9149256e-27 3.3914102e-28 2.5519041e-23 2.4747324e-29], sum to 1.0000
[2019-03-08 09:59:15,496] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3911
[2019-03-08 09:59:15,501] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.83333333333334, 69.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9156857811963433, 6.911199999999999, 6.9112, 372307.5443817786, 372307.5443817786, 99998.38708094486], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3228600.0000, 
sim time next is 3229200.0000, 
raw observation next is [22.0, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9204123616669568, 6.911199999999999, 6.9112, 374232.0811097564, 374232.0811097564, 100404.3122959947], 
processed observation next is [0.0, 0.391304347826087, 0.6363636363636364, 0.69, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9204123616669568, -8.881784197001253e-17, 0.0, 0.1559300337957318, 0.1559300337957318, 0.2510107807399868], 
reward next is 0.5325, 
noisyNet noise sample is [array([1.0339186], dtype=float32), 0.8433192]. 
=============================================
[2019-03-08 09:59:16,427] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 09:59:16,441] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0060
[2019-03-08 09:59:16,447] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 47.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.753252369451027, 6.911200000000001, 6.9112, 306186.2987733135, 306186.2987733134, 86197.71275028848], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3260400.0000, 
sim time next is 3261000.0000, 
raw observation next is [24.0, 47.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7520880699653866, 6.9112, 6.9112, 305712.4722948007, 305712.4722948007, 86099.84490535404], 
processed observation next is [0.0, 0.7391304347826086, 0.7272727272727273, 0.47, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7520880699653866, 0.0, 0.0, 0.12738019678950027, 0.12738019678950027, 0.2152496122633851], 
reward next is 0.5072, 
noisyNet noise sample is [array([0.13230094], dtype=float32), 0.9069707]. 
=============================================
[2019-03-08 09:59:16,465] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[50.8567  ]
 [50.853382]
 [50.84686 ]
 [50.835804]
 [50.821957]], R is [[50.85700226]
 [50.85588455]
 [50.85519409]
 [50.85533142]
 [50.85683823]].
[2019-03-08 09:59:17,191] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3049266e-05 1.5863890e-32 4.8991999e-35 9.9996698e-01 3.5722491e-36], sum to 1.0000
[2019-03-08 09:59:17,198] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2680
[2019-03-08 09:59:17,205] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.0, 64.0, 1.0, 2.0, 0.2021796353048277, 1.0, 2.0, 0.2021796353048277, 0.0, 2.0, 0.0, 6.9112, 6.9112, 314462.2251617278, 314462.2251617278, 83119.71036796196], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3276000.0000, 
sim time next is 3276600.0000, 
raw observation next is [20.5, 66.16666666666667, 1.0, 2.0, 0.1998866192675658, 1.0, 2.0, 0.1998866192675658, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 311090.7344704144, 311090.7344704143, 82772.52378226804], 
processed observation next is [0.0, 0.9565217391304348, 0.5681818181818182, 0.6616666666666667, 1.0, 1.0, 0.1998866192675658, 1.0, 1.0, 0.1998866192675658, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.12962113936267267, 0.1296211393626726, 0.2069313094556701], 
reward next is 0.5369, 
noisyNet noise sample is [array([-0.7504404], dtype=float32), -0.7502078]. 
=============================================
[2019-03-08 09:59:19,393] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0284484e-19 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:19,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5366
[2019-03-08 09:59:19,412] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.0, 68.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 261648.8622759869, 261648.8622759869, 75841.80953183924], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3313800.0000, 
sim time next is 3314400.0000, 
raw observation next is [19.0, 68.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 260121.9438356485, 260121.9438356485, 75583.77390127828], 
processed observation next is [0.0, 0.34782608695652173, 0.5, 0.68, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.10838414326485354, 0.10838414326485354, 0.1889594347531957], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09259363], dtype=float32), 0.47501194]. 
=============================================
[2019-03-08 09:59:19,593] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.1314905e-16 5.7929719e-37 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:19,604] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9937
[2019-03-08 09:59:19,611] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.66666666666667, 60.66666666666667, 1.0, 2.0, 0.3917219440898864, 1.0, 2.0, 0.3917219440898864, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 579857.970037984, 579857.9700379841, 116691.0915341535], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3522000.0000, 
sim time next is 3522600.0000, 
raw observation next is [27.5, 62.0, 1.0, 2.0, 0.4060357762343738, 1.0, 2.0, 0.4060357762343738, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 600433.3426196615, 600433.3426196616, 119589.2025148117], 
processed observation next is [1.0, 0.782608695652174, 0.8863636363636364, 0.62, 1.0, 1.0, 0.4060357762343738, 1.0, 1.0, 0.4060357762343738, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.25018055942485895, 0.250180559424859, 0.2989730062870292], 
reward next is 0.7173, 
noisyNet noise sample is [array([0.4480252], dtype=float32), -0.65606725]. 
=============================================
[2019-03-08 09:59:23,975] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.1951817e-01 3.7831754e-33 1.9366280e-38 8.0481879e-02 2.1633439e-37], sum to 1.0000
[2019-03-08 09:59:23,983] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4092
[2019-03-08 09:59:23,999] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333334, 78.0, 1.0, 2.0, 0.3450197012677116, 1.0, 2.0, 0.3450197012677116, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 527904.2736396814, 527904.2736396813, 105057.5885272269], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3399600.0000, 
sim time next is 3400200.0000, 
raw observation next is [21.5, 78.0, 1.0, 2.0, 0.7141727858392917, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 548993.0971031454, 548993.0971031454, 107706.4809224564], 
processed observation next is [1.0, 0.34782608695652173, 0.6136363636363636, 0.78, 1.0, 1.0, 0.7141727858392917, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.22874712379297726, 0.22874712379297726, 0.269266202306141], 
reward next is 0.7282, 
noisyNet noise sample is [array([-0.7321131], dtype=float32), 0.01729226]. 
=============================================
[2019-03-08 09:59:24,440] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.3086695e-27 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:24,446] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8827
[2019-03-08 09:59:24,451] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.33333333333334, 98.0, 1.0, 2.0, 0.2122328747086643, 1.0, 2.0, 0.2122328747086643, 0.0, 2.0, 0.0, 6.9112, 6.9112, 329019.0114533824, 329019.0114533824, 84711.38780728164], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3392400.0000, 
sim time next is 3393000.0000, 
raw observation next is [17.5, 97.0, 1.0, 2.0, 0.2155328734467475, 1.0, 2.0, 0.2155328734467475, 0.0, 2.0, 0.0, 6.9112, 6.9112, 334002.6650338799, 334002.6650338799, 85142.2113761987], 
processed observation next is [1.0, 0.2608695652173913, 0.4318181818181818, 0.97, 1.0, 1.0, 0.2155328734467475, 1.0, 1.0, 0.2155328734467475, 0.0, 1.0, 0.0, 0.0, 0.0, 0.13916777709744996, 0.13916777709744996, 0.21285552844049674], 
reward next is 0.5604, 
noisyNet noise sample is [array([1.6651357], dtype=float32), 0.067540824]. 
=============================================
[2019-03-08 09:59:24,459] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[61.61653 ]
 [61.602036]
 [61.585846]
 [61.454693]
 [61.282722]], R is [[61.5771637 ]
 [61.5162468 ]
 [61.45113373]
 [61.3811264 ]
 [61.30534363]].
[2019-03-08 09:59:29,248] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9884629e-01 3.0224281e-32 7.2976387e-29 1.1536963e-03 3.2481449e-28], sum to 1.0000
[2019-03-08 09:59:29,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2791
[2019-03-08 09:59:29,265] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 70.0, 1.0, 2.0, 0.7750653836796546, 1.0, 2.0, 0.6539730034831234, 1.0, 1.0, 0.9703463333336437, 6.9112, 6.9112, 1429701.277766283, 1429701.277766283, 302341.5004126662], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3501600.0000, 
sim time next is 3502200.0000, 
raw observation next is [27.0, 70.0, 1.0, 2.0, 0.7343983596572502, 1.0, 2.0, 0.6336394914719212, 1.0, 2.0, 0.9711697393255158, 6.911199999999999, 6.9112, 1385160.57448704, 1385160.57448704, 292129.6210815213], 
processed observation next is [1.0, 0.5217391304347826, 0.8636363636363636, 0.7, 1.0, 1.0, 0.7343983596572502, 1.0, 1.0, 0.6336394914719212, 1.0, 1.0, 0.9711697393255158, -8.881784197001253e-17, 0.0, 0.5771502393696, 0.5771502393696, 0.7303240527038033], 
reward next is 0.6774, 
noisyNet noise sample is [array([0.2991911], dtype=float32), 0.2400489]. 
=============================================
[2019-03-08 09:59:31,783] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 09:59:31,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7351
[2019-03-08 09:59:31,806] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.33333333333334, 62.0, 1.0, 2.0, 0.9651811226994369, 1.0, 2.0, 0.9651811226994369, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 1406642.061191491, 1406642.061191491, 314103.9151184953], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3511200.0000, 
sim time next is 3511800.0000, 
raw observation next is [28.5, 62.0, 1.0, 2.0, 0.7757616908861895, 1.0, 2.0, 0.6543211570863908, 1.0, 1.0, 0.9699642028337626, 6.911200000000002, 6.9112, 1430417.137103572, 1430417.137103572, 303356.2893211576], 
processed observation next is [1.0, 0.6521739130434783, 0.9318181818181818, 0.62, 1.0, 1.0, 0.7757616908861895, 1.0, 1.0, 0.6543211570863908, 1.0, 0.5, 0.9699642028337626, 1.7763568394002506e-16, 0.0, 0.5960071404598217, 0.5960071404598217, 0.758390723302894], 
reward next is 0.6736, 
noisyNet noise sample is [array([-1.0652964], dtype=float32), 1.6799197]. 
=============================================
[2019-03-08 09:59:34,072] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 9.6525247e-30 1.7631322e-32 1.4212812e-15 3.0462666e-33], sum to 1.0000
[2019-03-08 09:59:34,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8017
[2019-03-08 09:59:34,093] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.83333333333333, 89.83333333333334, 1.0, 2.0, 0.7386296622432448, 1.0, 2.0, 0.6357551427649185, 1.0, 2.0, 0.969210195362675, 6.911199999999998, 6.9112, 1389743.668231479, 1389743.668231479, 293690.4195835084], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3582600.0000, 
sim time next is 3583200.0000, 
raw observation next is [23.66666666666666, 90.66666666666667, 1.0, 2.0, 0.487134880159502, 1.0, 2.0, 0.487134880159502, 1.0, 2.0, 0.8830796676990156, 6.911200000000001, 6.9112, 1064519.387123725, 1064519.387123725, 226232.0046868431], 
processed observation next is [1.0, 0.4782608695652174, 0.7121212121212118, 0.9066666666666667, 1.0, 1.0, 0.487134880159502, 1.0, 1.0, 0.487134880159502, 1.0, 1.0, 0.8830796676990156, 8.881784197001253e-17, 0.0, 0.4435497446348854, 0.4435497446348854, 0.5655800117171078], 
reward next is 0.6722, 
noisyNet noise sample is [array([-0.06524645], dtype=float32), -0.11087685]. 
=============================================
[2019-03-08 09:59:34,207] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3495838e-02 1.2978158e-35 0.0000000e+00 9.8650420e-01 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:34,212] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1232
[2019-03-08 09:59:34,216] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.16666666666667, 93.16666666666667, 1.0, 2.0, 0.3804637927471727, 1.0, 1.0, 0.3804637927471727, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 566908.5473371804, 566908.5473371802, 114018.908996711], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3552600.0000, 
sim time next is 3553200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3710879164667511, 1.0, 2.0, 0.3710879164667511, 0.0, 2.0, 0.0, 6.9112, 6.9112, 552865.5891755321, 552865.5891755321, 112281.6704436676], 
processed observation next is [1.0, 0.13043478260869565, 0.6363636363636364, 0.94, 1.0, 1.0, 0.3710879164667511, 1.0, 1.0, 0.3710879164667511, 0.0, 1.0, 0.0, 0.0, 0.0, 0.23036066215647172, 0.23036066215647172, 0.280704176109169], 
reward next is 0.7034, 
noisyNet noise sample is [array([0.57481945], dtype=float32), -0.49646595]. 
=============================================
[2019-03-08 09:59:36,629] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3429818e-09 3.6797694e-35 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:36,637] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8997
[2019-03-08 09:59:36,652] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.83333333333333, 75.66666666666667, 1.0, 2.0, 0.9073231244425627, 1.0, 2.0, 0.7201018738645772, 1.0, 2.0, 0.9779208474079657, 6.911199999999998, 6.9112, 1574207.082461311, 1574207.082461311, 344646.536304516], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3592200.0000, 
sim time next is 3592800.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.8205862875057067, 1.0, 2.0, 0.6767334553961494, 1.0, 2.0, 0.9763567800738808, 6.911200000000001, 6.9112, 1479471.214073963, 1479471.214073963, 317015.8676694382], 
processed observation next is [1.0, 0.6086956521739131, 0.8636363636363636, 0.74, 1.0, 1.0, 0.8205862875057067, 1.0, 1.0, 0.6767334553961494, 1.0, 1.0, 0.9763567800738808, 8.881784197001253e-17, 0.0, 0.6164463391974846, 0.6164463391974846, 0.7925396691735954], 
reward next is 0.6667, 
noisyNet noise sample is [array([-1.1448269], dtype=float32), 0.6266607]. 
=============================================
[2019-03-08 09:59:38,775] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 1.965326e-32 0.000000e+00], sum to 1.0000
[2019-03-08 09:59:38,785] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2891
[2019-03-08 09:59:38,800] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.66666666666666, 90.66666666666666, 1.0, 2.0, 0.7832625214652311, 1.0, 2.0, 0.7832625214652311, 0.0, 2.0, 0.0, 6.9112, 6.9112, 1153711.53417653, 1153711.53417653, 237518.6049128119], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3663600.0000, 
sim time next is 3664200.0000, 
raw observation next is [22.83333333333334, 89.83333333333333, 1.0, 2.0, 0.5305630318417313, 1.0, 2.0, 0.5305630318417313, 1.0, 1.0, 0.9543314595372484, 6.9112, 6.9112, 1159565.816730174, 1159565.816730174, 241623.6603281107], 
processed observation next is [1.0, 0.391304347826087, 0.6742424242424245, 0.8983333333333333, 1.0, 1.0, 0.5305630318417313, 1.0, 1.0, 0.5305630318417313, 1.0, 0.5, 0.9543314595372484, 0.0, 0.0, 0.4831524236375725, 0.4831524236375725, 0.6040591508202767], 
reward next is 0.6856, 
noisyNet noise sample is [array([1.4407653], dtype=float32), 0.46987298]. 
=============================================
[2019-03-08 09:59:43,900] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 6.050755e-36 0.000000e+00], sum to 1.0000
[2019-03-08 09:59:43,910] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4619
[2019-03-08 09:59:43,917] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6417057659715174, 6.9112, 6.9112, 260798.8157315948, 260798.8157315948, 75319.207613153], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3895800.0000, 
sim time next is 3896400.0000, 
raw observation next is [18.0, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6398659652351114, 6.9112, 6.9112, 260050.3470357914, 260050.3470357914, 75161.809297317], 
processed observation next is [0.0, 0.08695652173913043, 0.45454545454545453, 0.77, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6398659652351114, 0.0, 0.0, 0.10835431126491309, 0.10835431126491309, 0.1879045232432925], 
reward next is 0.4943, 
noisyNet noise sample is [array([0.8853962], dtype=float32), 1.0474004]. 
=============================================
[2019-03-08 09:59:45,186] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.9998939e-01 0.0000000e+00 0.0000000e+00 1.0587177e-05 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:45,196] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4755
[2019-03-08 09:59:45,210] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.16666666666666, 68.33333333333333, 1.0, 2.0, 0.9821001029938913, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9547116395713053, 6.911199999999999, 6.9112, 1122218.522380633, 1122218.522380633, 259720.4050569571], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3751800.0000, 
sim time next is 3752400.0000, 
raw observation next is [24.33333333333333, 67.66666666666667, 1.0, 2.0, 0.7607624010915198, 1.0, 1.0, 0.7607624010915198, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 1140136.901002863, 1140136.901002863, 226507.655694676], 
processed observation next is [1.0, 0.43478260869565216, 0.7424242424242422, 0.6766666666666667, 1.0, 1.0, 0.7607624010915198, 1.0, 0.5, 0.7607624010915198, 0.0, 0.5, 0.0, 8.881784197001253e-17, 0.0, 0.4750570420845262, 0.4750570420845262, 0.56626913923669], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7320208], dtype=float32), -0.06911221]. 
=============================================
[2019-03-08 09:59:50,742] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.9999809e-01 0.0000000e+00 0.0000000e+00 1.9301824e-06 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:50,757] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9868
[2019-03-08 09:59:50,762] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.16666666666667, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7308626981029059, 6.911200000000001, 6.9112, 297074.839475568, 297074.839475568, 84317.78511828576], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3840600.0000, 
sim time next is 3841200.0000, 
raw observation next is [19.0, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7172204137886871, 6.911200000000001, 6.9112, 291523.4349760148, 291523.4349760148, 83175.14895192644], 
processed observation next is [0.0, 0.4782608695652174, 0.5, 0.73, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7172204137886871, 8.881784197001253e-17, 0.0, 0.12146809790667282, 0.12146809790667282, 0.20793787237981612], 
reward next is 0.5007, 
noisyNet noise sample is [array([1.2335485], dtype=float32), 0.6948252]. 
=============================================
[2019-03-08 09:59:53,913] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1218220e-29 1.6051562e-37 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 09:59:53,923] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1369
[2019-03-08 09:59:53,931] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.83333333333334, 59.0, 1.0, 2.0, 0.2207844189922693, 1.0, 2.0, 0.2207844189922693, 0.0, 2.0, 0.0, 6.9112, 6.9112, 341586.2583979118, 341586.2583979118, 85964.30473828511], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3923400.0000, 
sim time next is 3924000.0000, 
raw observation next is [23.0, 57.0, 1.0, 2.0, 0.2160810595700323, 1.0, 2.0, 0.2160810595700323, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 334642.1671989483, 334642.1671989484, 85288.22108330429], 
processed observation next is [0.0, 0.43478260869565216, 0.6818181818181818, 0.57, 1.0, 1.0, 0.2160810595700323, 1.0, 1.0, 0.2160810595700323, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.13943423633289512, 0.13943423633289517, 0.21322055270826074], 
reward next is 0.5605, 
noisyNet noise sample is [array([-0.0981015], dtype=float32), 0.10813268]. 
=============================================
[2019-03-08 09:59:53,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[50.83515 ]
 [50.969315]
 [51.099728]
 [50.995293]
 [52.792995]], R is [[50.7229805 ]
 [50.7834053 ]
 [50.84820938]
 [50.91576385]
 [51.05741119]].
[2019-03-08 09:59:56,609] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 1.584527e-36 0.000000e+00], sum to 1.0000
[2019-03-08 09:59:56,621] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3215
[2019-03-08 09:59:56,630] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.66666666666667, 51.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7457174939386938, 6.9112, 6.9112, 303119.9159031262, 303119.9159031262, 85565.00834757432], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3928800.0000, 
sim time next is 3929400.0000, 
raw observation next is [24.0, 50.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7522817265385493, 6.9112, 6.9112, 305791.2831798796, 305791.2831798796, 86116.64185317], 
processed observation next is [0.0, 0.4782608695652174, 0.7272727272727273, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7522817265385493, 0.0, 0.0, 0.12741303465828316, 0.12741303465828316, 0.21529160463292502], 
reward next is 0.5073, 
noisyNet noise sample is [array([-0.60510594], dtype=float32), 1.4618845]. 
=============================================
[2019-03-08 10:00:03,618] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.9586516  0.         0.         0.04134841 0.        ], sum to 1.0000
[2019-03-08 10:00:03,628] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9379
[2019-03-08 10:00:03,635] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7099141076880113, 6.9112, 6.9112, 288550.4040292816, 288550.4040292816, 82564.28712288653], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4062000.0000, 
sim time next is 4062600.0000, 
raw observation next is [16.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.709731686359085, 6.911200000000001, 6.9112, 288476.175286961, 288476.1752869609, 82549.03845945715], 
processed observation next is [1.0, 0.0, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.709731686359085, 8.881784197001253e-17, 0.0, 0.12019840636956709, 0.12019840636956704, 0.20637259614864287], 
reward next is 0.4992, 
noisyNet noise sample is [array([0.2570645], dtype=float32), 0.61132973]. 
=============================================
[2019-03-08 10:00:04,725] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.9998522e-01 3.8573694e-19 4.6808117e-25 1.4736985e-05 4.6066053e-26], sum to 1.0000
[2019-03-08 10:00:04,733] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6570
[2019-03-08 10:00:04,746] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [20.0, 86.0, 1.0, 2.0, 0.674655151685656, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 518975.9165408275, 518975.9165408274, 97804.58993235708], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4095000.0000, 
sim time next is 4095600.0000, 
raw observation next is [20.33333333333334, 83.33333333333334, 1.0, 2.0, 0.4121335913350138, 0.0, 2.0, 0.0, 1.0, 1.0, 0.7766037574219775, 6.911199999999999, 6.9112, 631408.2637274126, 631408.2637274127, 141732.4098406324], 
processed observation next is [1.0, 0.391304347826087, 0.5606060606060609, 0.8333333333333335, 1.0, 1.0, 0.4121335913350138, 0.0, 1.0, 0.0, 1.0, 0.5, 0.7766037574219775, -8.881784197001253e-17, 0.0, 0.26308677655308854, 0.2630867765530886, 0.354331024601581], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.56120145], dtype=float32), -0.29583874]. 
=============================================
[2019-03-08 10:00:05,213] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0000000e+00 1.1424036e-27 0.0000000e+00 1.5265433e-19 0.0000000e+00], sum to 1.0000
[2019-03-08 10:00:05,222] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9508
[2019-03-08 10:00:05,234] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.66666666666666, 74.66666666666667, 1.0, 2.0, 0.2895691053012216, 1.0, 2.0, 0.2895691053012216, 0.0, 1.0, 0.0, 6.9112, 6.9112, 439951.8976727124, 439951.8976727124, 97036.88429339149], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4102800.0000, 
sim time next is 4103400.0000, 
raw observation next is [22.83333333333334, 73.83333333333333, 1.0, 2.0, 0.2973254476921421, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5562294639103191, 6.911199999999999, 6.9112, 452074.3586000347, 452074.3586000348, 114208.3388370424], 
processed observation next is [1.0, 0.4782608695652174, 0.6742424242424245, 0.7383333333333333, 1.0, 1.0, 0.2973254476921421, 0.0, 0.5, 0.0, 1.0, 0.5, 0.5562294639103191, -8.881784197001253e-17, 0.0, 0.1883643160833478, 0.18836431608334783, 0.285520847092606], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00562842], dtype=float32), -1.081343]. 
=============================================
[2019-03-08 10:00:08,125] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0000000e+00 4.5360656e-34 2.8342324e-37 1.4652508e-23 2.4584255e-37], sum to 1.0000
[2019-03-08 10:00:08,135] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1159
[2019-03-08 10:00:08,141] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 100.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7578951786021281, 6.911200000000002, 6.9112, 308075.7661273431, 308075.766127343, 86589.79632065145], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4142400.0000, 
sim time next is 4143000.0000, 
raw observation next is [18.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9166986033591189, 6.911200000000001, 6.9112, 372719.935991886, 372719.935991886, 100085.3360829431], 
processed observation next is [1.0, 0.9565217391304348, 0.45454545454545453, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9166986033591189, 8.881784197001253e-17, 0.0, 0.15529997332995252, 0.15529997332995252, 0.25021334020735775], 
reward next is 0.5320, 
noisyNet noise sample is [array([-0.7289059], dtype=float32), -1.8629886]. 
=============================================
[2019-03-08 10:00:08,147] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[51.036346]
 [50.706036]
 [50.95447 ]
 [50.368374]
 [50.03736 ]], R is [[51.31555557]
 [50.80239868]
 [50.87900925]
 [50.37022018]
 [49.86651993]].
[2019-03-08 10:00:17,478] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.3105781e-25 4.2692065e-29 1.1786632e-19 2.3424730e-31], sum to 1.0000
[2019-03-08 10:00:17,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4972
[2019-03-08 10:00:17,501] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [19.0, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9475729996275213, 6.911199999999999, 6.9112, 385291.7155567948, 385291.7155567949, 102741.4688927744], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4323000.0000, 
sim time next is 4323600.0000, 
raw observation next is [19.0, 94.0, 1.0, 1.0, 0.3072854220309161, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5808072067528844, 6.911199999999999, 6.9112, 472068.6363821847, 472068.6363821849, 116396.5910368191], 
processed observation next is [1.0, 0.043478260869565216, 0.5, 0.94, 1.0, 0.5, 0.3072854220309161, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5808072067528844, -8.881784197001253e-17, 0.0, 0.19669526515924363, 0.1966952651592437, 0.29099147759204774], 
reward next is 0.5794, 
noisyNet noise sample is [array([-0.3371376], dtype=float32), 0.3818402]. 
=============================================
[2019-03-08 10:00:20,217] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3612514e-11 0.0000000e+00], sum to 1.0000
[2019-03-08 10:00:20,224] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5484
[2019-03-08 10:00:20,242] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 65.0, 1.0, 2.0, 0.714420179854053, 1.0, 1.0, 0.714420179854053, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 1059568.978993314, 1059568.978993315, 211581.8232886212], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4356000.0000, 
sim time next is 4356600.0000, 
raw observation next is [26.16666666666667, 63.83333333333334, 1.0, 2.0, 0.471003233483659, 1.0, 2.0, 0.471003233483659, 1.0, 1.0, 0.8462280083789124, 6.911199999999999, 6.9112, 1032121.977855124, 1032121.977855125, 217292.6272812821], 
processed observation next is [1.0, 0.43478260869565216, 0.825757575757576, 0.6383333333333334, 1.0, 1.0, 0.471003233483659, 1.0, 1.0, 0.471003233483659, 1.0, 0.5, 0.8462280083789124, -8.881784197001253e-17, 0.0, 0.4300508241063017, 0.4300508241063021, 0.5432315682032053], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.82643366], dtype=float32), 1.9936991]. 
=============================================
[2019-03-08 10:00:21,324] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0392615e-13 0.0000000e+00], sum to 1.0000
[2019-03-08 10:00:21,335] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0858
[2019-03-08 10:00:21,347] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.15, 68.5, 1.0, 2.0, 0.2398851157352527, 1.0, 2.0, 0.2398851157352527, 1.0, 2.0, 0.4381511409841849, 6.9112, 6.9112, 534036.4997548669, 534036.4997548669, 141119.7815339519], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4393800.0000, 
sim time next is 4394400.0000, 
raw observation next is [24.86666666666667, 69.66666666666667, 1.0, 2.0, 0.3241892006340411, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5971615737858552, 6.9112, 6.9112, 485357.0643576137, 485357.0643576137, 121397.3393486669], 
processed observation next is [1.0, 0.8695652173913043, 0.7666666666666668, 0.6966666666666668, 1.0, 1.0, 0.3241892006340411, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5971615737858552, 0.0, 0.0, 0.2022321101490057, 0.2022321101490057, 0.30349334837166725], 
reward next is 0.5712, 
noisyNet noise sample is [array([-2.0839987], dtype=float32), -0.6565516]. 
=============================================
[2019-03-08 10:00:24,413] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.451456e-18 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-08 10:00:24,427] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3150
[2019-03-08 10:00:24,435] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.5, 85.5, 1.0, 2.0, 0.2761359379592732, 1.0, 2.0, 0.2761359379592732, 0.0, 2.0, 0.0, 6.9112, 6.9112, 422599.0224328178, 422599.0224328178, 94376.1759708843], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4422600.0000, 
sim time next is 4423200.0000, 
raw observation next is [20.33333333333333, 86.33333333333334, 1.0, 2.0, 0.2740797517677776, 1.0, 2.0, 0.2740797517677776, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 419665.5253624182, 419665.5253624181, 94032.62196479736], 
processed observation next is [0.0, 0.17391304347826086, 0.5606060606060604, 0.8633333333333334, 1.0, 1.0, 0.2740797517677776, 1.0, 1.0, 0.2740797517677776, 0.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.17486063556767426, 0.1748606355676742, 0.2350815549119934], 
reward next is 0.6376, 
noisyNet noise sample is [array([0.26139238], dtype=float32), -0.060691927]. 
=============================================
[2019-03-08 10:00:28,064] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 10:00:28,071] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1513
[2019-03-08 10:00:28,086] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [20.0, 94.0, 1.0, 2.0, 0.5876061168234678, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 449606.0420057683, 449606.0420057682, 90161.93219106078], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4500000.0000, 
sim time next is 4500600.0000, 
raw observation next is [20.0, 94.00000000000001, 1.0, 2.0, 0.2447622723012271, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4592607972657211, 6.911199999999999, 6.9112, 373195.885951029, 373195.8859510291, 102558.8162553954], 
processed observation next is [0.0, 0.08695652173913043, 0.5454545454545454, 0.9400000000000002, 1.0, 1.0, 0.2447622723012271, 0.0, 1.0, 0.0, 1.0, 0.5, 0.4592607972657211, -8.881784197001253e-17, 0.0, 0.15549828581292877, 0.1554982858129288, 0.2563970406384885], 
reward next is 0.5198, 
noisyNet noise sample is [array([-0.3066398], dtype=float32), 0.2662282]. 
=============================================
[2019-03-08 10:00:33,347] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 10:00:33,356] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2331
[2019-03-08 10:00:33,361] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.0, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4772832872460627, 6.911199999999999, 6.9112, 193933.9110378441, 193933.9110378441, 55076.77643099489], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4596600.0000, 
sim time next is 4597200.0000, 
raw observation next is [14.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4715540457150146, 6.9112, 6.9112, 191603.8035285586, 191603.8035285586, 54366.15046337765], 
processed observation next is [1.0, 0.21739130434782608, 0.2727272727272727, 0.94, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4715540457150146, 0.0, 0.0, 0.07983491813689943, 0.07983491813689943, 0.13591537615844412], 
reward next is 0.5035, 
noisyNet noise sample is [array([1.4148526], dtype=float32), -1.5987521]. 
=============================================
[2019-03-08 10:00:34,355] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 10:00:34,364] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3702
[2019-03-08 10:00:34,373] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5497201186813571, 6.9112, 6.9112, 223397.3637664423, 223397.3637664423, 64705.43822359059], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4584000.0000, 
sim time next is 4584600.0000, 
raw observation next is [16.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.566787231263306, 6.911199999999999, 6.9112, 230339.7702420871, 230339.7702420872, 66116.70284805063], 
processed observation next is [1.0, 0.043478260869565216, 0.36363636363636365, 0.88, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.566787231263306, -8.881784197001253e-17, 0.0, 0.0959749042675363, 0.09597490426753633, 0.16529175712012656], 
reward next is 0.4977, 
noisyNet noise sample is [array([0.5870808], dtype=float32), -0.4789026]. 
=============================================
[2019-03-08 10:00:43,294] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 10:00:43,303] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3342
[2019-03-08 10:00:43,308] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8928673421301746, 6.911200000000001, 6.9112, 363016.888531125, 363016.8885311249, 98042.09744115834], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4759200.0000, 
sim time next is 4759800.0000, 
raw observation next is [19.0, 88.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8934871478213776, 6.9112, 6.9112, 363269.2371458731, 363269.2371458731, 98095.14950728281], 
processed observation next is [1.0, 0.08695652173913043, 0.5, 0.8800000000000001, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8934871478213776, 0.0, 0.0, 0.1513621821441138, 0.1513621821441138, 0.24523787376820702], 
reward next is 0.5290, 
noisyNet noise sample is [array([-0.9540639], dtype=float32), 0.4432047]. 
=============================================
[2019-03-08 10:00:50,898] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 5.6316800e-32 1.6594446e-34 2.3779128e-36 1.1718755e-34], sum to 1.0000
[2019-03-08 10:00:50,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6627
[2019-03-08 10:00:50,920] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 80.5, 1.0, 2.0, 0.7116370097751695, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9547116395713053, 6.9112, 6.9112, 924481.1239683735, 924481.1239683735, 203491.1523241133], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4890600.0000, 
sim time next is 4891200.0000, 
raw observation next is [22.0, 79.66666666666667, 1.0, 2.0, 0.4298854050367689, 1.0, 1.0, 0.4298854050367689, 1.0, 2.0, 0.7879248476497416, 6.911200000000001, 6.9112, 960921.5398536241, 960921.539853624, 201761.5419000207], 
processed observation next is [1.0, 0.6086956521739131, 0.6363636363636364, 0.7966666666666667, 1.0, 1.0, 0.4298854050367689, 1.0, 0.5, 0.4298854050367689, 1.0, 1.0, 0.7879248476497416, 8.881784197001253e-17, 0.0, 0.40038397493901, 0.40038397493900996, 0.5044038547500518], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5642523], dtype=float32), -1.7878162]. 
=============================================
[2019-03-08 10:00:53,858] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2164058e-14 1.6364937e-33 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-08 10:00:53,870] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2728
[2019-03-08 10:00:53,877] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.0, 78.0, 1.0, 2.0, 0.2799143124480366, 1.0, 2.0, 0.2799143124480366, 0.0, 2.0, 0.0, 6.9112, 6.9112, 426705.8679111084, 426705.8679111084, 95309.96670523475], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5098800.0000, 
sim time next is 5099400.0000, 
raw observation next is [22.0, 78.0, 1.0, 2.0, 0.2787070046260297, 1.0, 2.0, 0.2787070046260297, 0.0, 2.0, 0.0, 6.9112, 6.9112, 424880.3366147865, 424880.3366147865, 95133.07513723265], 
processed observation next is [0.0, 0.0, 0.6363636363636364, 0.78, 1.0, 1.0, 0.2787070046260297, 1.0, 1.0, 0.2787070046260297, 0.0, 1.0, 0.0, 0.0, 0.0, 0.17703347358949437, 0.17703347358949437, 0.23783268784308162], 
reward next is 0.6380, 
noisyNet noise sample is [array([-0.3289551], dtype=float32), -0.8441539]. 
=============================================
[2019-03-08 10:00:55,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8185908e-35 0.0000000e+00], sum to 1.0000
[2019-03-08 10:00:55,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9176
[2019-03-08 10:00:55,952] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7144553618031738, 6.9112, 6.9112, 290398.2913923446, 290398.2913923446, 82354.34997880379], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4985400.0000, 
sim time next is 4986000.0000, 
raw observation next is [20.0, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7124483138339663, 6.9112, 6.9112, 289581.5977731548, 289581.5977731548, 82182.57220972251], 
processed observation next is [1.0, 0.7391304347826086, 0.5454545454545454, 0.64, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7124483138339663, 0.0, 0.0, 0.12065899907214783, 0.12065899907214783, 0.20545643052430626], 
reward next is 0.5034, 
noisyNet noise sample is [array([-0.6261182], dtype=float32), 1.9060267]. 
=============================================
[2019-03-08 10:00:55,961] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[48.45682 ]
 [48.453926]
 [47.221466]
 [46.138977]
 [47.296726]], R is [[48.50337601]
 [48.52208328]
 [48.5409584 ]
 [48.05554962]
 [48.27769089]].
[2019-03-08 10:00:57,086] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8097404e-18 1.7092044e-26 1.4675110e-30 1.0000000e+00 8.2331489e-32], sum to 1.0000
[2019-03-08 10:00:57,094] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3477
[2019-03-08 10:00:57,100] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.5, 64.0, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 275953.6606809978, 275953.6606809978, 77836.33332360827], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4991400.0000, 
sim time next is 4992000.0000, 
raw observation next is [19.33333333333334, 65.33333333333333, 1.0, 2.0, 0.19, 1.0, 2.0, 0.19, 0.0, 2.0, 0.0, 6.9112, 6.9112, 275197.3314224715, 275197.3314224715, 77702.43279045644], 
processed observation next is [1.0, 0.782608695652174, 0.5151515151515155, 0.6533333333333333, 1.0, 1.0, 0.19, 1.0, 1.0, 0.19, 0.0, 1.0, 0.0, 0.0, 0.0, 0.11466555475936313, 0.11466555475936313, 0.1942560819761411], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5814989], dtype=float32), -0.8104064]. 
=============================================
[2019-03-08 10:00:57,112] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[48.578323]
 [48.017216]
 [50.661602]
 [50.627155]
 [50.63928 ]], R is [[48.11816406]
 [47.63698196]
 [47.72975159]
 [47.75918961]
 [47.78964615]].
[2019-03-08 10:01:07,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-08 10:01:07,098] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1486
[2019-03-08 10:01:07,114] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 66.66666666666666, 1.0, 2.0, 0.2829922732634086, 1.0, 2.0, 0.2829922732634086, 1.0, 2.0, 0.5100945952403068, 6.911200000000001, 6.9112, 618044.2572068004, 618044.2572068002, 154610.8117872229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5147400.0000, 
sim time next is 5148000.0000, 
raw observation next is [28.0, 66.0, 1.0, 2.0, 0.8234663527927324, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 602913.5757344427, 602913.5757344427, 133165.6675292025], 
processed observation next is [0.0, 0.6086956521739131, 0.9090909090909091, 0.66, 1.0, 1.0, 0.8234663527927324, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.2512139898893511, 0.2512139898893511, 0.3329141688230063], 
reward next is 0.6468, 
noisyNet noise sample is [array([-0.8767262], dtype=float32), -0.4887587]. 
=============================================
[2019-03-08 10:01:07,124] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[53.684425]
 [53.297195]
 [52.89914 ]
 [53.33331 ]
 [54.056915]], R is [[53.21053696]
 [53.24949265]
 [52.71699905]
 [52.18983078]
 [52.26413727]].
[2019-03-08 10:01:13,318] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.389646e-38 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-08 10:01:13,334] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8981
[2019-03-08 10:01:13,341] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.3, 100.0, 1.0, 2.0, 0.3710580938468487, 1.0, 2.0, 0.3710580938468487, 0.0, 2.0, 0.0, 6.9112, 6.9112, 552875.80763132, 552875.80763132, 112268.2609782923], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5265000.0000, 
sim time next is 5265600.0000, 
raw observation next is [21.23333333333333, 100.0, 1.0, 2.0, 0.3632946776529105, 1.0, 2.0, 0.3632946776529105, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 541730.2371132971, 541730.2371132972, 110785.9978451341], 
processed observation next is [1.0, 0.9565217391304348, 0.6015151515151514, 1.0, 1.0, 1.0, 0.3632946776529105, 1.0, 1.0, 0.3632946776529105, 0.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.22572093213054045, 0.2257209321305405, 0.27696499461283525], 
reward next is 0.6986, 
noisyNet noise sample is [array([-0.5788157], dtype=float32), -1.0220832]. 
=============================================
[2019-03-08 10:01:16,574] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 4.8730853e-33 0.0000000e+00 2.4107502e-19 0.0000000e+00], sum to 1.0000
[2019-03-08 10:01:16,580] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3789
[2019-03-08 10:01:16,592] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.03333333333333, 74.66666666666667, 1.0, 2.0, 0.3457210397818236, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6376640705199988, 6.9112, 6.9112, 518309.1534612288, 518309.1534612288, 126507.8378511941], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5520000.0000, 
sim time next is 5520600.0000, 
raw observation next is [23.85, 75.0, 1.0, 2.0, 0.3500927859964315, 1.0, 1.0, 0.3500927859964315, 0.0, 1.0, 0.0, 6.9112, 6.9112, 525860.2485416018, 525860.2485416018, 107831.4513908282], 
processed observation next is [1.0, 0.9130434782608695, 0.7204545454545456, 0.75, 1.0, 1.0, 0.3500927859964315, 1.0, 0.5, 0.3500927859964315, 0.0, 0.5, 0.0, 0.0, 0.0, 0.21910843689233409, 0.21910843689233409, 0.2695786284770705], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8807494], dtype=float32), -0.07330259]. 
=============================================
